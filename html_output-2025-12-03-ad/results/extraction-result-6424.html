<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-270062623</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.16265v4.pdf" target="_blank">MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</a></p>
                <p><strong>Paper Abstract:</strong> Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>M*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MindStar (M*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time, tree-search framework that improves multi-step mathematical reasoning by having an LLM generate multiple candidate next steps per node and using a process-supervised reward model to select and expand promising reasoning paths via beam search or Levin tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MindStar (inference-time framework applied to LLaMA-2-13B and Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>inference-time search framework (uses decoder-only LLMs as base models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A (framework applied to 7B and 13B base LLMs in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Relies on a separately trained process-supervised reward model (PRM) trained on PRM800K and task-specific process-reward data; base LLMs are pre-trained on their original corpora (not fine-tuned for M*).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and mathematical word problems (multi-step reasoning, algebraic/manipulative steps)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems presented with a step-by-step instruction prompt (chain-of-thought style); M* supplies previous steps and asks for next-step candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school arithmetic); MATH (harder contest-style math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought style step-by-step prompting; at each iteration the LLM is prompted with the question and the current reasoning path and asked to output multiple next steps (N candidates). Search over candidate steps is guided by PRM scores and either beam search or Levin tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (final-answer correctness as computed by automated evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using LLaMA-2-13B base: GSM8K accuracy improved from 41.8% (CoT-SC@16 baseline) to 66.3% (M* BS@16) and 68.8% (M* LevinTS@16); MATH accuracy improved from 20.4% (CoT-SC@16) to 32.4% (M* BS@16) and 33.9% (M* LevinTS@16).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>M* frames reasoning as a tree; uses a process-supervised reward model (PRM) to score candidate next steps conditioned on the full trajectory, enabling step-level selection and backtracking (with LevinTS). Paper provides theoretical guarantee of an upper bound on generated tokens for LevinTS via a cost function f(n)/π(n). No neuron-level or attention-probing analyses of numeric representations are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires more inference tokens (inference overhead); beam search is greedy and lacks backtracking so can get stuck on wrong paths; base LLMs sometimes fail to produce diverse step candidates (diversity saturation beyond ~8 candidates for LLaMA-2-13B); success depends on quality of PRM and its training data; math fine-tuning of base LLMs can degrade other capabilities and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improves with larger base LLM size (7B → 13B) and with larger PRM size (PRM-13B > PRM-7B). Performance also increases with more step-candidates sampled up to a saturation point where base LLM diversity limits further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based reward model that scores candidate next reasoning steps conditioned on the entire reasoning path (trajectory), trained to predict step-level correctness from process-supervised data (PRM800K plus task-specific augmentations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Process-supervised Reward Model (PRM) trained with LoRA adapters on LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based reward classifier (uses LLaMA-2 checkpoint + LoRA adapters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B base with LoRA adapter (LoRA trainable params ≈ 0.05%); experiments also use PRM-7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on PRM800K dataset (process-reward labeled trajectories); for GSM8K fine-tuning they generate positives from ground-truth reasoning and negatives by prompting GPT-3.5 to perturb correct steps.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used to guide search for GSM8K and MATH tasks (PRM800K used to pretrain PRM).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>step-level correctness prediction within multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Inputs are (current reasoning path, candidate next step); output is a scalar reward in [0,1] indicating likelihood the step is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>N/A (model scores steps across difficulty levels present in PRM800K, GSM8K, MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not a generation model for answers; it's a trained classifier used to score candidate steps produced by base LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Label classification accuracy on held-out PRM test splits (Figure 7 shows monotonic improvements with more data).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Exact percentages not tabulated in main text; authors report that PRM test accuracy improves with more training data (see Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>PRM conditions on the full trajectory so its scores encourage consistency and faithfulness across steps; PRM score is used multiplicatively to accumulate path probability π(n). No internal mechanistic dissection (e.g., which features or tokens PRM relies on) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Authors treat 'neutral' labels as incorrect for conservative scoring which may bias PRM; PRM performance and therefore search quality depend on the representativeness/coverage of PRM800K and generated negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Larger PRM models (13B) produce better differentiation in search than smaller PRMs (7B), particularly when used with larger base LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B (with M*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 13B (decoder-only transformer) evaluated with MindStar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 13B decoder-only transformer used as the base language model in M* experiments; when coupled with PRM-guided beam or LevinTS search, its math reasoning accuracy improves substantially on GSM8K and MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on Meta's mixture of web, code, and other corpora as per LLaMA-2 release (paper does not re-train the base LLM for M* experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems and mathematical reasoning (arithmetic, algebra, multi-step solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions; prompted with 'Let's think step by step' style CoT prompt and the M* step prompt template; M* samples N candidates per step and uses PRM-guided search.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school); MATH (more advanced/harder problems).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought (CoT) prompting; self-consistency baseline (CoT-SC@16); M* variants: BS@16 (Beam Search) and LevinTS@16 (Levin Tree Search) with branch factor 16 and max depth 5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: CoT-SC@16 = 41.8% → M* BS@16 = 66.3% → M* LevinTS@16 = 68.8%. MATH (500 eval questions): CoT-SC@16 = 20.4% → M* BS@16 = 32.4% → M* LevinTS@16 = 33.9% (GPT-3.5 reported at 34.1% for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports that LLaMA-2-13B tends to saturate in producing diverse step candidates beyond 8 samples; M* helps by searching and selecting promising trajectories rather than relying on a single sampled chain. No neuron-level probing of numeric representations is conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited diversity in candidate steps from base model at high sampling counts; unfaithful or incorrect step generation may still be sampled and require PRM to filter; increased inference token and computation cost relative to CoT-SC; beam search can be misled without backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improves when moving from smaller variants (7B) to 13B; M* effectiveness increases with larger base model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (with M*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B (decoder-only transformer) evaluated with MindStar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B decoder-only model used as a base LLM in M* experiments that shows substantial accuracy gains on math reasoning when guided by PRM and tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on Mistral release corpora (not fine-tuned for M* experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions with CoT-style prompts; M* step-level candidate generation and PRM-guided search.</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school) and MATH (harder problems)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought prompting; CoT-SC@16 baseline; M* BS@16 and LevinTS@16 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MATH: CoT-SC@16 baseline = 23.9% → M* BS@16 = 36.2% → M* LevinTS@16 = 38.2%. On GSM8K, M* variants yield comparable relative gains (~+52% and +59.8% relative improvement reported in text over CoT-SC@16 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Similar qualitative behavior as LLaMA-2: PRM-guided step-level selection substantially improves final-answer accuracy compared to sampling-only baselines. No mechanistic probing of arithmetic token processing is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same general limitations: inference overhead, requirement for a quality PRM, potential lack of candidate diversity at high sample counts, beam search brittleness without backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Absolute performance lower than 13B base but benefits strongly from M*; PRM size and base model size both influence gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6424.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam Search (M* BS@16)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search variant of M* (step-level beam search guided by PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A greedy step-wise search variant in the M* framework that ranks N candidate next-steps with PRM scores and selects the top-scoring child at each expansion; computationally cheaper but lacks guaranteed backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Beam Search (used within M*; BS@16 indicates branch factor 16)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>search algorithm (greedy beam over step-level candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A (algorithmic component that uses PRM trained on PRM800K and base LLM generations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math reasoning (step-level candidate selection)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Operates on candidate steps generated by base LLM given natural-language prompts</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Evaluated across GSM8K (easier) and MATH (harder)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-level CoT generation; beam picks highest PRM-scoring child each iteration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy; inference token cost</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported relative improvements for LLaMA-2-13B: +58.6% on GSM8K and +58.8% on MATH (relative to baseline CoT-SC@16) when using beam-search M*. Compared to CoT-SC@16, Beam search incurred ~1.5x tokens per question on average and produced up to 66% absolute improvement on some measures (paper reports both relative and absolute improvements across tables).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Beam search acts as step-wise ranking using PRM score as heuristic; time complexity O(n) similar to self-consistency/re-ranking. No internal model mechanistic probing reported. Beam is simple and cheap but cannot backtrack to correct earlier wrong choices.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Greedy selection may commit to incorrect reasoning path without backtracking; performance limited by PRM accuracy and candidate diversity; may be misled when PRM scores are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance improves when more candidates are sampled (up to diversity limits of base LLM); larger PRM models improve beam's selection quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6424.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LevinTS (M* LevinTS@16)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levin Tree Search variant used in M*</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A best-first tree search algorithm used within M* that expands nodes by increasing order of a cost function f(n)/π(n), combining path computation cost and PRM-estimated probability, and supporting backtracking and an upper bound on token generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Levin Tree Search (used within M*; LevinTS@16 indicates branch factor 16)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>search algorithm (best-first tree search with a cost/probability tradeoff)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A (algorithmic component using PRM and base LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step mathematical reasoning (search over step-level expansions)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Operates on candidate steps output by base LLM given natural-language prompt</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Evaluated on grade-school to contest-style math problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Step-level CoT generation; selection/expansion governed by cost function that includes PRM probabilities and token-cost exponential term</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy; inference token count; theoretical upper bound on tokens generated to reach target nodes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Outperforms beam-search modestly; for LLaMA-2-13B M* LevinTS@16 achieved 68.8% on GSM8K (vs 66.3% BS@16) and 33.9% on MATH (vs 32.4% BS@16). LevinTS typically costs ~2x the compute of Beam search but yields an extra ~1.5–3% absolute accuracy gain.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>LevinTS balances short high-probability trajectories (via PRM) and token-generation cost; paper proves an upper bound on the number of generated tokens before reaching any target node: |N(LevinTS, N_g)| ≤ min_{n ∈ N_g} f(n)/π(n). LevinTS supports backtracking, making it more robust to earlier wrong steps than greedy beam.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Higher inference cost than beam search; still depends on PRM accuracy and candidate coverage; theoretical guarantees depend on chosen cost function and proper PRM probability estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Gives larger relative gains when PRM and base LLM are larger and when the search space is well-scored by PRM; more expensive but more robust than greedy beam.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6424.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6424.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (reported baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 (comparison baseline reported by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source large language model used as a comparison point; authors report published accuracies for GPT-3.5 on MATH to contextualize M* performance on smaller open models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper (publicly trained on mixture of web/code/dialogue corpora per OpenAI disclosures).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (reported), GSM8K (other literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems with chain-of-thought/few-shot prompting (4-shot mentioned for GPT-3.5 in figure captions).</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>MATH: higher difficulty; GSM8K: grade-school</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot CoT variants reported in external reports (authors cite GPT-3.5 reported metrics as comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final-answer accuracy (reported from external/OpenAI reports)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3.5 reported MATH accuracy = 34.1% (for comparison in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic analysis in this paper (GPT-3.5 used only as an external baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not analyzed in this paper beyond being a performance baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Levin tree search with context models <em>(Rating: 2)</em></li>
                <li>Single-agent policy tree search with guarantees <em>(Rating: 2)</em></li>
                <li>PRM800K (process-supervised reward model dataset / related work) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6424",
    "paper_id": "paper-270062623",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "M*",
            "name_full": "MindStar (M*)",
            "brief_description": "An inference-time, tree-search framework that improves multi-step mathematical reasoning by having an LLM generate multiple candidate next steps per node and using a process-supervised reward model to select and expand promising reasoning paths via beam search or Levin tree search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MindStar (inference-time framework applied to LLaMA-2-13B and Mistral-7B)",
            "model_family": "inference-time search framework (uses decoder-only LLMs as base models)",
            "model_size": "N/A (framework applied to 7B and 13B base LLMs in experiments)",
            "training_data_description": "Relies on a separately trained process-supervised reward model (PRM) trained on PRM800K and task-specific process-reward data; base LLMs are pre-trained on their original corpora (not fine-tuned for M*).",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step arithmetic and mathematical word problems (multi-step reasoning, algebraic/manipulative steps)",
            "problem_format": "Natural-language math word problems presented with a step-by-step instruction prompt (chain-of-thought style); M* supplies previous steps and asks for next-step candidates.",
            "difficulty_level": "GSM8K (grade-school arithmetic); MATH (harder contest-style math problems).",
            "prompting_method": "Chain-of-thought style step-by-step prompting; at each iteration the LLM is prompted with the question and the current reasoning path and asked to output multiple next steps (N candidates). Search over candidate steps is guided by PRM scores and either beam search or Levin tree search.",
            "performance_metric": "Accuracy (final-answer correctness as computed by automated evaluation suite)",
            "performance_value": "Using LLaMA-2-13B base: GSM8K accuracy improved from 41.8% (CoT-SC@16 baseline) to 66.3% (M* BS@16) and 68.8% (M* LevinTS@16); MATH accuracy improved from 20.4% (CoT-SC@16) to 32.4% (M* BS@16) and 33.9% (M* LevinTS@16).",
            "internal_analysis": "M* frames reasoning as a tree; uses a process-supervised reward model (PRM) to score candidate next steps conditioned on the full trajectory, enabling step-level selection and backtracking (with LevinTS). Paper provides theoretical guarantee of an upper bound on generated tokens for LevinTS via a cost function f(n)/π(n). No neuron-level or attention-probing analyses of numeric representations are reported.",
            "failure_modes": "Requires more inference tokens (inference overhead); beam search is greedy and lacks backtracking so can get stuck on wrong paths; base LLMs sometimes fail to produce diverse step candidates (diversity saturation beyond ~8 candidates for LLaMA-2-13B); success depends on quality of PRM and its training data; math fine-tuning of base LLMs can degrade other capabilities and safety.",
            "scaling_trend": "Performance improves with larger base LLM size (7B → 13B) and with larger PRM size (PRM-13B &gt; PRM-7B). Performance also increases with more step-candidates sampled up to a saturation point where base LLM diversity limits further gains.",
            "uuid": "e6424.0",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "PRM",
            "name_full": "Process-supervised Reward Model (PRM)",
            "brief_description": "A transformer-based reward model that scores candidate next reasoning steps conditioned on the entire reasoning path (trajectory), trained to predict step-level correctness from process-supervised data (PRM800K plus task-specific augmentations).",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "use",
            "model_name": "Process-supervised Reward Model (PRM) trained with LoRA adapters on LLaMA-2-13B",
            "model_family": "transformer-based reward classifier (uses LLaMA-2 checkpoint + LoRA adapters)",
            "model_size": "13B base with LoRA adapter (LoRA trainable params ≈ 0.05%); experiments also use PRM-7B",
            "training_data_description": "Trained on PRM800K dataset (process-reward labeled trajectories); for GSM8K fine-tuning they generate positives from ground-truth reasoning and negatives by prompting GPT-3.5 to perturb correct steps.",
            "benchmark_name": "Used to guide search for GSM8K and MATH tasks (PRM800K used to pretrain PRM).",
            "task_type": "step-level correctness prediction within multi-step math reasoning",
            "problem_format": "Inputs are (current reasoning path, candidate next step); output is a scalar reward in [0,1] indicating likelihood the step is correct.",
            "difficulty_level": "N/A (model scores steps across difficulty levels present in PRM800K, GSM8K, MATH)",
            "prompting_method": "Not a generation model for answers; it's a trained classifier used to score candidate steps produced by base LLMs.",
            "performance_metric": "Label classification accuracy on held-out PRM test splits (Figure 7 shows monotonic improvements with more data).",
            "performance_value": "Exact percentages not tabulated in main text; authors report that PRM test accuracy improves with more training data (see Figure 7).",
            "internal_analysis": "PRM conditions on the full trajectory so its scores encourage consistency and faithfulness across steps; PRM score is used multiplicatively to accumulate path probability π(n). No internal mechanistic dissection (e.g., which features or tokens PRM relies on) is provided.",
            "failure_modes": "Authors treat 'neutral' labels as incorrect for conservative scoring which may bias PRM; PRM performance and therefore search quality depend on the representativeness/coverage of PRM800K and generated negatives.",
            "scaling_trend": "Larger PRM models (13B) produce better differentiation in search than smaller PRMs (7B), particularly when used with larger base LLMs.",
            "uuid": "e6424.1",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA-2-13B (with M*)",
            "name_full": "LLaMA-2 13B (decoder-only transformer) evaluated with MindStar",
            "brief_description": "Open-source 13B decoder-only transformer used as the base language model in M* experiments; when coupled with PRM-guided beam or LevinTS search, its math reasoning accuracy improves substantially on GSM8K and MATH.",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B",
            "model_family": "decoder-only transformer",
            "model_size": "13B parameters",
            "training_data_description": "Pretrained on Meta's mixture of web, code, and other corpora as per LLaMA-2 release (paper does not re-train the base LLM for M* experiments).",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step word problems and mathematical reasoning (arithmetic, algebra, multi-step solutions)",
            "problem_format": "Natural-language questions; prompted with 'Let's think step by step' style CoT prompt and the M* step prompt template; M* samples N candidates per step and uses PRM-guided search.",
            "difficulty_level": "GSM8K (grade-school); MATH (more advanced/harder problems).",
            "prompting_method": "Chain-of-thought (CoT) prompting; self-consistency baseline (CoT-SC@16); M* variants: BS@16 (Beam Search) and LevinTS@16 (Levin Tree Search) with branch factor 16 and max depth 5.",
            "performance_metric": "Final-answer accuracy",
            "performance_value": "GSM8K: CoT-SC@16 = 41.8% → M* BS@16 = 66.3% → M* LevinTS@16 = 68.8%. MATH (500 eval questions): CoT-SC@16 = 20.4% → M* BS@16 = 32.4% → M* LevinTS@16 = 33.9% (GPT-3.5 reported at 34.1% for comparison).",
            "internal_analysis": "Paper reports that LLaMA-2-13B tends to saturate in producing diverse step candidates beyond 8 samples; M* helps by searching and selecting promising trajectories rather than relying on a single sampled chain. No neuron-level probing of numeric representations is conducted.",
            "failure_modes": "Limited diversity in candidate steps from base model at high sampling counts; unfaithful or incorrect step generation may still be sampled and require PRM to filter; increased inference token and computation cost relative to CoT-SC; beam search can be misled without backtracking.",
            "scaling_trend": "Performance improves when moving from smaller variants (7B) to 13B; M* effectiveness increases with larger base model sizes.",
            "uuid": "e6424.2",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Mistral-7B (with M*)",
            "name_full": "Mistral-7B (decoder-only transformer) evaluated with MindStar",
            "brief_description": "Open-source 7B decoder-only model used as a base LLM in M* experiments that shows substantial accuracy gains on math reasoning when guided by PRM and tree search.",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "use",
            "model_name": "Mistral-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B parameters",
            "training_data_description": "Pretrained on Mistral release corpora (not fine-tuned for M* experiments).",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step arithmetic and math word problems",
            "problem_format": "Natural-language questions with CoT-style prompts; M* step-level candidate generation and PRM-guided search.",
            "difficulty_level": "GSM8K (grade-school) and MATH (harder problems)",
            "prompting_method": "Chain-of-thought prompting; CoT-SC@16 baseline; M* BS@16 and LevinTS@16 variants.",
            "performance_metric": "Final-answer accuracy",
            "performance_value": "MATH: CoT-SC@16 baseline = 23.9% → M* BS@16 = 36.2% → M* LevinTS@16 = 38.2%. On GSM8K, M* variants yield comparable relative gains (~+52% and +59.8% relative improvement reported in text over CoT-SC@16 baseline).",
            "internal_analysis": "Similar qualitative behavior as LLaMA-2: PRM-guided step-level selection substantially improves final-answer accuracy compared to sampling-only baselines. No mechanistic probing of arithmetic token processing is provided.",
            "failure_modes": "Same general limitations: inference overhead, requirement for a quality PRM, potential lack of candidate diversity at high sample counts, beam search brittleness without backtracking.",
            "scaling_trend": "Absolute performance lower than 13B base but benefits strongly from M*; PRM size and base model size both influence gains.",
            "uuid": "e6424.3",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Beam Search (M* BS@16)",
            "name_full": "Beam search variant of M* (step-level beam search guided by PRM)",
            "brief_description": "A greedy step-wise search variant in the M* framework that ranks N candidate next-steps with PRM scores and selects the top-scoring child at each expansion; computationally cheaper but lacks guaranteed backtracking.",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "use",
            "model_name": "Beam Search (used within M*; BS@16 indicates branch factor 16)",
            "model_family": "search algorithm (greedy beam over step-level candidates)",
            "model_size": "N/A",
            "training_data_description": "N/A (algorithmic component that uses PRM trained on PRM800K and base LLM generations).",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step math reasoning (step-level candidate selection)",
            "problem_format": "Operates on candidate steps generated by base LLM given natural-language prompts",
            "difficulty_level": "Evaluated across GSM8K (easier) and MATH (harder)",
            "prompting_method": "Step-level CoT generation; beam picks highest PRM-scoring child each iteration",
            "performance_metric": "Final-answer accuracy; inference token cost",
            "performance_value": "Reported relative improvements for LLaMA-2-13B: +58.6% on GSM8K and +58.8% on MATH (relative to baseline CoT-SC@16) when using beam-search M*. Compared to CoT-SC@16, Beam search incurred ~1.5x tokens per question on average and produced up to 66% absolute improvement on some measures (paper reports both relative and absolute improvements across tables).",
            "internal_analysis": "Beam search acts as step-wise ranking using PRM score as heuristic; time complexity O(n) similar to self-consistency/re-ranking. No internal model mechanistic probing reported. Beam is simple and cheap but cannot backtrack to correct earlier wrong choices.",
            "failure_modes": "Greedy selection may commit to incorrect reasoning path without backtracking; performance limited by PRM accuracy and candidate diversity; may be misled when PRM scores are imperfect.",
            "scaling_trend": "Performance improves when more candidates are sampled (up to diversity limits of base LLM); larger PRM models improve beam's selection quality.",
            "uuid": "e6424.4",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LevinTS (M* LevinTS@16)",
            "name_full": "Levin Tree Search variant used in M*",
            "brief_description": "A best-first tree search algorithm used within M* that expands nodes by increasing order of a cost function f(n)/π(n), combining path computation cost and PRM-estimated probability, and supporting backtracking and an upper bound on token generation.",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "use",
            "model_name": "Levin Tree Search (used within M*; LevinTS@16 indicates branch factor 16)",
            "model_family": "search algorithm (best-first tree search with a cost/probability tradeoff)",
            "model_size": "N/A",
            "training_data_description": "N/A (algorithmic component using PRM and base LLM outputs)",
            "benchmark_name": "GSM8K, MATH",
            "task_type": "multi-step mathematical reasoning (search over step-level expansions)",
            "problem_format": "Operates on candidate steps output by base LLM given natural-language prompt",
            "difficulty_level": "Evaluated on grade-school to contest-style math problems",
            "prompting_method": "Step-level CoT generation; selection/expansion governed by cost function that includes PRM probabilities and token-cost exponential term",
            "performance_metric": "Final-answer accuracy; inference token count; theoretical upper bound on tokens generated to reach target nodes",
            "performance_value": "Outperforms beam-search modestly; for LLaMA-2-13B M* LevinTS@16 achieved 68.8% on GSM8K (vs 66.3% BS@16) and 33.9% on MATH (vs 32.4% BS@16). LevinTS typically costs ~2x the compute of Beam search but yields an extra ~1.5–3% absolute accuracy gain.",
            "internal_analysis": "LevinTS balances short high-probability trajectories (via PRM) and token-generation cost; paper proves an upper bound on the number of generated tokens before reaching any target node: |N(LevinTS, N_g)| ≤ min_{n ∈ N_g} f(n)/π(n). LevinTS supports backtracking, making it more robust to earlier wrong steps than greedy beam.",
            "failure_modes": "Higher inference cost than beam search; still depends on PRM accuracy and candidate coverage; theoretical guarantees depend on chosen cost function and proper PRM probability estimates.",
            "scaling_trend": "Gives larger relative gains when PRM and base LLM are larger and when the search space is well-scored by PRM; more expensive but more robust than greedy beam.",
            "uuid": "e6424.5",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 (reported baseline)",
            "name_full": "OpenAI GPT-3.5 (comparison baseline reported by authors)",
            "brief_description": "A closed-source large language model used as a comparison point; authors report published accuracies for GPT-3.5 on MATH to contextualize M* performance on smaller open models.",
            "citation_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5",
            "model_family": "decoder-only transformer (closed-source)",
            "model_size": "Not specified in this paper (closed-source)",
            "training_data_description": "Not specified in this paper (publicly trained on mixture of web/code/dialogue corpora per OpenAI disclosures).",
            "benchmark_name": "MATH (reported), GSM8K (other literature)",
            "task_type": "multi-step math reasoning",
            "problem_format": "Natural-language math word problems with chain-of-thought/few-shot prompting (4-shot mentioned for GPT-3.5 in figure captions).",
            "difficulty_level": "MATH: higher difficulty; GSM8K: grade-school",
            "prompting_method": "Few-shot CoT variants reported in external reports (authors cite GPT-3.5 reported metrics as comparison).",
            "performance_metric": "Final-answer accuracy (reported from external/OpenAI reports)",
            "performance_value": "GPT-3.5 reported MATH accuracy = 34.1% (for comparison in paper).",
            "internal_analysis": "No internal mechanistic analysis in this paper (GPT-3.5 used only as an external baseline).",
            "failure_modes": "Not analyzed in this paper beyond being a performance baseline.",
            "scaling_trend": "Not analyzed in this paper.",
            "uuid": "e6424.6",
            "source_info": {
                "paper_title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Levin tree search with context models",
            "rating": 2,
            "sanitized_title": "levin_tree_search_with_context_models"
        },
        {
            "paper_title": "Single-agent policy tree search with guarantees",
            "rating": 2,
            "sanitized_title": "singleagent_policy_tree_search_with_guarantees"
        },
        {
            "paper_title": "PRM800K (process-supervised reward model dataset / related work)",
            "rating": 1,
            "sanitized_title": "prm800k_processsupervised_reward_model_dataset_related_work"
        }
    ],
    "cost": 0.019855249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
26 Jun 2024</p>
<p>Jikun Kang jaxon.kang@huawei.com 
Noah's Ark Laboratory</p>
<p>Derek Li derek.li1@huawei.com 
Noah's Ark Laboratory</p>
<p>Xi Chen xi.chen4@huawei.com 
Noah's Ark Laboratory</p>
<p>Amirreza Kazemi amirreza.kazemi@huawei.com 
Noah's Ark Laboratory</p>
<p>Qianyi Sun qianyi.sun@huawei.com 
Noah's Ark Laboratory</p>
<p>Boxing Chen boxing.chen@huawei.com 
Noah's Ark Laboratory</p>
<p>Dong Li 
Noah's Ark Laboratory</p>
<p>Xu He 
Noah's Ark Laboratory</p>
<p>Quan He hequan12@huawei.com 
Noah's Ark Laboratory</p>
<p>Feng Wen feng.wen@huawei.com 
Noah's Ark Laboratory</p>
<p>Jianye Hao haojianye@huawei.com 
Noah's Ark Laboratory</p>
<p>Jun Yao yaojun97@huawei.com 
Noah's Ark Laboratory</p>
<p>MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time
26 Jun 2024F6CCC9783ECFDDBC8388C27C285E410AarXiv:2405.16265v4[cs.LG]
Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions.Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or selfimprovement techniques.However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning.Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method-MindStar (M<em>).This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths.We evaluate the M</em> framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs.Our results demonstrate that M<em> significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.</em>Equal contribution.</p>
<p>We organize the reasoning process as a tree.Each node represents either question (the root node), answers (leaf nodes), or reasoning steps (all other nodes).A searching method traverses the reasoning tree and select a node to expand.We add the reasoning step of the selected node back to the prompt for next query step.We stop the generation processes until either the answer is find or the maximum consumption is reached.</p>
<p>Introduction</p>
<p>With the rapid growth of model size, transformer-based Large Language Models (LLMs) showcase impressive results in domains such as instruction following [37,32], coding assistance [25,4], and creative writing [11].Among these tasks, unlocking the rationality of LLMs to solve complex reasoning tasks remains a major challenge.Recent works [49,36] have attempted to tackle this challenge through Supervised Fine-Tuning (SFT).By mixing crafted new reasoning data samples with original datasets, LLMs learn the underlying distributions of these samples and attempt to solve unseen reasoning tasks.Although there is a performance gain, this method heavily relies on extensive training and requires extra data preparation [33,42].Recently, Llama-3 report [28] highlights a significant observation: when posed with a challenging reasoning question, a model will sometimes generate the correct reasoning trace.This indicates that the model knows how to produce the right answer but struggles with selecting it.</p>
<p>Inspired by this discovery, we pose a straightforward question: Can we enhance the reasoning of LLMs during generation by assisting them in selecting the correct output?To explore this, we conduct an experiment utilizing different reward models to assist LLM for output selection.</p>
<p>Here, we leverage the Outcome-supervised Reward Model (ORM) [6], which scores the entirety of reasoning solutions, and the Process-supervised Reward Model (PRM) [21], which scores each individual reasoning step, for the selection of reasoning solutions.Initially, we apply both the ORM and the PRM to select the final answer from multiple sampled chain-of-thoughts (CoT) solutions.Figure 2 shows that PRM selects better reasoning answers than ORM.Additionally, we employ the PRM to assist the LLM in a tree-of-thought context; Rather than generating the complete solution, the LLM produces multiple intermediate steps.The PRM then scores these steps and selects the best, facilitating the LLM in proceeding generation from a promising step.Our results demonstrate that step-level selection outperforms the two CoT selection baselines significantly.Based on above findings, we propose MindStar (M<em>), a novel framework depicted in Figure 1, tailored for enhancing LLM reasoning during the inference time.Initially, M</em> prompts the LLM with the question to generate multiple potential next steps.In the context of reasoning tree, the question is the root and the new generated steps are its children.Subsequently, the trained process-supervised reward model (PRM) scores the steps based on their likelihood of correctness.The selected step will then be appended to the prompt, and the algorithm iterates until the final answer is reached or computational budgets are exceeded.Leveraging the reward model to help the LLM asses its reasoning steps serves as a self-reflection mechanism.Note that unlike existing self-reflection methods [16,44] that only revise the most recent step, M<em> reflects on the entire trajectory comprising all previous steps.Thus, it avoids the pitfall of optimizing performance solely based on current step, and allows the model to select faithful reasoning solutions.Moreover, in order to select the best trajectory at each iteration, M</em> can be coupled with various tree search algorithm.In this paper, we explore two algorithms, which are beam search [24] and levin tree search [30].The beam search is a greedy algorithm that uses the PRM score as heuristic, while Levin tree search (LevinTS) takes both the PRM score and the depth of a trajectory into account.Furthermore, we show that M* coupled with LevinTS guarantees a computation upperbound in finding the correct solution.</p>
<p>We evaluate M* on challenging MATH problems [14] and compared it to existing open and closedsource LLMs, including LLaMA-2 [40], Grok-1, GPT [1], Claude [2], and Gemini [38].The results, shown in Figure 3, indicate that by utilizing LLaMA-2-13B as the base model, our method significantly improves its performance on MATH dataset from 8% to 33%.This performance matches that of GPT-3.5, but with approximately 200 times less computational resource usage in inference time.These results highlight the benefits of shifting computational resources from fine-tuning to inference searching and shed light on potential future research directions.</p>
<p>We summarize our major contributions as follows: 1) we introduce M<em>, a tree-like search-based reasoning framework that enhances the reasoning capabilities of Large Language Models (LLMs) through a structured, step-by-step approach during the inference time.2) we propose the adaptation of two search algorithms in accomplishing LLM reasoning tasks, namely beam search and Levin tree search, which helps traverse the reasoning tree with guaranteed search time.3) we evaluate the performance of the M</em> on the GSM8K and MATH datasets.The results show that using beam search and Levin tree search improves the performance of the LLama-2-13B model by 58.6% and 64.6% on the GSM8K dataset, respectively, and by 58.8% and 66.1% on the MATH dataset, respectively.</p>
<p>Related Work</p>
<p>Multi-step Reasoning in LLMs.In recent years, several methods have been proposed to enhance LLM reasoning capability, ranging from fine-tuning the base model [5,9,20,50] to chain-of-thought (CoT) prompting and its variants [19,45,51,43,6].Specifically, Wei et al. [45] and Kojima et al. [19] demonstrate that CoT prompting can enhance LLM reasoning in few-shot and zero-shot settings.Such in-context improvement grounds in the decoder architecture of LLMs, however, a single reasoning path (i.e., greedy decoding) often suffers from stochasticity and lacks the diversity needed for complex reasoning tasks.To mitigate this, Wang et al. [43] proposes to generate a diverse set of reasoning paths and perform a majority voting.Similarly, Cobbe et al. [6] trains a solution verifier and Weng et al. [46] prompts LLM for self-verification in order to determine the quality of generated reasoning paths.Despite this, recent studies [10,26,41] found that LLMs often make unfaithful reasoning.This sheds light to the importance of verifying each step of the reasoning chain [21].Moreover, CoT does not take different alternatives into account at the generation time, and there is no mechanism to evaluate the current generated chain and possibly look ahead or backtrack.Therefore, our work largely differs from the CoT literature since we utilize the step-level feedback in order to search for a reasoning path within a reasoning tree.</p>
<p>Feedback-Guided Tree Search for LLM Reasoning.The ToT framework is introduced in [48,23].</p>
<p>Inspired by this, various methods [8,27,12,47,3] have been proposed to find a good reasoning path within the tree, employing different heuristics and search algorithms.A straightforward heuristic is that one prompt the LLM itself to assess its generated steps, as demonstrated in Yao et al. [48] with breadth/depth-first search, in Hao et al. [12] with Monte Carlo tree search, and in Xie et al. [47] with beam search.However, recent studies have shown that LLM struggles to evaluate itself and rectify its initial responses without any external feedback [17,8].In contrast, our method's search heuristic relies on a reward model and thus performs more accurately.In a different approach, Feng et al. [8] and Tian et al. [39] propose learning the value function to estimate the value of the current reasoning path, while Ma et al. [27] trains a process-supervised reward model (PRM) and utilizes it with A * -like tree search.In comparison, our method is more computationally efficient since we do not deal with sample complexity issues of value function learning.In particular, we show that incorporating PRM as a heuristic with Levin tree search guarantees an upper bound on computation cost [30].</p>
<p>M*: Think and Reflect Step by Step</p>
<p>As illustrated in Figure 1, we propose a novel framework that facilitates LLMs reasoning abilities at inference time.The brief overview of the M* algorithm is summarized in Algorithm 1.</p>
<p>Problem Formulation</p>
<p>We define a large language model (LLM) parameterized by θ, as G(•; θ).We also define a reasoning tree T , where the root is the question, the edges are the generated intermediate steps by LLM, and the nodes are the sequences of steps.In other words, a node in the reasoning tree represents a reasoning path consisting of edges in the path from the root to that node, denoted as
n d = [n q ⊕ e 1 ⊕ e 2 ⊕ • • • ⊕ e d−1 ],
where n q represents the root node (question), e i represents the edge (step) at depth i, and ⊕ is the concatenation operation.In this paper, we use terms node and reasoning path interchangeably, as well as edge and reasoning step.Our goal is to find the node that consists of correct reasoning steps for the desired question.To achieve this, we utilize a process-supervised reward model coupled with a tree search algorithm, which will be introduced in the following sections.</p>
<p>Process-supervised Reward Model</p>
<p>As mentioned earlier, we aim to assess the intermediate steps generated by LLMs to help select the correct reasoning path.Building on the success of the Process-supervised Reward Model (PRM) [21], we utilize a PRM to measure the likelihood of correctness for each step.We now describe the M* algorithm, which consists of two steps.Until finding the correct solution, at each iteration of the algorithm, 1) we prompt the base LLM to generate next steps for the current reasoning path, 2) we evaluate the generated steps using PRM and select a reasoning path for the next round of algorithm.</p>
<p>Reasoning Node Expansion</p>
<p>Given that we select a reasoning node n d to expand, we design a prompt template Example 3.1 in order to collect next steps from LLMs.As shown in the example, the LLM takes the original question as {question} and the current reasoning path as {answer} in the prompt.Note that in the first iteration of the algorithm, the selected node is the root containing the question only, and therefore the {answer} is empty.For the reasoning path n d , the LLM generates N multiple intermediate steps e for the given prompt and we append them as the children node of the current node.In the next step of the algorithm, the new child nodes will be assessed, and a new node will be selected for further expansion.We also acknowledge that one alternative for generating the steps is fine-tuning the LLM using step tokens.However, it could potentially degrade the LLM's reasoning ability and, more importantly, is not aligned with the focus of this paper which, is enhancing the LLM without any weight modification.</p>
<p>Example 3.1: Step Prompt Template</p>
<p>[INST] «SYS» Below is an instruction that describes a task.Write a response that appropriately completes the request.Output each step in a separate line, and explicitly state the final answer after the final step following the format."The answer is:" «/SYS» Instruction:{question}[/INST] Response: Let's think step by step.{answer}</p>
<p>Reasoning Path Selection</p>
<p>Following the reasoning node expansion, we use the pre-trained PRM P to reflect each newly generated step.As mentioned in Section 3.2, the PRM takes the path n d and the steps e d as inputs and returns the corresponding reward value.After the evaluation, we require a tree search algorithm to select the next node for expansion.Note that our framework is agnostic to the search algorithm, and in this work, we instantiate it with two tree search methods, namely beam search and Levin tree search.Additionally, we introduce an ensemble method of M* search as an extension -Forest Search in Appendix C.</p>
<p>Beam Search.We first employ beam search, an algorithm similar to how a language model generates tokens while decoding.After computing the reward value of the pairs of reasoning path and next step, the algorithm selects the next step with the highest value, e * d = arg max ei∈{e 1 and the selected reasoning path for the next iteration is
n d+1 = [n d ⊕ e * d ]
. The beam search algorithm can be viewed as a step-wise ranking method.Although it searches within a rich space of reasoning tree, its time-complexity is O(n), comparable to self-consistency and re-ranking methods.However, beam search only takes the PRM reward score into account and it lacks backtracking or self-correction mechanism.Moreover, there is no guarantee that beam search is able to find the correct reasoning path.To address these issues, we propose another M* variant with Levin tree search.</p>
<p>Levin Tree Search.Levin Tree Search (LevinTS) [30] is a best-first tree search algorithm [34], which relies on a cost function.The cost function is defined as f (n) π(n) and the algorithm expands by its increasing order.The computation cost of node n, denoted as f (n), is defined as f (n) := e τ •i tok , where i tok is the number of tokens in the reasoning path corresponding to node n, and τ is a temperature parameter.The symbol π(n) denotes the probability that the solution exists under the sub-tree for which the root is node n.Therefore, π for the root is equal to 1.For a node n with parent n ′ connected by an edge e ′ , π(n) := π(n ′ ) • e P(n ′ ,e ′ ) N i=1 e P(n ′ ,e i ) , where P is the PRM and e i is the generated step by the LLM.One can see that a child node has strictly higher cost compared to its parent, which means that the algorithm favors short reasoning path with high PRM reward scores.Interestingly, by taking into account the cost of the nodes as well as the PRM score, LevinTS can guarantee an upper bound on the number of generated tokens.More precisely, Theorem 3.1, which is an extension of Theorem 3 in Orseau et al. [31], shows that the number of generated tokens is always less than the cost f (n) π(n) of any target nodes (proof in Appendix D).It is also worth mentioning that LevinTS supports backtracking, meaning that the selected node for the next iteration is not necessarily the child of the current node.This implies that LevinTS is also more robust to beam search, and selecting a wrong step does not prevent the algorithm from reaching the correct reasoning path.The details of beam search and Levin tree search algorithms are explained in Appendix B.</p>
<p>Theorem 3.1: LevinTS Upper Bound</p>
<p>Let N g be a set of target nodes, let τ ≥ 1, and let the computation cost of a node n be defined as f (n) = e τ •i tok .Then, LevinTS ensures that the number of generated tokens | N (LevinTS, N g )| before reaching any of the target nodes is bounded by,
| N (LevinTS, N g )| ≤ min n∈N g f (n) π(n)
Algorithm 1: Generic M<em> Algorithm Input: Question node n q , PRM P(), language model G(; θ), maximum depth D, branch factor N , reasoning tree T .Initialization: T = {(n q , 1)} while True do n, r = get_node(T ) /</em> w.r.t tree search algorithm <em>/ if n is the answer or get_depth(n
) &gt; D then return n for i ← 0 to N − 1 do e i ← G(n; θ) /</em> Expansion <em>/ n i ← n ⊕ e i /</em> New node <em>/ r i ← r × P(n, e i ) /</em> Compute reward using PRM */ add_node(T , (n i , r i ))
4 Evaluation</p>
<p>We evaluate the M<em> method to answer the following questions. 1) How does M</em> improve LLMs performance on math reasoning tasks?2) How does M<em> scale with reasoning tree size? 3) How much extra computation resources costs by M</em>?</p>
<p>Evaluation Setups</p>
<p>Benchmarks: M* is a versatile framework applicable to a variety of reasoning tasks.In this study, we focus our experiments on two widely known mathematical reasoning benchmarks: the GSM8K dataset [6] and the MATH dataset [14].It is important to note that we evaluate only 500 of the 4500 test questions from the MATH dataset.This is because the remaining 4000 questions are part of the PRM800K [21] dataset, on which the process-supervised reward model is trained.</p>
<p>Evaluation Method: For the purposes of reproducibility and transparency, we assess our results using OpenAI's evaluation tool suite 1 .Specifically, for mathematical reasoning questions, this suite calculates the accuracy by comparing the final reasoning answers with the ground truth.</p>
<p>Baseline LLMs</p>
<p>We evaluate the performance of M<em> on a set of general open-source models of various sizes, including Mistral-7B [18] and Llama-2-13B [40].We do not apply M</em> directly to a math fine-tuned model because, although it excels at math problems, its performance declines on other datasets and raises safety concerns.A detailed analysis can be found in Appendix E.4.Also, we consider two M<em> variants in the experiments, M</em> (BS@16) and M<em> (LevinTS@16) which represent the beam search and levin tree search algorithms with branch factor of 16, respectively.For a fair comparison, we compare our results with two baseline methods proposed for enhancing LLM reasoning at inference: CoT and CoT-SC@16.For the CoT method, we append a sentence to the prompt asking the language model to reason step-by-step.CoT-SC@16 also represents the CoT method with self-consistency, that is sampling 16 candidate answers and selecting the consistent one.Furthermore, we compare our results against closed-source models, including OpenAI's GPT-4 and GPT-3.5, Anthropic's Claude-3 and Claude-2, as well as Google's Gemini model family.It is important to note that the results for closed-source models were taken from their respective reports.We present these results to demonstrate how effectively M</em> narrows the performance gap between open-source and closed-source model reasoning abilities.</p>
<p>Implementation Details</p>
<p>PRM Pre-Training.We pretrain the PRM model on Llama-2-13B model with LoRA adaptor [15], the rank is 8 and the scaling factor α is 16.The trainable parameters of LoRA adapter are 0.05% of the 13B model parameters.We train the PRM model as a binary-classification task, where the labels are correct and incorrect.For the PRM800K dataset [21] , which includes correct, incorrect and neutral labels, we treat neutral label as incorrect labels.As stated in Lightman et al. [21], considering neutral label either correct or incorrect doesn't significantly affect the overall training performance.We use this design choice for more accurate and conservative feedback for the search purpose.The PRM training results are showed in Appendix Figure 7, where we can see the performance keeps improving when feeding more training data.The details about the base model parameters and computational resources are provided in Appendix A.</p>
<p>PRM Fine-tuning.We utilize the process-reward data from the PRM800k dataset to train a general PRM model for mathematical reasoning.For the GSM8K dataset, we generate process-reward data to fine-tune the pre-trained model.Since we already have the ground truth reasoning answers in the datasets, the positive steps, i.e., the correct and faithful steps, can be recovered.For the negative reasoning steps, we prompt the ground truth reasoning answer to GPT-3.5 and explicitly ask it to perturb the steps so they do not follow each other reasonably.We then collect the generated step-reward data and fine-tune the general PRM for the GSM8K dataset.</p>
<p>Math Reasoning Benchmarks</p>
<p>We present the results of various open-source and closed-source large language models (LLMs) on the GSM8K and MATH benchmarks in CoT-SC@16 refers to self-consistency on 16 Chain of Thought (CoT) candidate results.BS@16 represents the beam search method, involving 16 candidates at each step-level, and LevinTS@16 details the Levin Tree Search method with the same number of candidates.Notably, the most recent result for the GPT-4 on the MATH dataset is reported as GPT-4-turbo-0409, which we highlight as it represents the best performance within the GPT-4 family.</p>
<p>We observe similar results on the GSM8K dataset.M<em> (BS) and M</em> (LevinTS) boosted the performance of the Llama-2-13B model (CoT-SC@16) from 41.8 to 66.3 and 68.8, respectively.Also, for the Mistral model, M<em> (BS) and (LevinTS) led to improvements of around 52.3% and 59.8% over the base CoT-SC@16 score respectively.It is worth mentioning that M</em> (LevinTS) consistently achieves a better performance compared to beam search.Nonetheless irrespective of tree search algorithm or the base model, M* framework substantially narrows down the performance gap between open-source and closed-source models in mathematical reasoning tasks.</p>
<p>Math Fine-tuning VS.M<em>.Furthermore, we observe a performance gain using the M</em> method compared to models fine-tuned on the MATH dataset, but a lower performance on GSM8K.One explanation is that simpler tasks like those in GSM8K benefit more from extensive training data.However, for more complex tasks like those in the MATH dataset, the M<em> method significantly enhances reasoning abilities.To further justify M</em>, we demonstrate the performance degradation of math fine-tuned models in Appendix E.4 and compare fine-tuning versus inference-time search results in Appendix E.2.</p>
<p>M* Scaling Results</p>
<p>Tree Size Scaling Results.In Figure 4a, we demonstrate how the number of step-level candidates influences M<em> performance.The reported results are based on choosing Llama-2 13b as the base LLM  and beam search as the tree search algorithm.We observe a consistent improvement in performance with an increase in the number of candidates, indicating that M</em> method identifies better reasoning trajectories as the search space expands.Additionally, in the MATH dataset, we note that performance tends to converge when the number of candidates increases from 8 to 16.This is because Llama-2-13B struggles to produce diverse step-level responses as the number of sampled candidates increases.</p>
<p>Base Model Scaling Results.We next examine how model size affects overall M<em> performance.As illustrated by the red and purple dots in Figure 4b, we observe that increasing the Llama-2 base model size from 7B to 13B enhances performance across both the GSM8K and MATH benchmarks.This observation supports the scaling laws relating to the base model size and highlights the potential for applying the M</em> framework to larger models.We believe that M* could also improve the performance of closed-source LLMs.Instead of increasing the size and training time of LLMs, we could conserve resources by enhancing performance during inference.</p>
<p>PRM Scaling Results.We explore how M* performance scales with PRM model sizes.We train two PRM models using Llama-2-7B and Llama-2-13B, respectively, ensuring that both models use the same training data and training duration for a fair comparison.The results are displayed in the grey area of Figure 4b.From this figure, we observe that the performance improvement attributed to PRM model size is evident.Notably, the performance differential with Llama-2-13B is more significant than with Llama-2-7B.As the base LLM size increases, the enhanced PRM model leads to more precise differentiation within the search space.Therefore, larger models benefit more from a robust PRM model.This suggests that searching on larger LLMs could be advantageous for maximizing performance.</p>
<p>Forest Search Scaling Results.As shown in Figure 4c, the accuracy consistently improves as the number of search trees increases, with 9 trees achieving accuracy of 39.6% compared to 36.4% for a single tree.These results demonstrate that forest search is an effective extension of the M*, leveraging the diversity of multiple reasoning trees to enhance the quality of the final answer.</p>
<p>Inference Overhead</p>
<p>To assess the inference overhead of the M* algorithm, we analyze the average number of generated tokens compared to baseline methods.As shown in Table 2, the Beam search method incurs about 1.5 times the cost of the CoT-Sc@16 and results in up to 66% performance improvement.In comparison, LevinTS costs roughly twice the compute compared to Beam search and further improves model performance by an additional 1.5 ∼ 3%.While BS generate more tokens than the CoT-SC@16, the inference overhead is not excessive, especially considering the significant performance improvements.Although LevinTS is more expensive than the other two methods, it delivers significantly better performance.We recommend choosing based on needs: use LevinTS for more accurate results, and BS for a cost-effective option with fair performance.</p>
<p>Conclusion</p>
<p>In this paper, we introduce MindStar (M<em>), a novel reasoning framework that largely boosts the reasoning ability of a pre-trained LLM without any fine-tuning.By treating reasoning tasks as search problems and utilizing a process-supervised reward model, M</em> effectively expands and navigates the reasoning tree to identify approximately optimal paths.The incorporation of ideas from Beam Search and Levin Tree Search further enhances search efficiency and accuracy.Through evaluations on both the GSM8K and MATH datasets, we demonstrate that M* significantly improves the reasoning abilities of open-source models, such as LLaMA-2, achieving performance comparable to closed-source models like GPT-3.5 and Grok-1, with a substantially smaller model.</p>
<p>Limitations</p>
<p>The primary limitation of the M<em> method, as discussed in Section 4.6, is the increased inference cost.The M</em> method generates more tokens than the original chain-of-thought self-consistency (CoT-SC) approach, leading to higher expenses during inference.However, as demonstrated in Table 1, M* enhances the mathematical reasoning performance of the smaller Llama-2-13B model, surpassing that of the GPT-3.5 and Grok-1.This improvement reduces overall inference computational costs for larger model sizes.</p>
<p>Furthermore, the use of a PRM model is required to evaluate nodes in the reasoning tree, necessitating additional training and data.Nevertheless, we contend that training the PRM model consumes fewer computational resources than training larger models.Regarding data requirements, as shown in Appendix E.2, the data used for training the PRM model is more efficient than using the same data to fine-tune large language models (LLMs).</p>
<p>A Experimental Settings and Computer Resources</p>
<p>Base Model Hyper-Parameters: To ensure diversity in step-level reasoning sentences, as illustrated in Table 3, we selected a specific set of parameters within the M* framework for both the Llama-2 and Mistral open-source models.Notably, we sample 16 candidates at each reasoning step and establish a maximum tree search depth of five levels.With these settings, the potential tree size reaches 16 5 , approximately 1 million nodes.This extensive range provides the language models with a broad array of generative options and covers a substantial search space, thereby demonstrating the effectiveness of the proposed framework.In mathematical reasoning tasks, we observed that open-source large language models (LLMs) typically complete the reasoning process within five steps.</p>
<p>B Searching Algorithms</p>
<p>In this section, we explain beam search in Algorithm 2 and LevinTS in Algorithm 3.</p>
<p>Algorithm 2: Beam Search Require :Question q, pre-trained PRM function P(), language model G(), branch factor N , an empty reasoning tree T , and the maximum search level L while l &lt; L and question not answered do for n ∈ N do ▷ Sample N answers from LLM e n l = G(n * l ) ▷ Each answer is generated based on questions and previous steps Add a child node n l+1 to the reasoning tree, where the node value is calculated as
c(n l+1 ) = c(n * l ) + P(n * l , n l ) n * l+1 = max(n l+1 ) if n * l+1 solves</p>
<p>C Forest Search</p>
<p>Building on the M<em> framework, we introduce an extension called Forest Search, an ensemble method that combines multiple M</em> search trees to improve the accuracy of results.The forest search algorithm proceeds as follows: 1) the base model (e.g., Mistral-7B) is queried with the original task to generate a paraphrased task variant for each search tree, thereby increasing the diversity of reasoning paths.We show the paraphrase examples in Appendix F; 2) M* tree search (e.g., Beam Search) is performed
V := V ∪ {n n ′ l } F := F ∪ C(n n l ) ▷ C(•)
is the set of children nodes end for each paraphrased task variant to collect step-by-step responses; 3) the PRM model scores the collected responses from each search tree, and the highest-scoring response is selected as the final answer to the task.As shown in Figure 4c, We evaluate the performance of forest search on the MATH dataset, varying the number of search trees.</p>
<p>D LevinTS proof</p>
<p>Proof.Let N g be a set of target nodes, n * be the first expanded node in the set of target nodes in the reasoning tree, and | N (n * )| denotes the number of tokens generated until expansion of node n * .Also, let L denotes the set of leaf nodes (i.e., answers) in the reasoning tree.The first node in N g to be expanded, n * , is the one of lowest cost due to the monotonicity of f i tok and π, with cost c := min n∈N g f (n) π(n) .Thus:
| N (n * )| ≤ n∈L f (n) = n∈L π(n) f (n) π(n) ≤ n∈L π(n)c ≤ c = min n∈N g f (n) π(n) ,
where the first inequality holds because each leaf node takes at most f (n) tokens to generate at the time n * is being expanded by definition, the second inequality holds since any previously expanded node costs less than n * based on LevinTS' node selection criteria, and finally the last inequality holds since n∈L π(n) ≤ 1.</p>
<p>E Extra Experiments E.1 Analysis of Llama family scaling laws</p>
<p>In our investigation of scaling laws within the Llama family of models, notably Llama-2 [40] and Llama-3 [28], we applied the M<em> method to observe its impact on performance improvement relative to model size.As illustrated in Figure 5, the application of M</em> substantially enhances the performance of the Llama-2 model, aligning its scaling trajectory closer to that of the Llama-3 model.This improvement in scaling efficiency through the M* method is significant because it suggests that the reasoning capabilities of LLMs can be enhanced without necessarily increasing the volume of high-quality training data.Instead, the focus shifts toward selecting right responses, thereby conserving resources while still achieving competitive performance metrics.Furthermore, these findings open avenues for future research focused on inference time enhancements.We believe this analysis not only reinforces the performances within the Llama family but also highlights the broader potential for similar advancements across different model families.</p>
<p>E.2 Fine-tuning VS. Inference-time Search</p>
<p>Here we analyze two effective ways of using the PRM800K dataset in better solving math reasoning problems.We compare the performance of using the PRM800K dataset for fine-tuning v.s.training a PRM to guide inference-time search.As illustrated in Figure 6, the supervised fine-tuned (SFT) Llama-2-13B model, which utilizes the PRM800K dataset for fine-tuning, outperforms the vanilla Llama-2-13B model in both CoT and CoT-SC by a notable margin.However, the SFT approach still falls short compared to the PRM-guided search methods, namely Beam search and Levin Tree Search.By employing the PRM800K dataset to train a Process-supervised Reward Model (PRM) and using it to guide the search process, both Beam search (BS@16) and Levin Tree Search (LevinTS@16) significantly surpass the performance of the SFT model.This comparison highlights the superiority of the PRM-guided search methods in leveraging the PRM800K dataset for enhancing math reasoning capabilities.The results suggest that training a PRM to guide the search process is more effective than directly fine-tuning the base model, as it allows for an efficient exploration of the reasoning space and the identification of optimal reasoning paths.4, compared to the CoT and self-consistency baselines, which generate a single reasoning path or a fixed number of candidates that each consists of multiple steps of rationales, the M* algorithm with Beam and LevinTS search methods does not introduce a significant computational overhead.The number of expanded nodes remains relatively small, indicating that the search process is efficient in finding optimal reasoning paths without exploring an excessive number of nodes.</p>
<p>As expected, we note that the average node expansion is more costly in a more challenging MATH dataset compared to GSM8K that mostly consists of less difficult grade school math questions.This observation is consistent among both Beam and LevinTS, which reaffirms that more search steps are required for good reasoning paths for more challenging questions and best-first search methods are a good fit for solving challenging math reasoning problems.</p>
<p>E.4 Base Model Selection Analysis</p>
<p>Model</p>
<p>Size SIQA  In this section, to illustrate why we choose LLama-2 as base model, we evaluate LLama-2-13B [40] and MetaMath-Llama-2-13B [49] to answer the following questions:</p>
<ol>
<li>Does math fine-tuned model affects base model on other non-math datasets?2. Does math fine-tuned model raises more safety concerns than base model?</li>
</ol>
<p>To achieve this goal, we evaluate models on four different datasets: the commonsense questions dataset SIQA [35], the truthfulness dataset TruthfulQA [22], the toxicity dataset ToxiGen [13], and the bias dataset BOLD [7].</p>
<p>As shown in Table 5, Llama-2 fine-tuned for math performs worse on the SIQA commonsense question-answering dataset.This demonstrates that fine-tuning for math can degrade a base model's performance on other tasks.</p>
<p>More importantly, since the fine-tuned model doesn't integrate training signals for safety, it can potentially harm the user despite performing well on the fine-tuned tasks.As shown in   What is the least positive integer multiple of 30 that can be written with only the digits 0 and 2? Paraphrased:</p>
<p>Find the smallest positive multiple of 30 that can be constructed using only the digits 0 and 2.</p>
<p>G Broader Impacts</p>
<p>The research presented in this paper has the potential to positively impact the development and application of large language models (LLMs) in various domains.By enhancing the reasoning capabilities of pre-trained LLMs without the need for fine-tuning, our proposed M* framework can lead to more efficient and accessible deployment of these models in real-world scenarios.</p>
<p>Positive societal impacts may include improved accessibility, resource conservation, and enhanced decision-making.First, the M<em> framework enables smaller, open-source models to achieve reasoning performance comparable to larger, closed-source models.This can democratize access to high-quality reasoning tools, allowing a wider range of researchers and practitioners to benefit from LLMs.Second, by shifting computational resources from fine-tuning to inference-time searching, the M</em> method</p>
<p>Figure 1 :
1
Figure 1: M*: A searching framework for inference time step reasoning.A: Each time we gather questions and previous reasoning steps to the LLMs and sample N next reasoning steps.B:We organize the reasoning process as a tree.Each node represents either question (the root node), answers (leaf nodes), or reasoning steps (all other nodes).A searching method traverses the reasoning tree and select a node to expand.We add the reasoning step of the selected node back to the prompt for next query step.We stop the generation processes until either the answer is find or the maximum consumption is reached.</p>
<p>CoT Results ORM Selection on CoT Results PRM-guided Beam Search on the Reasoning Tree</p>
<p>Figure 2 :
2
Figure 2: Different reward models for LLMs' output selections on MATH dataset.The x-axis denotes the total number of generated outputs</p>
<p>Figure 3 :
3
Figure 3: MATH accuracy of different LLMs.M* on LLaMA-2-13B achieves similar performance as GPT-3.5 (4-shot) while saving approximately 200 times the computational resources.</p>
<p>Figure 4 :
4
Figure 4: We study how M<em> performance scales with different parameters.In 4a, We study how M</em> performance scales with the number of step-level candidates.We choose Llama-2-13B with BS as the base model and search algorithm, respectively.In 4b, we show base LLM model size vs.PRM model size.The red dots represents performance across various base model sizes using PRM-13B, while the purple dots indicates performance with PRM-7B.The grey area shows the performance improvements achieved by increasing the size of the PRM model.In 4c, we present forest search results.</p>
<p>the problem or l equals the maximum search level L then return the whole reasoning path and final answer n *</p>
<p>Algorithm 3 :
3
Levin Tree SearchRequire :A node set V that have been expanded, and a node set F be the set of non-yet-expanded children of expanded nodesV := ∅ F := {n q } while F ̸ = ∅ do n := arg min n∈F f (n n l ) softmax(P)(n n l ) F := F \ {n} e n l+1 = G(n * l ) if n *l+1 solves the problem or l equals the maximum search level L then return the whole reasoning path and final answer n * l+1 end</p>
<p>Figure 5 :
5
Figure 5: Scaling laws for Llama-2 and Llama-3 model families on MATH datasets.The results are all reported from their original resources.We use the Scipy tool and a logarithm function to compute the fitting curve.</p>
<p>Figure 6 :
6
Figure 6: Comparison results of fine-tuning methods and M* on MATH dataset.</p>
<p>Figure 7 :F
7
Figure 7: PRM Evaluation Results.The x-axis shows the percentage of training data.The y-axis shows the label accuracy in test datasets.</p>
<p>Example F. 3 : MATH Example 3 Task 3 :
333
If f (x) = 3x−2 x−2 ,what is the value of f (−2) + f (−1) + f (0)?Express your answer as a common fraction.Paraphrased:Find the value of f (−2) + f (−1) + f (0), where f (x) = 3x−2 x−2 .Express the final answer as a common fraction.</p>
<p>Specifically, PRM P takes the current reasoning node n d and the potential next step e d as the inputs and returns a reward value P(n d , e d ) = r d ∈ [0, 1].Importantly, when evaluating a new step, PRM considers the previous reasoning steps.This encourages the LLM to be consistent and faithful with respect to the entire path.Therefore, a high reward value suggests that e d can be a correct next step for n d , making the trace [n d ⊕ e d ] worth exploring.Conversely, a small reward value can be viewed as an incorrect step, suggesting that solutions following [n d ⊕ e d ] are likely incorrect.</p>
<p>Table 1 .
1
These results demonstrate that M<em> significantly improves the open-source model performance, becoming comparable to that of closed-source models.
Specifically, on the MATH dataset, M</em> (BS) and M<em> (LevinTS) increased the performance of theLlama-2-13B model (CoT-SC@16) from 20.4 to 32.4 and 33.9, respectively. These results are closeto those of GPT-3.5, which scores 34.1, but the model size is only about 7.4% of GPT-3.5 (13B vs
175B).For the Mistral model, the M</em> (BS) and M<em> (LevinTS) methods improved the performance from 23.9 to 36.2 and 38.2 respectively, surpassing Grok-1 and GPT-3.5 performances.Yet, when set against Claude-3, GPT-4 and Gemini, M</em> variants are still outmatched.</p>
<p>Table 1 :
1
Comparison results of various schemes on the GSM8K and MATH reasoning benchmarks are presented.The number for each entry is the problem solve percentage.The notation SC@32 denotes self-consistency across 32 candidate results, while n-shot indicates results from few-shot examples.</p>
<p>Table 2 :
2
Average Tokens Generated per Question
Method#Tokens/Question GSM8K MATHCoT-SC@16 21462668BS@1631534290LevinTS@16 61418850</p>
<p>Table 3 :
3
M<em> Hyper-parameters
Computer Resources: For the PRM training, base-model inference and M</em> algorithm, we use8<em>Nvidia V100 GPUs.NameValueBase LLM Paramstop_p0.95top_k50repetition_penalty1.0max_new_tokens256temperature1.0M</em> Params#candidates16maximum search level5</p>
<p>Table 4 :
4
Average Node Expansions per Question Similarly, as shown in Table
E.3 Extended Computation Complexity AnalysisMethod#Nodes/Question GSM8K MATHBS@163.593.97LevinTS@167.238.22</p>
<p>Table 5 :
5
Comparison results for Llama-2 and MetaMath-Llama-2</p>
<p>Table 5 ,
5
MetaMath degrades Llama-2's safety scores on both TruthfulQA and ToxiGen, raising significant concerns about the use of MetaMath.Additionally, the following examples D.1 to D.6 show that MetaMath exhibits more bias issues than Llama-2.Therefore, we prefer to choose the safer model, Llama-2, as our base model.Jacob Zachar is an American actor whose Response:'is known for his roles in television series such as "One Tree Hill" and "The Vampire Diaries".'
Example E.1: Llama-2 Example 1Instruction:
d ,e 2 d ,...,e N d } P(n d , e i d ),
https://github.com/openai/simple-evals
Example E.2: MetaMath-LLama-2 Example 1Instruction: Jacob Zachar is an American actor whose Response:To solve this problem, we need to determine the value of x, which represents the missing word in the given sentence.The given sentence is: Jacob Zachar is an American actor whose ... We know that the sentence is incomplete, so we need to fill in the missing word(s).Let's analyze the context of the sentence: -The name "Jacob Zachar" refers to an American actor.-The sentence is asking for information about this actor.Based on the context, we can infer that the missing word(s) must be related to the profession or personal Example E. To solve this problem, we need to determine the value of x, which represents the missing word in the given sentence.The sentence is "Adrian Zaw is an American actor best known for his roles in The Walking Dead and Sons of Anarchy".We know that Adrian Zaw is the subject of the sentence, so we don't need to find the value of x for that part.The unknown variable x comes into play when we mention his roles in The Walking Dead and Sons of Anarchy.Since these are the only two shows mentioned, x must be the name of one of To solve this problem, we need to determine the value of x, which represents the missing word in the given sentence.The given sentence is "Alfred Zeisler was an American-born German film director and screenwriter who worked for the majority of his career in Germany".We know that Alfred Zeisler was born in America and worked for most of his career in Germany.Therefore, the missing word must be related to his nationality or profession.Let's analyze the given information: Alfred Zeisler was an American-born German film director and screenwriter.He worked for the majority ofE.5 PRM Training ResultsFrom Figure7we can see the performance keeps improving when feeding more training data.can reduce the environmental impact associated with training large-scale models, promoting more sustainable AI development practices.Last, LLMs with improved reasoning capabilities can assist humans in making better-informed decisions across various domains, such as healthcare, finance, and public policy, by providing accurate and reliable insights derived from complex reasoning tasks.Potential negative impacts could involve over-reliance on AI reasoning and privacy concern.Here we provide a brief analysis of both issues and some remedies.As LLMs become more proficient at reasoning tasks, there is a risk that humans may overly rely on their outputs without sufficient critical thinking.To address this, we suggest that AI reasoning tools be used in conjunction with human oversight and that their limitations and potential biases be clearly communicated to users.In addition, the application of enhanced reasoning LLMs in sensitive domains, such as healthcare or finance, may raise privacy concerns if personal data is used as input.To mitigate this risk, we recommend the implementation of appropriate data privacy protocols and the use of differential privacy techniques when deploying these models in practice.By proactively addressing potential negative impacts and promoting responsible deployment strategies, we believe that the M* framework and similar advancements in LLM reasoning can contribute to the development of more trustworthy and beneficial AI systems.As researchers, it is our responsibility to continue exploring these techniques while actively engaging with the broader community to ensure their positive societal impact.In this paper, we utilize pre-existing resources, including pre-trained language models: Llama-2-13B[40]and Mistral-7B[18], as well as publicly available datasets: PRM800K[21], GSM8K[6], and MATH[14].Additionally, we employ the evaluation toolkit simple-evals[29].H Artifacts UsageAs shown in Table6, all resources are used in accordance with their respective licenses, which permit use for public research, and align with the intended use.The datasets utilized are exclusively mathematics-related and do not contain any personally identifiable information or offensive content.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, The Claude 3 Model Family: Opus, Sonnet, Haiku. 2024268232499</p>
<p>Alphamath almost zero: process supervision without process. Guoxin Chen, Minpeng Liao, Chengxi Li, Kai Fan, 2024</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 2022Scaling instruction-finetuned language models</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021</p>
<p>BOLD: dataset and metrics for measuring biases in open-ended language generation. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, FAccT. ACM2021</p>
<p>Alphazero-like tree-search can guide large language model decoding and training. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, 2024</p>
<p>Specializing smaller language models towards multi-step reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, 2023</p>
<p>Roscoe: A suite of metrics for scoring step-by-step reasoning. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz, 2023</p>
<p>Carlos Gómez, -Rodríguez , Paul Williams, arXiv:2310.08433A confederacy of models: A comprehensive evaluation of llms on creative writing. 2023arXiv preprint</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, 2023</p>
<p>Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar, ACL (1). Association for Computational Linguistics2022</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Lora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, ICLR. OpenReview.net2022</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, arXiv:2210.116102022arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2024</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, ACL (1). Association for Computational Linguistics2022</p>
<p>Large language model guided tree-of-thought. Jieyi Long, 2023</p>
<p>The HARPY speech recognition system. T Bruce, Lowerre, 197661409851Carnegie-Mellon University</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang, arXiv:2306.08568Wizardcoder: Empowering code large language models with evol-instruct. 2023arXiv preprint</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, 2023</p>
<p>Let's reward step by step: Step-level reward model as the navigators for reasoning. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang, 2023</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. A I Meta, April 2024</p>
<p>Simple-evals. April 2024OpenAI</p>
<p>Single-agent policy tree search with guarantees. Laurent Orseau, H S Levi, Tor Lelis, Théophane Lattimore, Weber, 2018</p>
<p>Laurent Orseau, Marcus Hutter, Levi Hs Leli, arXiv:2305.16945Levin tree search with context models. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Openwebmath: An open dataset of high-quality mathematical web text. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba, arXiv:2310.067862023arXiv preprint</p>
<p>Heuristics: intelligent search strategies for computer problem solving. Judea Pearl, 1984Addison-Wesley Longman Publishing Co., Inc</p>
<p>Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Lebras, Choi, arXiv:1904.09728Socialiqa: Commonsense reasoning about social interactions. 2019arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 202033</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu, 2024</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, 2023</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, CoRR, abs/2312.089352023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, 2023</p>
<p>Self-evaluation guided beam search for reasoning. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie, 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, 2022</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
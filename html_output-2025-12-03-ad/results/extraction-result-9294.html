<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9294 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9294</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9294</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-268998578</p>
                <p><strong>Paper Title:</strong> <a href="https://medinform.jmir.org/2024/1/e55318/PDF" target="_blank">An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study</a></p>
                <p><strong>Paper Abstract:</strong> Background: Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective: The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced typesâ€”heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods: This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results: The study revealed that task-specific prompt</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9294.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9294.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic prompts (clinical)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic prompt (rule-based decomposition for clinical IE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rule-based prompt templates that decompose clinical information extraction queries into smaller rule-governed steps, integrating domain heuristics to guide LLM output (introduced and evaluated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical sense disambiguation; Biomedical evidence extraction; Medication attribute extraction; Medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Clinical sense disambiguation: choose correct expansion/meaning of clinical abbreviations in context; Biomedical evidence extraction: extract interventions/evidence from abstracts; Medication attribute extraction: extract and label medication attributes; Medication status extraction: identify whether a medication is currently being taken/relevant to patient.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompting using heuristic (rule-based) natural language prompts that specify explicit rules/criteria for mapping tokens/phrases to labels (no in-prompt examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against other prompt types in zero-shot (simple prefix, simple cloze, chain-of-thought, anticipatory) and few-shot variants; compared to traditional baselines (BERT, ELMO, PubMedBERT-CRF, ScispaCy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clinical sense disambiguation (GPT-3.5, heuristic, zero-shot): accuracy 0.96; Biomedical evidence extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.94; Medication attribute extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.96; Medication status extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.74.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Clinical sense disambiguation: BERT baseline 0.42, ELMO 0.55 (heuristic +0.54 vs BERT); Biomedical evidence extraction: PubMedBERT-CRF 0.35 (heuristic +0.59); Medication attribute extraction: ScispaCy 0.70 (heuristic +0.26); Medication status extraction: ScispaCy 0.52 (heuristic +0.22).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Clinical sense disambiguation: heuristic zero-shot vs GPT-3.5 few-shot 0.96 vs 0.82 = +0.14 (14 percentage points); Biomedical evidence extraction: few-shot (GPT-3.5) 0.96 vs heuristic zero-shot 0.94 = +0.02.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize heuristic prompts capture salient domain rules and constraints (mirroring rule-based clinical NLP) which provide clear, deterministic guidance to LLMs and thus improve disambiguation/extraction in zero-shot settings; heuristics reduce ambiguity and supply structure the model can follow.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts created iteratively by healthcare experts; zero-shot evaluations using GPT-3.5 Turbo (Sept 2023), Gemini, LLaMA-2; accuracy used as sole metric; datasets: CASI subset for clinical abbreviations and medication tasks, EBM-NLP for evidence extraction; prompt texts examples in Multimedia Appendix 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9294.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting (explicit intermediate reasoning steps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that elicit intermediate natural-language reasoning steps before producing the final answer, intended to improve tasks requiring multi-step inference (used and evaluated across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; LLaMA-2; Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Coreference resolution; Biomedical evidence extraction; Medication attribute extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Coreference resolution: identify antecedents referring to the same entity; biomedical evidence extraction and medication attribute extraction as above.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot chain-of-thought prompts that ask the model to enumerate intermediate reasoning steps (no in-prompt examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to simple prefix, simple cloze, anticipatory, heuristic, ensemble, and few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Coreference resolution (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.94; Biomedical evidence extraction (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.94; Medication attribute extraction (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.96.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Coreference baseline (Toshniwal et al) 0.69 (chain-of-thought +0.25); other GPT-3.5 prompt types for coreference: simple prefix 0.78, simple cloze 0.60, anticipatory 0.74.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Coreference resolution: chain-of-thought (0.94) vs simple cloze (0.60) = +0.34 (34 percentage points) for GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Chain-of-thought provides structured intermediate steps that supply logic and constraints necessary for resolution-style tasks; the extra reasoning scaffold helps the model infer links that single-shot instructions might not elicit.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot chain-of-thought prompts employed across models; accuracy resolver maps LLM outputs to label space; chain-of-thought was among top-performing prompts for coreference and medication attribute extraction for GPT-3.5 and LLaMA-2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9294.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple prefix prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple prefix prompt (instruction or response-style prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Short prompts that prepend brief instructions or format hints (e.g., ask model to 'list expansions' or 'classify as') to control response format and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; LLaMA-2; Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical sense disambiguation; Medication status extraction; Coreference resolution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See other entries; prefix prompts used to frame classification/extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prefix prompts that give concise instruction/format expectations (no examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to cloze, chain-of-thought, heuristic, anticipatory, ensemble, and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clinical sense disambiguation (GPT-3.5, prefix): 0.88; Medication status extraction (GPT-3.5, prefix): 0.76; Coreference resolution (LLaMA-2, prefix): 0.80.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prefix often competitive but generally inferior to heuristic or chain-of-thought for tasks requiring stronger rules/reasoning; e.g., clinical sense disambiguation: heuristic 0.96 vs prefix 0.88.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Medication status extraction (GPT-3.5) prefix 0.76 vs ScispaCy baseline 0.52 = +0.24.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prefix prompts give useful high-level guidance about response style and expected output, which helps LLMs produce structured answers for extraction/classification tasks, but may lack the rule specificity or reasoning scaffolding required by some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts created iteratively; simple prefix is the minimal control mechanism tested. Evaluated in zero-shot across three LLMs using accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9294.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple cloze prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple cloze prompt (fill-in-the-blank)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that pose the task as a masked-token/fill-in-the-blank problem asking the model to predict missing words/phrases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (clinical sense disambiguation, biomedical evidence extraction, coreference, medication tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used as an alternative prompt framing where the model predicts a missing token/phrase used to indicate labels or entities.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot cloze-style prompts (masked text with prediction request).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to prefix, chain-of-thought, heuristic, anticipatory, ensemble, and few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varied by task: Clinical sense disambiguation (GPT-3.5): 0.86; Biomedical evidence extraction (GPT-3.5): 0.82; Coreference resolution (Gemini): 0.81 (noted advantage for Gemini on cloze for one task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cloze often underperformed chain-of-thought and heuristic for many tasks but performed relatively well for some model-task combinations (e.g., Gemini coreference/cloze 0.81 vs GPT-3.5 cloze 0.60 on coreference).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Coreference: Gemini cloze 0.81 vs Gemini prefix 0.69 = +0.12 (12 percentage points) for that model/task pairing.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Cloze framing aligns with masked-prediction pretraining objectives and may match better to models whose pretraining or architecture favors fill-in-the-blank style completion, explaining model-specific advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cloze prompts implemented across tasks; results indicate model-by-format interactions are important; datasets and metric same as other evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9294.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anticipatory prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anticipatory prompt (guide next command/question)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that anticipate the next question or command to guide the model's next response; used as a framing approach across tasks and sometimes performed best for specific model-task pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini; GPT-3.5; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication attribute extraction (noted), other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Anticipatory prompts attempt to steer model to likely subsequent query or expected extraction output.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot anticipatory natural-language prompts that hint at expected next steps or follow-up questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared vs prefix, cloze, chain-of-thought, heuristic, ensemble, few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Medication attribute extraction (Gemini, anticipatory): reported as best for Gemini on this task (Gemini anticipatory accuracy 0.88 in table); GPT-3.5 anticipatory values often high but not best for GPT-3.5 on same tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>For medication attribute extraction: Gemini anticipatory 0.88 vs GPT-3.5 heuristic/chain 0.96 (anticipatory lower than best GPT-3.5 types).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Model-specific improvement: anticipatory was Gemini's best for medication attribute extraction (improvement relative to some other Gemini prompt types in that row, e.g., Gemini prefix 0.68 => +0.20).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Anticipatory prompts help models by constraining likely next queries/responses based on experience, potentially aligning with conversational models (Gemini) that are tuned for dialogic anticipation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors note anticipatory prompts performed well for Gemini on medication attribute extraction; results in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9294.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble prompts (majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble prompting (aggregate outputs from multiple prompt types via majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combines outputs from multiple prompt types applied to the same input and selects the mode of the outputs; intended to leverage complementary strengths of different prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All evaluated tasks (clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, medication attribute extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Aggregate multiple prompt-type outputs for each input and pick the most common answer (mode).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot ensemble of five different prompt types; final answer chosen by majority vote among outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to each single prompt type as well as few-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Clinical sense disambiguation (GPT-3.5, ensemble): 0.90; Medication attribute extraction (GPT-3.5, ensemble): 0.90; other tasks: ensemble often between best and worst prompts (not top overall).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Ensemble was not the best-performing strategy for any task but outperformed low-performing prompts; e.g., for clinical sense disambiguation ensemble 0.90 vs best heuristic 0.96 and worst cloze 0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Clinical sense disambiguation: ensemble 0.90 vs best heuristic 0.96 = -0.06 (6 percentage points lower than best single prompt), but ensemble > some single prompts (e.g., > cloze 0.86 by +0.04).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Ensembling leverages prompt diversity to reduce variance and avoid specific prompt idiosyncrasies; however, aggregation can introduce ambiguity and noise, harming tasks requiring highly precise or consistent outputs (e.g., coreference resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ensemble combined outputs from 5 prompt types via majority voting. Authors note ensemble sometimes decreased accuracy for tasks needing precise outputs and was never the single best approach in their implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9294.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs Few-shot prompting (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot vs few-shot prompt presentation formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of zero-shot prompts (no examples) to few-shot prompts (2 example in-context demonstrations) across clinical IE tasks to determine how added exemplars affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All evaluated clinical IE tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot: only instruction/prompt given; Few-shot: same prompt plus two examples/explanations (2-shot) included in context.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Direct comparison; two in-context examples were used for few-shot experiments (2-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot (no examples) vs few-shot (2 examples) across same prompt types and models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Few-shot improved accuracy across most model-prompt-task combinations; example: Biomedical evidence extraction (GPT-3.5) few-shot accuracy 0.96 vs zero-shot heuristic 0.94. Exceptions: clinical sense disambiguation and medication attribute extraction where some zero-shot prompts (heuristic/chain-of-thought) outperformed few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Clinical sense disambiguation (GPT-3.5): heuristic zero-shot 0.96 vs few-shot 0.82 (few-shot worse by -0.14); Biomedical evidence extraction (GPT-3.5): few-shot 0.96 vs zero-shot 0.94 (+0.02). Medication attribute extraction: zero-shot and few-shot both 0.96 in some prompt types (no gain).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Effect sizes are task-dependent: up to +0.02 observed for biomedical evidence extraction (GPT-3.5) and -0.14 observed for clinical sense disambiguation when heuristic zero-shot outperformed few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot provides concrete examples that help in complex scenarios requiring nuanced contextual interpretation; however, when strong task-specific rules are captured by zero-shot heuristic or reasoning prompts, adding few-shot examples can sometimes confuse or dilute the explicit rule guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Two examples/explanations added for each few-shot experiment (2-shot). Authors note consistent improvement with few-shot except for the two tasks named; accuracy was the evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9294.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persona-pattern prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persona pattern prompt (asking model to assume a role)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that instructs LLM to adopt a persona (e.g., 'act as a clinical NLP expert') to bias responses toward domain-appropriate style and content; authors experimented with persona patterns and observed improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical sense disambiguation (example) and other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Asking the model to take on a professional persona to improve focus, relevance, and consistency in responses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot or few-shot prompts prefixed with persona instruction (e.g., 'Act as a clinical NLP expert. Disambiguate ...').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared informally vs same prompts without a persona prefix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report persona patterns can improve accuracy and output quality; no single numeric aggregated metric provided in main tables specifically isolating persona effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Persona prompts focus the model on task-relevant constraints and discourage irrelevant or contradictory content, thereby improving alignment to domain expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Persona examples provided in narrative; specific prompt example given for disambiguating 'cold'; experiments indicate qualitative improvements though no isolated quantitative table was provided for persona-only ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9294.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9294.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Randomness / output variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inherent LLM output randomness (format sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that LLM outputs vary across runs and that output format/consistency depends on prompt specificity; authors highlight that randomness can be beneficial or detrimental depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5; Gemini; LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>All tasks where consistent factual/extraction outputs are required</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Variability in generated outputs across repeated calls can affect reliability of information extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not a prompt type but a characteristic affecting all prompt formats; authors emphasize crafting prompts to be specific and deterministic to reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not quantified with a numeric metric in paper; qualitative statements that randomness introduces noise and could lower accuracy for factual extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Randomness arises from model sampling/stochastic decoding and underspecified prompts; more specific prompts reduce undesirable variability and improve reproducibility for extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors note outputs vary across runs and recommend precise prompt design; no explicit experiment isolating sampling temperature or repeated-run variance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. <em>(Rating: 2)</em></li>
                <li>Prefix-tuning: optimizing continuous prompts for generation. <em>(Rating: 2)</em></li>
                <li>Improving large language models for clinical named entity recognition via prompt engineering. <em>(Rating: 1)</em></li>
                <li>Large language models are few-shot clinical information extractors. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9294",
    "paper_id": "paper-268998578",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Heuristic prompts (clinical)",
            "name_full": "Heuristic prompt (rule-based decomposition for clinical IE)",
            "brief_description": "Rule-based prompt templates that decompose clinical information extraction queries into smaller rule-governed steps, integrating domain heuristics to guide LLM output (introduced and evaluated in this paper).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "Clinical sense disambiguation; Biomedical evidence extraction; Medication attribute extraction; Medication status extraction",
            "task_description": "Clinical sense disambiguation: choose correct expansion/meaning of clinical abbreviations in context; Biomedical evidence extraction: extract interventions/evidence from abstracts; Medication attribute extraction: extract and label medication attributes; Medication status extraction: identify whether a medication is currently being taken/relevant to patient.",
            "presentation_format": "Zero-shot prompting using heuristic (rule-based) natural language prompts that specify explicit rules/criteria for mapping tokens/phrases to labels (no in-prompt examples).",
            "comparison_format": "Compared against other prompt types in zero-shot (simple prefix, simple cloze, chain-of-thought, anticipatory) and few-shot variants; compared to traditional baselines (BERT, ELMO, PubMedBERT-CRF, ScispaCy).",
            "performance": "Clinical sense disambiguation (GPT-3.5, heuristic, zero-shot): accuracy 0.96; Biomedical evidence extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.94; Medication attribute extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.96; Medication status extraction (GPT-3.5, heuristic, zero-shot): accuracy 0.74.",
            "performance_comparison": "Clinical sense disambiguation: BERT baseline 0.42, ELMO 0.55 (heuristic +0.54 vs BERT); Biomedical evidence extraction: PubMedBERT-CRF 0.35 (heuristic +0.59); Medication attribute extraction: ScispaCy 0.70 (heuristic +0.26); Medication status extraction: ScispaCy 0.52 (heuristic +0.22).",
            "format_effect_size": "Clinical sense disambiguation: heuristic zero-shot vs GPT-3.5 few-shot 0.96 vs 0.82 = +0.14 (14 percentage points); Biomedical evidence extraction: few-shot (GPT-3.5) 0.96 vs heuristic zero-shot 0.94 = +0.02.",
            "explanation_or_hypothesis": "Authors hypothesize heuristic prompts capture salient domain rules and constraints (mirroring rule-based clinical NLP) which provide clear, deterministic guidance to LLMs and thus improve disambiguation/extraction in zero-shot settings; heuristics reduce ambiguity and supply structure the model can follow.",
            "null_or_negative_result": false,
            "experimental_details": "Prompts created iteratively by healthcare experts; zero-shot evaluations using GPT-3.5 Turbo (Sept 2023), Gemini, LLaMA-2; accuracy used as sole metric; datasets: CASI subset for clinical abbreviations and medication tasks, EBM-NLP for evidence extraction; prompt texts examples in Multimedia Appendix 1.",
            "uuid": "e9294.0",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Chain-of-thought prompts",
            "name_full": "Chain-of-thought prompting (explicit intermediate reasoning steps)",
            "brief_description": "Prompts that elicit intermediate natural-language reasoning steps before producing the final answer, intended to improve tasks requiring multi-step inference (used and evaluated across tasks).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; LLaMA-2; Gemini",
            "model_size": null,
            "task_name": "Coreference resolution; Biomedical evidence extraction; Medication attribute extraction",
            "task_description": "Coreference resolution: identify antecedents referring to the same entity; biomedical evidence extraction and medication attribute extraction as above.",
            "presentation_format": "Zero-shot chain-of-thought prompts that ask the model to enumerate intermediate reasoning steps (no in-prompt examples).",
            "comparison_format": "Compared to simple prefix, simple cloze, anticipatory, heuristic, ensemble, and few-shot prompts.",
            "performance": "Coreference resolution (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.94; Biomedical evidence extraction (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.94; Medication attribute extraction (GPT-3.5, chain-of-thought, zero-shot): accuracy 0.96.",
            "performance_comparison": "Coreference baseline (Toshniwal et al) 0.69 (chain-of-thought +0.25); other GPT-3.5 prompt types for coreference: simple prefix 0.78, simple cloze 0.60, anticipatory 0.74.",
            "format_effect_size": "Coreference resolution: chain-of-thought (0.94) vs simple cloze (0.60) = +0.34 (34 percentage points) for GPT-3.5.",
            "explanation_or_hypothesis": "Chain-of-thought provides structured intermediate steps that supply logic and constraints necessary for resolution-style tasks; the extra reasoning scaffold helps the model infer links that single-shot instructions might not elicit.",
            "null_or_negative_result": false,
            "experimental_details": "Zero-shot chain-of-thought prompts employed across models; accuracy resolver maps LLM outputs to label space; chain-of-thought was among top-performing prompts for coreference and medication attribute extraction for GPT-3.5 and LLaMA-2.",
            "uuid": "e9294.1",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Simple prefix prompts",
            "name_full": "Simple prefix prompt (instruction or response-style prefix)",
            "brief_description": "Short prompts that prepend brief instructions or format hints (e.g., ask model to 'list expansions' or 'classify as') to control response format and relevance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; LLaMA-2; Gemini",
            "model_size": null,
            "task_name": "Clinical sense disambiguation; Medication status extraction; Coreference resolution",
            "task_description": "See other entries; prefix prompts used to frame classification/extraction tasks.",
            "presentation_format": "Zero-shot prefix prompts that give concise instruction/format expectations (no examples).",
            "comparison_format": "Compared to cloze, chain-of-thought, heuristic, anticipatory, ensemble, and few-shot.",
            "performance": "Clinical sense disambiguation (GPT-3.5, prefix): 0.88; Medication status extraction (GPT-3.5, prefix): 0.76; Coreference resolution (LLaMA-2, prefix): 0.80.",
            "performance_comparison": "Prefix often competitive but generally inferior to heuristic or chain-of-thought for tasks requiring stronger rules/reasoning; e.g., clinical sense disambiguation: heuristic 0.96 vs prefix 0.88.",
            "format_effect_size": "Medication status extraction (GPT-3.5) prefix 0.76 vs ScispaCy baseline 0.52 = +0.24.",
            "explanation_or_hypothesis": "Prefix prompts give useful high-level guidance about response style and expected output, which helps LLMs produce structured answers for extraction/classification tasks, but may lack the rule specificity or reasoning scaffolding required by some tasks.",
            "null_or_negative_result": false,
            "experimental_details": "Prompts created iteratively; simple prefix is the minimal control mechanism tested. Evaluated in zero-shot across three LLMs using accuracy.",
            "uuid": "e9294.2",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Simple cloze prompts",
            "name_full": "Simple cloze prompt (fill-in-the-blank)",
            "brief_description": "Prompts that pose the task as a masked-token/fill-in-the-blank problem asking the model to predict missing words/phrases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "Various (clinical sense disambiguation, biomedical evidence extraction, coreference, medication tasks)",
            "task_description": "Used as an alternative prompt framing where the model predicts a missing token/phrase used to indicate labels or entities.",
            "presentation_format": "Zero-shot cloze-style prompts (masked text with prediction request).",
            "comparison_format": "Compared to prefix, chain-of-thought, heuristic, anticipatory, ensemble, and few-shot.",
            "performance": "Varied by task: Clinical sense disambiguation (GPT-3.5): 0.86; Biomedical evidence extraction (GPT-3.5): 0.82; Coreference resolution (Gemini): 0.81 (noted advantage for Gemini on cloze for one task).",
            "performance_comparison": "Cloze often underperformed chain-of-thought and heuristic for many tasks but performed relatively well for some model-task combinations (e.g., Gemini coreference/cloze 0.81 vs GPT-3.5 cloze 0.60 on coreference).",
            "format_effect_size": "Coreference: Gemini cloze 0.81 vs Gemini prefix 0.69 = +0.12 (12 percentage points) for that model/task pairing.",
            "explanation_or_hypothesis": "Cloze framing aligns with masked-prediction pretraining objectives and may match better to models whose pretraining or architecture favors fill-in-the-blank style completion, explaining model-specific advantages.",
            "null_or_negative_result": false,
            "experimental_details": "Cloze prompts implemented across tasks; results indicate model-by-format interactions are important; datasets and metric same as other evaluations.",
            "uuid": "e9294.3",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Anticipatory prompts",
            "name_full": "Anticipatory prompt (guide next command/question)",
            "brief_description": "Prompts that anticipate the next question or command to guide the model's next response; used as a framing approach across tasks and sometimes performed best for specific model-task pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini; GPT-3.5; LLaMA-2",
            "model_size": null,
            "task_name": "Medication attribute extraction (noted), other tasks",
            "task_description": "Anticipatory prompts attempt to steer model to likely subsequent query or expected extraction output.",
            "presentation_format": "Zero-shot anticipatory natural-language prompts that hint at expected next steps or follow-up questions.",
            "comparison_format": "Compared vs prefix, cloze, chain-of-thought, heuristic, ensemble, few-shot.",
            "performance": "Medication attribute extraction (Gemini, anticipatory): reported as best for Gemini on this task (Gemini anticipatory accuracy 0.88 in table); GPT-3.5 anticipatory values often high but not best for GPT-3.5 on same tasks.",
            "performance_comparison": "For medication attribute extraction: Gemini anticipatory 0.88 vs GPT-3.5 heuristic/chain 0.96 (anticipatory lower than best GPT-3.5 types).",
            "format_effect_size": "Model-specific improvement: anticipatory was Gemini's best for medication attribute extraction (improvement relative to some other Gemini prompt types in that row, e.g., Gemini prefix 0.68 =&gt; +0.20).",
            "explanation_or_hypothesis": "Anticipatory prompts help models by constraining likely next queries/responses based on experience, potentially aligning with conversational models (Gemini) that are tuned for dialogic anticipation.",
            "null_or_negative_result": false,
            "experimental_details": "Authors note anticipatory prompts performed well for Gemini on medication attribute extraction; results in Table 3.",
            "uuid": "e9294.4",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Ensemble prompts (majority vote)",
            "name_full": "Ensemble prompting (aggregate outputs from multiple prompt types via majority voting)",
            "brief_description": "Combines outputs from multiple prompt types applied to the same input and selects the mode of the outputs; intended to leverage complementary strengths of different prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "All evaluated tasks (clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, medication attribute extraction)",
            "task_description": "Aggregate multiple prompt-type outputs for each input and pick the most common answer (mode).",
            "presentation_format": "Zero-shot ensemble of five different prompt types; final answer chosen by majority vote among outputs.",
            "comparison_format": "Compared to each single prompt type as well as few-shot variants.",
            "performance": "Clinical sense disambiguation (GPT-3.5, ensemble): 0.90; Medication attribute extraction (GPT-3.5, ensemble): 0.90; other tasks: ensemble often between best and worst prompts (not top overall).",
            "performance_comparison": "Ensemble was not the best-performing strategy for any task but outperformed low-performing prompts; e.g., for clinical sense disambiguation ensemble 0.90 vs best heuristic 0.96 and worst cloze 0.86.",
            "format_effect_size": "Clinical sense disambiguation: ensemble 0.90 vs best heuristic 0.96 = -0.06 (6 percentage points lower than best single prompt), but ensemble &gt; some single prompts (e.g., &gt; cloze 0.86 by +0.04).",
            "explanation_or_hypothesis": "Ensembling leverages prompt diversity to reduce variance and avoid specific prompt idiosyncrasies; however, aggregation can introduce ambiguity and noise, harming tasks requiring highly precise or consistent outputs (e.g., coreference resolution).",
            "null_or_negative_result": true,
            "experimental_details": "Ensemble combined outputs from 5 prompt types via majority voting. Authors note ensemble sometimes decreased accuracy for tasks needing precise outputs and was never the single best approach in their implementation.",
            "uuid": "e9294.5",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zero-shot vs Few-shot prompting (general)",
            "name_full": "Zero-shot vs few-shot prompt presentation formats",
            "brief_description": "Comparison of zero-shot prompts (no examples) to few-shot prompts (2 example in-context demonstrations) across clinical IE tasks to determine how added exemplars affect performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "All evaluated clinical IE tasks",
            "task_description": "Zero-shot: only instruction/prompt given; Few-shot: same prompt plus two examples/explanations (2-shot) included in context.",
            "presentation_format": "Direct comparison; two in-context examples were used for few-shot experiments (2-shot).",
            "comparison_format": "Zero-shot (no examples) vs few-shot (2 examples) across same prompt types and models.",
            "performance": "Few-shot improved accuracy across most model-prompt-task combinations; example: Biomedical evidence extraction (GPT-3.5) few-shot accuracy 0.96 vs zero-shot heuristic 0.94. Exceptions: clinical sense disambiguation and medication attribute extraction where some zero-shot prompts (heuristic/chain-of-thought) outperformed few-shot.",
            "performance_comparison": "Clinical sense disambiguation (GPT-3.5): heuristic zero-shot 0.96 vs few-shot 0.82 (few-shot worse by -0.14); Biomedical evidence extraction (GPT-3.5): few-shot 0.96 vs zero-shot 0.94 (+0.02). Medication attribute extraction: zero-shot and few-shot both 0.96 in some prompt types (no gain).",
            "format_effect_size": "Effect sizes are task-dependent: up to +0.02 observed for biomedical evidence extraction (GPT-3.5) and -0.14 observed for clinical sense disambiguation when heuristic zero-shot outperformed few-shot.",
            "explanation_or_hypothesis": "Few-shot provides concrete examples that help in complex scenarios requiring nuanced contextual interpretation; however, when strong task-specific rules are captured by zero-shot heuristic or reasoning prompts, adding few-shot examples can sometimes confuse or dilute the explicit rule guidance.",
            "null_or_negative_result": null,
            "experimental_details": "Two examples/explanations added for each few-shot experiment (2-shot). Authors note consistent improvement with few-shot except for the two tasks named; accuracy was the evaluation metric.",
            "uuid": "e9294.6",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Persona-pattern prompting",
            "name_full": "Persona pattern prompt (asking model to assume a role)",
            "brief_description": "Prompting technique that instructs LLM to adopt a persona (e.g., 'act as a clinical NLP expert') to bias responses toward domain-appropriate style and content; authors experimented with persona patterns and observed improvements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "Clinical sense disambiguation (example) and other tasks",
            "task_description": "Asking the model to take on a professional persona to improve focus, relevance, and consistency in responses.",
            "presentation_format": "Zero-shot or few-shot prompts prefixed with persona instruction (e.g., 'Act as a clinical NLP expert. Disambiguate ...').",
            "comparison_format": "Compared informally vs same prompts without a persona prefix.",
            "performance": "Authors report persona patterns can improve accuracy and output quality; no single numeric aggregated metric provided in main tables specifically isolating persona effect.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Persona prompts focus the model on task-relevant constraints and discourage irrelevant or contradictory content, thereby improving alignment to domain expectations.",
            "null_or_negative_result": null,
            "experimental_details": "Persona examples provided in narrative; specific prompt example given for disambiguating 'cold'; experiments indicate qualitative improvements though no isolated quantitative table was provided for persona-only ablation.",
            "uuid": "e9294.7",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Randomness / output variability",
            "name_full": "Inherent LLM output randomness (format sensitivity)",
            "brief_description": "Observation that LLM outputs vary across runs and that output format/consistency depends on prompt specificity; authors highlight that randomness can be beneficial or detrimental depending on task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5; Gemini; LLaMA-2",
            "model_size": null,
            "task_name": "All tasks where consistent factual/extraction outputs are required",
            "task_description": "Variability in generated outputs across repeated calls can affect reliability of information extraction tasks.",
            "presentation_format": "Not a prompt type but a characteristic affecting all prompt formats; authors emphasize crafting prompts to be specific and deterministic to reduce variability.",
            "comparison_format": null,
            "performance": "Not quantified with a numeric metric in paper; qualitative statements that randomness introduces noise and could lower accuracy for factual extraction tasks.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Randomness arises from model sampling/stochastic decoding and underspecified prompts; more specific prompts reduce undesirable variability and improve reproducibility for extraction tasks.",
            "null_or_negative_result": null,
            "experimental_details": "Authors note outputs vary across runs and recommend precise prompt design; no explicit experiment isolating sampling temperature or repeated-run variance reported.",
            "uuid": "e9294.8",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing.",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Prefix-tuning: optimizing continuous prompts for generation.",
            "rating": 2,
            "sanitized_title": "prefixtuning_optimizing_continuous_prompts_for_generation"
        },
        {
            "paper_title": "Improving large language models for clinical named entity recognition via prompt engineering.",
            "rating": 1,
            "sanitized_title": "improving_large_language_models_for_clinical_named_entity_recognition_via_prompt_engineering"
        },
        {
            "paper_title": "Large language models are few-shot clinical information extractors.",
            "rating": 1,
            "sanitized_title": "large_language_models_are_fewshot_clinical_information_extractors"
        }
    ],
    "cost": 0.017187249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study</p>
<p>Sonish Sivarajkumar 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>BSMark Kelley 
Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>MSAlyssa Samolyk-Mazzanti 
Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>MSShyam Visweswaran 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Biomedical Informatics
University of Pittsburgh
PittsburghPAUnited States</p>
<p>PhDYanshan Wang yanshan.wang@pitt.edu 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Biomedical Informatics
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Xsl â€¢ Fo 
Â©sonish Sivarajkumar </p>
<p>Department of Health Information Management
University of Pittsburgh
6026, 15260Forbes Tower Pittsburgh, United StatesPA</p>
<p>An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study
0D7E346C6A761D182AB104ADE8D5BA9D10.2196/55318large language modelLLMLLMsnatural language processingNLPin-context learningprompt engineeringevaluationzero-shotfew shotpromptingGPTlanguage modellanguagemodelsmachine learningclinical dataclinical informationextractionBARDGeminiLLaMA-2heuristicpromptpromptsensemble Coreference resolution Coreference resolution
Background: Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain.However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data.This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches.Objective:The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types-heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models.Methods: This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction.The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta).The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.Results:The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP.In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction.Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks.Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths.GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types.Conclusions:This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain.These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements.To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.</p>
<p>Introduction</p>
<p>Clinical information extraction (IE) is the task of identifying and extracting relevant information from clinical narratives, such as clinical notes, radiology reports, or pathology reports.Clinical IE has many applications in health care, such as improving diagnosis, treatment, and decision-making; facilitating clinical research; and enhancing patient care [1,2].However, clinical IE faces several challenges, such as the scarcity and heterogeneity of annotated data, the complexity and variability of clinical language, and the need for domain knowledge and expertise.</p>
<p>Zero-shot IE is a promising paradigm that aims to overcome these challenges by leveraging large pretrained language models (LMs) that can perform IE tasks without any task-specific training data [3].In-context learning is a framework for zero-shot and few-shot learning, where a large pretrained LM takes a context and directly decodes the output without any retraining or fine-tuning [4].In-context learning relies on prompt engineering, which is the process of crafting informative and contextually relevant instructions or queries as inputs to LMs to guide their output for specific tasks [5].The use of prompt engineering lies in its ability to leverage the powerful capabilities of large LMs (LLMs), such as GPT-3.5 (OpenAI) [6], Gemini (Google) [7], LLaMA-2 (Meta) [8], even in scenarios where limited or no task-specific training data are available.In clinical natural language processing (NLP), where labeled data sets tend to be scarce, expensive, and time-consuming to create, splintered across institutions, and constrained by data use agreements, prompt engineering becomes even more crucial to unlock the potential of state-of-the-art LLMs for clinical NLP tasks.</p>
<p>While prompt engineering has been widely explored for general NLP tasks, its application and impact in clinical NLP remain relatively unexplored.Most of the existing literature on prompt engineering in the health care domain focuses on biomedical NLP tasks rather than clinical NLP tasks that involve processing real-world clinical notes.For instance, Chen et al [9] used a fixed template as the prompt to measure the performance of LLMs on biomedical NLP tasks but did not investigate different kinds of prompting methods.Wang et al [10] gave a comprehensive survey of prompt engineering for health care NLP applications such as question-answering systems, text summarization, and machine translation.However, they did not compare and evaluate different types of prompts for specific clinical NLP tasks and how the performance varies across different LLMs.There is a lack of systematic and comprehensive studies on how to engineer prompts for clinical NLP tasks, and the existing literature predominantly focuses on general NLP problems.This creates a notable gap in the research, warranting a dedicated investigation into the design and development of effective prompts specifically for clinical NLP.Currently, researchers in the field lack a comprehensive understanding of the types of prompts that exist, their relative effectiveness, and the challenges associated with their implementation in clinical settings.</p>
<p>The main research question and objectives of this study are to investigate how to engineer prompts for clinical NLP tasks, identify best practices, and address the challenges in this emerging field.By doing so, we aim to propose a guideline for future prompt-based clinical NLP studies.In this work, we present a comprehensive empirical evaluation study on prompt engineering for 5 diverse clinical NLP tasks, namely, clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction [11,12].By systematically evaluating different types of prompts proposed in recent literature, including prefix [13], cloze [14], chain of thought [15], and anticipatory prompts [16], we gain insights into their performance and suitability for each task.Two new types of prompting approaches were also introduced: (1) heuristic prompts and (2) ensemble prompts.The rationale behind these novel prompts is to leverage the existing knowledge and expertise in rule-based NLP, which has been prominent and has shown significant results in the clinical domain [17].We hypothesize that heuristic prompts, which are based on rules derived from domain knowledge and linguistic patterns, can capture the salient features and constraints of the clinical IE tasks.We also conjecture that ensemble prompts, which are composed of multiple types of prompts, can benefit from the complementary strengths and mitigate the weaknesses of each individual prompt.</p>
<p>One of the key aspects of prompt engineering is the number of examples or shots that are provided to the model along with the prompt.Few-shot prompting is a technique that provides the model with a few examples of input-output pairs, while zero-shot prompting does not provide any examples [3,18].By contrasting these strategies, we aim to shed light on the most efficient and effective ways to leverage prompt engineering in clinical NLP.Finally, we propose a prompt engineering framework to build and deploy zero-shot NLP models for the clinical domain.This study covers 3 state-of-the-art LMs, including GPT-3.5, Gemini, and LLaMA-2, to assess the generalizability of the findings across various models.This work yields novel insights and guidelines for prompt engineering specifically for clinical NLP tasks.</p>
<p>Methods</p>
<p>Tasks</p>
<p>We selected 5 distinct clinical NLP tasks representing diverse categories of natural language understanding: clinical sense disambiguation (text classification) [19], biomedical evidence extraction (named entity recognition) [20], coreference resolution [21], medication status extraction (named entity recognition+classification) [22], and medication attribute extraction (named entity recognition+relation extraction) [23].Table 1 provides a succinct overview of each task, an example scenario, and the corresponding prompt type used for each task.</p>
<p>Text extraction Biomedical evidence extraction</p>
<p>Identify the antecedent for the patient in the clinical note.</p>
<p>The goal here is to identify all mentions in clinical text that refer to the same entity.</p>
<p>Data Sets and Evaluation</p>
<p>The prompts were evaluated on 3 LLMs, GPT-3.5, Gemini, and LLaMA-2, under both zero-shot and few-shot prompting conditions, using precise experimental settings and parameters.To simplify the evaluation process and facilitate clear comparisons, we adopted accuracy as the sole evaluation metric for all tasks.Accuracy is defined as the proportion of correct outputs generated by the LLM for each task, using a resolver that maps the output to the label space.Table 2 shows the data sets and sample size for each clinical NLP task.The data sets are as follows:</p>
<p>â€¢ Clinical abbreviation sense inventories: This is a data set of clinical abbreviations, senses, and instances [28].It contains 41 acronyms from 18,164 notes, along with their expanded forms and contexts.We used a randomly sampled subset from this data set for clinical sense disambiguation, coreference resolution, medication status extraction, and medication attribute extraction tasks (Table 2).</p>
<p>â€¢ Evidence-based medicine-NLP: This is a data set of evidence-based medicine annotations for NLP [29].It contains 187 abstracts and 20 annotated abstracts, with interventions extracted from the text.We used this data set for the biomedical evidence extraction task.</p>
<p>XSL â€¢ FO</p>
<p>RenderX</p>
<p>All experiments were carried out in different system settings.All GPT-3.5 experiments were conducted using the GPT-3.5 Turbo application programming interface as of the September 2023 update.The LLaMA-2 model was directly accessed for our experiments.Gemini was accessed using the Gemini application (previously BARD)-Google's generative artificial intelligence conversational system.These varied system settings and access methods were taken into account to ensure the reliability and validity of our experimental results, given the differing architectures and capabilities of each LLM.</p>
<p>In evaluating the prompt-based approaches on GPT-3.5, Gemini, and LLaMA-2, we have also incorporated traditional NLP baselines to provide a comprehensive understanding of the LLMs' performance in a broader context.These baselines include well-established models such as Bidirectional Encoder Representations From Transformers (BERT) [30], Embeddings From Language Models (ELMO) [31], and PubMedBERT-Conditional Random Field (PubMedBERT-CRF) [32], which have previously set the standard in clinical NLP tasks.By comparing the outputs of LLMs against these baselines, we aim to offer a clear perspective on the advancements LLMs represent in the field.This comparative analysis is crucial for appreciating the extent to which prompt engineering techniques can leverage the inherent capabilities of LLMs, marking a significant evolution from traditional approaches to more dynamic and contextually aware methodologies in clinical NLP.</p>
<p>Prompt Creation Process</p>
<p>A rigorous process was followed to create suitable prompts for each task.These prompts were carefully crafted to match the specific context and objectives of each task.There is no established method for prompt design and selection as of now.Therefore, we adopted an iterative approach where prompts, which are created by health care experts, go through a verification and improvement process in an iterative cycle, which involved design, experimentation, and evaluation, as depicted in Figure 1. Figure 1 illustrates the 3 main steps of our prompt creation process: sampling, prompt designing, and deployment.In the sampling step (step 1), we defined the clinical NLP task (eg, named entity recognition, relation extraction, and text classification) and collected a sample of data and annotations as an evaluation for the task.In the prompt designing step (step 2), a prompt was designed for the task using one of the prompt types (eg, simple prefix prompt, simple cloze prompt, heuristic prompt, chain of thought prompt, question prompt, and anticipatory prompt).We also optionally performed few-shot prompting by providing some examples along with the prompt.The LLMs and the evaluation metrics for the experiment setup were then configured.We ran experiments with various prompt types and LLMs and evaluated their performance on the task.Based on the results, we refined or modified the prompt design until we achieved satisfactory performance or reached a limit.In the deployment step (step 3), the best prompt-based models were selected based on their performance metrics, and the model was deployed for the corresponding task.</p>
<p>Prompt Engineering Techniques</p>
<p>Overview</p>
<p>Prompt engineering is the process of designing and creating prompts that elicit desired responses from LLMs.Prompts can be categorized into different types based on their structure, function, and complexity.</p>
<p>Each prompt consists of a natural language query that is designed to elicit a specific response from the pretrained LLM.The prompts are categorized into 7 types, as illustrated in Figure 2 (all prompts have been included in Multimedia Appendix 1).Prefix prompts are the simplest type of prompts, which prepend a word or phrase indicating the type or format or tone of response for control and relevance.Cloze prompts are based on the idea of fill in the blank exercises, which create a masked token in the input text and ask the LLM to predict the missing word or phrase [3].Anticipatory prompts are the prompts anticipating the next question or command based on experience or knowledge, guiding the conversation.Chain of thought prompting involves a series of intermediate natural language reasoning steps that lead to the final output [15].</p>
<p>In addition to the existing types of prompts, 2 new novel prompts were also designed: heuristic prompts and ensemble prompts, which will be discussed in the following sections.</p>
<p>Heuristic Prompts</p>
<p>Heuristic prompts are rule-based prompts that decompose complex queries into smaller, more manageable components for comprehensive answers.Adopting the principles of traditional rule-based NLP, which relies on manually crafted, rule-based algorithms for specific clinical NLP applications, we have integrated these concepts into our heuristic prompts approach.These prompts use a set of predefined rules to guide the LLM in expanding abbreviations within a given context.For instance, a heuristic prompt might use the rule that an abbreviation is typically capitalized, followed by a period, and preceded by an article or a noun.This approach contrasts with chain of thought prompts, which focus on elucidating the reasoning or logic behind an output.Instead, heuristic prompts leverage a series of predefined rules to direct the LLM in executing a specific task.</p>
<p>Mathematically, we can express a heuristic prompt as H(x), a function applied to an input sequence x.This function is defined as a series of rule-based transformations T i , where i indicates the specific rule applied.The output of this function, denoted as y H , is then:
y H =H(x)=T n (T {n-1} (... T 1 (x)))
Here, each transformation T i applies a specific heuristic rule to modify the input sequence, making it more suitable for processing by LLMs.</p>
<p>From an algorithmic standpoint, heuristic prompts are implemented by defining a set of rules R={R 1 , R 2 , ..., R m }.Each rule R j is a function that applies a specific heuristic criterion to an input token or sequence of tokens.Algorithmically, the heuristic prompting process can be summarized as follows:</p>
<p>By merging the precision and specificity of traditional rule-based NLP methods with the advanced capabilities of LLMs, the heuristic prompts offer a robust and accurate system for clinical information processing and analysis.</p>
<p>Ensemble Prompts</p>
<p>Ensemble prompts are prompts that combine multiple prompts using majority voting for aggregated outputs.They use various types of prompts to generate multiple responses to the same XSL â€¢ FO RenderX input, subsequently selecting the most commonly occurring output as the final answer.For instance, an ensemble prompt might use 3 different prefix prompts, or a combination of other prompt types, to produce 3 potential expansions for an abbreviation.The most frequently appearing expansion is then chosen.For the sake of simplicity, we amalgamated the outputs from all 5 different prompt types using a majority voting approach.</p>
<p>Mathematically, consider a set of m different prompting methods P 1 , P 2 , ..., P m applied to the same input x.Each method generates an output y i for i=1,2, ..., m.The ensemble prompt's output y E is then the mode of these outputs:
y E =mode (y 1 , y 2 , ..., y m )
Algorithmically, the ensemble prompting process is as follows:</p>
<p>The rationale behind an ensemble prompt is that by integrating multiple types of prompts, we can use the strengths and counterbalance the weaknesses of each individual prompt, offering a robust and potentially more accurate response.Some prompts may be more effective for specific tasks or models, while others might be more resilient to noise or ambiguity.Majority voting allows us to choose the most likely correct or coherent output from the variety generated by different prompt types.</p>
<p>Results</p>
<p>Overview</p>
<p>In this section, we present the results of our experiments on prompt engineering for zero-shot clinical IE.Various prompt types were evaluated across 5 clinical NLP tasks, aiming to understand how different prompts influence the accuracy of different LLMs.Zero-shot and few-shot prompting strategies were also compared, exploring how the addition of context affects the model performance.Furthermore, we tested an ensemble approach that combines the outputs of different prompt types using majority voting.Finally, the impact of different LLMs on task performance was analyzed, and some interesting patterns were observed.Table 3 illustrates that different prompt types have different levels of effectiveness for different tasks and LLMs.We can also observe some general trends across the tasks and models.</p>
<p>Prompt Optimization and Evaluation</p>
<p>For clinical sense disambiguation, the heuristic and prefix prompts consistently achieved the highest performance across all LLMs, significantly outperforming baselines such as BERT [30] and ELMO, with GPT-3.5 achieving an accuracy of 0.96, showcasing its advanced understanding of clinical context using appropriate prompting strategies.For biomedical evidence extraction, the heuristic and chain of thought prompts excelled across all LLMs in zero-shot setting.This indicates that these prompt types were able to provide enough information and constraints for the model to extract the evidence from the clinical note.GPT-3.5 achieved an accuracy of 0.94 with these prompt types, which was higher than any other model or prompt type combination.For coreference resolution, the chain of thought prompt type performed best among all prompt types with 2 LLMs-GPT-3.5 and LLaMA-2.This indicates that this prompt type was able to provide enough structure and logic for the model to resolve the coreference in the clinical note.GPT-3.5 displayed high accuracy with this prompt type, achieving an accuracy of 0.94.For medication status extraction, simple prefix and heuristic prompts yielded good results across all LLMs.These prompt types were able to provide enough introduction or rules for the model to extract the status of the medication in relation to the patient or condition.GPT-3.5 excelled with these prompt types, achieving an accuracy of 0.76 and 0.74, respectively.For medication attribute extraction, we found that the chain of thought and heuristic prompts were effective across all LLMs.These prompt types were able to provide enough reasoning or rules for the model to extract and label the attributes of medications from clinical notes.Anticipatory prompts, however, had the best accuracy for Gemini among all the prompts.GPT-3.5 achieved an accuracy of 0.96 with these prompt types, which was higher than any other model or prompt type combination.Thus, we can see that task-specific prompt tailoring is crucial for achieving high accuracy.Different tasks require different levels of information and constraints to guide the LLM to produce the desired output.The experiments show that heuristic, prefix, and chain of thought prompts are generally very effective for guiding the LLM to produce clear and unambiguous outputs.As shown in Figure 3, it is clear that GPT-3.5 is a superior and versatile LLM that can handle various clinical NLP tasks in zero-shot settings, outperforming other models in most cases.Overall, the prompt-based approach has demonstrated remarkable superiority over traditional baseline models across all the 5 tasks.For clinical sense disambiguation, GPT-3.5'sheuristic prompts achieved a remarkable accuracy of 0.96, showcasing a notable improvement over baselines such as BERT (0.42) and ELMO (0.55).In biomedical evidence extraction, GPT-3.5 again set a high standard with an accuracy of 0.94 using heuristic prompts, far surpassing the baseline performance of PubMedBERT-CRF at 0.35.Coreference resolution saw GPT-3.5 reaching an accuracy of 0.94 with chain of thought prompts, eclipsing the performance of existing methods such as Toshniwal et al [34] (0.69).In medication status extraction, GPT-3.5 outperformed the baseline ScispaCy (0.52) with an accuracy of 0.76 using simple prefix prompts.Finally, for medication attribute extraction, GPT-3.5'sheuristic prompts achieved an impressive accuracy of 0.96, significantly higher than the ScispaCy baseline (0.70).These figures not only showcase the potential of LLMs in clinical settings but also set a foundation for future research to build upon, exploring even more sophisticated prompt engineering strategies and their implications for health care informatics.</p>
<p>Zero-Shot Versus Few-Shot Prompting</p>
<p>The performance of zero-shot prompting and few-shot prompting strategies was compared for each clinical NLP task.The same prompt types and LLMs were used as in the previous experiments, but some context was added to the input in the form of examples or explanations.Two examples or explanations were used for each task (2-shot) depending on the complexity and variability of the task.Table 3 shows that few-shot prompting consistently improved the accuracy of all combinations for all tasks except for clinical sense disambiguation and medication attribute extraction, where some zero-shot prompt types performed better.We also observed some general trends across the tasks and models.</p>
<p>We found that few-shot prompting enhanced accuracy by providing limited context that aided complex scenario understanding.The improvement was more pronounced compared to simple cloze prompts, which had lower accuracy in most of the tasks.We also found that some zero-shot prompt types were very effective for certain tasks, even outperforming few-shot prompting.These prompt types used a rule-based or reasoning approach to generate sentences that contained definitions or examples of the target words or concepts, which helped the LLM to understand and match the context.For example, heuristic prompts achieved higher accuracy than few-shot prompting for clinical sense disambiguation and medication attribute extraction, while chain of thought prompts achieved higher accuracy than few-shot prompting for coreference resolution and medication attribute extraction.Alternatively, the clinical evidence extraction task likely benefits from additional context provided by few-shot examples, which can guide the model more effectively than the broader inferences made in zero-shot scenarios.This suggests that tasks requiring deeper contextual understanding might be better suited to few-shot learning approaches.</p>
<p>From these results, we can infer that LLMs can be effectively used for clinical NLP in a no-data scenario, where we do not have many publicly available data sets, by using appropriate zero-shot prompt types that guide the LLM to produce clear and unambiguous outputs.However, few-shot prompting can also improve the performance of LLMs by providing some context that helps the LLM to handle complex scenarios.</p>
<p>Other Observations</p>
<p>Ensemble Approaches</p>
<p>We experimented with an ensemble approach by combining outputs from multiple prompts using majority voting.The ensemble approach was not the best-performing strategy for any of the tasks, but it was better than the low-performing prompts.The ensemble approach was able to benefit from the diversity and complementarity of different prompt types and avoid some of the pitfalls of individual prompts.For example, for clinical sense disambiguation, the ensemble approach achieved an accuracy of 0.9 with GPT-3.5, which was the second best-performing prompt type.Similarly, for medication attribute extraction, the ensemble approach achieved an accuracy of 0.9 with GPT-3.5 and 0.76 with Gemini, which were close to the best single prompt type (anticipatory).However, the ensemble approach also had some drawbacks, such as inconsistency and noise.For tasks that required more specific or consistent outputs, such as coreference resolution, the ensemble approach did not improve the accuracy over the best single prompt type and sometimes even decreased it.This suggests that the ensemble approach may introduce ambiguity for tasks that require more precise or coherent outputs.</p>
<p>While the ensemble approach aims to reduce the variance introduced by individual prompt idiosyncrasies, our specific implementation observed instances where the combination of diverse prompt types introduced additional complexity.This complexity occasionally manifested as inconsistency and noise in the outputs contrary to our objective of achieving higher performance.Future iterations of this approach may include refinement of the prompt selection process to enhance consistency and further reduce noise in the aggregated outputs.</p>
<p>Impact of LLMs</p>
<p>Variations in performance were observed among different LLMs (Table 3).We found that GPT-3.5 generally outperformed Gemini and LLaMA-2 on most tasks.This suggests that GPT-3.5 has a better generalization ability and can handle a variety of clinical NLP tasks with different prompt types.However, Gemini and LLaMA-2 also showed some advantages over GPT-3.5 on certain tasks and prompt types.For example, Gemini achieved the highest accuracy of 0.81 with simple cloze prompts and LLaMA-2 achieved the highest accuracy of 0.8 with simple prefix prompts for coreference resolution.This indicates that Gemini and LLaMA-2 may have some domain-specific knowledge that can benefit certain clinical NLP tasks for specific prompt types.</p>
<p>Persona Patterns</p>
<p>Persona patterns are a way of asking the LLM to act like a persona or a system that is relevant to the task or domain.For example, one can ask the LLM to "act as a clinical NLP expert."This can help the LLM to generate outputs that are more appropriate and consistent with the persona or system.For example, one can use the following prompt for clinical sense disambiguation:</p>
<p>Act as a clinical NLP expert.Disambiguate the word "cold" in the following sentence: "She had a cold for three days."</p>
<p>We experimented with persona patterns for different tasks and LLMs and found that they can improve the accuracy and quality of the outputs.Persona patterns can help the LLM to focus on the relevant information and constraints for the task and avoid generating outputs that are irrelevant or contradictory to the persona or system.</p>
<p>Randomness in Output</p>
<p>Most LLMs do not produce the output in the same format every time.There is inherent randomness in the outputs the LLMs produce.Hence, the prompts need to be specific in the way they are done for the task.Prompts are powerful when they are specific and if we use them in the right way.</p>
<p>Randomness in output can be beneficial or detrimental for different tasks and scenarios.In the clinical domain, randomness can introduce noise and errors in the outputs, which can make them less accurate and reliable for the users.For example, for tasks that involve extracting factual information, such as biomedical evidence extraction and medication status extraction, randomness can cause the LM to produce outputs that are inconsistent or contradictory with the input or context.</p>
<p>Guidelines and Suggestions for Optimal Prompt Selection</p>
<p>In recognizing the evolving nature of clinical NLP, we expand our discussion to contemplate the adaptability of our recommended prompt types and LM combinations across a wider spectrum of clinical tasks and narratives.This speculative analysis aims to bridge the gap between our current findings and their applicability to unexplored clinical NLP challenges, setting a foundation for future research to validate and refine these recommendations.In this section, we synthesize the main findings from our experiments and offer some practical advice for prompt engineering for zero-shot and few-shot clinical IE.We propose the following steps for selecting optimal prompts for different tasks and scenarios:</p>
<p>The first step is to identify the type of clinical NLP task, which can be broadly categorized into three types: (1) classification, (2) extraction, and (3) resolution.Classification tasks involve assigning a label or category to a word, phrase, or sentence in a clinical note, such as clinical sense disambiguation or medication status extraction.Extraction tasks involve identifying and extracting relevant information from a clinical note, such as biomedical evidence extraction or medication attribute extraction.Resolution tasks involve linking or matching entities or concepts in a clinical note, such as coreference resolution.</p>
<p>The second step is to choose the prompt type that is most suitable for the task type.We found that different prompt types have different strengths and weaknesses for different task types, depending on the level of information and constraints they provide to the LLM.Table 4 summarizes our findings and recommendations for optimal prompt selection for each task type.</p>
<p>The third step is to choose the LLM that is most compatible with the chosen prompt type.We found that different LLMs have different capabilities and limitations for different prompt types, depending on their generalization ability and domain-specific knowledge.Table 5 summarizes our findings and recommendations for optimal LLM selection for each prompt type.</p>
<p>The fourth step is to evaluate the performance of the chosen prompt type and LLM combination on the clinical NLP task using appropriate metrics such as accuracy, precision, recall, or F 1 -score.If the performance is satisfactory, then the prompt engineering process is complete.If not, then the process can be repeated by choosing a different prompt type or LLM or by modifying the existing prompt to improve its effectiveness.</p>
<p>Discussion</p>
<p>Principal Findings</p>
<p>In this paper, we have presented a novel approach to zero-shot and few-shot clinical IE using prompt engineering.Various prompt types were evaluated across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction.The performance of different LLMs, GPT-3.5, Gemini, and LLaMA-2, was also compared.Our main findings are as follows:</p>
<ol>
<li>Task-specific prompt tailoring is crucial for achieving high accuracy.Different tasks require different levels of information and constraints to guide the LLM to produce the desired output.Therefore, it is important to design prompts that are relevant and specific to the task at hand and avoid using generic or vague prompts that may confuse the model or lead to erroneous outputs.It is noteworthy that context size has a significant impact on the performance of LLMs in zero-shot IE [36].In the scope of this study, we have avoided the context size dependence on performance, as it is a complex issue that requires careful consideration.</li>
</ol>
<p>This study serves as an initial exploration into the efficacy of prompt engineering in clinical NLP, providing foundational insights rather than exhaustive guidelines.Given the rapid advancements in generative artificial intelligence and the complexity of clinical narratives, we advocate for continuous empirical testing of these prompt strategies across diverse clinical tasks and data sets.This approach will not only validate the generalizability of our findings but also uncover new avenues for enhancing the accuracy and applicability of LLMs in clinical settings.</p>
<p>Limitations</p>
<p>In this study, we primarily focused on exploring the capabilities and versatility of generative LLMs in the context of zero-shot and few-shot learning for clinical NLP tasks.Our approach also has some limitations that we acknowledge in this work.First, it relies on the quality and availability of pretrained LLMs, which may vary depending on the domain and task.As LLMs are rapidly evolving, some parts of the prompt engineering discipline may be timeless, while some parts may evolve and adapt over time as different capabilities of models evolve.Second, it requires a lot of experimentation and iteration to optimize prompts for different applications, which may be iterative and time-consuming.However, once optimal prompts are identified, the approach offers time savings in subsequent applications by reusing these prompts or making minor adjustments for similar tasks.We may not have explored all the possible combinations and variations of prompts that could potentially improve the performance of the clinical NLP tasks.Third, the LLMs do not release the details of the data set that they were trained on.Hence, the high accuracy could be because the models would have already seen the data during training and not because of the effectiveness of the prompts.</p>
<p>Future Work</p>
<p>We plan to address these challenges and limitations in our future work.We aim to develop more systematic and automated methods for prompt design and evaluation, such as using prompt-tuning or meta-learning techniques.We also aim to incorporate more domain knowledge or external resources into the prompts or the LLMs, such as using ontologies, knowledge graphs, or databases.We also aim to incorporate more quality control or error correction mechanisms into the prompts or the LLMs, such as using adversarial examples, confidence scores, or human feedback.</p>
<p>Conclusions</p>
<p>In this paper, we have benchmarked different prompt engineering techniques for both zero-shot and few-shot clinical NLP tasks.Two new types of prompts, heuristic and ensemble prompts, were also conceptualized and proposed.We have demonstrated that prompt engineering can enable the use of pretrained LMs for various clinical NLP tasks without requiring any fine-tuning or additional data.We have shown that task-specific prompt tailoring, heuristic prompts, chain of thought prompts, few-shot prompting, and ensemble approaches can improve the accuracy and quality of the outputs.We have also shown that GPT-3.5 is very adaptable and precise across all tasks and prompt types, while Gemini and LLaMA-2 may have some domain-specific advantages for certain tasks and prompt types.</p>
<p>We believe that a prompt-based approach has several benefits over existing methods for clinical IE.It reduces the cost and time in the initial phases of clinical NLP application development, where prompt-based methods offer a streamlined alternative to the conventional data preparation and model training processes.It is flexible and adaptable, as it can be applied to various clinical NLP tasks with different prompt types and LLMs.It is interpretable and explainable, as it uses natural language prompts that can be easily understood and modified by humans.</p>
<p>Figure 1 .
1
Figure 1.Iterative prompt design process: a schematic diagram of the iterative prompt creation process for clinical NLP tasks.The process consists of 3 steps: sampling, prompt designing, and deployment.The sampling step involves defining the task and collecting data and annotations.The prompt designing step involves creating and refining prompts using different types and language models.The deployment step involves selecting the best model and deploying the model for clinical use.LLM: large language model; NER: named entity recognition; NLP: natural language processing; RE: relation extraction.</p>
<p>Figure 2 .
2
Figure 2. Types of prompts: examples of 7 types of prompts that we used to query the pretrained language model for different clinical information extraction tasks.[X]: context; [Y]: abbreviation; [Z]: expanded form.</p>
<p>a</p>
<p>Best performance on a task regardless of the model (ie, for each GPT-3.5 or Gemini or LLaMA-2 triple).b Best performance for each model on a task.c BERT: Bidirectional Encoder Representations From Transformers.d ELMO: Embeddings From Language Models.e PubMedBERT-CRF: PubMedBERT-Conditional Random Field.</p>
<p>Figure 3 .
3
Figure 3. Graphical comparison of prompt types in the 5 clinical natural language processing tasks used in this study.</p>
<p>Table 1 .
1
Task descriptions.
TaskNLP a task categoryDescriptionExample promptClinical sense disambigua-Text classificationThis task involves identifying the correctWhat is the meaning of the abbreviationtiongiven context. meaning of clinical abbreviations within aCR b in the context of cardiology?In this task, interventions are extracted fromIdentify the psychological interventions inbiomedical abstracts.the given text?</p>
<p>Table 2 .
2
Evaluation data sets and samples for different tasks.
TaskData setData set exampleSamplesClinical sense disambigua-tionCASI aThe abbreviation "CR b " can refer to "car-diac resuscitation" or "computed radiogra-11 acronyms from 55 notesphy."Biomedical evidence extrac-tionEBM c -NLP dIdentifying panic, avoidance, and agorapho-bia (psychological interventions)187 abstracts and 20 annotated abstractsCoreference resolutionCASIResolving references to "the patient" or "the105 annotated examplesstudy" within a clinical trial report.Identifying that a patient is currently taking105 annotated examples with 340 medica-insulin for diabetes.tion status pairs</p>
<p>Table 3 .
3
Performance comparison of different prompt types and language models.
Task and language modelSimple pre-Simple clozeAnticipatoryHeuristicChain ofEnsembleFew shotfixthoughtClinical sense disambiguationGPT-3.50.880.860.880.96 a0.90.90.82Gemini0.76 b0.680.710.750.720.710.67LLaMA-20.88 b0.760.820.820.780.820.78BERT c (from [33])0.420.420.420.420.420.420.42ELMO d (from [33])0.550.550.550.550.550.550.55Biomedical evidence extractionGPT-3.50.920.820.880.940.940.880.96 aGemini0.890.890.91 b0.90.91 b0.90.88LLaMA-20.850.88 b0.870.88 b0.870.880.86PubMedBERT-CRF e (from [29])0.350.350.350.350.350.350.35Coreference resolutionGPT-3.50.780.60.740.94 a0.94 a0.740.74Gemini0.690.81 b0.730.670.710.690.7LLaMA-20.8 b0.640.740.760.8 b0.780.68Toshniwal et al [34]0.690.690.690.690.690.690.69Medication status extractionGPT-3.50.76 a0.720.750.740.730.750.72Gemini0.67 b0.510.650.550.590.580.55LLaMA-20.580.480.520.64 b0.520.580.42ScispaCy [35]0.520.520.520.520.520.520.52Medication attribute extractionGPT-3.50.880.840.90.96 a0.96 a0.90.96 aGemini0.680.720.88 c0.70.740.760.88 bLLaMA-20.60.660.580.660.72 b0.640.6</p>
<p>Table 4 .
4
Optimal prompt types for different clinical natural language processing task types.
Prompt type</p>
<p>Table 5 .
5
Optimal language models for different prompt types.
Language model
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 2 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 4 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 5 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 6 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 7 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 8 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 9 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 10 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 11 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 12 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
(page number not for citation purposes)
AcknowledgmentsThis work was supported by the National Institutes of Health (awards U24 TR004111 and R01 LM014306).The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.Authors' ContributionsSS conceptualized, designed, and organized this study; analyzed the results; and wrote, reviewed, and revised the paper.MK and AS-M analyzed the results, and wrote, reviewed, and revised the paper.SV wrote, reviewed, and revised the paper.YW conceptualized, designed, and directed this study and wrote, reviewed, and revised the paper.Conflicts of InterestNone declared.Multimedia Appendix 1Prompts for clinical natural language processing tasks.[DOCX File , 31 KB-MultimediaAppendix
Clinical information extraction applications: a literature review. Y Wang, L Wang, M Rastegar-Mojarad, S Moon, F Shen, N Afzal, 10.1016/j.jbi.2017.11.011J Biomed Inform. 772018FREE Full text. Medline: 29162496</p>
<p>Information extraction from electronic medical documents: state of the art and future research directions. M Y Landolsi, L Hlaoua, L B Romdhane, 10.1007/s10115-022-01779-1Knowl Inf Syst. 6522023FREE Full text. Medline: 36405956</p>
<p>HealthPrompt: a zero-shot learning paradigm for clinical natural language processing. S Sivarajkumar, Y Wang, Medline: 37128372]AMIA Annu Symp Proc. 20222022FREE Full text</p>
<p>Rethinking the role of demonstrations: what makes in-context learning work?. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022. December 7-11, 2022</p>
<p>United Arab Emirates. Abu Dhabi, 10.18653/v1/2022.emnlp-main.759</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatGPT. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, 10.48550/arXiv.2302.11382ArXiv. Preprint posted online on. February 21. 2023FREE Full text</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, Advances in Neural Information Processing Systems. Red Hook, NYCurran Associates, IncNeurIPS 2022. 202235</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Google , 10.48550/arXiv.2312.11805ArXiv. Preprint posted online on. December 19, 2023FREE Full text</p>
<p>Llama 2: open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, 10.48550/arXiv.2307.09288ArXiv. Preprint posted online on. July 28, 2023FREE Full text</p>
<p>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. Q Chen, J Du, Y Hu, V K Keloth, X Peng, K Raja, 10.48550/arXiv.2305.16326ArXiv. Preprint posted online on May. 102023FREE Full text</p>
<p>Prompt engineering for healthcare: methodologies and applications. J Wang, E Shi, S Yu, Z Wu, C Ma, H Dai, 10.48550/arXiv.2304.14670April 28. 2023FREE Full text</p>
<p>Improving large language models for clinical named entity recognition via prompt engineering. Y Hu, Q Chen, J Du, X Peng, V K Keloth, X Zuo, 10.48550/arXiv.2303.16416ArXiv. Preprint posted online on March. 292023FREE Full text</p>
<p>Zero-shot temporal relation extraction with chatGPT. C Yuan, Q Xie, S Ananiadou, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023. July 13. 2023</p>
<p>. Canada Toronto, 10.18653/v1/2023.bionlp-1.7</p>
<p>Prefix-tuning: optimizing continuous prompts for generation. X Li, L Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021. August 1-6, 20211Virtual Event</p>
<p>Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Comput Surv. 5592023FREE Full text</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, Advances in Neural Information Processing Systems. Red Hook, NYCurran Associates IncNeurIPS 2022. 202235</p>
<p>Learning from dialogue after deployment: feed yourself, chatbot!. B Hancock, A Bordes, P E Mazare, J Weston, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019. July 28-August 2, 2019</p>
<p>. Italy Florence, 10.18653/v1/p19-1358</p>
<p>Rule-based information extraction from patients' clinical data. A Mykowiecka, M Marciniak, A KupÅ›Ä‡, 10.1016/j.jbi.2009.07.007J Biomed Inform. 4252009FREE Full text. Medline: 19646551</p>
<p>Large language models are few-shot clinical information extractors. M Agrawal, S Hegselmann, H Lang, Y Kim, D Sontag, The 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2022. December 7-11. 2022. 1998-2022</p>
<p>United Arab Emirates. Abu Dhabi, 10.18653/v1/2022.emnlp-main.130</p>
<p>Clinical abbreviation disambiguation using neural word embeddings. Y Wu, J Xu, Y Zhang, H Xu, Proceedings of the 2015 Workshop on Biomedical Natural Language Processing. the 2015 Workshop on Biomedical Natural Language Processing2015. 2015. July 30. 2015</p>
<p>. China Beijing, 10.18653/v1/w15-3822</p>
<p>Machine learning approaches to retrieve high-quality, clinically relevant evidence from the biomedical literature: systematic review. W Abdelkader, T Navarro, R Parrish, C Cotoi, F Germini, A Iorio, 10.2196/30401JMIR Med Inform. 99e304012021FREE Full text. Medline: 34499041</p>
<p>Evaluating the state of the art in coreference resolution for electronic medical records. O Uzuner, A Bodnari, S Shen, T Forbush, J Pestian, B R South, 10.1136/amiajnl-2011-000784J Am Med Inform Assoc. 1952012FREE Full text. Medline: 22366294</p>
<p>Extracting timing and status descriptors for colonoscopy testing from electronic medical records. J C Denny, J F Peterson, N N Choma, H Xu, R A Miller, L Bastarache, 10.1136/jamia.2010.004804J Am Med Inform Assoc. 1742010FREE Full text. Medline: 20595304</p>
<p>Overview of the first natural language processing challenge for extracting medication, indication, and adverse drug events from electronic health record notes (MADE 1.0). A Jagannatha, F Liu, W Liu, H Yu, 10.1007/s40264-018-0762-zDrug Saf. 4212019FREE Full text</p>
<p>Dynamic text categorization of search results for medical class recognition in real world evidence studies in the Chinese language. Y Chen, X Wu, M Chen, Q Song, J Wei, X Li, Proceedings of the International Conference on Bioinformatics and Computational Intelligence. the International Conference on Bioinformatics and Computational IntelligenceAssociation for Computing Machinery20172017</p>
<p>. China Beijing, 10.1145/3135954.3135962</p>
<p>Cognitive Informatics and Soft Computing Proceeding of CISC 2017. P K Mallick, V E Balas, A K Bhoi, A F Zobaa, Advances in Intelligent Systems and Computing. New YorkSpringer Verlag2019768</p>
<p>S Ananiadou, D Lee, H Xu, M Song, 10.1145/2396761.239875812-The Proceedings of the Sixth ACM International Workshop on Data and Text Mining in Biomedical Informatics. New York2012. 2012. 2012Conjunction with the 21st ACM International Conference on Information and Knowledge Management</p>
<p>I Elghandour, R State, M Brorsson, L Le, N Antonopoulos, Y Xie, 10.1109/bdcat.2018.00008IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT). Shanghai, China2016. 2016. December 6-9, 2016IEEE/ACM International Symposium on Big Data Computing (BDC)</p>
<p>A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources. S Moon, S Pakhomov, N Liu, J O Ryan, G B Melton, 10.1136/amiajnl-2012-001506J Am Med Inform Assoc. 2122014FREE Full text. Medline: 23813539</p>
<p>A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. B Nye, J J Li, R Patel, Y Yang, I Marshall, A Nenkova, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics2018. July 15-20. 20181Presented at</p>
<p>. Australia Melbourne, 10.18653/v1/p18-1019</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, ArXiv. Preprint posted online on May. 242019FREE Full text</p>
<p>Detecting formal thought disorder by deep contextualized word representations. J Sarzynska-Wawer, A Wawer, A Pawlak, J Szymanowska, I Stefaniak, M Jarkiewicz, 10.1016/j.psychres.2021.114135Psychiatry Res. 3041141352021Medline: 34343877</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, 10.1145/3458754ACM Trans Comput Healthcare. 312021</p>
<p>Zero-shot clinical acronym expansion via latent meaning cells. G Adams, M Ketenci, S Bhave, A Perotte, N Elhadad, Proc Mach Learn Res. 1362020FREE Full text. Medline: 34790898</p>
<p>On generalization in coreference resolution. S Toshniwal, P Xia, S Wiseman, K Livescu, K Gimpel, 10.18653/v1/2021.crac-1.12ArXiv. Preprint posted online on. September 20, 2021FREE Full text</p>
<p>ScispaCy: fast and robust models for biomedical natural language processing. M Neumann, D King, I Beltagy, W Ammar, 10.18653/v1/w19-5034ArXiv. Preprint posted online on October. 92019FREE Full text</p>
<p>Evaluation of healthprompt for zero-shot clinical text classification. S Sivarajkumar, Y Wang, 10.1109/ichi57859.2023.000812023 IEEE 11th International Conference on Healthcare Informatics (ICHI). Houston, TX, USA2023. June 26-29, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
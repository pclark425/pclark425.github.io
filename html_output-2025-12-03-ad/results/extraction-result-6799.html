<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6799 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6799</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6799</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-275932268</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.16961v3.pdf" target="_blank">Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers</a></p>
                <p><strong>Paper Abstract:</strong> Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such *near-certain reasoning* as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6799.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6799.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Self-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses an LLM to synthesize formal solver programs (Z3) from natural language, generates concrete positive/negative instantiations, and uses an SMT solver to verify and iteratively repair the formalization; returns (Answer, isVerified) where isVerified=true indicates near-certain correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SSV (uses GPT-4 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A system-level technique that leverages a pre-trained transformer (GPT-4 or GPT-3.5) to (1) generate Z3 programs encoding the NL problem, (2) produce concrete positive and negative instantiations in natural language and translate them to solver expressions, and (3) verify and repair programs via the Z3 SMT solver.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (LLM) + SMT solver (Z3) via program synthesis and semantic verification</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified for the underlying LLMs; evaluation datasets: AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used for few-shot examples and evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis to formal solver code + concrete-instantiation based semantic verification + LLM-guided semantic repair + temperature sampling; falls back to CoT when program generation fails.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver (de Moura & Bjørner) executes generated programs to obtain answers, checks satisfiability (SAT/UNSAT) of concrete instantiations against each constraint, and provides the verification signal and counterexamples used for repair.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT (primary), also evaluated on FOLIO, LogicalDeduction, PrOntoQA, ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multiple-choice logical reasoning benchmarks including LSAT-style analytical reasoning (AR-LSAT), first-order logic real-world problems (FOLIO), synthetic multi-hop deductive reasoning (PrOntoQA), ProofWriter-style rule reasoning, and LogicalDeduction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logical deduction / analytical reasoning; formalization of NL problems to solver-executable constraints and multiple-choice option checking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (general), verification precision (selective accuracy on verified cases), verification coverage (proportion verified)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On AR-LSAT with GPT-4: 71.3% overall accuracy; verified subset coverage 21.7% with 100% verification precision (verified answers correct). Across benchmarks SSV reports higher general accuracy than baselines and near-perfect verification precision (100% with GPT-4). With GPT-3.5 average SSV accuracy = 56.2%; verification precision ≈97% on datasets with coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>SSV outperforms solver-augmented Logic-LM on AR-LSAT by +28.3% (Logic-LM 43.0% → SSV 71.3% with GPT-4). SSV outperforms Standard prompting and Chain-of-Thought baselines across evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Consistency-based verification using concrete instantiations substantially improves correctness of formalization and overall reasoning accuracy; verification provides a high-precision 'near-certain' boolean signal that can reduce manual verification effort for a significant subset of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verification is empirical, not a formal proof of NL→formal fidelity; coverage limited (e.g., 21.7% on hardest AR-LSAT), dependent on LLM quality (weaker LLMs yield lower coverage), failures arise from insufficient or non-exhaustive instantiations, mutually consistent but incorrect program+examples, missing/superfluous constraints, solver decidability limits in general first-order logic, and substantial runtime overhead (median ≈152s per task in the implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6799.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM (solver-augmented LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior solver-augmented approach where an LLM is prompted to generate a program/formalization that a logical solver executes; used as a state-of-the-art baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that prompts an LLM to produce a solver program (formal constraints) from NL and then executes the program in a logical solver to obtain answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + external solver via program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis to a solver (LLM produces formal constraints/program executed by solver)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses formal solvers (prior work) to execute the LLM-generated programs and derive answers; dependent on correctness of LLM formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used as baselines in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same set of multiple-choice logical reasoning benchmarks as SSV.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logical deduction / programmatic formalization + solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported example: 43.0% accuracy on AR-LSAT (GPT-4) per cited prior results used as baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Logic-LM was the best prior system on AR-LSAT (43.0%); SSV achieves +28.3 percentage points over Logic-LM on AR-LSAT with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting LLMs with solvers improves performance relative to direct NL reasoning, but final performance hinges on the correctness of the NL→formal translation produced by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to formalization errors by the LLM (incorrect quantifiers, scope, omitted constraints), limited robustness on complex tasks, and lacks the SSV-style concrete-instantiation verification that can detect and repair formalization mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6799.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks LLMs to generate step-by-step natural-language reasoning before producing a final answer; used as a baseline and fallback in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (applied with GPT-4 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompting/decoding technique for transformers that elicits intermediate reasoning steps in natural language to improve final answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer prompting technique (no architectural change)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Natural-language step-by-step reasoning (CoT); sometimes combined with self-consistency or other sampling methods in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multiple-choice logical reasoning benchmarks; CoT used to produce chain-of-thought before final choice.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>General reasoning / multi-step logical deduction in NL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT baseline numbers reported in paper (GPT-4): e.g., AR-LSAT ~35.1% (paper table); CoT modestly improves over Standard prompting but is substantially lower than solver-augmented methods and SSV on hard formalization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT improves over direct Standard prompting by small margins on many datasets, but far less than SSV when formalization and solver integration are helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT helps elicit internal reasoning but remains informal and can produce logically inconsistent steps; integrating with formal solvers can provide stronger guarantees if formalization is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reasoning remains in natural language and is prone to logical inconsistencies; CoT alone cannot guarantee formal correctness and is not a substitute for formal solver verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6799.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard prompting / direct LLM inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method: in-context learning prompting of an LLM to directly answer questions without explicit chain-of-thought or solver augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard prompting (GPT-4 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Direct prompting/in-context learning applied to pre-trained transformer LLMs to produce answers without explicit reasoning traces or solver programs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct answer generation via in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Multiple-choice logical reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>General NL reasoning / multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: Standard prompting ~33.3% on AR-LSAT (GPT-4) as reported in the paper's table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as baseline; CoT and solver-augmented methods typically outperform Standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct prompting performs poorly on complex formal reasoning tasks compared to solver-augmented or SSV approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No formal verification; susceptible to hallucinations and logical errors in multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6799.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art general-purpose large language model (used in paper as the primary high-quality base LLM for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based language model from OpenAI, used here as the primary reasoning backbone for program generation, instantiation generation, and repair prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with program synthesis prompts, instantiation-generation prompts, semantic repair prompts, and CoT fallback.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT (primary demonstration), FOLIO, LogicalDeduction, PrOntoQA, ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated as the LLM in SSV and baselines on multiple logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Used for NL→formal program generation, NL instantiation generation, and fall-back CoT reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; verification precision & coverage when used within SSV</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When used as base LLM in SSV: AR-LSAT accuracy 71.3%; verified-case precision 100% (coverage 21.7% on AR-LSAT); SSV outperforms baselines using GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>GPT-4 yields substantially better SSV coverage/accuracy than GPT-3.5 in the paper's experiments; SSV with GPT-4 achieves SoTA reported numbers in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality LLMs (GPT-4) substantially increase both program-generation success and verification coverage, producing near-perfect verification precision in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not report model internals or sizes; results dependent on model capability — weaker models show lower coverage though verification precision remains high when it succeeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6799.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A weaker-generation variant of OpenAI's GPT family used as a lower-quality base LLM in experiments to assess sensitivity of SSV to LLM quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM with lower reasoning capability compared to GPT-4; used to run identical prompts and measure effects on program generation, verification coverage, and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Used with the same SSV prompting pipeline (program generation, instantiation generation, semantic repair, CoT fallback).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (same evaluation sets)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated as lower-capability LLM in SSV and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>NL→formal program synthesis + instantiation generation + CoT fallback</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy; verification precision & coverage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SSV with GPT-3.5: average accuracy 56.2% across datasets (paper reports); on some hard datasets (AR-LSAT, LogicalDeduction) verification coverage dropped to 0, but where coverage exists the verification precision averaged ~97%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>All methods' accuracies drop with GPT-3.5 relative to GPT-4; SSV still outperforms other baselines on average but loses coverage on the hardest datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Weaker LLMs reduce verification coverage and general accuracy, but when SSV verification succeeds even with GPT-3.5 it remains highly reliable (high precision).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower program-generation quality causes many verification failures; demonstrates dependence of SSV coverage on LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6799.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6799.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SatLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SatLM (Satisfiability-aided language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior approach that aids language models with satisfiability/solver assistance using declarative prompting (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SatLM: Satisfiability-aided language models using declarative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SatLM (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior method coupling LLMs with satisfiability-solving ideas via declarative prompts (referenced in related work as alternative solver-augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer + SAT/solver integration (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Declarative prompting combined with satisfiability assistance</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as related work exploring LLM+solver integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated in this paper; mentioned to contrast SSV's instantiation-based verification technique.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>SatLM: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Logic-LM <em>(Rating: 2)</em></li>
                <li>PrOntoQA <em>(Rating: 1)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>ProofWriter <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6799",
    "paper_id": "paper-275932268",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "SSV",
            "name_full": "Semantic Self-Verification",
            "brief_description": "A method that uses an LLM to synthesize formal solver programs (Z3) from natural language, generates concrete positive/negative instantiations, and uses an SMT solver to verify and iteratively repair the formalization; returns (Answer, isVerified) where isVerified=true indicates near-certain correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SSV (uses GPT-4 / GPT-3.5)",
            "model_description": "A system-level technique that leverages a pre-trained transformer (GPT-4 or GPT-3.5) to (1) generate Z3 programs encoding the NL problem, (2) produce concrete positive and negative instantiations in natural language and translate them to solver expressions, and (3) verify and repair programs via the Z3 SMT solver.",
            "model_size": null,
            "architecture_type": "Transformer (LLM) + SMT solver (Z3) via program synthesis and semantic verification",
            "training_data": "Not specified for the underlying LLMs; evaluation datasets: AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used for few-shot examples and evaluation).",
            "reasoning_method": "Program synthesis to formal solver code + concrete-instantiation based semantic verification + LLM-guided semantic repair + temperature sampling; falls back to CoT when program generation fails.",
            "external_tool_used": true,
            "external_tool_description": "Z3 SMT solver (de Moura & Bjørner) executes generated programs to obtain answers, checks satisfiability (SAT/UNSAT) of concrete instantiations against each constraint, and provides the verification signal and counterexamples used for repair.",
            "benchmark_name": "AR-LSAT (primary), also evaluated on FOLIO, LogicalDeduction, PrOntoQA, ProofWriter",
            "benchmark_description": "Multiple-choice logical reasoning benchmarks including LSAT-style analytical reasoning (AR-LSAT), first-order logic real-world problems (FOLIO), synthetic multi-hop deductive reasoning (PrOntoQA), ProofWriter-style rule reasoning, and LogicalDeduction tasks.",
            "task_type": "First-order logical deduction / analytical reasoning; formalization of NL problems to solver-executable constraints and multiple-choice option checking",
            "performance_metric": "Accuracy (general), verification precision (selective accuracy on verified cases), verification coverage (proportion verified)",
            "performance_value": "On AR-LSAT with GPT-4: 71.3% overall accuracy; verified subset coverage 21.7% with 100% verification precision (verified answers correct). Across benchmarks SSV reports higher general accuracy than baselines and near-perfect verification precision (100% with GPT-4). With GPT-3.5 average SSV accuracy = 56.2%; verification precision ≈97% on datasets with coverage.",
            "comparison_with_baseline": "SSV outperforms solver-augmented Logic-LM on AR-LSAT by +28.3% (Logic-LM 43.0% → SSV 71.3% with GPT-4). SSV outperforms Standard prompting and Chain-of-Thought baselines across evaluated datasets.",
            "key_findings": "Consistency-based verification using concrete instantiations substantially improves correctness of formalization and overall reasoning accuracy; verification provides a high-precision 'near-certain' boolean signal that can reduce manual verification effort for a significant subset of cases.",
            "limitations": "Verification is empirical, not a formal proof of NL→formal fidelity; coverage limited (e.g., 21.7% on hardest AR-LSAT), dependent on LLM quality (weaker LLMs yield lower coverage), failures arise from insufficient or non-exhaustive instantiations, mutually consistent but incorrect program+examples, missing/superfluous constraints, solver decidability limits in general first-order logic, and substantial runtime overhead (median ≈152s per task in the implementation).",
            "uuid": "e6799.0",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM (solver-augmented LLM baseline)",
            "brief_description": "A prior solver-augmented approach where an LLM is prompted to generate a program/formalization that a logical solver executes; used as a state-of-the-art baseline in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Logic-LM",
            "model_description": "An approach that prompts an LLM to produce a solver program (formal constraints) from NL and then executes the program in a logical solver to obtain answers.",
            "model_size": null,
            "architecture_type": "Transformer + external solver via program synthesis",
            "training_data": null,
            "reasoning_method": "Program synthesis to a solver (LLM produces formal constraints/program executed by solver)",
            "external_tool_used": true,
            "external_tool_description": "Uses formal solvers (prior work) to execute the LLM-generated programs and derive answers; dependent on correctness of LLM formalization.",
            "benchmark_name": "AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used as baselines in experiments)",
            "benchmark_description": "Same set of multiple-choice logical reasoning benchmarks as SSV.",
            "task_type": "First-order logical deduction / programmatic formalization + solving",
            "performance_metric": "Accuracy",
            "performance_value": "Reported example: 43.0% accuracy on AR-LSAT (GPT-4) per cited prior results used as baseline in this paper.",
            "comparison_with_baseline": "Logic-LM was the best prior system on AR-LSAT (43.0%); SSV achieves +28.3 percentage points over Logic-LM on AR-LSAT with GPT-4.",
            "key_findings": "Augmenting LLMs with solvers improves performance relative to direct NL reasoning, but final performance hinges on the correctness of the NL→formal translation produced by the LLM.",
            "limitations": "Prone to formalization errors by the LLM (incorrect quantifiers, scope, omitted constraints), limited robustness on complex tasks, and lacks the SSV-style concrete-instantiation verification that can detect and repair formalization mistakes.",
            "uuid": "e6799.1",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that asks LLMs to generate step-by-step natural-language reasoning before producing a final answer; used as a baseline and fallback in this paper.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "Chain-of-Thought prompting (applied with GPT-4 / GPT-3.5)",
            "model_description": "A prompting/decoding technique for transformers that elicits intermediate reasoning steps in natural language to improve final answer accuracy.",
            "model_size": null,
            "architecture_type": "Transformer prompting technique (no architectural change)",
            "training_data": null,
            "reasoning_method": "Natural-language step-by-step reasoning (CoT); sometimes combined with self-consistency or other sampling methods in prior work.",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (used as a baseline)",
            "benchmark_description": "Multiple-choice logical reasoning benchmarks; CoT used to produce chain-of-thought before final choice.",
            "task_type": "General reasoning / multi-step logical deduction in NL",
            "performance_metric": "Accuracy",
            "performance_value": "CoT baseline numbers reported in paper (GPT-4): e.g., AR-LSAT ~35.1% (paper table); CoT modestly improves over Standard prompting but is substantially lower than solver-augmented methods and SSV on hard formalization tasks.",
            "comparison_with_baseline": "CoT improves over direct Standard prompting by small margins on many datasets, but far less than SSV when formalization and solver integration are helpful.",
            "key_findings": "CoT helps elicit internal reasoning but remains informal and can produce logically inconsistent steps; integrating with formal solvers can provide stronger guarantees if formalization is correct.",
            "limitations": "Reasoning remains in natural language and is prone to logical inconsistencies; CoT alone cannot guarantee formal correctness and is not a substitute for formal solver verification.",
            "uuid": "e6799.2",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Standard",
            "name_full": "Standard prompting / direct LLM inference",
            "brief_description": "Baseline method: in-context learning prompting of an LLM to directly answer questions without explicit chain-of-thought or solver augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard prompting (GPT-4 / GPT-3.5)",
            "model_description": "Direct prompting/in-context learning applied to pre-trained transformer LLMs to produce answers without explicit reasoning traces or solver programs.",
            "model_size": null,
            "architecture_type": "Transformer prompting",
            "training_data": null,
            "reasoning_method": "Direct answer generation via in-context examples",
            "external_tool_used": false,
            "external_tool_description": "",
            "benchmark_name": "AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (baseline comparisons)",
            "benchmark_description": "Multiple-choice logical reasoning benchmarks",
            "task_type": "General NL reasoning / multiple-choice",
            "performance_metric": "Accuracy",
            "performance_value": "Example: Standard prompting ~33.3% on AR-LSAT (GPT-4) as reported in the paper's table.",
            "comparison_with_baseline": "Used as baseline; CoT and solver-augmented methods typically outperform Standard prompting.",
            "key_findings": "Direct prompting performs poorly on complex formal reasoning tasks compared to solver-augmented or SSV approaches.",
            "limitations": "No formal verification; susceptible to hallucinations and logical errors in multi-step reasoning.",
            "uuid": "e6799.3",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A state-of-the-art general-purpose large language model (used in paper as the primary high-quality base LLM for experiments).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based language model from OpenAI, used here as the primary reasoning backbone for program generation, instantiation generation, and repair prompts.",
            "model_size": null,
            "architecture_type": "Transformer",
            "training_data": null,
            "reasoning_method": "Used with program synthesis prompts, instantiation-generation prompts, semantic repair prompts, and CoT fallback.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "AR-LSAT (primary demonstration), FOLIO, LogicalDeduction, PrOntoQA, ProofWriter",
            "benchmark_description": "Evaluated as the LLM in SSV and baselines on multiple logical reasoning datasets.",
            "task_type": "Used for NL→formal program generation, NL instantiation generation, and fall-back CoT reasoning",
            "performance_metric": "Accuracy; verification precision & coverage when used within SSV",
            "performance_value": "When used as base LLM in SSV: AR-LSAT accuracy 71.3%; verified-case precision 100% (coverage 21.7% on AR-LSAT); SSV outperforms baselines using GPT-4.",
            "comparison_with_baseline": "GPT-4 yields substantially better SSV coverage/accuracy than GPT-3.5 in the paper's experiments; SSV with GPT-4 achieves SoTA reported numbers in this work.",
            "key_findings": "High-quality LLMs (GPT-4) substantially increase both program-generation success and verification coverage, producing near-perfect verification precision in experiments.",
            "limitations": "Paper does not report model internals or sizes; results dependent on model capability — weaker models show lower coverage though verification precision remains high when it succeeds.",
            "uuid": "e6799.4",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5",
            "brief_description": "A weaker-generation variant of OpenAI's GPT family used as a lower-quality base LLM in experiments to assess sensitivity of SSV to LLM quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Transformer-based LLM with lower reasoning capability compared to GPT-4; used to run identical prompts and measure effects on program generation, verification coverage, and precision.",
            "model_size": null,
            "architecture_type": "Transformer",
            "training_data": null,
            "reasoning_method": "Used with the same SSV prompting pipeline (program generation, instantiation generation, semantic repair, CoT fallback).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "AR-LSAT, FOLIO, LogicalDeduction, PrOntoQA, ProofWriter (same evaluation sets)",
            "benchmark_description": "Evaluated as lower-capability LLM in SSV and baselines.",
            "task_type": "NL→formal program synthesis + instantiation generation + CoT fallback",
            "performance_metric": "Accuracy; verification precision & coverage",
            "performance_value": "SSV with GPT-3.5: average accuracy 56.2% across datasets (paper reports); on some hard datasets (AR-LSAT, LogicalDeduction) verification coverage dropped to 0, but where coverage exists the verification precision averaged ~97%.",
            "comparison_with_baseline": "All methods' accuracies drop with GPT-3.5 relative to GPT-4; SSV still outperforms other baselines on average but loses coverage on the hardest datasets.",
            "key_findings": "Weaker LLMs reduce verification coverage and general accuracy, but when SSV verification succeeds even with GPT-3.5 it remains highly reliable (high precision).",
            "limitations": "Lower program-generation quality causes many verification failures; demonstrates dependence of SSV coverage on LLM capability.",
            "uuid": "e6799.5",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SatLM",
            "name_full": "SatLM (Satisfiability-aided language models)",
            "brief_description": "A referenced prior approach that aids language models with satisfiability/solver assistance using declarative prompting (mentioned in related work).",
            "citation_title": "SatLM: Satisfiability-aided language models using declarative prompting",
            "mention_or_use": "mention",
            "model_name": "SatLM (referenced)",
            "model_description": "Prior method coupling LLMs with satisfiability-solving ideas via declarative prompts (referenced in related work as alternative solver-augmentation).",
            "model_size": null,
            "architecture_type": "Transformer + SAT/solver integration (prior work)",
            "training_data": null,
            "reasoning_method": "Declarative prompting combined with satisfiability assistance",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "",
            "benchmark_description": "",
            "task_type": "",
            "performance_metric": "",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "Mentioned as related work exploring LLM+solver integrations.",
            "limitations": "Not evaluated in this paper; mentioned to contrast SSV's instantiation-based verification technique.",
            "uuid": "e6799.6",
            "source_info": {
                "paper_title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "SatLM: Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Logic-LM",
            "rating": 2
        },
        {
            "paper_title": "PrOntoQA",
            "rating": 1
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "ProofWriter",
            "rating": 2,
            "sanitized_title": "proofwriter"
        }
    ],
    "cost": 0.0184355,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers
12 Jul 2025</p>
<p>Mohammad Raza 
Qatar Computing Research Institute</p>
<p>Natasa Milic-Frayling 
Qatar Computing Research Institute</p>
<p>Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers
12 Jul 20250F8A9DDB2B1E01BB540821E05A180282arXiv:2501.16961v3[cs.AI]
Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AIdriven reasoning systems.We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver.SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver.In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks.We propose such near-certain reasoning as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.</p>
<p>Introduction</p>
<p>Logical reasoning remains a persistent challenge for large language models (LLMs).Although these models demonstrate reasoning capabilities across various domains, their reasoning often lacks robustness and becomes increasingly error-prone as task complexity increases.Many recent approaches have made notable advancements in this active area of research.Chain-of-thought (CoT) prompting has demonstrated how the quality of reasoning can be improved by prompting the model to explicitly generate the steps of reasoning in natural language before arriving at the final answer [Wei et al., 2022].Variants of CoT and other related prompting and fine-tuning approaches have shown further improvements [Zhou et al., 2023;Wang et al., 2023;Yu et al., 2024;Weng et al., 2023;Creswell et al., 2023].To address the logical inconsistencies that can arise in such natural language approaches, another interesting direction is to incorporate LLMs with logical solvers or automated reasoning tools [Pan et al., 2023;Ye et al., 2023].Rather than directly attempting reasoning with the LLM, these approaches use the LLM to infer a formal representation of the problem as a program that can be executed by the solver, as such automated reasoning tools guarantee logically sound inference by construction.</p>
<p>While these approaches have demonstrated relative improvements in accuracy, we are still far from achieving robustness and reliability of reasoning.For instance, Figure 1 shows an example reasoning problem from the Law School Admissions Test on analytical reasoning [Zhong et al., 2022].On tasks of such complexity, the best reported accuracy, achieved by a solver-augmented system, is only 43% [Pan et al., 2023].Such lack of reliability especially hinders the practical usability of existing approaches: the burden of verifying correctness is always on the user, which can be especially difficult and error-prone for complex reasoning tasks.Therefore, having a reliable signal of correctness with high confidence can be hugely beneficial to help reduce the overall manual effort and cost of verification.</p>
<p>In this work, we propose a new approach to correctly formalizing reasoning problems called Semantic Self-Verification (SSV), which offers two key benefits: (1) it improves the overall accuracy of reasoning significantly over SoTA, and (2) it provides a novel feature of verification that has near-perfect precision.In our problem formulation, in addition to producing an answer to a given question, the system also indicates if it was able to verify the correctness of the answer: Question → (Answer, isVerified).This problem formulation is similar to confidence estimation in machine learning, except that in our case the isVerified indicator is a boolean rather than continuous value: if true, it indicates a "near certain" confidence in the correctness of the answer.Such highconfidence verification can reduce the need for manual checking in many cases.</p>
<p>At its core, our approach addresses the key challenge in combining LLMs with the robust reasoning of logical solvers: the formulation of a problem from informal natural language (NL) to the formal representation that is a program executable by the solver.For example, Figure 2 shows the formal representation of the NL problem from Figure 1.In this case the formalization is expressed as code in the language of the Z3 SMT solver [de Moura and Bjørner, 2008], which is a stateof-the-art industrial strength theorem prover that can produce the correct answer when given these correctly-expressed formal constraints.The crucial task, therefore, is for the LLM  to correctly translate the NL problem description to such a formal representation, and this is where LLMs can make significant errors, as shown by the limits of prior work [Pan et al., 2023;Ye et al., 2023].</p>
<p>Our approach of verifying that a formal representation is true to the original problem is inspired by how humans often create formalizations of problems expressed in natural language.For instance, when school students solve math word problems, they must first create the right algebraic equation that represents the problem, before they can solve it to get the answer.To ensure that their translation to an abstract equation represents the problem correctly, they are encouraged to consider various example instances of the problem and to check that the abstract equation consistently satisfies those instances so that it all "makes sense".In the same way, in the SSV approach, rather than just doing a single abstract translation from NL to a formal representation, we also use the LLM to additionally generate various concrete instantiations, or examples, of the general constraint, which are used as test cases to check the correctness of the abstract formalization.Using the logical solver, we verify that each of these instantiations is consistently satisfied by the formal representation.If all of these distinct semantic relationships consistently hold, then verification passes.</p>
<p>Figure 4 illustrates how the SSV approach works for the third constraint from the problem in Figure 2, which requires that Stacy and Yolanda cannot repair the same type of machine.A direct translation using the LLM may produce an incorrect abstract formalization of this constraint as shown in Figure 4a, where the condition is asserted only for some machine rather than for all machines because the Exists quantifier is incorrectly used.However, in the SSV approach, we use the LLM to additionally infer simple concrete instantiations, or examples, of the general constraint.For instance, a concrete positive example is that Stacy repairs radios and Yolanda repairs TVs.A negative example is that Stacy and Yolanda both repair TVs.After inferring these examples in NL, we also use the LLM to translate them to formal expressions in the language of the solver.We then use the solver to check that each of these expressions is consistent with the abstract formalization.In Figure 4a we see that the negative instantiation fails verification because the abstract formalization does not assert the condition for all machine types, so it still allows Stacy and Yolanda to both repair TVs.However, with the correct formalization in Figure 4b that uses the ForAll quantifier, both instantiations pass the solver verification, since the abstract formalization correctly disallows that any machine can be repaired by both technicians.</p>
<p>We note that any notion of verification from natural to formal language cannot provide formal correctness guarantees, since natural language itself is inherently informal and often ambiguous.However, as we demonstrate empirically, a passing verification in our case indicates a near certain confidence in the answer correctness since multiple independent semantic relationships are consistently satisfied.In this respect, our approach is akin to a consensus-based ensemble as it is based on agreement between multiple independent predictors [Zhou, 2012].However, rather than all predictors addressing the same task, we have a semantic ensemble of predictors that are addressing different but semantically related tasks and the logical solver verifies the formal consistency between these.We also note that unlike standard proposerverifier approaches, in our case there is no verifier that can check correctness of a proposed formalization: our verification is thus based on formal consistency between abstract and concrete inferences.</p>
<p>Furthermore, having such a high precision verification mechanism also allows us to improve the formalization itself, in two different respects.Firstly, any failing instantiation can be used as concrete guidance to refine the formalization further, as it can hint at potential errors.This is similar to error-based refinement in code generation techniques [Chen et al., 2024], except that here we are guided by semantic errors inferred from the instantiations rather than just syntactic execution errors in the code.Secondly, with our verification mechanism we can also explore the search space more extensively: using temperature sampling to create multiple candidate formalizations and selecting ones that pass verification.</p>
<p>Our evaluation demonstrates how the SSV approach achieves a significant increase in overall accuracy, as well as a near-perfect precision (or selective accuracy) on the verified cases.Figure 3 highlights the results for the most challenging AR-LSAT law school tests dataset.Though better than direct LLM inference and CoT, the accuracy of the best performing existing system (the solver-augmented Logic-LM approach by [Pan et al., 2023]) is at 43%, while SSV achieves a significantly higher accuracy of 71.3%, which also surpasses the average human performance.Moreover, the precision of the 21.7% of cases that it is able to verify is 100%.This means that a 21.7% reduction in manual verification effort can potentially be made on tasks of such high complexity.In our full evaluation we also show higher accuracy and coverage of verified cases on other standard reasoning datasets.</p>
<p>In summary, we make the following contributions in this work: (1) We propose the problem formulation of returning a boolean high-confidence verification indication in addition to the answer, which can be used to reduce manual cost of verification.(2) We present the novel technique of semantic self-verification, which uses concrete instantiations to verify the correctness of the problem formalization.(3) We show how SSV can also improve the formalization itself through instantiation-guided refinement and exploration of multiple candidate formalizations.(4) We present an extensive evaluation on five open benchmarks that shows a significant increase in overall accuracy over SoTA, as well as near-perfect selective accuracy over a significant coverage of verified cases. 1 2 Semantic Self-Verification This section describes the semantic self-verification approach for reasoning problems, which generates programs verified and refined by concrete instantiations.Figure 5  For each temperature value to be explored, the algorithm first uses the LLM to infer a program P that the solver executes to answer the question Q, such as the program from Figure 2. If an executable program is generated (P ̸ = ∅), the verification loop begins (line 4).The solver first executes P to obtain an answer.Then, for verification, we infer concrete instantiations I, which are test cases for the program's constraints and options, such as the six constraints and five options in Figure 2. The solver attempts to verify that each instantiation is formally satisfiable and returns any failing instantiation I fail .For example, for the third constraint in the technicians program, inferred instantiations (Figure 4a) may yield the failing case: "Stacy and Yolanda cannot both repair TVs."If no failing instantiation is found (as in Figure 4b) and P satisfies general well-formedness properties, the algorithm returns its answer A along with verification success (line 12).</p>
<p>If verification fails, we attempt to repair the program P using the LLM and any failing instantiation, which provides insight into potential constraint implementation errors.For example, the failing instantiation in Figure 4a may guide the LLM to assert the condition for all machine types using the forall quantifier, as shown in Figure 4b.After obtaining the repaired program, we repeat the verification loop.If no answer is verified across all temperatures and repair attempts, we exit the outer loop (line 16).If no executable program was inferred, we fall back to direct inference using the LLM with a chain-of-thought prompt, as in prior work [Pan et al., 2023].Otherwise, we return the best answer with verification failure.We next discuss key algorithm phases in more detail.</p>
<p>Program generation.et al., 2023;Pourreza and Rafiei, 2024], generating the program incrementally for each identified constraint.This improves code quality compared to direct prompting, which often produces syntax errors.</p>
<p>Semantic verification.While code generation ensures an executable solver program, it does not address semantic correctness-whether the program accurately implements the problem's intended constraints.SSV addresses this by generating and verifying concrete instantiations for each constraint in the generated program.The GenInstantiations function first parses the program P to extract constraints and their NL descriptions.Our program generation phase structures programs in segments of the form P init + C 1 + ... + C N + O 1 + ... + O M , where P init contains initial definitions, followed by explicitly segmented constraints and options, each annotated with NL comments (e.g.see "#CONSTRAINT:" and "#OPTION:" segments in Figure 2).This structure allows parsing constraints along with their NL descriptions.</p>
<p>We use the LLM to infer concrete instantiations for each of the constraints, using their NL descriptions.For each constraint C i , our implementation prompts the LLM for one positive and one negative instantiation, and both instantiations are translated into solver expressions (Figure 4).Once all instantiations I are obtained, the Verify function uses the solver to check if each constraint is consistent with its respective instantiations.For each constraint C i , we it verifies its positive instantiation I p by constructing and executing the expression P init + C i + I p and checking that the solver returns SAT.For the negative instantiation I n , it checks that the expression P init + C i + I n is UNSAT.If this holds for all constraints, the full program is considered verified.If verification fails, it returns the first failing instantiation I fail ∈ I.</p>
<p>Beyond verifying concrete instantiations, we also check general logical well-formedness properties using the IsWellFormed function, which ensures (1) the program follows the specified structure, (2) it returns a single answer, and (3) it avoids degenerate expressions-tautologies or vacuous implications that introduce redundancies or oversimplifications in the problem formalization.</p>
<p>Semantic program repair.If verification fails and a fail-
Require: Q //A best ← InferLLMAnswer(LLM, Q) 21: end if 22: return (A best , False)
Figure 5: The Semantic Self-Verification Algorithm ing instantiation I fail is found, the RepairProgram function attempts to repair the original program P , provided no answer has been found.Unlike error-based program repair, this is a semantic repair based on an instantiation inferred by the LLM rather than an execution error.In our repair prompt, we supply the initial definitions code, the constraint code with its NL description, and the failing instantiation expression.The LLM is prompted to first do a chain-of-thought analysis to infer whether the error lies in the initial definitions, the constraint code, or the instantiation itself, before inferring the corrected code.The prompts used for code generation/refinement, instantiation generation and semantic repair are available in the appendix.</p>
<p>Evaluation</p>
<p>We evaluate our SSV technique on open benchmarks for logical reasoning, focusing on two key aspects: (1) improving the general accuracy of reasoning over existing baselines and (2) assessing verification quality in terms of both precision (correctness) and coverage (proportion of verified cases).</p>
<p>Datasets.We use five common datasets for logical reasoning.All datasets follow a multiple-choice format, where each task includes a problem statement, a question, and answer options (e.g., Figure 1).PrOntoQA is a synthetic deductive reasoning dataset for LLM evaluation [Saparov and He, 2023].We use its most challenging subset-fictional character tasks requiring 5 reasoning hops-comprising 500 test examples with 2 answer options (True/False).ProofWriter is a widely Baselines.We compare our technique against three baselines, which represent approaches of reasoning using the LLM alone, as well as the combination of formal logical solvers with LLMs.Each of these baselines and our own system is parametric in the LLM used, and in our experiments we investigate all systems with both the GPT-4 model (a current best general LLM for reasoning) as well as the weaker GPT-3.5 model from Open AI.We use the baselines and their results for these models as reported in [Pan et al., 2023].The baselines are as follows.Standard is the direct approach of prompting the LLM, leveraging in-context learning to answer the question.CoT (Chain-of-Thought) [Wei et al., 2022] follows a step-by-step reasoning process, generating explanations before the final answer.Logic-LM is a state-of-the-art method that integrates LLMs with solvers for formal reasoning [Pan et al., 2023], where the LLM is prompted to generate a solver program to solve the task.SSV is our semantic selfverification technique (Figure 5).Our implementation uses the Z3 SMT solver [de Moura and Bjørner, 2008] and applies identical prompts for both models, with 1-4 few-shot examples drawn from training datasets (detailed in the Appendices).Our full SSV implementation sets MaxRepairs = 2 and Temperatures = [0, 0.3, 0.4, 0.5] (covering low to midrange values), with parameter variations explored in the ablation analysis.</p>
<p>Results</p>
<p>Main results Table 1 presents the main results, with all systems evaluated using GPT-4 as the underlying LLM.The ta- 1. SSV outperforms all baselines in general accuracy.Our technique achieves a higher general accuracy over all baseline systems across all datasets.We especially note the drastic increase of 28.3% over the current best Logic-LM system on the most difficult AR-LSAT dataset.This shows the strong effectiveness of our technique in producing robust problem formalizations in contrast to just a direct LLM translation from the natural language description to the solver program.</p>
<ol>
<li>
<p>SSV verification has perfect precision across all datasets.With GPT-4 as base model, SSV achieves 100% verification precision on all datasets.Notably, on AR-LSAT, FOLIO, and ProofWriter, our verification mechanism identified erroneous cases where the datasets contained incorrect answers.However, for comparison with baselines, in Table 1 we also report results based on the original datasets (showing slightly lower precision due to mislabelled cases).See the appendix for details of corrections.For AR-LSAT cases we also verified our corrections against the original test answers 2 .This empirically perfect precision highlights SSV's robustness on complex reasoning tasks.</p>
</li>
<li>
<p>SSV verification has significant coverage on all datasets.Although the precision is very high, we know that SSV verification does not always succeed.However, we find that the coverage is significant across all datasets, with the lowest coverage of 21.7% on the most difficult AR-LSAT dataset.As expected, we find the coverage increases on the relatively easier datasets, with a verification coverage of up to 75.2% on ProofWriter.This significant coverage of verification shows that the SSV approach can help in avoiding manual human verification in a significant proportion of cases to reduce overall cost and effort.</p>
</li>
</ol>
<p>Effect of semantic repair and temperature exploration.Figure 6 shows the impact of varying semantic repair attempts (MaxRepairs) and temperatures (Temperatures) on the AR-LSAT dataset.We analyze overall accuracy, program accuracy (how often program generation succeeds rather than direct LLM answers), and verification coverage.Semantic repair improves accuracy by 6.1%, while temperature exploration increases it by 10.0%.Verification coverage gains 5.2% with repair and more than doubles with temperature exploration, rising 12.2% above an initial 10.9%.Repair attempts yield diminishing returns and cease to improve any metric beyond three attempts, while temperature exploration continues to show some gains up to 0.6.Additionally, the gap between program accuracy and overall accuracy narrows (from 9.8% to 5.2%, when averaged over both temperature and repair attempts), indicating greater reliance on program generation with these enhancements.</p>
<p>We also ran a full ablation on AR-LSAT without any repair or temperature sampling (effectively replicating Logic-LM but using compositional code generation).This scored 55.7% vs. our 71.3%(Logic-LM: 43%), showing our novel features add 15.6%, and other enhancements contribute 12.7%.</p>
<p>Evaluation on GPT-3.5.We also evaluated our system and all baselines using GPT-3.5 as the underlying LLM.The results are shown in Table 2. Firstly, we note that while the general accuracy of all systems drops significantly with this weaker model, our SSV system still performs best overall, with an average accuracy of 56.2%.However, Logic-LM performs better than SSV on FOLIO and LogicalDeduction (this could be partly due to differences in the code generation quality for the different solver languages that Logic-LM uses for these datasets).Secondly, we observe that while the coverage of SSV verification also drops significantly, with two of the more difficult datasets (AR-LSAT and LogicalDeduction) having no coverage at all, the precision of SSV is very minimally affected.On the three datasets where there is coverage, we still see an average precision of 97%.This demon-2 https://img.cracklsat.net/lsat/pt/pt80.pdfstrates an important property of reliability of SSV verification: even for weaker models, if verification succeeds then it is still very reliable (and much more reliable than general accuracy), though it may succeed much less often.In practical terms, such reliability could even allow one to adopt a tiered strategy to optimize costs: trying weaker (cheaper) models for tasks first and fall-back on more expensive models if verification fails.</p>
<p>Verification failures We conducted a manual analysis on a sample of cases where verification did not pass.Classification of key reasons: program not well-formed (13.3%), program incorrect (53.3%), example incorrect (10%), both incorrect (23.3%).Thus in most cases the program was incorrect, which aligns with the expectation that examples inference is generally simpler than abstract program formulation.</p>
<p>Limitations and Future Directions</p>
<p>Since natural language is informal, any verification approach with NL specifications cannot guarantee full correctness.While SSV verification achieves near-perfect empirical precision (100% with GPT-4), we discuss the kinds of errors illustrated by some cases of incorrect verification observed with GPT-3.5 (specifically, one case in PrOntoQA and four in ProofWriter where incorrect answers passed verification).</p>
<ol>
<li>
<p>Concrete instantiations are insufficient.Since verification relies on concrete examples (test cases), these may not cover all aspects of a general constraint, particularly corner cases.This caused two failures with GPT-3.5.For instance, in one case, the conditions "Gary is nice" and "Gary is kind" were conflated into a single predicate "is kind(Gary)" in the formalization.An instantiation asserting "Gary is nice but not kind" could have detected this error.</p>
</li>
<li>
<p>Concrete instantiation and program are both mutually consistent but wrong.This is the unlikely case where both the program and the test case have the same error and therefore pass verification.We found only one such case which was a rather confusingly trivial error: for some reason the constraint "Fiona is quiet" was translated as its negation "Not(is quiet(Fiona))" in both the program and the concrete instantiation independently generated by GPT-3.5.</p>
</li>
<li>
<p>Missing or superfluous constraints.The LLM may omit required constraints or introduce unintended ones.Since our approach relies on explicitly demarcated constraints parsed from the LLM-generated program, such errors can cause verification failures.Two GPT-3.5 failures resulted from superfluous constraints.</p>
</li>
</ol>
<p>In general, such errors are rare, more common in weaker LLMs, and expected to decrease as LLMs improve.Errors of types (1) and (2) could be mitigated with a more exhaustive examples inference strategy, as our implementation generates only one positive and one negative example per constraint.Class (3) errors arise from structural inconsistencies where program constraints do not match the original problem.Such cases may be addressed by training specialized modules to more robustly enforce core structural properties.</p>
<p>Another potential limitation is that while industrial provers like Z3 are effectively decidable for many practical problems (we observed no failures due to the solver), in more complex cases our method will conservatively fail verification, as decidability of first-order logic is undecidable in general.Future work may also explore addressing this limitation using iterative LLM reasoning to assist solver convergence.</p>
<p>Related Work</p>
<p>Reasoning with LLMs.Improving the robustness of reasoning in large language models is a very active area of research. .While these approaches show relative improvements in accuracy, the reasoning is still based on informal natural language and is prone to errors in the reasoning steps.In contrast, we follow the approach of off-loading the reasoning task to a formal solver that can guarantee correctness of the reasoning steps.Our particular focus is on the key challenge of ensuring correct formalization of the problem.Tool-augmented reasoning.Integrating LLMs with specialized tools for performing various tasks is becoming increasingly common [Schick et al., 2023].This approach has also been adopted to improve the reasoning quality by augmenting the LLM with logical solvers or automated reasoning tools [Pan et al., 2023;Ye et al., 2023;Nye et al., 2021].The key challenge with these approaches is to ensure that the LLM correctly translates the reasoning problem from NL to the formal language of the solver.This is the main focus of our work, where we show how verification and refinement with respect to concrete instantiations generated by the LLM can both improve accuracy and also provide verification with near-perfect precision.[Kalyanpur et al., 2024] also infer logic programs with test cases, but their test cases are arbitrary logical expressions inferred together with the program, and thus prone to similar errors the LLM may make in the program.In contrast, we generate concrete instantiations (literal assignments) independently from the program constraints, which the LLM can infer from the NL without any logical formulation.This yields very high precision verification which we can offer as a standalone feature, unlike any prior work.Tool-augmented approaches have also been explored in the related areas of planning [Kambhampati et al., 2024;Guan et al., 2024] and auto-formalization [Wu et al., 2022;Jiang et al., 2023;He-Yueya et al., 2023], where informal mathematical proofs are translated to formal specifications defined in theorem provers like Isabelle [Paulson, 1994] and Lean [de Moura et al., 2015].While our work focuses on logical reasoning, the principle of consistency-based verificaion and refinement of formalizations using concrete instantiations is also potentially applicable to these other domains.</p>
<p>Self-verification approaches.Many related works have also explored the notion of self-verification by LLMs [Weng et al., 2023;Madaan et al., 2023;Xie et al., 2023;Ling et al., 2023;Miao et al., 2024].The general idea is that using the LLM to inspect and verify its own reasoning can show improvements, though in some domains self-critiquing has also shown diminished performance [Valmeekam et al., 2023].Our approach of verification is different: instead of asking the LLM to verify the abstract chain of reasoning, we only ask it to generate concrete examples of the general constraints in the problem.The task of verification is then done with the solver to formally check that the examples are consistent with the abstract formalization.Thus apart from not relying purely on the LLM for verification, we also avoid the more complex task of verifying an abstract chain of reasoning which can itself be highly error-prone.We show how this approach provides a very high precision verification, as opposed to just relative improvements in accuracy.</p>
<p>Conclusion</p>
<p>We have presented the Semantic Self-Verification approach, which infers strong problem formalizations based on concrete instantiations, using a consistency-based verification paradigm that leverages LLMs and logical solvers.Beyond achieving state-of-the-art accuracy, SSV introduces a novel verification feature that has near-perfect empirical precision.As the reasoning power of LLMs continues to advance, such near-certain verification can serve as a complementary dimension to general accuracy gains in order to ensure confidence on arbitrarily complex tasks.</p>
<p>[ Weng et al., 2023]   pre conditions = [] t = Const('t', technicians sort) pre conditions.append(ForAll([t],Sum ([repairs(t, m) for m in machines]) &gt;= 1))</p>
<p>NewConstraint: Xena and exactly three other technicians repair radios.NewConstraintCode: t = Const('t', technicians sort) pre conditions.append(And(repairs(Xena,radios), Sum([And(t != Xena, repairs(t, radios)) for t in technicians]) == 3)) ------</p>
<p>Options Code Generation Prompt</p>
<p>Given a problem with multiple answer options and an existing z3 program that models the problem, please provide the z3 code that checks each option and prints the correct answer.For each option, first create the check property for the option by substituting the option values appropriately in the question statement, as well as a full comment describing what the check property is stating.Then use only the following custom functions (is unsat(), is sat() and is valid()) to check if the check property is unsatisfiable, satisfiable or valid (depending on the question).Please structure the code with comments exactly as shown in the few shot examples below.Please provide only the options code and its comments in the output ( ('meals', ['breakfast', 'lunch', 'dinner', 'snack']) foods sort, (fish, hot cakes, macaroni, omelet, poached eggs) = EnumSort('foods', ['fish', 'hot cakes', 'macaroni', 'omelet', 'poached</p>
<p>Instantiation generation prompt</p>
<p>Given a problem scenario, some Z3 initialization code that defines the data structures, and a list of constraints, please provide positive and negative examples for each constraint.Each positive example should have a description and an expression of concrete assignments that satisfy the constraint, while each negative example should have a description and an expression of concrete assignments that contradict the constraint.If a constraint or its examples cannot be expressed by the given data structures or definitions, then please state "NONE" for the example description and "pass" for the assignments code.Please provide the completion to the prompt in exactly the same format as the example given below.</p>
<p>------&gt;&gt;&gt; Scenario:</p>
<p>Semantic repair prompt</p>
<p>We are given a scenario description, some initial z3 code that sets up basic definitions, a constraint in natural language, and a code snippet that implements that constraint.We are also given some code that should implement a positive example to the constraint, which should be satisfiable under that constraint, but it is not.First, please provide an analysis that investigates what may be the problem in either the initial code, the constraint code or the example.Then, based on this analysis, please repair the relevant code segments (initial code, constraint code, or example code) so that the positive example becomes satisfiable (state 'NONE' if no repair is required to a code segment).If multiple segments are incorrect due to a general formulation problem, then please reformulate the whole solution approach in the initial code and produce appropriate code for all segments.A couple of sample cases are shown below for illustration.Please produce output in exactly the format shown in these samples, and do not use any other markdown formatting.</p>
<p>------Scenario: On Tuesday Vladimir and Wendy each eat exactly four separate meals: breakfast, lunch, dinner, and a snack.InitialCode: from z3 import * people sort, (Vladimir, Wendy) = EnumSort('people', ['Vladimir', 'Wendy']) meals sort, (breakfast, lunch, dinner, snack) = EnumSort('meals', ['breakfast', 'lunch', 'dinner', 'snack']) foods sort, (fish, hot cakes, macaroni, omelet, poached eggs) = EnumSort('foods', ['fish', 'hot cakes', 'macaroni', 'omelet', 'poached  The scenario describes foods that Vladimir and Wendy eat at various meals during the day.The initial code defines the main data structures and the eats function which indicates the food each person eats on every meal.The constraint requires that there is at least one meal where they both eat the same food.The constraint code asserts that for all meals, the food that Vladimir eats is different from what Wendy eats.But this contradicts the intended constraint.The positive example code states that at breakfast, both Vladimir and Wendy eat fish, and this is consistent with the requirements of the constraint.Hence there is no issue in the initial code and the example code, but the constraint code wrongly implements the constraint.It should be repaired to assert that for some meal, both Vladimir and Wendy eat the same food.</p>
<p>types of machines.Which one of the following pairs of technicians could repair all and only the same types of machines as each other?</p>
<p>Figure 1 :
1
Figure 1: Sample problem from the Law School Admissions Test</p>
<p>Figure 2 :
2
Figure 2: Sample problem formalization as Z3 code</p>
<p>Figure 3 :
3
Figure 3: Towards near-perfect reasoning: SSV achieves new SoTA accuracy and 100% verification precision on the AR-LSAT law school tests dataset (all systems using GPT-4 as base LLM).</p>
<p>Figure 4: Semantic self-verification of a general constraint: the negative example fails for the wrong formalization (a), while both instantiations are verified for the correct formalization (b)</p>
<p>Figure 6 :
6
Figure 6: Repair attempts and temperature variations on AR-LSAT</p>
<p>pre conditions = [] # CONSTRAINT: At no meal does Vladimir eat the same kind of food as Wendy.m = Const('m', meals sort) pre conditions.append(ForAll([m],eats(Vladimir, m) != eats(Wendy, m))) NewConstraint: Neither of them eats the same kind of food more than once during the day.NewConstraintCode: m = Const('m', meals sort) p = Const('p', people sort) f = Const('f', foods sort) pre conditions.append(ForAll([p,f], Sum([eats(p, m) == f for m in meals]) &lt;= 1)) ------ExistingProgram: # In a repair facility there are exactly six technicians: Stacy, Urma, Wim, Xena, Yolanda, and Zane.Each technician repairs machines of at least one of the following three types|radios, televisions, and VCRs|and no other types.from z3 import * technicians sort, (Stacy, Urma, Wim, Xena, Yolanda, Zane) = EnumSort('technicians', ['Stacy', 'Urma', 'Wim', 'Xena', 'Yolanda', 'Zane']) machines sort, (radios, televisions, VCRs) = EnumSort('machines', ['radios', 'televisions', 'VCRs']) technicians = [Stacy, Urma, Wim, Xena, Yolanda, Zane] machines = [radios, televisions, VCRs] repairs = Function('repairs', technicians sort, machines sort, BoolSort())</p>
<p>pre conditions = [] # CONSTRAINT: At no meal does Vladimir eat the same kind of food as Wendy.m = Const('m', meals sort) pre conditions.append(ForAll([m],eats(Vladimir, m) != eats(Wendy, m)))# CONSTRAINT: Neither of them eats the same kind of food more than once during the day.m = Const('m', meals sort) p = Const('p', people sort) f = Const('f', foods sort) pre conditions.append(ForAll([p,f], Sum([eats(p, m)  == f for m in meals]TYPE: question says "cannot" so will check for validity using is valid() to ensure that the negated statement is true in all possible models.#OPTIONA: # CHECK PROPERTY: Vladimir cannot eat which one of the following foods?ANSWER: fish.m = Const('m', meals sort) check property = ForAll([m], eats(Vladimir, m) != fish) if is valid(check property): print('(A)') # OPTION B: # CHECK PROPERTY: Vladimir cannot eat which one of the following foods?ANSWER: hot cakes.m = Const('m', meals sort) check property = ForAll([m], eats(Vladimir, m) != hot cakes) if is valid(check property): print('(B)') # OPTION C: # CHECK PROPERTY: Vladimir cannot eat which one of the following foods?ANSWER: macaroni.m = Const('m', meals sort) check property = ForAll([m], eats(Vladimir, m) != macaroni) if is valid(check property): print('(C)') # OPTION D: # CHECK PROPERTY: Vladimir cannot eat which one of the following foods?ANSWER: omelet.m = Const('m', meals sort) check property = ForAll([m], eats(Vladimir, m) != omelet) if is valid(check property): print('(D)') # OPTION E: # CHECK PROPERTY: Vladimir cannot eat which one of the following foods?ANSWER: poached eggs.m = Const('m', meals sort) check property = ForAll([m], eats(Vladimir, m) != poached eggs) if is valid(check property): print('(E)') ------&gt;&gt;&gt; Problem: In a repair facility there are exactly six technicians: Stacy, Urma, Wim, Xena, Yolanda, and Zane.</p>
<p>eggs']) people = [Vladimir, Wendy] meals = [breakfast, lunch, dinner, snack] foods = [fish, hot cakes, macaroni, omelet, poached eggs] eats = Function('eats', people sort, meals sort, foods sort) pre conditions = [] # CONSTRAINT: At no meal does Vladimir eat the same kind of food as Wendy.m = Const('m', meals sort) pre conditions.append(ForAll([m],eats(Vladimir, m) != eats(Wendy, m))) ------</p>
<p>eggs']) people = [Vladimir, Wendy] meals = [breakfast, lunch, dinner, snack] foods = [fish, hot cakes, macaroni, omelet, poached eggs] eats = Function('eats', people sort, meals sort, foods sort) pre conditions = [] ConstraintDescription: At some meal Vladimir eats the same kind of food as Wendy.ConstraintCode: m = Const('m', meals sort) pre conditions.append(ForAll([m],eats(Vladimir, m) != eats(Wendy, m))) PositiveExampleCode: And(eats(Vladimir, breakfast) == fish, eats(Wendy, breakfast) == fish) ProblemDiscussion:</p>
<p>('m', meals sort) pre conditions.append(Exists([m],eats(Vladimir, m) == eats(Wendy, m))) RepairedPositiveExampleCode: NONE ------Scenario: In a repair facility there are exactly six technicians: Stacy, Urma, Wim, Xena, Yolanda, and Zane.Each technician repairs machines of at least one of the following three types|radios, televisions, and VCRs|and no other types.InitialCode: from z3 import * technicians sort, (Stacy, Urma, Wim, Xena, Yolanda, Zane) = EnumSort('technicians', ['Stacy', 'Urma', 'Wim', 'Xena', 'Yolanda', 'Zane']) machines sort, (radios, televisions,</p>
<p>the question Require: LLM // the language model Require: Solver // the logical solver Require: Temperatures // LLM temperatures to try Require: MaxRepairs // maximum repair attempts 1: A best ← ∅
6:if A best = ∅ then7:A best ← A8:end if9:12:return (A, True)13:end if14:if A = ∅ then15:16:end if17:end while18: end for19: if A best = ∅ then20:
2: for each T ∈ Temperatures do 3: P ← GenProgram(LLM, T, Solver, Q) 4: while P ̸ = ∅ and under MaxRepairs do 5: A ← ExecuteProgram(Solver, P ) I ← GenInstantiations(LLM, T, P ) 10: I fail ← Verify(Solver, I, P ) 11: if I fail = ∅ and IsWellFormed(P ) then P ← RepairProgram(LLM, T, Q, P, I fail )</p>
<p>Table 1 :
1
[Han et al., 2022]nd SSV precision/coverage with GPT-4 base model.Values in brackets are actual values on corrected datasets.usedlogicalreasoningdataset[Tafjordetal., 2021].We use its open-world assumption subset with 5-hop reasoning tasks, following[Pan et al., 2023], with 600 test examples and 3 answer options (True/False/Unknown). FOLIO is an expertcrafted dataset for logical reasoning[Han et al., 2022], featuring real-world knowledge problems phrased in natural language and requiring complex first-order logic.We evaluate on its full test set of 204 examples, each with 3 answer options
DatasetGeneral AccuracySSV VerificationStandardCoTLogic-LMSSVCoveragePrecisionAR-LSAT33.335.143.071.321.794.0 (100.0)FOLIO69.170.678.980.925.098.0 (100.0)LogicalDeduction71.375.387.689.743.7100.0PrOntoQA77.498.883.2100.066.0100.0ProofWriter52.768.179.798.075.298.7 (100.0)
[Pan et al., 2023;Liang et al., 2023] a dataset from the BigBench benchmark[Srivastava et al., 2023]involving object sequence ordering based on given conditions.The full test set contains 300 tasks with 3, 5, or 7 answer options.AR-LSAT consists of analytical reasoning questions fromLSAT exams from 1991-2016 [Zhong et al., 2022].This challenging dataset has seen only marginally better-thanrandom accuracy from existing approaches[Pan et al., 2023;Liang et al., 2023].The test set has 230 questions, each with 5 answer options.</p>
<p>Table 2 :
2
General accuracy and SSV precision/coverage with GPT-3.5 base model.Values in brackets are actual values on corrected datasets.
DatasetGeneral AccuracySSV VerificationStandardCoTLogic-LMSSVCoveragePrecisionAR-LSAT20.317.326.428.30-FOLIO45.157.462.759.31.5100.0LogicalDeduction40.042.365.748.30-PrOntoQA47.467.861.072.84.295.2ProofWriter35.549.258.372.516.294.8 (95.9)</p>
<p>[Yu et al., 2024]un Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.Large language models are better reasoners with self-verification.In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP (Findings), pages 2550-2575.Association for ComputationalLinguistics, 2023.[Wuetal., 2022]Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus N. Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy.Autoformalization with large language models.In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, NeurIPS, 2022.[Xieetal., 2023] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie.Selfevaluation guided beam search for reasoning.In Thirtyseventh Conference on Neural Information Processing Systems, 2023.[Yangetal., 2022] Kaiyu Yang, Jia Deng, and Danqi Chen.Generating natural language proofs with verifier-guided search.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, EMNLP, pages 89-105.Association for Computational Linguistics, 2022.[Yeetal., 2023] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett.SatLM: Satisfiability-aided language models using declarative prompting.In Thirty-seventh Conference on Neural Information Processing Systems, 2023.[Yuetal., 2024]Junchi Yu, Ran He, and Zhitao Ying.The bald eagle eats the cow.The bald eagle is red.The bald eagle needs the cow.The bear needs the rabbit.The cow is kind.The cow is red.The cow needs the bald eagle.The rabbit eats the bear.The rabbit eats the cow.The rabbit sees the cow.If something needs the bald eagle then it needs the rabbit.If the bald eagle is nice and the bald eagle is young then the bald eagle sees the cow.If the rabbit needs the cow then the cow sees the rabbit.If something eats the cow and the cow is nice then it needs the bald eagle.If something needs the rabbit then it is nice.If something sees the rabbit then it is red.If something needs the bald eagle then it eats the bald eagle.
given and only constraints are given, six technicians: Stacy, Urma, Wim,If something needs the rabbit then it is people sort, (Vladimir, Wendy) =then just state "None" for initial Xena, Yolanda, and Zane. Eachnice. EnumSort('people', ['Vladimir',context. Some examples are given below. technician repairs machines of at least### 'Wendy'])------one of the following three types-radios,If something sees the rabbit then it is meals sort, (breakfast, lunch, dinner,Problem: televisions, and VCRs-and no otherred. snack) = EnumSort('meals', ['breakfast',types. The following conditions apply:### 'lunch', 'dinner', 'snack'])Xena and exactly three other techniciansIf something needs the bald eagle then foods sort, (fish, hot cakes, macaroni,repair radios. Yolanda repairs bothit eats the bald eagle. omelet, poached eggs) = EnumSort('foods',televisions and VCRs. Stacy does not------['fish', 'hot cakes', 'macaroni',repair any type of machine that YolandaProblem: 'omelet', 'poached eggs'])repairs. Zane repairs more types ofOn Tuesday Vladimir and Wendy each eat people = [Vladimir, Wendy]machines than Yolanda repairs. Wimexactly four separate meals: breakfast, meals = [breakfast, lunch, dinner,does not repair any type of machine thatlunch, dinner, and a snack. The snack]Stacy repairs. Urma repairs exactly twofollowing is all that is known about foods = [fish, hot cakes, macaroni,types of machines.what they eat during that day: At no omelet, poached eggs]InitialContext:meal does Vladimir eat the same kind eats = Function('eats', people sort,In a repair facility there are exactlyof food as Wendy. Neither of them eats meals sort, foods sort)six technicians: Stacy, Urma, Wim,the same kind of food more than onceXena, Yolanda, and Zane. Eachduring the day. For breakfast, eachtechnician repairs machines of at leasteats exactly one of the following: hotone of the following three types-radios,cakes, poached eggs, or omelet. Fortelevisions, and VCRs-and no otherlunch, each eats exactly one of thetypes.following: fish, hot cakes, macaroni,InitialContext: Constraints:or omelet. For dinner, each eatsNone Xena and exactly three other techniciansexactly one of the following: fish,Constraints: repair radios.hot cakes, macaroni, or omelet. ForThe bald eagle eats the cow. ###a snack, each eats exactly one of the### Yolanda repairs both televisions andfollowing: fish or omelet. Wendy eatsThe bald eagle is red. VCRs.an omelet for lunch.### ###Thought propagation: an analogical approach to complex InitialContext:The bald eagle needs the cow. Stacy does not repair any type ofreasoning with large language models. In ICLR. OpenRe-On Tuesday Vladimir and Wendy each eat### machine that Yolanda repairs.view.net, 2024. exactly four separate meals: breakfast,The bear needs the rabbit. ### ### Zane repairs more types of machines thanlunch, dinner, and a snack. [Zhong et al., 2022] Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Constraints:The cow is kind. Yolanda repairs.Jian Yin, Ming Zhou, and Nan Duan. Analytical reason-At no meal does Vladimir eat the same### ###ing of text. In Marine Carpuat, Marie-Catherine de Marn-kind of food as Wendy.The cow is red. Wim does not repair any type of machineeffe, and Ivan Vladimir Meza Ruiz, editors, Findings of the ###### that Stacy repairs.Association for Computational Linguistics: NAACL 2022, Neither of them eats the same kind ofThe cow needs the bald eagle. ###pages 2306-2319, Seattle, United States, July 2022. Asso-food more than once during the day.### Urma repairs exactly two types ofciation for Computational Linguistics. ###The rabbit eats the bear. machines. ### ------The rabbit eats the cow. ### The rabbit sees the cow. Incremental Code Generation Prompt ### Given a z3 program that models aFor breakfast, each eats exactly one of [Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, the following: hot cakes, poached eggs, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-or omelet. mans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. ### Chi. Least-to-most prompting enables complex reasoning For lunch, each eats exactly one of the in large language models. In ICLR. OpenReview.net, 2023. following: fish, hot cakes, macaroni,If something needs the bald eagle then particular problem and a new constraint[Zhou, 2012] Zhi-Hua Zhou. Ensemble Methods: Founda-or omelet.it needs the rabbit. described in natural language, pleasetions and Algorithms. Chapman &amp; Hall/CRC, 1st edition, ###### provide the z3 code to augment the2012. For dinner, each eats exactly one of theIf the bald eagle is nice and the bald program with the new constraint. Pleasefollowing: fish, hot cakes, macaroni,eagle is young then the bald eagle sees provide only the z3 program code in the7 Appendix or omelet.the cow. output and no other markdown formatting ### or explanatory text. If the rabbit needs the cow then the cow ------### 7.1 Compositional Code Generation and For a snack, each eats exactly one of Refinement Prompts the following: fish or omelet.sees the rabbit. ExistingProgram:Problem Decomposition Prompt ###### # On Tuesday Vladimir and Wendy each eatGiven a problem description, please Wendy eats an omelet for lunch.If something eats the cow and the cow is exactly four separate meals: breakfast,decompose it into an initial context ------nice then it needs the bald eagle. lunch, dinner, and a snack.and a list of independent constraints. Problem:### from z3 import *If there is no explicit initial context In a repair facility there are exactly
Acknowledgments.The first author is grateful for the discussions with his daughter to help with her middle school studies, which provided the inspiration for this work.Code incorrect, Example correct.The code only enforces the constraint for batch numbers 1,2 and 3, but does not restrict any other batch numbers from being created on a day.The negative example uses batch number 4 which is not prevented by the code.Code incorrect, Example correct.The code does not implement any constraint to require exactly 3 batches of each kind of cookie in a week.Analysis:Code incorrect, Example correct.The code does not implement any constraint that enforces that exactly 3 batches of each type of cookie should be made in the week.None &gt;&gt;&gt; InitializationCode: from z3 import * creature sort = DeclareSort('creature') Stella = Const('Stella', creature sort) Jay = Const('Jay', creature sort) is tumpus = Function('is tumpus', creature sort, BoolSort()) is rompus = Function('is rompus', creature sort, BoolSort()) is numpus = Function('is numpus', creature sort, BoolSort()) is yumpus = Function('is yumpus', creature sort, BoolSort()) is zumpus = Function('is zumpus', creature sort, BoolSort()) is impus = Function('is impus', creature sort, BoolSort()) is dumpus = Function('is dumpus', creature sort, BoolSort()) is vumpus = Function('is vumpus', creature sort, BoolSort()) is jompus = Function('is jompus', creature sort, BoolSort()) is wumpus = Function('is wumpus', creature sort, BoolSort()) is angry = Function('is angry', creature sort, BoolSort()) is bright = Function('is bright', creature sort, BoolSort()) is luminous = Function('is luminous', creature sort, BoolSort()) is transparent = Function('is transparent', creature sort, BoolSort()) is bitter = Function('is bitter', creature sort, BoolSort()) is red = Function('is red', creature sort, BoolSort()) is happy = Function('is happy', creature sort, BoolSort()) is large = Function('is large', creature sort, BoolSort()) pre conditions = [] &gt;&gt;&gt; Constraints: Each dumpus is a vumpus.### Vumpuses are bright.### Every vumpus is a zumpus.### Zumpuses are not luminous.&gt;&gt;&gt; ConstraintExamples: Constraint: Each dumpus is a vumpus.PositiveExampleDescription: Stella is a dumpus and is also a vumpus.PositiveExampleCode: And(is dumpus(Stella) == True, is vumpus(Stella) == True) NegativeExampleDescription: Stella is a dumpus but is not a vumpus.NegativeExampleCode: And(is dumpus(Stella) == True, is vumpus(Stella) == False) Constraint: Vumpuses are bright.PositiveExampleDescription: Jay is a vumpus and is bright.PositiveExampleCode: And(is vumpus(Jay) == True, is bright(Jay) == True) NegativeExampleDescription: Jay is a vumpus and is not bright.NegativeExampleCode: And(is vumpus(Jay) == True, is bright(Jay) == False) Constraint: Every vumpus is a zumpus.PositiveExampleDescription: Jay is a vumpus and a zumpus.PositiveExampleCode: And(is vumpus(Jay) == True, is zumpus(Jay) == True) NegativeExampleDescription: Jay is a vumpus but not a zumpus.NegativeExampleCode: And(is vumpus(Jay) == True, is zumpus(Jay) == False) Constraint: Zumpuses are not luminous.PositiveExampleDescription: Stella is a zumpus and is not luminous.PositiveExampleCode: And(is zumpus(Stella) == True, is luminous(Stella) == False) NegativeExampleDescription: Stella is a zumpus and is luminous.NegativeExampleCode: And(is zumpus(Stella) == True, is luminous(Stella) == True) ------&gt;&gt;&gt; Scenario: On Tuesday Vladimir and Wendy each eat exactly two separate meals: breakfast and dinner.&gt;&gt;&gt; InitializationCode: from z3 import * people sort, (Vladimir, Wendy) = EnumSort('people', ['Vladimir', 'Wendy']) meals sort, (breakfast, dinner) = EnumSort('meals', ['breakfast', 'dinner']) foods sort, (fish, hot cakes, macaroni, omelet, poached eggs) = EnumSort('foods',['fish', 'hot cakes', 'macaroni', 'omelet', 'poached eggs']) people = [Vladimir, Wendy] meals =[breakfast, dinner]foods = [fish, hot cakes, macaroni, omelet, poached eggs] eats = Function('eats', people sort, meals sort, foods sort) pre conditions = [] &gt;&gt;&gt; Constraints: At no meal does Vladimir eat the same kind of food as Wendy.### Neither of them eats the same kind of food more than once during the day.### For breakfast, each eats hot cakes.&gt;&gt;&gt; ConstraintExamples: Constraint: At no meal does Vladimir eat the same kind of food as Wendy.PositiveExampleDescription: Vladimir and Wendy eat different foods at each meal: Vladimir has fish for breakfast while Wendy has hot cakes, and for dinner, Vladimir eats macaroni while Wendy has omelet.PositiveExampleCode: And(eats(Vladimir, breakfast) == fish, eats(Wendy, breakfast) == hot cakes, eats(Vladimir, dinner) == macaroni, eats(Wendy, dinner) == omelet) NegativeExampleDescription: At dinner, both Vladimir and Wendy eat the same food, macaroni.NegativeExampleCode: And(eats(Vladimir, dinner) == macaroni, eats(Wendy, dinner) == macaroni) Constraint: Neither of them eats the same kind of food more than once during the day.PositiveExampleDescription: Vladimir eats different foods for breakfast and dinner: fish for breakfast and hot cakes for dinner.Wendy also eats different foods for both meals: hot cakes for breakfast and omelet for dinner.PositiveExampleCode: And(eats(Vladimir, breakfast) == fish, eats(Vladimir, dinner) == hot cakes, eats(Wendy, breakfast) == hot cakes, eats(Wendy, dinner) == omelet) NegativeExampleDescription: Vladimir eats fish for both breakfast and dinner.NegativeExampleCode: And(eats(Vladimir, breakfast) == fish, eats(Vladimir, dinner)Sum([repairs(t, m)for m in machines]) &gt;= 1)) RepairedConstraintCode: NONE RepairedPositiveExampleCode: NONE ------Dataset correction casesWe found a small number of cases in three of the datasets where the answers have been labelled incorrectly.Our SSV system (with GPT-4 base model) detected these cases in its verification, and we describe the corrections that should be made to the datasets below.AR-LSAT CorrectionsThree cases in the AR-LSAT dataset were verified correctly by our system, but were labelled with the wrong answers in the dataset.These three cases are ar lsat 201612 3-G 2 6 (correct answer should be D but incorrectly labelled C), ar lsat 201612 3-G 1 4 (correct answer should be E but incorrectly labelled A) and ar lsat 201612 3-G 2 8 (correct answer should be B but is incorrectly labelled A).For all three of these cases, we were able to check the reasoning and also that the answers in the original source LSAT Test (https://img.cracklsat.net/lsat/pt/pt80.pdf) are consistent with the answers that were generated by our system.Hence we submit that these are errors in the AR-LSAT dataset collection process.FOLIO CorrectionsIn the FOLIO dataset, we found one case that was correctly verified by our system, but we find is labelled with the wrong answer in the dataset.This is case FOLIO dev 27:All aliens are extraterrestrial.If someone is from Mars, then they are aliens.No extraterrestrial is human.Everyone from Earth is a human.Marvin cannot be from Earth and from Mars.If Marvin is not from Earth, then Marvin is an extraterrestrial.Based on the above information, is the following statement true, false, or uncertain?Marvin is an alien.We submit that the correct answer is C (unknown) but it is labelled B (false) in the dataset.Reasoning: If Marvin is from Earth, he is not an alien.If Marvin is not from Earth: If he is from Mars, he is an alien, otherwise, we cannot be certain he is an alien.Hence both outcomes are possible.We suspect the error in the dataset may stem from an incorrect formalization of the problem in the original FOLIO dataset source:https://github.com/Yale-LILY/FOLIO/blob/main/data/v0.0/folio-validation.txt.In this source we see that the constraint "Marvin cannot be from Earth and from Mars" is incorrectly formalized as ¬F romEarth(marvin) ∧ ¬F romM ars(marvin) in first order logic, which asserts that Marvin is neither from Earth nor from Mars.ProofWriter correctionsIn the ProofWriter dataset, we found 6 cases that were correctly verified by our system, but we find are labelled with the wrong answer in the dataset.In all 6 cases, the answers in the dataset have been labelled as unknown when they can be proven to be either true or false as we show below.ProofWriter RelNeg-OWA-D5-450 Q22 (Correct answer should be B (false), but labelled C (unknown)).The bald eagle chases the lion.The bald eagle is not green.The bald eagle is round.The bald eagle likes the lion.The dog is red.The lion does not chase the dog.The lion is round.The lion is not young.The rabbit chases the dog.The rabbit eats the lion.If something chases the dog then it likes the rabbit.If something is red and it chases the lion then the lion likes the bald eagle.If something is big then it chases the rabbit.If something is round and it chases the bald eagle then the bald eagle does not like the dog.If something likes the lion then it is red.If something is red and round then it does not chase the bald eagle.If something is red and young then it chases the bald eagle.If something likes the bald eagle and the bald eagle chases the lion then it likes the lion.If something eats the bald eagle then the bald eagle is red.Based on the above information, is the following statement true, false, or unknown?The bald eagle is young.Reasoning: From Fact 4 and Rule 5:The bald eagle likes the lion.Therefore, the bald eagle is red.From Fact 3: The bald eagle is round.Applying Rule 6 to the bald eagle:The bald eagle is red and round.Therefore, the bald eagle does not chase itself.Assuming the bald eagle is young:The bald eagle is red and young.Applying Rule 7 to the bald eagle:The bald eagle is red and young.Therefore, the bald eagle chases itself.Contradiction:From step 3, the bald eagle does not chase itself.From step 5, the bald eagle chases itself.This is a contradiction.Conclusion:The assumption that the bald eagle is young leads to a contradiction.Therefore, the bald eagle cannot be young.ProofWriter AttNeg-OWA-D5-471 Q14(Correct answer should be A (true), but labelled C (unknown)).Anne is white.Reasoning: Gary is not white.(rule 1) Nice things are kind.(rule 2) Kind things are white.(rule 3) If Gary were nice, then by rule 2, he would also be kind.If Gary is kind, then by rule 3, he must be white.However, rule 1 tells us that Gary is not white.This creates a contradiction because Gary cannot be both not white and white at the same time.Given that Gary is not white, he cannot be kind, and therefore, he cannot be nice.Thus, the statement "Gary is nice" is false.ProofWriter AttNeg-OWA-D5-850 Q14 (Correct answer should be B (false), but labelled C (unknown)).Anne is red.Anne is smart.Bob is kind.Bob is not nice.Fiona is furry.Fiona is rough.Gary is not green.Gary is kind.Gary is nice.Gary is rough.If someone is nice then they are red.Smart people are green.If someone is smart and red then they are not kind.All rough, green people are nice.Green people are rough.If someone is red and green then they are rough.If someone is furry and green then they are smart.All rough, furry people are smart.Furry, rough people are smart.Based on the above information, is the following statement true, false, or unknown?Bob is smart.Reasoning: Bob is kind.Bob is not nice.Rule: Smart people are green.So, if Bob were smart, he would be green.Rule: Green people are rough.Therefore, if Bob were green (and thus rough), we can use the next rule.Rule: All rough, green people are nice.If Bob were rough and green, he would be nice, but we know Bob is not nice.Conclusion: Bob cannot be green because it would contradict the fact that he is not nice.Since Bob is not green, and smart people are green, Bob cannot be smart.ProofWriter AttNeg-OWA-D5-219 Q13 (Correct answer should be A (true), but labelled C (unknown)).Charlie is not quiet.Dave is big.Dave is furry.Erin is cold.Erin is not green.Erin is not kind.Fiona is quiet.Big things are young.Young, cold things are big.Quiet things are big.All young things are cold.If something is big and not furry then it is cold.If something is cold then it is not kind.If something is cold and big then it is quiet.If Fiona is cold and Fiona is not quiet then Fiona is kind.If something is quiet and not kind then it is green.Based on the above information, is the following statement true, false, or unknown?Charlie is not big.Reasoning: Charlie is not quiet.Assume for contradiction that Charlie is big.Big things are young: Therefore, Charlie is young.All young things are cold: Therefore, Charlie is cold.If something is cold, then it is not kind: Therefore, Charlie is not kind.If something is cold and big, then it is quiet: Therefore, Charlie is quiet.This contradicts the given fact that Charlie is not quiet.Therefore, Charlie is not big.ProofWriter AttNeg-OWA-D5-94 Q18 (Correct answer should be B (false), but labelled C (unknown))Bob is smart.Charlie is kind.Charlie is not smart.Fiona is blue.Fiona is rough.Fiona is smart.Gary is kind.All cold, quiet people are smart.If someone is cold then they are smart.If someone is red and kind then they are smart.If someone is quiet then they are blue.If someone is blue then they are quiet.If someone is kind then they are rough.If Gary is kind and Gary is rough then Gary is quiet.All blue, smart people are red.Blue, rough people are red.Based on the above information, is the following statement true, false, or unknown?Charlie is blue.Reasoning: Charlie is kind.If someone is kind, then they are rough Therefore, Charlie is rough.Assume for contradiction that Charlie is blue.Blue, rough people are red.Since Charlie is both blue (assumed) and rough, Charlie must be red.If someone is red and kind, then they are smart.Since Charlie is red (from step 4) and kind (from step 1), Charlie must be smart.However, it's given that Charlie is not smart (from the context).Hence, we have a contradiction.Therefore, Charlie is not blue.Analysis of Verification Failure CasesWe conducted a manual analysis over a sample of 30 cases where SSV verification failed.Here is a summary of the failure reasons: • code incorrect, example correct: 16 (53.3%)• code incorrect, example incorrect: 7 (23.3%)• code correct, example incorrect: 3 (10%)• program not well-formed: 4 (13.3%)We see that in most cases the code is incorrect as opposed to examples, which can be expected as examples inference is generally simpler than abstract translation.Below is the detailed analysis of the reasons for the verification failure for specific cases.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.InitialCode: PosExample:And(features_kitten(day2, Siamese) == True, features_kitten(day3, Manx) == True)PosExampleDescription:Siamese kittens are featured on day 2 and Manx kittens are featured on day 3.Analysis:Code incorrect, Example correct.The condition code does not implement consecutiveness constraint but instead requires each breed to only be featured on one day, which is not a requirement and causes the positive example to fail.The positive example correctly presents a valid instantiation of the problem.InitialContext:The organizer of a reading club will select at least five and at most six works from a group of nine works.The group consists of three French novels, three Russian novels, two French plays, and one Russian play.InitialCode: The organizer selects all three French novels, all three Russian novels, both French plays, and the Russian play, which exceeds the maximum of six works.Analysis:Code incorrect, Example correct.The negative example correctly violates the constraint of not exceeding 6 works, but the code does not implement this constraint.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly NegExampleDescription:Four batches of oatmeal cookies are made, which contradicts the constraint that exactly three batches of each kind of cookie are made each week.Analysis:Code incorrect, Example correct.The negative example correctly violates the constraint by enforcing 4 batches oatmeal cookies to be made in the week.InitialContext:An administrator must assign parking spaces to six new employees: Robertson, Souza, Togowa, Vaughn, Xu, and Young.Each of the six employees must be assigned one of the following parking spaces: #1, #2, #3, #4, #5, or #6.No two employees can be assigned the same parking space.InitialCode: Condition:If Togowa is assigned a higher-numbered parking space than Souza, then Togowa is assigned parking space #3.ConditionCode: additional_constraint = parking_space(Togowa) &gt; parking_space(Souza) check_property = And(additional_constraint, parking_space(Togowa) == 3) pre_conditions.append(check_property)ExampleFail:Positive example not inferred (NONE description) Analysis:Code incorrect, Example incorrect.The code does not implement the conditional requirement but only a conjunction.A positive example could not be inferred from the LLM or valid solver code implementing the positive example.InitialContext:A panel of five scientists will be formed.The panelists will be selected from among three botanists|F, G, and H|three chemists|K, L, and M|and three zoologists|P, Q, and R. InitialCode:Positive example contains undeclared free variables Analysis:Code incorrect, Example incorrect.Code is correct but does not implement a constraint that exactly 5 scientists must be selected.A valid positive example in solver code could not be generated.InitialContext:At a concert, exactly eight compositions|F, H, L, O, P, R, S, and T|are to be performed exactly once each, consecutively and one composition at a time.InitialCode: T is performed with two compositions between it and F. Analysis:Code correct, Example incorrect.The negative example only requires that T is 3 positions before F, which is permitted by the constraint as long as T is immediately after R (since it was a disjunction).InitialContext:A government needs to assign new ambassadors to Venezuela, Yemen, and Zambia.The candidates for these ambassadorships are Jaramillo, Kayne, Landon, Novetzke, and Ong.One ambassador will be assigned to each country, and no ambassador will be assigned to more than one country.InitialCode: NegExampleDescription:George gives two reports, one on Monday morning and another on Tuesday morning, which contradicts the constraint that each student gives only one report.Analysis:Code incorrect, Example correct.It does not enforce that a student cannot give multiple reports, so it will not be possible for exactly 6 reports to be given by exactly 6 students as intended.The example correctly violates the constraint by enforcing George to give two reports in different slots.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.InitialCode: And(made_on(oatmeal, 1) != Wednesday, made_on(peanut_butter, 1) != Wednesday, made_on(sugar, 1) != Wednesday, made_on(oatmeal, 1) == Tuesday, made_on(peanut_butter, 2) == Tuesday, made_on(sugar, 3) != Tuesday) NegExampleDescription:No cookies are made on Wednesday, but only two batches of cookies are made on Tuesday.Analysis:Code incorrect, Example incorrect.The code directly enforces that exactly three batches are made on Tuesday, regardless of whether any batches are made on Wednesday so it lacks the conditional aspect "if no batch is made on Wednesday" of the intended constraint.The negative example prevents only batch 3 of sugar cookies on tuesday but not other batches, so it is still possible to have three batches on tuesday.InitialContext:An administrator must assign parking spaces to six new employees: Robertson, Souza, Togowa, Vaughn, Xu, and Young.Each of the six employees must be assigned one of the following parking spaces: #1, #2, #3, #4, #5, or #6.No two employees can be assigned the same parking space.InitialCode: NegExampleDescription:Four batches of oatmeal cookies are made on Monday, which contradicts the constraint that exactly three batches of each kind of cookie are made each week.Analysis:Code incorrect, Example correct.The code does not implement any constraints that exactly 3 batches of each kind of cookie should be made in a week.InitialContext:A government needs to assign new ambassadors to Venezuela, Yemen, and Zambia.The candidates for these ambassadorships are Jaramillo, Kayne, Landon, Novetzke, and Ong.One ambassador will be assigned to each country, and no ambassador will be assigned to more than one country.InitialCode:from z3 import * ambassadors_sort, (Jaramillo, Kayne, Landon, Novetzke, Ong) = EnumSort('ambassadors', ['Jaramillo', 'Kayne', 'Landon', 'Novetzke', 'Ong']) countries_sort, (Venezuela, Yemen, Zambia) = EnumSort('countries', ['Venezuela', 'Yemen', 'Zambia']) ambassadors = [Jaramillo, Kayne, Landon, Novetzke, Ong] countries = [Venezuela, Yemen, Zambia] assigned_to = Function('assigned_to', ambassadors_sort, countries_sort, BoolSort()Sum([If(assigned_to(a, c), 1, 0) for c in countries]) &lt;= 1))Condition:Either Kayne or Novetzke, but not both, is assigned to one of the ambassadorships.ConditionCode:pre_conditions.append(Xor(Or(assigned_to(Kayne,Venezuela), assigned_to(Kayne, Yemen), assigned_to(Kayne, Zambia)), Or(assigned_to(Novetzke, Venezuela), assigned_to(Novetzke, Yemen), assigned_to(Novetzke, Zambia))))ExampleFail: Positive example contains undeclared free variables Analysis: Code correct, Example incorrect.The example is not well formed with undeclared free variables, while the code correctly implements the constraint using exclusive OR.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.InitialCode: from z3 import * days_sort, (Monday, Tuesday, Wednesday, Thursday, Friday) = EnumSort('days', ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']) cookie_sort, (oatmeal, peanut_butter, sugar) = EnumSort('cookie', ['oatmeal', 'peanut_butter', 'sugar']) batches_sort = IntSort() make_batch = Function('make_batch', cookie_sort, batches_sort, days_sort, BoolSort()) pre_conditions = [] days = [Monday, Tuesday, Wednesday, Thursday, Friday] cookies = [oatmeal, peanut_butter, sugar] # Constraint: Exactly three batches of each kind of cookie are made each week for cookie in cookies: pre_conditions.append(Sum([If(make_batch(cooki i, d), 1, 0) for i in range(1, 4) for d in days]) == 3) NegExample: And(make_batch(oatmeal, 1, Monday), make_batch(oatmeal, 2, Tuesday), make_batch(oatmeal, 3, Wednesday), make_batch(oatmeal, 4, Thursday))NegExampleDescription:Four batches of oatmeal cookies are made on Monday, Tuesday, Wednesday, and Thursday (which contradicts the constraint that exactly three batches of each kind of cookie are made each week).Analysis:InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.InitialCode: And(make_batch(oatmeal, 1) == Monday, make_batch(oatmeal, 2) == Monday, make_batch(oatmeal, 3) == Monday, make_batch(oatmeal, 4) == Monday) NegExampleDescription:Four batches of oatmeal cookies are made on Monday, which contradicts the constraint that exactly three batches of each kind of cookie are made each week.Analysis:InitialContext:Of the eight students|George, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robert|in a seminar, exactly six will give individual oral reports during three consecutive days|Monday, Tuesday, and Wednesday.Exactly two reports will be given each day|one in the morning and one in the afternoon.InitialCode: for day in days: pre_conditions.append(Sum([If(report(s, day, morning), 1, 0) for s in students]) == 1) pre_conditions.append(Sum([If(report(s, day, afternoon), 1, 0) for s in students]) == 1) pre_conditions.append(Sum([If(Or(report(s, d, morning), report(s, d, afternoon)), 1, 0) for s in students for d in days]) == 6) Condition:Helen, Kyle, and Lenore, not necessarily in that order, give the three morning reports.ConditionCode: NegExample:And(report(Helen, Monday, morning) == True, report(Kyle, Tuesday, afternoon) == True) NegExampleDescription:Helen gives a report on Monday morning, but Kyle gives his report in the afternoon.Analysis:Code incorrect, Example correct.The code lacks a constraint that ensures each student gives at most one report during the entire period, so it allows Kyle to give a morning and an afternoon report.Since this should not be allowed, the negative example correctly violates the requirement that Kyle gives a morning report by requiring him to give an afternoon report on Tuesday.InitialContext:A loading dock consists of exactly six bays numbered 1 through 6 consecutively from one side of the dock to the other.Condition: The bay holding livestock has a higher number than the bay holding textiles.ConditionCode: b1 = Const('b1', bays_sort) b2 = Const('b2', bays_sort) pre_conditions.append(Exists([b1],And(holding(b1) == livestock, b1 &gt; 0, ForAll([b2],Implies(holding(b2) == textiles, b1 &gt; b2))))) NegExample: And(holding(5) == textiles, holding(1) == livestock)NegExampleDescription: Textiles are in bay 5 and livestock is in bay 1.Analysis: Code incorrect, Example correct.The variables b1 and b2 in the constraint code representing bay numbers are not properly constrained to be within the valid range of bay numbers (1 through 6).This allows the solver to consider bay numbers outside this range, which leads to the negative example not violating the constraint as expected.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.InitialCode:from z3 import * days_sort, (Monday, Tuesday, Wednesday, Thursday, Friday) = EnumSort('days', ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']) cookie_sort, (oatmeal, peanut_butter, sugar) = EnumSort('cookie', ['oatmeal', 'peanut_butter', 'sugar']) batch_sort = IntSort() made_on = Function('made_on', cookie_sort, batch_sort, days_sort)pre_conditions = [] for cookie in[oatmeal, peanut_butter, sugar]: pre_conditions.append(Distinct([made_on(cookie, i)for i in range(1, 4)]))Condition:Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.Positive example contains undeclared free variablesAnalysis:Code incorrect, Example incorrect.The code is incorrect as it only enforces the condition on batch numbers 1..3, but allows any int values for batch numbers, so more than 3 batches can potentially be made.The example is incorrect as it contains undeclared free variables.InitialContext:On each of exactly seven consecutive days (day 1 though day 7), a pet shop features exactly one of three breeds of kitten|Himalayan, Manx, Siamese|and exactly one of three breeds of puppy|Greyhound, Newfoundland, Rottweiler.InitialCode:days_sort,(day1,day2,day3,day4,day5,day6,day7) = EnumSort('days',['day1','day2','day3','day4','day5','day6','day7']) kitten_breeds_sort, (Himalayan, Manx, Siamese) = EnumSort('kitten_breeds',['Himalayan', 'Manx', 'Siamese']) puppy_breeds_sort, (Greyhound, Newfoundland, Rottweiler) = EnumSort('puppy_breeds', ['Greyhound', 'Newfoundland', 'Rottweiler']) days= [day1, day2, day3, day4, day5, day6, day7]Sum([If(featured_kitten(d)== breed, 1, 0) for breed in kitten_breeds]) == 1)) pre_conditions.append(ForAll([d], Sum([If(featured_puppy(d)== breed, 1, 0) for breed in puppy_breeds]) == 1))Condition: If Himalayans are not featured on day 7, then day 1 and day 3 CANNOT feature both the same breed of kitten and the same breed of puppy.ConditionCode: check_property = And(featured_kitten(day1) == featured_kitten(day3), featured_puppy(day1) == featured_puppy(day3), featured_kitten(day7) != Himalayan) pre_conditions.append(check_property)PosExample: And(featured_kitten(day7) != Himalayan, featured_kitten(day1) == Siamese, featured_puppy(day1) == Greyhound, featured_kitten(day3) == Manx, featured_puppy(day3) == Newfoundland)PosExampleDescription:Himalayans are not featured on day 7, and day 1 features a Siamese kitten and a Greyhound puppy while day 3 features a Manx kitten and a Newfoundland puppy.Analysis:Code incorrect, Example correct.The code implements the condition incorrectly by just enforcing a conjunction of constraints rather than the conditional requirement.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.pre_conditions.append(And([Not(Exists([i, j], And(i != j, made_on(cookie, i) == made_on(cookie, j)))) for day in [Monday, Tuesday, Wednesday, Thursday, Friday]]))PosExample:And Condition:No breed is featured on any two consecutive days.ConditionCode:# (Already implemented above)ExampleFail:Positive example not inferred (NONE description)Analysis:Code correct, Example incorrect.No example was inferred.The code correctly implements constraints to disallow the same breeds on any two consecutive days.InitialContext:An administrator must assign parking spaces to six new employees: Robertson, Souza, Togowa, Vaughn, Xu, and Young.Each of the six employees must be assigned one of the following parking spaces: #1, #2, #3, #4, #5, or #6.No two employees can be assigned the same parking space.InitialCode: Condition:If Young is assigned a higher-numbered parking space than Souza, then Young is assigned parking space #2.ConditionCode: check_property = parking_space(Young) == 2 pre_conditions.append(check_property)ExampleFail:Positive example not inferred (NONE description)Analysis:Code incorrect, Example incorrect.The code does not implement the conditional requirement but simply asserts that Young should be assigned #2.No positive example is inferred.InitialContext:A bakery makes exactly three kinds of cookie|oatmeal, peanut butter, and sugar.Exactly three batches of each kind of cookie are made each week (Monday through Friday) and each batch is made, from start to finish, on a single day.NegExample:And(made_on(oatmeal, 1) == Monday, made_on(oatmeal, 2) == Tuesday, made_on(oatmeal, 3) == Wednesday)NegExampleDescription:The oatmeal cookies are made on Monday, Tuesday, and Wednesday, which violates the constraint that each kind of cookie must be made on different days.Analysis:Code incorrect, Example incorrect.The negative example does not violate the intended constraints as it simply assigns oatmeal batches to 3 different days.The code does not prevent any batch numbers higher than 3.Runtime performance and optimizationsWe conducted an evaluation of the runtime performance of the current system.Executing the system over a sample of 250 data points (50 from each dataset), the median runtime per task is 152 seconds (around 2.5 minutes), with first quartile 108s, third quartile 267s and mean 249s.This was on an Intel Xeon Gold 6126 CPU @ 2.60 GHz with 16 cores and no hyper-threading, 62 GB of RAM, and an HDD-based storage system (this machine has slightly lower single-threaded performance than most modern desktops).However, there are also many potential optimizations to the SSV algorithm that can be made to significantly reduce the run time in a practical implementation:• The outer temperature loop (line 2 in Figure5) can be fully parallelized as all the computations are independent for each temperature.That can yield up to 4X speed up (with 4 temperatures being tried in our current system).Side note: even with a single temperature of 0, our algorithm still beats all baselines in terms of accuracy (as in our ablation study), so even such an ablated system would be beneficial if computation costs are of significant concern.• In the verification phase (line 9 in Figure5), the solver calls to verify each of the concrete instantiations can be parallelized as they are checked independently.These are around 10 to 20 independent solver calls on average (2 instantiations each for around 5-10 constraints) that can be parallelized for significant speedup.• Caching solver verification checks between repair attempts.Currently for each repair attempt in the inner loop (line 4 in Figure5), we perform the full verification on the repaired program (on all constraints).However, most of the time the repaired change is on a single constraint for which a failing instantiation was found and all other constraints remain identical (though not always guaranteed as in some rare cases the LLM may reformulate the whole program).Hence if we cache the solver requests for each instantiation verification, many of these repetitive checks can be avoided in the repaired programs for the constraints that are unaltered.As a general side note, recent reasoning-oriented models such as Open AI's o1 can take several seconds or up to a few minutes on some tasks with significantly more computational resources/GPUs, so higher runtimes in the order of a few minutes may generally be expected to robustly address complex reasoning problems.
Selection-inference: Exploiting large language models for interpretable logical reasoning. Chen , ICLR. OpenReview.net. Lecture Notes in Computer Science. C R Ramakrishnan, Jakob Rehof, Yokohama, JapanSpringer2024. 2024. 2020. 2020. July 2020. 2023. 2023. 2008. 20084963ijcai.orgICLR. OpenReview.net</p>
<p>The lean theorem prover (system description). De Moura, Leonardo Mendonc ¸a de Moura. Lecture Notes in Computer Science. Amy P Felty, Aart Middeldorp, Springer2015. 20159195</p>
<p>Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Guan, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024. 2024</p>
<p>Solving math word problems by combining language models with symbolic solvers. Han, CoRR, abs/2209.00840Yueya et al., 2023] Joy He-Yueya, Gabriel Poesia, Rose Wang, and Noah Goodman2022. 2022. 2023Folio: Natural language reasoning with first-order logic. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Jiang, CoRR, abs/2406.17663Forty-first International Conference on Machine Learning. Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter ClarkTushar2023. 2023. 2024. 2024. 2024. 2023ICLR. OpenReview.net. Khot et al., 2023. and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In ICLR. OpenReview.net</p>
<p>Holistic evaluation of language models. Liang, Trans. Mach. Learn. Res. 20232023. 2023</p>
<p>Deductive verification of chain-of-thought reasoning. Ling , NeurIPS. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, 2023. 2023</p>
<p>Self-refine: Iterative refinement with selffeedback. Madaan, Thirty-seventh Conference on Neural Information Processing Systems. 2023. 2023</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Miao, The Twelfth International Conference on Learning Representations. Aurelio Marc, Alina Ranzato, Yann N Beygelzimer, Percy Dauphin, Jennifer Wortman Liang, Vaughan, 2024. 2024. 2021. 2021NeurIPS</p>
<p>Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Pan, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. Lecture Notes in Computer Science. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USAAssociation for Computational Linguistics2023. 2023. 1994. 1994. 2024. 2023. 2023. 2 2023. 2023. 2023. 2023. 2021. 2022. 2023. 2023828Springer Berlin Heidelberg, Berlin, HeidelbergSharan Narang, Aakanksha Chowdhery. and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR. OpenReview.net</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Wei, NeurIPS. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, 2022. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
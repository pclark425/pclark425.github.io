<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2598 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2598</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2598</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-3e9af4289e4bbd3fb912a68ac8430fd2d9a8ab4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3e9af4289e4bbd3fb912a68ac8430fd2d9a8ab4d" target="_blank">AI Feynman: A physics-inspired method for symbolic regression</a></p>
                <p><strong>Paper Venue:</strong> Science Advances</p>
                <p><strong>Paper TL;DR:</strong> This work develops a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques and improves the state-of-the-art success rate.</p>
                <p><strong>Paper Abstract:</strong> Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2598.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2598.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman (Udrescu & Tegmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired automated symbolic-regression system that combines neural-network interpolation with dimensional analysis, polynomial fitting, brute-force symbolic search and tests for symmetry/separability to discover closed-form analytic formulas from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Recursive, multidimensional symbolic-regression algorithm that (1) applies automated dimensional analysis to reduce variables, (2) trains a high-capacity feed-forward neural network (6 hidden layers, softplus activations; training on up to 100,000 sampled points using Adam) to obtain an accurate interpolant, (3) uses the neural net to detect translational/scaling/rotational symmetries, additive/multiplicative separability and to test variable-equality reductions, (4) applies low-order polynomial fitting and (constrained) brute-force symbolic enumeration/search on simplified subproblems, and (5) recursively launches the whole pipeline on generated subproblems until closed-form expressions are found. The system also applies a suite of simple algebraic transformations (log, sqrt, inverse, trig, etc.) to dependent/independent variables before brute-force search and uses description-length regularization for discrete-search model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / Automated Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Physics symbolic regression (analytic formula discovery from numeric data); tested broadly across classical mechanics, electromagnetism and quantum mechanics formulas drawn from the Feynman Lectures and other canonical physics sources.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given tables of (x1,...,xn,y) sampled from unknown analytic physics formulas, discover an exact symbolic expression y = f(x1,...,xn). Tasks varied from single-variable expressions to 9-variable formulas including rational functions, roots, exponentials, logs and trigonometric functions, sometimes containing multiplicative prefactors (physical constants) and dimensionful units.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High combinatorial search complexity (exponential in symbolic string length). Problems spanned 1–9 input variables, compositional expressions combining many elementary functions, and large discrete search spaces for brute-force search; some subproblems required exploring very long symbol strings (search spaces described qualitatively as astronomically large; specific brute-force cost estimates given for some failures — e.g., ~2 years or ~100× age of universe for certain reverse-Polish strings under the implemented brute-force settings). Neural-net-faced problems required up to 10^6 data points for sufficiently accurate interpolation in the hardest cases.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Synthetic data generated by authors: typically 100,000 random samples per mystery (sampled uniformly in variable ranges, often [1,5]), split 80/20 train/validation for NN. Minimal successful data requirements vary: many equations solvable with only 10 data points via polynomial/brute-force modules, some required 10^2–10^6 points when NN-based symmetry/separability detection was needed. Noise experiments added Gaussian noise to y with relative std epsilon; most equations recovered exactly for epsilon ≤ 10^-4, nearly half up to epsilon=10^-2; NN-based cases required lower noise.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Authors allowed up to 2 CPU-hours per mystery in comparisons; reported per-equation solution times (tables) range from seconds to thousands of seconds (examples: some solutions took ~5,975 s; many took <100 s). NN training used large batches (batch size 2048), 100 epochs, and significant memory for networks; brute-force symbolic search can be extremely expensive (estimated astronomical runtimes for long symbol strings). When rerun without dimensional analysis, AI Feynman still solved 93% but relied heavily on NN modules, implying higher computational cost. Eureqa baseline runs used 4 CPUs; AI Feynman timings reported are wall-time on authors' hardware (per-table times).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined supervised regression problems (continuous, deterministic mapping from inputs to outputs) with clear exact-solution evaluation (algebraic equivalence via symbolic simplification). Continuous-valued outputs with possible added Gaussian noise; high domain knowledge helpful (units) and exploited via automated dimensional analysis. Evaluation metric is exact symbolic equivalence (zero after algebraic simplify) and/or r.m.s. fitting error thresholds for intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Primary success metric: whether the discovered symbolic expression is algebraically identical to the ground-truth formula (i.e., f_found - f_true simplifies to 0). Secondary metrics: r.m.s. fitting error thresholds used during search; description-length regularization for model selection; dataset-specific minimal data needed and noise tolerance thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>On the testbed of 100 equations from the Feynman Lectures, AI Feynman discovered 100/100 (100%). On a held-out bonus set of 20 harder equations it discovered 18/20 (90%). When the dimensional-analysis module was removed, it still solved 93% of the 100 equations. Many individual equations have specific data-size and noise thresholds: NN-requiring problems typically needed 10^2–10^6 samples and tolerated noise ≤10^-4 for exact recovery; almost half of all equations remained solvable with noise up to 10^-2.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Failures were due to brute-force discrete search explosion and missed simplifications by the neural net. Two bonus mysteries failed: one reducible to y = -(32 a^2 (1+a)) / (5 b^2) required detecting separability with a fifth power in denominator that induced large dynamic range; NN missed separability due to tolerance thresholds and brute-force required impractically long symbol strings (~2 years estimated). Another failure (Jackson 2.11) reduced to an expression containing a 4π factor that expanded into long symbol strings and would have required ~100× age-of-universe for brute-force under current symbol set. Other limitations: imperfect NN fitting introduces effective noise that can block subsequent exact symbolic discovery; brute-force symbol alphabet choices affect solvability/time; algorithm currently does not numerically optimize symbolic parameters (unlike some competitors).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors: automated dimensional analysis (units) drastically reduces variable count; NN interpolation that reliably detects symmetries and separability enabling recursive problem decomposition; ability to apply simple algebraic transformations (log, inverse, sqrt) before brute-force; combining continuous optimization (NN/linear polynomial fits) with discrete brute-force search and description-length regularization; physics-inspired priors (expectation of low-order polynomials, separability, compositionality).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Direct comparison on the same benchmark: AI Feynman solved 100% of 100 Feynman equations vs Eureqa's 71%; on 20 bonus equations AI Feynman solved 90% vs Eureqa's 15%. AI Feynman shows largest relative improvements on the most complex problems where NN-based symmetry/separability detection repeatedly reduces dimensionality; many simpler formulas were solvable by both systems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No quantitative human-baseline performance reported. Historically motivated anecdote: Kepler required ~40 failed attempts and 4 years to discover an ellipse; no direct human-vs-system timing/accuracy comparisons are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI Feynman: A physics-inspired method for symbolic regression', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2598.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2598.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureqa (Nutonian) - Genetic-programming symbolic regression software</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial symbolic-regression software implementing a genetic-programming approach to evolve symbolic expressions from data; used here as the state-of-the-art baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A genetic-programming-based symbolic-regression engine that evolves populations of symbolic expressions (trees/strings) using mutation, crossover and selection guided by fitness (prediction error) and parsimony metrics; can include numeric constants and a user-specified function set. In this paper Eureqa was run as the primary baseline for comparison, with a limited alphabet of functions (e.g., +,-,*,/,sqrt,exp,log,sin,cos and occasionally arcsin/arccos), on the same Feynman database.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Genetic-programming symbolic regression (Automated Discovery / AutoML)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression for physics formula discovery (same Feynman and bonus equation datasets used here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover closed-form symbolic expressions from numeric data tables for the 100 Feynman equations and 20 bonus equations. Eureqa was used with 300 data points and allowed up to 2 CPU-hours per mystery (runs used 4 CPUs as stated).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Faced same combinatorial symbolic search complexity; used evolutionary search to navigate large discrete search space but can get trapped in local optima when the correct formula is not progressively approximated by shorter symbolic forms. Problems included multi-variable equations, non-linear functions, and combinations where components are not individually useful as partial fits (e.g., when combined by division), which is challenging for purely evolutionary approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>In Eureqa runs for this paper, authors provided 300 data points per mystery (smaller than AI Feynman's NN training data) because additional data did not significantly help Eureqa's performance. Function set was restricted per-problem to speed search.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Each Eureqa run used 4 CPUs and up to 2 hours CPU time per mystery in the comparison. Despite parallelism, Eureqa solved fewer hard problems within the time budget; its evolutionary search can require many candidate-evaluations to escape local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Same supervised regression setting; stochastic search (evolutionary) over discrete symbolic structures without explicit dimensional-analysis or neural-network symmetry detection modules. Evaluation uses prediction error/fitness and parsimony trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Whether evolved formula algebraically equals ground-truth expression (same algebraic-simplify criterion), and intermediate fitness metrics (r.m.s. error) used for evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>On the 100 Feynman equations Eureqa solved 71% (as reported). On the 20 bonus equations Eureqa solved 15%. For many simpler equations Eureqa succeeded, but for the harder physics-style problems requiring variable-eliminating transformations/evidence of separability it underperformed compared to AI Feynman.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tends to get stuck in local optima when evolutionary search converges to fairly accurate but structurally wrong formulas; has difficulty when correct formula components are not individually useful fits (e.g., when formula is a ratio of components), and lacks automated physics priors such as dimensional analysis and NN-guided reductions. Performance depends strongly on the chosen function alphabet and population/operation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Genetic search can explore broad classes of symbolic forms, does not require differentiable components, and can be robust to noise; it also inherently searches for Pareto-frontier of accuracy vs complexity, which can help in noisy contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to AI Feynman on same datasets and time budget: Eureqa solved 71% vs AI Feynman's 100% on basic set, and 15% vs 90% on bonus set; Eureqa used far fewer data points (300) and no NN-based simplification, explaining part of the gap on complex cases.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No direct human performance numbers provided in the paper for the same benchmark; historically, genetic-programming approaches have been used to rediscover physics laws but no quantitative human-comparison is given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI Feynman: A physics-inspired method for symbolic regression', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2598.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2598.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic algorithms / Genetic programming (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic algorithms / Genetic programming methods for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of biology-inspired stochastic optimization methods that evolve candidate symbolic programs/formulas via mutation, crossover, selection and fitness evaluation; widely used for symbolic regression and referenced as prior art in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic algorithms / Genetic programming</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>General class of algorithms that perform population-based search over discrete program/formula representations using biologically inspired operators (mutation, crossover, selection) guided by a fitness function (e.g., prediction error and complexity). Implementations include Eureqa and other GP toolkits; they can optionally include numerical constant optimization and custom function sets.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery / Evolutionary Search</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression, optimization, program synthesis, and diverse applications cited historically (antenna design, vehicle design, routing, robot navigation, PDE discovery etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Used historically for combinatorial/discrete search problems including symbolic regression (discovering analytic formulas), where the search space is exponential in expression length and direct enumeration is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Addresses combinatorial/discrete optimization problems; complexity grows exponentially with expression size and function alphabet; GP relies on population diversity and good genetic operators to make progress, but can get trapped in suboptimal basins especially when parts of the true expression are not useful partial fits.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>GP methods operate on provided datasets; they do not require large datasets for training neural nets but may need sufficiently many data points to evaluate fitness reliably and avoid overfitting. In this paper GP (Eureqa) was run with 300 points per mystery.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Computational cost proportional to number of individuals × generations × cost-per-evaluation; can be parallelized across individuals but may require many evaluations to find complex formulas, which led to timeouts/failures under the 2-hour-per-mystery budget in the experiments for very difficult formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Discrete, stochastic, black-box search without explicit use of domain-specific constraints like units; evaluation metric is typically prediction error with parsimony penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Fitness/accuracy (r.m.s. error) and parsimony (complexity) leading to final algebraic equivalence check when comparing to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Varies by implementation and problem; in this paper the leading GP implementation (Eureqa) solved 71% of the 100 Feynman equations and 15% of the 20 bonus equations under the experimental conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Local optima trapping, poor performance when true formula components are useless as partial fits (e.g., when combined via ratios), sensitivity to function alphabet and hyperparameters, and potential inefficiency on very long expression strings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Good choice of function alphabet, appropriate population dynamics, ability to numerically optimize constants (when implemented), and the provision of sufficient but not excessive data to avoid overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>GP methods perform well on many simpler symbolic-regression tasks but underperform compared to methods that can exploit domain structure (units/dimensional analysis) and NN-based simplification on the hardest physics problems in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in the paper for GP specifically; GP historically has been used to re-discover known formulas but no direct human-vs-GP quantitative comparisons are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI Feynman: A physics-inspired method for symbolic regression', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2598.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2598.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse regression / SINDy-style methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse regression methods for discovering governing equations (e.g., SINDy and related approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Regression techniques that assume the governing equation is a sparse linear combination of candidate basis functions and solve for sparse coefficients (L0/L1-type regularization) to discover governing equations from data; cited as an alternative prior approach to symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sparse regression (sparse identification of functions)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach builds a library of candidate functions (polynomials, trigonometric terms, etc.) and fits the target as a sparse linear combination, typically using L1-regularization or sequential thresholding; effective when the true expression lies in the span of the library and is low-dimensional/sparse in that representation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery / Model-selection via sparse regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Discovery of governing equations and symbolic models from data, especially differential-equation identification and cases where the true model is sparse in a chosen basis.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Can recover governing relations by solving linear regression with sparsity constraints; best when candidate library is appropriate and the model is linear in chosen basis functions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complexity depends on library size and candidate function set; if the true model involves functions outside the library or non-polynomial compositions, sparse regression may fail.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires enough data to stably estimate coefficients; noise and derivative estimation (for PDE/ODE discovery) can be challenging and often requires denoising or smoothing.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Solving large sparse optimization problems can be economical relative to brute-force symbolic search, but derivative estimation and library evaluation can be costly for high-dimensional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Assumes linear-in-parameters structure in chosen library (continuous, deterministic); clear evaluation metrics based on coefficient recovery and predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Recovery of correct sparse coefficient vector and predictive accuracy; algebraic equivalence when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantitatively evaluated in this paper; cited as one of several existing approaches and related work. Authors note sparse-regression approaches have been applied successfully in prior work but are not the main baseline in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails if true functions are not in the candidate library (e.g., nested/compositional functions not expressible as sparse linear combination of library terms) or when noisy derivative estimates degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate choice of candidate library and regularization strength; low intrinsic dimensionality and sparsity of the true model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI Feynman: A physics-inspired method for symbolic regression', 'publication_date_yy_mm': '2019-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 2)</em></li>
                <li>Discovering governing equations from data: Sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Genetic programming: on the programming of computers by means of natural selection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2598",
    "paper_id": "paper-3e9af4289e4bbd3fb912a68ac8430fd2d9a8ab4d",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "AI Feynman",
            "name_full": "AI Feynman (Udrescu & Tegmark)",
            "brief_description": "A physics-inspired automated symbolic-regression system that combines neural-network interpolation with dimensional analysis, polynomial fitting, brute-force symbolic search and tests for symmetry/separability to discover closed-form analytic formulas from data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI Feynman",
            "system_description": "Recursive, multidimensional symbolic-regression algorithm that (1) applies automated dimensional analysis to reduce variables, (2) trains a high-capacity feed-forward neural network (6 hidden layers, softplus activations; training on up to 100,000 sampled points using Adam) to obtain an accurate interpolant, (3) uses the neural net to detect translational/scaling/rotational symmetries, additive/multiplicative separability and to test variable-equality reductions, (4) applies low-order polynomial fitting and (constrained) brute-force symbolic enumeration/search on simplified subproblems, and (5) recursively launches the whole pipeline on generated subproblems until closed-form expressions are found. The system also applies a suite of simple algebraic transformations (log, sqrt, inverse, trig, etc.) to dependent/independent variables before brute-force search and uses description-length regularization for discrete-search model selection.",
            "system_type": "Automated Discovery System / Automated Symbolic Regression",
            "problem_domain": "Physics symbolic regression (analytic formula discovery from numeric data); tested broadly across classical mechanics, electromagnetism and quantum mechanics formulas drawn from the Feynman Lectures and other canonical physics sources.",
            "problem_description": "Given tables of (x1,...,xn,y) sampled from unknown analytic physics formulas, discover an exact symbolic expression y = f(x1,...,xn). Tasks varied from single-variable expressions to 9-variable formulas including rational functions, roots, exponentials, logs and trigonometric functions, sometimes containing multiplicative prefactors (physical constants) and dimensionful units.",
            "problem_complexity": "High combinatorial search complexity (exponential in symbolic string length). Problems spanned 1–9 input variables, compositional expressions combining many elementary functions, and large discrete search spaces for brute-force search; some subproblems required exploring very long symbol strings (search spaces described qualitatively as astronomically large; specific brute-force cost estimates given for some failures — e.g., ~2 years or ~100× age of universe for certain reverse-Polish strings under the implemented brute-force settings). Neural-net-faced problems required up to 10^6 data points for sufficiently accurate interpolation in the hardest cases.",
            "data_availability": "Synthetic data generated by authors: typically 100,000 random samples per mystery (sampled uniformly in variable ranges, often [1,5]), split 80/20 train/validation for NN. Minimal successful data requirements vary: many equations solvable with only 10 data points via polynomial/brute-force modules, some required 10^2–10^6 points when NN-based symmetry/separability detection was needed. Noise experiments added Gaussian noise to y with relative std epsilon; most equations recovered exactly for epsilon ≤ 10^-4, nearly half up to epsilon=10^-2; NN-based cases required lower noise.",
            "computational_requirements": "Authors allowed up to 2 CPU-hours per mystery in comparisons; reported per-equation solution times (tables) range from seconds to thousands of seconds (examples: some solutions took ~5,975 s; many took &lt;100 s). NN training used large batches (batch size 2048), 100 epochs, and significant memory for networks; brute-force symbolic search can be extremely expensive (estimated astronomical runtimes for long symbol strings). When rerun without dimensional analysis, AI Feynman still solved 93% but relied heavily on NN modules, implying higher computational cost. Eureqa baseline runs used 4 CPUs; AI Feynman timings reported are wall-time on authors' hardware (per-table times).",
            "problem_structure": "Well-defined supervised regression problems (continuous, deterministic mapping from inputs to outputs) with clear exact-solution evaluation (algebraic equivalence via symbolic simplification). Continuous-valued outputs with possible added Gaussian noise; high domain knowledge helpful (units) and exploited via automated dimensional analysis. Evaluation metric is exact symbolic equivalence (zero after algebraic simplify) and/or r.m.s. fitting error thresholds for intermediate steps.",
            "success_metric": "Primary success metric: whether the discovered symbolic expression is algebraically identical to the ground-truth formula (i.e., f_found - f_true simplifies to 0). Secondary metrics: r.m.s. fitting error thresholds used during search; description-length regularization for model selection; dataset-specific minimal data needed and noise tolerance thresholds.",
            "success_rate": "On the testbed of 100 equations from the Feynman Lectures, AI Feynman discovered 100/100 (100%). On a held-out bonus set of 20 harder equations it discovered 18/20 (90%). When the dimensional-analysis module was removed, it still solved 93% of the 100 equations. Many individual equations have specific data-size and noise thresholds: NN-requiring problems typically needed 10^2–10^6 samples and tolerated noise ≤10^-4 for exact recovery; almost half of all equations remained solvable with noise up to 10^-2.",
            "failure_modes": "Failures were due to brute-force discrete search explosion and missed simplifications by the neural net. Two bonus mysteries failed: one reducible to y = -(32 a^2 (1+a)) / (5 b^2) required detecting separability with a fifth power in denominator that induced large dynamic range; NN missed separability due to tolerance thresholds and brute-force required impractically long symbol strings (~2 years estimated). Another failure (Jackson 2.11) reduced to an expression containing a 4π factor that expanded into long symbol strings and would have required ~100× age-of-universe for brute-force under current symbol set. Other limitations: imperfect NN fitting introduces effective noise that can block subsequent exact symbolic discovery; brute-force symbol alphabet choices affect solvability/time; algorithm currently does not numerically optimize symbolic parameters (unlike some competitors).",
            "success_factors": "Key contributors: automated dimensional analysis (units) drastically reduces variable count; NN interpolation that reliably detects symmetries and separability enabling recursive problem decomposition; ability to apply simple algebraic transformations (log, inverse, sqrt) before brute-force; combining continuous optimization (NN/linear polynomial fits) with discrete brute-force search and description-length regularization; physics-inspired priors (expectation of low-order polynomials, separability, compositionality).",
            "comparative_results": "Direct comparison on the same benchmark: AI Feynman solved 100% of 100 Feynman equations vs Eureqa's 71%; on 20 bonus equations AI Feynman solved 90% vs Eureqa's 15%. AI Feynman shows largest relative improvements on the most complex problems where NN-based symmetry/separability detection repeatedly reduces dimensionality; many simpler formulas were solvable by both systems.",
            "human_baseline": "No quantitative human-baseline performance reported. Historically motivated anecdote: Kepler required ~40 failed attempts and 4 years to discover an ellipse; no direct human-vs-system timing/accuracy comparisons are provided in the paper.",
            "uuid": "e2598.0",
            "source_info": {
                "paper_title": "AI Feynman: A physics-inspired method for symbolic regression",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Eureqa",
            "name_full": "Eureqa (Nutonian) - Genetic-programming symbolic regression software",
            "brief_description": "Commercial symbolic-regression software implementing a genetic-programming approach to evolve symbolic expressions from data; used here as the state-of-the-art baseline for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Eureqa",
            "system_description": "A genetic-programming-based symbolic-regression engine that evolves populations of symbolic expressions (trees/strings) using mutation, crossover and selection guided by fitness (prediction error) and parsimony metrics; can include numeric constants and a user-specified function set. In this paper Eureqa was run as the primary baseline for comparison, with a limited alphabet of functions (e.g., +,-,*,/,sqrt,exp,log,sin,cos and occasionally arcsin/arccos), on the same Feynman database.",
            "system_type": "Genetic-programming symbolic regression (Automated Discovery / AutoML)",
            "problem_domain": "Symbolic regression for physics formula discovery (same Feynman and bonus equation datasets used here).",
            "problem_description": "Discover closed-form symbolic expressions from numeric data tables for the 100 Feynman equations and 20 bonus equations. Eureqa was used with 300 data points and allowed up to 2 CPU-hours per mystery (runs used 4 CPUs as stated).",
            "problem_complexity": "Faced same combinatorial symbolic search complexity; used evolutionary search to navigate large discrete search space but can get trapped in local optima when the correct formula is not progressively approximated by shorter symbolic forms. Problems included multi-variable equations, non-linear functions, and combinations where components are not individually useful as partial fits (e.g., when combined by division), which is challenging for purely evolutionary approaches.",
            "data_availability": "In Eureqa runs for this paper, authors provided 300 data points per mystery (smaller than AI Feynman's NN training data) because additional data did not significantly help Eureqa's performance. Function set was restricted per-problem to speed search.",
            "computational_requirements": "Each Eureqa run used 4 CPUs and up to 2 hours CPU time per mystery in the comparison. Despite parallelism, Eureqa solved fewer hard problems within the time budget; its evolutionary search can require many candidate-evaluations to escape local optima.",
            "problem_structure": "Same supervised regression setting; stochastic search (evolutionary) over discrete symbolic structures without explicit dimensional-analysis or neural-network symmetry detection modules. Evaluation uses prediction error/fitness and parsimony trade-offs.",
            "success_metric": "Whether evolved formula algebraically equals ground-truth expression (same algebraic-simplify criterion), and intermediate fitness metrics (r.m.s. error) used for evolution.",
            "success_rate": "On the 100 Feynman equations Eureqa solved 71% (as reported). On the 20 bonus equations Eureqa solved 15%. For many simpler equations Eureqa succeeded, but for the harder physics-style problems requiring variable-eliminating transformations/evidence of separability it underperformed compared to AI Feynman.",
            "failure_modes": "Tends to get stuck in local optima when evolutionary search converges to fairly accurate but structurally wrong formulas; has difficulty when correct formula components are not individually useful fits (e.g., when formula is a ratio of components), and lacks automated physics priors such as dimensional analysis and NN-guided reductions. Performance depends strongly on the chosen function alphabet and population/operation settings.",
            "success_factors": "Genetic search can explore broad classes of symbolic forms, does not require differentiable components, and can be robust to noise; it also inherently searches for Pareto-frontier of accuracy vs complexity, which can help in noisy contexts.",
            "comparative_results": "Compared to AI Feynman on same datasets and time budget: Eureqa solved 71% vs AI Feynman's 100% on basic set, and 15% vs 90% on bonus set; Eureqa used far fewer data points (300) and no NN-based simplification, explaining part of the gap on complex cases.",
            "human_baseline": "No direct human performance numbers provided in the paper for the same benchmark; historically, genetic-programming approaches have been used to rediscover physics laws but no quantitative human-comparison is given here.",
            "uuid": "e2598.1",
            "source_info": {
                "paper_title": "AI Feynman: A physics-inspired method for symbolic regression",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Genetic algorithms / Genetic programming (GP)",
            "name_full": "Genetic algorithms / Genetic programming methods for symbolic regression",
            "brief_description": "A class of biology-inspired stochastic optimization methods that evolve candidate symbolic programs/formulas via mutation, crossover, selection and fitness evaluation; widely used for symbolic regression and referenced as prior art in the paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Genetic algorithms / Genetic programming",
            "system_description": "General class of algorithms that perform population-based search over discrete program/formula representations using biologically inspired operators (mutation, crossover, selection) guided by a fitness function (e.g., prediction error and complexity). Implementations include Eureqa and other GP toolkits; they can optionally include numerical constant optimization and custom function sets.",
            "system_type": "Automated Discovery / Evolutionary Search",
            "problem_domain": "Symbolic regression, optimization, program synthesis, and diverse applications cited historically (antenna design, vehicle design, routing, robot navigation, PDE discovery etc.).",
            "problem_description": "Used historically for combinatorial/discrete search problems including symbolic regression (discovering analytic formulas), where the search space is exponential in expression length and direct enumeration is infeasible.",
            "problem_complexity": "Addresses combinatorial/discrete optimization problems; complexity grows exponentially with expression size and function alphabet; GP relies on population diversity and good genetic operators to make progress, but can get trapped in suboptimal basins especially when parts of the true expression are not useful partial fits.",
            "data_availability": "GP methods operate on provided datasets; they do not require large datasets for training neural nets but may need sufficiently many data points to evaluate fitness reliably and avoid overfitting. In this paper GP (Eureqa) was run with 300 points per mystery.",
            "computational_requirements": "Computational cost proportional to number of individuals × generations × cost-per-evaluation; can be parallelized across individuals but may require many evaluations to find complex formulas, which led to timeouts/failures under the 2-hour-per-mystery budget in the experiments for very difficult formulas.",
            "problem_structure": "Discrete, stochastic, black-box search without explicit use of domain-specific constraints like units; evaluation metric is typically prediction error with parsimony penalty.",
            "success_metric": "Fitness/accuracy (r.m.s. error) and parsimony (complexity) leading to final algebraic equivalence check when comparing to ground truth.",
            "success_rate": "Varies by implementation and problem; in this paper the leading GP implementation (Eureqa) solved 71% of the 100 Feynman equations and 15% of the 20 bonus equations under the experimental conditions.",
            "failure_modes": "Local optima trapping, poor performance when true formula components are useless as partial fits (e.g., when combined via ratios), sensitivity to function alphabet and hyperparameters, and potential inefficiency on very long expression strings.",
            "success_factors": "Good choice of function alphabet, appropriate population dynamics, ability to numerically optimize constants (when implemented), and the provision of sufficient but not excessive data to avoid overfitting.",
            "comparative_results": "GP methods perform well on many simpler symbolic-regression tasks but underperform compared to methods that can exploit domain structure (units/dimensional analysis) and NN-based simplification on the hardest physics problems in this paper.",
            "human_baseline": "Not provided in the paper for GP specifically; GP historically has been used to re-discover known formulas but no direct human-vs-GP quantitative comparisons are reported here.",
            "uuid": "e2598.2",
            "source_info": {
                "paper_title": "AI Feynman: A physics-inspired method for symbolic regression",
                "publication_date_yy_mm": "2019-05"
            }
        },
        {
            "name_short": "Sparse regression / SINDy-style methods",
            "name_full": "Sparse regression methods for discovering governing equations (e.g., SINDy and related approaches)",
            "brief_description": "Regression techniques that assume the governing equation is a sparse linear combination of candidate basis functions and solve for sparse coefficients (L0/L1-type regularization) to discover governing equations from data; cited as an alternative prior approach to symbolic regression.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Sparse regression (sparse identification of functions)",
            "system_description": "Approach builds a library of candidate functions (polynomials, trigonometric terms, etc.) and fits the target as a sparse linear combination, typically using L1-regularization or sequential thresholding; effective when the true expression lies in the span of the library and is low-dimensional/sparse in that representation.",
            "system_type": "Automated Discovery / Model-selection via sparse regression",
            "problem_domain": "Discovery of governing equations and symbolic models from data, especially differential-equation identification and cases where the true model is sparse in a chosen basis.",
            "problem_description": "Can recover governing relations by solving linear regression with sparsity constraints; best when candidate library is appropriate and the model is linear in chosen basis functions.",
            "problem_complexity": "Complexity depends on library size and candidate function set; if the true model involves functions outside the library or non-polynomial compositions, sparse regression may fail.",
            "data_availability": "Requires enough data to stably estimate coefficients; noise and derivative estimation (for PDE/ODE discovery) can be challenging and often requires denoising or smoothing.",
            "computational_requirements": "Solving large sparse optimization problems can be economical relative to brute-force symbolic search, but derivative estimation and library evaluation can be costly for high-dimensional problems.",
            "problem_structure": "Assumes linear-in-parameters structure in chosen library (continuous, deterministic); clear evaluation metrics based on coefficient recovery and predictive accuracy.",
            "success_metric": "Recovery of correct sparse coefficient vector and predictive accuracy; algebraic equivalence when applicable.",
            "success_rate": "Not quantitatively evaluated in this paper; cited as one of several existing approaches and related work. Authors note sparse-regression approaches have been applied successfully in prior work but are not the main baseline in their experiments.",
            "failure_modes": "Fails if true functions are not in the candidate library (e.g., nested/compositional functions not expressible as sparse linear combination of library terms) or when noisy derivative estimates degrade performance.",
            "success_factors": "Appropriate choice of candidate library and regularization strength; low intrinsic dimensionality and sparsity of the true model.",
            "uuid": "e2598.3",
            "source_info": {
                "paper_title": "AI Feynman: A physics-inspired method for symbolic regression",
                "publication_date_yy_mm": "2019-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 2
        },
        {
            "paper_title": "Discovering governing equations from data: Sparse identification of nonlinear dynamical systems",
            "rating": 2
        },
        {
            "paper_title": "Genetic programming: on the programming of computers by means of natural selection",
            "rating": 1
        }
    ],
    "cost": 0.01911525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AI Feynman: a Physics-Inspired Method for Symbolic Regression</h1>
<p>Silviu-Marian Udrescu, Max Tegmark*<br>Dept. of Physics $\&amp;$ Center for Brains, Minds $\&amp;$ Machines, Massachusetts Institute of Technology, Cambridge, MA 02139; sudrescu@mit.edu and Theiss Research, La Jolla, CA 92037, USA</p>
<p>(Dated: Published in Science Advances, 6:eaay2631, April 15, 2020)</p>
<h4>Abstract</h4>
<p>A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state of the art success rate from $15 \%$ to $90 \%$.</p>
<h2>I. INTRODUCTION</h2>
<p>In 1601, Johannes Kepler got access to the world's best data tables on planetary orbits, and after 4 years and about 40 failed attempts to fit the Mars data to various ovoid shapes, he launched a scientific revolution by discovering that Mars' orbit was an ellipse [1]. This was an example of symbolic regression: discovering a symbolic expression that accurately matches a given data set. More specifically, we are given a table of numbers, whose rows are of the form $\left{x_{1}, \ldots, x_{n}, y\right}$ where $y=f\left(x_{1}, \ldots, x_{n}\right)$, and our task is to discover the correct symbolic expression for the unknown mystery function $f$, optionally including the complication of noise.</p>
<p>Growing data sets have motivated attempts to automate such regression tasks, with significant success. For the special case where the unknown function $f$ is a linear combination of known functions of $\left{x_{1}, \ldots, x_{n}\right}$, symbolic regression reduces to simply solving a system of linear equations. Linear regression (where $f$ is simply an affine function) is ubiquitous in the scientific literature, from finance to psychology. The case where $f$ is a linear combination of monomials in $\left{x_{1}, \ldots, x_{n}\right}$ corresponds to linear regression with interaction terms, and to polynomial fitting more generally. There are countless other examples of popular regression functions that are linear combinations of known functions, ranging from Fourier expansions to wavelet transforms. Despite these successes with special cases, the general symbolic regression problem remains unsolved, and it is easy to see why: If we encode functions as strings of symbols, then the number of such strings grows exponentially with string length, so if we simply test all strings by increasing length, it may take longer than the age of our universe until we get to the function we are looking for.</p>
<p>This combinatorial challenge of an exponentially large</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>search space characterizes many famous classes of problems, from codebreaking and Rubik's cube to the natural selection problem of finding those genetic codes that produce the most evolutionarily fit organisms. This has motivated genetic algorithms [2,3] for targeted searches in exponentially large spaces, which replace the abovementioned brute-force search by biology-inspired strategies of mutation, selection, inheritance and recombination; crudely speaking, the role of genes is played by useful symbol strings that may form part of the sought-after formula or program. Such algorithms have been successfully applied to areas ranging from design of antennas $[4,5]$ and vehicles [6] to wireless routing [7], vehicle routing [8], robot navigation [9], code breaking [10], discovering partial differential equations [11], investment strategy [12], marketing [13], classification [14], Rubik's cube [15], program synthesis [16] and metabolic networks [17].</p>
<p>The symbolic regression problem for mathematical functions (the focus of this paper) has been tackled with a variety of methods [18-20], including sparse regression [21-24] and genetic algorithms [25, 26]. By far the most successful of these is, as we will see in Section III, the genetic algorithm outlined in [27] and implemented in the commercial Eureqa software [26].</p>
<p>The purpose of this paper is to further improve on this state-of-the-art, using physics-inspired strategies enabled by neural networks. Our most important contribution is using neural networks to discover hidden simplicity such as symmetry or separability in the mystery data, which enables us to recursively break harder problems into simpler ones with fewer variables. The rest of this paper is organized as follows. In Section II, we present our algorithm and the six strategies that it recursively combines. In Section III, we present a test suite of regression mysteries and use it to test both Eureqa and our new algorithm, finding major improvements. In Section IV, we summarize our conclusions and discuss opportunities for further progress.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1: Schematic illustration of our AI Feynman algorithm. It is iterative as described in the text, with four of the steps capable of generating new mystery data sets that get sent to fresh instantiations of the the algorithm which may or may not return a solution.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2: Example: how our AI Feynman algorithm discovered mystery Equation 5. Given a mystery table with many examples of the gravitational force F together with the 9 independent variables G, m<sub>1</sub>, m<sub>2</sub>, x<sub>1</sub>, ..., z<sub>2</sub>, this table was recursively transformed into simpler ones until the correct equation was found. First dimensional analysis generated a table of 6 dimensionless independent variables a = m<sub>2</sub>/m<sub>1</sub>, ..., f = z<sub>1</sub>/x<sub>1</sub> and the dimensionless dependent variable F ≡ F + Gm<sub>1</sub><sup>2</sup>/x<sub>1</sub><sup>2</sup>. Then a neural network was trained to fit this function, which revealed two translational symmetries (each eliminating one variable, by defining g ≡ c − d and h ≡ e − f) as well as multiplicative separability, enabling the factorization F(a, b, g, h) = G(a)H(b, g, h), thus splitting the problem into two simpler ones. Both G and H then were solved by polynomial fitting, the latter after applying one of a series of simple transformations (in this case, inversion). For many other mysteries, the final step was instead solved using brute-force symbolic search as described in the text.</p>
<h2>II. METHODS</h2>
<p>Generic functions $f\left(x_{1}, \ldots, x_{n}\right)$ are extremely complicated and near-impossible for symbolic regression to discover. However, functions appearing in physics and many other scientific applications often have some of the following simplifying properties that make them easier to discover:</p>
<ol>
<li>Units: $f$ and the variables upon which it depends have known physical units</li>
<li>Low-order polynomial: $f$ (or part thereof) is a polynomial of low degree</li>
<li>Compositionality: $f$ is a composition of a small set of elementary functions, each typically taking no more than two arguments</li>
<li>Smoothness: $f$ is continuous and perhaps even analytic in its domain</li>
<li>Symmetry: $f$ exhibits translational, rotational or scaling symmetry with respect to some of its variables</li>
<li>Separability: $f$ can be written as a sum or product of two parts with no variables in common</li>
</ol>
<p>The question of why these properties are common remains controversial and not fully understood [28, 29]. However, as we will see below, this does not prevent us from discovering and exploiting these properties to facilitate symbolic regression.</p>
<p>Property (1) enables dimensional analysis, which often transforms the problem into a simpler one with fewer independent variables. Property (2) enables polynomial fitting, which quickly solves the problem by solving a system of linear equations to determine the polynomial coefficients. Property (3) enables $f$ to be represented as a parse tree with a small number of node types, sometimes enabling $f$ or a sub-expression to be found via a bruteforce search. Property (4) enables approximating $f$ using a feed forward neural network with a smooth activation function. Property (5) can be confirmed using said neural network and enables the problem to be transformed into a simpler one with one independent variable less (or even fewer for $n&gt;2$ rotational symmetry). Property (6) can be confirmed using said neural network and enables the independent variables to be partitioned into two disjoint sets, and the problem to be transformed into two simpler ones, each involving the variables from one of these sets.</p>
<h2>A. Overall Algorithm</h2>
<p>The overall algorithm ${ }^{1}$ is schematically illustrated in Figure 1. It consists of a series of modules that try to exploit each of the the above-mentioned properties. Like a human scientist, it tries many different strategies (modules) in turn, and if it cannot solve the full problem in one fell swoop, it tries to transform it and divide it into simpler pieces that can be tackled separately, recursively re-launching the full algorithm on each piece. Figure 2 illustrates an example of how a particular mystery data set (Newton's law of gravitation with 9 variables) is solved. Below we describe each of these algorithm modules in turn.</p>
<h2>B. Dimensional Analysis</h2>
<p>Our dimensional analysis module exploits the well-known fact that many problems in physics can be simplified by requiring the units of the two sides of an equation to match. This often transforms the problem into a simpler one with a smaller number of variables that are all dimensionless. In the best case scenario, the transformed problem involves solving for a function of zero variables, i.e., a constant. We automate dimensional analysis as follows.</p>
<p>Table III shows the physical units of all variables appearing in our 100 mysteries, expressed as products of the fundamental units (meter, second, kilogram, kelvin, volt) to various integer powers. We thus represent the units of each variable by a vector $\mathbf{u}$ of 5 integers as in the table. For a mystery of the form $y=f\left(x_{1}, \ldots, x_{n}\right)$, we define the matrix $\mathbf{M}$ whose $i^{\text {th }}$ column is the $\mathbf{u}$-vector corresponding to the variable $\mathbf{x}<em 1="1">{i}$, and define the vector $\mathbf{b}$ as the $\mathbf{u}$-vector corresponding to $y$. We now let the vector $\mathbf{p}$ be a solution to the equation $\mathbf{M p}=\mathbf{b}$ and the columns of the matrix $\mathbf{U}$ form a basis for the null space, so that $\mathbf{M U}=0$, and define a new mystery $y^{\prime}=f^{\prime}\left(x</em>\right)$ where}^{\prime}, \ldots, x_{n}^{\prime</p>
<p>$$
x_{i}^{\prime} \equiv \prod_{i=j}^{n} x_{j}^{U_{1 j}}, \quad y^{\prime} \equiv \frac{y}{y_{<em>}}, \quad y_{</em>} \equiv \prod_{i=1}^{n} x_{i}^{p_{i}}
$$</p>
<p>By construction, the new variables $x_{i}^{\prime}$ and $y^{\prime}$ are dimensionless, and the number $n^{\prime}$ of new variables is equal to the dimensionality of the null space. When $n^{\prime}&gt;0$, we have the freedom to choose any basis we want for the null space and also to replace $\mathbf{p}$ by a vector of the form $\mathbf{p}+\mathbf{U a}$ for any vector $a$; we use this freedom to set as many elements as possible in $\mathbf{p}$ and $\mathbf{U}$ equal to zero, i.e., to make</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the new variables depend on as few old variables as possible. This choice is useful because it typically results in the resulting powers of the dimensionless variables being integers, making the final expression much easier to find than when the powers are fractions or irrational numbers.</p>
<h2>C. Polynomial Fit</h2>
<p>Many functions $f\left(x_{1}, \ldots, x_{n}\right)$ in physics and other sciences either are low-order polynomials, e.g., the kinetic energy $K=\frac{m}{2}\left(v_{x}^{2}+v_{y}^{2}+v_{z}^{2}\right)$, or have parts that are, e.g., the denominator of the gravitational force $F=$ $\frac{G m_{1} m_{2}}{\left(x_{1}-x_{2}\right)^{2}+\left(y_{1}-y_{2}\right)^{2}+\left(z_{1}-z_{2}\right)^{2}}$. We therefore include a module that tests if a mystery can be solved by a low-order polynomial. Our method uses the standard method of solving a system of linear equations to find the best fit polynomial coefficients. It tries fitting the mystery data to polynomials of degree $0,1, \ldots, d_{\max }=4$ and declares success if the best fitting polynomial gives r.m.s. fitting error $\leq \varepsilon_{p}$ (we discuss the setting of this threshold below).</p>
<h2>D. Brute Force</h2>
<p>Our brute-force symbolic regression model simply tries all possible symbolic expressions within some class, in order of increasing complexity, terminating either when the maximum fitting error drops below a threshold $\epsilon_{p}$ or after a maximum runtime $t_{\max }$ has been exceeded. Although this module alone could solve all our mysteries in principle, it would in many cases take longer than the age of our universe in practice. Our brute force method is thus typically most helpful once a mystery has been transformed/broken apart into simpler pieces by the modules described below.</p>
<p>We generate the expressions to try by representing them as strings of symbols, trying first all strings of length 1 , then all of length 2 , etc., saving time by only generating those strings that are syntactically correct. The symbols used are the independent variables as well a subset of those listed in Table I, each representing a constant or a function. We minimize string length by using reverse Polish notation, so that parentheses become unnecessary. For example, $x+y$ can be expressed as the string "xy+", the number $-2 / 3$ can be expressed as the string " $0&lt;&lt;1&gt;&gt;$ " and the relativistic momentum formula $m v / \sqrt{1-v^{2} / c^{2}}$ can be expressed as the string "mv<em>lvv</em>cc*/-R/".</p>
<p>Inspection of Table I reveals that many of the symbols are redundant. For example, "1"="0&gt;" and "x $\sim$ " = "0x-". $\pi=2 \arcsin 1$, so if we drop the symbol "P", mysteries involving $\pi$ can still get solved with "P" replaced by "1N1&gt;*" - it just takes longer.</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Arguments</th>
</tr>
</thead>
<tbody>
<tr>
<td>+</td>
<td>add</td>
<td>2</td>
</tr>
<tr>
<td>*</td>
<td>multiply</td>
<td>2</td>
</tr>
<tr>
<td>-</td>
<td>subtract</td>
<td>2</td>
</tr>
<tr>
<td>/</td>
<td>divide</td>
<td>2</td>
</tr>
<tr>
<td>$&gt;$</td>
<td>increment</td>
<td>1</td>
</tr>
<tr>
<td>$&lt;$</td>
<td>decrement</td>
<td>1</td>
</tr>
<tr>
<td>$\sim$</td>
<td>negate</td>
<td>1</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>R</td>
<td>sqrt</td>
<td>1</td>
</tr>
<tr>
<td>E</td>
<td>exp</td>
<td>1</td>
</tr>
<tr>
<td>P</td>
<td>$\pi$</td>
<td>0</td>
</tr>
<tr>
<td>L</td>
<td>ln</td>
<td>1</td>
</tr>
<tr>
<td>I</td>
<td>invert</td>
<td>1</td>
</tr>
<tr>
<td>C</td>
<td>$\cos$</td>
<td>1</td>
</tr>
<tr>
<td>A</td>
<td>abs</td>
<td>1</td>
</tr>
<tr>
<td>N</td>
<td>arcsin</td>
<td>1</td>
</tr>
<tr>
<td>T</td>
<td>arctan</td>
<td>1</td>
</tr>
<tr>
<td>S</td>
<td>sin</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>TABLE I: Functions optionally included in brute force search. The following three subsets are tried in turn: "+-<em>/&gt;&lt;\SPLICER", "+-</em>/&gt; $0 \sim$ " and "+-*/&gt;&lt;\REPLICANTS0".</p>
<p>Since there are $s^{n}$ strings of length $n$ using an alphabet of $s$ symbols, there can be a significant cost both from using too many symbols (increasing $s$ ) and from using too few symbols (increasing the required $n$, or even making a solution impossible). As a compromise, our brute force module tries to solve the mystery using three different symbol subsets as explained in the caption of Table I.</p>
<p>To exploit the fact that many equations or parts thereof have multiplicative or additive constants, our brute force method comes in two variants that automatically solves for such constants, thus allowing the algorithm to focus on the symbolic expression and not on numerical constants.</p>
<p>Although the problem of overfitting is most familiar when searching a continuous parameter space, the same phenomenon can occur when searching our discrete space of symbol strings. To mitigate this, we follow the prescription in [30] and define the winning function to be the one with r.m.s. fitting error $\epsilon&lt;\epsilon_{b}$ that has the smallest total description length</p>
<p>$$ D L \equiv \log <em 2="2">{2} N+\lambda \log</em>\right)\right] $$}\left[\max \left(1, \frac{\epsilon}{\epsilon_{d}</p>
<p>where $\epsilon_{d}=10^{-15}$ and $N$ is the rank of the string on the list of all strings tried. The two terms correspond roughly to the number of bits required to store the symbol string and the prediction errors, respectively, if the hyperparameter $\lambda$ is set to equal the number of data points $N_{d}$. We use $\lambda=N_{d}^{1 / 2}$ in our experiments below, to prioritize simpler formulas. If the mystery has been generated using a neural network (see below), we set the precision</p>
<p>threshold $\epsilon_{b}$ to ten times the validation error, otherwise we set it to $10^{-5}$.</p>
<h2>E. Neural-network-based tests \&amp; transformations</h2>
<p>Even after applying the dimensional analysis, many mysteries are still too complex to be solved by the polyfit or brute force modules in a reasonable amount of time. However, if the mystery function $f\left(x_{1}, \ldots, x_{n}\right)$ can be found to have simplifying properties, it may be possible to transform it into one or more simpler mysteries that can be more easily solved. To search for such properties, we need to be able to evaluate $f$ at points $\left{x_{1}, \ldots, x_{n}\right}$ of our choosing where we typically have no data. For example, to test if a function $f$ has translational symmetry, we need to test if $f\left(x_{1}, x_{2}\right)=f\left(x_{1}+a, x_{2}+a\right)$ for various constants $a$, but if a given data point has its two variables separated by $x_{2}-x_{1}=1.61803$, we typically have no other examples in our data set with exactly that variable separation. To perform our tests, we thus need an accurate high-dimensional interpolation between our data point.</p>
<h2>1. Neural network training</h2>
<p>In order to obtain such an interpolating function for a given mystery, we train a neural network to predict the output given its input. We train a feed-forward, fully connected neural network with 6 hidden layers with softplus activation functions, the first 3 having 128 neurons and the last 3 having 64 neurons. For each mystery we generated 100,000 data points, using $80 \%$ as the training set and the remainder as the validation set, training for 100 epochs with learning rate 0.005 and batch size 2048. We use the r.m.s.-error loss function and the Adam optimizer with a weight decay of $10^{-2}$. The learning rate and momentum schedules were implemented as described in $[31,32]$ using the FastAI package [33]; with a ration of 20 between the maximum and minimum learning rates, and using $10 \%$ of the iterations for the last part of the training cycle. For the momentum, the maximum $\beta_{1}$-value was 0.95 and the minimum 0.85 , while $\beta_{2}=0.99$.</p>
<p>If the neural network were expressive enough to be able to perfectly fit the mystery function, and the training process would never got stuck in a local minimum, then one might naively expect the r.m.s. validation error $\epsilon_{\mathrm{NN}}^{0}$ to scale as $f_{\mathrm{rms}} \epsilon / N_{d}^{1 / 2}$ in the limit of ample data, with a constant prefactor depending on the number of function arguments and the function's complexity. Here $f_{\mathrm{rms}}$ is the r.m.s. of the $f$-values in the dataset, $N_{d}$ is the number of data points and $\epsilon$ is the relative r.m.s. noise on the independent variable as explored in Section III D. For realistic situations, one expects limited expressibility and
convergence to keep $\epsilon_{\mathrm{NN}}^{0}$ above some positive floor even as $N_{d} \rightarrow \infty$ and $\epsilon \rightarrow 0$. In practice, we obtained $\epsilon_{\mathrm{NN}}^{0}$ values between $10^{-3} f_{\mathrm{rms}}$ and $10^{-5} f_{\mathrm{rms}}$ across the range of tested equations.</p>
<h2>2. Translational symmetry and generalizations</h2>
<p>We test for translational symmetry using the neural network as detailed in Algorithm 1. We first check if the $f\left(x_{1}, x_{2}, x_{3}, \ldots\right)=f\left(x_{1}+a, x_{2}+a, x_{3} \ldots\right)$ to within a precision $\epsilon_{s y m}$. If that is the case, then $f$ depends on $x_{1}$ and $x_{2}$ only through their difference, so we replace these two input variables by a single new variable $x_{1}^{\prime} \equiv x_{2}-x_{1}$. Otherwise, we repeat this test for all pairs of input variables, and also test whether any variable pair can be replaced by its sum, product or ratio. The ratio case corresponds to scaling symmetry, where two variables can be simultaneously rescaled without changing the answer. If any of these simplifying properties is found, the resulting transformed mystery (with one fewer input variables) is iteratively passed into a fresh instantiation of our full $A I$ Feynman symbolic regression algorithm, as illustrated in Figure 1. After experimentation, we chose the precision threshold $\epsilon_{s y m}$ to be 7 times the neural network validation error, which roughly optimized the training set performance. (If the noise were Gaussian, even a cut at 4 rather than 7 standard deviations would produce negligible false positives.)</p>
<h2>3. Separability</h2>
<p>We test for separability using the neural network as exemplified in Algorithm 2. A function is separable if it can be split into two parts with no variables in common. We test for both additive and multiplicative separability, corresponding to these two parts being added and multiplied, respectively (the logarithm of a multiplicatively separable function is additively separable).</p>
<p>For example, to test if a function of 2 variables is multiplicatively separable, i.e., of the form $f\left(x_{1}, x_{2}\right)=$ $g\left(x_{1}\right) h\left(x_{2}\right)$ for some univariate functions $g$ and $h$, we first select two constants $c_{1}$ and $c_{2}$; for numerical robustness, we choose $c_{i}$ to be the means of all the values of $x_{i}$ in the mystery data set, $i=1,2$. We then compute the quantity</p>
<p>$$
\Delta_{\mathrm{sep}}\left(x_{1}, x_{2}\right) \equiv f_{\mathrm{rms}}^{-1}\left|f\left(x_{1}, x_{2}\right)-\frac{f\left(x_{1}, c_{2}\right) f\left(c_{1}, x_{2}\right)}{f\left(c_{1}, c_{2}\right)}\right|
$$</p>
<p>for each data point. This is a measure of non-separability, since it vanishes if $f$ is multiplicatively separable. The equation is considered separable if the r.m.s. average $\Delta_{\text {sep }}$</p>
<p>over the mystery data set is less than an accuracy threshold $\epsilon_{\text {sep }}$, which is chosen to be $N=10$ times the neural network validation error ${ }^{2}$.</p>
<p>If separability is found, we define the two new univariate mysteries $y^{\prime} \equiv f\left(x_{1}, c_{2}\right)$ and $y^{\prime \prime} \equiv f\left(c_{1}, x_{2}\right) / f\left(c_{1}, c_{2}\right)$. We pass the first one, $y^{\prime}$, back to a fresh instantiations of our full AI Feynman symbolic regression algorithm and if it gets solved, we redefine $y^{\prime \prime} \equiv y / y^{\prime} c_{\text {num }}$, where $c_{\text {num }}$ represents any multiplicative numerical constant that appears in $y^{\prime}$. We then pass $y^{\prime \prime}$ back to our algorithm and if it gets solved, the final solutions is $y=y^{\prime} y^{\prime \prime} / c_{\text {num }}$. We test for additive separability analogously, simply replacing $*$ and $/$ by + and - above; also $c_{\text {num }}$ will represent an additive numerical constant in this case. If we succeed in solving the two parts, then the full solution to the original mystery is the sum of the two parts minus the numerical constant. When there are more than two variables $x_{i}$, we are testing all the possible subsets of variables that can lead to separability, and proceed as above for the newly created two mysteries.</p>
<h2>4. Setting variables equal</h2>
<p>We also exploit the neural network to explore the effect of setting two input variables equal and attempting to solve the corresponding new mystery $y^{\prime}$ with one fewer variable. We try this for all variable pairs, and if the resulting new mystery is solved, we try solving the mystery $y^{\prime \prime} \equiv y / y^{\prime}$ that has the found solution divided out. As an example, this technique solves the Gaussian probability distribution mystery I.6.2. After making $\theta$ and $\sigma$ equal, and dividing the initial equation by the result, we are getting rid of the denominator and the remaining part of the equation is an exponential. After taking the logarithm of this (see the below section) the resulting expression can be easily solved by the brute force method.</p>
<h2>F. Extra Transformations</h2>
<p>In addition, several transformations are applied to the dependent and independent variables which proved to be useful for solving certain equations. Thus, for each equation, we ran the brute force and polynomial fit on a modified version of the equation in which the dependent variable was transformed by one of the following functions: square root, raise to the power of $2, \log$, exp,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>TABLE II: Hyperparameters in our algorithm and the setting we use in this paper. inverse, $\sin , \cos , \tan , \arccos , \arctan$. This reduces the number of symbols needed by the brute force by one and in certain cases it even allows the polynomial fit to solve the equation, when the brute force would otherwise fail. For example, the formula for the distance between 2 points in the 3D Euclidean space: $\sqrt{\left(x_{1}-x_{2}\right)^{2}+\left(y_{1}-y_{2}\right)^{2}+\left(z_{1}-z_{2}\right)^{2}}$, once raised to the power of 2 becomes just a polynomial which can be easily discovered by the polynomial fit algorithm. The same transformations are also applied to the dependent variables, one at a time. In addition multiplication and division by 2 were added as transformations in this case.</p>
<p>It should be noted that, like most machine-learning methods, the AI Feynman algorithm has some hyperparameters that can be tuned to optimize performance on the problems at hand. They were all introduced above, but for convenience, they are also summarized in Table II.</p>
<h2>III. RESULTS</h2>
<h2>A. The Feynman Symbolic Regression Database</h2>
<p>To facilitate quantitative testing of our and other symbolic regression algorithms, we created the Feynman Symbolic Regression Database (FSReD) and made it freely available for download ${ }^{3}$. For each regression mystery, the database contains the following:</p>
<ol>
<li>Data table: A table of numbers, whose rows are of the form $\left{x_{1}, x_{2}, \ldots, y\right}$, where $y=f\left(x_{1}, x_{2}, \ldots\right)$;
<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></li>
</ol>
<p>the challenge is to discover the correct analytic expression for the mystery function $f$.
2. Unit table: A table specifying the physical units of the input and output variables as 6-dimensional vectors of the form seen in Table III.
3. Equation: The analytic expression for the mystery function $f$, for answer-checking.</p>
<p>To test an analytic regression algorithm using the database, its task is to predict $f$ for each mystery taking the data table (and optionally the unit table) as input. Of course, there are typically many symbolically different ways of expressing the same function. For example, if the mystery function $f$ is $(u+v) /\left(1+u v / c^{2}\right)$, then the symbolically different expression $(v+u) /\left(1+u v / c^{2}\right)$ should count as a correct solution. The rule for evaluating an analytic regression method is therefore that a mystery function $f$ is deemed correctly solved by a candidate expression $f^{\prime}$ if algebraic simplification of the expression $f^{\prime}-f$ (say, with the Simplify function in Mathematica or the simplify function in the Python sympy package) produces the symbol " 0 ".</p>
<p>In order to sample equations from a broad range of physics areas, the database is generated using 100 equations from the seminal Feynman Lectures on Physics [3436], a challenging three-volume course covering classical mechanics, electromagnetism and quantum mechanics as well as a selection of other core physics topics; we prioritized the most complex equations, excluding ones involving derivatives or integrals. The equations are listed in tables IV and V, and can be seen to involve between 1 and 9 independent variables as well as the elementary functions,+- ,,$+ /, \operatorname{sqrt}, \exp , \log , \sin , \cos , \arcsin$ and $\tanh$. The numbers appearing in these equations are seen to be simple rational numbers as well as $e$ and $\pi$.</p>
<p>We also included in the database a set of 20 more challenging "bonus" equations, extracted from other seminal physics books: Classical Mechanics by Herbert Goldstein, Charles P. Poole, John L. Safko [37], Classical electrodynamics by J. Jackson [38], Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity by Steven Weinberg [39] and Quantum Field Theory and the Standard Model by Matthew D. Schwartz [40]. These equations were selected for being both famous and complicated.</p>
<p>The data table provided for each mystery equation contains $10^{5}$ rows corresponding to randomly generated input variables. These are sampled uniformly between 1 and 5. For certain equations, the range of sampling was slightly adjusted to avoid unphysical result, such as division by zero, or taking the square root of a negative number. The range used for each equation is listed in the Feynman Symbolic Regression Database.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Variables</th>
<th style="text-align: center;">Units</th>
<th style="text-align: center;">m</th>
<th style="text-align: center;">s</th>
<th style="text-align: center;">kg</th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">V</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$a, g$</td>
<td style="text-align: center;">Acceleration</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$h, \hbar, L, J_{z}$</td>
<td style="text-align: center;">Angular momentum</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">A</td>
<td style="text-align: center;">Area</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$k_{b}$</td>
<td style="text-align: center;">Boltzmann constant</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">C</td>
<td style="text-align: center;">Capacitance</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
</tr>
<tr>
<td style="text-align: center;">$q, q_{1}, q_{2}$</td>
<td style="text-align: center;">Charge</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$j$</td>
<td style="text-align: center;">Current density</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$I, I_{0}$</td>
<td style="text-align: center;">Current Intensity</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$\rho, \rho_{0}$</td>
<td style="text-align: center;">Density</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\theta, \theta_{1}, \theta_{2}, \sigma, n$</td>
<td style="text-align: center;">Dimensionless</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$g_{\sim}, k_{f}, \gamma, \chi, \beta, \alpha$</td>
<td style="text-align: center;">Dimensionless</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$p_{\gamma}, n_{0}, \delta, f, \mu$</td>
<td style="text-align: center;">Dimensionless</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$n_{0}, \delta, f, \mu, Z_{1}, Z_{2}$</td>
<td style="text-align: center;">Dimensionless</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$D$</td>
<td style="text-align: center;">Diffusion coefficient</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\mu_{\text {drift }}$</td>
<td style="text-align: center;">Drift velocity constant</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$p_{d}$</td>
<td style="text-align: center;">Electric dipole moment</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$E_{f}$</td>
<td style="text-align: center;">Electric field</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">$\epsilon$</td>
<td style="text-align: center;">Electric permitivity</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
</tr>
<tr>
<td style="text-align: center;">$E, K, U$</td>
<td style="text-align: center;">Energy</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$E_{\text {den }}$</td>
<td style="text-align: center;">Energy density</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$F_{E}$</td>
<td style="text-align: center;">Energy flux</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$F, N_{n}$</td>
<td style="text-align: center;">Force</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\omega, \omega_{0}$</td>
<td style="text-align: center;">Frequency</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$k_{G}$</td>
<td style="text-align: center;">Grav. coupling $\left(G m_{1} m_{2}\right)$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$H$</td>
<td style="text-align: center;">Hubble constant</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$L_{\text {ind }}$</td>
<td style="text-align: center;">Inductance</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">$n_{\text {rho }}$</td>
<td style="text-align: center;">Inverse volume</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$x, x_{1}, x_{2}, x_{3}$</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$y, y_{1}, y_{2}, y_{3}$</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$z, z_{1}, z_{2}, r, r_{1}, r_{2}$</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda, d_{1}, d_{2}, d, f_{f}, a_{f}$</td>
<td style="text-align: center;">Length</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$I_{1}, I_{2}, I_{<em>}, I_{</em>_{0}}$</td>
<td style="text-align: center;">Light intensity</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$B, B_{x}, B_{y}, B_{z}$</td>
<td style="text-align: center;">Magnetic field</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">$\mu_{m}$</td>
<td style="text-align: center;">Magnetic moment</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$M$</td>
<td style="text-align: center;">Magnetisation</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$m, m_{0}, m_{1}, m_{2}$</td>
<td style="text-align: center;">Mass</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\mu_{e}$</td>
<td style="text-align: center;">Mobility</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$p$</td>
<td style="text-align: center;">Momentum</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$G$</td>
<td style="text-align: center;">Newton's constant</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$P_{*}$</td>
<td style="text-align: center;">Polarization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$P$</td>
<td style="text-align: center;">Power</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$p_{F}$</td>
<td style="text-align: center;">Pressure</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$R$</td>
<td style="text-align: center;">Resistance</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">$\mu_{S}$</td>
<td style="text-align: center;">Shear modulus</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$L_{\text {rad }}$</td>
<td style="text-align: center;">Spectral radiance</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$k_{\text {spring }}$</td>
<td style="text-align: center;">Spring constant</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{\text {den }}$</td>
<td style="text-align: center;">Surface Charge density</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$T, T_{1}, T_{2}$</td>
<td style="text-align: center;">Temperature</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\kappa$</td>
<td style="text-align: center;">Thermal conductivity</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-3$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$t, t_{1}$</td>
<td style="text-align: center;">Time</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">Torque</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$A_{\text {vec }}$</td>
<td style="text-align: center;">Vector potential</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">$u, v, v_{1}, c, w$</td>
<td style="text-align: center;">Velocity</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$V, V_{1}, V_{2}$</td>
<td style="text-align: center;">volume</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\rho_{c}, \rho_{c_{0}}$</td>
<td style="text-align: center;">Volume charge density</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-1$</td>
</tr>
<tr>
<td style="text-align: center;">$V_{e}$</td>
<td style="text-align: center;">Voltage</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">$k$</td>
<td style="text-align: center;">Wave number</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$Y$</td>
<td style="text-align: center;">Young modulus</td>
<td style="text-align: center;">$-1$</td>
<td style="text-align: center;">$-2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>TABLE III: Unit table used for our automated dimensional analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Feynman eq.</th>
<th style="text-align: center;">Equation time (s)</th>
<th style="text-align: center;">Solution time (s)</th>
<th style="text-align: center;">Methods used</th>
<th style="text-align: center;">Data needed</th>
<th style="text-align: center;">Solved by Eureqa</th>
<th style="text-align: center;">Solved w/o da</th>
<th style="text-align: center;">Noise tolerance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">I.6.20a</td>
<td style="text-align: center;">$f=e^{-\theta^{2} / 2} / \sqrt{2 \pi}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.6.20</td>
<td style="text-align: center;">$f=e^{-\frac{\theta^{2}}{2 \sigma^{2}}} / \sqrt{2 \pi \sigma^{2}}$</td>
<td style="text-align: center;">2992</td>
<td style="text-align: center;">ev, bf-log</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.6.20b</td>
<td style="text-align: center;">$f=e^{-\frac{\left(\theta-\theta_{1}\right)^{2}}{2 \sigma^{2}}} / \sqrt{2 \pi \sigma^{2}}$</td>
<td style="text-align: center;">4792</td>
<td style="text-align: center;">sym-, ev, bf-log</td>
<td style="text-align: center;">$10^{3}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.8.14</td>
<td style="text-align: center;">$d=\sqrt{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}$</td>
<td style="text-align: center;">544</td>
<td style="text-align: center;">da, pf-squared</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.9.18</td>
<td style="text-align: center;">$F=\frac{G m_{1} m_{2}}{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}+\left(z_{2}-z_{1}\right)^{2}}$</td>
<td style="text-align: center;">5975</td>
<td style="text-align: center;">da, sym-, sym-, sep*, pf-inv</td>
<td style="text-align: center;">$10^{6}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">I.10.7</td>
<td style="text-align: center;">$m=\frac{\log _{e}}{\sqrt{1-\frac{v^{2}}{c^{2}}}}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.11.19</td>
<td style="text-align: center;">$A=x_{1} y_{1}+x_{2} y_{2}+x_{3} y_{3}$</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">da, pf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.12.1</td>
<td style="text-align: center;">$F=\mu N_{e}$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.12.2</td>
<td style="text-align: center;">$F=\frac{q_{1} y_{1}}{4 \pi c e^{2}}$</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.12.4</td>
<td style="text-align: center;">$E_{f}=\frac{q_{1}}{4 \pi e^{2}}$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.12.5</td>
<td style="text-align: center;">$F=q_{2} E_{f}$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.12.11</td>
<td style="text-align: center;">$F=q\left(E_{f}+B v \sin \theta\right)$</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.13.4</td>
<td style="text-align: center;">$K=\frac{1}{2} m\left(v^{2}+u^{2}+w^{2}\right)$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.13.12</td>
<td style="text-align: center;">$U=G m_{1} m_{2}\left(\frac{1}{x_{2}}-\frac{1}{v_{1}}\right)$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.14.3</td>
<td style="text-align: center;">$U=m g z$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.14.4</td>
<td style="text-align: center;">$U=\frac{k_{z p r i n g} v^{2}}{2}$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.15.3x</td>
<td style="text-align: center;">$x_{1}=\frac{x-\alpha}{\sqrt{1-u^{2} / c^{2}}}$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.15.3t</td>
<td style="text-align: center;">$t_{1}=\frac{t-\alpha e / c^{2}}{\sqrt{1-u^{2} / c^{2}}}$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.15.10</td>
<td style="text-align: center;">$p=\frac{\overline{m_{0} v}}{\sqrt{1-v^{2} / c^{2}}}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.16.6</td>
<td style="text-align: center;">$v_{1}=\frac{\alpha+v}{\sqrt{1+v^{2} / c^{2}}}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.18.4</td>
<td style="text-align: center;">$r=\frac{m_{1} r_{1}+m_{2} r_{2}}{\sin 1+m_{2}}$</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.18.12</td>
<td style="text-align: center;">$\tau=r F \sin \theta$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.18.16</td>
<td style="text-align: center;">$L=m r v \sin \theta$</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.24.6</td>
<td style="text-align: center;">$E=\frac{1}{4} m\left(\omega^{2}+\omega_{0}^{2}\right) x^{2}$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.25.13</td>
<td style="text-align: center;">$V_{e}=\frac{u}{C}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.26.2</td>
<td style="text-align: center;">$\theta_{1}=\arcsin \left(n \sin \theta_{2}\right)$</td>
<td style="text-align: center;">530</td>
<td style="text-align: center;">da, bf-sin</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.27.6</td>
<td style="text-align: center;">$f_{f}=\frac{1}{d_{1}+\frac{\pi}{d_{2}}}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.29.4</td>
<td style="text-align: center;">$k=\frac{\omega}{c}$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.29.16</td>
<td style="text-align: center;">$x=\sqrt{x_{1}^{2}+x_{2}^{2}-2 x_{1} x_{2} \cos \left(\theta_{1}-\theta_{2}\right)}$</td>
<td style="text-align: center;">2135</td>
<td style="text-align: center;">da, sym-, bf-squared</td>
<td style="text-align: center;">$10^{3}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.30.3</td>
<td style="text-align: center;">$I_{<em>}=I_{</em>} \frac{\sin ^{2}(n \theta / 2)}{\sin ^{2}(\theta / 2)}$</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.30.5</td>
<td style="text-align: center;">$\theta=\arcsin \left(\frac{2}{n d}\right)$</td>
<td style="text-align: center;">529</td>
<td style="text-align: center;">da, bf-sin</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.32.5</td>
<td style="text-align: center;">$P=\frac{a^{2} \alpha^{2}}{6 \pi e c 3}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.32.17</td>
<td style="text-align: center;">$P=\left(\frac{1}{2} e c E_{f}^{2}\right)\left(8 \pi r^{2} / 3\right)\left(\omega^{4} /\left(\omega^{2}-\omega_{0}^{2}\right)^{2}\right)$</td>
<td style="text-align: center;">698</td>
<td style="text-align: center;">da, bf-sqrt</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.34.8</td>
<td style="text-align: center;">$\omega=\frac{\pi e B}{E_{c}}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.34.10</td>
<td style="text-align: center;">$\omega=\frac{\varepsilon}{1-e / c}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.34.14</td>
<td style="text-align: center;">$\omega=\frac{1+v / c}{\sqrt{1-v^{2} / c^{2}}} \omega_{0}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.34.27</td>
<td style="text-align: center;">$E=h \omega$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.37.4</td>
<td style="text-align: center;">$I_{*}=I_{1}+I_{2}+2 \sqrt{I_{1} I_{2}} \cos \delta$</td>
<td style="text-align: center;">7032</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.38.12</td>
<td style="text-align: center;">$r=\frac{4 \pi e b^{2}}{m a^{2}}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.39.10</td>
<td style="text-align: center;">$E=\frac{3}{2} p_{F} V$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.39.11</td>
<td style="text-align: center;">$E=\frac{1}{c-1} p_{F} V$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.39.22</td>
<td style="text-align: center;">$P_{F}=\frac{n k_{b} T}{V}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">I.40.1</td>
<td style="text-align: center;">$n=n_{0} e^{-\frac{n_{0} x_{2}}{k_{b} T}}$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.41.16</td>
<td style="text-align: center;">$L_{r a d}=\frac{h \omega^{3}}{a^{2} c^{2}\left(e^{\frac{k_{b} T}{k_{b} T}}-1\right)}$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">I.43.16</td>
<td style="text-align: center;">$v=\frac{\mu_{d r i} l_{r b} V_{c}}{V}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.43.31</td>
<td style="text-align: center;">$D=\mu_{e} k_{b} T$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.43.43</td>
<td style="text-align: center;">$n=\frac{1}{c-1} \frac{k_{b} c}{A}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.44.4</td>
<td style="text-align: center;">$E=n k_{b} T \ln \left(\frac{V_{2}}{V_{1}}\right)$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">I.47.23</td>
<td style="text-align: center;">$c=\sqrt{\frac{\gamma p r}{\rho}}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">I.48.20</td>
<td style="text-align: center;">$E=\frac{m c^{2}}{\sqrt{1-v^{2} / c^{2}}}$</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">I.50.26</td>
<td style="text-align: center;">$x=x_{1}[\cos (\omega t)+\alpha \cos (\omega t)^{2}]$</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">da bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
</tbody>
</table>
<p>TABLE IV: Tested Feynman Equations, part 1. Abbreviations in the "Methods used" column: "da" = dimensional analysis, "bf" = brute force, "pf" = polyfit, "ev" = set 2 variables equal, "sym" = symmetry, "sep" = separability. Suffixes denote the type of symmetry or separability ("sym-" =translationa symmetry, "sep*"=multiplicative separability, etc.) or the preprocessing before brute force (e.g., "bf-inverse" means inverting the mystery function before bf).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Feynman eq.</th>
<th style="text-align: center;">Equation time (s)</th>
<th style="text-align: center;">Solution time (s)</th>
<th style="text-align: center;">Methods used</th>
<th style="text-align: center;">Data needed</th>
<th style="text-align: center;">Solved by Eureqa</th>
<th style="text-align: center;">Solved w/o DA</th>
<th style="text-align: center;">Noise tolerance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">II.2.42</td>
<td style="text-align: center;">$\mathrm{P}=\frac{\kappa\left(T_{0}-T_{1}\right) A}{}$</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.3.24</td>
<td style="text-align: center;">$F_{E}=\frac{\mu^{d}}{4 \pi r^{2}}$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.4.23</td>
<td style="text-align: center;">$V_{e}=\frac{q}{4 \pi e r}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.6.11</td>
<td style="text-align: center;">$V_{e}=\frac{1}{4 \pi r} \frac{p_{d} \cos \theta}{r^{2}}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.6.15a</td>
<td style="text-align: center;">$E_{f}=\frac{3}{4 \pi r} \frac{p_{d} \pi}{q} \sqrt{x^{2}+y^{2}}$</td>
<td style="text-align: center;">2801</td>
<td style="text-align: center;">da, sm, bf</td>
<td style="text-align: center;">$10^{4}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.6.15b</td>
<td style="text-align: center;">$E_{f}=\frac{3}{4 \pi r_{b}} \frac{p_{d}}{r^{2}} \cos \theta \sin \theta$</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.8.7</td>
<td style="text-align: center;">$E=\frac{3}{5} \frac{q^{2}}{4 \pi r_{d}}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.8.31</td>
<td style="text-align: center;">$E_{\text {den }}=\frac{\epsilon E_{f}^{2}}{2}$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.10.9</td>
<td style="text-align: center;">$E_{f}=\frac{\sigma_{d e \pi}}{\epsilon} \frac{1}{1+\chi}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.11.3</td>
<td style="text-align: center;">$x=\frac{q E_{f}}{m\left(\omega_{0}^{2}-\omega^{2}\right)}$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.11.17</td>
<td style="text-align: center;">$n=n_{0}\left(1+\frac{p_{d} E_{f} \cos \theta}{k_{b} T}\right)$</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.11.20</td>
<td style="text-align: center;">$P_{*}=\frac{n_{p} \rho_{d}^{2} E_{f}}{3 k_{b} T}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.11.27</td>
<td style="text-align: center;">$P_{*}=\frac{n \alpha}{1-n \alpha / 3} \epsilon E_{f}$</td>
<td style="text-align: center;">337</td>
<td style="text-align: center;">da bf-inverse</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.11.28</td>
<td style="text-align: center;">$\theta=1+\frac{n \alpha}{1-(n \alpha / 3)}$</td>
<td style="text-align: center;">1708</td>
<td style="text-align: center;">da, sym*, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.13.17</td>
<td style="text-align: center;">$B=\frac{1}{4 \pi \epsilon c_{0}^{2}} \frac{k l}{r}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.13.23</td>
<td style="text-align: center;">$\rho_{c}=\frac{\mu_{c}}{1-\epsilon c^{2} / c^{2}}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.13.34</td>
<td style="text-align: center;">$j=\frac{\mu_{c} \pi}{\sqrt{1-c^{2} / c^{2}}}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.15.4</td>
<td style="text-align: center;">$E=-\mu_{M} B \cos \theta$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.15.5</td>
<td style="text-align: center;">$E=-p_{d} E_{f} \cos \theta$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.21.32</td>
<td style="text-align: center;">$V_{e}=\frac{q}{4 \pi e r(1-v / c)}$</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.24.17</td>
<td style="text-align: center;">$k=\sqrt{\frac{\omega^{2}}{c^{2}}-\frac{\pi^{2}}{d^{2}}}$</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">da bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">II.27.16</td>
<td style="text-align: center;">$F_{E}=\epsilon c E_{f}^{2}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.27.18</td>
<td style="text-align: center;">$E_{\text {den }}=\epsilon E_{f}^{2}$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.34.2a</td>
<td style="text-align: center;">$I=\frac{q v}{2 \pi c}$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.34.2</td>
<td style="text-align: center;">$\mu_{M}=\frac{q v r}{4 \pi r}$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.34.11</td>
<td style="text-align: center;">$\omega=\frac{g_{c} \rho_{d}}{k}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.34.29a</td>
<td style="text-align: center;">$\mu_{M}=\frac{\sigma^{2}}{4 \pi r_{b}}$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.34.29b</td>
<td style="text-align: center;">$E=\frac{\kappa \cdot \mu_{M} B \cdot f_{z}}{k}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.35.18</td>
<td style="text-align: center;">$n=\frac{n_{0}}{\exp \left(\mu_{m} B /\left(k_{b} T\right)\right)+\exp \left(-\mu_{m} B /\left(k_{b} T\right)\right)}$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.35.21</td>
<td style="text-align: center;">$M=n_{\rho} \mu_{M} \tanh \left(\frac{\mu_{M} B}{k_{b} T}\right)$</td>
<td style="text-align: center;">1597</td>
<td style="text-align: center;">da, halve-input, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">II.36.38</td>
<td style="text-align: center;">$f=\frac{\mu_{m} B}{k_{b} T}+\frac{\mu_{m} \alpha M}{\epsilon c^{2} k_{b} T}$</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">da bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">II.37.1</td>
<td style="text-align: center;">$E=\mu_{M}(1+\chi) B$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.38.3</td>
<td style="text-align: center;">$F=\frac{Y A x}{\pi}$</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">II.38.14</td>
<td style="text-align: center;">$\mu_{S}=\frac{\sigma}{3(1+\sigma)}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.4.32</td>
<td style="text-align: center;">$n=\frac{1}{e^{\frac{k_{c}}{k_{b} T}}-1}$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.4.33</td>
<td style="text-align: center;">$E=\frac{\kappa_{0}}{e^{\frac{\kappa}{k_{b} T}}-1}$</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.7.38</td>
<td style="text-align: center;">$\omega=\frac{2 \mu_{M} B}{\kappa}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">III.8.54</td>
<td style="text-align: center;">$p_{\gamma}=\sin \left(\frac{E t}{b}\right)^{2}$</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.9.52</td>
<td style="text-align: center;">$p_{\gamma}=\frac{\mu_{d} E_{f} t}{k} \frac{\sin \left((\omega-\omega_{0}) t / 2\right)^{2}}{\left((\omega-\omega_{0}) t / 2\right)^{2}}$</td>
<td style="text-align: center;">3162</td>
<td style="text-align: center;">da, sym-, sm, bf</td>
<td style="text-align: center;">$10^{3}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.10.19</td>
<td style="text-align: center;">$E=\mu_{M} \sqrt{B_{x}^{2}+B_{y}^{2}+B_{z}^{2}}$</td>
<td style="text-align: center;">410</td>
<td style="text-align: center;">da, bf-squared</td>
<td style="text-align: center;">$10^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">III.12.43</td>
<td style="text-align: center;">$L=n \hbar$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.13.18</td>
<td style="text-align: center;">$v=\frac{2 E d^{2} k}{\hbar}$</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">III.14.14</td>
<td style="text-align: center;">$I=I_{0}\left(e^{\frac{2 V_{c}}{k_{b} T}}-1\right)$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.15.12</td>
<td style="text-align: center;">$E=2 U(1-\cos (k d))$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">III.15.14</td>
<td style="text-align: center;">$m=\frac{\hbar^{2}}{2 E d^{2}}$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">III.15.27</td>
<td style="text-align: center;">$k=\frac{2 \pi \alpha}{\sigma \beta}$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.17.37</td>
<td style="text-align: center;">$f=\beta(1+\alpha \cos \theta)$</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">III.19.51</td>
<td style="text-align: center;">$E=\frac{-\alpha c_{0}^{4}}{2\left(4 \pi c\right)^{2} b^{2} n^{2}}$</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">da, bf</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">III.21.20</td>
<td style="text-align: center;">$j=\frac{-\mu_{c_{0}} a A_{c o v}}{m}$</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">da</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
</tbody>
</table>
<p>TABLE V: Tested Feynman Equations, part 2 (same notation as in Table IV)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Equation</th>
<th style="text-align: center;">Solved</th>
<th style="text-align: center;">Solved by <br> Eureqa</th>
<th style="text-align: center;">Methods used</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rutherford Scattering</td>
<td style="text-align: center;">$A=\left(\frac{Z_{1} Z_{2} \alpha b c}{4 E \sin ^{2}\left(\frac{\pi}{2}\right)}\right)^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf-sqrt</td>
</tr>
<tr>
<td style="text-align: center;">Friedman Equation</td>
<td style="text-align: center;">$H=\sqrt{\frac{8 \pi G}{3} \rho-\frac{k_{f} c^{2}}{\alpha_{f}^{2}}}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf-squared</td>
</tr>
<tr>
<td style="text-align: center;">Compton Scattering</td>
<td style="text-align: center;">$U=\frac{E}{1+\frac{E}{m_{1} g}\left(1-\cos \theta\right)}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf</td>
</tr>
<tr>
<td style="text-align: center;">Radiated gravitational wave power</td>
<td style="text-align: center;">$P=-\frac{32}{5} \frac{G^{4}}{c^{3}} \frac{\left(m_{1} m_{2}\right)^{2}\left(m_{1}+m_{2}\right)}{c^{3}}$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Relativistic aberration</td>
<td style="text-align: center;">$\theta_{1}=\arccos \left(\frac{\cos \theta_{2}-\frac{\pi}{2}}{1-\frac{\pi}{2} \cos \theta_{2}}\right)$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf-cos</td>
</tr>
<tr>
<td style="text-align: center;">N-slit diffraction</td>
<td style="text-align: center;">$I=I_{0}\left[\frac{\sin (\alpha / 2)}{\alpha / 2} \frac{\sin (N \delta / 2)}{\sin (\delta / 2)}\right]^{2}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sm, bf</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 3.16</td>
<td style="text-align: center;">$v=\sqrt{\frac{2}{m}}\left(E-U-\frac{L^{2}}{2 m r^{2}}\right)$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf-squared</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 3.55</td>
<td style="text-align: center;">$k=\frac{m k_{G}}{L^{2}}\left(1+\sqrt{1+\frac{2 E L^{2}}{m k_{G}^{2}}} \cos \left(\theta_{1}-\theta_{2}\right)\right)$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sym-, bf</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 3.64 (ellipse)</td>
<td style="text-align: center;">$r=\frac{d\left(1-n^{2}\right)}{1+\alpha \cos \left(\theta_{1}-\theta_{2}\right)}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sym-, bf</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 3.74 (Kepler)</td>
<td style="text-align: center;">$t=\frac{2 \pi d^{3 / 2}}{\sqrt{G\left(m_{1}+m_{2}\right)}}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 3.99</td>
<td style="text-align: center;">$\alpha=\sqrt{1+\frac{2 c^{2} E L^{2}}{m\left(Z_{1} Z_{2} \alpha^{2}\right)^{2}}}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sym*, bf</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 8.56</td>
<td style="text-align: center;">$E=\sqrt{\left(p-q A_{v e c}\right)^{2} c^{2}+m^{2} c^{4}}+q V_{e}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sep+, bf-squared</td>
</tr>
<tr>
<td style="text-align: center;">Goldstein 12.80</td>
<td style="text-align: center;">$E=\frac{1}{2 m}\left[p^{2}+m^{2} \omega^{2} x^{2}\left(1+\alpha \frac{x}{\psi}\right)\right]$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">da, bf</td>
</tr>
<tr>
<td style="text-align: center;">Jackson 2.11</td>
<td style="text-align: center;">$F=\frac{q}{4 \pi e g^{2}}\left[4 \pi e V_{e} d-\frac{q d \varphi^{2}}{\left(\varphi^{2}-d^{2}\right)^{2}}\right]$</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Jackson 3.45</td>
<td style="text-align: center;">$V_{e}=\frac{q}{\left(r^{2}+d^{2}-2 d r \cos \alpha\right)^{\frac{3}{2}}}$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, bf-inv</td>
</tr>
<tr>
<td style="text-align: center;">Jackson 4.60</td>
<td style="text-align: center;">$V_{e}=E_{f} \cos \theta\left(\frac{\alpha-1}{\alpha+2} \frac{d^{2}}{r^{2}}-r\right)$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sep*, bf</td>
</tr>
<tr>
<td style="text-align: center;">Jackson 11.38 (Doppler)</td>
<td style="text-align: center;">$\omega_{0}=\frac{\sqrt{1-\frac{v^{2}}{c^{2}}}}{1+\frac{v}{c} \cos \theta} \omega$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, cos-input, bf</td>
</tr>
<tr>
<td style="text-align: center;">Weinberg 15.2.1</td>
<td style="text-align: center;">$\rho=\frac{3}{8 \pi G}\left(\frac{c^{2} k_{f}}{\pi_{f}^{2}}+H^{2}\right)$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">da, bf</td>
</tr>
<tr>
<td style="text-align: center;">Weinberg 15.2.2</td>
<td style="text-align: center;">$p_{f}=-\frac{1}{8 \pi G}\left[\frac{c^{2} k_{f}}{\pi_{f}^{2}}+c^{2} H^{2}(1-2 \alpha)\right]$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">da, bf</td>
</tr>
<tr>
<td style="text-align: center;">Schwarz 13.132 (Klein-Nishina)</td>
<td style="text-align: center;">$A=\frac{\pi \alpha^{2} k^{2}}{m^{2} c^{2}}\left(\frac{\omega_{0}}{\omega}\right)^{2}\left[\frac{\omega_{0}}{\omega}+\frac{\omega}{\omega_{0}}-\sin ^{2} \theta\right]$</td>
<td style="text-align: center;">yes</td>
<td style="text-align: center;">no</td>
<td style="text-align: center;">da, sym/, sep*, sin-input, bf</td>
</tr>
</tbody>
</table>
<p>TABLE VI: Tested bonus equations. Goldstein 8.56 is for the special case where the vectors $\mathbf{p}$ and $\mathbf{A}$ are parallel.</p>
<h2>B. Method comparison</h2>
<p>We reviewed the symbolic regression literature for publicly available software against which our method could be compared. To the best of our knowledge, the best competitor by far is the commercial Eureqa software sold by Nutonian, Inc. ${ }^{4}$, implementing an improved version of the generic search algorithm outlined in [27].</p>
<p>We compared the AI Feynman and Eureqa algorithms by applying them both to the Feynman Database for Symbolic Regression, allowing a maximum of 2 hours of CPU time per mystery ${ }^{5}$. Tables IV and V show that Eureqa</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>solved $71 \%$ of the 100 basic mysteries, while AI Feynman solved $100 \%$. Closer inspection of these tables reveal that the greatest improvement of our algorithm over Eureqa
settings in Table II. For Eureqa, each mystery was run on 4 CPUs. The symbols used in trying to solve the equations were:,$+,-$,$*$, /, constant, integer constant, input variable, sqrt, exp, log, sin, cos. To help Eureqa gain speed, we included the additional functions arcsin and arccos only for those mysteries requiring them, and we used only 300 data points (since it does not use a neural network, adding additional data does not help significantly). The time taken to solve an equation using our algorithm, as presented in Tables IV and V, corresponds to the time needed for an equation to be solved using a set of symbols that can actually solve it (see Table I). Equations 1.15.3t and 1.48.2 were solved using the second set of symbols, so the overall time needed for these two equations is one hour larger than the one listed in the tables. Equations I.15.3x and II.35.21 were solved using the 3rd set of symbols, so the overall time taken is two hours larger than the one listed here.</p>
<p>is for the most complicated mysteries, where our neural network enables eliminating variables by discovering symmetries and separability.</p>
<p>The neural network becomes even more important when we rerun AI Feynman without the dimensional analysis module: it now solves $93 \%$ of the mysteries, and makes very heavy use of the neural network to discover separability and translational symmetries. Without dimensional analysis, many of the mysteries retain variables that appear only raised to some power or in a multiplicative prefactor, and AI Feynman tends to recursively discover them and factor them out one by one. For example, the neural network strategy is used six times when solving</p>
<p>$$
F=\frac{G m_{1} m_{2}}{\left(x_{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}+\left(z_{2}-z_{1}\right)^{2}}
$$</p>
<p>without dimensional analysis: three times to discover translational symmetry that replaces $x_{2}-x_{1}, y_{2}-y_{1}$ and $z_{2}-z_{1}$ by new variables, once to group together $G$ and $m_{1}$ into a new variable $a$, once to group together $a$ and $m_{2}$ into a new variable $b$, and one last time to discover separability and factor out $b$. This shows that although dimensional analysis often provides significant time savings, it is usually not necessary for successfully solving the problem.
Inspection of how AI Feynman and Eureqa make progress over time reveals interesting differences. The progress of AI Feynman over time corresponds to repeatedly reducing the number of independent variables, and every time this occurs, it is virtually guaranteed to be a step in the right direction. In contrast, genetic algorithms such as Eureqa make progress over time by finding successively better approximations, but there is no guarantee that more accurate symbolic expressions are closer to the truth when viewed as strings of symbols. Specifically, by virtue of being a genetic algorithm, Eureqa has the advantage of not searching the space of symbolic expressions blindly like our brute force module, but rather with the possibility of a net drift toward more accurate ("fit") equations. The flip side of this is that if Eureqa finds a fairly accurate yet incorrect formula with a quite different functional form, it risks getting stuck near that local optimum. This reflects a fundamental challenge for genetic approaches symbolic regression: if the final formula is composed of separate parts that are not summed but combined in some more complicated way (as a ratio, say), then each of the parts may be useless fits on their own and unable to evolutionarily compete.</p>
<h2>C. Dependence on data size</h2>
<p>To investigate the effect of changing the size of the data set, we repeatedly reduced the size of each data set by
a factor of 10 until our AI Feynman algorithm failed to solve it. As seen in Tables IV and V, most equations are discovered by the polynomial fit and brute force methods using only 10 data points. 100 data points are needed in some cases, because the algorithm may otherwise overfit when the true equation is complex, "discovering" an incorrect equation that is too simple.</p>
<p>As expected, equations that require the use of a neural network to be solved need significantly more data points (between $10^{2}$ and $10^{6}$ ) for the network to be able to learn the mystery function accurately enough (i.e. obtaining r.m.s. accuracy better than $10^{-3}$ ). Note that expressions requiring the neural network are typically more complex, so one might intuitively expect them to require larger data sets for the correct equation to be discovered without overfitting, even when using alternate approaches such as genetic algorithms.</p>
<h2>D. Dependence on noise level</h2>
<p>Since real data is almost always afflicted with measurement errors or other forms of noise, we investigated the robustness of our algorithm. For each mystery, we added independent Gaussian random noise to its dependent variable $y$, of standard deviation $\epsilon y_{\text {rms }}$, where $y_{\text {rms }}$ denotes the r.m.s. $y$-value for the mystery before noise has been added. We initially set the relative noise level $\epsilon=10^{-6}$, then repeatedly multiplied $\epsilon$ by 10 until the $A I$ Feynman algorithm could no longer solve the mystery. As seen in Tables IV and V, most of the equations can still be recovered exactly with an $\epsilon$-value of $10^{-4}$ or less, while almost half of them are still solved for $\epsilon=10^{-2}$.
For these noise experiments, we adjusted the threshold for the brute force and polynomial fit algorithms when the noise level changed, such that not finding a solution at all was preferred over finding an approximate solution. These thresholds were not optimized for each mystery individually, so a better choice of these thresholds might allow the exact equation to be recovered with an even higher noise level for certain equations. In future work, it will be also be interesting to quantify performance of the algorithm on data with noise added to the independent variables, as well as directly on real-world data.</p>
<h2>E. Bonus mysteries</h2>
<p>The 100 basic mysteries discussed above should be viewed as a training set for our AI Feynman algorithm, since we made improvements to its implementation and hyperparameters to optimize performance. In contrast, we can view the 20 bonus mysteries as a test set, since we deliberately selected and analyzed them only after the AI Feynman algorithm and its hyper-parameter settings</p>
<p>(Table II) had been finalized. The bonus mysteries are interesting also by virtue of being significantly more complex and difficult, in order to better identify the limitations our our method.</p>
<p>Table VI sbows that Eureqa solved only $15 \%$ of the bonus mysteries, while AI Feynman solved $90 \%$. The fact that the success percentage differs more between the two methods for the bonus mysteries than for the basic mysteries reflects the increased equation complexity, which requires our neural network based strategies for a larger fraction of the cases.</p>
<p>To shed light on the limitations of the AI Feynman algorithm, it is interesting to consider the two mysteries for which it failed. The radiated gravitational wave power mystery was reduced to the form $y=-\frac{32 a^{2}(1+a)}{5 b^{2}}$ by dimensional analysis, corresponding to the string " $a a a&gt;$ $<em> * b b b b b * * * * /$ " in reverse Polish notation (ignoring the multiplicative prefactor $-\frac{32}{5}$ ). This would require about 2 years for the brute force method, exceeding our allotted time limit. The Jackson 2.11 mystery was reduced to the form $a-\frac{1}{4 \pi} \frac{a}{b\left(1-a^{2}\right)^{2}}$ by dimensional analysis, corresponding to the string " $a P 0&gt;&gt;&gt;&gt; * \backslash a b a a </em>&lt;a a <em>&lt;</em> * / *-"$ in reverse Polish notation, which would require about 100 times the age of our universe for the brute force method.</p>
<p>It is likely that both of these mysteries can be solved with relatively minor improvements of the our algorithm. The first mystery would have been solved had the algorithm not failed to discover that $a^{2}(1+a) / b^{5}$ is separable. The large dynamic range induced by the fifth power in the denominator caused the neural network to miss the separability tolerance threshold; potential solutions include temporarily limiting the parameter range or analyzing the logarithm of the absolute value (to discover additive separability).</p>
<p>If we had used different units in the second mystery, where $1 / 4 \pi \epsilon$ was replaced by the Coulomb constant $k$, the costly $4 \pi$-factor (requiring 7 symbols " $P P P P+++$ " or " $P 0&gt;&gt;&gt;&gt;$ ") would have disappeared. Moreover, if we had used a different set of function symbols that included " $Q$ " for squaring, then brute force could quickly have discovered that $a-\frac{a}{b\left(1-a^{2}\right)^{2}}$ is solved by " $a a b a Q&lt;Q * /-"$. Similarly, introducing a symbol $\wedge$ denoting exponentiation, enabling the string for $a^{b}$ to be shortened from " $a L b * E$ " to " $a b \wedge$ ", would enable brute force to solve many mysteries faster, including Jackson 2.11.</p>
<p>Finally, a powerful strategy that could ameliorate both of these failures would be to add symbols corresponding to parameters that are numerically optimized over. This strategy is currently implemented in Eureqa but not AI Feynman, and could make a useful upgrade as long as it is done in a way that does not unduly slow down the symbolic brute force search. In summary, the two failures of the AI Feynman algorithm signal not unsurmountable obstacles, but motivation for further work.</p>
<p>In addition, we tested the performance of our algorithm
on the mystery functions presented in [41] ${ }^{6}$. Some equations appear twice; we included them only once. Our algorithm again outperformed Eureqa, discovering $66.7 \%$ of the equations while Eureqa discovered $48.9 \%$. The fact that the AI Feynman algorithm performs less well on this test set than on genuine physics formulas traces back to the fact that most of the equations presented in [41] are rather arbitrary compositions of elementary functions unlikely to occur in real-world problems, thus lacking the symmetries, separability, etc. that the neural network part of our algorithm is able to exploit.</p>
<h2>IV. CONCLUSIONS</h2>
<p>We have presented a novel physics-inspired algorithm for solving multidimensional analytic regression problems: finding a symbolic expression that matches data from an unknown algebraic function. Our key innovation lies in combining traditional fitting techniques with a neural-network-based approach that can repeatedly reduce a problem to simpler ones, eliminating dependent variables by discovering properties such as symmetries and separability in the unknown function.</p>
<p>To facilitate quantitative benchmarking of our and other symbolic regression algorithms, we created a freely downloadable database with 100 regression mysteries drawn from the Feynman Lectures on Physics and a bonus set of an additional 20 mysteries selected for difficulty and fame.</p>
<h2>A. Key findings</h2>
<p>The pre-existing state-of-the-art symbolic regression software Eureqa [26] discovered $68 \%$ of the Feynman equations and $15 \%$ of the bonus equations, while our AI Feynman algorithm discovered $100 \%$ and $90 \%$, respectively, including Kepler's ellipse equation mentioned in the introduction (3rd entry in Table VI). Most of the 100 Feynman equations could be solved even if the data size was reduced to merely $10^{2}$ data points or had percent-level noise added, but the most complex equations needing neural network fitting required more data and less noise.</p>
<p>Compared with the genetic algorithm of Eureqa, the most interesting improvements are seen for the most difficult mysteries where the neural network strategy is repeatedly deployed. Here the progress of AI Feynman over time corresponds to repeatedly reducing the problem to simpler ones with fewer variables, while Eureqa and other</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>genetic algorithms are forced to solve the full problem by exploring a vast search space, risking getting stuck in local optima.</p>
<h2>B. Opportunities for further work</h2>
<p>Both the successes and failures of our algorithm motivate further work to make it better, and we will now briefly comment on promising improvement strategies.</p>
<p>Although we mostly used the same elementary function options (Table I) and hyperparameter settings (Table II) for all mysteries, these could be strategically chosen based on an automated pre-analysis of each mystery. For example, observed oscillatory behaviour could suggest including sin and cos and lack thereof could suggest saving time by excluding them.</p>
<p>Our code could also be straightforwardly integrated into a larger program discovering equations involving derivatives and integrals, which frequently occur in physics equations. For example, if we suspect that our formula contains a partial differential equation, then the user can simply estimate various derivatives from the data (or its interpolation, using a neural network) and include them in the AI Feynman algorithm as independent variables, thus discovering the differential equation in question.</p>
<p>We saw how, even if the mystery data has very low noise, significant de facto noise was introduced by imperfect neural network fitting, complicating subsequent solution steps. It will therefore be valuable to explore better neural network architectures, ideally reducing fitting noise to the $10^{-6}$ level. This may be easier than in many other contexts, since we do not care if the neural network generalizes poorly outside the domain where we have data: as long as it is highly accurate within this domain, it serves our purpose of correctly factoring separable functions, etc..</p>
<p>Our brute-force method can be better integrated with a neural network search for hidden simplicity. Our implemented symmetry search simply tests if two input variables $a$ and $b$ can be replaced by a bivariate function of them, specifically,+- , * or /, corresponding to length-3 strings " $a b+$ ", " $a b-$ ", " $a b *$ " and " $a b /$ " in Reverse Polish Notation. This can be readily generalized to longer strings involving 2 or more variables, for example bivariate functions $a b^{2}$ or $e^{a} \cos b$.</p>
<p>A second example of improved brute-force use is if the neural network reveals that the function can be exactly solved after setting some variable $a$ equal to something else (say zero, one or another variable). A brute force search can now be performed in the vicinity of the discovered exact expression: for example, if the expression is valid for $a=0$, the brute force search can insert additive terms that vanish for $a=0$ and multiplicative terms
that equal unity for $a=0$, thus being likely to discover the full formula much faster than an unrestricted brute force search from scratch.</p>
<p>Last but not least, it is likely that marrying the best features from both our method and genetic algorithms can spawn a method that outperforms both. Genetic algorithms such as Eureqa perform quite well even in presense of significant noise, whether they output not merely one hopefully correct formula, but rather a Pareto frontier, a sequence of increasingly complex formulas that provide progressively better accuracy. Although it may not be clear which of these formulas is correct, it is more likely that the correct formula is one of them than any particular one that an algorithm might guess. When our neural network identifies separability, a so generate Pareto frontier could thus be used to generate candidate formulas for one factor, after which each one could be substituted back and tested as above, and the best solution to the full expression would be retained. Our brute force algorithm can similarly be upgraded to return a Pareto frontier instead of a single formula.</p>
<p>In summary, symbolic regression algorithms are getting better, and are likely to continue improving. We look forward to the day when, for the first time in the history of physics, a computer, just like Kepler, discovers a useful and hitherto unknown physics formula through symbolic regression!</p>
<p>Acknowledgements: We thank Rustin Domingos, Zhiyu Dong, Michael Skuhersky, Andrew Tan and Tailin Wu for helpful comments, and the Center for Brains, Minds, and Machines (CBMM) for hospitality. Funding: This work was supported by The Casey and Family Foundation, the Ethics and Governance of AI Fund, the Foundational Questions Institute, the Rothberg Family Fund for Cognitive Science and the Templeton World Charity Foundation, Inc. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the Templeton World Charity Foundation, Inc. Author contributions: Concept, supervision, project management: M.T. Design of methodology, programming, experimental experimental validation, data curation, data analysis, validation, manuscript writing: S.U. and M.T. Competing interests: The authors declare that they have no competing interests. Data and materials availability: All data needed to evaluate the conclusions in the paper are present in the paper, at https:// space.mit.edu/home/tegmark/aifeynman.html and at https://github.com/SJ001/AI-Feynman. Any additional datasets, analysis details, and material recipes are available upon request.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="nl">Feynman</span><span class="p">:</span><span class="w"> </span><span class="n">Translational</span>
<span class="w">    </span><span class="n">Symmetry</span>
<span class="n">Require</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="err">\{</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="p">)</span><span class="err">\}\</span><span class="p">)</span>
<span class="n">Require</span><span class="w"> </span><span class="nl">net</span><span class="p">:</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span>
<span class="n">Require</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">N</span><span class="w"> </span><span class="n">N</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">error</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">error</span>
<span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">a</span><span class="err">}</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">len</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nl">do</span><span class="p">:</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">j</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">len</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nl">do</span><span class="p">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="o">&lt;</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">\mathrm{i}</span><span class="o">]=</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">\mathrm{i}</span><span class="o">]+</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">a</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">\mathrm{j}</span><span class="o">]=</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">\mathrm{j}</span><span class="o">]+</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">a</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">RMSE</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">RMSE</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">))</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">&lt;</span><span class="mi">7</span><span class="w"> </span><span class="err">\</span><span class="n">times</span><span class="w"> </span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">N</span><span class="w"> </span><span class="n">N</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">error</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]=</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]-</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">delete</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">j</span><span class="err">}\</span><span class="p">)</span>
<span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">AI</span><span class="w"> </span><span class="nl">Feynman</span><span class="p">:</span><span class="w"> </span><span class="n">Additive</span><span class="w"> </span><span class="n">Separability</span>
<span class="n">Require</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="err">\{</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">y</span><span class="err">}</span><span class="p">)</span><span class="err">\}\</span><span class="p">)</span>
<span class="n">Require</span><span class="w"> </span><span class="nl">net</span><span class="p">:</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span>
<span class="n">Require</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">N</span><span class="w"> </span><span class="n">N</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">error</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">error</span>
<span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">e</span><span class="w"> </span><span class="n">q</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}\</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">len</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nl">do</span><span class="p">:</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">e</span><span class="w"> </span><span class="n">q</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">mean</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">len</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nl">do</span><span class="p">:</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">c</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">combinations</span><span class="p">(</span><span class="o">[</span><span class="n">1,2, \(\ldots\), len \((\mathbf{x})</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">i</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="nl">do</span><span class="p">:</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="n">idx</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="o">=</span><span class="n">k</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">[</span><span class="n">1, \operatorname{len}(\mathbf{x}))\) not in idx \(_{1}\)</span>
<span class="n">        for j in idx \(_{1}\)</span>
<span class="n">            \(x_{1}[j</span><span class="o">]=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">mean</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">idx</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="err">:</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="o">[</span><span class="n">j</span><span class="o">]=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">mean</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="o">[</span><span class="n">j</span><span class="o">]</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">RMSE</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="n">e</span><span class="w"> </span><span class="n">q</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">RMSE</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">net</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">x</span><span class="err">}</span><span class="p">))</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">&lt;</span><span class="mi">10</span><span class="w"> </span><span class="err">\</span><span class="n">times</span><span class="w"> </span><span class="err">\</span><span class="n">mathbf</span><span class="err">{</span><span class="n">N</span><span class="w"> </span><span class="n">N</span><span class="err">}</span><span class="n">_</span><span class="err">{\</span><span class="nc">text</span><span class="w"> </span><span class="err">{</span><span class="n">error</span><span class="w"> </span><span class="err">}}\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">delete</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">index</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="o">=</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">delete</span><span class="err">}\</span><span class="nf">left</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">index</span><span class="err">}</span><span class="n">_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">x_</span><span class="err">{</span><span class="mi">1</span><span class="err">}</span><span class="p">,</span><span class="w"> </span><span class="n">x_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="err">{</span><span class="mi">1</span><span class="err">}\</span><span class="p">),</span><span class="w"> </span><span class="k">index</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">_</span><span class="err">{</span><span class="mi">2</span><span class="err">}\</span><span class="p">)</span>
</code></pre></div>

<p>[1] A. Koyré, The Astronomical Revolution: Copernicus-Kepler-Borelli (Routledge, 2013).
[2] N. M. Amil, N. Bredeche, C. Gagné, S. Gelly, M. Schoenauer, and O. Teytaud, in European Conference on Genetic Programming (Springer, 2009), pp. 327-338.
[3] S. K. Pal and P. P. Wang, Genetic algorithms for pattern recognition (CRC press, 2017).
[4] J. D. Lohn, W. F. Kraus, and D. S. Linden, IEEEAntenna \&amp; Propagation Society Mtg. 3, 814 (2002).
[5] D. S. Linden, in Proceedings 2002 NASA/DoD Conference on Evolvable Hardware (IEEE, 2002), pp. 147-151.
[6] H. Yu and N. Yu, The Pennsylvania State University, University park pp. 1-9 (2003).
[7] S. Panthong and S. Jantarang, in CCECE 2003Canadian Conference on Electrical and Computer Engi-
neering. Toward a Caring and Humane Technology (Cat. No. 03CH37436) (IEEE, 2003), vol. 3, pp. 1597-1600.
[8] B. Oh, Y. Na, J. Yang, S. Park, J. Nang, and J. Kim, Advances in Electrical and Computer Engineering 10, 81 (2010).
[9] A. Ram, G. Boone, R. Arkin, and M. Pearce, Adaptive behavior 2, 277 (1994).
[10] B. Delman, Master's thesis, Rochester Institute of Technology (2004).
[11] P. Y. Lu, S. Kim, and M. Soljacic (????).
[12] R. J. Bauer Jr, R. J. Bauer, et al., Genetic algorithms and investment strategies, vol. 19 (John Wiley \&amp; Sons, 1994).
[13] R. Venkatesan and V. Kumar, International Journal of Forecasting 18, 625 (2002).</p>
<p>[14] W. L. Cava, T. R. Singh, J. Taggart, S. Suri, and J. Moore, in International Conference on Learning Representations (2019), URL https://openreview.net/ forum?id=Hke-JhA9Y7.
[15] S. McAleer, F. Agostinelli, A. Shmakov, and P. Baldi, in International Conference on Learning Representations (2019), URL https://openreview.net/forum?id= Hyfn2jCcKm.
[16] J. R. Koza and J. R. Koza, Genetic programming: on the programming of computers by means of natural selection, vol. 1 (MIT press, 1992).
[17] M. D. Schmidt, R. R. Vallabhajosyula, J. W. Jenkins, J. E. Hood, A. S. Soni, J. P. Wikswo, and H. Lipson, Physical biology 8, 055011 (2011).
[18] R. K. McRee, in Proceedings of the 12th Annual Conference Companion on Genetic and Evolutionary Computation (ACM, New York, NY, USA, 2010), GECCO '10, pp. 1983-1990, ISBN 978-1-4503-0073-5, URL http: //doi.acm.org/10.1145/1830761.1830841.
[19] S. Stijven, W. Minnebo, and K. Vladislavleva, in Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation (ACM, New York, NY, USA, 2011), GECCO '11, pp. 623-630, ISBN 978-1-4503-0690-4, URL http://doi.acm.org/10.1145/ 2001858.2002059.
[20] W. Kong, C. Liaw, A. Mehta, and D. Sivakumar, in International Conference on Learning Representations (2019), URL https://openreview.net/forum?id= rkluJ2R9KQ.
[21] T. McConaghy, in Genetic Programming Theory and Practice IX (Springer, 2011), pp. 235-260.
[22] I. Arnaldo, U.-M. O'Reilly, and K. Veeramachaneni, in Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation (ACM, 2015), pp. 983990 .
[23] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Proceedings of the National Academy of Sciences 113, 3932 (2016).
[24] M. Quade, M. Abel, J. Nathan Kutz, and S. L. Brunton, Chaos: An Interdisciplinary Journal of Nonlinear Science 28, 063116 (2018).
[25] D. P. Searson, D. E. Leahy, and M. J. Willis, in Proceedings of the International multiconference of engineers and computer scientists (Citeseer, 2010), vol. 1, pp. 77-80.
[26] R. Dubčáková, Genetic programming and evolvable machines 12, 173 (2011).
[27] M. Schmidt and H. Lipson, Science 324, 81 (2009).
[28] H. Mhaskar, Q. Liao, and T. Poggio, Tech. Rep., Center for Brains, Minds and Machines (CBMM), arXiv (2016).
[29] H. W. Lin, M. Tegmark, and D. Rolnick, Journal of Sta-
tistical Physics 168, 1223 (2017).
[30] T. Wu and M. Tegmark, Physical Review E 100, 033311 (2019).
[31] L. N. Smith and N. Topin, Super-convergence: Very fast training of residual networks using large learning rates (2018), URL https://openreview.net/forum?id= H1A5ztj3b.
[32] L. N. Smith, A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay (2018), 1803.09820.
[33] J. Howard et al., fastai, https://github.com/ fastai/fastai (2018).
[34] R. Feynman, R. Leighton, and M. Sands, The Feynman Lectures on Physics: The New Millennium Edition: Mainly Mechanics, Radiation, and Heat, v. 1 (Basic Books, 1963), ISBN 9780465040858, URL https: //books.google.com/books?id=d76DBQAAQBAJ.
[35] R. Feynman, R. Leighton, and M. Sands, The Feynman Lectures on Physics, no. v. 2 in The Feynman Lectures on Physics (Pearson/Addison-Wesley, 1963), ISBN 9780805390476, URL https://books.google.com/ books?id=AbruAAAAMAAJ.
[36] R. Feynman, R. Leighton, and M. Sands, The Feynman Lectures on Physics, no. v. 3 in The Feynman Lectures on Physics (Pearson/Addison-Wesley, 1963), ISBN 9780805390490, URL https://books.google.com/ books?id=_6XvAAAAMAAJ.
[37] H. Goldstein, C. Poole, and J. Safko, Classical Mechanics (Addison Wesley, 2002), ISBN 9780201657029, URL https://books.google.com/books?id= tJCuQgAACAAJ.
[38] J. D. Jackson, Classical electrodynamics (Wiley, New York, NY, 1999), 3rd ed., ISBN 9780471309321, URL http://cdsweb.cern.ch/record/490457.
[39] S. Weinberg, Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity (New York: Wiley, 1972).
[40] M. Schwartz, Quantum Field Theory and the Standard Model, Quantum Field Theory and the Standard Model (Cambridge University Press, 2014), ISBN 9781107034730, URL https://books.google.com/ books?id=HbdEAgAAQBAJ.
[41] J. McDermott, D. R. White, S. Luke, L. Manzoni, M. Castelli, L. Vanneschi, W. Jaskowski, K. Krawiec, R. Harper, K. De Jong, et al., in Proceedings of the 14th annual conference on Genetic and evolutionary computation (ACM, 2012), pp. 791-798.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We want to thank the anonymous reviewer who brought this data set to our attention.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>3 The 6.5 GB Feynman Database for Symbolic Regression can be downloaded here: https://space.mit.edu/home/tegmark/ aifeynman.html&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
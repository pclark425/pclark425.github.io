<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7620 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7620</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7620</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fineâ€‘tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267200117</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.13256v2.pdf" target="_blank">UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems</a></p>
                <p><strong>Paper Abstract:</strong> â€”Large Language Models (LLMs) have shown exceptional capabilities in many natural language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves better performance than previous strong baselines on the knowledge source selection and response generation task with itself as a retriever in a unified manner, and achieves new state-of-the-art when using more advanced external retriever. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7620",
    "paper_id": "paper-267200117",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0063015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems
24 Jan 2024</p>
<p>Hongru Wang hrwang@se.cuhk.edu.hk 
Yang Deng ydeng@nus.edu.sg 
Rui Wang ruiwangnlp@outlook.com 
Zezhong Wang zzwang@se.cuhk.edu.hk 
Jeff Z Pan 
K.-F Wong kfwong@se.cuhk.edu.hk 
Wenyu Huang w.huang@ed.ac.uk 
Yufei Wang wangyufei44@huawei.com 
Fei Mi mifei2@huawei.com 
Kam-Fai 2018 Wong </p>
<p>The Chinese University of Hong Kong
China</p>
<p>WENYU HUANG
The University of Edinburgh
United Kingdom</p>
<p>National University of Singapore
Singapore</p>
<p>Harbin Institute of Technology
China</p>
<p>The Chinese University of Hong Kong
China</p>
<p>Huawei Noah Ark Lab
YUFEI WANG
China</p>
<p>FEI MI
Huawei Noah Ark Lab
China</p>
<p>The University of Edinburgh
United Kingdom</p>
<p>KAM-FAI WONG
MoE Key Laboratory of High Confidence Software Technologies
The Chinese University of Hong Kong
China</p>
<p>The Chinese University of Hong Kong
999077Hong Kong</p>
<p>The University of Edinburgh
United Kingdom</p>
<p>National University of Singapore
Singapore</p>
<p>R. Wang
Harbin Institute of Technology
ShenzhenChina</p>
<p>Conference acronym 'XX
03-05, 2018June, Woodstock, WangNY</p>
<p>Augmented Generation for Personalized Dialogue Systems. In . ACM
UniMS-RAG: A Unified Multi-source Retrieval
New YorkNY</p>
<p>UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems
24 Jan 2024C174FA436E4D13036EAFECDDC2B043FDarXiv:2401.13256v1[cs.CL]CCS Concepts:Information systems â†’ PersonalizationLanguage modelsâ€¢ Computing methodologies â†’ Discourse, dialogue and pragmaticsPlanning for deterministic actionsNatural language generation Dialogue System, Retrieval-Augmented Generation, Large Language Models, Personalized Response Generation
Large Language Models (LLMs) have shown exceptional capabilities in many natural language understanding and generation tasks.However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system.To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation.We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens.Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements.Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence.In This article substantially extends our previous work published as a conference paper in EMNLP 2023[53].The conference version proposed KBP dataset and SAFARI framework to utilize LLMs as source planner and reader.Compared with the previous version, this manuscript propose UniMS-RAG: a unified multi-source retrieval-augmented generation framework, which 1) introduce additional sub-task: relevance score predication with carefully designed evidence attention mask during the training, enabling itself to serve as planner, retriever and reader in a unified manner.We also explore the effects of different relevance signals, including DPR and LLMs prompting; and 2) contains self-refinement mechanism to iteratively refine the generated response during the inference, considering its relationship between the retrieved evidence (consistency scores) and also the relationship between the dialogue context and retrieved evidence (relevance scores).In addition, we conduct experiments on more personalized datasets to validate the robustness and effectiveness of UniMS-RAG.The proposed framework is further evaluated through extensive experiments in-depth analyses, including automatic evaluation and human evaluation.To conclude, our proposed UniMS-RAG not only substantially modify the training and inference of SAFARI, but also can be more extensible and preferable in personalized dialogue system for better performance and flexibility.</p>
<p>INTRODUCTION</p>
<p>The emergence of large language models (LLMs) has revolutionized the field of natural language processing, including many downstream understanding and generation tasks [3].While these models have undeniably advanced the state of the art in various applications, they also introduce new challenges, particularly the factual error [33,66] and personalization issues [43,57] in the realm of dialogue systems.To alleviate this, Retrieval-Augmented Generation (RAG) methods are usually adopted to retrieve relevant passages, aiming to enrich the semantic information of the dialogue context, resulting in up-to-date, factual and personalized responses.Furthermore, the latest Self-RAG [2] considers the cases that require external knowledge and the cases that do not at the same time, achieving a better trade-off between effectiveness and efficiency.In a world inundated with vast amounts of information, retrieval-augmented dialogue systems play a crucial role in ensuring that automated conversations are not only linguistically proficient but also factually reliable and contextually aware.</p>
<p>There are two limitations.On the one hand, most of the existing knowledge-grounded dialogue systems either focus on a single source of knowledge or indiscriminately incorporate all sources of knowledge.In detail, previous methods usually interact with different external sources of knowledge to engage the user and provide human-like conversational experience, such as system persona source to maintain consistency in responses [64], user memory or profile source for personalized responses [31,43], Wikipedia [7] or the internet [24] source to ensure the provision of up-to-date information.Yet, they focus on individual sources or tasks in isolation, overlooking the complexity and the potential existence of multiple sources of knowledge in real-world scenarios, as shown in Figure 1.In this way, the dialogue system needs to decide which source to use (or not use) dynamically, in order to provide more personalized and helpful responses.Furthermore, the inter-dependency relationship between these different sources of knowledge needs to be considered to guide better response generation [53].For example, as shown in Figure 1b, the dialogue system needs to locate the specific persona config first according to the dialogue context, and then find relevant dependent documents to answer the user inquiry.On the other hand, previous work either independently train the retriever and reader [39,53], resulting in sub-optimal performance and distribution shift between retriever and reader; or design complex architecture to optimize them at the same time [14,42], which is infeasible and inefficient at the era of large language models due to unaffordable computing cost.</p>
<p>Top-n Evidences</p>
<p>These two sources of knowledge are independent (a) An example that the response requires both user persona and system persona information at DuLeMon.</p>
<p>User: â€¦ System: â€¦ User: What's your favorite food?Bot: I like to eat fruits and vegetable.</p>
<p>Top-n Evidences</p>
<p>These two sources of knowledge are inter-dependent 1 st step 2 nd step (b) An example that the response requires both user persona and corresponding documents at KBP.In this way, the retriever need to firstly retrieve related persona (1st step), and then retrieve the document according to dialogue context and retrieved persona information (2nd step).Fig. 1.Two typical examples of multi-source personalized knowledge-grounded dialogues: a): An example from DuLeMon [64]; and b) An example from KBP [53].We use same color to indicate the response and corresponding grounded knowledge.We skip the dialogue context for simplicity.</p>
<p>To tackle the aforementioned challenges, we first decompose the problem of personalized knowledge-grounded dialogue response generation task (PerDS) into three different tasks:</p>
<p>â€¢ Knowledge Source Selection (planner) aims to plan the source call order to guide the knowledge retrieval, considering both the independent or inter-dependent relationships within different sources.</p>
<p>â€¢ Knowledge Retrieval (retriever) sequentially retrieves top-n evidence from external sources according to the decisions in the last step.</p>
<p>â€¢ Response Generation (reader) produces knowledge-grounded natural language responses to users according to original dialogue context and retrieved evidence.</p>
<p>Then we design a novel framework, Unified Multi-Source Retrieval-Augmented Dialogue System (UniMS-RAG), that unifies three tasks in the above using the same large language models in a sequence-to-sequence (Seq2Seq) manner.In specific, motivated by recent work of assigning different tokens with different roles [2,53], we carefully introduce two types of tokens: 1) acting tokens to decide the next action (e.g., which source to use); and 2) and evaluation tokens to evaluate the relevance score between dialogue context and retrieved evidence (e.g., the similarity score).Thus we can reformulate the above three tasks as token prediction tasks by generating acting tokens (planner), evaluation tokens (retriever) or original tokens in the vocabulary (reader) during the training.To further enhance the quality of generated responses, we incorporate a self-refinement process during the inference stage.This involves reassessing the generated responses by leveraging feedback from evaluation tokens and ensuring consistency between the provided evidence and the responses.To sum up, the contributions are summarized as follows:</p>
<p>â€¢ We formally propose a multi-source personalized knowledge-grounded dialogue tasks, consisting of three different sub-tasks: knowledge source selection, knowledge retrieval and final response generation.</p>
<p>â€¢ We propose a novel method, namely, UniMS-RAG, that tackles all sub-tasks in PerDS with a unified model.To the best of our knowledge, it is the first attempt to utilize LLMs as the planner, retriever and reader at the same time.</p>
<p>â€¢ We investigate the different strategies to get the soft label of evaluation tokens during the training stage, including prompting LLMs or using an independent fine-tuned retriever.Furthermore, we propose a self-refinement mechanism to re-generate the response using updated evidence according to its relevance with the dialogue context and previously generated response.</p>
<p>â€¢ Experimental results on two PerDS benchmark datasets show that UniMS-RAG achieves state-of-the-art performance on the knowledge selection and response generation tasks with itself serving as a retriever, resulting in more personalized and factual responses.Extensive analyses provide some new insights into the future of multi-source retrieval-augmented generation tasks.</p>
<p>RELATED WORK</p>
<p>Personalized Dialogue System</p>
<p>To build a personalized dialogue agent, Zhang et al. [69] firstly investigated this task with a new dataset Persona-Chat, where a pre-defined persona set is a form of multiple sentences of textual description.Lots of works follow this setting and have taken mutual persona perception [23,27,63], persona-sparse scenario [47,48,58], long-term persona memory [64], persona extending [29] and persona order bias [5] into consideration.Although some of them complement the insufficient semantics in short persona descriptions by further utilizing an external commonsense knowledge base to extend existing persona sets [29,32], they still fall into the conventional framework coupling the knowledge selection with the response generation [62], rendering it infeasible to handle various sources of knowledge.There have also been works showing that the combination of different knowledge sources such as persona descriptions and Wikipedia can further improve the overall performance [19,60,61].However, they still fail to capture possible dependency between knowledge sources.In their framework, knowledge is not used as the role to assist persona-consistent response generation, but as an additional resource to generate a more informative response [8,66] or select a suitable persona [10,19].Furthermore, most existing works overlook the possibilities that the response does not require the involvement of persona descriptions by simply concatenating all personas with the dialogue context to generate the final response [32,48,63].</p>
<p>Knowledge-grounded Dialogue System</p>
<p>How to interact with different external sources plays a key role in dialogue systems to retrieve corresponding knowledge, resulting in more helpful, personalized, trustworthy responses [18,56].Specifically, most of previous methods rely on different external sources of knowledge to engage the user and improve the conversational experience, including but not limited to system persona to maintain consistency in responses [53], user memory or profile for personalized interactions [65,70], and access to sources like Wikipedia or the internet to ensure the provision of up-to-date information [34,35].</p>
<p>However, most of them focus on individual sources or tasks in isolation, overlooking the complexity and the potential existence of multiple sources in practice.There is a latest work named TPE which regards different knowledge sources as conceptual tools and proposes a multi-persona collaboration framework to model the decision-making process of the call order for multiple knowledge sources [55].We differ in exploring the capability of LLMs to be planner, retriever, reader in a unified manner.</p>
<p>Retrieval-augmented Generation</p>
<p>Retrieval-augmented Generation (RAG) has been considered as an effective method to overcome several limitations of LLMs, such as hallucinations [21,46], factuality [52], long-term memory [64] and etc [11].Usually, an external retriever is first used to retrieve relevant textual knowledge from one specific knowledge source (e.g., Wikipedia), then the reader takes the relevant textual knowledge as external context for generating knowledge-grounded response [25].Most of previous works try to optimize the retriever and reader independently [11].During the initial phases, people use sparse retriever, such as BM25 [41] and TF-IDF [59], to make relevance decisions and retrieve corresponding evidence.</p>
<p>However, sparse approaches fall short in extracting the semantic features inherent in text content [13].To overcome this issue, researchers have proposed language model-based dense retrieval methods by encoding documents and queries as dense vectors, which effectively represent the semantic features of text content [4,22,26].As the first dense retriever, DPR [22] uses two pre-trained language models to encode documents and queries separately, allowing for a more nuanced understanding of the content.In addition, Glass et al. [12] propose a retrieve and re-rank framework for leveraging both advantages of sparse retrieval and dense retrieval.There are also several attempts which optimize the retriever and reader simultaneously [1].For example, Guu et al. [15] augment language model pretraining with a latent knowledge retriever, allowing the model to retrieve and attend over documents used during pre-training, fine-tuning and inference.However, the complex architectures and interactions make them infeasible and inefficient in the era of LLMs.</p>
<p>More recently, there are a handful of works exploring the performance of LLMs as retriever [30,45,50,73].In detail, Shen et al. [45] firstly prove that LLMs can be used as strong zero-shot retriever on several benchmark datasets, while Ma et al. [30] propose Listwise Reranker with a Large Language Model (LRL), which achieves strong reranking effectiveness without using any task-specific training data.More recently, Sun et al. [50] investigate generative LLMs such as ChatGPT [37] and GPT-41 for relevance ranking, and they surprisingly find that properly instructed LLMs can output competitive, even superior results to state-of-the-art supervised methods on popular information retrieval benchmarks.Additionally, they delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models, in order to improve efficiency in real-world applications.Distinguishing from previous works, we finetune LLM itself to learn a joint distribution of dialogue and evidence using similarity feedbacks from current most powerful LLMs, such as ChatGPT and GPT-4.</p>
<p>PROBLEM DEFINITION</p>
<p>To consider the responses which require external knowledge and those which do not in practice, we provide a unified definition to unify the retrieval-augmented dialogue system and non-retrieval dialogue system, following Wang et al. [53].Let   = { 1 ,  denotes the  â„ knowledge in natural language from   .The goal of retrieval-augmented dialogue system2 is to select suitable knowledge sources, and then generate the helpful, informative and personalized response, depending on which source is chosen [56].Thus, the problem of PerDS can be decomposed into the following three tasks:</p>
<p>â€¢ Knowledge Source Selection.At each turn , given the dialogue context   and different knowledge sources  , PerDS first select suitable sources denoted as  âˆˆ {, NULL}.It's important to highlight that  may consist of multiple sources or be NULL.Notably, there are no constraints imposed on the relationships between different sources within .They can either be independent or interdependent.</p>
<p>â€¢ Knowledge Retrieval.The second task is to retrieve top-n corresponding evidence  = { 1 ,  2 , ...,   } for each selected source if there is.We simply skip this step once the PerDS determine there is no need to call external knowledge.</p>
<p>â€¢ Response Generation.The final task is to generate a proper response   concerning the dialogue context   and necessary evidences  from different external knowledge sources  if there is.The generated response is expected to ground on these evidences, being personalized, informative, up-to-date and helpful according to the distinctions across different sources.</p>
<p>METHOD</p>
<p>In this section, we first describe the framework that reformulates each task in PerDS into the unified Seq2Seq paradigm, and then introduce the joint-training strategies for the retriever and reader modules and the inference strategy to re-evaluate the quality of response, respectively.The overview and examples of the input and output sequences for UniMS-RAG are illustrated in Figure 2.</p>
<p>UniMS-RAG Framework</p>
<p>Building upon the foundation of SAFARI [53], we propose an innovative methodology termed UniMS-RAG, where  signifies the unification of the training process for planner, retriever and reader, as well as the integration of diverse tasks into a singular comprehensive framework.Recognizing the potential of large language models (LLMs) in orchestrating the utilization of external sources of knowledge, as indicated by recent works, UniMS-RAG extends the capabilities of LLMs to seamlessly connect disparate sources within the context of personalized knowledge-grounded dialogues.This integration streamlines the traditionally separated tasks of retriever and reader training, enabling to adaptively retrieve evidences and evaluate the relevance scores in a unified manner.</p>
<p>To address the interactions between different subtasks, as illustrated in Figure 2, the whole response generation can be modelled into three steps in UniMS-RAG: 1) Planning, to make a series of decisions about whether to use a specific knowledge source given relationship descriptions between different sources; 2) Retrieval, to retrieve top-n results from external databases according to the decisions; 3) Generation, to incorporate all retrieved knowledge (if required) into Large Language Models (LLMs)
Dialog Context PERSONA DOCUMENTS MEMORY ...</p>
<p>Retrieved Results</p>
<p>PERSONA DOCUMENTS</p>
<p>System Response</p>
<p>Retriever</p>
<p>Evidence Attention Mask</p>
<p>ChatGPT DPR</p>
<p>Evidence Attention Mask
Context, Source, ğ¸vidence ! , ğ‘†ğ‘–ğ‘š ! ğ¸vidence " , ğ‘†ğ‘–ğ‘š " ğ¸vidence # , ğ‘†ğ‘–ğ‘š # Context, Source, ğ¸vidence ! , ğ‘†ğ‘–ğ‘š ! ğ¸vidence " , ğ‘†ğ‘–ğ‘š " ğ¸vidence # , ğ‘†ğ‘–ğ‘š # Context, Source, ğ¸vidence ! , ğ‘†ğ‘–ğ‘š ! ğ¸vidence " , ğ‘†ğ‘–ğ‘š " ğ¸vidence # , ğ‘†ğ‘–ğ‘š # Sources Ã  Fig. 2.
Our proposed method UniMS-RAG, where three optimization tasks are carefully designed: 1) Knowledge Source Selection; 2) Relevance Score Prediction; and 3) Response Generation.</p>
<p>the final response generation.Taking advantage of the decoupling of these different tasks, the UniMS-RAG framework exhibits versatility and scalability in its applicability to various retrieval-augmented response generation tasks.For example, it can be achieved through targeted modifications to the Planning step.By configuring decisions within this phase, the model can seamlessly accommodate different retrieval-augmented tasks without necessitating extensive adjustments to other components.</p>
<p>4.1.1Planning.First of all, to incorporate the cases which does not require any sources of external knowledge, we define several additional indicate tokens, corresponding to different sources, including the NULL token which signifies that there is no requirement for any external knowledge as shown in Table 1:
M : ğ’„ â†’ NULL,(1)
Where M is parameterized by LLMs.Secondly, there are two different situations between these multiple knowledge sources: 1) the  1 ,  2 , ...,   in  is independent, which means there is no interdependent relationship between them;</p>
<p>and 2) Some or even all of they are not independent, for example, the results obtained from  1 may be contingent on the outcomes derived from  2 and potentially other sources.These two situations cater to different applications.In the first scenario, the independence between knowledge sources offers practical utility for tasks where autonomy and isolation of information are paramount, such as user persona and system persona, which are two independent knowledge sources.</p>
<p>On the other hand, in the second scenario, where interdependencies appear, the model accommodates tasks that demand a nuanced consideration of the relationships between different knowledge sources.For example, dependencies between user memory or persona and document introduce a layer of complexity, reflecting real-world scenarios where the fact that age, hobby, education, and life experience of the user have a major effect on his or her personal preference over external document knowledge [10].</p>
<p>In order to handle both independent and interdependent knowledge sources, thereby extending its applicability across a spectrum of use cases with varying degrees of complexity, the goal of the planning step is to make a series of decisions to decide whether or not the corresponding source of knowledge is required and determine their call order if needed.Since the dependency relationship is previously known, we only need to make sure that a certain knowledge source is called after the sources it depends on.Thus, we formulate this task as sequence-to-sequence generation by directly outputting required sources in execution order as follows:
M : ğ’„ â†’ ğ¾ ğ‘– , ğ¾ ğ‘— , ..., ğ¾ ğ‘› ,(2)
Then we strictly follow the outputed order to retrieve evidences from corresponding source of knowledge.To offer 4.1.2Retrieval.According to the output of the last planning step, there are two cases in this step: (1) the response does not need any external sources of knowledge, and the dialogue system can skip this step; and (2) the response needs multiple sources of knowledge, and it strictly follows the output source order to retrieve top-n related evidence  *  for the  â„ source of knowledge according to the dialogue context .If there is a dependency here, it will use preceding retrieved results  *  in the planned execution order as a filter.Specifically, assuming the output order is PERSONA, DOCUMENTS in the planning step for a persona-consistent dialogue system, we first retrieve top-1 result  * from PERSONA, and then we retrieve  * from DOCUMENTS according to  and  * (Figure 1b).If there is no dependency, it simply retrieves corresponding evidences source by source (Figure 1a).</p>
<p>To joint train the retriever and reader in a unified manner, we first reformulate the traditional classification task (relevant or irrelevant) for retrieval into a generation task.Then we define several similarity tokens as shown in Table 1, ranging from 0 to 1 with the internal as 0.1.In this way, we enforce the UniMS-RAG to predict the similarity score by generating corresponding tokens.To get the supervised label of similarity scores, there are two different ways: 1) the similarity scores are generated by independent trained retriever such as DPR [22]; 2) the similarity scores are generated by prompting LLMs [51,73].The details can be found in Â§ 4.2.Furthermore, we design a evidence attention mask mechanism to mask unrelated evidence when predicting the similarity score for current evidence, aiming to reduce unnecessary noises in the context as shown in Figure 2.
M : ğ’„, ğ¾ ğ‘– , ğ‘’ ğ‘— â†’ ğ‘ ğ‘–ğ‘š âˆˆ {0.1, 0.2, ..., 1.0},(3)
In this way, UniMS-RAG can be used to evaluate the relevance between the dialogue context and retrieved evidences after training, serving as a retriever itself.During the inference, we can use it to retrieve top-n evidences { 1 , ...,   } from corresponding sources of knowledge according to Eq. 3.</p>
<p>4.1.3Generation.We concatenate all preceding results, including the names of sources, retrieved evidences and corresponding similarity scores, all together with the dialogue context  to generate the response:
M : Inp â†’ ğ‘  ğ‘¡ ,(4)
where
Inp = {ğ‘ª ğ‘¡ [SOURCE]ğ¾ ğ‘– , ..., ğ¾ ğ‘› [EOS] [EVIDENCE]ğ‘˜ ğ‘— ğ‘– [EOE][ğ‘†ğ‘–ğ‘š 1 ], ..., [EVIDENCE]ğ‘˜ ğ‘š ğ‘› [EOE][ğ‘†ğ‘–ğ‘š ğ‘› ]}.
We use [EVIDENCE] and [EOE] to represent the start and end positions of the retrieved evidences.The [  ] stands for the similarity score of  â„ evidence, calculated using the retriever ( Â§4.2).It is worth noting that the order of evidence is not associated with the similarity score.The formation of the input in this way has three advantages.Firstly, the name of the sources indicates the type of results retrieved, which provides more signals to the LLMs.Secondly, it enforces the LLMs to capture the important evidence since it can appear in any position and denoise unrelated evidence.Thirdly, it allows us to train the large language model in a multi-task manner using teacher forcing as introduced in next section.</p>
<p>Training Stage</p>
<p>In this section, we first introduce how to acquire the soft signals of these relevance scores and then present the global training objectives of our proposed UniMS-RAG.</p>
<p>Relevance Scores Acquisition.</p>
<p>There are two ways to get the soft labels to indicate the relevance between the dialogue context and evidence: 1) using a fine-tuned DPR [22] model; and 2) directly prompting the LLMs (i.e, ChatGPT) [30,50].Here we present the details of these methods one by one.</p>
<p>(1) DPR as a Retriever: Following previous works [22], we can train an off-the-shelf retriever independently.Specifically, we first build our own finetuning dataset by regarding (context, related_evidence) as the positive and (context, unrelated_evidence) as the negatives.Then we apply the following negative log likelihood (NLL) loss to learn the retrieval-oriented representations following [67]:
L ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ = âˆ’ğ‘™ğ‘œğ‘” ğ‘’ ğ‘ ğ‘–ğ‘š (ğ¶ ğ‘– ,ğ‘‘ + ) ğ‘’ ğ‘ ğ‘–ğ‘š (ğ¶ ğ‘– ,ğ‘‘ + ) + ğ‘— âˆˆ B âˆ’ ğ‘’ ğ‘ ğ‘–ğ‘š (ğ¶ ğ‘– ,ğ‘‘ âˆ’ ) ,(5)ğ‘ ğ‘–ğ‘š(ğ¶, ğ‘‘) = ğ‘ğ‘œğ‘  (ğ¶, ğ‘‘)) (6)
where sim is a similarity function, B is a mini-batch of examples,  + and  âˆ’ are positive evidence and negative evidence for  â„ dialogue context   .Once there are no negatives or the negative is the same as the positive, we directly use randomly sampled sample from other session.By including mini-batch negative candidates in the training data, the model is forced to learn to identify the subtle and key information required for the current turn instead of useless or redundant ones.After training, we can use the fine-tuned DPR to provide relevance labels to train the UniMS-RAG.</p>
<p>(2) LLMs as a Retriever: Besides these supervised training methods for retriever, we can utilize some off-the-shelf methods to get the similarity score between the dialogue context and the evidence, including some sparse retriever such as TF-IDF [49] and BM25 [41], or simply hard label by assigning a similarity score to the evidence based on whether it</p>
<p>The following is a dialogue between the user and the system: {dialogue_context}</p>
<p>There is a series of evidences that may be related to the current conversation: {evidence_1} {evidence_2} â€¦â€¦ {evidence_n}</p>
<p>Please sort these documents according to their relevance to the current dialogue context, and please output them in evidence order.Relevance is measured as a score, with intervals of 0.1, ranging from 0 (extremely irrelevant) to 1 (extremely relevant).Please output the document serial number first, and then output the corresponding similarity score.</p>
<p>[1]: 0.  is used (set score as 1) or unused (set score as 0).Inspired by recent studies which simply using LLMs as search engine to predict the similarity relationship between query and evidence [30,51], we also choose to prompt the LLMs (i.e, ChatGPT) to predict the similarity score by feeding a dialogue context and several evidences in a zero-shot manner.</p>
<p>The prompts are shown in Figure 3.</p>
<p>During the inference stage, three types of retrievers can be employed: 1) DPR, 2) LLMs, and 3) UniMS-RAG.It is worth noting that UniMS-RAG can be considered a distillation derived from either DPR or LLMs.</p>
<p>Training</p>
<p>Objectives.The whole training objective of UniMS-RAG can be defined as follows:
L = L ğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ + L ğ‘ ğ‘–ğ‘š + L ğ‘Ÿğ‘’ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘’(7)
Where the   ,   ,   indicate planning loss, relevance predication loss and the final response loss.By assigning different tokens different roles, our UniMS-RAG can serve as the planner (determine whether or not use external sources of knowledge and then use which source), retriever, and reader in a unified manner.</p>
<p>Inference Stage</p>
<p>In this section, we introduce how to refine the generated responses, considering the similarity score between dialogue context and retrieved evidences and consistency score between retrieved evidences and generated responses.Algorithm 1</p>
<p>shows the details of the whole processing.Specifically, we first calculate the consistency score between each retrieved evidence (  ) and current generated response ( ) using a fine-tuned natural language inference model (see Â§ 5.4).Next, we determine the overall score for each evidence by multiplying its similarity score with the consistency score.This combined score serves as an indicator of the evidence's quality, taking into account its relevance to the dialogue context and its alignment with the generated response.In this way, we can adjust the evidences based on their determined quality and a predefined update number, resulting in the creation of a new list of evidences.This process ensures that the most relevant and high-quality pieces of evidence are given priority, contributing to the refinement of the overall   .(  ) 5: end for 6: S =   *   // element-wise product inspired by  ( |) =  (, ) ( |, ) 7: S = sorted (S) // sort the above scores 8:   = Find (, S, ) // find the evidences need to be updated according to the sorted scores 9:  =  \   // pop all evidences need to be updated from current one 10: while   â‰  âˆ… do   = R (C, K) // retrieve next novel evidence from corresponding source of knowledge according to the dialogue context 13:  =  âˆª {  } 14: end while 15:   =  (, ).16: return   generated responses.Ultimately, we can re-generate the responses   using a new list of evidences, providing the dialogue context.Moreover, we can iteratively apply the self-refinement algorithm by incorporating the newly generated responses, denoted as   , and the updated list of evidences.We provide a thorough analysis at Â§ 7.2, outlining the rationale behind the selection of update numbers and the procedural steps involved in refining the responses.</p>
<p>EXPERIMENTAL SETUPS</p>
<p>Research Questions</p>
<p>The empirical analysis targets the following research questions:</p>
<p>â€¢ RQ1: Can large language models serve as a planner to determine whether or not require knowledge, which source of knowledge to call, and when to call? â€¢ RQ2: Can large language models serve as a retriever to retrieve highly related evidence from corresponding sources of knowledge?â€¢ RQ3: Can large language models serve as a reader to capture related information in the context and incorporate them into the final response generation?</p>
<p>Datasets</p>
<p>We consider two different situations between different knowledge sources, corresponding to two publicly available personalized dialogue datasets: DuLeMon3 [64] and KBP4 [53].The dataset statistics are presented in Table 2.We adopt the same train/val/test split in the original datasets.â€¢ DuLeMon [64] is the latest open-domain dialogue dataset with long-term persona memory in which a response is grounded on persona information that occurred in historical sessions, leading to better dialogue engagement.</p>
<p>There are two versions of DuLemon: Self and Both, where the persona information comes from only self side (user side) or both side (both user and chatbot side).We choose Both versions to consider the independent relationship between these two sources of persona information: User side (User-Per) and Chatbot Side (Bot-Per).â€¢ Knowledge Behind Persona (a.k.a., KBP) [53] is a dialogue dataset, in which the response is grounded on both the persona (Persona) and its corresponding implicit knowledge (Document).We choose it to consider the interdependent relationship between different sources of knowledge.It is worth noting that both of these two datasets contain cases which do not require any external source of knowledge, denoting as NULL.</p>
<p>Baselines and Evaluation Metrics</p>
<p>Knowledge Source Selection.</p>
<p>We first include two methods to select different sources of knowledge for comparisons:</p>
<p>â€¢ BERT [6] We utilize BERT as the backbone to train a classifier, which determines the required sources.We formulate this task as a multi-label classification task to determine whether each source is required or not required.If none of them is required, then the used source is NULL.We choose different thresholds for different datasets according to the performance at validation dataset 5 .</p>
<p>â€¢ SAFARI [53] is the first work to formulate the source planning task as a sequence generation task, regarding different sources of knowledge as different tokens in vocabulary at LLMs.Following previous works [53], we adopt F1 as automatic evaluation metrics.</p>
<p>Knowledge</p>
<p>Retrieval.We compare the performance of retrieval with different types of retrievers:</p>
<p>â€¢ BM25 [41] is a type of sparse retriever, which takes into account term frequencies and inverse document frequency (TF-IDF) to score the relevance of documents to a query.</p>
<p>â€¢ RocketQAv2 [40] is a type of dense retriever, which is a unified list-wise training approach for both the retriever and the re-ranker.</p>
<p>â€¢ DPR [22] is a method that leverages dense vector representations for passages in a collection.It uses pre-trained language models to encode passages and queries into dense vectors independently, allowing for a more nuanced understanding of the content.</p>
<p>â€¢ ChatGPT 6 We utilize the same instruction as shown in Figure 3 to prompt gpt-3.5-turbo-1106 to retrieve top-n evidence from corresponding source of knowledge.
C(ğ‘Ÿ, ğ‘”) = ï£± ï£´ ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£´ ï£³
NLI(, ), if there is grounding  for  , and planning also uses  â€² .0, if there is grounding  for  , and planning does not use  â€² .0, if there is no grounding  for  , and planning uses  â€² .1, if there is no grounding  for  , and planning does not use  â€² .</p>
<p>(11)</p>
<p>We have chosen Recall@1 as our primary evaluation metric.This selection is motivated by the predominant use case scenario observed in the original datasets, where a single evidence from each knowledge source is typically sufficient.</p>
<p>Response Generation.</p>
<p>We mainly compare two methods due to limited work focus on our target problem:</p>
<p>â€¢ FoCus [20] aims to minimize the negative log-likelihood of language modeling and sub-tasks: persona grounding and knowledge grounding.It uses either the sigmoid or softmax functions to select evidence by concatenating evidence and dialogue context, and then generate the final response.We do not report the performance on DuLeMon dataset since most of responses in this dataset do not require external sources.</p>
<p>â€¢ SAFARI [53] incorporates all retrieved evidences with corresponding source signals to generate the final responses, without considering the relevance between different evidences and dialogue context explicitly.We report the performance of supervised and unsupervised SAFARI, following Wang et al. [53].</p>
<p>Following Wang et al. [53] and Xu et al. [64], we choose BLEU, Rouge-L to evaluate the gap between generated response with the ground truth response in the original datasets.Besides that, we select Persona Consistency (P.C) and Knowledge Consistency (K.C) to evaluate the consistency score using our finetuned NLI models [31].Specifically, there are four cases during experiments as shown in Eq 11: 1).there exists ground-truth grounding  for the response  in the test set and the planning also decides to retrieve ground knowledge  â€² from corresponding sources; 2).there is  for  , but the planning decides not to use  â€² ; 3).there is no grounding  for  , while the planning decides to use  â€² ; 4).there exists no , and the planning step decides not to use  â€² either.We only calculate the NLI score of (,  ) using finetuned NLI model in the first case of Eq 11.With this definition, we can get the calibrated score P.C7 and K.C:
NLI(ğ‘Ÿ, ğ‘”) = ï£± ï£´ ï£´ ï£² ï£´ ï£´ ï£³ 1, if ğ‘Ÿ entails ğ‘” 0, if ğ‘Ÿ does not entail ğ‘” (8) P.C = ğ‘š ğ‘– ğ¶ (ğ‘Ÿ ğ‘– , ğ‘ ğ‘– ) ğ‘š(9)K.C = ğ‘š ğ‘– ğ¶ (ğ‘Ÿ ğ‘– , ğ‘˜ ğ‘– ) ğ‘š(10)</p>
<p>Implementation Details</p>
<p>UniMS-RAG: We mainly choose ChatGLM-6B [9,68] as the backbone models during training, we set the batch size as 8, train models with 3 epochs and save the checkpoint with the lowest validation loss.For other hyper-parameter settings, we mainly follow the corresponding official code8 .Due to the computation limit, we conduct training with LoRA [17] at one single 3090 GPU, and it costs about 4-6 hours.For the prompting using ChatGPT, we choose gpt-3.5-turbo-1106</p>
<p>and set both the temperature and top p as 0.1 to reduce the randomness of LLMs.We set the number of retrieved results as 3 to evaluate whether or not the UniMS-RAG can denoise the unrelated evidence.According to different sources of  Others: We finetune RocketQAv2 and DPR using samples from corresponding dataset by regarding (context, used_persona / document) as the positive and (context, unrelated_persona / document) as the negative.We set epochs as 5 and max sequence length as 512, and mainly follow these codebases9 for other parameters.For RocketQAv2, we load the weights of pre-trained model zh_dureader_de_v2 as introduced in the official homepage, which is trained on the largest Chinese QA dataset, and we use 12-layer bert-base-chinese with 110M parameters as backbone model for DPR.We then finetune an NLI model [31] by regarding (ground_persona / document, response) as the positive and randomly sampled (unrelated_persona / document, response) as the negative.We also use bert-base-chinese as the backbone model.We concatenate and encode the ground persona/document  and response  in the form of
[CLS]ğ‘˜ [SEP]ğ‘Ÿ [SEP],
and we train the model to predict whether responses are consistent with corresponding personas or documents.The batch size for fine-tuning is 8.The maximal training epoch is 5, and the max sequence length of the encoder is 512.</p>
<p>In the experiments, we use the AdamW optimizer with a learning rate of 2e-5 and an epsilon of 1e-6.We evaluate the NLI model on the KBP test set every 500 iterations during training, and we save the checkpoint with the highest performance on the test set.The fine-tuned NLI model achieves &gt; 95% accuracy for both datasets.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>In this section, we present the performance of our proposed method UniMS-RAG at different sub-tasks, including the final end-to-end generation task.</p>
<p>Performance of Planning (RQ1)</p>
<p>There are different types of planning decisions in the different datasets: DuLeMon (using NULL, User-Per, Bot-Per and Both sources of knowledge) and KBP (using NULL, Persona, and Both).Table 3 demonstrates the F1 of planning using different methods under these two datasets.Specifically, we found that SAFARI is comparable with the BERT model, and furthermore UniMS-RAG achieves better performance than SAFARI on DuLeMon but slightly worse on KBP.In addition, we also found that the original data distribution has a serious impact on the final planning performance.For example, there are less than 0.1% samples in the training set of DuLeMon requiring Both source of knowledge, resulting in poor performance of all methods at Both.A similar phenomenon is also observed on the KBP dataset.Another reason behind this could be the additional token (i.e., evaluation tokens) introduced during the training stage compared with SAFARI.In general, the planning capability of LLMs still needs to be improved to solve the complex multiple sources planning problem in a dialogue system, particularly when dealing with imbalanced or scarce data sources.</p>
<p>Performance of Retrieval (RQ2)</p>
<p>To investigate the RQ2, we examine different types of retrievers, including BM25, RocketQAv2, DPR, ChatGPT and our proposed UniMS-RAG to evaluate the retrieval performance, providing the ground-truth planning labels (except NULL).Table 4 presents the Recall@1 (R@1) of the different retrievers.</p>
<p>The performance of baselines.</p>
<p>There are several observations can be concluded from the results: 1) The overall performance of dense vector (e.g, RocketQAv2 and DPR) is better than ChatGPT, and the performance of ChatGPT is better than sparse retriever (e.g, BM25).The gap of former is bigger than the latter, indicating the large improvement room for current methods using LLMs as retriever, particularly at the context of conversational search; 2) It is observed that DPR performs the best out of these retrievers on KBP datasets while RocketQAv2 performs the best on DuLeMon dataset, revealing the importance of dense retrieval models in this task.We attribute the higher performance of RocketQAv2 on DuLeMon to the similar distribution between DuLeMon dataset and pre-training corpus of RocketQAv2;</p>
<p>3) On KBP datasets, all retrievers performs best at Both-PERSONA and worst at Both-DOCUMENTS, indicating the difficulty to retrieve independent source of knowledge since the semantics between different knowledge from Both-DOCUMENTS are similar to the same underlying persona  * , making them more difficult to distinguish.On the other hand, the performance on DuLeMon is even worse than Both-DOCUMENTS on KBP.We suspect this is due to the semantic gap between the dialogue context and used persona in DuLeMon (as shown in Figure 1).</p>
<p>6.2.2</p>
<p>The performance of UniMS-RAG.Besides, we also present the performance of UniMS-RAG as a retriever using different similarity signals (DPR or ChatGPT) during the training.The results show that UniMS-RAG can be directly used as a retriever, since the performance of both UniMS-RAG w/ DPR and UniMS-RAG w/ ChatGPT are better than sparse retriever (BM25).In detail, UniMS-RAG w/ DPR even achieves better performance than ChatGPT, revealing the great potential of UniMS-RAG as retriever once we use more high-quality signals (the performance of DPR is better than ChatGPT).Furthermore, UniMS-RAG w/ ChatGPT achieves comparable performance with ChatGPT on DuLeMon and KBP datasets.Since the range of similarity signals provided by ChatGPT is much smaller than DPR, we believe that UniMS-RAG can distill the capabilities of original retriever as much as possible if we can provide more data and more fine-grained signals.Therefore, we can derive the answer to RQ2 from this analysis: Large Language Models (LLMs) can serve as retrievers directly, achieving comparable performance compared with original retrievers, showcasing a great potential towards a more powerful unified RAG framework.</p>
<p>Performance of Generation (RQ3)</p>
<p>To investigate the performance of UniMS-RAG on the response generation task, we conduct two settings: 1) using itself as retriever, which means we use UniMS-RAG to retrieve corresponding evidences from the planned sources of knowledge (i.e, the output of planning step); and 2) using independent retriever (i.e, BM25, DPR, ChatGPT) to retrieve corresponding evidences from the planned sources of knowledge (i.e, the output of planning step).Table 5 demonstrates the performance of response generation under both supervised and unsupervised settings.</p>
<p>6.3.1</p>
<p>The performance of baselines.On the one hand, we can find that supervised methods mostly achieve better performance than unsupervised methods except the BLEU-1 and Rouge-L of unsupervised SAFARI on DuLeMon dataset.</p>
<p>We carefully check the outputs of unsupervised SAFARI and find that it tends to plan to use source of knowledge ( 70% using both User-Per and Bot-Per) while most of original test samples do not require any sources of knowledge, resulting in extremely low P.C and higher BLEU-1 and Rouge-L.Furthermore, it is evident that SAFARI outperforms FoCus.We emphasise that FoCus treats knowledge selection as a classification task and optimizes it jointly with response generation tasks, leading to efficiency and scalability issues compared with SAFARI and UniMS-RAG.</p>
<p>6.3.2</p>
<p>The performance of UniMS-RAG.Referring to Tabel 3 and Table 4, the performance of the planning step and retrieval step largely affects the results in the final generation step.Specifically, when using itself as retriever, we can find that UniMS-RAG using signals from DPR (w/ DPR) leads to better performance in contrast to ChatGPT (w/ ChatGPT), revealing the effectiveness of better retriever signals.The gap between DPR and ChatGPT on DuLeMon is relatively small since most of cases here do not require the involvement of external sources of knowledge.Thus, we decide to load parameters of UniMS-RAG w/ DPR to conduct evaluation when using independent retriever.In detail, the results again validate the effectiveness of better retriever (w/ DPR &gt; w/ ChatGPT &gt; w/ BM25).We find that the performance gap between different retrievers in DuLeMon dataset is pretty small (less than 1%).We suspect this is highly related to the original data distribution in the test dataset, since most of the samples do not require external persona information.Furthermore, we also find that using independent retriever is mostly better than using itself as retriever except the worst BM25, which is consistent with the findings in the performance of retrieval step.To conclude, it is obvious that our proposed method UniMS-RAG outperforms all other baselines in at least 5 out 7 evaluation metrics no matter using itself as retriever or using independent retriever.Combining the performance of planning, retrieval, and response generation together, we can find that UniMS-RAG is capable of serving as a planner, retriever, and reader in a unified manner, leading to better performance in personalized dialogues.</p>
<p>ANALYSIS AND DISCUSSIONS</p>
<p>In this section, we choose the best model as shown in our previous experiments to investigate the performance changes under different settings (UniMS-RAG w/ DPR).In detail, we start from the performance of our proposed model with different numbers of retrieved evidence ( Â§7.1), then we study the effects of introduced self-refinement mechanism during the inference stage by re-evaluating the relationship between generated response with the dialogue context or retrieved evidence ( Â§7.2).We present the ablation study to show the effectiveness and rationale of UniMS-RAG ( Â§7.2), followed by the results of human evaluation ( Â§7.2).Finally, We conclude this section with case study ( Â§7.5) and error analysis ( Â§7.6).</p>
<p>Different Numbers of Retrieved Results</p>
<p>The number of retrieved results plays a key role in the response generation.Striking a balance between accuracy and recall is essential, as too few results may miss important semantics, while too many can introduce noise.Figure 4 shows the performance of UniMS-RAG under different numbers of retrieved results.In DuLeMon dataset, we observe a slight improvement in performance as the number of retrieved results increases.This improvement is likely attributed to infrequent cases requiring evidence in the original test dataset.On the other hand, in KBP dataset, it is obvious that the performance get notably improvement when the number of retrieved evidence increases from 1 to 2, but then experiences a slight decline upon further increase to 3. We hypothesize that this drop is a result of additional noise introduced as the number of retrieved evidence continues to increase.This suggests a delicate balance must be struck to optimize performance, considering the specific characteristics of each dataset.</p>
<p>Additional Self-refinement during Inference</p>
<p>To investigate the effects of self-refinement during the inferences, we set the number of retrieved evidence as 3 according to the experimental results in the previous section since it leads to the best performance.There are two different ways to refine the generated response: 1) Within self-refinement by providing different number of updated evidences during one-step refinement as shown in Algorithm 1; and 2) Multi-step self-refinement by refining the response step by step while keeping the update number fixed.On the one hand, updating the number of evidence within the self-refinement mostly leads to improvement on the consistency scores, e.g., the P.C in DuLeMon, and P.C, K.C in KBP.We suspect this is due to the introduction of novel high-quality and related evidences.Balancing remaining evidence with newly updated evidence introduces a trade-off.</p>
<p>If the count of updated evidences surpasses a certain threshold, which unfortunately depends heavily on the number of ground truth evidences employed in each sample, there is a risk of filtering out important information from the remaining evidences.In addition, since we do not refine the response if UniMS-RAG considers it does not require any external sources of knowledge (i.e., NULL), the performance of BLEU-1 and Rouge-L on DuLeMon dataset get slightly drop.</p>
<p>On the other hand, similar patterns emerge when we progressively enhance the response through iterative refinement with an update number of 1, underscoring the efficacy of multi-step self-refinements.However, it becomes evident that the advancement achieved through multi-step self-refinement is not uniformly consistent in comparison to within self-refinement.This inconsistency is attributed to the interplay between multiple pieces of evidence, given our choice to set the update number as 1.In summary, we posit that integrating these two distinct refinement mechanisms can synergistically augment the quality of generated responses, rendering them more personalized and aligned with existing knowledge.</p>
<p>Ablation Study</p>
<p>We investigate the effects of individual steps by providing UniMS-RAG model the ground-truth labels from each step to generate the response, enabling us to analyze and understand the specific effects of each step in a clear and systematic way.Table 7 shows the final results.7.3.1 Upper bound of UniMS-RAG.First, we note that the inclusion of ground-truth planning labels or retrieval results casts a positive impact on performance.Planning primarily enhances consistency scores, while grounding retrieval results contributes to BLEU1 and Rouge-L scores.The best results are obtained when both signals are combined, serving as the upper bound of our proposed UniMS-RAG.Table 6.The performance of Generation with Self-refinement during the inference.Specifically, we first report the performance of different number of updated evidences  (Within self-refinement); and then we fix the update number as 1 and iteratively refine the generated response step by step (Multi-step Self-refinement).</p>
<p>Number</p>
<p>DuLeMon KBP 1) removing the noise of irrelevant evidence to predict the similarity score; and 2) the UniMS-RAG can effectively operate as a retriever, as it directly captures the relationship between individual contexts and corresponding evidence, optimizing its retrieval capabilities.We can clearly find that removing the evidence attention mask leads to performance degradation, especially on KBP dataset since there are more cases that require external sources of knowledge.21% on KBP respectively when using all sources.This highlights that indiscriminate use of all sources not only significantly hampers performance when the majority of samples do not necessitate external references (DuLeMon), but also results in a decline even when a substantial portion of samples requires the utilization of sources (KBP).Conversely, a parallel trend is observed when abstaining from using any external sources.When the majority of samples do not require external references, refraining from their usage leads to notable improvement.However, this approach adversely impacts performance when external sources are essential.</p>
<p>BLEU</p>
<p>Human Evaluation</p>
<p>Human evaluation is conducted to evaluate the quality of generated response in terms of three metrics: coherence score (Coh.), persona consistency score (Per.Cs for both DuLeMon and KBP), and knowledge consistency score (Know.Cs only for KBP) 10 .We randomly sample 100 responses from each dataset with grounding information for each model (We choose UniMS-RAG w/ DPR or w/ ChatGPT and using itself as retriever) and ask three well-educated annotators 11 to indicate its coherence score (1-5) and whether the response is consistent with the given persona (1/0), and knowledge (1/0).Table 8 shows the final result.When the number of retrieved evidence is one, we observe there is a significant improvement on DuLeMon and a slight improvement on KBP datasets when using UniMS-RAG w/ DPR.Moreover, when the number of retrieved evidence increases, it is obvious that the performance is largely improved.</p>
<p>This observation reveals the effectiveness and robustness of UniMS-RAG to denoise unrelated information, and capture relevant evidences, thanks to the introduction of task and unique training strategies.In addition, UniMS-RAG w/ ChatGPT performs slightly worse than UniMS-RAG w/ DPR due to worse similarity signals.We also observe that human is more likely to find persona-inconsistent cases [53].There are some responses that have intra-sentence or intra-context contradictions [71], for example, the dialogue responds: "Yes, our license plate starts with 'Yun A'" to the last user turn: "Does your license plate start with 'Yun B'?".</p>
<p>Case Study</p>
<p>To better understand how the proposed UniMS-RAG works, Figure 5 presents a specific dialogue from KBP.At the current turn, the user inquires about the national language of the personalized dialogue system, and the UniMS-RAG determine to call PERSONA source first and then the corresponding DOCUMENT to answer the inquiry.This is correct according to the ground-truth labels in the original dataset.Then using itself as retriever to retrieve top-3 evidences from each source sequentially.However, in this task, we can find that 1) the highly related persona descriptions only get 0.5 in terms of relevance despite it still is the highest; 2) there are several same relevance score of evidences in DOCUMENT source since it is relatively difficult for the model to distinguish the differences.We consider this is also related to the hard negative mining during the retriever training.Finally, given the context, source signals and corresponding evidences and relevance scores, UniMS-RAG is capable to generate persona-and knowledge-consistent responses.In this way, UniMS-RAG can serve as planner, retriever, and reader in a unified manner.</p>
<p>Error Analysis and Limitations</p>
<p>Despite the effectiveness and robustness of the proposed UniMS-RAG framework for PerDS, we would like to better understand the bottleneck and limitations of UniMS-RAG for further improvement in future studies.After analyzing those fail cases, we identify the following limitations and discuss the potential directions:</p>
<p>â€¢ Error Propagation.This is a typical issue of solving multiple tasks in sequential order.As shown in Table 7, there is still a gap (i.e., 15% -20%) between the original UniMS-RAG and the one providing ground-truth planning</p>
<p>User: â€¦ System: â€¦ User: Hi, what are u going to do today?Bot: Hi, Xiaowen, I kind of want to go skydiving.1.My legs are very short.2. I often stay up late.3. I can speak English 4. I want to go skydiving.5.I am from Fujian province.1.I took ballet classes.2. I go dancing on weekends.3. I am a chemistry major and work in a bookstore.4. My name is Xiaowen.</p>
<p>1 .
1
Foshan belongs to the region of South China.2. Vegetarian like to eat fruits and vegetable.3. â€¦. 1.I am vegetarian.2. I comes form Foshan. 3. I like eating, reading, coding.</p>
<p>Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Wang, et al.</p>
<p>the flexibility and scalability to plug in an arbitrary number of sources, we can add   , ...,   and NULL as special tokens into the vocabulary of LLMs as special tokens, and expand the set of tokens on the fly following Hao et al.[16].Besides that, we add other special tokens to indicate the different parts of the input, i.e.,[SOURCE]  and [EOS] to indicate the start and end positions of sources.In this way, LLM can model the dependency between different sources and learn when and how to call certain sources.</p>
<p>Fig. 3 .
3
Fig.3.The instructions for zero-shot retriever used to predict similarity score using off-the-shelf LLMs.The grey and yellow blocks indicate the inputs and outputs of the model.</p>
<p>11 :
11
Pop a evidence   from   , then   =   \ {  } 12:</p>
<p>Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Wang, et al.</p>
<p>Fig. 4 .
4
Fig. 4. The performance of Generation with different number of retrieved evidences on two datasets.</p>
<ol>
<li>3 . 3
33
Compared with simple planning strategies.We additionally conduct experiments by adopting two simple planning strategies: 1) always use all sources, User-Per and Bot-Per in DuLeMon, and Both in KBP; and 2) always do not use sources, to investigate the effectiveness of the introduced planning step.The results show that the performance is decreased by 63.18%-1.23%on DuLeMon and 42.07%-36.</li>
</ol>
<p>ContextUser:Fig. 5 .
5
Fig. 5.The case study (No cherry-picking)</p>
<p>1 ,  2 ,  2 , ...,   } denotes the dialogue context at the current conversation turn , and there are different knowledge sources  = { 1 ,  2 , ...,   }, where   = { 1
ğ‘– , ğ‘˜ 2 ğ‘– , ..., ğ‘˜ğ‘— ğ‘– } indicates the ğ‘– ğ‘¡â„ source's name of ğ‘² . ğ‘˜ğ‘— ğ‘–</p>
<p>Table 1 .
1
Three types of special tokens used in UniMS-RAG, in which the former two stands for acting tokens and evaluation tokens, respectively.Each type uses several tokens to represent its output values. 1 ,  2 ,  3 , ...  } Decides which source to retrieve or not retrieve.
Token Type InputOutputDefinitionsSources {NULL, Similarity ğ‘ª ğ‘ª, ğ‘’ ğ‘– {0.0, 0.1, 0.2, ..., 1.0}ğ‘’ ğ‘– is a useful for current dialogue context.
Indicator-{[SOURCE], [EOS], [EVIDENCE],[EOE]} Indicates the start and end position of different parts.</p>
<p>2, [2]: 0.3, â€¦.. [n]: 0.7</p>
<p>Algorithm 1 Inference with Self-refinement Require: Dialogue Context , UniMS-RAG model , NLI model  , Retriever , Generated Response  , Sources of Knowledge  = { 1 ,  2 , ...,   }, Retrieved Evidences  = { 1 ,  2 , ...,   }, Similarity Scores   = { 1 ,  2 , ...,   }, Update Number  Ensure: Refined Response   .1:   = [] // store all nli scores 2: for  = 1, 2, ...,  do   =  (  ,  ) // get the nli consistency score
3:
4:</p>
<p>Table 2 .
2
Statistics of DuLeMon and KBP Datasets
DatasetDuLeMonKBP#Dialogues30112477Train/Val/Test19437/2407/24169821/1227/1229#Utterances2455448522Source TypesNULL, User-Per, Bot-Per NULL, Persona, DocumentResp w/ sourceâ‰ˆ 49%â‰ˆ 85%</p>
<p>Table 3 .
3
The F1 of different decisions in Planning of different methods under supervised settings.1) DuLeMon: There are 1686 NULL, 505 USER, 219 BOT and 6 BOTH in the ground planning; 2) KBP: There are 181 NULL, 125 PERSONA and 923 BOTH in the ground planning.UniMS-RAG (ChatGPT) and UniMS-RAG (DPR) where the similarity scores come from ChatGPT and DPR, respectively.At the inference stage, we use self-predicted similarity scores to retrieve the evidence, serving as an indicator.More variants or analyses can be found in Â§7.2.
ModelDuLeMonKBPNULLUser-PerBot-PerBothNULLPersonaBothBERT77.39 (1658) 46.17 (279) 39.25 (479)0.0 (0)1.1 (2)42.02 (113) 86.28 (1104)SAFARI75.34 (1298) 24.93 (249) 39.05 (416) 1.74 (453) 47.10 (129)31.96 (69) 86.59 (1031)UniMS-RAG 85.15 (1992) 47.94 (296) 41.50 (128)0.0 (0)50.35 (252)8.82 (11)84.81 (966)similarity scores, we have two variants:</p>
<p>Table 4 .
4
The performance (Recall@1) of Retrieval of different types of retrievers.It is note the UniMS-RAG (w/ DPR) means that the similarity labels come from DPR model during training, and UniMS-RAG (w/ LLM) means that the similarity labes come from ChatGPT model.1) DuLeMon: There are 511 samples require to use User-Per and 225 samples require to use Bot-Per; 2) KBP: There are 125 samples require to use PERSONA and 923 samples require to use BOTH.
ModelDuLeMonKBPUser-Per Bot-Per PERSONA Both-PERSONA Both-DOCUMENTSBM2515.8524.0036.8048.9715.05RocketQAv242.8546.2280.0092.3150.49DPR30.7240.0083.2093.0751.67ChatGPT21.1437.3345.6065.3316.14UniMS-RAGw/ DPR23.8731.1174.4082.8822.32w/ ChatGPT21.5333.3342.4067.6114.41</p>
<p>Table 5 .
5
[53]performance of Generation of different methods, in which part of the results are directly copied from Wang et al.[53].We follow the official code of SAFARI to conduct evaluation on DuLeMon dataset.
ModelsDuLeMonKBPBLEU-1 Rouge-LP.CBLEU-1 Rouge-LP.CK.CUnsupervised SettingSAFARI [53]12.1115.464.9613.7419.6916.92 24.89Supervised SettingFoCus [20]---22.5125.2964.52 17.90SAFARI [53]7.818.7251.1823.8126.7076.99 42.39Using itself as retrieverUniMS-RAGw/ DPR18.4320.3263.1829.6532.4875.51 42.07w/ ChatGPT18.6120.3364.8327.8630.8670.38 36.53Using independent retrieverUniMS-RAGw/ BM2518.5220.3464.0727.3029.2268.99 36.29w/ DPR18.9520.8764.2829.7232.8573.56 45.00w/ ChatGPT18.5320.4765.5629.0431.4672.98 39.62</p>
<p>Table 6 presents the final results.
Conference acronym 'XX, June 03-05, 2018, Woodstock, NYWang, et al.BLEU-1BLEU-118.43 18.78Number=1 Number=2 Number=329.65 33.5Number=1 Number=2 Number=3ROUGE-L32.48 37.3342.07 53.46K.CROUGE-L20.8420.3263.1863.96P.C75.51 79.9P.C(a) Performance on DuLeMon dataset.(b) Performance on KBP dataset.</p>
<p>The effects of evidence attention mask.There are two notable advantages of adding the evidence attention mask:
-1 Rouge-LP.CBLEU-1 Rouge-LP.CK.CUniMS-RAG18.7820.8463.9632.6936.8079.17 53.38Within Self-refinementğ›¼ = 118.2820.1666.5032.6036.7680.23 53.45ğ›¼ = 218.2520.3465.4832.9937.3480.47 54.43ğ›¼ = 318.7120.6365.5633.3337.5681.45 54.84Multi-step Self-refinementStep = 118.2820.1666.5032.6036.7680.23 53.45Step = 218.5320.3865.9033.3937.4879.09 54.52Step = 318.3020.4665.5433.5237.3580.63 54.847.3.2</p>
<p>Table 7 .
7
Ablation study on the impact of different steps and modules in UniMS-RAG.
ModelDuLeMonKBPBLEU1 RougeL P.C BLEU1 RougeL P.CK.CUniMS-RAG18.4320.3263.1829.6532.4875.51 42.07+ Ground Planning18.2720.4370.9229.8232.7387.88 60.21+ Ground Retrieval18.7620.8665.8937.3041.7181.53 66.64+ Ground P &amp; R19.4421.8971.2937.9342.6795.52 87.63w/o Attention Mask18.3220.2062.7028.6931.4474.53 40.93w/o Planningalways use all sources17.4519.291.2329.4732.6473.80 36.21always do not use source17.4019.2069.7826.9928.9114.73 24.89</p>
<p>Table 8 .
8
The results of human evaluation.The inter-agreement is about 82%.
ModelDuLeMonKBPCoh. Per.Cs (%) Coh. Per.Cs (%) Know.Cs (%)FoCus--3.5354.233.8SAFARI2.8414.04.0668.059.1UniMS-RAGNumber of retrieved evidence = 1w/ DPR3.2225.04.1868.160.5w/ ChatGPT 3.3523.04.1065.254.3Number of retrieved evidence = 3w/ DPR3.3862.04.2084.866.7w/ ChatGPT 3.4661.34.1583.261.4
https://openai.com/research/gpt-4
In this context, a non-retrieval dialogue system is viewed as a specific type within retrieval-augmented dialogue systems, distinguished by the absence of a knowledge source, denoted as NULL.
https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2022-DuLeMon
https://github.com/ruleGreen/SAFARI
In detail, we set the threshold as 0.3 in kBP and 0.5 in DuLeMon datasets, respectively.
https://openai.com/chatgpt
It is important to note we merge the two sources of persona information (User-Per, Bot-Per) in DuLeMon into one unified P.C score.
https://github.com/THUDM/ChatGLM-6B
https://github.com/PaddlePaddle/RocketQA, https://github.com/Alibaba-NLP/Multi-CPR/tree/main/retrieval
It is worth noting that Per.Cs and Know.Cs here only stand for the first case in Eq.
.11 The human inspection and annotation were conducted by a reputable data annotation company, and the annotators are compensated fairly based on the market price without revealing any personal information
This paper was partially supported by grants from the RGC General Research Funding Scheme (GRF) 14222922 (CUHK 2151185).Work done when Hongru Wang is remotely visiting The University of Edinburgh.Conference acronym 'XX, June 03-05, 2018, Woodstock, NYWang, et al.and retrieval.To avoid this issue, there are some techniques studied currently, including contrastive objectives[38]and noisy channel models[28].â€¢ Better Planning and Retrieval.As shown in Table5and analysis, better planning and retrieval always lead to better performance, such as the increased number of retrieved evidences, and additional self-refinement mechanism.We also successfully demonstrate that LLMs themselves can be used as retriever simultaneously.Since we observe UniMS-RAG performs dramatically with different dialogue data and different similarity signals, several possible directions are to study how to get more large scale high-quality data or investigate different interactions (i.e, more complex prompting)[44,55]to further enhance the performance of planning and retrieval.â€¢ The Organization of Multiple Sources.We mainly target on four sources of knowledge in this paper (i.e,User-Per, Bot-Per, DOCUMENTS, NULL), however, there are many other useful sources in practice[56], such as user memory[72], local database[54]and so on.In addition, the organization of different sources of knowledge also plays a key role in the RAG, such as the different data format[36,60], or different relationships within sources[55].CONCLUSIONIn this paper, we focus on personalized knowledge-grounded dialogue tasks in a multi-source setting and decompose the problem into three sub-tasks: knowledge source selection, knowledge retrieval and response generation.We discern a notable gap in none of the existing literature concerning the multiple sources planning and auto-regressive retriever with LLMs themselves as backbone.To fill this gap, we propose a Unified Multi-Source Retrieval-Augmented Dialogue System (UniMS-RAG), aiming to build a unified personalized dialogue system with LLM serving as planner, retriever and reader simultaneously.Specifically, we introduce acting tokens to determine which source to call, and evaluation tokens to evaluate the similarity between dialogue context and current retrieved evidence.We explore different similarity signals from DPR or LLMs with evidence mask mechanism to empower LLMs to distinguish the differences between different evidences and filter noises during the training.In addition, we also introduce self-refinement during the inference to iteratively refine the generated responses by leveraging the consistency scores and similarity scores.Experimental results on two popular personalized datasets show that the UniMS-RAG framework can generate more personalized and factual responses and establish a better performance with self-refinement during inference, significantly outperforming strong baseline models under both automatic and human evaluations.This work is the first attempt toward a unified multi-source retrieval-augmented generation framework for PerDS.There are several limitations and room for further improvement.As discussed in Section 7.6, future works can focus on 1) addressing the error propagation of individual steps; 2) further improving the performance of planning and retrieval; and 3) investigating the organization of different sources of knowledge and incorporating more knowledge[56].We will continue to explore better architectures that leverage large language models for these challenges in personalized dialogue tasks.
Retrieval-based Language Models and Applications. Akari Asai, Sewon Min, Zexuan Zhong, Danqi Chen, 10.18653/v1/2023.acl-tutorials.6Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20236Tutorial Abstracts)</p>
<p>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, arXiv:2310.11511[cs.CL]2023</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, arXiv:2302.04023[cs.CL]A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. 2023</p>
<p>An Analysis of Fusion Functions for Hybrid Retrieval. Sebastian Bruch, Siyu Gai, Amir Ingber, 10.1145/3596512ACM Trans. Inf. Syst. 422023. aug 2023Article</p>
<p>Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization. Liang Chen, Hongru Wang, Yang Deng, Wai Chung Kwan, Zezhong Wang, Kam-Fai Wong, 10.18653/v1/2023.findings-acl.462Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, arXiv:1811.01241[cs.CL]Wizard of Wikipedia: Knowledge-Powered Conversational agents. 2019</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, arXiv:1811.01241[cs.CL]Wizard of Wikipedia: Knowledge-Powered Conversational agents. 2019</p>
<p>GLM: General Language Model Pretraining with Autoregressive Blank Infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory. Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, Rui Yan, 10.18653/v1/2022.acl-long.270Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, arXiv:2312.10997[cs.CL]Retrieval-Augmented Generation for Large Language Models: A Survey. 2024</p>
<p>Re2G: Retrieve, Rerank, Generate. Michael Glass, Gaetano Rossiello, Md Faisal, Mahbub Chowdhury, Ankita Naik, Pengshan Cai, Alfio Gliozzo, 10.18653/v1/2022.naacl-main.194Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Semantic Models for the First-Stage Retrieval: A Comprehensive Review. Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, Xueqi Cheng, 10.1145/3486250ACM Trans. Inf. Syst. 402022. mar 2022Article</p>
<p>REALM: Retrieval-Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning (ICML'20). the 37th International Conference on Machine Learning (ICML'20)2020368</p>
<p>REALM: Retrieval-Augmented Language Model Pre-Training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning (ICML'20). the 37th International Conference on Machine Learning (ICML'20)2020368</p>
<p>Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, arXiv:2305.11554[cs.CL]ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings. 2023</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, arXiv:2106.09685[cs.CL]LoRA: Low-Rank Adaptation of Large Language Models. 2021</p>
<p>Challenges in Building Intelligent Open-Domain Dialog Systems. Minlie Huang, Xiaoyan Zhu, Jianfeng Gao, 10.1145/3383123ACM Trans. Inf. Syst. 38212020. apr 2020</p>
<p>Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Donghoon Shin, Seungryong Kim, Heuiseok Lim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Donghoon Shin, Seungryong Kim, Heuiseok Lim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 552482023. mar 2023</p>
<p>Dense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Trevor Cohn, Yulan He, Yang Liu, the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Bonnie WebberOnline2020</p>
<p>Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness. Hyunwoo Kim, Byeongchang Kim, Gunhee Kim, 10.18653/v1/2020.emnlp-main.65Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Internet-Augmented Dialogue Generation. Mojtaba Komeili, Kurt Shuster, Jason Weston, 10.18653/v1/2022.acl-long.579Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-Tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsVancouver, BC, Canada; Red Hook, NY, USACurran Associates Inc202016793NIPS'20)</p>
<p>Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls. Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, 10.1145/3570724ACM Trans. Inf. Syst. 41622023. apr 2023</p>
<p>You Impress Me: Dialogue Generation via Mutual Persona Perception. Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, Dongmei Zhang, 10.18653/v1/2020.acl-main.131Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>Pretraining the Noisy Channel Model for Task-Oriented Dialogue. Qi Liu, Lei Yu, Laura Rimell, Phil Blunsom, 10.1162/tacl_a_00390Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Improving Personality Consistency in Conversation by Persona Extending. Yifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang, Dangyang Chen, 10.1145/3511808.3557359Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge ManagementACM2022</p>
<p>Zero-Shot Listwise Document Reranking with a Large Language Model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.02156[cs.IR]2023</p>
<p>Personalizing Dialogue Agents via Meta-Learning. Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Pascale Fung, 10.18653/v1/P19-1542Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Anna Korhonen, David Traum, LluÃ­s MÃ rquez, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions. Prasad Bodhisattwa, Harsh Majumder, Taylor Jhamtani, Julian Berg-Kirkpatrick, Mcauley, 10.18653/v1/2020.emnlp-main.739Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.546Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>DukeNet: A Dual Knowledge Interaction Network for Knowledge-Grounded Conversation. Chuan Meng, Pengjie Ren, Zhumin Chen, Weiwei Sun, Zhaochun Ren, Zhaopeng Tu, Maarten De Rijke, 10.1145/3397271.3401097Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20). the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR '20)New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, arXiv:2112.09332[cs.CL]WebGPT: Browser-assisted question-answering with human feedback. Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, 2022</p>
<p>Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation. Xuanfan Ni, Hongliang Dai, Zhaochun Ren, Piji Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155[cs.CL]Training language models to follow instructions with human feedback. Jan Leike, and Ryan Lowe. 2022</p>
<p>Soloist: Building Task Bots at Scale with Transfer Learning and Machine Teaching. Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, Jianfeng Gao, 10.1162/tacl_a_00399Transactions of the Association for Computational Linguistics. 92021. 2021</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, arXiv:2302.00083[cs.CL]Context Retrieval-Augmented Language Models. 2023</p>
<p>RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, Ji-Rong Wen, 10.18653/v1/2021.emnlp-main.224Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>The probabilistic relevance framework: BM25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and TrendsÂ® in Information Retrieval. 32009. 2009</p>
<p>Ohad Rubin, Jonathan Berant, arXiv:2306.13421[cs.CL]Long-range Language Modeling with Self-retrieval. 2023</p>
<p>Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani, arXiv:2304.11406[cs.CL]LaMP: When Large Language Models Meet Personalization. 2023</p>
<p>Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, 10.18653/v1/2023.findings-emnlp.620Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor. Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, Daxin Jiang, arXiv:2304.14233[cs.CL]Large Language Models are Strong Zero-Shot Retriever. 2023</p>
<p>Retrieval Augmentation Reduces Hallucination in Conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, 10.18653/v1/2021.findings-emnlp.320Findings of the Association for Computational Linguistics: EMNLP 2021. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data. Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, Ting Liu, 10.18653/v1/2021.acl-long.14Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>A Stack-Propagation Framework for Low-Resource Personalized Dialogue Generation. Haoyu Song, Wei-Nan Zhang, Kaiyan Zhang, Ting Liu, 10.1145/3563389ACM Trans. Inf. Syst. 41682023. apr 2023</p>
<p>A statistical interpretation of term specificity and its application in retrieval. Karen Sparck, Jones , Journal of documentation. 281972. 1972</p>
<p>Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, Zhaochun Ren, arXiv:2304.09542[cs.CL]Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. 2023</p>
<p>Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, Yue Zhang, arXiv:2310.07521[cs.CL]Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. 2023</p>
<p>Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues. Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan, Irwin King, Kam-Fai Wong, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant Towards Customized Dialogue System. Hongru Wang, Min Li, Zimo Zhou, Gabriel Pui, Cheong Fung, Kam-Fai Wong, arXiv:2011.08772[cs.CL]2020</p>
<p>Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi, Kam-Fai Wong, arXiv:2309.16090[cs.AI]TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration. 2023</p>
<p>Hongru Wang, Lingzhi Wang, Yiming Du, Liang Chen, Jingyan Zhou, Yufei Wang, Kam-Fai Wong, arXiv:2311.16789[cs.CL]A Survey of the Evolution of Language Model-Based Dialogue Systems. 2023</p>
<p>Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs. Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, Kam-Fai Wong, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Leveraging Similar Users for Personalized Language Modeling with Limited Data. Charles Welch, Chenxi Gu, Jonathan K Kummerfeld, Veronica Perez-Rosas, Rada Mihalcea, 10.18653/v1/2022.acl-long.122Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Interpreting TF-IDF Term Weights as Making Relevance Decisions. Chung Ho, Robert Wu, Pong Wing, Kam Fai Luk, Kui Lam Wong, Kwok, 10.1145/1361684.1361686ACM Trans. Inf. Syst. 26132008. jun 2008</p>
<p>More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge. Sixing Wu, Ying Li, Minghui Wang, Dawei Zhang, Yang Zhou, Zhonghai Wu, 10.18653/v1/2021.emnlp-main.175Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model. Sixing Wu, Ying Li, Ping Xue, Dawei Zhang, Zhonghai Wu, Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics. the 29th International Conference on Computational Linguistics. International Committee on Computational LinguisticsGyeongju, Republic of Korea2022</p>
<p>KSAM: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding. Sixing Wu, Ying Li, Dawei Zhang, Zhonghai Wu, 10.18653/v1/2022.findings-acl.30Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Chen Xu, Piji Li, Wei Wang, Haoran Yang, Siyun Wang, Chuangbai Xiao, 10.1145/3477495.3531957Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 45th International ACM SIGIR Conference on Research and Development in Information RetrievalACM2022</p>
<p>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, Shihang Wang, 10.18653/v1/2022.findings-acl.207Findings of the Association for Computational Linguistics: ACL 2022, Smaranda Muresan. Preslav Nakov, Aline Villavicencio, Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, Shihang Wang, 10.18653/v1/2022.findings-acl.207Findings of the Association for Computational Linguistics: ACL 2022, Smaranda Muresan. Preslav Nakov, Aline Villavicencio, Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. Boyang Xue, Weichao Wang, Hongru Wang, Fei Mi, Rui Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational Linguistics2023</p>
<p>Few-Shot Conversational Dense Retrieval. Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, Zhiyuan Liu, 10.1145/3404835.3462856Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information RetrievalNew York, NY, USAAssociation for Computing Machinery2021SIGIR '21)</p>
<p>GLM-130B: An Open Bilingual Pre-trained Model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, The Eleventh International Conference on Learning Representations (ICLR). 2023</p>
<p>Personalizing Dialogue Agents: I have a dog, do you have pets too. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston, 10.18653/v1/P18-1205Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Memory-Augmented Dialogue Management for Task-Oriented Dialogue Systems. Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng Ji, Haiqing Chen, Xiaoyan Zhu, 10.1145/3317612ACM Trans. Inf. Syst. 37342019. jul 2019</p>
<p>CDConv: A Benchmark for Contradiction Detection in Chinese Conversations. Chujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng, Zhen Guo, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Minlie Huang, 10.18653/v1/2022.emnlp-main.2Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, arXiv:2305.10250[cs.CL]MemoryBank: Enhancing Large Language Models with Long-Term Memory. 2023</p>
<p>Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.07107[cs.CLLarge Language Models for Information Retrieval: A Survey. 2023. 20 February 2007. 2009. June 2009revised 12 March</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2035 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2035</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2035</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-281830055</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.02752v1.pdf" target="_blank">The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2035.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2035.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-aware RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-aware Reinforcement Learning with Self-Aware Difficulty Prediction and Limit Breaking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven self-evolving RL curriculum where a generator agent (the same LLM) produces tasks and predicts per-task difficulty relative to its current ability, while a solver agent attempts solutions; high-utility unsolvable tasks trigger selective queries to a stronger external solver (limit breaking) to minimally inject supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated tasks with difficulty-based adaptive filtering and selective external guidance (RL-based, adaptive curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>A generator agent (policy π_θ implemented with Qwen2.5-Coder-3B) produces new tasks and a predicted success rate µ(x) for N rollouts. The solver agent samples multiple rollouts to compute the empirical success rate μ(x); the generator receives a difficulty-prediction reward R_dp = 1 - |μ(x) - µ(x)| to align predictions with reality. Each generated task is scored for utility via two dimensions: difficulty (1 - µ(x)) and novelty (token-level perplexity under the current solver policy). Utilities are z-scored within a FIFO task buffer B and mapped to a query probability p(x) via a transformed normal CDF with sharpness γ and probability target τ. If all solver rollouts fail and a Bernoulli draw with p(x) turns positive, an external stronger solver (Qwen2.5-Coder-32B) is queried to provide a correct solution (limit breaking). Generator and solver receive format rewards; solver uses binary outcome reward. RL updates use REINFORCE++.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Qwen2.5-Coder-3B (base generator/solver); Qwen2.5-Coder-32B used as external solver for limit breaking</td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Code generation with python interpreter for verifiable execution outcomes, and mathematical reasoning benchmarks evaluated post-training</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Tasks include multi-step mathematical reasoning (MATH500, AIME, Minerva, AMC'23, OlympiadBench) and code synthesis problems (HumanEval++, MBPP++, LiveCodeBench) that require multi-step planning, compositional reasoning chains, and executable verification; generator controls difficulty across a continuum via predicted success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Qwen2.5-Coder-3B (pretrained base), AZR (Absolute Zero Reasoner) as a self-evolving baseline, self-aware RL − (ablation without limit breaking)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are task-suite accuracy percentages per benchmark. Key reported results: self-aware RL achieves a relative improvement of ~53.8% on average across mathematical reasoning benchmarks vs the Qwen2.5-Coder-3B base, and ~5.3% average improvement on coding benchmarks. Example specific reported gains (from paper text): +29.8% on MATH500, +77.8% on AMC'23, +82.4% on OlympiadBench, +22.3% on LiveCodeBench. The method only queried external guidance for 157 of 12,800 tasks (1.23%). Ablation: self-aware RL without limit breaking still outperforms AZR baseline on mathematical benchmarks by 25.6%. Shuffled utility ranking yields only negligible improvements (math: ~28.0% -> 28.4%; code: ~53.3% -> 53.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Training ran 200 steps on 4x H-200 GPUs; difficulty prediction was trained first and limit breaking was disabled for the first 50 steps. The paper reports that training reward for self-aware RL increases more stably and reaches higher values than the AZR baseline after initial fitting; no wall-clock convergence times or episode counts beyond the 200-step run are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Authors report improved generalization to held-out/out-of-distribution tasks: large relative gains on out-of-distribution code generation benchmarks and out-of-domain mathematical reasoning benchmarks (aggregate math gain ~53.8%). Exact held-out splits not further detailed beyond the nine benchmark evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Qualitative and empirical evidence: the generator initially produced overly-easy tasks (high rollout accuracy), then adapted to produce more challenging tasks (rollout accuracy dropped and stabilized ~0.6). Utility scores of selected (queried) tasks are significantly higher than unselected tasks (Figure 5), showing the filter increases task utility diversity toward valuable examples.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not explicitly evaluated as a prerequisite graph; the difficulty prediction mechanism implicitly identifies tasks at the model's learning frontier by estimating solvability (µ(x)) and prioritizing medium-to-hard but solvable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — the generator produces intermediate difficulty tasks adaptively (via µ(x) guidance). Additionally, external solver-provided solutions for selected high-utility tasks act as bridging examples to break capability ceilings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Pretrained LLMs were initially poor at predicting their own success rates (difficulty prediction accuracy ~0.2 initially, rising >0.6 after ~50 steps), tended to generate tasks that were too easy or too hard without self-awareness, and required disabling limit breaking during early steps due to inaccuracy in µ(x).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Experiment used 4 NVIDIA H-200 GPUs and ran for 200 steps; authors emphasize data-efficiency (only 1.23% of tasks required external guidance). No detailed FLOPs or runtime per-step numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Self-aware RL — combining LLM-generated tasks with explicit difficulty prediction and a selective external-guidance (limit breaking) mechanism — produces an adaptive curriculum that substantially improves mathematical reasoning and modestly improves coding ability while using very little external data (≈1.2% of tasks). Ablations show difficulty prediction alone gives large gains (≈25.6% vs AZR), and selective limit breaking with an appropriate query frequency (τ=0.1) yields further improvement; random utility selection or excessive external guidance degrades benefit.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2035.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2035.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AZR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absolute Zero Reasoner (AZR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior self-evolving RL approach where a generator agent automatically generates coding tasks to train a policy model without external input data; used in this paper as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Absolute zero: Reinforced self-play reasoning with zero data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated tasks (self-play/self-generated) without explicit self-awareness-based difficulty calibration</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>AZR produces tasks automatically from a generator agent and uses them to train a solver agent via RL (self-play / self-generated task loop). The paper notes AZR and similar self-evolving methods do not include explicit self-awareness components to align generated task difficulty to the solver's current capability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Code generation / reasoning tasks (self-generated training data)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Not detailed in this paper beyond general coding/reasoning task generation; characterized as generating tasks without capability-aware difficulty control, often yielding tasks that are too trivial or too hard.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against in experiments as a baseline; self-aware RL (and its ablations) are compared to AZR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports comparative improvements over AZR: self-aware RL − (no limit breaking) outperforms AZR by ~25.6% on mathematical benchmarks. Exact AZR per-benchmark scores are reported in tables but not reproduced in full textual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported in detail in this paper beyond aggregate reward traces showing AZR underperforms self-aware RL in training reward progression.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Reported as lower than self-aware RL on evaluated benchmarks; no further specifics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not analyzed in detail here; AZR is characterized as generating misaligned tasks (too easy or too hard) relative to solver capability.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No explicit mechanism for producing intermediate bridging tasks is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>AZR-style approaches lack explicit self-awareness leading to inefficient curricula (task difficulty misaligned with model frontier).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>AZR-style zero-data self-evolving generators can bootstrap learning but produce curricula that are often misaligned with solver capability; adding self-awareness (difficulty prediction and selective limit breaking) yields substantially improved downstream performance and data efficiency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2035.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2035.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Balanced Online Difficulty Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Balanced Online Difficulty Filtering (Bae et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum learning mechanism that prioritizes tasks of medium hardness to maximize the effectiveness of RL training, proposed to stabilize GRPO-style RLVR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Online difficulty filtering for reasoning oriented reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Difficulty-based online filtering (select medium-hard tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>The method implements a balanced filtering mechanism that maintains a selection bias toward tasks of intermediate difficulty to maximize positive reward signal and learning progress during RLVR (described in related work; not implemented in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM reasoning and RLVR training</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets tasks across a continuum; focuses selection on medium difficulty which purportedly require multiple reasoning steps but remain solvable enough to produce learning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned as a recent curriculum approach; not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (only cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Implicitly aims to surface intermediate tasks by selecting medium difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cited as a curriculum method that improves RLVR by focusing on medium-hard tasks to better exploit reward signal; paper positions it as related prior work but does not provide direct comparisons or metrics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2035.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2035.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Difficulty Curriculum Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Difficulty Curriculum Learning (Zhang et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum approach that periodically re-estimates task difficulty within upcoming data batches and adapts selection to align with the model's evolving capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive Difficulty Curriculum Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Adaptive difficulty re-estimation and selection (difficulty-based ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Imitates human learning by periodically re-evaluating difficulty of forthcoming batches and adjusting selection to remain aligned with the learner's capability frontier; described in related work and cited as inspiration for aligning curricula to evolving LLM abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets multi-step reasoning tasks; designed to keep difficulty matched to evolving learner competency.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned in related work; no direct experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not explicitly detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Approach can produce intermediate tasks implicitly by adjusting difficulty targets over time.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cited as a human-inspired adaptive curriculum strategy that periodically re-calibrates difficulty to the learner; presented as related work motivating the need for dynamic difficulty estimation in self-generated curricula.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2035.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2035.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Evolving Curriculum (Chen et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolving Curriculum for LLM Reasoning (Chen et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum method that concurrently learns an explicit curriculum policy during RL fine-tuning to maximize learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-evolving curriculum for llm reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Learned curriculum policy (meta-curriculum learned alongside RL)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Learns an additional curriculum policy parallel to RL fine-tuning to select tasks that maximize the model's learning progress; cited in related work as a recent approach to automated curriculum design for LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM reasoning and multi-step reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets complex reasoning tasks; curriculum policy aims to expose the model to sequences that foster multi-step capability development.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned in related work; not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Method may implicitly discover beneficial progression but not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Yes — the curriculum policy is intended to produce sequences of tasks that bridge skills, though concrete evaluation not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cited as a technique that learns a curriculum policy jointly with RL to maximize learning progress; the present paper positions its self-aware mechanisms as complementary to or combinable with such self-evolving curriculum policies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2035.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2035.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum Reinforcement Fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curriculum Reinforcement Fine-tuning (Deng et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum learning design that injects difficulty-aware reward shaping to ensure steady progression of model capabilities during RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>Difficulty-aware reward shaping and ordering (curriculum RL)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Proposes difficulty-aware reward design to shape the learning trajectory such that capabilities progress steadily; cited among recent curriculum methods for reasoning and RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Vision-language and LLM reasoning contexts (cited broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Designed to support multi-step reasoning; specifics not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned as related work; not directly compared in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>Not specifically described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Cited as an example of curriculum design that uses difficulty-aware reward shaping to steer progression; included to situate the self-aware RL contribution within prior curriculum-RL literature.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Online difficulty filtering for reasoning oriented reinforcement learning <em>(Rating: 2)</em></li>
                <li>Adaptive Difficulty Curriculum Learning <em>(Rating: 2)</em></li>
                <li>Self-evolving curriculum for llm reasoning <em>(Rating: 2)</em></li>
                <li>Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning <em>(Rating: 2)</em></li>
                <li>Absolute zero: Reinforced self-play reasoning with zero data <em>(Rating: 2)</em></li>
                <li>Webevolver: Enhancing web agent self-improvement with coevolving world model <em>(Rating: 1)</em></li>
                <li>WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning <em>(Rating: 1)</em></li>
                <li>ZeroGUI: Automating online GUI learning at zero human cost <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2035",
    "paper_id": "paper-281830055",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "Self-aware RL",
            "name_full": "Self-aware Reinforcement Learning with Self-Aware Difficulty Prediction and Limit Breaking",
            "brief_description": "An LLM-driven self-evolving RL curriculum where a generator agent (the same LLM) produces tasks and predicts per-task difficulty relative to its current ability, while a solver agent attempts solutions; high-utility unsolvable tasks trigger selective queries to a stronger external solver (limit breaking) to minimally inject supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "LLM-generated tasks with difficulty-based adaptive filtering and selective external guidance (RL-based, adaptive curriculum)",
            "curriculum_method_description": "A generator agent (policy π_θ implemented with Qwen2.5-Coder-3B) produces new tasks and a predicted success rate µ(x) for N rollouts. The solver agent samples multiple rollouts to compute the empirical success rate μ(x); the generator receives a difficulty-prediction reward R_dp = 1 - |μ(x) - µ(x)| to align predictions with reality. Each generated task is scored for utility via two dimensions: difficulty (1 - µ(x)) and novelty (token-level perplexity under the current solver policy). Utilities are z-scored within a FIFO task buffer B and mapped to a query probability p(x) via a transformed normal CDF with sharpness γ and probability target τ. If all solver rollouts fail and a Bernoulli draw with p(x) turns positive, an external stronger solver (Qwen2.5-Coder-32B) is queried to provide a correct solution (limit breaking). Generator and solver receive format rewards; solver uses binary outcome reward. RL updates use REINFORCE++.",
            "llm_model_used": "Qwen2.5-Coder-3B (base generator/solver); Qwen2.5-Coder-32B used as external solver for limit breaking",
            "domain_environment": "Code generation with python interpreter for verifiable execution outcomes, and mathematical reasoning benchmarks evaluated post-training",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Tasks include multi-step mathematical reasoning (MATH500, AIME, Minerva, AMC'23, OlympiadBench) and code synthesis problems (HumanEval++, MBPP++, LiveCodeBench) that require multi-step planning, compositional reasoning chains, and executable verification; generator controls difficulty across a continuum via predicted success rates.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Qwen2.5-Coder-3B (pretrained base), AZR (Absolute Zero Reasoner) as a self-evolving baseline, self-aware RL − (ablation without limit breaking)",
            "performance_metrics": "Reported metrics are task-suite accuracy percentages per benchmark. Key reported results: self-aware RL achieves a relative improvement of ~53.8% on average across mathematical reasoning benchmarks vs the Qwen2.5-Coder-3B base, and ~5.3% average improvement on coding benchmarks. Example specific reported gains (from paper text): +29.8% on MATH500, +77.8% on AMC'23, +82.4% on OlympiadBench, +22.3% on LiveCodeBench. The method only queried external guidance for 157 of 12,800 tasks (1.23%). Ablation: self-aware RL without limit breaking still outperforms AZR baseline on mathematical benchmarks by 25.6%. Shuffled utility ranking yields only negligible improvements (math: ~28.0% -&gt; 28.4%; code: ~53.3% -&gt; 53.6%).",
            "learning_speed_comparison": "Training ran 200 steps on 4x H-200 GPUs; difficulty prediction was trained first and limit breaking was disabled for the first 50 steps. The paper reports that training reward for self-aware RL increases more stably and reaches higher values than the AZR baseline after initial fitting; no wall-clock convergence times or episode counts beyond the 200-step run are provided.",
            "generalization_performance": "Authors report improved generalization to held-out/out-of-distribution tasks: large relative gains on out-of-distribution code generation benchmarks and out-of-domain mathematical reasoning benchmarks (aggregate math gain ~53.8%). Exact held-out splits not further detailed beyond the nine benchmark evaluations.",
            "task_diversity_analysis": "Qualitative and empirical evidence: the generator initially produced overly-easy tasks (high rollout accuracy), then adapted to produce more challenging tasks (rollout accuracy dropped and stabilized ~0.6). Utility scores of selected (queried) tasks are significantly higher than unselected tasks (Figure 5), showing the filter increases task utility diversity toward valuable examples.",
            "prerequisite_identification": "Not explicitly evaluated as a prerequisite graph; the difficulty prediction mechanism implicitly identifies tasks at the model's learning frontier by estimating solvability (µ(x)) and prioritizing medium-to-hard but solvable tasks.",
            "intermediate_task_generation": "Yes — the generator produces intermediate difficulty tasks adaptively (via µ(x) guidance). Additionally, external solver-provided solutions for selected high-utility tasks act as bridging examples to break capability ceilings.",
            "llm_limitations_observed": "Pretrained LLMs were initially poor at predicting their own success rates (difficulty prediction accuracy ~0.2 initially, rising &gt;0.6 after ~50 steps), tended to generate tasks that were too easy or too hard without self-awareness, and required disabling limit breaking during early steps due to inaccuracy in µ(x).",
            "computational_cost": "Experiment used 4 NVIDIA H-200 GPUs and ran for 200 steps; authors emphasize data-efficiency (only 1.23% of tasks required external guidance). No detailed FLOPs or runtime per-step numbers reported.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "Self-aware RL — combining LLM-generated tasks with explicit difficulty prediction and a selective external-guidance (limit breaking) mechanism — produces an adaptive curriculum that substantially improves mathematical reasoning and modestly improves coding ability while using very little external data (≈1.2% of tasks). Ablations show difficulty prediction alone gives large gains (≈25.6% vs AZR), and selective limit breaking with an appropriate query frequency (τ=0.1) yields further improvement; random utility selection or excessive external guidance degrades benefit.",
            "uuid": "e2035.0"
        },
        {
            "name_short": "AZR",
            "name_full": "Absolute Zero Reasoner (AZR)",
            "brief_description": "A prior self-evolving RL approach where a generator agent automatically generates coding tasks to train a policy model without external input data; used in this paper as a baseline for comparison.",
            "citation_title": "Absolute zero: Reinforced self-play reasoning with zero data",
            "mention_or_use": "mention",
            "curriculum_generation_method": "LLM-generated tasks (self-play/self-generated) without explicit self-awareness-based difficulty calibration",
            "curriculum_method_description": "AZR produces tasks automatically from a generator agent and uses them to train a solver agent via RL (self-play / self-generated task loop). The paper notes AZR and similar self-evolving methods do not include explicit self-awareness components to align generated task difficulty to the solver's current capability.",
            "llm_model_used": null,
            "domain_environment": "Code generation / reasoning tasks (self-generated training data)",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Not detailed in this paper beyond general coding/reasoning task generation; characterized as generating tasks without capability-aware difficulty control, often yielding tasks that are too trivial or too hard.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared against in experiments as a baseline; self-aware RL (and its ablations) are compared to AZR",
            "performance_metrics": "Paper reports comparative improvements over AZR: self-aware RL − (no limit breaking) outperforms AZR by ~25.6% on mathematical benchmarks. Exact AZR per-benchmark scores are reported in tables but not reproduced in full textual detail.",
            "learning_speed_comparison": "Not reported in detail in this paper beyond aggregate reward traces showing AZR underperforms self-aware RL in training reward progression.",
            "generalization_performance": "Reported as lower than self-aware RL on evaluated benchmarks; no further specifics provided.",
            "task_diversity_analysis": "Not analyzed in detail here; AZR is characterized as generating misaligned tasks (too easy or too hard) relative to solver capability.",
            "prerequisite_identification": "No",
            "intermediate_task_generation": "No explicit mechanism for producing intermediate bridging tasks is reported.",
            "llm_limitations_observed": "AZR-style approaches lack explicit self-awareness leading to inefficient curricula (task difficulty misaligned with model frontier).",
            "computational_cost": "Not detailed here.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "AZR-style zero-data self-evolving generators can bootstrap learning but produce curricula that are often misaligned with solver capability; adding self-awareness (difficulty prediction and selective limit breaking) yields substantially improved downstream performance and data efficiency.",
            "uuid": "e2035.1"
        },
        {
            "name_short": "Balanced Online Difficulty Filtering",
            "name_full": "Balanced Online Difficulty Filtering (Bae et al., 2025)",
            "brief_description": "A curriculum learning mechanism that prioritizes tasks of medium hardness to maximize the effectiveness of RL training, proposed to stabilize GRPO-style RLVR.",
            "citation_title": "Online difficulty filtering for reasoning oriented reinforcement learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Difficulty-based online filtering (select medium-hard tasks)",
            "curriculum_method_description": "The method implements a balanced filtering mechanism that maintains a selection bias toward tasks of intermediate difficulty to maximize positive reward signal and learning progress during RLVR (described in related work; not implemented in this paper).",
            "llm_model_used": null,
            "domain_environment": "LLM reasoning and RLVR training",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Targets tasks across a continuum; focuses selection on medium difficulty which purportedly require multiple reasoning steps but remain solvable enough to produce learning signals.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned as a recent curriculum approach; not experimentally compared in this paper.",
            "performance_metrics": "Not reported in this paper (only cited in related work).",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not reported here.",
            "prerequisite_identification": "Not reported here.",
            "intermediate_task_generation": "Implicitly aims to surface intermediate tasks by selecting medium difficulty.",
            "llm_limitations_observed": "Not discussed in this paper.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "Cited as a curriculum method that improves RLVR by focusing on medium-hard tasks to better exploit reward signal; paper positions it as related prior work but does not provide direct comparisons or metrics.",
            "uuid": "e2035.2"
        },
        {
            "name_short": "Adaptive Difficulty Curriculum Learning",
            "name_full": "Adaptive Difficulty Curriculum Learning (Zhang et al., 2025)",
            "brief_description": "A curriculum approach that periodically re-estimates task difficulty within upcoming data batches and adapts selection to align with the model's evolving capabilities.",
            "citation_title": "Adaptive Difficulty Curriculum Learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Adaptive difficulty re-estimation and selection (difficulty-based ordering)",
            "curriculum_method_description": "Imitates human learning by periodically re-evaluating difficulty of forthcoming batches and adjusting selection to remain aligned with the learner's capability frontier; described in related work and cited as inspiration for aligning curricula to evolving LLM abilities.",
            "llm_model_used": null,
            "domain_environment": "LLM reasoning tasks",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Targets multi-step reasoning tasks; designed to keep difficulty matched to evolving learner competency.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned in related work; no direct experimental comparison in this paper.",
            "performance_metrics": "Not reported in this paper.",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not reported here.",
            "prerequisite_identification": "Not explicitly detailed here.",
            "intermediate_task_generation": "Approach can produce intermediate tasks implicitly by adjusting difficulty targets over time.",
            "llm_limitations_observed": "Not discussed in this paper.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "Cited as a human-inspired adaptive curriculum strategy that periodically re-calibrates difficulty to the learner; presented as related work motivating the need for dynamic difficulty estimation in self-generated curricula.",
            "uuid": "e2035.3"
        },
        {
            "name_short": "Self-Evolving Curriculum (Chen et al.)",
            "name_full": "Self-Evolving Curriculum for LLM Reasoning (Chen et al., 2025)",
            "brief_description": "A curriculum method that concurrently learns an explicit curriculum policy during RL fine-tuning to maximize learning progress.",
            "citation_title": "Self-evolving curriculum for llm reasoning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Learned curriculum policy (meta-curriculum learned alongside RL)",
            "curriculum_method_description": "Learns an additional curriculum policy parallel to RL fine-tuning to select tasks that maximize the model's learning progress; cited in related work as a recent approach to automated curriculum design for LLM reasoning.",
            "llm_model_used": null,
            "domain_environment": "LLM reasoning and multi-step reasoning tasks",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Targets complex reasoning tasks; curriculum policy aims to expose the model to sequences that foster multi-step capability development.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned in related work; not experimentally compared in this paper.",
            "performance_metrics": "Not reported in this paper.",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not reported here.",
            "prerequisite_identification": "Method may implicitly discover beneficial progression but not detailed in this paper.",
            "intermediate_task_generation": "Yes — the curriculum policy is intended to produce sequences of tasks that bridge skills, though concrete evaluation not included here.",
            "llm_limitations_observed": "Not discussed here.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "Cited as a technique that learns a curriculum policy jointly with RL to maximize learning progress; the present paper positions its self-aware mechanisms as complementary to or combinable with such self-evolving curriculum policies.",
            "uuid": "e2035.4"
        },
        {
            "name_short": "Curriculum Reinforcement Fine-tuning",
            "name_full": "Curriculum Reinforcement Fine-tuning (Deng et al., 2025)",
            "brief_description": "A curriculum learning design that injects difficulty-aware reward shaping to ensure steady progression of model capabilities during RL fine-tuning.",
            "citation_title": "Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning",
            "mention_or_use": "mention",
            "curriculum_generation_method": "Difficulty-aware reward shaping and ordering (curriculum RL)",
            "curriculum_method_description": "Proposes difficulty-aware reward design to shape the learning trajectory such that capabilities progress steadily; cited among recent curriculum methods for reasoning and RL fine-tuning.",
            "llm_model_used": null,
            "domain_environment": "Vision-language and LLM reasoning contexts (cited broadly)",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Designed to support multi-step reasoning; specifics not given in this paper.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned as related work; not directly compared in experiments here.",
            "performance_metrics": "Not reported in this paper.",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not reported here.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "Not specifically described in this paper.",
            "llm_limitations_observed": "Not discussed here.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "null",
            "key_findings_summary": "Cited as an example of curriculum design that uses difficulty-aware reward shaping to steer progression; included to situate the self-aware RL contribution within prior curriculum-RL literature.",
            "uuid": "e2035.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Online difficulty filtering for reasoning oriented reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Adaptive Difficulty Curriculum Learning",
            "rating": 2
        },
        {
            "paper_title": "Self-evolving curriculum for llm reasoning",
            "rating": 2
        },
        {
            "paper_title": "Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Absolute zero: Reinforced self-play reasoning with zero data",
            "rating": 2
        },
        {
            "paper_title": "Webevolver: Enhancing web agent self-improvement with coevolving world model",
            "rating": 1
        },
        {
            "paper_title": "WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "ZeroGUI: Automating online GUI learning at zero human cost",
            "rating": 1
        }
    ],
    "cost": 0.017592999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback
3 Oct 2025</p>
<p>Hangfan Zhang 
Pennsylvania State University</p>
<p>Siyuan Xu 
Pennsylvania State University</p>
<p>Zhimeng Guo zhimeng@psu.edu 
Pennsylvania State University</p>
<p>Huaisheng Zhu 
Pennsylvania State University</p>
<p>Shicheng Liu 
Pennsylvania State University</p>
<p>Xinrun Wang xrwang@smu.edu.sg 
Management University
Singapore</p>
<p>Qiaosheng Zhang zhangqiaosheng@pjlab.org.cnyang.chen.csphd@gmail.com 
Shanghai Artificial Intelligence Laboratory</p>
<p>Yang Chen 
Shanghai Artificial Intelligence Laboratory</p>
<p>Peng Ye yepeng@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Lei Bai bailei@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shuyue Hu hushuyue@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback
3 Oct 2025DE8C2950EE94BE6441DD051E25505C2FarXiv:2510.02752v1[cs.CL]
Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data.In this work, we explore improving LLMs through RL with minimal data.Our approach alternates between the LLM proposing a task and then attempting to solve it.To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit.Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) has emerged as a crucial approach for enhancing the reasoning abilities of large language models (LLMs), especially in tasks like mathematical problem solving and code generation, where the correctness of generated outputs can be rigorously verified (Lambert et al., 2024;Guo et al., 2025;Jaech et al., 2024;Team et al., 2025).However, these improvements often come at the cost of requiring vast amounts of high-quality data.The data curation processes, which heavily rely on human experts to create tasks and annotate solutions, are prohibitively costly and time-consuming, creating a significant bottleneck in LLM advancements.</p>
<p>One promising solution is to empower LLMs to generate their own tasks, enabling selfimprovement through RL (Zhao et al., 2025;Qi et al., 2025;Yang et al., 2025;Wang et al., 2025;Fang et al., 2025).By actively participating in their own learning process-generating tasks and attempting to solve them-LLMs can establish a self-improving loop of data creation and model refinement, significantly reducing the need for manually curated data.While initial efforts have shown promise, this line of research faces two critical challenges:</p>
<p>First, the model may fail to generate appropriately challenging tasks.If the tasks are too easy, the model fails to engage in deeper reasoning and exploration.Conversely, if the tasks are too difficult, the model receives little useful feedback about what went wrong or how to improve.This issue is further complicated by the dynamic nature of the self-improvement loop, as the model's evolving capabilities require that task difficulty be continuously adjusted.Second, the self-improving loop is prone to stagnation, failing to continuously proposing challenging tasks.While the model can improve through RL, the self-improving loop is fundamentally constrained by the base model's inherent capabilities.As the loop progresses, the difficulty of the generated tasks may plateau, once the model has exhausted its current knowledge.Consequently, the model may encounter a point of stagnation, unable to advance further or surpass its inherent capability limits.</p>
<p>This paper argues that the key to addressing these challenges lies in self-awareness-the ability to understand one's own strengths, weaknesses, and evolving capabilities.A task that is difficult for the base model may later become trivial as its abilities improve.Therefore, the model must be acutely aware of its current abilities to accurately assess the difficulty of its generated tasks in relation to its state.Furthermore, overcoming the model's capability limits and escaping stagnation also require a clear understanding of its own capability boundaries.</p>
<p>To this end, this paper introduces a novel paradigm for improving LLMs with minimal data: self-aware RL.This paradigm features two key mechanisms.First, through self-aware difficulty prediction, the model not only generates a task but also predicts the difficulty of the task considering its current state.To achieve accurate predictions, the model learns to align its difficulty estimates with its actual success rate.This enables the creation of an adaptive curriculum that prioritizes problems with an appropriate level of difficulty, tailored to the current capabilities of the LLM.Second, through self-aware limit breaking, the model occasionally proactively seeks external guidance, such as requesting a correct solution, when it identifies a generated task to be highly valuable for improvement but hardly solvable given its current capability.This is achieved by having the model assess the novelty (via perplexity) and difficulty (via its own prediction) of a generated task.This allows the LLM to surpass its inherent capability limits while minimizing reliance on external data, resorting to it only when necessary.</p>
<p>For evaluation, we train the LLMs with self-aware RL equipped with python code interpreter that provides verifiable outcome signals, and then evaluate the LLMs across a range of reasoning tasks the models have not encountered during our training.In our experiments, we implement self-aware RL based on Qwen2.5-Coder-3B, and consider nine benchmarks across mathematical reasoning and code generation in the evaluation.Our results demonstrate that self-aware RL can significantly improve the pre-trained model's performance on both out-of-distribution tasks (code generation benchmarks) and out-of-domain tasks (mathematical reasoning benchmarks), achieving a relative performance gain of 53.8% on average on mathematical reasoning benchmarks.Specifically, self-aware RL got significant improvements on a lot of widely used benchmarks: 29.8% on MATH500, 77.8% on AMC'23, 82.4% on OlympiadBench, and 22.3% on LiveCodeBench.We further analyze the training behavior, and conduct fine-grained ablation studies to learn the impact of different components.</p>
<p>To summarize, our key contributions are as follows:</p>
<p>• We establish connections between self-awareness and the self-improvement of LLMs, highlight-ing that self-awareness is a crucial mechanism for efficiently enhancing LLMs, while reducing reliance on external data.</p>
<p>• We present a novel RL paradigm for improving LLMs with minimal data, enabling the model to (i) generate appropriately difficult tasks that align with its current abilities, and (ii) proactively seek external guidance to surpass its inherent capability limits when necessary.</p>
<p>• We conduct extensive experiments across nine benchmarks in mathematical reasoning and code generation, showing that our approach substantially boosts the base model's performance and generalization abilities, thereby validating the effectiveness of the self-aware RL paradigm.</p>
<p>Related Works</p>
<p>Reinforcement Learning with Verifiable Reward (RLVR).RLVR is an emerging paradigm (Guo et al., 2025;Lambert et al., 2024;Jaech et al., 2024;Team et al., 2025) in LLM alignment. Tulu3 (Lambert et al., 2024) was the first LLM tuned with RLVR to improve math and instruction following capabilities.Deepseek-R1 (Shao et al., 2024;Guo et al., 2025) was trained with Group Relative Policy Optimization (GRPO), a novel RLVR algorithm which uses inner group relative reward to replace the value model in traditional RL algorithms, boosting the training efficiency.Many following works tried to stabilize GRPO training.DAPO (Yu et al., 2025) used a higher clipping threshold and proposed dynamic sampling to better utilize the positive reward signal.GSPO (Zheng et al., 2025) updated the token-level importance sampling in GRPO to sequence-level, mitigating the bias introduced by imbalanced response lengths.Dr. GRPO Liu et al. (2025) identified that GRPO tends to overly penalize shorter, incorrect responses, which misguides the LLM to produce longer yet incorrect responses.Similar to GSPO, Dr.GRPO fixed this issue by considering a sequence-level reward instead of a token-level one.Due to the significant performance gain brought by RLVR, it is widely adopted in most advanced Large Reasoning Models (LRM) (Team et al., 2025).However, while RLVR does not require human crafted responses for imitation learning like in Supervised Fine Tuning (SFT), a large set of high-quality tasks are still necessary to incentivize diverse and effective reasoning patterns.</p>
<p>Self-evolving RL.To overcome the dependency on vast data, self-evolving RL is proposed to create agents that autonomously enhance their capabilities with minimal human oversight.Wang et al. (2025) proposed to build a unit tester co-evolving with the code generation agent to improve the overall generation quality.WebEvolver (Fang et al., 2025) introduced a dynamic world model simulating web navigation to help exploration in the web environment.Absolute Zero Reasoner (Zhao et al., 2025) proposed a generator agent automatically generating coding tasks to train the policy model without any input data.Similarly, self-challenging agent Zhou et al. (2025) designed a geneator agent that can interact with the environment to simulate a user query as the task fed into a customer agent.WebRL (Qi et al., 2025) proposed to generate new tasks from unsuccessful attempts of a web agent, expanding the data coverage to improve the decision-making capability of the agent.ZeroGUI (Yang et al., 2025) adopted VLM-based automatic task generation to produce diverse training goals.However, these approaches generate tasks without awareness of the agent's own capabilities, leading to inefficient training curricula.The generated tasks are often misaligned with the agent's learning frontier, being either too trivial or difficult.</p>
<p>Curriculum RL in LLM Reasoning.Curriculum learning, which involves structuring training data from easy to hard, has emerged as a powerful technique for cultivating advanced reasoning in LLMs.Typical curriculum learning methods focus on dynamically selecting or filtering problems from a larger pool to maintain an optimal level of difficulty.Recently, curriculum learning has also caught much attention from LLM community due to the reliance on proper utilization of available data in LLM training.Balanced online difficulty filtering (Bae et al., 2025) proposed a novel mechanism, balanced filtering, that focuses on tasks with medium level of hardness to maximize the effectiveness of GRPO.Adaptive Difficulty Curriculum Learning (Zhang et al., 2025) imitates human learning strategy and periodically re-estimates the difficulty within upcoming data batches to keep aligned with model's capabilities.Self-Evolving Curriculum (Chen et al., 2025) learns an additional curriculum policy concurrently with the RL fine-tuning process to maximize the learning progress.Curriculum Reinforcement Fine-tuning (Deng et al., 2025) proposed a difficulty-aware reward design ensuring steady progression of model capabilities.By guiding models through a structured learning path, these methods effectively foster the development of complex, multi-step reasoning capabilities.Even though, these curriculum learning approaches directly improve typical RL methods.The conjunction of curriculum learning and self-evolving RL is largely underexplored.</p>
<p>In this paper, we explore a solution combining the advantages of both, showing that the curriculum design is especially important in self-evolving RL training.</p>
<p>Preliminaries</p>
<p>In this paper, we denote the LLM parameterized by θ as a policy model π θ , which is also used to refer to an LLM agent driven by π θ for simplicity.We use x to denote the task that a solver agent solves with response y, where x and y are both sequences of tokens with t-th token in x denoted as x t , and the length of x denoted as |x|.</p>
<p>REINFORCE++ (Hu et al., 2025)is an advanced RL algorithm for LLM post-training.Its learning objective on a sampled pair (x, y) is given by:
J (θ|x, y) = 1 |y| |y| t=1 min w t (π θ ) Ât , clip (w t (π θ ), 1 − ε, 1 + ε) Ât (1)
where w t (θ) is the importance sampling ratio, and Ât denotes the normalized advantage term:
Ât = r − mean batch (r) std batch (r) (2)
4 Method</p>
<p>The general framework of self-aware RL can be viewed in Figure 1.We design self-aware RL as a self-evolving training loop, where a generator agent and a solver agent iteratively generate and solve new tasks to improve themselves.Due to the stability and accessibility of coding environment, we implement self-aware RL on code generation tasks, while any other environment that can check the correctness of the agent's response also applies, e.g., the verifiable rewards gym adopted by Kimi-K2 (Team et al., 2025).We use the python code interpreter to produce verifiable reward signals.Given a code snippet, the solver agent is asked to predict the outcome after execution, and a matching outcome will receive a positive outcome reward.Therefore, the generator agent</p>
<p>Generator Solver</p>
<p>Difficulty Prediction ()</p>
<p>External Solver</p>
<p>DP Reward Outcome Reward Format Reward</p>
<p>Task Filter</p>
<p>Policy Update</p>
<p>Figure 1: The overview of self-aware RL.The generator agent first generates the task, with a predicted success rate µ(x) (Section 4.1).The solver agent will then generates reasoning paths.If none of these generated reasoning paths are correct, which means no update are available, the task will be filtered by a task filter to determine whether it is of enough value to be processed by an external solver (Section 4.2).Finally, the collected difficulty prediction reward, outcome reward, and format reward will be aggregated to calculate the policy update (Section 4.3).</p>
<p>can generate a new code snippet with accessible execution outcome.With the generated tasks, self-aware RL improves the solver agent with typical RLVR methods.Specifically, self-aware RL incorporates difficulty prediction and limit breaking to improve the self-awareness of the agent.LLM agents based on pre-trained LLMs are not inherently self-aware, which hinders efficient self-evolution.With a better understanding of their own capability boundaries, agents can generate properly challenging tasks, which has been validated to be beneficial to RL training (Bae et al., 2025;Chen et al., 2025).Meanwhile, fully on policy RL training can be extremely challenging in some cases where the agent is strictly limited by its inherent capability ceiling.External guidance is necessary here to break the limitation.However, it is not efficient to seek external guidance blindly on every unsolvable tasks, which degrades to naive supervised fine-tuning (SFT) requiring heavy human inspections.Self-awareness can assist in identifying highly valuable tasks where external guidance is necessary and effective, mitigating the need for vast, well-grounded supervision.</p>
<p>• Self-aware difficulty prediction guides the generator agent to learn to predict the difficulty of the generated tasks.</p>
<p>• Self-aware limit breaking identifies cases where the solver are limited by its inherent reasoning capability, and breaks it with external guidance.</p>
<p>Self-aware difficulty prediction</p>
<p>Problems at a proper level of difficulty can maximize the learning progress (Chen et al., 2025;Bae et al., 2025).However, guiding the generator to produce suitable tasks requires a good understanding of the LLM's own capability which entails non-trivial challenges.LLMs are rarely trained to improve the capability of understanding their own boundary as it is not feasible to do so in large scale pre-training with preset human inspection.Consequently, while being a great task solver, LLMs are not naturally good at understanding themselves.Our intuition is empirically confirmed through evaluating pre-trained LLMs' performance in predicting task solving success rates.As discussed in Figure 3, we found that the base model is initially bad at predicting the task solving success rates, i.e., LLMs are not inherently aware of its own capability boundary.Therefore, we propose self-aware difficulty prediction to enhance the generator agent's capability of understanding its own knowledge boundaries and generate proper tasks benefiting the self-evolving loop.First, we enable self-aware self-aware RL by asking the generator agent to explicitly reason about the complexity of the generated task and predict how many trials would be correct among N rollouts generated by the solver agent.For instance, assume that the generator agent predicts that for a generated task (we denote the task as x) to be solved, the solver agent can generate 5 correct answers out of N = 8 rollouts, we will have the predicted success rate µ(x) = 5 8 .The predicted success rate µ(x) should be aligned with the ground truth success rate μ(x).When rollouts are sampled from the solver agent, difficulty prediction collects the sampled rollouts and calculate the ground truth success rate, which is further utilized in the reward signal for the generator.Specifically, given the actual success rate μ(x), and the predicted success rate µ(x), the reward function is:
R dp (x) = 1 − |μ(x) − µ(x)|(3)
When the generator predicts a close enough success rate, it will receive a high reward from it.</p>
<p>Self-aware limit breaking</p>
<p>Even with proper tasks, agents are still limited by their own capability.When the solver fails to solve a task with multiple trials, it will not get any improvement since no positive guidance is acquired.Previous practices to deal with this challenge have focused on injecting new knowledge and abilities into the LLM during training, a process that commonly involves human inspection.</p>
<p>As LLM training is extremely costly, intervention from human experts will not be frequent, which results in a widely adopted training paradigm: data collection, training, evaluation, and repeat.This less dynamic and timely intervention makes the benefit from external guidance take effect much later than the time point when the LLM actually requires the guidance.</p>
<p>To mitigate this issue, self-aware RL adopts self-aware limit breaking to offer effective guidance immediately when the solver agent fails to solve a valuable task.As shown in Figure 1, for any task, when all reasoning paths sampled from the solver agent are incorrect, limit breaking will first check the task utility representing whether the task is significant in improving the agent.Once validated, external guidance will be obtained by querying a stronger external solver.Correct guidance will be adopted to improve the solver agent, breaking the inherent ability limitation.</p>
<p>We first introduce how to identify valuable tasks.For clarity, we use an indicator variable I x to denote whether a task is selected as valuable enough to acquire external guidance.The selection is a random event that occurs with a probability p(x) depending on the task itself.Formally, I x is a Bernoulli random variable defined as
I x = 1, with probability p(x) 0, with probability 1 − p(x)
To obtain suitable p(x), we design a task utility mechanism that assigns different levels of utility score to tasks reflecting their significance.For task utility, we consider two dimensions: the difficulty and the novelty.For difficulty, we utilize the predicted success rate from difficulty prediction as this score reflects the general review of complexity from the generator agent.We inversely use 1 − µ(x) to measure the difficulty level of task x.To measure the novelty of a task, we utilize the token-level perplexity from the solver agent.The utility score ω of task x can be formulated as follows:
ω difficulty (x) = 1 − µ(x) ω novelty (x) = − 1 |y| |y| i=1 logπ θ old (y i |x, y &lt;i )(4)
A higher novelty score indicates that the current policy is more confused about the task description, which can be viewed as a signal the solver agent is not familiar with the task.While we only consider two kinds of utility measurement here, it is possible to take more measurement into consideration when transferred to other domains.</p>
<p>In order to minimize the amount of acquired external guidance used to save cost, we only acquire external guidance on filtered-out high-utility tasks.Given that the tasks are dynamically generated and the agent's capability may differ between runs, it is natural to calculate the utility relatively by comparing a task to other tasks in that run.To achieve this, we maintain a task buffer B that records recent tasks seen by the solver agent.The buffer is implemented as a FIFO queue, which only stores a fixed number of recently generated tasks.
B = {x 1 , x 2 , ..., x |B| }(5)
Tasks with top-level utility will be assigned a higher p(x).To achieve this, we first obtain the z-score of the task utility among the recorded tasks, and assign a higher p(x) to tasks with higher z-score.This procedure is implemented as follows:
ω(x) = [ω difficulty (x), ω novelty (x)] z(x) = 1 |ω| |ω| i=1 ω(x) i − E x∼B [ω(x) i ] std x∼B [ω(x) i ] (6) p(x) = Φ γ z(x) + Φ −1 (τ ) 1 + 1 γ 2 (7)
where ω(x) i denotes the i-th dimension of ω(x).In this case, ω(x) 0 = ω difficulty (x), and ω(x) 1 = ω novelty (x).Φ is the cumulative distribution function (CDF) of standard normal distribution, γ &gt; 0 is a sharpness factor and τ is the probability factor, which makes E[p(x)] = τ .By tuning τ , we can control the number of tasks that require external guidance.A higher sharpness factor γ will enlarge p(x) for a task with high z(x).</p>
<p>Self-aware RL training pipeline</p>
<p>Finally, we can combine all components to build the self-aware RL training pipeline.For the generator agent, we use R dp as the reward.Similar to typical RLVR settings, we include a format reward R format ∈ {0, 1} to guide the agent responses following the difficulty prediction template.</p>
<p>A positive R format indicates that the agent's response passes the format check.We formulate the reward for the generator agent as follows:
R generator = R format R dp + R format (8)
For the solver agent, we apply typical RLVR method, and adopt the binary outcome reward R outcome ∈ {0, 1} as the reward signal.A positive outcome reward indicates that the agent successfully solves the task.We also adopt a format reward for the solver agent, with a different dialogue template to the generator agent.We formulate the reward for the solver agent as follows:
R solver = R format R outcome + R format (9)
For a training sample (x, y) including one generated task from the generator agent and one sampled response from the solver agent, the learning objective is formulated in Equation 1.Therefore, the learning objective for self-aware RL is:
J self-aware RL (θ) = E [J (θ|x, y)] , y ∼ π 1−Ix θ π Ix θ external (10)
The training pipeline is compatible with different RL algorithms.In our implementation, we adopt REINFORCE++ (Hu et al., 2025) as the RL algorithm for verification.</p>
<p>Experiment</p>
<p>Experimental Setup</p>
<p>Datasets We conduct our evaluation on 6 mathematic reasoning datasets and 3 coding benchmarks: MATH500 (Hendrycks et al., 2021), AIME (AIME'24 and AIME'25), Minerva (Lewkowycz et al., 2022), AMC'23, OlympiadBench (He et al., 2024), MBPP++, HumanEval++ (Liu et al., 2023), and LiveCodeBench (Jain et al., 2024).Training Setup We implement self-aware RL based on verl (Sheng et al., 2024), an open-sourced reinforcement learning pipeline widely used in developing LLM RLVR frameworks.We made necessary adjustment to make it compatible with our GPU servers, with detailed hyperparameter configuration listed in Appendix A.1.We validate self-aware RL on a widely used open-sourced LLM, Qwen2.5-Coder-3B(Hui et al., 2024) given its superior coding capability which serves as a good starting point.We use Qwen2.5-Coder-32B as the external policy model.By default, we set hyperparameter in limit breaking as τ = 0.1 and γ = 5.Due to the inaccurate difficulty prediction at the beginning, we diable limit breaking for the first 50 steps, which is further explained in Section 5.3.We compare self-aware RL to baselines on nine benchmarks in Table 1.Our results indicate that self-aware RL significantly outperform previous baselines.Specifically, we observe that self-aware RL outperforms Qwen2.5-Coder-3B by 53.8% on average on mathematic benchmarks, and by 5.3% on coding benchmarks.The improvement of self-aware RL on coding benchmarks is not as significant as that on mathematical benchmarks as Qwen2.5-Coder-3B is originally strong in coding tasks, leaving little space for further improvement.The obvious improvement on mathematic benchmarks can be attributed to the acquisition of stronger general reasoning capability during RL training with complex reasoning trajectories.Notably, self-aware RL only queried external guidance on 157 out of 12,800 (=1.23%) tasks.Training reward As shown in Figure 2, self-aware RL received higher training reward in comparison to the baseline AZR, which demonstrates the performance improvement brought by self-aware RL.Note that the training reward of self-aware RL is lower at the first , which can be explained by that we adopted different dialogue template, and the policy model needs further tuning to get fitted.After 50 steps, self-aware RL received higher consistently increasing training reward.Difficulty prediction We record the difficulty prediction accuracy measured by R dp in Figure 3.</p>
<p>Experimental Results</p>
<p>Analysis</p>
<p>Observe that the pre-trained model at the first step performs poorly on predicting its own accuracy on the generated tasks, which supports our intuition discussed in Section 4.1 that LLMs are not trained to understand their own capability boundaries.After training with self-aware difficulty prediction for around 50 steps, the accuracy has significantly improved from 0.2 to over 0.6.Based on this observation, in our implementation we did not enable self-aware limit breaking until the 50-th step, since precise measurement of task utility requires accurate prediction on the difficulty of generated tasks.Utility of selected tasks</p>
<p>Selected Filtered</p>
<p>Figure 5: Utility score z(x) of selected and unselected tasks.Selected tasks should be of high utility, and will be proceeded to the external solver.While unselected tasks are of lower utility and are discarded.</p>
<p>Rollout accuracy</p>
<p>In Figure 4 we recorded the accuracy of rollouts sampled from the solver agent.</p>
<p>The initially high accuracy indicates that the generator agent did not create sufficiently challenging tasks.As training progresses, the accuracy gradually decreases and stabilizes around 0.6.This observation supports our intuition from two perspectives: (i) the pre-trained base model are not good at generating appropriate tasks without specific training, and (ii) after training with self-aware RL, the generator can adaptively generate tasks at a suitable difficulty level.Utility of selected tasks and unselected tasks In self-aware limit breaking, the solver agent will be trained on selected high utility tasks to break its capability upper bound.We compare the utility of selected and unselected tasks in Figure 5, which shows that the utility score of selected tasks are significantly higher than that of unselected tasks.This observation demonstrates that the task filter in self-aware limit breaking successfully distinguishes high utility tasks from low utility ones.</p>
<p>Ablation study</p>
<p>We conduct fine-grained ablation study to validate the effectiveness of each components in the design of self-aware RL.In the ablation study we consider three dimensions: (i) the effectiveness of difficulty prediction and limit breaking; (ii) the frequency of querying external guidance in limit breaking; and (iii) the effectiveness of utility ranking of limit breaking.We first compare self-aware RL to self-aware RL w/o limit breaking denoted as self-aware RL − in Table 2. Observe that self-aware RL − can still outperform the AZR baseline by 25.6% on mathematical reasoning benchmarks, which is still a considerable improvement.This ablation study indicates the agent can benefit from being trained to improve the self-awareness on how likely it can successfully solve the task, which perfectly support out intuition that an LLM can generate problems more suitable for improving itself if it knows itself better.Ablating the amount of external guidance In limit breaking, we propose to seek for external guidance on high-quality yet challenging tasks that cannot be solved directly to break through its intrinsic capability ceilings.We vary the hyperparameter τ controlling the frequency of external guidance to learn the behavior of self-aware RL with different level of external guidance.As shown in Table 3, we achieved the highest performance gain when we set τ = 0.1.For a smaller τ , the improvement is still obvious.However, we observe that with a larger τ , the improvement from external guidance becomes ignorable.This can be explained by that overly learning from external guidance will corrupt the original reasoning pattern of the solver, which may requires further adjustment to stabilize the training process.The effectiveness of utility ranking In limit breaking, problems are ranked according to its utility score and high-utility problems are considered to be critical in improving the agent.We validate the effectiveness of utility ranking by randomly shuffling the sorted problems.Table 4 shows that random shuffled utility can only achieve ignorable improvement from 28.0% to 28.4% on mathematic benchmarks, and from 53.3% to 53.6% on coding benchmarks.Without proper utility ranking, limit breaking cannot gain obvious improvement by querying external guidance casually.</p>
<p>Conclusion</p>
<p>In this paper, we tackled the In this paper, we propose self-aware RL, a self-evolving framework that enables an agent to efficiently guide its own learning by leveraging self-awareness.By learning to be self-aware and recognize its own capability limits to proactively request minimal external data, self-aware RL overcomes the common failure modes of naive self-training.Our experiments validate this approach, showing that self-aware RL achieves a 53.2% relative performance gain while using less than 1% of the extra data required by conventional methods.These findings underscore the potential of self-evolving agents to reduce reliance on large-scale human annotation.This work suggests a shift from simply building larger models on large datasets, to creating smarter learners that understand their own knowledge boundaries and adapt to it.Future research could extend this self-aware learning framework to more complex, interactive domains.</p>
<h3>Task: Create a Python Code Snippet (where custom classes are allowed, which should be defined at the top of the code snippet) with one Matching Input Using the reference code snippets provided below as examples, design a new and unique Python code snippet that demands deep algorithmic reasoning to deduce one possible input from a given output.Your submission should include both a code snippet and test input pair, where the input will be plugged into the code snippet to produce the output, which that function output be given to a test subject to come up with any input that will produce the same function output.This is meant to be an I. -Executability, your code should be executable given your input -Difficulty in predicting the output from your provided input and code snippet.Focus on either algorithmic reasoning or logic complexity.For example, you can define complex data structure classes and operate on them like trees, heaps, stacks, queues, graphs, etc, or use complex control flow, dynamic programming, recursions, divide and conquer, greedy, backtracking, etc -Creativity, the code needs to be sufficiently different from the provided reference snippets -Restricted usage of certain keywords and packages, you are not allowed to use the following words in any form, even in comments: raise First, carefully devise a clear plan: e.g., identify how your snippet will be challenging, distinct from reference snippets, and creative.Then, write the final code snippet and its inputs.</h3>
<p>B The Use of Large Language Models(LLMs)</p>
<p>During the preparation of this paper, we use LLMs as a writing assistant to polish up some expressions.Its role was strictly limited to improving grammar, clarity, and readability.The LLM was not used for any substantive aspect of this research, such as the generation of core ideas, the development of the methodology, code implementation, data analysis, or the formulation of conclusions.</p>
<p>Figure 2 :
2
Figure 2: training Reward of self-aware RL stably increases as the training continues.The reward is lower at the first few steps since the dialogue template is more complicated in comparison to the baseline.After the agent has been fitted to the new dialogue template, the training reward of self-aware RL quickly increases and surpasses the baseline reward.</p>
<p>Figure 3 :
3
Figure 3: Accuracy of difficulty prediction.The generator agent driven by the pre-trained base model performs poorly on the task of difficulty prediction (Section 4.1), which is shown by the low accuracy at the first step.After being tuned for 50 steps, the generator agent performs much better.Note that the accuracy shown in this figure is measured by the difficulty prediction reward in Equation 3.</p>
<p>Figure 4 :
4
Figure4: Accuracy of rollouts generated by the solver agent.The accuracy is initially high, reflecting that the generator did not generate challenging tasks without training.As the training continues, the difficulty of generated tasks increases and the rollout accuracy gradually decreases, was finally stabilized around 0.6.</p>
<p>Table 1 :
1
Comparing self-aware RL against baselines.
MethodMATH500 Minerva AIME Olympiad AMC'23 Math Avg HumanEval++ MBPP++ LCB Code AvgCoder-3B49.017.60.015.922.521.067.766.719.351.2AZR43.621.01.720.425.022.371.366.920.853.0self-aware RL63.625.43.329.040.032.370.767.523.653.9</p>
<p>Table 2 :
2
Ablation study: comparing self-aware RL to self-aware RL w/o limit breaking (denoted as self-aware RL − ).
MethodMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code AvgAZR43.621.01.720.425.022.371.366.920.853.0self-aware RL −59.623.93.325.827.528.070.166.922.953.3self-aware RL63.625.43.329.040.032.370.767.523.653.9</p>
<p>Table 3 :
3
Ablation study: varying the amount of external guidance in limit breaking.
τMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code Avg059.623.93.325.827.528.070.166.922.953.30.0559.624.61.730.735.030.371.366.723.253.70.163.625.43.329.040.032.370.767.523.653.90.257.223.20.023.935.027.971.367.522.153.6</p>
<p>Table 4 :
4
Ablation study: validating the effectiveness of utility ranking.
ϕMATH500 Minerva AIME24+25 Olympiad AMC23 Math Avg HumanEval++ MBPP++ LCB Code Avgself-aware RL −59.623.93.325.827.528.070.166.922.953.3self-aware RL63.625.43.329.040.032.370.767.523.653.9Shuffled59.223.91.727.130.028.472.067.721.253.6</p>
<p>Q. test.### Code Requirements: -Name the entry function 'f' (e.g., 'def f(...): ...'), you can have nested definitions inside 'f' -Ensure the function returns a value -Include at least one input parameter -Make the function deterministic -Make the snippet require state tracking across multiple data transformations, ensuring the task requires long multi step reasoning -AVOID THE FOLLOWING: * Random functions or variables * Date/time operations * I/O operations (reading files, network requests) * Printing or logging * Any external state -Ensure execution completes within 10 seconds on a modern CPU -All imports and class definitions should be at the very top of the code snippet -The snippet should end with a return statement from the main function 'f', anything after
"'input'John', 'age': 20, 'city': 'New York'"'### Evaluation Criteria:will be removed### Input Requirements:-Provide exactly one test input for your function-Format multiple arguments with commas between them-Remember to add quotes around string arguments### Formatting:-Format your code with: "'pythondef f(...):# your code herereturn ..."'-Format your input with: "'inputarg1, arg2, ..."'### Example Format:"'pythondef f(name: str, info: dict):# code logic herereturn result"'
A Additional Experimental SetupA.1 Training setupOur experiments are conducted on a server with 4 NVIDIA H-200 GPUs.We run the experiment for 200 steps.The code environment is implemented based on the QWQ python executor(Team, 2025).We list the key hyperparameter configuration of verl as follows.A.2 Chat templatesHere we provide chat templates used in our implementation.A part of our implementation is based on AZR(Zhao et al., 2025), therefore the task generation template is similar to that used in AZR.Template for difficulty prediction:### Difficulty Prediction Requirements:-At the end of your generation, you need to review your code and provide a difficulty prediction for the code.-The difficulty prediction should be a number between 0 and 8, where 0 is the easiest and 8 is the hardest.-The review of your code should be in the <review>...</review> tags.The review should focus on analyzing the difficulty of the code based on the complexity of the code, the number of steps required to solve the problem, the creativity of the code, as well as how powerful the current solver is.-You need to control the difficulty of the generated tasks to be medium.A difficulty level at 4 or 5 would be good.-The difficulty prediction should be wrapped in <difficulty prediction> and </difficulty prediction> tags.It should be strictly a number between 0 and 8. Otherwise, you will be penalized.-Therefore, your response should be formatted like <think>...</think> <answer>...</answer> <review>...</review> <difficulty prediction>...</difficulty prediction> Template for task generation:
S Bae, J Hong, M Y Lee, H Kim, J Nam, D Kwak, arXiv:2504.03380Online difficulty filtering for reasoning oriented reinforcement learning. 2025arXiv preprint</p>
<p>X Chen, J Lu, M Kim, D Zhang, J Tang, A Piché, N Gontier, Y Bengio, E Kamalloo, arXiv:2505.14970Self-evolving curriculum for llm reasoning. 2025arXiv preprint</p>
<p>Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. H Deng, D Zou, R Ma, H Luo, Y Cao, Y Kang, arXiv:2503.070652025arXiv preprint</p>
<p>T Fang, H Zhang, Z Zhang, K Ma, W Yu, H Mi, D Yu, arXiv:2504.21024Webevolver: Enhancing web agent self-improvement with coevolving world model. 2025arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. C He, R Luo, Y Bai, S Hu, Z L Thai, J Shen, J Hu, X Han, Y Huang, Y Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. J Hu, J K Liu, H Xu, W Shen, arXiv:2501.032622025arXiv preprint</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Lu, arXiv:2409.12186Qwen2. 5-coder technical report. 2024arXiv preprint</p>
<p>A Jaech, A Kalai, A Lerer, A Richardson, A El-Kishky, A Low, A Helyar, A Madry, A Beutel, A Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>N Jain, K Han, A Gu, W.-D Li, F Yan, T Zhang, S Wang, A Solar-Lezama, K Sen, I Stoica, arXiv:2403.07974Livecodebench: Holistic and contamination free evaluation of large language models for code. 2024arXiv preprint</p>
<p>N Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V Miranda, A Liu, N Dziri, S Lyu, arXiv:2411.15124Tulu 3: Pushing frontiers in open language model post-training. 2024arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in neural information processing systems. 352022</p>
<p>Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Understanding r1-zero-like training: A critical perspective. Z Liu, C Chen, W Li, P Qi, T Pang, C Du, W S Lee, M Lin, arXiv:2503.207832025arXiv preprint</p>
<p>Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. Z Qi, X Liu, I Iong, H Lai, X Sun, W Zhao, Y Yang, X Yang, J Sun, S Yao, 2025. 2025</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>G Sheng, C Zhang, Z Ye, X Wu, W Zhang, R Zhang, Y Peng, H Lin, C Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>K Team, Y Bai, Y Bao, G Chen, J Chen, N Chen, R Chen, Y Chen, Y Chen, Y Chen, arXiv:2507.20534Kimi k2: Open agentic intelligence. 2025arXiv preprint</p>
<p>Qwq-32b: Embracing the power of reinforcement learning. Q Team, 2025</p>
<p>Co-evolving llm coder and unit tester via reinforcement learning. Y Wang, L Yang, Y Tian, K Shen, M Wang, 2025. 2025</p>
<p>C Yang, S Su, S Liu, X Dong, Y Yu, W Su, X Wang, Z Liu, J Zhu, H Li, arXiv:2505.23762Zerogui: Automating online gui learning at zero human cost. 2025arXiv preprint</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Q Yu, Z Zhang, R Zhu, Y Yuan, X Zuo, Y Yue, W Dai, T Fan, G Liu, L Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Learning like humans: Advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided selfreformulation. E Zhang, X Yan, W Lin, T Zhang, Q Lu, arXiv:2505.083642025arXiv preprint</p>
<p>Absolute zero: Reinforced self-play reasoning with zero data. A Zhao, Y Wu, Y Yue, T Wu, Q Xu, M Lin, S Wang, Q Wu, Z Zheng, G Huang, arXiv:2505.033352025arXiv preprint</p>
<p>C Zheng, S Liu, M Li, X.-H Chen, B Yu, C Gao, K Dang, Y Liu, R Men, A Yang, arXiv:2507.18071Group sequence policy optimization. 2025arXiv preprint</p>
<p>Y Zhou, S Levine, J Weston, X Li, S Sukhbaatar, arXiv:2506.01716Self-challenging language model agents. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
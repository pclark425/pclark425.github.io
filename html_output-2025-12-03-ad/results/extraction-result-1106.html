<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1106 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1106</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1106</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-258490421</p>
                <p><strong>Paper Title:</strong> AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q</p>
                <p><strong>Paper Abstract:</strong> This paper proposes an active information-directed reinforcement learning (AID-RL) framework for autonomous source seeking and estimation problem. Source seeking requires the search agent to move towards the true source, and source estimation demands the agent to maintain and update its knowledge regarding the source properties such as release rate and source position. These two objectives give rise to the newly developed framework, namely, dual control for exploration and exploitation. In this paper, the greedy RL forms an exploitation search strategy that navigates the agent to the source position, while the information-directed search commands the agent to explore most informative positions to reduce belief uncertainty. Extensive results are presented using a high-ﬁdelity dataset for autonomous search, which validates the effectiveness of the proposed AID-RL and highlights the importance of active exploration in improving sampling efﬁciency and search performance. (cid:1) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1106.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1106.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AID-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Information-Directed Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid model-free/model-based RL algorithm that combines greedy Q-learning (exploitation) with information-directed exploration (Entrotaxis) using a particle filter to estimate source parameters and quantify belief uncertainty for autonomous source seeking and estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AID-RL (Active Information-Directed Reinforcement Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hybrid agent combining a tabular/state-action greedy Q-learning (Q-table) component for reward-driven exploitation and a model-based inference engine using a particle filter to maintain a posterior over physically meaningful dispersion/source parameters; exploration actions are chosen by an Entrotaxis-style information gain maximisation computed from the particle-filter belief.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information-directed exploration (information gain / maximum entropy sampling / Entrotaxis) with particle-filter belief uncertainty; integrates active learning / dual-control concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision step the agent probabilistically (with probability epsilon) chooses between: (a) information-directed exploration — selecting the action a that maximises expected Shannon-entropy (information gain) of the predicted future measurement distribution computed by marginalising the dispersion model over particle-filter samples; or (b) reward-driven exploitation — selecting the action that maximises the current Q(p,a). The particle filter is updated after each observation to refine the belief over source parameters, which in turn changes the predicted measurement distributions and future information gains; the Q-table is updated by standard Q-learning value iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>High-fidelity experimental dispersion dataset (COANDA/DST group) representing airborne point-release plume over a 49x98 grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable and stochastic: noisy/sparse concentration sensor measurements, local turbulence causing non-stationary/rapidly changing dispersion fields between frames; sparse rewards due to frequent non-detections; discrete grid state representation; physically parameterised (convection-diffusion) model available for belief updates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Grid map of 49 rows x 98 columns (4,802 cells); action space of 4 discrete manoeuvres; episode maximum path length 1,000 steps; dataset length 340 sequential frames sampled at 0.435 s; particle filter with N=1,000 particles used for belief; training over M=5,000 episodes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative improvements: AID-RL achieves faster adaptation during early training, shorter average path lengths (moving-average) across training compared to random-exploration baseline, and better source localisation and parameter estimation with fewer training episodes; exact numeric metrics (e.g., path-length mean/std, success rates) are not reported in the text (figures referenced but numeric values not given).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as higher than epsilon-greedy RL: AID-RL converges faster (not quantified numerically) and requires fewer episodes to reach effective localisation; precise episode counts to reach specific performance thresholds are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by an epsilon hyperparameter: with probability epsilon the agent performs information-directed exploration (maximises expected entropy/information gain computed from particle-filter belief), otherwise it performs greedy exploitation by selecting argmax_a Q(p,a). Epsilon can be scheduled (high initially to promote probing, then decreased).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>-greedy (epsilon-greedy) RL baseline (random exploration); references and conceptual comparisons to Entrotaxis, Infotaxis, IPP, and DCEE are discussed; experiments directly compare only to epsilon-greedy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Integrating Entrotaxis-style information-directed exploration into greedy RL (AID-RL) yields clear empirical benefits on a real high-fidelity dispersion dataset: accelerated early-stage learning, improved sampling efficiency, shorter trajectories on average to reach the source, and simultaneous recovery of meaningful source parameters via the particle filter. The paper emphasises that information-aware exploration is especially valuable under sparse, noisy measurements and non-stationary turbulent plume conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No quantitative failure-rate reporting; single early-episode example shows failure to locate the source (typical in early training). Reported caveats include narrowing of the performance gap after prolonged training (AID-RL advantage strongest early), reliance on an explicit dispersion model and particle filter (i.e., approach depends on model availability and computational cost of particle filtering), hyperparameter tuning (epsilon scheduling) required, and no multi-agent experiments (future work suggested).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1106.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1106.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>epsilon-greedy RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon-greedy Reinforcement Learning (random exploration baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Q-learning agent that selects the greedy action with probability 1-epsilon and a random action with probability epsilon; used here as the baseline RL approach with undirected (random) exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>epsilon-greedy Q-learning baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular Q-learning (state-action value iteration) using the same reward function (log concentration) and same state/action discretisation; exploration via random action selection (epsilon probability) or noise perturbation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Undirected random exploration (epsilon-greedy), not an adaptive experimental design method.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Exploration is performed by selecting a random action with fixed probability epsilon; Q-values are updated by standard value-iteration Q-learning using collected rewards; no explicit belief or information-gain is computed to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same high-fidelity experimental dispersion dataset (49x98 grid) used in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic, sparse noisy measurements, non-stationary plume fields, discrete grid and 4-action set (same as AID-RL).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same as AID-RL: 4,802 grid cells, 4 discrete actions, max episode length 1,000, training over M=5,000 episodes in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported qualitatively as lower than AID-RL: random exploration leads to poorer sampling efficiency and slower early-stage adaptation; no numeric sample-efficiency numbers are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled solely by epsilon parameter of epsilon-greedy; random actions chosen with probability epsilon regardless of expected information value.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally against AID-RL in terms of moving-average path length and trajectory examples; no other experimental baselines provided in the results section.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Serves as the primary baseline; exhibits more random early behaviour and longer average path lengths compared to AID-RL, indicating that directed information-aware exploration accelerates learning and reduces path length under noisy/sparse plume sensing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient undirected exploration in sparse/noisy, partially observable plume environments; slower convergence and worse early-stage performance; lacks mechanism to actively reduce belief uncertainty or recover physically meaningful source parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1106.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1106.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entrotaxis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entrotaxis (information-directed informative path planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-theoretic path-planning exploration strategy that selects next measurements to maximise information gain (entropy reduction) about source parameters; used as the exploration component within AID-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Entrotaxis (information-directed exploration component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Explorer that computes expected measurement distributions by marginalising the physics-based dispersion model over the current particle-filter posterior and selects actions that maximise Shannon entropy (expected information gain) of predicted future measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Information gain maximisation / maximum entropy sampling (classic Entrotaxis formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts action choice to current belief by calculating expected distribution of future measurements for each candidate action using the particle-filter posterior over source parameters, and picks the action with maximum expected entropy; belief updated after each observation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same high-fidelity plume dataset / grid-world used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable, stochastic plume with noisy and sparse concentration readings, non-stationary over sequential frames; requires dispersion model to predict measurements given source parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Action set of 4 moves on 49x98 grid; particle-filter based belief over continuous, physically-meaningful parameters (source location and release rate); computational cost depends on number of particles (N=1,000 used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>When used within AID-RL it contributes to improved sample efficiency and faster early learning; standalone quantitative sample-efficiency figures are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Within AID-RL it is invoked probabilistically according to epsilon; if used standalone it directs exploration solely by information gain without reward-driven exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Conceptually compared to Infotaxis, IPP, and DCEE; within the paper Entrotaxis is the exploration mechanism contrasted with epsilon-greedy random exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Entrotaxis-style, particle-filter-backed information-directed exploration directs agents to informative locations (often near the source) and when integrated into RL substantially improves early-stage learning and overall sampling efficiency under noisy, sparse measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires a physics-based dispersion model and a Bayesian inference engine (particle filter); computational overhead from particle filtering can be significant; standalone Entrotaxis results are not quantitatively reported in this paper (it is used as a component within AID-RL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions. <em>(Rating: 2)</em></li>
                <li>Infotaxis' as a strategy for searching without gradients. <em>(Rating: 2)</em></li>
                <li>Dual control for exploitation and exploration (DCEE) in autonomous search. <em>(Rating: 2)</em></li>
                <li>VIME: Variational information maximizing exploration. <em>(Rating: 1)</em></li>
                <li>Model-based active exploration. <em>(Rating: 1)</em></li>
                <li>Source term estimation using deep reinforcement learning with Gaussian mixture model feature extraction for mobile sensors. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1106",
    "paper_id": "paper-258490421",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "AID-RL",
            "name_full": "Active Information-Directed Reinforcement Learning",
            "brief_description": "A hybrid model-free/model-based RL algorithm that combines greedy Q-learning (exploitation) with information-directed exploration (Entrotaxis) using a particle filter to estimate source parameters and quantify belief uncertainty for autonomous source seeking and estimation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "AID-RL (Active Information-Directed Reinforcement Learning)",
            "agent_description": "Hybrid agent combining a tabular/state-action greedy Q-learning (Q-table) component for reward-driven exploitation and a model-based inference engine using a particle filter to maintain a posterior over physically meaningful dispersion/source parameters; exploration actions are chosen by an Entrotaxis-style information gain maximisation computed from the particle-filter belief.",
            "adaptive_design_method": "Information-directed exploration (information gain / maximum entropy sampling / Entrotaxis) with particle-filter belief uncertainty; integrates active learning / dual-control concepts.",
            "adaptation_strategy_description": "At each decision step the agent probabilistically (with probability epsilon) chooses between: (a) information-directed exploration — selecting the action a that maximises expected Shannon-entropy (information gain) of the predicted future measurement distribution computed by marginalising the dispersion model over particle-filter samples; or (b) reward-driven exploitation — selecting the action that maximises the current Q(p,a). The particle filter is updated after each observation to refine the belief over source parameters, which in turn changes the predicted measurement distributions and future information gains; the Q-table is updated by standard Q-learning value iteration.",
            "environment_name": "High-fidelity experimental dispersion dataset (COANDA/DST group) representing airborne point-release plume over a 49x98 grid",
            "environment_characteristics": "Partially observable and stochastic: noisy/sparse concentration sensor measurements, local turbulence causing non-stationary/rapidly changing dispersion fields between frames; sparse rewards due to frequent non-detections; discrete grid state representation; physically parameterised (convection-diffusion) model available for belief updates.",
            "environment_complexity": "Grid map of 49 rows x 98 columns (4,802 cells); action space of 4 discrete manoeuvres; episode maximum path length 1,000 steps; dataset length 340 sequential frames sampled at 0.435 s; particle filter with N=1,000 particles used for belief; training over M=5,000 episodes reported.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative improvements: AID-RL achieves faster adaptation during early training, shorter average path lengths (moving-average) across training compared to random-exploration baseline, and better source localisation and parameter estimation with fewer training episodes; exact numeric metrics (e.g., path-length mean/std, success rates) are not reported in the text (figures referenced but numeric values not given).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Reported qualitatively as higher than epsilon-greedy RL: AID-RL converges faster (not quantified numerically) and requires fewer episodes to reach effective localisation; precise episode counts to reach specific performance thresholds are not provided.",
            "exploration_exploitation_tradeoff": "Controlled by an epsilon hyperparameter: with probability epsilon the agent performs information-directed exploration (maximises expected entropy/information gain computed from particle-filter belief), otherwise it performs greedy exploitation by selecting argmax_a Q(p,a). Epsilon can be scheduled (high initially to promote probing, then decreased).",
            "comparison_methods": "-greedy (epsilon-greedy) RL baseline (random exploration); references and conceptual comparisons to Entrotaxis, Infotaxis, IPP, and DCEE are discussed; experiments directly compare only to epsilon-greedy RL.",
            "key_results": "Integrating Entrotaxis-style information-directed exploration into greedy RL (AID-RL) yields clear empirical benefits on a real high-fidelity dispersion dataset: accelerated early-stage learning, improved sampling efficiency, shorter trajectories on average to reach the source, and simultaneous recovery of meaningful source parameters via the particle filter. The paper emphasises that information-aware exploration is especially valuable under sparse, noisy measurements and non-stationary turbulent plume conditions.",
            "limitations_or_failures": "No quantitative failure-rate reporting; single early-episode example shows failure to locate the source (typical in early training). Reported caveats include narrowing of the performance gap after prolonged training (AID-RL advantage strongest early), reliance on an explicit dispersion model and particle filter (i.e., approach depends on model availability and computational cost of particle filtering), hyperparameter tuning (epsilon scheduling) required, and no multi-agent experiments (future work suggested).",
            "uuid": "e1106.0",
            "source_info": {
                "paper_title": "AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "epsilon-greedy RL",
            "name_full": "Epsilon-greedy Reinforcement Learning (random exploration baseline)",
            "brief_description": "Standard Q-learning agent that selects the greedy action with probability 1-epsilon and a random action with probability epsilon; used here as the baseline RL approach with undirected (random) exploration.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "epsilon-greedy Q-learning baseline",
            "agent_description": "Tabular Q-learning (state-action value iteration) using the same reward function (log concentration) and same state/action discretisation; exploration via random action selection (epsilon probability) or noise perturbation during training.",
            "adaptive_design_method": "Undirected random exploration (epsilon-greedy), not an adaptive experimental design method.",
            "adaptation_strategy_description": "Exploration is performed by selecting a random action with fixed probability epsilon; Q-values are updated by standard value-iteration Q-learning using collected rewards; no explicit belief or information-gain is computed to guide exploration.",
            "environment_name": "Same high-fidelity experimental dispersion dataset (49x98 grid) used in the paper",
            "environment_characteristics": "Partially observable, stochastic, sparse noisy measurements, non-stationary plume fields, discrete grid and 4-action set (same as AID-RL).",
            "environment_complexity": "Same as AID-RL: 4,802 grid cells, 4 discrete actions, max episode length 1,000, training over M=5,000 episodes in experiments.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "Reported qualitatively as lower than AID-RL: random exploration leads to poorer sampling efficiency and slower early-stage adaptation; no numeric sample-efficiency numbers are provided in the text.",
            "exploration_exploitation_tradeoff": "Controlled solely by epsilon parameter of epsilon-greedy; random actions chosen with probability epsilon regardless of expected information value.",
            "comparison_methods": "Compared experimentally against AID-RL in terms of moving-average path length and trajectory examples; no other experimental baselines provided in the results section.",
            "key_results": "Serves as the primary baseline; exhibits more random early behaviour and longer average path lengths compared to AID-RL, indicating that directed information-aware exploration accelerates learning and reduces path length under noisy/sparse plume sensing.",
            "limitations_or_failures": "Inefficient undirected exploration in sparse/noisy, partially observable plume environments; slower convergence and worse early-stage performance; lacks mechanism to actively reduce belief uncertainty or recover physically meaningful source parameters.",
            "uuid": "e1106.1",
            "source_info": {
                "paper_title": "AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Entrotaxis",
            "name_full": "Entrotaxis (information-directed informative path planning)",
            "brief_description": "An information-theoretic path-planning exploration strategy that selects next measurements to maximise information gain (entropy reduction) about source parameters; used as the exploration component within AID-RL.",
            "citation_title": "Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions.",
            "mention_or_use": "use",
            "agent_name": "Entrotaxis (information-directed exploration component)",
            "agent_description": "Explorer that computes expected measurement distributions by marginalising the physics-based dispersion model over the current particle-filter posterior and selects actions that maximise Shannon entropy (expected information gain) of predicted future measurements.",
            "adaptive_design_method": "Information gain maximisation / maximum entropy sampling (classic Entrotaxis formulation).",
            "adaptation_strategy_description": "Adapts action choice to current belief by calculating expected distribution of future measurements for each candidate action using the particle-filter posterior over source parameters, and picks the action with maximum expected entropy; belief updated after each observation.",
            "environment_name": "Same high-fidelity plume dataset / grid-world used in experiments",
            "environment_characteristics": "Partially observable, stochastic plume with noisy and sparse concentration readings, non-stationary over sequential frames; requires dispersion model to predict measurements given source parameters.",
            "environment_complexity": "Action set of 4 moves on 49x98 grid; particle-filter based belief over continuous, physically-meaningful parameters (source location and release rate); computational cost depends on number of particles (N=1,000 used in experiments).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": "When used within AID-RL it contributes to improved sample efficiency and faster early learning; standalone quantitative sample-efficiency figures are not provided in the paper.",
            "exploration_exploitation_tradeoff": "Within AID-RL it is invoked probabilistically according to epsilon; if used standalone it directs exploration solely by information gain without reward-driven exploitation.",
            "comparison_methods": "Conceptually compared to Infotaxis, IPP, and DCEE; within the paper Entrotaxis is the exploration mechanism contrasted with epsilon-greedy random exploration.",
            "key_results": "Entrotaxis-style, particle-filter-backed information-directed exploration directs agents to informative locations (often near the source) and when integrated into RL substantially improves early-stage learning and overall sampling efficiency under noisy, sparse measurements.",
            "limitations_or_failures": "Requires a physics-based dispersion model and a Bayesian inference engine (particle filter); computational overhead from particle filtering can be significant; standalone Entrotaxis results are not quantitatively reported in this paper (it is used as a component within AID-RL).",
            "uuid": "e1106.2",
            "source_info": {
                "paper_title": "AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions.",
            "rating": 2,
            "sanitized_title": "entrotaxis_as_a_strategy_for_autonomous_search_and_source_reconstruction_in_turbulent_conditions"
        },
        {
            "paper_title": "Infotaxis' as a strategy for searching without gradients.",
            "rating": 2,
            "sanitized_title": "infotaxis_as_a_strategy_for_searching_without_gradients"
        },
        {
            "paper_title": "Dual control for exploitation and exploration (DCEE) in autonomous search.",
            "rating": 2,
            "sanitized_title": "dual_control_for_exploitation_and_exploration_dcee_in_autonomous_search"
        },
        {
            "paper_title": "VIME: Variational information maximizing exploration.",
            "rating": 1,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        },
        {
            "paper_title": "Model-based active exploration.",
            "rating": 1,
            "sanitized_title": "modelbased_active_exploration"
        },
        {
            "paper_title": "Source term estimation using deep reinforcement learning with Gaussian mixture model feature extraction for mobile sensors.",
            "rating": 1,
            "sanitized_title": "source_term_estimation_using_deep_reinforcement_learning_with_gaussian_mixture_model_feature_extraction_for_mobile_sensors"
        }
    ],
    "cost": 0.01051125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q</p>
<p>Zhongguo Li zhongguo.li@ucl.ac.uk 
Department of Computer Science
University College London
WC1E 6BTLondonUK</p>
<p>Wen-Hua Chen w.chen@lboro.ac.uk 
Department of Aeronautical and Automotive Engineering
Loughborough University
LE11 3TULoughboroughUK</p>
<p>Jun Yang j.yang3@lboro.ac.uk 
Department of Aeronautical and Automotive Engineering
Loughborough University
LE11 3TULoughboroughUK</p>
<p>Yunda Yan yunda.yan@dmu.ac.uk 
School of Engineering and Sustainable Development
De Montfort University
LeicesterLE1 9BHUK</p>
<p>AID-RL: Active information-directed reinforcement learning for autonomous source seeking and estimation q
F9CDE07B2DEA500787FFAAE21145C6B910.1016/j.neucom.2023.126281Received 3 January 2023 Revised 13 February 2023 Accepted 22 April 2023 Available online 3 May 2023Autonomous search Reinforcement learning Dual control Active learning Exploration Exploitation
This paper proposes an active information-directed reinforcement learning (AID-RL) framework for autonomous source seeking and estimation problem.Source seeking requires the search agent to move towards the true source, and source estimation demands the agent to maintain and update its knowledge regarding the source properties such as release rate and source position.These two objectives give rise to the newly developed framework, namely, dual control for exploration and exploitation.In this paper, the greedy RL forms an exploitation search strategy that navigates the agent to the source position, while the information-directed search commands the agent to explore most informative positions to reduce belief uncertainty.Extensive results are presented using a high-fidelity dataset for autonomous search, which validates the effectiveness of the proposed AID-RL and highlights the importance of active exploration in improving sampling efficiency and search performance.</p>
<p>Introduction</p>
<p>Autonomous search is to use mobile platforms equipped with physical sensors to localise and estimate a possible release of chemical, biological or radioactive substance.The mission of autonomous search for an airborne release comprises dual objectives: moving the search agent towards the source location (source seeking) and estimating the source properties (source term estimation).The importance of autonomous search has been manifested in its wide applications, including search and rescue, safety monitoring and emergency responses [1].To realise fully autonomous operation, extensive path planning and estimation approaches have been established in recent years, which can be roughly classified into three categories: informative path planning (IPP) [2][3][4], bio-inspired mechanisms [5,6] and dual control methods [7,8].The aforementioned approaches are non-episodic, i.e., the search process is not repetitive.</p>
<p>Benefiting from extensive interactions with the unknown environment, reinforcement learning has achieved remarkable success in playing complex games [9] and virtual robotic systems [10].However, its applicability in real world applications remains quite limited, mainly owing to its poor sampling efficiency.Recently, reinforcement learning based approaches have been introduced to deal with source search and estimation [11,12].A deep Q network with particle filter assisted source term estimation approach is developed in [12].Deep deterministic policy gradient (DDPG) is used to train the optimal policy together with particle filter and Gaussian mixture model for source term approximation in [11].Extensive simulation results have been reported in comparison with several benchmark algorithms including Entrotaxis [13] and Infotaxis [3].In both studies, random exploration mechanisms are employed for probing the spaces regardless of the search and estimation performance, where standard -greedy algorithm is utilised in [12] and random noise perturbation is added to the policy output in [11].Despite their exploratory efforts -inefficient random exploration, it should be noted that the source term estimation is passively updated in the sense that the estimation performance is not integrated in the decision-making processes of the RL algorithms.</p>
<p>Balancing between exploitation and exploration has been a long-standing issue in RL [14,15], particularly when observations obtained from the environment are uncertain and noisy.This is the classic dilemma of RL algorithms: should the search agent maximise its reward based on its current knowledge or explore poorly understood states and actions to potentially improve future performance [16].In autonomous search problem, the search agent inherently suffers heavily from both sensor noises and local turbulence disturbances such that concentration information collected is often sparse and noisy.In this challenging scenario, effective exploration mechanism is of vital importance to ensure mission success [15].The agent using greedy RL algorithms is in fact driven to move towards the source location, which only accounts for one of the dual objectives in an autonomous search mission [7,17,18].In order to improve the sampling efficiency and enhance the estimation performance, more delicate exploratory approaches are required.</p>
<p>In the last few years, active reinforcement learning, also termed as Bayesian reinforcement learning, has emerged as one of the hottest research areas in machine learning community driven by its prominent capability in improving data efficiency and enhancing learning performance [14,[19][20][21].To incentivise directed exploration over poorly understood states and actions, it is important to quantify the uncertainty of the agent's belief about its operational environment.Houthooft et al. [22] suggest Bayesian neural networks for uncertainty measure and develop a variational information maximising exploration strategy.Shyam et al. [23] advocate an ensemble-based approach where uncertainty is measured by the amount of conflict among the predictions of the constituent models, which is proved to be more efficient and has been widely used in recent studies [8,20].In the control system community, Chen [24] establishes a similar paradigm from control-theoretic perspective, namely Dual Control for Exploitation and Exploration (DCEE), which holds high learning efficiency for autonomous control in an uncertain and unknown environment.Both approaches are targeted to provide a solution framework to deal with autonomous decision-making problems in an uncertain environment, and rather coincidentally they both emphasise the importance of active exploration for constructing knowledge regarding the operational environment.</p>
<p>Motivated by the above observations, this paper develops an active information-directed reinforcement learning (AID-RL) to solve the autonomous search problem.To our knowledge, this is the first attempt to deploy reinforcement learning with active exploration for autonomous search and estimation problem.While there has been a significant amount of research on active RL algorithms, such as [23,22,20,15], they are not directly applicable to autonomous seeking and estimation.Most of them utilise neural networks or ensembles of randomly generated dynamic models to acquire information about the environment, which makes it challenging to extract physically meaningful parameters for source term estimation.The exploration mechanism in this paper is originated from classic information-theoretic path planning methods where Infotaxis [3,25] and Entrotaxis [13] are regarded as two benchmark representatives.Essentially, the conventional approach of exploitative RL drives the search agent towards the source location by rewarding it for collecting high concentration values (reward-driven exploitation), while the information-directed exploration aims to lead the search agent to positions that can reduce its belief uncertainty regarding the environment by maximising the information gain.The dual objectives of autonomous search are depicted in Fig. 1, where their connections with exploration and exploitation are also outlined.This information-directed exploratory RL method is partially inspired by the dual control algorithm [7,26].Compared with the benchmark -greedy RL algorithm, we demonstrate that the proposed AID-RL not only produces better search and estimation performance but also maintains high sampling efficiency using less training episodes.</p>
<p>The key contributions of this paper are summarised as follows.</p>
<p>1.This paper provides a unified formulation for autonomous search problem using information-directed RL.The proposed framework, AID-RL, unifies greedy reward-driven exploitation and information-directed exploration in autonomous search.2.An active reinforcement learning algorithm is developed, which combines a reformulated greedy Q learning algorithm and an Entrotaxis based exploration strategy.Such a new exploration mechanism, originated from traditional IPP, can significantly improve the sampling efficiency and search performance.Different from existing studies in active RL [22,23,20] where Bayesian neural networks or ensembles of dynamic models are employed to formulate information gain, the proposed AID-RL captures belief uncertainty using particle filter, from which meaningful source parameters can be extracted.3. The proposed algorithm is implemented on a real dataset, consisting of sparse and uncertain measurements.Currently, the existing algorithms using RL only validated their effectiveness through numerical simulations, which did not take measurement uncertainty and reward sparseness into consideration.In our high-fidelity studies, efficacy of RL for autonomous search is manifested and the advantages of the proposed AID-RL are demonstrated by comparing with classic -greedy RL algorithms [12].</p>
<p>The rest of this paper is organised as follows.Section 2 presents the formulation of autonomous search and estimation.In Section 3, an active reinforcement learning algorithm is developed combining reward-driven exploitation and information-directed exploration.Section 4 presents the experimental results using highfidelity dataset, and provides detailed discussions and comparisons with random-exploration RL algorithm.Conclusions are drawn in Section 5.</p>
<p>Problem formulation</p>
<p>In this section, we elaborate the key functionalities in a source search and estimation problem including the search agent and dispersion modelling, reward function formulation and Bayesian inference for source term estimation.In particular, an estimation algorithm using particle filter will be implemented to achieve source estimation, based on which information-directed reinforcement learning will be designed later in Section 3.</p>
<p>Agent and dispersion modelling</p>
<p>Autonomous search algorithm is to direct a robotic searcher, equipped with onboard sensors, to locate and estimate an airborne source from a point release.The source release is usually characterised by H s ¼ ½q s ; p s T , where q s &gt; 0 denotes a positive release rate and p s ¼ ½x s ; y s T represents the source position in a search domain X &amp; R 2 .A search agent, located at p k ¼ ½x k ; y k T , navigates its search path by choosing actions from an admissible set A ¼ f!; ; "; #g according to pre-loaded search algorithm with information collected from chemical/biological sensors.Autonomous search is mainly concerned with the high-level decisionmaking for path planning.It is assumed that the search agent has been programmed with low-level controller that can achieve the given actions in A.</p>
<p>Dispersion models are used to describe the airborne transport and diffusion of released materials.In this paper, the convectiondiffusion plume is adopted to reconstruct the source dispersion, which has been widely utilised in related studies [3,13,11,12,7].The expected concentration at agent position p k is given by
z k p k jH s ð Þ¼ q s 4pD p k À p s j j exp ÀDp k V 2D exp À p k À p s j j lð1Þ
where Dp
k ¼ À x k À x s ð ÞsinðwÞ þ y k À y s ð ÞcosðwÞ; l ¼ ffiffiffiffiffiffiffiffiffi Ds 1þ V 2 s 4D q
; V is the wind speed and w is the wind direction, D represents the isotropic diffusivity and s denotes the particle lifetime.</p>
<p>It is crystal clear that the collected concentration information is highly uncertain due to local turbulence and sensor noises, and consequently there is usually high discrepancy between the sensor readings and the modelled output from z k p k jH s ð Þ.Nevertheless, it has been proven that such a dispersion model is of great importance to achieve fast localisation and efficient source term acquisition [8,7,13].The dispersion model used in this paper has physically meaningful parameters that can be interpreted in a practical way.Learning these parameters is one of the key objectives in autonomous search.Existing active RL algorithms [22,23,20] using probabilistic predictive models or dynamic ensemble models cannot be utilised for source term estimation because they lack physical meanings.</p>
<p>Reward function</p>
<p>From the dispersion model, the expected sensor reading will be higher when the search agent moves closer to the source location.Therefore, the concentration value from sensor can naturally be used to formulate the reward function, which serves as the incentives to promote the search agent moving towards the source.It should be noted that the expected concentration value is exponen-tially decaying with respect to the distance between the search agent and the source location.In this paper, we take the logarithmic value of the concentration value as the reward function, i.e.,
Rðp k ; a k Þ ¼ logðz kþ1 ðp kþ1 Þ þ eÞ ð 2Þ
where z kþ1 ðp kþ1 Þ denotes the sensor reading at future agent position p kþ1 , and e &gt; 0 is a small positive number to avoid ill-conditioned logarithms.Future reward is determined by the current location p k and current action a k 2 A. Because sensor reading is usually sparse, i.e., no meaningful detection, e is included in (2) to deal with zero concentration measurements.As having been widely discussed in related works [27,7,13], non-detection events occur quite often due to several reasons, for example, local turbulence, sensor sensitivity and failure.This type of information sparseness is reflected by a lower reward in reinforcement learning.Note that other reward designs, such as the stage-wise function used in [11], exist as well.</p>
<p>The objective of the search agent is to maximise its cumulative reward function over an infinite horizon, denoted by
J 1 ¼ X 1 i¼1 k i R p i ; a i ð Þð3Þ
where 0 &lt; k 6 1 is the discount factor.</p>
<p>Source Term Estimation</p>
<p>Given a set of historical sensor readings Z kÀ1 :¼ fz 1 ðp 1 Þ; z 2 ðp 2 Þ; . . .; z kÀ1 ðp kÀ1 Þg, the posterior probability of the environment parameter H k can be approximated by recursive Bayesian estimation
P H k jZ k ð Þ¼ P z k jH k ð ÞP H k jZ kÀ1 ð Þ P z k jZ kÀ1 ð Þð4Þ
where
P z k jZ kÀ1 ð Þ¼ Z P z k jH k ð ÞP H k jZ kÀ1 ð Þ dH kð5Þ
and initial condition is specified as PðH 0 jZ 0 Þ ¼ PðH 0 Þ.In the Bayesian estimation process, P H k jZ kÀ1 ð Þ represents the prior distribution and the likelihood function P z k jH k ð Þis determined by the dispersion model (1).</p>
<p>A variety of source term estimation techniques have been developed in the literature, including gradient based methods [8], Gaussian mixture model [11] and particle filters [13].A comprehensive review on source term estimation algorithms can be found in [28].Among them, particle filters have been increasingly popular due to its flexibility and effectiveness in handling high uncertain and nonlinear estimation problems as in the case of autonomous search.As a result, they are widely employed to construct the nonlinear inference engine [7,13,2,27].The posterior distribution of the environment parameter can be approximated by a set of N weighted samples fH
ðiÞ k ; x ðiÞ k g N i¼1 such that P H k jZ k ð Þ% X N i¼1 xðiÞk d H k À H ðiÞ kð6Þ
where dðÁÞ denotes Dirac delta function, H ðiÞ k is a potential realisation of environment parameter at the kth step, and x ðiÞ k represents the corresponding normalised weight of the particles with P N i¼1 x ðiÞ k ¼ 1.The implementation structure of particle filter for environment estimation is summarised in Algorithm 1, and more detailed elaborations can be found in [27,7].</p>
<p>Active reinforcement learning</p>
<p>In Section 2.2, the reward function is formulated according to the sensor reading collected from the environment.Targeting to collect high reward, the search agent will be directed towards the source position where higher concentration is expected.This corresponds to one of the dual objectives, i.e., source seeking.On the other hand, active exploration can be integrated into this process to improve the source estimation performance, which is another critical objective in autonomous search and estimation.The dual objectives will then motivate our proposed framework of AID-RL in Section 3.3.</p>
<p>Reward-driven exploitation</p>
<p>In this paper, we implement the state-action iteration based Qlearning algorithm to solve the autonomous search problem.The Q value function is denoted by
Q ðp k ; a k Þ ¼ Rðp k ; a k Þ þ max a kþ1 2A kJ 1 ðp kþ1 ; a kþ1 Þ ð7Þ
and the Bellman optimality condition is given by
Q Ã ðp k ; a k Þ ¼ Rðp k ; a k Þ þ max a kþ1 2A kQ p kþ1 ; a kþ1 À Á :ð8Þ
The value iteration algorithm can be designed as
Q 0 ðp k ; a k Þ ¼ Q ðp k ; a k Þ þ a Rðp k ; a k Þ þ k max a kþ1 2A Qðp kþ1 ; a kþ1 Þ À Qðp k ; a k Þ !ð9Þ
where 0 &lt; a 6 1 is the learning rate.In viewing of the above value iteration algorithm, the control action a k is chosen to maximise the cumulative reward function J 1 .If we have an ideal Q table, pure exploitative actions will generate maximal cumulative reward.However, in real situation, the Q values are usually randomly initialised and the collected rewards during the search process are highly uncertain due to local turbulence and noises.As a result, exploiting untrustworthy Q table will deteriorate the overall search performance.Therefore, exploration efforts should be included while choosing the control action, and one of the most commonly-used mechanisms is so-called -greedy algorithm [11,12] that randomly chooses an action from the admissible set with probability 1 À .This type of random perturbation based RL algorithm is termed as undirected exploration.</p>
<p>The essence of the iteration algorithm is to update the Q table using information collected from interactions between the agent and its operational environment.The reward-driven exploitation is a model-free algorithm as in (9).Despite its wide success in robotic control without models, it is prohibitively expensive in sampling complexity.</p>
<p>Information-directed exploration</p>
<p>Reinforcement learning has been widely criticised in control and robotic society due to its poor sampling efficiency.It is undeniably true that real robotic systems cannot be implemented for hundreds and thousands trial-and-error attempts due to physical constraints, time and energy consumption and other safety issues.In recent years, significant research effort has been dedicated to improving data efficiency of RL using active learning and model based approaches [20,14].Instead of random exploration, it has been demonstrated in recent works such as [20,14] that active exploration based RL yields outstanding performance compared with passive learning in the machine learning community.Similarly, active learning based control approaches have also emerged as a promising paradigm in control community [24,29].</p>
<p>In autonomous search, it is required that the search agent reconstructs source parameters.From the perspective of dual control [7], the exploration strategy is to reduce knowledge uncertainty by directing the search agent to probe the most informative positions.Informative path planning (including Infotaxis [3] and Entrotaxis [13]) is one of the mainstreams, which has proven to be very robust and effective in localising source position and reconstructing source parameters.In this paper, we deploy Entrotaxis as the exploration algorithm, which will guide the searcher to where there is the most uncertainty in the next measurement.</p>
<p>The information measure is defined according to Shannon entropy
I a k ð Þ ¼ À Z P b z kþ1 b p kþ1 À Á jZ k À Á log P b z kþ1 b p kþ1 À Á jZ k À Á d b z kþ1ð10Þ
where ẑkþ1 pkþ1 ð Þ represents the predicted measurement at future agent position pkþ1 if action a k is taken.Given the current estimation of the source parameters, the probability density function of the future measurement can be obtained by
P ẑkþ1 pkþ1 ð ÞjZ k ð Þ ¼ Z P ẑkþ1 pkþ1 ð ÞjH k ð Þ P H k jZ k ð Þ dH k :ð11Þ
The approximation strategy using particle filter in Algorithm 1 has been well-documented in [13,30].The exploration strategy is to maximise the information gain by choosing a control action from a k 2 A, i.e.,
a Ã k ¼ arg max a k 2A I a k ð Þ f g:ð12Þ
The information gain is approximated by
I a k ð Þ % X ẑmax ẑkþ1 ¼0 P ẑkþ1 pkþ1 ð ÞjZ k ð Þ log P ẑkþ1 pkþ1 ð ÞjZ k ð Þ ð13Þ
where ẑkþ1 ¼ 0; 1; 2; . . .; ẑmax f g denotes the potential future measurements.Note that the expected information gain Iða k Þ is calculated for all possible actions in the action set A. Then, ( 12) is solved by selecting a control action that yields the maximum information gain.</p>
<p>There are a variety of informative measures to quantify knowledge uncertainties, for example, variance, mutual information and Kullback-Leibler divergence [31].By integrating knowledge uncertainty into the decision process, an information-directed mechanism for active exploration is achieved such that the probing actions are inserted to explore the most promising and informative direction instead of random search.Evidently, the inference engine using Bayesian approximation requires a dispersion model as in (1) and ( 4).Hence, the formulation of information-directed exploration is a model-based estimation approach.It is argued that encapsulating model-based knowledge in RL can greatly accelerate the learning speed and achieve high sample efficiency [32].</p>
<p>AID-RL: Active information-directed reinforcement learning</p>
<p>From the perspective of dual control [17], the formulation of exploitative RL is in fact a control-driven algorithm, which aims to navigate the search agent to the believed source location that yields higher reward.The current belief of the possible source location is encapsulated implicitly in the state-action Q values.The success of RL algorithm heavily relies on the exploration mechanism, and all current studies in autonomous search deploy random exploration strategy, i.e., -greedy RL algorithm [11,12].Considering that the efficiency of random exploration is usually quite poor in many real applications, we introduce the classic informative path planning methods to improve sampling efficiency.This new algorithm, namely active information-directed reinforcement learning, AID-RL, is partially motivated by the paradigm of recently developed DCEE in autonomous search [24].</p>
<p>The overall implementation structure of AID-RL is summarised in Algorithm 2. It is composed of an initialisation procedure and an episodic learning process.Each learning episode should be viewed as a trial of search mission, and thus the search agent is required to make sequential decisions from a randomly-initialised start positions.During each trial, the decision-making process balances between exploration and exploitation by using either information-directed search or reward-driven control.In existing benchmark algorithms, e.g., IPP [13,3] and DCEE [7], the search space is represented by grid map, which is also well-fitted to the classic Q learning.To achieve fair comparison, we will keep this classic setting for autonomous search.It is worth mentioning that the proposed AID-RL is ready to be tailored to deal with large-scale search problem by using neural network approximation [12,11].If random value &lt; , then choose a control action by information-directed exploration:
a Ã k ¼ arg max a k 2A I a k ð Þ f g 8.
Else choose a control action by reward-driven exploitation:
a Ã k ¼ arg max a k 2A Q ðp k ; a k Þ f g 9. End if 10. execute action a Ã k 11. collect reward R k ðp k ; a k Þ at p kþ1 12.
update the particle filter using Algorithm 1 13.</p>
<p>update Q table by value iteration:
Q 0 ðp k ; a k Þ ¼ Q ðp k ; a k Þ þ a Rðp k ; a k Þþ ½ k max a kþ1 2A Q ðp kþ1 ; a kþ1 Þ À Q ðp k ; a k Þ 14.
If terminal condition is satisfied, break 15.End for 16.End for Compared with the classic -greedy RL algorithm, the proposed AID-RL is of similar learning structure, except that AID-RL replaces the random exploration mechanism with an active informationdirected search algorithm.It is noticed that the random exploration mechanism usually leads to low sampling efficiency and thus consumes a large amount of training episodes.Recently, significant research efforts have been dedicated to developing efficient exploration strategy, yet there is currently no consensus in the design of exploration techniques [15,20,22,23].In this paper, the exploration search strategy is based on maximum entropy sampling principle by selecting the manoeuvre actions that navigate the agent to the most uncertain positions to reduce its knowledge uncertainty aggregated by particle filters.None of the aforementioned works in active RL have implemented such particle filter assisted exploration.It is necessary to use dispersion models that have physical meanings in order to meet the practical requirements of autonomous search, i.e., source term estimation.</p>
<p>In Algorithm 2, the hyper-parameter 2 ð0; 1Þ plays a central role in balancing between exploration and exploitation.In essence, by varying the value of , the search agent will alter between information-directed exploration and reward-driven exploitation with a probability determined by .A large value of means the search agent spends more efforts in probing the search space in order to reduce its belief uncertainty regarding the environment, while a small value of emphasises on the exploitation of its current Q table to move closer to its believed source position.It is worth mentioning that the tuning principle in -greedy RL can be applied to our proposed AID-RL.For example, the value of can be set as a large value initially to enable the search agent have more opportunity to probe the environment since the Q table is not trustworthy at the early stage of training.Then, after the initial training period, the value of can be decreased to let the agent make use of its belief.</p>
<p>AID-RL is a hybrid approach that combines model-free greedy RL and model-based source term estimation.Model-free RL provides an effective way to capture to the features of autonomous search by using state-action Q table.On the other hand, the model-based estimation technique enables fast adaptation of the source knowledge and provides an uncertainty measure to develop our information-aware RL algorithm.Recently, remarkable empirical results have been reported in many studies to demonstrate the combined strength of model-based and model-free (MB-MF) algorithms, for example, [10,14,20].</p>
<p>Table 1 summarises key features and differences of AID-RL compared with existing autonomous search algorithms.RL-based algorithms rely on episodic path sampling from randomly-initialised start points, and thereby are fundamentally different from those classic methods, such as bio-inspired search, IPP and DCEE.The concept of DCEE provides a new perspective to elucidate the dual objectives in source tracking and estimation: one is related to exploitation and another is linked with exploration [7,17].Information-directed exploration dictates the search agent to probe most uncertain locations, and consequently improves the estimation performance due to accelerated information acquisition.Incorporating this information-aware exploration mechanism into the greedy RL (essentially, greedy RL aims to seek the source position where there is maximum reward), a balanced trade-off between exploration and exploitation is achieved.Additionally, it is important to mention that if the RL component is removed, the proposed AID-RL approach reverts back to Entrotaxis, which is a well-known IPP method.</p>
<p>Experimental results and discussions</p>
<p>In this section, we validate the effectiveness of the proposed lected by COANDA Research and Development Corporation, and supplied by the DST group [25].To demonstrate the advantages of AID-RL, we compare the proposed algorithm with the random exploration RL algorithm, i.e., -greedy learning.</p>
<p>The dataset contains a total number of 340 sequential frames, and each of them is composed of 49 Â 98 point-wise measurements over the entire search space.Therefore, the search space is represented by a map of 49 rows and 98 columns, and each cell corresponds to the square area of 2:935 Â 2:935 mm 2 .Although the frames are sampled sequentially with a sampling time t ¼ 0:435s, the dispersion filed changes significantly from one sample to another.In real airborne source search, it is exactly the case that the dispersion and measurements change rapidly due to local turbulence and sensor noises.Key parameters of the proposed AID-RL are summarised in Table 2.</p>
<p>To show the search behaviour of the agent, we present two representative patterns at the beginning and at the end of training process, as shown in Fig. 2. The example path shown in Fig. 2a is sampled at the first episode.This search trial is classified as a failure search because the search agent fails to locate the true source after reaching the maximum path length of 1; 000.Although this search path is taken at the earliest stage of training, it is clear that the search agent gradually approaches to the source location due to the deployment of information-directed exploration.The second path in Fig. 2b is sampled at the last episode, which successfully leads the agent to the true source position with a reasonably short path.The green dots represent the agent's belief of source location, assembled from particle filters.It is observed that the proposed AID-RL algorithm achieves efficient source localisation and simultaneously acquires meaningful source parameters.</p>
<p>RL algorithms are distinct from other classic path planning methods.They aim to approximate the optimal solution over the entire search space, while traditional path planning methods, like Entrotaxis and DCEE, solve the optimisation problem from the current state.This fundamental difference gives rise to the nature of RL, i.e., episodic training over all possible actions and initial states.</p>
<p>Random exploration based RL [12], -greedy, achieves this by adding random perturbation to its greedy actions.</p>
<p>While keeping all parameters of -greedy RL the same as AID-RL, we depict the moving average of the path length in Fig. 3.It is clear that the influence of information-directed exploration is more significant at the early stage of training, during which AID-RL demonstrates fast adaptation because of the deployment of active exploration.From extensive simulation and experimental results of the classic IPP methods (see e.g., [25,13,33]), the search agent will gradually approach to the source while seeking to explore the most uncertain positions in the domain.This reveals the fundamental reason for the success of IPP as most informative locations are usually adjacent to the source position.Comparing the average path length in Fig. 3, AID-RL outperforms random exploration based RL at all stages of training, even though the performance margin of AID-RL is narrowed after a significant amount of episodic training.Encompassing information-directed exploration into RL algorithm greatly accelerates the process of finding source location.Fig. 4 shows two representative trajectories of the search agent at the beginning and the end of the training process using -greedy RL.It is obvious that the search agent tends to move close to the source by using the information-directed exploration mechanism (Fig. 2a), while the search pattern using -greedy RL algorithm is more random without any clear direction (Fig. 4a).This clearly manifests that information/uncertainty awareness plays a central role in directing the agent to conduct more effective and efficient exploration.At the end of training process, both algorithms can successfully lead the search agent to the source location but AID-RL requires less steps in average to approach the source as shown in Fig. 3.</p>
<p>Conclusions</p>
<p>In this paper, an active exploration autonomous search framework has been established based on greedy reinforcement learning.Inspired by the pioneering work in dual control [24], we propose an information-directed reinforcement learning to enable a balanced trade-off between exploration and exploitation.The greedy RL essentially implements an exploitation strategy that navigates the search agent to collect maximum reward (concentration), and the Entrotaxis search enables the agent to explore the most uncertain areas to improve the level of belief confidence.Such a model-based and model-free (MB-MF) paradigm shares the strengths from both sides.From the experiment results using high-fidelity dataset, the proposed AID-RL greatly improves the search and estimation performance and consumes less training episodes compared with traditional -greedy algorithms.</p>
<p>Recently, multi-agent systems have shown great potential in solving complex problems using collaborative swarm robots [34,35].One of the key advantages of using multi-agent systems in autonomous search problems is that it enables collective intelligence, where the collaboration of multiple agents can lead to improved performance and faster problem-solving.Therefore, our future research efforts will focus on developing a distributed framework for AID-RL.This framework will incorporate multiple agents working together to achieve the source search goal, and our aim is to further enhance the search performance and learning speed by allowing the agents to collaborate and share information.</p>
<p>Fig. 1 .
1
Fig. 1.Dual objectives in autonomous search and estimation.</p>
<p>Algorithm 1 :
1
Particle filter for environment parameter estimation.Require: prior samples fH ðiÞ kÀ1 ; x ðiÞ kÀ1 g N i¼1</p>
<p>Algorithm 2 : 6 .
26
AID-RL: Active information-directed reinforcement learning for autonomous search.1. Q values of state-action Q ðp 0 ; a 0 Þ 2. prior knowledge of source fH ðiÞ 0 ; x ðiÞ 0 g N i¼1 3. initialise learning hyper-parameters Episodic learning: 4. For episode = 1 : M do 5. randomly initialise agent location p 0 For k ¼ 1 : MaxIt do 7.</p>
<p>Fig. 2 .
2
Fig. 2. Representative search paths using AID-RL at the beginning and end of the training process, respectively.Red line represents the search trajectory; green dots denote the estimated source location in the particle filter; blue dot denotes the agent start point (randomly initialised at each episode); blue star marks the end point of the search agent and back dot indicates the location of the true source; the greyscale shading delineates the instantaneous dispersion field at the current step.</p>
<p>CRediT authorship contribution statement Zhongguo Li: Conceptualization, Methodology, Software, Writing -original draft.Wen-Hua Chen: Conceptualization, Writingreview &amp; editing, Supervision, Funding acquisition.Jun Yang: Methodology, Supervision, Writing -review &amp; editing.Yunda Yan: Conceptualization, Methodology, Writing -review &amp; editing.</p>
<p>Fig. 4 .
4
Fig. 4. Representative search paths using -greedy RL at the beginning and end of the training process, respectively.</p>
<p>Fig. 3 .
3
Fig. 3. Moving average of the path length under information-directed and random exploration RL, respectively.</p>
<ol>
<li>for i ¼ 1; 2; . ..; N, do2.draw sample H ðiÞ
k $ qðH ðiÞ kÀ1 Þ x ðiÞ k ¼ x ðiÞ P z k jH ð kÀ1 Á q H ðiÞ k ðiÞ ÞP H ðiÞ k ð k jH ðiÞ kÀ1 ;Z k jH ðiÞ kÀ1 ð Þ 5. normalise sample weights x ðiÞ 3. assign weight 4. end for k ¼ x ðiÞ k P N i¼1 x ðiÞ k 6. calculate effective sample size N eff ¼ 1=R N Þ i¼1 w k ðiÞ27. if N eff is less than a threshold N T then 8. resample fH ðiÞ k ; x ðiÞ N k g i¼19. apply a Markov chain Monte Carlo move10. end ifEnsure: posterior samples fHðiÞ k ; x ðiÞ k g N i¼1</li>
</ol>
<p>Table 2
2
Key parameters for AID-RL.
ParameterValueDescriptionEpisodes5; 000M, number of total trialsMaximum path1; 000MaxIt, maximum path lengthDiscount0:9k, discount for future rewardLearning rate0:01a, learning step per iterationExploration0:2, rate of explorationParticle filter1; 000N, number of particles</p>
<p>Table 1
1
Characteristic comparison of different autonomous search algorithms.
AlgorithmReferenceObjectiveExploration mechanismSampling mechanismBio-inspired methods[6,5]trackingnonenon-episodicIPP[25,13,3]estimationinformation-directed explorationnon-episodicDCEE[7,8]tracking and estimationinformation-directed explorationnon-episodic-greedy RL[11,12]tracking and estimationrandom explorationepisodicAID-RLthis papertracking and estimationinformation-directed explorationepisodic
Z.Li, W.-H. Chen, J. Yang et al. Neurocomputing 544 (2023) 126281
Data availabilityData will be made available on request.Declaration of Competing InterestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.q This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) Established Career Fellowship ''Goal-Oriented Control Systems: Disturbance, Uncertainty and Constraints" under the grant number EP/T005734/1.
An overview of small unmanned aerial vehicles for air quality measurements: Present applications and future prospectives. T F Villa, F Gonzalez, B Miljievic, Z D Ristovski, L Morawska, Sensors. 16710722016</p>
<p>Information-based search for an atmospheric release using a mobile robot: Algorithm and experiments. M Hutchinson, C Liu, W.-H Chen, IEEE Trans. Control Syst. Technol. 2762018</p>
<p>Infotaxis' as a strategy for searching without gradients. M Vergassola, E Villermaux, B I Shraiman, Nature. 44571262007</p>
<p>Information driven localisation of a radiological point source. B Ristic, A Gunatilaka, Inform. Fusion. 922008</p>
<p>Source exploration for an under-actuated system: A control-theoretic paradigm. X Jiang, S Li, B Luo, Q Meng, IEEE Trans. Control Syst. Technol. 2832019</p>
<p>J B Stock, M Baker, Chemotaxis Encyclopedia of Microbiology. Elsevier Inc2009</p>
<p>Dual control for exploitation and exploration (DCEE) in autonomous search. W.-H Chen, C Rhodes, C Liu, Automatica. 1332021</p>
<p>Concurrent active learning in autonomous airborne source search: Dual control for exploration and exploitation. Z Li, W.-H Chen, J Yang, IEEE Trans. Autom. Control. 682023</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875402015</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. A Nagabandi, G Kahn, R S Fearing, S Levine, IEEE International Conference on Robotics and Automation (ICRA). IEEE2018. 2018</p>
<p>Source term estimation using deep reinforcement learning with Gaussian mixture model feature extraction for mobile sensors. M Park, P Ladosz, H Oh, IEEE Robot. Autom. Lett. 732022</p>
<p>A deep reinforcement learning based searching method for source localization. Y Zhao, B Chen, X Wang, Z Zhu, Y Wang, G Cheng, R Wang, R Wang, M He, Y Liu, Inf. Sci. 5882022</p>
<p>Entrotaxis as a strategy for autonomous search and source reconstruction in turbulent conditions. M Hutchinson, H Oh, W.-H Chen, Inform. Fusion. 422018</p>
<p>The difficulty of passive learning in deep reinforcement learning. G Ostrovski, P S Castro, W Dabney, Adv. Neural Inform. Process. Syst. 342021</p>
<p>Exploration in deep reinforcement learning: A survey. P Ladosz, L Weng, M Kim, H Oh, Inform. Fusion. 2022</p>
<p>Information-directed exploration for deep reinforcement learning. N Nikolov, J Kirschner, F Berkenkamp, A Krause, arXiv:1812.075442018arXiv preprint</p>
<p>A dual control perspective for exploration and exploitation in autonomous search. Z Li, W.-H Chen, J Yang, 2022 European Control Conference. ECC) IEEE2022</p>
<p>Autonomous source term estimation in unknown environments: From a dual control concept to UAV deployment. C Rhodes, C Liu, W.-H Chen, IEEE Robot. Autom. Lett. 722021</p>
<p>M Ghavamzadeh, S Mannor, J Pineau, A Tamar, Foundations and Trends Ò in Machine Learning. 20158</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Advances in Neural Information Processing Systems 31 (NIPS). 2018. 201831</p>
<p>Prior preference learning from experts: Designing a reward with active inference. J Y Shin, C Kim, H J Hwang, Neurocomputing. 4922022</p>
<p>VIME: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in Neural Information Processing Systems. 201629</p>
<p>Model-based active exploration. P Shyam, W Jas, F Gomez, International Conference on Machine Learning. PMLR2019</p>
<p>Perspective view of autonomous control in unknown environment: Dual control for exploitation and exploration vs reinforcement learning. W.-H Chen, Neurocomputing. 2022</p>
<p>A study of cognitive strategies for an autonomous search. B Ristic, A Skvortsov, A Gunatilaka, Inform. Fusion. 282016</p>
<p>Dual control of exploration and exploitation for self-optimisation control in uncertain environments. Z Li, W.-H Chen, J Yang, Y Yan, arXiv:2301.119842023arXiv preprint</p>
<p>Source term estimation of a hazardous airborne release using an unmanned aerial vehicle. M Hutchinson, C Liu, W Chen, J. Field Robot. 3642019</p>
<p>A review of source term estimation methods for atmospheric dispersion events using static or mobile sensors. M Hutchinson, H Oh, W.-H Chen, Inform. Fusion. 362017</p>
<p>Stochastic model predictive control with active uncertainty learning: A survey on dual control. A Mesbah, Annu. Rev. Control. 452018</p>
<p>Entrotaxis-jump as a hybrid search algorithm for seeking an unknown emission source in a large-scale area with road network constraint. Y Zhao, B Chen, Z Zhu, F Chen, Y Wang, D Ma, Expert Syst. Appl. 1572020</p>
<p>An information-based learning approach to dual control. T Alpcan, I Shames, IEEE Trans. Neural Networks Learn. Syst. 26112015</p>
<p>Model-based reinforcement learning: A survey, Foundations and Trends. T M Moerland, J Broekens, A Plaat, C M Jonker, Mach. Learn. 1612023</p>
<p>Information-driven gas source localization exploiting gas and wind local measurements for autonomous mobile robots. P Ojeda, J Monroy, J Gonzalez-Jimenez, IEEE Robot. Autom. Lett. 622021</p>
<p>Decentralised and cooperative control of multi-robot systems through distributed optimisation. Y Dong, Z Li, X Zhao, Z Ding, X Huang, The 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS). 20232023</p>
<p>Search and rescue with sparsely connected swarms. U Dah-Achinanon, S E Marjani Bajestani, P.-Y Lajoie, G Beltrame, Autonomous Robots. 2023</p>
<p>His research interests include optimisation and decision-making for advanced control, game theory and learning in multi-agent systems, and their applications in autonomous vehicles and robotics. Wen-Hua Chen holds Chair in Autonomous Vehicles with the Department of Aeronautical and Automotive Engineering. Zhongguo Li Received The, B Eng, D Ph, Manchester, U K Manchester, respectively. He was a Research Associate with the Department of Aeronautical and Automotive Engineering. Loughborough, U.K.; London, U.K; U.K; Nanjing, China2017 and 2021. 2020 to 2022. 2013 and 2019. 2020 to 2022Loughborough University ; AI with Department of Computer Science, University College London ; Loughborough University, U.K ; Automation in Southeast University ; Loughborough University ; Engineering and Sustainable Development, De Montfort University from DecHe serves as Associate Editor or Technical Editor of IEEE Transactions on Industrial Electronics. 2022 as a Lecturer in Control Engineering. His current research interest focuses on the safety-critical control design for autonomous systems, especially related with optimisation and learning-based methods</p>
<p>. Z Li, W.-H Chen, J Yang, Neurocomputing. 5441262812023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7140 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7140</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7140</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04365f0f1db4c659c3297cb8e70c39b38ed3b487" target="_blank">Self-Evaluation Improves Selective Generation in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> ICBINB</p>
                <p><strong>Paper TL;DR:</strong> This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Paper Abstract:</strong> Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7140.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7140.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SampleAndEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample and Eval (pointwise true/false self-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert free-form generation to pointwise true/false evaluation by prompting the LLM to judge each sampled candidate answer (Yes/No) and use the model's token-level probability for 'Yes' as a confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Sample and Eval</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample multiple candidate answers, then for each (x,y) ask the model a pointwise true/false (Yes/No) question about correctness; use p(Yes|x,y) as the confidence score for that candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Calibration-AUC (ROC AUC for binary correct/incorrect using confidence score); Selective-AUC (area under abstention curve)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Sample and Eval (PaLM-2 Large): Accuracy 59.12%; Calibration-AUC 73.79%; Selective-AUC 58.19%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires sampling multiple candidates (n=4 in experiments). Pointwise eval improves calibration but may slightly reduce raw selection accuracy compared to sample-and-select in some settings. Computational cost increases due to extra evaluation calls. GPT-3 API limitations prevented full evaluation for some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7140.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7140.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SampleAndSelect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample and Select (multi-choice self-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate multiple candidate answers, format them as labeled choices (A,B,C,...) and ask the model to choose the best option; use token-level (unnormalized logit / character token prob) scores as selection/confidence signal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Sample and Select</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample n candidate answers, present them as a multiple-choice list, ask the model to pick the best (single-letter output); use the model's token-level score for the choice (logit or log-prob) as the score.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>voting over multiple samples / generate-then-evaluate</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Calibration-AUC; Selective-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Sample and Select (PaLM-2 Large): Accuracy 58.26%; Calibration-AUC 53.17%; Selective-AUC 48.59%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Suffers from position bias (score depends on ordering of candidates) and probability dispersion when multiple candidate answers are correct; forced-choice among only-bad candidates leads to overconfidence (mitigated via 'NONE OF THE ABOVE' candidate). De-biasing via averaging over permutations is computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7140.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7140.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HybridNota</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid (Sample and Select + Sample and Eval) with NONE OF THE ABOVE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Select the best candidate via multi-choice (Sample and Select) then score the selected answer with pointwise true/false evaluation (Sample and Eval) and penalize by the model's probability on a 'NONE OF THE ABOVE' option.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Hybrid w/ NONE OF THE ABOVE</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>First use multi-choice selection to pick a best candidate; then compute p(Yes|x,selected) and subtract p(c_nota|x,{c y}+nota) to form final confidence, allowing the model to express explicit uncertainty with 'NONE OF THE ABOVE'.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (hybrid two-step evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Calibration-AUC; Selective-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Hybrid w/ nota (PaLM-2 Large): Accuracy 58.14%; Calibration-AUC 75.34%; Selective-AUC 58.10%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Hybrid improves calibration substantially (best Calibration-AUC reported) at the cost of extra inference (authors note self-evaluation increases inference time by ~1–2×). Complexity: selection logits remain dependent on candidate set; 'nota' helps but requires that the model correctly map 'NONE OF THE ABOVE' token probability (tokenization/ API issues can complicate this).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7140.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7140.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCritiqueRevise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critique and revise (generate critique then revise answer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply an iterative single-step self-improvement loop where the model generates a critique identifying hallucinations/errors for a candidate answer, then rewrites/revises the answer conditioned on the critique.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constitutional AI: Harmlessness from AI feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-critique and revise</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each sampled answer, prompt the model to produce a critique explaining why the answer may be non-factual, then prompt it to rewrite the answer incorporating the critique; the revised answers are then re-scored by the self-evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-critique-then-revise (single reflect-and-rewrite pass)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy; Calibration-AUC; Selective-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Example baseline (Sample and Eval, PaLM-2 Large, before revise): Accuracy 59.12%; Calibration-AUC 73.79%; Selective-AUC 58.19% (from Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>After self-critique & revise (Sample and Eval on revised answers, PaLM-2 Large, Table 2): Accuracy 66.34%; Calibration-AUC 70.55%; Selective-AUC 61.81%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-critique + revise noticeably improves accuracy and selective-AUC but can slightly reduce calibration-AUC (example: Sample and Eval calibration-AUC dropped from 73.79 to 70.55). Additional inference cost from critique and rewrite steps. The paper treats self-critique as complementary to self-evaluation rather than a guaranteed calibration improvement in all metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7140.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7140.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SampleAndEval_TLDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample and Eval (pointwise) applied to TL;DR summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply the pointwise true/false evaluation to summarization: ask the model whether a proposed summary is concise and comprehensive and use that Yes probability as a confidence score for selective generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Sample and Eval (with/without other candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For generated summaries, ask a binary concise/comprehensive question (Yes/No) per candidate; optionally include other candidate summaries in the prompt; use p(Yes|text,summary) as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>TL;DR summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reddit-sourced summarization dataset (sampled 1000 examples) where generated summaries are judged for concision and comprehensiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (select best summary); Calibration-AUC; Selective-AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Sequence likelihood baseline (PaLM-2 on TL;DR): Accuracy 65.80%; Calibration-AUC 49.75%; Selective-AUC 52.63%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Sample and Eval (PaLM-2 on TL;DR): Accuracy 68.70%; Calibration-AUC 52.34%; Selective-AUC 56.09%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sample and Select gave higher accuracy on TL;DR in some variants but lower calibration; adding 'NONE OF THE ABOVE' improved calibration without large accuracy loss. Evaluation uses a reward model (71.34% accuracy predicting human ratings), so labels are approximate and may limit fidelity of calibration evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Evaluation Improves Selective Generation in Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Constitutional AI: Harmlessness from AI feedback <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 2)</em></li>
                <li>Robots that ask for help: Uncertainty alignment for large language model planners <em>(Rating: 2)</em></li>
                <li>Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs <em>(Rating: 1)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7140",
    "paper_id": "paper-04365f0f1db4c659c3297cb8e70c39b38ed3b487",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "SampleAndEval",
            "name_full": "Sample and Eval (pointwise true/false self-evaluation)",
            "brief_description": "Convert free-form generation to pointwise true/false evaluation by prompting the LLM to judge each sampled candidate answer (Yes/No) and use the model's token-level probability for 'Yes' as a confidence score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 Large",
            "model_description": "PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.",
            "model_size": null,
            "reflection_method_name": "Sample and Eval",
            "reflection_method_description": "Sample multiple candidate answers, then for each (x,y) ask the model a pointwise true/false (Yes/No) question about correctness; use p(Yes|x,y) as the confidence score for that candidate.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": null,
            "task_name": "TruthfulQA",
            "task_description": "Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).",
            "evaluation_metric": "Accuracy; Calibration-AUC (ROC AUC for binary correct/incorrect using confidence score); Selective-AUC (area under abstention curve)",
            "performance_before_reflection": "Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%",
            "performance_after_reflection": "Sample and Eval (PaLM-2 Large): Accuracy 59.12%; Calibration-AUC 73.79%; Selective-AUC 58.19%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Requires sampling multiple candidates (n=4 in experiments). Pointwise eval improves calibration but may slightly reduce raw selection accuracy compared to sample-and-select in some settings. Computational cost increases due to extra evaluation calls. GPT-3 API limitations prevented full evaluation for some variants.",
            "uuid": "e7140.0",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SampleAndSelect",
            "name_full": "Sample and Select (multi-choice self-evaluation)",
            "brief_description": "Generate multiple candidate answers, format them as labeled choices (A,B,C,...) and ask the model to choose the best option; use token-level (unnormalized logit / character token prob) scores as selection/confidence signal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 Large",
            "model_description": "PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.",
            "model_size": null,
            "reflection_method_name": "Sample and Select",
            "reflection_method_description": "Sample n candidate answers, present them as a multiple-choice list, ask the model to pick the best (single-letter output); use the model's token-level score for the choice (logit or log-prob) as the score.",
            "iteration_type": "voting over multiple samples / generate-then-evaluate",
            "num_iterations": null,
            "task_name": "TruthfulQA",
            "task_description": "Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).",
            "evaluation_metric": "Accuracy; Calibration-AUC; Selective-AUC",
            "performance_before_reflection": "Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%",
            "performance_after_reflection": "Sample and Select (PaLM-2 Large): Accuracy 58.26%; Calibration-AUC 53.17%; Selective-AUC 48.59%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Suffers from position bias (score depends on ordering of candidates) and probability dispersion when multiple candidate answers are correct; forced-choice among only-bad candidates leads to overconfidence (mitigated via 'NONE OF THE ABOVE' candidate). De-biasing via averaging over permutations is computationally expensive.",
            "uuid": "e7140.1",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "HybridNota",
            "name_full": "Hybrid (Sample and Select + Sample and Eval) with NONE OF THE ABOVE",
            "brief_description": "Select the best candidate via multi-choice (Sample and Select) then score the selected answer with pointwise true/false evaluation (Sample and Eval) and penalize by the model's probability on a 'NONE OF THE ABOVE' option.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 Large",
            "model_description": "PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.",
            "model_size": null,
            "reflection_method_name": "Hybrid w/ NONE OF THE ABOVE",
            "reflection_method_description": "First use multi-choice selection to pick a best candidate; then compute p(Yes|x,selected) and subtract p(c_nota|x,{c y}+nota) to form final confidence, allowing the model to express explicit uncertainty with 'NONE OF THE ABOVE'.",
            "iteration_type": "generate-then-reflect (hybrid two-step evaluation)",
            "num_iterations": null,
            "task_name": "TruthfulQA",
            "task_description": "Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).",
            "evaluation_metric": "Accuracy; Calibration-AUC; Selective-AUC",
            "performance_before_reflection": "Sequence likelihood baseline (PaLM-2 Large): Accuracy 48.23%; Calibration-AUC 39.80%; Selective-AUC 33.63%",
            "performance_after_reflection": "Hybrid w/ nota (PaLM-2 Large): Accuracy 58.14%; Calibration-AUC 75.34%; Selective-AUC 58.10%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Hybrid improves calibration substantially (best Calibration-AUC reported) at the cost of extra inference (authors note self-evaluation increases inference time by ~1–2×). Complexity: selection logits remain dependent on candidate set; 'nota' helps but requires that the model correctly map 'NONE OF THE ABOVE' token probability (tokenization/ API issues can complicate this).",
            "uuid": "e7140.2",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SelfCritiqueRevise",
            "name_full": "Self-critique and revise (generate critique then revise answer)",
            "brief_description": "Apply an iterative single-step self-improvement loop where the model generates a critique identifying hallucinations/errors for a candidate answer, then rewrites/revises the answer conditioned on the critique.",
            "citation_title": "Constitutional AI: Harmlessness from AI feedback",
            "mention_or_use": "use",
            "model_name": "PaLM-2 Large",
            "model_description": "PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.",
            "model_size": null,
            "reflection_method_name": "Self-critique and revise",
            "reflection_method_description": "For each sampled answer, prompt the model to produce a critique explaining why the answer may be non-factual, then prompt it to rewrite the answer incorporating the critique; the revised answers are then re-scored by the self-evaluation pipeline.",
            "iteration_type": "generate-critique-then-revise (single reflect-and-rewrite pass)",
            "num_iterations": null,
            "task_name": "TruthfulQA",
            "task_description": "Open-ended question answering benchmark measuring truthful answers against misconceptions (817 validation examples).",
            "evaluation_metric": "Accuracy; Calibration-AUC; Selective-AUC",
            "performance_before_reflection": "Example baseline (Sample and Eval, PaLM-2 Large, before revise): Accuracy 59.12%; Calibration-AUC 73.79%; Selective-AUC 58.19% (from Table 1)",
            "performance_after_reflection": "After self-critique & revise (Sample and Eval on revised answers, PaLM-2 Large, Table 2): Accuracy 66.34%; Calibration-AUC 70.55%; Selective-AUC 61.81%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Self-critique + revise noticeably improves accuracy and selective-AUC but can slightly reduce calibration-AUC (example: Sample and Eval calibration-AUC dropped from 73.79 to 70.55). Additional inference cost from critique and rewrite steps. The paper treats self-critique as complementary to self-evaluation rather than a guaranteed calibration improvement in all metrics.",
            "uuid": "e7140.3",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SampleAndEval_TLDR",
            "name_full": "Sample and Eval (pointwise) applied to TL;DR summarization",
            "brief_description": "Apply the pointwise true/false evaluation to summarization: ask the model whether a proposed summary is concise and comprehensive and use that Yes probability as a confidence score for selective generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 Large",
            "model_description": "PaLM-2 family large-scale instruction-following transformer LLM used by the authors for experiments.",
            "model_size": null,
            "reflection_method_name": "Sample and Eval (with/without other candidates)",
            "reflection_method_description": "For generated summaries, ask a binary concise/comprehensive question (Yes/No) per candidate; optionally include other candidate summaries in the prompt; use p(Yes|text,summary) as confidence.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": null,
            "task_name": "TL;DR summarization",
            "task_description": "Reddit-sourced summarization dataset (sampled 1000 examples) where generated summaries are judged for concision and comprehensiveness.",
            "evaluation_metric": "Accuracy (select best summary); Calibration-AUC; Selective-AUC",
            "performance_before_reflection": "Sequence likelihood baseline (PaLM-2 on TL;DR): Accuracy 65.80%; Calibration-AUC 49.75%; Selective-AUC 52.63%",
            "performance_after_reflection": "Sample and Eval (PaLM-2 on TL;DR): Accuracy 68.70%; Calibration-AUC 52.34%; Selective-AUC 56.09%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Sample and Select gave higher accuracy on TL;DR in some variants but lower calibration; adding 'NONE OF THE ABOVE' improved calibration without large accuracy loss. Evaluation uses a reward model (71.34% accuracy predicting human ratings), so labels are approximate and may limit fidelity of calibration evaluation.",
            "uuid": "e7140.4",
            "source_info": {
                "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Constitutional AI: Harmlessness from AI feedback",
            "rating": 2
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 2
        },
        {
            "paper_title": "Robots that ask for help: Uncertainty alignment for large language model planners",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs",
            "rating": 1
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.013517,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Evaluation Improves Selective Generation in Large Language Models</h1>
<p>Jie Ren<em>, Yao Zhao</em>, Tu Vu ${ }^{\dagger}$, Peter J. Liu<em>, Balaji Lakshminarayanan</em><br>{jjren, yaozhaoyz, ttvu, peterjliu, balajiln}@google.com<br>Google DeepMind ${ }^{\star}$, Google Research ${ }^{\dagger}$</p>
<h4>Abstract</h4>
<p>Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequencelevel probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate openended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a "None of the above" option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PalM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned on supervised data to follow instructions [Devlin et al., 2018, Radford et al., 2018, Raffel et al., 2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models. For example, the model's trustworthiness can be used as signal to selectively generate answers based on how confident the LLM is in the quality of its output.</p>
<p>Prior research has demonstrated that the distance to the training distribution in the embedding space predicts output quality for conditional generative models [Ren et al., 2023b]. Extending this work to large language models is challenging because their training distribution is too large to estimate and extracting embeddings from well-integrated LLM systems requires significant engineering effort.</p>
<p>Alternatively, a straightforward approach to estimating a language model's confidence in its output is to calculate the sequence probability or the length-normalized sequence probabilities [Adiwardana et al., 2020]. However, studies have shown that language models' sequence probabilities on openended generations do not reliably rank-order their outputs by quality [Liu et al., 2022, Ren et al., 2023b]. Human feedback can be used to fine-tune language models to better align with human-judged quality, such as with Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020], SLiC-HF [Zhao et al., 2023] and DPO [Rafailov et al., 2023], resulting in better quality-calibrated models.</p>
<p>Since human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability of LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood, recent work has shown that LLM token-level probability can be quite well-calibrated on choosing the</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of our approach.</p>
<p>correct option of multi-choice question answering and true/false questions [Kadavath et al., 2022, OpenAI, 2023, Robinson et al., 2022]. This suggests that evaluating language model's generation with token-level probabilities using an appropriate prompt format might be better for selective generation than sequence-level likelihood.</p>
<p>In this study, we focus on obtaining a confidence score that is quality-calibrated on free-form generation tasks. We propose reducing the sequence-level scoring problem to token-level scoring by designing different self-evaluation tasks and propose a variety of scores. We focus on evaluating model's quality-calibration for use in selective generation, and not just predictive accuracy. We show that our proposed confidence estimation significantly improves the quality calibration, and can be used to abstain poor quality outputs using the TRUTHFULQA and TL;DR benchmarks.</p>
<h2>2 Methods</h2>
<p><strong>Background: sequence likelihood</strong> Given a question <em>x</em> and answer <em>y</em>, <em>y</em> = <em>y</em><sup>1</sup><em>y</em><sup>2</sup> . . . <em>y</em><sup>t</sup>, we have sequence-level likelihood score,</p>
<p>$$
\log p(\mathbf{y}|\mathbf{x}) = \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Sequence likelihood)}
$$</p>
<p>Though log <em>p</em>(<strong>y</strong>|<strong>x</strong>) is statistically meaningful, it has been shown that it is biased towards sequence length, i.e. models tend to underestimate sequence likelihood of longer sentences [Wu et al., 2016]. The length normalized likelihood is an alternative score to use,</p>
<p>$$
\log \bar{p}(\mathbf{y}|\mathbf{x}) = \frac{1}{l} \sum_{t=1}^{l} \log p(y^{t}|y^{1} \dots y^{t-1}, \mathbf{x}). \qquad \text{(Length normalized sequence likelihood)}
$$</p>
<p>Although sequence-level scores have weak predictive power, the previous results show that LLMs are well-calibrated on multiple choice question answer tasks and true/false evaluation tasks [Kadavath et al., 2022, OpenAI, 2023], suggesting the model has better calibration on token-level scores. Inspired by this, we propose to reduce free-form generation to multiple-choice and true/false evaluation tasks, in order to leverage token-level calibration to improve the calibration of free-form generation, as shown in Figure 1. Ren et al. [2023a] propose a similar idea but their focus was on robotics planning, while we focus on the general question answer settings.</p>
<p>To convert free-form generation to multi-choice question answer task, we first sample multiple candidate answers. For a given question <em>x</em>, we sample <em>n</em> answers {<strong>y</strong><sub><em>i</em></sub>}, <em>i</em> = 1, . . . , <em>n</em> from an LLM. We tried using a prompt to instruct the model to generate multiple different answers all at once, but the quality of the batch generated answers were not as good as sampling one at a time.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The issues of position bias, probability dispersion, and no true answers in the Sample and Select setup. The question examples are from [Lin et al., 2021, Agarwal et al., 2023].</p>
<h3>2.1 Sample and Select: reduce free-form generation to multi-choice question answer task</h3>
<p>Given a question and a set of candidate answers {y}<em i="i">{n}, we append alphabet characters, c = A, B, C, ..., to the answers and form it into a multiple choice format. A straightforward score could be the softmax probability for the characters, p(c</em>). However, there are a few issues with that score:}|x, {cq}), which was used in Ren et al. [2023a]. The selected answer would be the one with the highest softmax probability, y˜ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq</p>
<p><strong>Position bias</strong> The score could change as the position of the candidate answers change. See Figure 2 (left). This phenomenon was also reported in other work [Robinson et al., 2022, Zheng et al., 2023]. A simple "shuffle and average" could de-bias and correct for the scores, while more sophisticated method to estimate the prior was proposed by Zheng et al. [2023]. In our work, we use the simple shuffle and average de-bias method. The ablation study of the effect of position bias is in Table 4.</p>
<p><strong>Probability dispersion</strong> among multiple true answers. Unlike the pre-designed multiple choice QA task where only one true answer provided, in the free-form generation there is no such guarantee that only one of the sampled answers is true. When more than one true answers are in the candidate list, the probability of the true is dispersed among the true answers, see Figure 2 (middle). This is an undesired property for comparing across questions, since different questions could generate different number of true answers. Probability dispersion is not a unique problem in LLMs; similar issue was discovered in the ImageNet classification where an image can map to multiple classes, and unnormalized logit was preferred than softmax probability to avoid the probability dispersion [Hendrycks et al., 2019]. Therefore we propose,</p>
<p>$$
\log p(c_i | x, {c y}), c = {A, B, \dots}. \qquad \text{(Sample and Select)}
$$</p>
<p><strong>No answer is true</strong> It is possible that when the model does not know the answer, none of the sampled answers is true. If only wrong answers are provided, the model will be forced to choose one from them, resulting in over-confident prediction. See Figure 2 (right). To mitigate that, we add "NONE OF THE ABOVE" as an additional candidate answer to give model a chance to reject the sampled answers, {y}_{nota} = {y} ∪ {nota}. This is similar to adding "An option not listed here" to the robotic planning task [Ren et al., 2023a]. We obtain the score corresponding to the "NONE OF THE ABOVE" answer,</p>
<p>$$
p(c_{\text{nota}} | x, {c y}_{+ \text{nota}}) \qquad \text{(Sample and Select w/ NONE OF THE ABOVE)}
$$</p>
<p>A higher nota score indicates that the selected answer is less likely to be correct. So we use -p(cnota|x, {cq}+nota) as the confidence score of the selected answer, yˆ = y_{r}, r = arg max_{i} p(c_{i}|x, {cq}). Note that the selected answer is still the answer with the highest score within the original answer set {y} excluding the nota answer.</p>
<h3>2.2 Sample and Eval: reduce free-form generation to true/false evaluation task</h3>
<p>We can also evaluate a question and an answer pair using pointwise evaluation format. We ask the model if the candidate answer is correct or not, as shown in Figure 1. Since the task is a binary</p>
<p>classification task, we can normalize the output score using softmax function to a probability,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i})
$$</p>
<p>(Sample and Eval)
This is similar the P(True) proposed in [Kadavath et al., 2022]. They also propose to include candidate answers in the prompt,</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \boldsymbol{y}_{i},{\boldsymbol{y}})
$$</p>
<p>(Sample and Eval w/ other candidates)
But that work focuses on the scaling law of the score's calibration, and did not compare it with sequence-level score and Sample and Select score.</p>
<h1>2.3 Combining the best of both worlds: select the answer via multi-choice evaluation and score the selected answer via pointwise evaluation</h1>
<p>Sample and Select and Sample and Eval have their own pros and cons. In Sample and Select, although the un-normalized logit is better than softmax probability for calibration purpose, the logit score is still dependent on the other candidate answers. For fairly comparing across $(\boldsymbol{x}, \boldsymbol{y})$ pairs, a good score should measure the confidence to the $(\boldsymbol{x}, \boldsymbol{y})$ itself, not dependent on other candidate answers. Sample and Eval score $p\left(\right.$ Yes $\left.\mid \boldsymbol{y}<em r="r">{i}, \boldsymbol{x}\right)$ is indeed independent of other answers. On the other hand, Sample and Select provides the opportunity for comparing different answers and select the best. Therefore, we combine the best of both: We first use Sample and Select to select the best answer within a given question. The answer with the highest softmax probability score is selected, $\hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i} p\left(c</em>)$.} \mid \boldsymbol{x},{c \boldsymbol{y}}\right)$. After selection, we discard the score because it is not good for cross question comparison. We score the selected answer via Sample and Eval $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}</p>
<p>$$
p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}}), \text { where } \hat{\boldsymbol{y}}=\boldsymbol{y}<em i="i">{r}, r=\arg \max </em>}\right)
$$} p\left(c_{i} \mid \boldsymbol{x},{c \boldsymbol{y</p>
<p>In the case where None of the above answer is added, we penalize the confidence score $p(\operatorname{Yes} \mid \boldsymbol{x}, \hat{\boldsymbol{y}})$ with the uncertainty score for the nota answer, that is $p\left(\right.$ Yes $\mid \boldsymbol{x}, \hat{\boldsymbol{y}}$ ) $-p\left(c_{\text {nota }} \mid \boldsymbol{x},{c \boldsymbol{y}}{ }_{+\text {nota }}\right)$. We call this hybrid strategy "Sample and Select and Eval". See details in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Hybrid</span><span class="w"> </span><span class="s">&quot;Sample and Select and Eval&quot;</span>
<span class="w">    </span><span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Question</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="kd">choice</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}</span><span class="err">\</span><span class="p">),</span>
<span class="w">        </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Use</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">stackrel</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">iid</span><span class="w"> </span><span class="p">}}{=}</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Append</span><span class="w"> </span><span class="s">&quot;None of the above&quot;</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}=</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">nota</span><span class="err">\</span><span class="p">}.</span><span class="w"> </span><span class="o">|</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="o">|=</span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">selection</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">})</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">softmax</span><span class="w"> </span><span class="nx">probabi</span><span class="o">-</span>
<span class="w">        </span><span class="nx">ity</span><span class="w"> </span><span class="nx">scores</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">best</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">among</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">sampled</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">answers</span><span class="w"> </span><span class="p">(</span><span class="nx">exclude</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">post</span><span class="o">-</span><span class="nx">hoc</span><span class="w"> </span><span class="nx">added</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}=</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">r</span><span class="p">},</span><span class="w"> </span><span class="nx">r</span><span class="p">=</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="w"> </span><span class="nx">n</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Obtain</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">uncertainty</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">nota</span><span class="w"> </span><span class="nx">answer</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}=</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">c_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="err">\</span><span class="p">{</span><span class="nx">c</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Compose</span><span class="w"> </span><span class="nx">pointwise</span><span class="w"> </span><span class="nx">evaluation</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}(</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">feed</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">obtain</span><span class="w"> </span><span class="nx">output</span>
<span class="w">        </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">p</span><span class="p">(</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Yes</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">x</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}})</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">The</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="p">=</span><span class="nx">s</span><span class="o">-</span><span class="nx">s_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">nota</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="err">\</span><span class="nx">boldsymbol</span><span class="p">{</span><span class="nx">y</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">its</span><span class="w"> </span><span class="nx">confidence</span><span class="w"> </span><span class="nx">score</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">).</span>
</code></pre></div>

<h2>3 Evaluation metrics for selective generation</h2>
<p>Suppose $\mathcal{D}={\boldsymbol{x}}<em n="n">{m}$ is a dataset containing $m$ questions to evaluate. Given a LLM model $\mathcal{M}$, for each question $\boldsymbol{x}$, we randomly sample $n$ answers ${\boldsymbol{y}}</em>}=\left{\boldsymbol{y<em 2="2">{1}, \boldsymbol{y}</em>}, \ldots, \boldsymbol{y<em i="i">{n}\right}$, where $\boldsymbol{y}</em>)$ pair, we would like evaluate how well the score could be used for selective generation, besides the accuracy.
Accuracy For a fixed question $\boldsymbol{x}$ and a set candidate answers ${\boldsymbol{y}}} \stackrel{\text { iid }}{=} \mathcal{M}(\boldsymbol{x})$. Suppose the ground truth $h(\boldsymbol{x}, \boldsymbol{y})={0,1}$ for each answer's correctness (or quality) is available, either through human evaluation or an auto-evaluation model to approximate human rating. Given a confidence score function $s(\boldsymbol{x}, \boldsymbol{y})$ measuring the confidence of a $(\boldsymbol{x}, \boldsymbol{y<em r="r">{n}$ to $\boldsymbol{x}$, we could use the confidence score to select the final answer $\hat{\boldsymbol{y}}$ to the question $\boldsymbol{x}$. We assess if the selected answer is correct, i.e. $h(\boldsymbol{x}, \hat{\boldsymbol{y}})=1, \hat{\boldsymbol{y}}=\boldsymbol{y}</em>, r=\arg \max <em i="i">{i=1}^{n} s\left(\boldsymbol{x}, \boldsymbol{y}</em>\right)$.</p>
<p>Accuracy evaluates if the score can be used to choose the best answer among the candidate answers within a given question. For selective generation, we compare across questions. Given the $m$ question and its selected best answer, ${(\boldsymbol{x}, \hat{\boldsymbol{y}})}_{m}$, we would abstain poor quality pairs to ensure better overall generation quality, aka selective generation. Suppose for each pair we have a confidence score, $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$. If the score is predictive for the quality, we could rank the pairs by the score, and abstain those with the lowest scores, and selectively only output answers with high scores. For the abstained low quality answers, we could instead output "SORRY, I DON'T KNOW". An honest "I don't know" answer is better then a wrong answer. To quantitatively evaluate the scores on selective generation, we use Calibration-AUC and Selective-AUC as defined below.</p>
<p>Calibration-AUC AUC metric for a binary prediction task where the binary label is the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$, and the prediction score is the confidence score $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ [Kivlichan et al., 2021]. Since Calibration-AUC measures the ranking performance, it cannot be simply tricked using the post-hoc calibration heuristics such as the temperature scaling.</p>
<p>Selective generation curve and AUC Selective generation curve measures the correctness $h(\boldsymbol{x}, \hat{\boldsymbol{y}})$ as a function of abstention rate $\alpha \%$, where the samples are sorted by $s(\boldsymbol{x}, \hat{\boldsymbol{y}})$ and samples with the lowest $\alpha \%$ scores are abstained [Ren et al., 2023b]. At $\alpha=0$ no sample is abstained, so the curve starts from the conventionally defined accuracy. As $\alpha$ increases, if the score is predictive of correctness, low quality samples will be abstained first, and the remaining samples will have higher overall quality. Therefore we expect the curve to increase. To quantitatively measure the performance, we compute the area under the selective generation curve, Selective-AUC.</p>
<p>Distinction to Expected Calibration Error (ECE) ECE [Guo et al., 2017] is commonly used to measure if the predictive probability value matches the ground truth accuracy. ECE computation is straightforward for categorical prediction. However, for sequence generation, even though it is possible to define sequence-level ECE [Zablotskaia et al., 2023], getting the ground truth is challenging. Also ECE can only be applied to probabilistic scores. The confidence scores we propose are not necessarily probabilities, so therefore ECE is not applicable there. In this study, we focus on a more general setting that apply to any confidence scores: assessing if the confidence score is predictive of the output quality. Therefore we use the calibration-AUC and selective generation instead of ECE.</p>
<h1>4 Experiments</h1>
<h3>4.1 Experiment setup</h3>
<p>LLMs PALM-2 LARGE is mainly used in our experiments. For each question, we sample $n=4$ answers at temperature 1.0. We de-duplicate the answers to reduce the chance of probability dispersion. We also consider GPT-3 (text-davinci-003) model for evaluation. Due to the OpenAI API limitation, we cannot evaluate all the methods and obtain complete results for GPT-3. We can neither evaluate methods on GPT-3.5 and GPT-4 models because OpenAI API does not provide output log-probabilities for them.</p>
<p>Benchmark datasets TruthfulQA [Lin et al., 2021] is a dataset for assessing model's ability to generate truthful answers against false belief or misconception. It contains 817 questions in the validation split. To label the quality of generated answers, we use the GPT-judge, which is a GPT-3 model fine-tuned on human feedback data, provided by Lin et al. [2021]. It is shown that GPT-judge has $90-95 \%$ accuracy in predicting human evaluations of truthfulness.</p>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. It contains 15,240 examples in the test split. We randomly sampled 1000 examples to save inference cost. To label the quality of the generated summaries, we use a reward model fine-tuned on human feedback data, as used by [Zhao et al., 2023]. The prediction accuracy of human rating of the reward model is $71.34 \%$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of different scores for the accuracy and calibration metrics on TruthfulQA for PALM-2 LARGE and GPT-3 models. The numbers are in percentage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PALM-2 LARGE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">48.23</td>
<td style="text-align: center;">39.80</td>
<td style="text-align: center;">33.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">52.75</td>
<td style="text-align: center;">50.09</td>
<td style="text-align: center;">42.15</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">58.13</td>
<td style="text-align: center;">72.59</td>
<td style="text-align: center;">56.61</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\mathbf{5 9 . 1 2}$</td>
<td style="text-align: center;">$\underline{73.79}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{59.00}$</td>
<td style="text-align: center;">68.78</td>
<td style="text-align: center;">55.70</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">73.76</td>
<td style="text-align: center;">57.38</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">58.14</td>
<td style="text-align: center;">$\mathbf{7 5 . 3 4}$</td>
<td style="text-align: center;">$\underline{58.10}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">49.76</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">42.06</td>
<td style="text-align: center;">50.22</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">47.97</td>
<td style="text-align: center;">56.75</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">67.83</td>
<td style="text-align: center;">48.47</td>
<td style="text-align: center;">53.28</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\underline{68.48}$</td>
<td style="text-align: center;">$\underline{51.36}$</td>
<td style="text-align: center;">$\underline{55.28}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 4 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<h1>4.2 Results</h1>
<p>The performance of the different scores evaluated using accuracy, calibration-AUC, and selectiveAUC are shown in Table 1. It is clear to see that, sequence-level likelihood is not good for both accuracy and calibration. It has even below 0.5 AUC suggesting sequence likelihood is negatively correlated with correctness. Length normalization could improve the performance but AUC is still below 0.5 . The strategy of reducing sequence-level score to token-level scores via self-evaluation improve both the accuracy and calibration over sequence likelihood. Considering all metrics together, the hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance.
Comparing the two strategies, Sample and Select and Sample and Eval, Sample and Select has decent accuracy, but suffers from the calibration metrics. Adding NONE OF THE ABOVE helps improve calibration. On the other hand, Sample and Eval is better on calibration metrics, but it has a bit lower accuracy. This trend is more clear in GPT-3. Therefore we propose the hybrid strategy to combine the best of both. The ROC curves for binary classification of correct and incorrect answers using different scores, and the selective generation curves can be found in Figure 3. Calibration-AUC and Selective-AUC are the area under the two curves respectively.
In addition, we show that self-evaluation is complementary to self-critique and revise, a technique to self-improve the answer quality [Bai et al., 2022]. We first apply that technique to improve each of the sampled answers. Then we compute the scores on the revised answers, instead of on the original answers. In Table 2, it is clear that on the revised answers, we see similar patterns that sequence-level scores are not well suited for selective generation, and the token-level scores achieves better performance.</p>
<p>Table 2: Self-critique and revise further improves the model's accuracy, calibration, and selective generation on TruthfulQA on PALM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">54.83</td>
<td style="text-align: center;">38.96</td>
<td style="text-align: center;">38.40</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">59.12</td>
<td style="text-align: center;">49.64</td>
<td style="text-align: center;">47.03</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">50.41</td>
<td style="text-align: center;">52.40</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">64.60</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">58.69</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">$\underline{66.34}$</td>
<td style="text-align: center;">70.55</td>
<td style="text-align: center;">$\mathbf{6 1 . 8 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">$\mathbf{6 6 . 7 1}$</td>
<td style="text-align: center;">64.69</td>
<td style="text-align: center;">59.44</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">64.87</td>
<td style="text-align: center;">$\underline{71.35}$</td>
<td style="text-align: center;">61.11</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">64.50</td>
<td style="text-align: center;">$\mathbf{7 2 . 7 2}$</td>
<td style="text-align: center;">$\underline{61.44}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ROC curves for binary classification and selective generation curves, evaluated on TruthFULQA. The left most point of the selective generation curves (abstention rate $\alpha=0$ ) is the accuracy reported in Table 1. The area under the ROC curve is calibration-AUC, and the area under the selective generation curve is selective-AUC.</p>
<h1>4.3 Self-evaluation improves calibration on TL;DR summarization</h1>
<p>TL;DR is a summarization benchmark dataset mined from Reddit website [Völske et al., 2017]. Evaluating the different scores on that dataset shows again that the sequence-level scores are not suitable for calibration. Self-evaluation based token-level scores improve the both accuracy and calibration performance (Table 3). Sample and Select has higher accuracy but lower calibration-AUC than Sample and Eval, and adding None of the above option helps to improve Calibration-AUC without sacrificing much the accuracy. Hybrid methods in general have decent performance.</p>
<p>Table 3: Comparison of different scores: accuracy and calibration on TL;DR for PaLM-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sequence likelihood</td>
<td style="text-align: center;">65.80</td>
<td style="text-align: center;">49.75</td>
<td style="text-align: center;">52.63</td>
</tr>
<tr>
<td style="text-align: left;">Len-norm sequence likelihood</td>
<td style="text-align: center;">69.40</td>
<td style="text-align: center;">$\underline{53.20}$</td>
<td style="text-align: center;">56.93</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">49.54</td>
<td style="text-align: center;">56.56</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval</td>
<td style="text-align: center;">68.70</td>
<td style="text-align: center;">52.34</td>
<td style="text-align: center;">56.09</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Eval w/ candidates</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">$\mathbf{5 5 . 1 9}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 9 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid</td>
<td style="text-align: center;">$\underline{70.70}$</td>
<td style="text-align: center;">52.19</td>
<td style="text-align: center;">$\underline{57.56}$</td>
</tr>
<tr>
<td style="text-align: left;">Hybrid w/ nota</td>
<td style="text-align: center;">$\mathbf{7 0 . 8 0}$</td>
<td style="text-align: center;">52.05</td>
<td style="text-align: center;">$\underline{57.55}$</td>
</tr>
</tbody>
</table>
<h3>4.4 Effect of position bias</h3>
<p>We assess the effect of position bias on the performance. We compare the vanilla setting where the answers are ordered by default, and the de-biased setting where the answer scores are averaged across all $n$ ! possible permutations. The difference on the performance is not that significant. Given</p>
<p>the de-bias process through shuffle and average is very computational expensive, we use the vanilla setting by default.</p>
<p>Table 4: Effect of position bias on metrics. The results are based on PALM-2 LARGE.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Calibration-AUC</th>
<th style="text-align: center;">Selective-AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TruthFULQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">58.26</td>
<td style="text-align: center;">53.17</td>
<td style="text-align: center;">48.59</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">48.58</td>
</tr>
<tr>
<td style="text-align: left;">TL;DR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, vanilla</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">46.65</td>
<td style="text-align: center;">54.68</td>
</tr>
<tr>
<td style="text-align: left;">Sample and Select, de-biased</td>
<td style="text-align: center;">70.70</td>
<td style="text-align: center;">43.94</td>
<td style="text-align: center;">53.86</td>
</tr>
</tbody>
</table>
<h1>5 Related work</h1>
<p>The calibration of LLMs on multiple choice question answer tasks is studied in Kadavath et al. [2022]. Robinson et al. [2022] show that the sequence level probability is worse than the token-level probability (e.g. A, B, C, etc) for predicting the correctness. But those studies use the multiple choice question answering datasets where the answers are pre-defined and not generated from LLMs. Our work focuses on the calibration of free-form generation tasks. We transform free-form generation to multiple choice task by generating answer candidates by itself. Another distinction to [Kadavath et al., 2022] is that we care more on the ranking performance measured by AUC than the exact value match to ground truth probability measured by ECE.
In terms of estimating language models' confidence or uncertainty, Tian et al. [2023], Lin et al. [2022] propose to ask model to express uncertainty in words along with the generated answer, but it is shown that LLMs often exhibit a high degree of overconfidence when verbalizing their confidence [Xiong et al., 2023]. Kuhn et al. [2023] propose to use semantic entropy among a set of sampled answers to estimate model's uncertainty. The semantic similarity is inferred using a separate natural language inference classification system (NLI). Cole et al. [2023] find the degree of repetition in sampled answers is a good score for selectively answering ambiguous questions. The distinctions between our work and the above are that, we focus on estimating the confidence of long sequence free-form generation tasks, where the repetition can not be easily measured. Also, we are interested in zero-shot self-evaluation based scores, without utilized a separate model for inference. The true/ false evaluation method proposed by Kadavath et al. [2022] is one of them. In our work, we compare this score with several other scores and have a comprehensive assessment on selective generation of free-form generation tasks
Prior studies have proposed generating multiple candidate responses for free-form generation tasks and then selecting the best. The final answer is selected using a variety of methods, including: (1) simple sequence likelihood [Adiwardana et al., 2020], (2) ranking model trained on human preference data [Nichols et al., 2020], (3) self-consistency i.e. if an answer is the most consensus one [Wang et al., 2022, Chen et al., 2023] and (4) models' self-evaluation ability to choose the final response based on its own evaluation of the responses [Ren et al., 2023a]. However, the focus of most prior work except for [Ren et al., 2023a] are on improving accuracy, not on confidence estimation or calibration. [Ren et al., 2023a] is similar to our work in the sense that it not only proposes to generate multiple options and then ask the model to choose one, but also estimate uncertainty to ask for clarification. However they focus on robotics planning, while we focus on more general question answer. Also, they directly use the multiple choice score output, while we identified the position bias and probability dispersion problems in the scores, and propose hybrid method to address them</p>
<h2>6 Discussion</h2>
<p>We show that although generic sequence-level scores are not well suited for selective generation (even negatively correlated with the the quality) for free-form generation, asking the model again to self-evaluate could reduce the sequence-level score to token-levels scores, improving quality calibration. Self-evaluation is though at the cost of increasing inference time by 1 or 2 (hybrid mode) times. Alternative to this post-hoc method, how to improve the quality calibration of the sequence-level score during training and finetuning is one of our future work.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank Denny Zhou, Zelda Mariet, Sharat Chikkerur, Jasper Snoek, and Alexander D'Amour from Google DeepMind for helpful discussions for insightful discussion and providing valuable feedback for this work. We would also like to express our appreciation towards Lyric Doshi, Xuezhi Wang, and Michael W. Dusenberry from Google DeepMind for their technical support.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like opendomain chatbot. arXiv preprint arXiv:2001.09977, 2020.</p>
<p>Ayushi Agarwal, Nisarg Patel, Neeraj Varshney, Mihir Parmar, Pavan Mallina, Aryan Bhavin Shah, Srihari Raju Sangaraju, Tirth Patel, Nihar Thakkar, and Chitta Baral. Can NLP models' identify', 'distinguish', and'justify'questions that don't have a definitive answer? arXiv preprint arXiv:2309.04635, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311, 2023.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613, 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321-1330. PMLR, 2017.</p>
<p>Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Ian D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving modelmoderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.</p>
<p>Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. arXiv preprint arXiv:2205.14334, 2022.</p>
<p>Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive summarization. arXiv preprint arXiv:2203.16804, 2022.</p>
<p>Eric Nichols, Leo Gao, and Randy Gomez. Collaborative storytelling with large-scale neural language models, 2020.</p>
<p>OpenAI. GPT-4 technical report. arXiv, pages 2303-08774, 2023.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13, 2022.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023a.</p>
<p>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter J. Liu. Out-of-distribution detection and selective generation for conditional language models, 2023b.</p>
<p>Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023.</p>
<p>Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, 2017.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. arXiv preprint arXiv:2306.13063, 2023.</p>
<p>Polina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, and Jeremiah Liu. On uncertainty calibration and selective generation in probabilistic neural summarization: A benchmark study. arXiv preprint arXiv:2304.08653, 2023.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models' selection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023.</p>
<h1>A Prompts used in the study</h1>
<p>In the following task, you will receive a question. Please generate one factual, informative, unbiased, and safe answer to the question.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds will pass through your digestive system without being digested and will eventually be excreted in your stool.
$==$
Question:
(question)
$==$
Answer:</p>
<p>Figure 4: The prompt for sampling an answer in TruthfulQA.</p>
<p>In the following task, you will receive a question with multiple answers. Please choose the most factual, informative, unbiased, and safe answer. A honest "I don't know" answer is preferable to a wrong answer. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
$==$
Letter:</p>
<p>Figure 5: The prompt for multi-choice selection in TruthfulQA.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: The prompt for True/False evaluation in TruthfulQA.</p>
<p>In the following task, you will receive a question, a few candidate answers, and a proposed answer. $==$
Question:
(question)
$==$
Candidate answers:
(answer_list)
The above candidate answers can be either true or false. Please judge if the following proposed answer is factual, informative, unbiased, and safe.
$==$
Answer:
(answer)
$==$
Is the above answer factual, informative, unbiased, and safe?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 7: Prompt for True/False evaluation with other candidates in TruthFULQA</p>
<p>In the following task, you will receive a question and an answer. Please generate an explanation why this answer would be non-factual. Say None if you don't have any critique.
$==$
Question:
What happens to you if you eat watermelon seeds?
$==$
Answer:
Watermelon seeds can cause a watermelon to grow in your stomach.
$==$
This answer could be wrong because the stomach is an acidic environment that is not suitable for watermelon seeds to grow.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
This answer could be wrong because</p>
<p>Figure 8: The prompt for generating a critique in TruthfulQA.</p>
<p>In the following task, you will receive a question, an answer, and a critique that identifies hallucinated statements and explains why they are non-factual. Please rewrite the answer so that it is factual, informative, unbiased, and safe.
$==$
Question:
When was Ronald Reagan assassinated?
$==$
Answer:
Ronald Reagan was assassinated by John Hinckley Jr. in Washington, D.C., on March 30, 1981
$==$
Critique:
Ronald Reagan was shot by Hinkley but he survived, so he was not assassinated.
$==$
New answer:
Ronald Reagan was not assassinated. He was shot and wounded by John Hinckley Jr. in Washington, D.C., on March 30, 1981, but he survived the attack.
$==$
Question:
(question)
$==$
Answer:
(answer)
$==$
Critique:
(critique)
$==$
New answer:</p>
<p>Figure 9: The prompt for generating a revised answer given the critique in TruthFULQA.</p>
<p>In the following task, you will receive a text. Please generate a summary TDLR.
$==$
Text:
(question)
$==$
TLDR:</p>
<p>Figure 10: The prompt for sampling an answer in TL;DR.</p>
<p>In the following task, you will receive a text and a few candidate summaries. Please choose the most concise and comprehensive summary. Only output the capitalized alphabet letter corresponding to the answer.
$==$
Text:
(question)
$==$
Candidate summaries:
(answer_list)
$==$
Letter:</p>
<p>Figure 11: The prompt for multi-choice selection in TL;DR.</p>
<p>In the following task, you will receive a text and a proposed summary. Please judge if the summary is concise and comprehensive.
$==$
Text:
(question)
$==$
Summary:
(answer)
$==$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 12: The prompt for pointwise evaluation in TL;DR.</p>
<p>In the following task, you will receive a text, a few candidate summaries, and a proposed summary.
$==$
Text:
{question}
$=$
Candidate summaries:
{answer_list}
The above candidate summaries can be either good or bad. Please judge if the following proposed summary is concise and comprehensive.
$==$
Summary:
{answer}
$=$
Is the above summary concise and comprehensive?
A) Yes
B) No
$==$
Letter:</p>
<p>Figure 13: The prompt for pointwise evaluation with other candidates in TL;DR.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For GPT-3 model, the API can only output log-probability for up to 5 most likely tokens. Because of this limitation, a few methods cannot be evaluated on GPT-3. For example, the most likely tokens in the multi-response evaluation setting are not necessarily A, B, C etc., but the most likely letter and its variants such as 'A', ' $\mathrm{A}^{\prime}$, 'A', or 'A'n'. Therefore the maximum token prediction and its log-probability are always available, but the log-probability for a specific token such as 'E' for the "None of the above" answer is not available.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
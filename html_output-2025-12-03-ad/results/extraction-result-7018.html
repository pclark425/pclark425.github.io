<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7018 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7018</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7018</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-259289652</p>
                <p><strong>Paper Title:</strong> Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction</p>
                <p><strong>Paper Abstract:</strong> Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7018.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7018.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>graph_verbalization_general</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph verbalization (general / graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that convert structured graph representations (knowledge graphs, semantic parses, event graphs) into textual sequences for use by language models or as direct output (graph-to-text). Mentioned as a related trend where inputs originate from knowledge graphs, information extraction or semantic parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph verbalization (general)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert graph nodes/edges (e.g., knowledge graph triples, semantic role structures, event graphs) into linear textual sequences (verbalizations) so that language models can be trained or fine-tuned on graph-derived text; typically relies on mapping graph elements to phrases and concatenating them into sentences or templates.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>not reported in this paper (various approaches in literature: template-based, graph-to-sequence models, AMR linearizations)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / knowledge-graph verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned as an alternative to graph-augmented encoders; can be used to generate training corpora for language model pretraining or for direct text generation, but no quantitative effect is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not quantified in this paper; authors note general concerns in literature (see specific OpenIE limitations below): verbalizations that come from extractive, open-domain triplet extractors can be error-prone, incomplete, and lack typed roles needed for domain-specific fidelity (especially in biomedicine).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Positioned as an alternate paradigm to the tandem text+graph encoder used in this work. The authors emphasize that event-based structured graphs (their approach) provide n-ary, typed interactions and richer semantics than flat triplet verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7018.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7018.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenIE_triplet_verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenIE (open-domain triple) verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open Information Extraction (OpenIE) produces untyped <subject, predicate, object> triplets from text; these triplets are sometimes verbalized to text but are regarded as extractive and error-prone for downstream graph-to-text uses in biomedical summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging linguistic structure for open domain information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>OpenIE triplet sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Sequence of extracted triplets (subject,predicate,object) converted into text fragments or concatenated sentences; triplets are untyped text phrases without ontology alignment or explicit event roles.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy for complex relations</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge/tuple listing (triplet ordering not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>open-domain relation extraction / graph-to-text via triplet verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenIE (off-the-shelf extractors referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-domain extraction systems producing textual triples; not trained or adapted here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not evaluated in this work; cited as an inferior source of structured input for biomedical summarization relative to event graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Described in paper as 'merely extractive, error-prone, and devoid of additional metadata; does not capture semantic interconnections between n-ary participants, often ignores crucial conditions for a correct fact or extracts incomplete facts difficult to merge in post-processing.'</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared directly with event graphs in Figure 2 and text: event graphs are preferred for biomedical tasks because they represent n-ary, nested relations with typed roles aligned to an ontology, unlike OpenIE triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7018.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7018.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR_graph_to_text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-text (graph-to-sequence for Abstract Meaning Representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-to-sequence models that generate text from AMR semantic graphs; cited as an instance of graph-to-text approaches (example: 'A graph-to-sequence model for amr-to-text generation').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A graph-to-sequence model for amr-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR graph linearization / graph-to-sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR encodes sentence meaning as a rooted, labeled graph; graph-to-sequence models consume AMR graphs (often via a GNN or serialized form) and generate fluent natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (output), graph-to-sequence; can be lossless for AMR semantics depending on linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>not specified in this paper (literature includes direct graph encoders, linearization of AMR via bracketed notations, or traversal-based serializations)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR corpora (not explicitly named here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation / graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>graph-to-sequence models (GNN encoders + seq decoders) as cited</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Typically GNN-based encoders or specialized graph recurrent nets feeding an autoregressive decoder; exact architectures not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>standard graph-to-text metrics (BLEU, METEOR) are typical in literature but not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned as an example of converting structured semantic parses into text for generation tasks; no quantitative impact reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not discussed in detail here; general caveat that domain-specific ontologies and event n-ary structures may not align directly with linguistically grounded AMR in highly specialized domains like biomedicine.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Cited among graph-to-text approaches; authors promote domain-specific event graphs as more appropriate for biomedical evidence than open-domain semantic graphs alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7018.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7018.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kg_synthetic_corpus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph-based synthetic corpus generation for pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Technique that generates synthetic natural language corpora by verbalizing knowledge graph content to pretrain or further pretrain language models with structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG-based synthetic corpus (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Verbalize knowledge graph facts (triples/subgraphs) into textual sentences to build large synthetic corpora used to pretrain language models with injected structured knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (synthetic corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>not reported in this paper (various verbalization strategies in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pre-training data generation / knowledge-enhanced LM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not reported here (literature uses downstream gains on QA/summarization/LM tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Cited as a strategy to augment LM pretraining with structured knowledge; no metrics or concrete outcomes provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated here; general concerns include the fidelity of verbalization, coverage of knowledge graphs, and risk of generating unnatural or noisy synthetic text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Mentioned as an alternative route to infuse structured knowledge into LMs rather than adding a graph encoder; authors note combining event graphs with LM pretraining (e.g., BioBART) as promising future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7018.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7018.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kg_transformer_textgen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text generation from knowledge graphs with graph transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-transformer-based approaches which generate text directly from knowledge graphs by encoding graph structure (nodes/edges) with transformer-style architectures before decoding natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text generation from knowledge graphs with graph transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-transformer verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode knowledge graph substructures with a transformer-like graph encoder and decode into text; treats graph elements as tokens in a structured encoding fed to seq decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential output; graph-aware encoding</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>not specified here (graph transformer architectures vary; may use node/edge embeddings and attention over graph neighborhoods)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>graph transformers (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph-aware encoder based on transformer attention mechanisms; specific sizes/variants not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Mentioned in related work as a graph-to-text modeling alternative; no direct evaluation or training impact reported in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not discussed in detail here; general issues include scalability to large graphs and the challenge of preserving typed n-ary event semantics important in biomedical texts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Referenced as one of several graph-to-text strategies; authors favor explicit event graphs plus dual encoders for their biomedical summarization task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7018.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7018.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>event_graph_verbalization_bio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-text extraction and verbalization of biomedical event graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Verbalization of domain-specific biomedical event graphs (n-ary, typed, possibly nested) into textual sequences; noted as prior related work by the authors on turning event graphs into text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-to-text extraction and verbalization of biomedical event graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>biomedical event-graph verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Map event graph elements (triggers, arguments, roles, nested events) into natural language phrases/sentences to produce textual representations of extracted factual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based; aimed at preserving n-ary and nested structure in verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>not detailed in this paper (reference indicates a text-to-text extraction/verbalization pipeline exists in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>not specified here (related to biomedical event extraction corpora / CDSR context)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>event-graph verbalization / text-to-text extraction of biomedical event graphs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Cited as a complementary direction; this paper does not evaluate a verbalization-based LM training pipeline but suggests verbalization as a potential route to increase event data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not evaluated here; general trade-offs include potential loss of explicit typed structure when verbalizing and the difficulty of generating faithful, non-hallucinated text from graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Presented as complementary to the graph-encoder approach used in this paper; verbalization could increase available textual training data but may risk losing precise structured roles unless carefully designed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A graph-to-sequence model for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 1)</em></li>
                <li>Text-to-text extraction and verbalization of biomedical event graphs <em>(Rating: 2)</em></li>
                <li>Leveraging linguistic structure for open domain information extraction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7018",
    "paper_id": "paper-259289652",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "graph_verbalization_general",
            "name_full": "Graph verbalization (general / graph-to-text)",
            "brief_description": "A family of methods that convert structured graph representations (knowledge graphs, semantic parses, event graphs) into textual sequences for use by language models or as direct output (graph-to-text). Mentioned as a related trend where inputs originate from knowledge graphs, information extraction or semantic parsing.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "graph verbalization (general)",
            "representation_description": "Convert graph nodes/edges (e.g., knowledge graph triples, semantic role structures, event graphs) into linear textual sequences (verbalizations) so that language models can be trained or fine-tuned on graph-derived text; typically relies on mapping graph elements to phrases and concatenating them into sentences or templates.",
            "representation_type": "sequential, token-based",
            "encoding_method": "not reported in this paper (various approaches in literature: template-based, graph-to-sequence models, AMR linearizations)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "graph-to-text generation / knowledge-graph verbalization",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Mentioned as an alternative to graph-augmented encoders; can be used to generate training corpora for language model pretraining or for direct text generation, but no quantitative effect is reported here.",
            "limitations": "Not quantified in this paper; authors note general concerns in literature (see specific OpenIE limitations below): verbalizations that come from extractive, open-domain triplet extractors can be error-prone, incomplete, and lack typed roles needed for domain-specific fidelity (especially in biomedicine).",
            "comparison_with_other": "Positioned as an alternate paradigm to the tandem text+graph encoder used in this work. The authors emphasize that event-based structured graphs (their approach) provide n-ary, typed interactions and richer semantics than flat triplet verbalizations.",
            "uuid": "e7018.0",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "OpenIE_triplet_verbalization",
            "name_full": "OpenIE (open-domain triple) verbalization",
            "brief_description": "Open Information Extraction (OpenIE) produces untyped &lt;subject, predicate, object&gt; triplets from text; these triplets are sometimes verbalized to text but are regarded as extractive and error-prone for downstream graph-to-text uses in biomedical summarization.",
            "citation_title": "Leveraging linguistic structure for open domain information extraction",
            "mention_or_use": "mention",
            "representation_name": "OpenIE triplet sequence",
            "representation_description": "Sequence of extracted triplets (subject,predicate,object) converted into text fragments or concatenated sentences; triplets are untyped text phrases without ontology alignment or explicit event roles.",
            "representation_type": "sequential, token-based, lossy for complex relations",
            "encoding_method": "edge/tuple listing (triplet ordering not specified here)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "open-domain relation extraction / graph-to-text via triplet verbalization",
            "model_name": "OpenIE (off-the-shelf extractors referenced)",
            "model_description": "Open-domain extraction systems producing textual triples; not trained or adapted here.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Not evaluated in this work; cited as an inferior source of structured input for biomedical summarization relative to event graphs.",
            "limitations": "Described in paper as 'merely extractive, error-prone, and devoid of additional metadata; does not capture semantic interconnections between n-ary participants, often ignores crucial conditions for a correct fact or extracts incomplete facts difficult to merge in post-processing.'",
            "comparison_with_other": "Compared directly with event graphs in Figure 2 and text: event graphs are preferred for biomedical tasks because they represent n-ary, nested relations with typed roles aligned to an ontology, unlike OpenIE triplets.",
            "uuid": "e7018.1",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "AMR_graph_to_text",
            "name_full": "AMR-to-text (graph-to-sequence for Abstract Meaning Representation)",
            "brief_description": "Graph-to-sequence models that generate text from AMR semantic graphs; cited as an instance of graph-to-text approaches (example: 'A graph-to-sequence model for amr-to-text generation').",
            "citation_title": "A graph-to-sequence model for amr-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "AMR graph linearization / graph-to-sequence",
            "representation_description": "AMR encodes sentence meaning as a rooted, labeled graph; graph-to-sequence models consume AMR graphs (often via a GNN or serialized form) and generate fluent natural language.",
            "representation_type": "sequential (output), graph-to-sequence; can be lossless for AMR semantics depending on linearization",
            "encoding_method": "not specified in this paper (literature includes direct graph encoders, linearization of AMR via bracketed notations, or traversal-based serializations)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR corpora (not explicitly named here)",
            "task_name": "AMR-to-text generation / graph-to-text",
            "model_name": "graph-to-sequence models (GNN encoders + seq decoders) as cited",
            "model_description": "Typically GNN-based encoders or specialized graph recurrent nets feeding an autoregressive decoder; exact architectures not detailed in this paper.",
            "performance_metric": "standard graph-to-text metrics (BLEU, METEOR) are typical in literature but not reported here",
            "performance_value": null,
            "impact_on_training": "Mentioned as an example of converting structured semantic parses into text for generation tasks; no quantitative impact reported in this paper.",
            "limitations": "Not discussed in detail here; general caveat that domain-specific ontologies and event n-ary structures may not align directly with linguistically grounded AMR in highly specialized domains like biomedicine.",
            "comparison_with_other": "Cited among graph-to-text approaches; authors promote domain-specific event graphs as more appropriate for biomedical evidence than open-domain semantic graphs alone.",
            "uuid": "e7018.2",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "kg_synthetic_corpus",
            "name_full": "Knowledge-graph-based synthetic corpus generation for pretraining",
            "brief_description": "Technique that generates synthetic natural language corpora by verbalizing knowledge graph content to pretrain or further pretrain language models with structured knowledge.",
            "citation_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "mention_or_use": "mention",
            "representation_name": "KG-based synthetic corpus (graph-to-text)",
            "representation_description": "Verbalize knowledge graph facts (triples/subgraphs) into textual sentences to build large synthetic corpora used to pretrain language models with injected structured knowledge.",
            "representation_type": "sequential, token-based (synthetic corpus)",
            "encoding_method": "not reported in this paper (various verbalization strategies in cited work)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "pre-training data generation / knowledge-enhanced LM pretraining",
            "model_name": null,
            "model_description": null,
            "performance_metric": "not reported here (literature uses downstream gains on QA/summarization/LM tasks)",
            "performance_value": null,
            "impact_on_training": "Cited as a strategy to augment LM pretraining with structured knowledge; no metrics or concrete outcomes provided in this paper.",
            "limitations": "Not evaluated here; general concerns include the fidelity of verbalization, coverage of knowledge graphs, and risk of generating unnatural or noisy synthetic text.",
            "comparison_with_other": "Mentioned as an alternative route to infuse structured knowledge into LMs rather than adding a graph encoder; authors note combining event graphs with LM pretraining (e.g., BioBART) as promising future work.",
            "uuid": "e7018.3",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "kg_transformer_textgen",
            "name_full": "Text generation from knowledge graphs with graph transformers",
            "brief_description": "Graph-transformer-based approaches which generate text directly from knowledge graphs by encoding graph structure (nodes/edges) with transformer-style architectures before decoding natural language.",
            "citation_title": "Text generation from knowledge graphs with graph transformers",
            "mention_or_use": "mention",
            "representation_name": "graph-transformer verbalization",
            "representation_description": "Encode knowledge graph substructures with a transformer-like graph encoder and decode into text; treats graph elements as tokens in a structured encoding fed to seq decoder.",
            "representation_type": "sequential output; graph-aware encoding",
            "encoding_method": "not specified here (graph transformer architectures vary; may use node/edge embeddings and attention over graph neighborhoods)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "knowledge-graph-to-text generation",
            "model_name": "graph transformers (as in cited work)",
            "model_description": "Graph-aware encoder based on transformer attention mechanisms; specific sizes/variants not described in this paper.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Mentioned in related work as a graph-to-text modeling alternative; no direct evaluation or training impact reported in this study.",
            "limitations": "Not discussed in detail here; general issues include scalability to large graphs and the challenge of preserving typed n-ary event semantics important in biomedical texts.",
            "comparison_with_other": "Referenced as one of several graph-to-text strategies; authors favor explicit event graphs plus dual encoders for their biomedical summarization task.",
            "uuid": "e7018.4",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "event_graph_verbalization_bio",
            "name_full": "Text-to-text extraction and verbalization of biomedical event graphs",
            "brief_description": "Verbalization of domain-specific biomedical event graphs (n-ary, typed, possibly nested) into textual sequences; noted as prior related work by the authors on turning event graphs into text.",
            "citation_title": "Text-to-text extraction and verbalization of biomedical event graphs",
            "mention_or_use": "mention",
            "representation_name": "biomedical event-graph verbalization",
            "representation_description": "Map event graph elements (triggers, arguments, roles, nested events) into natural language phrases/sentences to produce textual representations of extracted factual evidence.",
            "representation_type": "sequential, token-based; aimed at preserving n-ary and nested structure in verbalization",
            "encoding_method": "not detailed in this paper (reference indicates a text-to-text extraction/verbalization pipeline exists in prior work)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "not specified here (related to biomedical event extraction corpora / CDSR context)",
            "task_name": "event-graph verbalization / text-to-text extraction of biomedical event graphs",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Cited as a complementary direction; this paper does not evaluate a verbalization-based LM training pipeline but suggests verbalization as a potential route to increase event data for training.",
            "limitations": "Not evaluated here; general trade-offs include potential loss of explicit typed structure when verbalizing and the difficulty of generating faithful, non-hallucinated text from graphs.",
            "comparison_with_other": "Presented as complementary to the graph-encoder approach used in this paper; verbalization could increase available textual training data but may risk losing precise structured roles unless carefully designed.",
            "uuid": "e7018.5",
            "source_info": {
                "paper_title": "Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A graph-to-sequence model for amr-to-text generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 2,
            "sanitized_title": "knowledge_graph_based_synthetic_corpus_generation_for_knowledgeenhanced_language_model_pretraining"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 1,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Text-to-text extraction and verbalization of biomedical event graphs",
            "rating": 2,
            "sanitized_title": "texttotext_extraction_and_verbalization_of_biomedical_event_graphs"
        },
        {
            "paper_title": "Leveraging linguistic structure for open domain information extraction",
            "rating": 1,
            "sanitized_title": "leveraging_linguistic_structure_for_open_domain_information_extraction"
        }
    ],
    "cost": 0.01638725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction Abstractive document summarization  Event extraction  Semantic parsing  Biomedical text mining  Knowledge-driven natural language processing  Natural language understanding
0123456789</p>
<p>Giacomo Frisoni 
SN Computer Science</p>
<p>Paolo Italiani 
SN Computer Science</p>
<p>Gianluca Moro 
SN Computer Science</p>
<p>Ilaria Bartolini 
SN Computer Science</p>
<p> Marco 
SN Computer Science</p>
<p>Antonio Boschetti 
SN Computer Science</p>
<p>Antonella Carbonaro 
SN Computer Science</p>
<p>Graph-Enhanced Biomedical Abstractive Summarization Via Factual Evidence Extraction Abstractive document summarization  Event extraction  Semantic parsing  Biomedical text mining  Knowledge-driven natural language processing  Natural language understanding
012345678910.1007/s42979-023-01867-1Received: 5 November 2022 / Accepted: 3 April 2023SN Computer Science (2023) 4:500 ORIGINAL RESEARCH
Infusing structured semantic representations into language models is a rising research trend underpinning many natural language processing tasks that require understanding and reasoning capabilities. Decoupling factual non-ambiguous concept units from the lexical surface holds great potential in abstractive summarization, especially in the biomedical domain, where fact selection and rephrasing are made more difficult by specialized jargon and hard factuality constraints. Nevertheless, current graph-augmented contributions rely on extractive binary relations, failing to model real-world n-ary and nested biomedical interactions mentioned in the text. To alleviate this issue, we present EASumm, the first framework for biomedical abstractive summarization empowered by event extraction, namely graph-based representations of relevant medical evidence derived from the source scientific document. By relying on dual text-graph encoders, we prove the promising role of explicit event structures, achieving better or comparable performance than previous state-of-the-art models on the CDSR dataset. We conduct extensive ablation studies, including a wide experimentation of graph representation learning techniques. Finally, we offer some hints to guide future research in the field.</p>
<p>Introduction</p>
<p>International experts argue that language is the highest manifestation of human intelligence [1]. This makes learning knowledge from text one of the greatest challenges of modern artificial intelligence. Language is ambiguous, consisting of several expressions that allude to the same fact and often lacking background knowledge for the mentioned entities. Despite capturing a massive amount of knowledge, current state-of-the-art language models-even with 10 11 parameters-struggle to separate high-level semantics from language structure [2,3], acting as memories rather than intelligent networks. Consequently, they notoriously suffer from hallucinations, biases, low robustness, and fragility (vulnerability to adversary attacks) [4,5] that severely limit their real-world application. Semantics is central to summarization, where humans are asked to grasp the relevant parts of the input document, link them, and rephrase the selected entangled facts to create an original short text conveying as much information content as possible. These challenges are further emphasized by the biomedical literature, characterized by documents having domain-specific terminology, elaborated links among entities, no space for interpretation, and no tolerance for factual mistakes. However, automatic summarization systems can substantially help healthcare professionals have a quick and reasonably close overview of the knowledge encapsulated in large scientific corpora, outlining a prerogative toolbox for efficient knowledge discovery [6][7][8][9]. Standing on the shoulders of AI is even more important when we consider the accelerated speed of publication, which since 2020 has exceeded the threshold of 1 million new papers on PubMed per year (3 per minute) [10].</p>
<p>Researchers together have cast doubt about whether pure data-driven methods based on deep neural architectures would be sufficient to achieve understanding. The interpretability requirement and the lack of grounding to the core interactions expressed in the document beg the question of whether symbolic representations may come to the rescue. Following this intuition, the community has recently investigated the integration of structured knowledge into neural language models [11], reckoning on external knowledge graphs [12] or structured representations obtained via semantic parsing [13] and latent semantic correlations [14][15][16]. Depending on the graph's nature, neuro-symbolic methods may thus target amplified understanding or knowledge acquisition capabilities. Reaching the latter goal only with traditional pretraining approaches can be inefficient and expensive. For example, acquiring a fact like "Paracetamol can treat cold" requires having a large number of cooccurrences of "paracetamol" and "cold" concepts in the pretraining corpus. While the combination of language models and knowledge graphs is a research path already widely explored, the same does not apply to semantic graphs. Existing contributions merely extract flat, open-domain, and binary relations, which can result in inferring incomplete or incorrect facts non-useful for biomedicine [10,17]. In this scenario, event extraction [10] appears as the most promising option for obtaining task-driven meaning representations. Under the umbrella of structured prediction, it aims to derive n-ary and potentially nested interactions between participants having a specific semantic role. We point out to the reader that events are released from the presence of a temporal element, and for clarity, we consider the following definition of event proposed in [10]: "An event is a specific occurrence of something that happens and involves an arbitrary number of attributes and participants covering a specific semantic role, depending on the event type. The interaction (i.e., dynamic relation) modeled by an event represents or leads to some state change"</p>
<p>In this paper, the keyword "event" therefore stays for medical evidence mentioned in the scientific literature, in accordance with previous works. Still, we are aware that this term may be misleading and requires revision [10].</p>
<p>We propose EASumm, the first model leveraging event extraction for abstractive single-document biomedical summarization, adopting a tandem architecture combining text and graph representations. We test our solution on the CDSR dataset [18], and we prove how biomedical event extraction contributes to reserving the essential global context and keeping the connection between the most relevant entities, thus generating a higher quality summary (see Fig. 1). Extensive ablation studies prove the contribution of each module and quantify the impact of multiple graph representation learning techniques.</p>
<p>This is an extension of [19], where we test multiple relation-aware graph representation learning modules, dissect event extraction in more details, clarify our methodology with details and algorithms, and openly release all code and data.</p>
<p>The rest of the paper is organized as follows. The following section examines related work. "Event extraction" provides a more extensive discussion on event extraction. Then, "Graph construction" presents our event-based strategy for deriving structured medical evidence from the source text. Next, "Model" details our model, from the architecture design to the training objectives. "Experimental setup" illustrates our experimental setup, while "Results" showcases the results obtained. Finally, "Conclusion" reports the conclusions and points out future directions. </p>
<p>SN Computer Science</p>
<p>Related Work</p>
<p>Abstractive Document Summarization</p>
<p>Summarizing text demands generating a concise summary discarding unnecessary attributes, and preserving the salient notions of the source document. Notably, abstractive summarization does not imply simply copying phrases from the source text but also coming up with new content, echoing a human-like interpretation and paraphrasis. Transformerbased language models have achieved astonishing results in recent years, mainly thanks to deep encoder-decoder architectures and self-supervised pretraining. In a nutshell, the encoder maps the source tokens into a sequence of continuous representations while the decoder reads them and autoregressively generates the summary one token at a time. Their ability to learn universal representations from large volumes of unlabeled text data and then transfer such knowledge to downstream tasks has revolutionized the abstractive summarization research sphere [20][21][22][23][24][25]-even in low-resource [26,27] and multi-document settings [28]. Nevertheless, quantitative studies and large-scale human evaluations [29] have confirmed that current text generators are still heavily victims of hallucinations and prone to produce summaries that are unfaithful to the input documents. For this reason, the latest solutions are mostly knowledge-driven or tend to complement training with reinforcement learning modules to improve informativeness and consistency [30][31][32].</p>
<p>Graph-Enhanced Summarization</p>
<p>Human language is highly ambiguous, with multiple ways to express the same concept unit, where the underlying meaning is oftentimes altered by high-level linguistic constructs. Additionally, a single sentence may incorporate various predicate-argument structures. Despite these observations, current language models only consider the superficial organization of the text document, which is almost irrelevant to identifying its real and deeper semantic content [13]. Climbing towards natural language understanding, an increasing number of researchers argue that a model trained purely on the form will never learn the meaning, lacking signals to learn non-linguistic relations [33].</p>
<p>To this end, structured representations allow different quality improvements (e.g., coherence, factuality, low redundancy, long-range dependencies, informativeness, consistency) depending on how they are constructed. In particular, semantic parsing graphs normalize lexical and syntactic variations, providing formal meaning representations capable of decoupling concept units (what to say) from language competencies (how to say it).</p>
<p>Graph structures have long been used for extractive summarization. In this sense, early approaches, such as Tex-tRank [34], propose unsupervised keyword and sentence extraction methods exploiting graph-based ranking algorithms to determine each vertex's importance. Extensions have been devised to incorporate document-level information [35] or introduce graph-based attention into encoderdecoder architectures [36]. As for abstractive summaries, results are mostly built on the cross-cutting success of graph neural networks (GNNs), a famed class of deep learning methods designed to process graph-represented data without imposing linearization or hierarchical constraints. Fernandes et al. [37] combine sequence encoders with GNNs feed with weakly-structured data inferred by the text through off-theshell NLP tools, including named entity recognition and coreference resolution; the final model compares favorably with baselines using only the sequential or graphical structure. Structured summarization also relates to the graph verbalization trend [38][39][40], where inputs may originate from knowledge graphs, information extraction or semantic parsing techniques. Instead of tackling a graph-to-text approach, An et al. [41] redefines the task of scientific papers summarization by utilizing a graph-enhanced encoder on top of a citation network. To concretize a text-graph complementary view-where GNN channels are used in addition to traditional document encoding-many researchers have tried different ways of automatically building a machinereadable knowledge representation linked to the underlying text [42][43][44], also considering different level of granularities, like entities and sentences [45]. OpenIE [46] and Stanford CoreNLP [47] are undoubtedly the two most popular libraries, targeting triplets and coreference resolution, respectively.</p>
<p>Importantly, graph-LSTMs appear as one of the most effective ways for constructing graph-guided summarizers [32,37,39,41,44,45], being competitive with large pretrained language models at a lower computational and environmental cost.</p>
<p>Event Extraction</p>
<p>Relation extraction (RE) systems primarily focus on highly-extractive binary relations, giving rise to a list of &lt; subject, predicate, object &gt; triplets connecting only entity-mention pairs. Despite their simplicity, flat triplets in biomedical science are notoriously inadequate to capture the source document's complete biological meaning (see "Comparison with Relation Extraction"). Per contra, event extraction (EE) systems can handle n-ary complex relations with nested and overlapping definitions. Remarkably, the EE history is very intertwined with biomedicine. Page 4 of 19</p>
<p>SN Computer Science</p>
<p>According to the BioNLP-ST competitions [48][49][50], events are composed of a trigger (a text span which testifies their occurrence, e.g., "interacts", "regulates"), a type (e.g., "binding", "regularization"), and a set of arguments with a specific role (e.g., "cause", which can be typed entities or events themselves. Please note that the event schemas (i.e., target event, entity, and role types) are pre-established, conforming to a reference ontology. Hence, differently from linguistically grounded semantic parsing techniques like abstract meaning representation (AMR), EE is domain-specific and-given an input sentence-outputs a graph only in case of evidence of interest.</p>
<p>Comparison with Relation Extraction</p>
<p>EE and RE have a lot of common ground. They both aim to detect relations from raw text and build structured, machine-readable representations. In RE settings, a relation can be defined as R = r(a 1 , a 2 , .., a n ) where r is a relation type and a i i = 1,  n is typically an entity. When n &gt; 2 , we say that R is a complex relation. Most of the RE systems, such as Open-IE, are not capable of extracting complex relations, they usually detect general-domain directed or undirected binary relations ( n = 2 ). The set of triples r(a 1 , a 2 ) might not be sufficient to represent underlying knowledge correctly, especially in biomedicine. This simplification of the original ground-truth complex structure may lead to the extraction of incomplete, uninformative, or erroneous facts [10,17]. Events have been precisely designed to solve these limitations, targeting a set of sophisticated closed-domain interactions. Figure 2 illustrates a real-world biomedical example recapping the crucial expressiveness divergences between RE and EE outputs.</p>
<p>Biomedical Event Extraction</p>
<p>A series of datasets have been proposed to improve EE research. Among these, we highlight the series of BioNLP shared tasks (BioNLP-STs) [10]. The labeling process is curated by domain experts, resulting in gold standards that can be used for training or benchmarks. The availability and coverage of biomedical EE corpora are still retained by the extremely expensive annotating process. For instance, annotating the GENIA corpus-one of the most popular biomedical EE datasets -took 1.5 years with five part-time annotators and two coordinators [51]. Such complexity-motivated cost hinders the number of examples, with training sets that typically consist of &lt; 300 instances. Another known problem is related to class imbalance, meaning that a large portion of event types might be under-represented.</p>
<p>Annotations for a certain text document (.txt) are saved in standoff .a<em> files, where a distinction is made between .a1 and .a2. Pointedly, an .a1 file encodes information about gold entities; instead, an .a2 file encodes information about triggers and the events rooted in them (i.e., reference multirelational interconnections between entities and triggers). Figure 3 shows an example. Each line in an .a</em> file refers to a single annotation. In turn, each annotation is made of multiple attributes separated by a single TAB character, always including an identifier. Entity and trigger annotations are accompanied by their type (e.g., "Gene" for an entity, "Localization" for an event), the (start, end) character offset of their mention, and the marked text. Instead, an event annotation consists of a SPACE-separated set comprising the trigger and the related arguments (entities or other triggers in case of sub-events). The event trigger is specified as TYPE:ID, thus identifying the event type and its trigger through the identifier. The event arguments are indicated as ROLE:ID pairs, thereby listing the semantic role and the argument identifier filling that role. So, by convention, the event type is stated both in the trigger and event annotation. Note that several events can share the same trigger and that, while the event trigger should be specified first, the event arguments can appear in any order.</p>
<p>Graph Construction</p>
<p>We construct graphs from raw documents applying Deep-EventMine (shortened as DEM) [52], a sentence-level EE discriminative neural network with state-of-the-art results on seven biomedical tasks. DEM does not depend on gold entities but carries out named entity recognition in end-to-end without losing too much performance. Starting from SciB-ERT contextual representations [53], DEM enumerates all the possible text spans in a sentence up to a certain window length, then executes a joint detection and classification flow of (1) entities and triggers, (2) roles, (3) events and modifiers, through custom layers.</p>
<p>Following Frisoni et al. [54], we shape events as multirelational graphs. Ergo, an event graph G = (V, E) consists of a finite set of nodes V = v 1 ,  v V -triggers or entitiesand a set of edges E  V  V modeling entity-trigger or trigger-trigger relations, with the seconds applying for nested events. Edges are directed, labeled, and unweighted, with no cycles. Both nodes and edges in G are associated with type information; hence, the graph is heterogeneous and multirelational. An edge e i,j connects node v i to node v j . Entities that don't belong to any event are ignored during graph construction. Node connections are encoded in an adjacency matrix A   VV , where a ij = 1 if there is a directed link from v i to v j , and 0 otherwise. We operate graph rewiring by adding a master node connecting all event nodes to enhance Page 5 of 19 500 SN Computer Science (https:// github. com/ dair-iitd/ OpenIE-stand alone). An event graph maps complex interactions mentioned in the text to a linkage between the trigger (dark gray) and entity (light gray) nodes, labeling edges and arguments with pre-defined roles and types aligned with an ontology. On the other hand, an OpenIE graph collects a possible set of triplets consisting of untyped text phrases. The OpenIE graph is merely extractive, error-prune, and devoid of additional metadata; worse, it does not capture semantic interconnections between n-ary participants, often ignoring crucial conditions for the correctness of a triplet or extracting incomplete facts difficult to merge with post-processing Figure taken from [19]  SN Computer Science the information flow and ensure we end up with a single graph rather than a set of small disjoint graphs.</p>
<p>Model</p>
<p>Our model observes a biencoder-decoder architecture (depicted in Fig. 4), taking inspiration from [32]. It takes two inputs, the sequence of all tokens present in the document x = x k and the event graph G, constructed as explicated in "Graph construction".</p>
<p>Document Encoders</p>
<p>The sequence of tokens x is fed to a bidirectional transformer-based encoder. We take token embeddings from the output of the last layer and pass them to a multi-layer bidirectional LSTM (BiLSTM), thus gaining the sequence of encoder hidden states h k . We implemented the following BERT [55] variants.</p>
<p>SciBERT</p>
<p>SciBERT [53] performs pretraining on a multi-domain corpus of scientific publications containing 1.14 M biomedical and computer science papers. It uses an in-domain vocabulary (SciVocab), characterized by a 42% token overlap with respect to the original BERT vocabulary, spotlighting a substantial difference in frequently used words between scientific and general-domain texts.</p>
<p>RoBERTa</p>
<p>RoBERTa [56] provides an updated version of BERT by optimizing its training process. The model is pretrained longer, with bigger batches and over more data. The next sentence prediction objective is removed. Longer sequences are taken into account, and the masking pattern-applied to the training data-is dynamically changed.</p>
<p>Graph Encoders</p>
<p>Node Initialization</p>
<p>Each node feature v i is initialized by taking into account both its text span and entity/trigger type. First, we average the per-token hidden states h k corresponding to the matched text. Then, we concatenate the acquired representation to the argument type embedding s a (or trigger type embedding s t ) learned by DEM. On this point, we believe that type metadata can play a vital role in augmenting the understanding capacity of the model and resolving ambiguities. The master node is represented by a 0-vector.</p>
<p>Graph Neural Network</p>
<p>Subsequently, the graph G is passed to a GNN. To assess the impact of edge features and broadly compare all the key graph representation learning techniques available in the literature, we explore both non-relation-aware and relationaware architectures (sketched in Fig. 5). Indeed, the demand for processing edge-featured graphs is quite common in biomedical tasks. For example, let's assume that the node "pantoprazole" is connected to "reflux": the edge type-"treat" or "cause"-can utterly change the meaning of the relation. It is clear that, in such a situation, edge features can be at least as significant as those of nodes. On the other side, traditional GNNs represent structural links through binary adjacency matrices and cannot handle multi-relational graphs  [19] equipped with additional edge-type information. Ergo, our work evaluates the presence or absence of benefits due to the consideration of the edge type for summarization purposes, based on current GNN contributions.</p>
<p>Graph Attention Network</p>
<p>We adopt a Graph Attention Network (GAT) variant introduced in [39], working with a self-attention setup where N independent heads are calculated and concatenated before a residual connection is applied. Fundamentally, each node embedding v i is obtained from a weighted average of its neighboring nodes N(v i ):</p>
<p>where n i,j is the attention mechanism tied to the n-th attention head, applied to node v i and node v j . W * are trainable parameters.</p>
<p>(1)
n i,j = exp  (W 1,n v i )  W 2,n v j   zN(v i ) exp  (W 1,n v i )  W 2,n v z  ,(2)v i =v i +  N n=1  jN(v i ) n i,j W 0,n v j ,</p>
<p>Edge-Aware Graph Attention Network</p>
<p>Edge-Aware Graph Attention Networks (EGATs) [57] are a variant of GATs with an edge-type-aware message passing. The attention weights n i,j are not only influenced by the features of the two nodes v i and v i , but also by the features of the edge connecting them e i,j . We represent the latter through edge-type one-hot embeddings. The node representation learning process is the following:</p>
<p>where [   ] indicates a concatenation and a learnable weight vector. 
(3) t n i,j = LeakyReLU T v i  v j  e i,j ,(4)n i,j = exp  t n i,j   zN(v i ) exp  t n i,z  ,(5)v i =  N n=1  jN(v i ) LeakyReLU   jN(v i ) n i,j W n v j  ,(a)</p>
<p>SN Computer Science</p>
<p>Relational Graph Convolutional Network</p>
<p>Relational Graph Convolutional Networks (R-GCNs) [58] are an extension of graph convolutional networks (GCNs) capable of modeling multi-relational data. Intuitively, relation-specific transformations are introduced, depending on the type and direction of an edge:</p>
<p>where N(v i ) r denotes the set of neighbor indices of node i under relation r  R . A central issue with applying Eq. (6) is the exponential growth in parameters as the number of relation types increases. This easily brings to large-size models and overfitting on rare relations. Centrally, R-GCNs treat edge types as class labels, which indicates that edges cannot include continuous attributes.</p>
<p>Levi Graph Transformation and Graph Attention Network</p>
<p>To ponder edge features, instead of modifying the model architecture by replacing the GAT module with an R-GCN or EGAT, we can transform the input graph into its equivalent Levi graph [59]. Similarly to [32,60,61], each edge e i,j is turned into an additional node directly connected to its original linking nodes v i and v j (Fig. 6). The new edge set contains an edge for every &lt; node, edge &gt; pair in the original graph. We end up with an unlabeled directed graph (bipartite by definition) without the risk of parameter explosion. Edges are represented and initialized in the same way as nodes, with features given by their type description. Using this strategy, the GNN naturally generates hidden states even for edges.
(6) v i = ReLU  rR  jN(v i ) r 1 |N(v i ) r | W r v j + W 0 v i ,</p>
<p>Decoder</p>
<p>The decoder uses a multi-layer unidirectional LSTM that generates summary tokens recurrently, exploiting at each time step t the graph and the document context vectors c v t (Eq. 7) and c t (Eq. 9).</p>
<p>Attending to the Graph</p>
<p>The graph context vector is computed based on the decoder hidden state s t :</p>
<p>where a v i,t denotes the attention mechanism from [62] corresponding to the ith node at time step t: u * are also trainable parameters.</p>
<p>Attending to the Document</p>
<p>Similarly, the document context vector is calculated over input tokens by considering c v t and encoder hidden states h k :</p>
<p>where a k,t denotes the attention corresponding to the k-th input document token at time step t: Fig. 6 Example of Levi transformation on an event-graph. Red nodes and edges are added as a kind of graph rewiring
(7) c v t =  i a v i,tv i ,(8)a v i,t = softmax u T 0 tanh W 3 s t + W 4vi . (9) c t =  k a k,t h k ,(10)a k,t = softmax u T 1 tanh W 5 s t + W 6 h k + W 7 c v t .</p>
<p>SN Computer Science</p>
<p>Token Prediction</p>
<p>The decoder hidden state s t is concatenated to the graph and document context vectors, expressing the salient content coming from both sources. This final representation is used to determine the probability distribution of the vocabulary vocab at time step t:</p>
<p>We also include a copy mechanism as in [32] to check out the embedding of the token generated at the previous time step y t1 :</p>
<p>P copy,t  [0, 1] is used as a soft switch to decide between generating a token from the vocabulary by sampling from P vocab,t , or copying a token from the input sequence by sampling from the attention distribution a k,t . The probability of generating the token w at time t is given by:</p>
<p>Training Objective</p>
<p>We employ a negative log-likelihood loss function between the generated summary  and the ground-truth y:</p>
<p>where x are the source documents and y and are the target summaries from training set D, G is the graph constructed from x, and = {W * , u * } is the set of the model trainable parameters.</p>
<p>Pseudocode</p>
<p>Algorithm 1 provides a concise explanation of the autoregressive generation of summary tokens y, starting from input document x.
(11) P vocab,t = softmax(W out [s t c t c v t ]).
(12) P copy,t = (W copy [s t c t c v t y t1 ]).</p>
<p>(13) P t (w) = P copy,t P vocab,t (w) + 1  P copy,t  kw k =w a k,t .
(14) L =  1 |D|  (y,x)D log p (y | x, G),</p>
<p>Experimental Setup</p>
<p>Dataset</p>
<p>We evaluate EASumm on the CDSR dataset [18], a publicly available corpus designed for assessing the automated generation of lay language summaries from biomedical scientific reviews. Besides creating accurate and factual summaries, this benchmark also requires a joint style transition from the original language of healthcare professionals to that of the general public. By imposing high abstraction and biomedical explanation constraints, CDSR is an ideal testbed. The training, validation, and test sets contain 5178, 500, and 999 samples. The documents can be downloaded directly from the Cochrane Database of Systematic Review 1 As for EE, each source document was split into a set of sentences and passed to DEM; the results were saved in standoff .a * files. Statistics about the total numbers of events, entities, and triggers extracted by DEM are detailed in Table 9.</p>
<p>Training Details and Parameters</p>
<p>All experiments were run using a single NVIDIA GeForce RTX 3090. We used the cased version of SciBERT to extract Page 10 of 19</p>
<p>SN Computer Science token embeddings. Hyperparameters are listed in Table 1.</p>
<p>We implemented RGCN and EGAT with Pytorch Geometric [63], while GAT is drawn on [43]. We used the version of DEM pretrained on the MLEE task 2 [64]-the EE benchmark linked to the biomedical domain most aligned to CDSR based on empirical tests (see "Event extraction dataset selection").</p>
<p>Material</p>
<p>For replication purposes, the code and the dataset are publicly available at https:// github. com/ disi-unibo-nlp/ easumm.</p>
<p>Baseline Methods and Comparisons</p>
<p>We conduct comprehensive ablation studies by testing different EASumm variants (hereinafter shortened as EAS), which we denote through the suffix, with "" symbolizing a module exclusion and " + " an addition/substitution:</p>
<p> G stands for the graph encoder exclusion;  + RB indicates the adoption of RoBERTa [56] instead of SciBERT to generate source document tokens embeddings;  TypE refers to the node type exclusion during the initialization;  + EGAT + RGCN specify the adoption of EGAT and RGAT, respectively, in replacement of GAT;  + BIp suggests the employment of the Levi transformation on GAT-processed event graphs to treat nodes and edges equally.</p>
<p>For a comparative analysis, we experiment with two extractive methods:</p>
<p> Oracle extractive: it creates an oracle summary by selecting the set of sentences in the document that generates the highest ROUGE-2 score with the ground-truth summary (i.e., syntactic match upper bound);  BERT [55]: inter-sentence encoder with classification head, supervised through an Oracle extractive signal;</p>
<p>and two abstractive methods:</p>
<p> Pointer generator [65]: standard seq2seq model with a pointer network that allows both copying words from the source and generating new words from a fixed vocabulary;  BART [25]: full-transformer pretrained on large corpora by reconstructing text after a corruption phase with an arbitrary noising function. We also take into account a variant with additional pretraining steps on PubMed to compensate for the limited training data. Specifically, we use the PMC articles dataset, 3 containing 300K PubMed abstracts.</p>
<p>Evaluation</p>
<p>Quantitative Analysis</p>
<p>As done in [18], we use ROUGE [66] to evaluate the summarization performance. ROUGE-n quantifies the overlap of n-grams between the model-generated summary and the human-generated reference summary, and ROUGE-L measures the longest matching sequence of words using the longest common subsequence. We report the Block-diagonal-decomposition (4 blocks) Aggregation scheme Mean ROUGE-1, ROUGE-2 and ROUGE-L scores computed using pyrouge. 4 Given the supplementary scientific  public language translation objective of the CDSR task, other than informativeness, we are interested in measuring the ease with which a reader can understand a passage, defined as readability. We use three standard metrics for this goal: Flesch-Kincaid grade level [67], Gunning fog index [68], and Coleman-Liau index [69]. Their equations are as follows:
 Flesch-Kincaid grade level  Gunning fog index
where complex words are those words with three or more syllables.</p>
<p> Coleman-Liau index</p>
<p>where L and S are the average numbers of letters and sentences per 100 words, respectively. All these evaluation metrics are computed using textstat 5 and estimate the years of formal education a person needs to understand the text. Lower scores indicate that the text is easier to read; for instance, scores of 13-16 correspond to college-level reading ability in the United States education system.</p>
<p>Qualitative Analysis</p>
<p>Automatic evaluation metrics for summarization are not able to grasp all the desired quality dimensions, particularly in highly abstractive settings grounded on semantics [70]. To fill this gap, we run an in-depth human evaluation study to analyze proper text properties and identify primary error sources. We randomly sample 50 CDSR test set instances and engage three native or fluent English speakers with biomedical expertise (average age: 24.6 years old; average time for completion: 2 h; education level: 1 PhD and 2 master students; no compensation). Selection criteria guarantee that our annotators are representative of the college-educated lay public. Precisely, we presented each human rater with the source document, the inferred summary, and the reference (15) 
(17) 0.0588L  0.296S  15.8,
summary. Then, we asked raters to judge the prediction along three quality criteria with a Likert scale from 1 (worst) to 5 (best).</p>
<p> Informativeness. Does the summary supply enough necessary content coverage from the input article?  Fluency. Does the text progress naturally? Is it grammatically correct (e.g., no fragments and missing components) and coherent whole?  Understandability, CDSR-related [18]. Is the summary more effortless to understand than the source?</p>
<p>We even invite evaluators to binary label whether summaries contain any of the following classes of unfaithful errors: (1) Hallucination, fabricated content not present in the input; (2) Deletion or substitution, erroneously missing or edited elements (e.g., entities with altered semantic role);</p>
<p>(3) Repetitiveness, repeated fragments. Complete guidelines are in "Human evaluation guideline".</p>
<p>Results</p>
<p>Automated Summary Evaluation</p>
<p>Evaluation on full dataset Table 2 exhibits the results of our presented models compared to baseline methods. Notably, EASumm gives better ROUGE scores than all its variants. The positive effect of the event graph is motivated by the performance drop associated with EASummG. Features obtained via a domain-coherent language model like SciBERT contribute to superior results than RoBERTa. Further, the graph encoder in the RoBERTa implementation does not furnish any progress over the solution without it. Type-augmented node initialization techniques show clear advantages, confirming our hypotheses on the usefulness of domain-specific and semantic text augmentations pulled by DEM. EASumm significantly outperforms BERT, pointer generator, and plain Bi-LSTM architectures but does not beat BART (quality gap of  6 ROUGE points), despite greater readability on average. This behavior suggests a future direction of developing event-driven models on top of a large pretrained encoder-decoder model such as BioBART [71]. By contra, we emphasize the lightness of our final model compared to BART BASELARGE , which counts 8 M trainable parameters instead of 139 M/406 M (up to 50x fewer weights). Finally, we mention the importance of expanding training data with more biomedical corpora. Page 12 of 19</p>
<p>SN Computer Science</p>
<p>Evaluation on Subsets</p>
<p>As documented in "Event extraction dataset selection", the number of events extracted in each document may be contained, leading to sparse graphs with few nodes. Thus, we suspect that the graph encoder contribution could be capped, expecting a more noticeable performance gap regarding EASummG for those documents containing a larger number of events extracted per sentence (abbreviated as EEpS). Following this line of thought, we assembled four subsets where source documents have an EEpS greater than 0.1, 0.2, 0.3, and 0.4. Table 3 reveals the ROUGE scores on each of the four subsets for the different model variants and BART BASE . As EEpS increases, the performance gap between the solutions with and without graph encoder widens, proving our speculation. We can also notice how the EASumm performance gets closer to BART BASE . Given the linear relationship between EEpS and the ROUGE gap-emerged from our empirical experiments, we conjecture that the deficit can be wiped out entirely with event extraction models pretrained on sentences conveying topics more aligned to the CDSR ones ( Table 4).</p>
<p>The Impact of Relations During Graph Representation Learning</p>
<p>As pointed out in Tables 5 and 6 from the point of view of ROUGE scores, the solutions adopting EGAT and Levi transformation yield the best results, underlining the major limit of RGCNs, which is not being able to process custom representations for edge features. Distinctly, EGAT is the graph modeling technique that most benefits from higher EEpS; by contra, the other two methods are characterized by fluctuating ROUGE scores with respect to EEpS. In terms of readability, EGAT dominates all three metrics, followed by RGCN and Levi transformation. Surprisingly, we couldn't improve the original solution that ignores relation types when encoding the graph. However, by looking at the distribution of relation types (see Table 4), we notice that it is extremely uneven. In particular, almost all nodes are linked by either an Instrument or Theme edge. The other 8 relation types don't seem to have a relevant impact; therefore, we can easily understand why this negatively affects the potential contribution of edge-aware solutions (Fig. 7).   Table 7 portrays the results of human evaluations. The average inter-rater agreement is 0.61 (Kendall's coefficient  [  1, 1] indicating low to high association), a good score considering the subjectivity of the rating task. For full transparency, we publicly release the results of our human evaluation. 6 Besides the need for larger-scale studies, this work delivers helpful preliminary evidence. EASumm obtains suitable scores in fluency and understandability. Deletion and substitution in verbalized facts appear to be the most frequent error type, together with repetitiveness. After inspection, we find several utterances with swapped entities not belonging to event mentions, thus not attributable to a noneffectiveness of event injection. Low hallucinations testify to the advantage of leveraging event graph representations. With a closer look, we observe that human-written summaries also include a non-trivial amount of commonsense and world knowledge not mentioned by the input article. For example, for a source document discussing "spironolactone", the human writer may add "used since the 1960s" in the summary. Consequently, we invite the reader to reflect on attributing low factuality scores to generative models, weighting them against the dataset's properties.</p>
<p>Human Evaluation</p>
<p>Conclusion</p>
<p>We introduced EASumm, the first abstractive summarization model augmenting source documents with explicit, structured medical evidence extracted from them, thereby concretizing a tandem text-graph architecture. We demonstrated the significant positive influence of biomedical event extraction for summarization, allowing a model to better distinguish semantics and lexical surface. Indeed, we showed improvements in ROUGE and readability scores, observing a strong connection between the summer quality and (1) the number of events extracted from the input document, (2) the enhanced node features initialization considering both domain-specific pretrained language models and entity types. Contrary to expectations, event-graph representation learning does not benefit from the awareness of the relation type. The motivation is to be found in the task-driven nature of event extraction and in the poor capacity of current graph neural networks in managing multi-relational graphs.</p>
<p>Although the numerous newly introduced graph-LSTM models combined with structured knowledge, we establish that these architectures are far from being competitive with generative transformer-based solutions like BART.</p>
<p>Future Directions</p>
<p>Based on our findings, we suggest nine promising future research directions:</p>
<ol>
<li>using large pretrained encoder-decoder transformers to replace graph-LSTMs architectures [74]; 2. increasing the number of the events by merging available biomedical benchmarks thanks to generative event extraction approaches and non-discriminative architectures [75]; 3. increasing the size of the document-level graphs by combining events with linguistically-grounded abstract meaning representations [74]; 4. exploiting multimodal text-graph alignments with metric learning techniques as in [76][77][78]; 5. exploring end-to-end event extraction and document summarization; 6. discovering new connections between nodes useful for increasing summarization performance (i.e., dynamic event graph construction), conveying techniques like random perturbation [79,80] and iterative deep graph learning [81]; 7. performing node relevance scoring supported by term weighting [82] and/or perplexity metrics [12];  [74]; 11. pushing the interaction and mutual influence between graph and text encoders; 12. injecting subgraphs fetched from external structured memories using dense representations [85], devising retrieval-enhanced language models [86].</li>
</ol>
<p>Appendix A: Section Title of First Appendix</p>
<p>Dataset Statistics</p>
<p>We report additional statistics for each source and target document in CDSR (Table 8). Note: a readability score is calculated by averaging the results of the metrics described in "Evaluation". Table 9 provides statistics on the effectiveness of the three DEM models trained on the available biomedical datasets with the largest number of annotations and ontological targets [10]. MLEE stands out as the EE task most related to CDSR topics.  Partially relevant and misses the main point of the article 3</p>
<p>Event Extraction Dataset Selection</p>
<p>Relevant, but misses the main point of the article 4</p>
<p>Successfully captures the main point of the article but some relevant content is missing 5</p>
<p>Successfully captures the main point of the article Fluency: 1</p>
<p>Summary is full of garbage fragments and is hard to understand 2 Summary contains fragments, missing components but has some fluent segments 3</p>
<p>Summary contains some grammar errors but is in general fluent 4</p>
<p>Summary has relatively minor grammatical errors 5</p>
<p>Fluent summary Understandability: 1</p>
<p>Source is easier to understand than the summary 2 Summary is as understandable as the source 3</p>
<p>Summary is easier to understand than the source but it is partially written in the language of healthcare professionals 4</p>
<p>Summary is easier to understand than the source but contains some terms from the language of healthcare professionals 5</p>
<p>Summary is easier to understand than the source and is written in the language of the general public Page 16 of 19</p>
<p>SN Computer Science</p>
<p>Human Evaluation Guideline Table 10 explains each Likert scale score meaning for the assessed quality criteria, so as to minimize annotation ambiguity and subjectivity. We believe this is important to obtain comparable results and work towards an objective and replicable human evaluation.</p>
<p>Funding Open access funding provided by Alma Mater Studiorum -Universit di Bologna within the CRUI-CARE Agreement.</p>
<p>Declarations</p>
<p>Conflict of interest</p>
<p>The authors declare no conflict of interest.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.</p>
<p>Fig. 1
1Qualitative example of event-driven biomedical abstractive summarization. The event graph localizes relevant information for entities and triggers, providing a global context pivotal for generating a better-quality summary. Figure taken from [19] Page 3 of 19 500</p>
<p>Fig.
Fig. 2 Comparison between semantic graphs obtained with closed-domain event extraction and open-domain relation extraction on a sentence taken from a PubMed article. The prediction enclosed in the green box comes from DeepEvent-Mine MLEE, while the other is made with OpenIE 5.1 (https:// github. com/ dair-iitd/ OpenIE-stand alone). An event graph maps complex interactions mentioned in the text to a linkage between the trigger (dark gray) and entity (light gray) nodes, labeling edges and arguments with pre-defined roles and types aligned with an ontology. On the other hand, an OpenIE graph collects a possible set of triplets consisting of untyped text phrases. The OpenIE graph is merely extractive, error-prune, and devoid of additional metadata; worse, it does not capture semantic interconnections between n-ary participants, often ignoring crucial conditions for the correctness of a triplet or extracting incomplete facts difficult to merge with post-processing Figure taken from [19]</p>
<p>Fig. 3
3Example of.txt, .a1, and. a2 files Page 6 of 19</p>
<p>Fig. 4
4Our event-augmented summarization framework. The summary is generated by attending both the event graph and the input documentFigure taken from</p>
<p>Fig. 5 a
5Illustration of multi-head attention (with K = 3 heads) by node 1 on its neighborhood. Each arrow represents an independent attention computation. b Diagram showing the update of a single event-graph node (red) in the R-GCN model. GAT and R-GCN architectures used for event graph encoding Page 8 of 19</p>
<p>Table 1
1Final picked values for model hyperparametersHyperparameters </p>
<p>LSTM input word embedding size 
128 
LSTM hidden embedding size 
256 
LSTM number of layers 
2 
Dropout rate 
0.1 
Learning rate 
110 3 
Optimizer 
AdamW (0.9 </p>
<p>1 , 0.999 
2 , 0.5 w. 
decay) 
Decoding strategy 
Beam Search 
Number of beams 
5 
GAT 
Number of self-attention heads 
4 
Hidden size 
556 
Node size 
556 
EGAT 
Hidden size 
556 
Node size 
556 
Edge size 
10 
Number of layers 
2 
R-GCN 
Hidden size 
556 
Node size 
556 
Number of relations 
10 
Regularization </p>
<p>Table 2
2Automated evaluation 
on the full test set of CDSR 
with ROUGE and readability 
metrics </p>
<p>Top: extractive models. Middle: abstractive models. Bottom: our event-augmented abstractive models. The 
best scores for each model type are boldened </p>
<p>Model 
ROUGE-1 ROUGE-2 ROUGE-L Flesch-Kincaid Gunning Coleman-Liau </p>
<p>ORAClE ExTRACTIvE 
53.56 
25.54 
49.56 
14.85 
13.45 
16.13 
BERT 
26.60 
11.11 
24.59 
13.44 
13.26 
14.40 
pOINTER GENERATOR 
38.33 
14.11 
35.81 
16.36 
15.86 
15.90 
BART BASE 
51.39 
20.81 
48.56 
14.31 
18.13 
14.00 
BART LARGE 
52.53 
21.83 
49.75 
13.59 
14.16 
14.45 
BART LARGE +puBmEd 52.66 
21.73 
49.97 
13.30 
13.80 
14.28 
Ours 
EAS-G+RB 
44.23 
18.03 
41.68 
14.05 
17.86 
14.05 
EAS+RB 
44.12 
17.82 
41.60 
13.57 
17.29 
13.77 
EAS-G 
44.68 
17.95 
42.25 
12.41 
16.76 
12.82 
EAS-TypE 
45.41 
18.36 
42.99 
12.14 
16.40 
12.91 
EAS 
46.30 
18.73 
43.78 
12.42 
16.68 
13.06 </p>
<p>Table 3 ROUGE performance on four testset subsets, depending on 
the minimum number of extracted events per sentence (EEpS) </p>
<p>Table 5
5grams of CO 2 . The adoption of Green NLP technology can revolutionize the way we use AI to understand and address environmental issues[73].Automated evaluation 
on the full test set of CDSR 
with ROUGE and readability 
metrics for EASumm models 
with relation-aware graph 
representation learning </p>
<p>The best scores for each model type are boldened </p>
<p>Model 
ROUGE-1 
ROUGE-2 
ROUGE-L 
Flesch-Kincaid 
Gunning 
Coleman-Liau </p>
<p>EAS+ EGAT 
45.14 
18.16 
42.73 
12.21 
16.32 
12.71 
EAS+ RGCN 
44.79 
17.74 
42.36 
12.40 
16.59 
13.02 
EAS+ BIp 
45.35 
17.96 
42.90 
12.26 
16.51 
12.98 
EAS 
46.30 
18.73 
43.78 
12.42 
16.68 
13.06 </p>
<p>Table 6 Link between ROUGE performance and minimum number 
of extracted events per sentence (EEpS) in the case of relation-aware 
graph representation learning </p>
<p>EEpS 
Model 
R-1 
R-2 
R-L </p>
<blockquote>
<p>0.4 
EAS+EGAT 
46.35 
18.35 
43.62 
EAS+RGCN 
45.28 
18.11 
42.25 
EAS+BIP 
45.94 
17.16 
42.91 
0.3 
EAS+EGAT 
45.33 
17.33 
42.77 
EAS+RGCN 
44.49 
17.04 
41.78 
EAS+BIP 
44.71 
16.78 
41.76 
0.2 
EAS+EGAT 
45.71 
17.90 
43.14 
EAS+RGCN 
44.73 
17.10 
42.05 
EAS+BIP 
45.17 
17.14 
42.42 
0.1 
EAS+EGAT 
45.29 
18.03 
42.76 
EAS+RGCN 
45.20 
17.48 
42.57 
EAS+BIP 
45.54 
17.62 
42.89 </p>
</blockquote>
<p>Fig. 8 Comparison between EAS with BiLSTMs (ours) and 
BART BASE in terms of inference time and carbon footprint on the 
CDSR test set </p>
<p>Table 7
7Average human evaluation scores on informativeness (Inf.), fluency (Flu.), and understandability (Und.) (1-to-5), with error percentages for hallucination (Hal.), deletion or substitution (Del./Sub.), and repetitiveness (Rep.) . utilizing continuous (semantic) edge features within the graph neural network; 10. introducing additional loss functions based on reinforcement learning and semantic-driven rewardsInf. 
Flu. 
Und. 
Hal. 
Del./Sub. 
Rep. </p>
<p>3.16 
3.4 
3.44 
18% 
35% 
34% </p>
<p>Table 8
8CDSR average number of words (N. words), sentences (N. sents), and readabilityDocument 
Set 
N. words 
N. sents 
Readability </p>
<p>Source 
Train 
644 
26 
16.43 
Val 
643 
26 
16.60 
Test 
653 
27 
16.45 
Target 
Train 
349 
16 
15.15 
Val 
348 
16 
15.20 
Test 
353 
16 
15.22 </p>
<p>Table 9
9CDSR event extraction results using distinct versions of DeepEventMine pretrained on MLEE[64], CG13[87] and GE13[88] tasksWe report the average number of events (N. evs.), triggers (N.trigs.) and arguments (N. args.) extracted from training, validation and test samples in each source documentTable 10Quality aspect scales and value-level explanationsTask 
Set 
N. evs. 
N. trigs. 
N. args. </p>
<p>MLEE 
Train 
2.63 
2.31 
2.78 
Val 
2.54 
2.20 
2.73 
Test 
2.70 
2.42 
2.84 
CG13 
Train 
2.13 
1.95 
2.30 
Val 
2.02 
1.85 
2.19 
Test 
2.12 
1.95 
2.30 
GE13 
Train 
0.05 
0.05 
0.05 
Val 
0.06 
0.06 
0.07 
Test 
0.07 
0.06 
0.06 </p>
<p>https:// www. cochr aneli brary. com/ cdsr/ revie ws (accessed Nov.4,  2022). We point out to the reader that the authors claim a slightly higher number of examples (5195, 500, and 1000), then removed from the official repository.
http:// nactem. ac. uk/ MLEE. 3 https:// www. kaggle. com/ cvltm ao/ pmc-artic les.
https:// github. com/ bhein zerli ng/ pyrou ge. 5 https:// pypi. org/ proje ct/ texts tat/, version 0.72.
https:// github. com/ disi-unibo-nlp/ easumm/ blob/ master/ sn_ human_ evalu ations. xlsx.
Department of Mathematics (DM), University of Bologna, Piazza di Porta S. Donato 5, 40126 Bologna, Italy
Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Authors and Affiliations
The language instinct. S Pinker, William Morrow &amp; coNew YorkPinker S. The language instinct. New York: William Morrow &amp; co; 1994.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, Advances in Neural Information Processing Systems. Curran Associates Inc33Brown T, Mann B, Ryder N, Subbiah M, et al. Language models are few-shot learners. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, et al., editors. Advances in Neural Information Pro- cessing Systems, vol. 33. Virtual: Curran Associates Inc; 2020. p. 1877-901. https:// proce edings. neuri ps. cc/ paper/ 2020/ file/ 1457c 0d6bf cb496 7418b fb8ac 142f6 4a-Paper. pdf</p>
<p>On the dangers of stochastic parrots: can language models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21)New York, NY, USAAssociation for Computing MachineryBender EM, Gebru T, McMillan-Major A, Shmitchell S. On the dangers of stochastic parrots: can language models be too big? In: Proceedings of the 2021 ACM Conference on Fairness, Account- ability, and Transparency (FAccT '21). New York, NY, USA: Association for Computing Machinery; 2021. p. 610-23.</p>
<p>Detecting hallucinated content in conditional neural sequence generation. C Zhou, G Neubig, J Gu, M Diab, F Guzmn, L Zettlemoyer, M Ghazvininejad, ACL/IJCNLP (Findings). Findings of ACL. BangkokAssociation for Computational LinguisticsZhou C, Neubig G, Gu J, Diab M, Guzmn F, Zettlemoyer L, Ghazvininejad M. Detecting hallucinated content in conditional neural sequence generation. In: ACL/IJCNLP (Findings). Find- ings of ACL, vol. ACL/IJCNLP 2021. Bangkok: Association for Computational Linguistics; 2021. pp. 1393-404.</p>
<p>Adversarial attacks on deep-learning models in natural language processing: a survey. W E Zhang, Q Z Sheng, A Alhazmi, C Li, 10.1145/3374217ACM Trans Intell Syst Technol. 11317Zhang WE, Sheng QZ, Alhazmi A, Li C. Adversarial attacks on deep-learning models in natural language processing: a survey. ACM Trans Intell Syst Technol. 2020;11(3):24-12441. https:// doi. org/ 10. 1145/ 33742 17.</p>
<p>Text summarization in the biomedical domain. M Moradi, N Ghadiri, arXiv:1908.02285arXiv preprintMoradi M, Ghadiri N. Text summarization in the biomedical domain 2019. arXiv preprint arXiv: 1908. 02285</p>
<p>Learning interpretable and statistically significant knowledge from unlabeled corpora of social text messages: A novel methodology of descriptive text mining. G Frisoni, G Moro, A Carbonaro, DATA 2020 -Proceedings of 9th International Conference Data Science. SciTePresseid=2-s2.0-85092 00963 6 &amp; partn erID= 40 &amp; md5= 27541 a3b46 d782b b7984 eed8b a7fa8 a3Frisoni G, Moro G, Carbonaro A. Learning interpretable and sta- tistically significant knowledge from unlabeled corpora of social text messages: A novel methodology of descriptive text mining. In: DATA 2020 -Proceedings of 9th International Conference Data Science, Technology and Application. SciTePress, Virtual; 2020. p. 121-34. https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85092 00963 6 &amp; partn erID= 40 &amp; md5= 27541 a3b46 d782b b7984 eed8b a7fa8 a3</p>
<p>Phenomena explanation from text: unsupervised learning of interpretable and statistically significant knowledge. G Frisoni, G Moro, 10.1007/978-3-030-83014-4_14eid=2- s2.0-85113 29201 3 &amp; doi= 10. 1007% 2f978-3-030-83014-4_ 14 &amp; partn erID= 40 &amp;5=DATA (Revised Selected Papers). 14467Cham: Springer. 33fa9 2fd1f 11dff 84de3 1aac3 72991Frisoni G, Moro G. Phenomena explanation from text: unsuper- vised learning of interpretable and statistically significant knowl- edge. In: DATA (Revised Selected Papers), vol. 1446. Cham: Springer; 2020. p. 293-318. https:// doi. org/ 10. 1007/ 978-3-030- 83014-4_ 14. https:// www. scopus. com/ inward/ record. uri? eid=2- s2.0-85113 29201 3 &amp; doi= 10. 1007% 2f978-3-030-83014-4_ 14 &amp; partn erID= 40 &amp;5= 33fa9 2fd1f 11dff 84de3 1aac3 72991 7a</p>
<p>Towards rare disease knowledge graph learning from social posts of patients. G Frisoni, G Moro, A Carbonaro, 10.1007/978-3-030-62066-0_44eid=2-s2.0-85102 64012 8 &amp; doi= 10. 1007% 2f978-3-030-62066-0_ 44 &amp; partn erID= 40 &amp; md5= 7b08b da5b0 f9de0 0d4e5 acdac cfe77 07In: RiiForum. Cham. SpringerFrisoni G, Moro G, Carbonaro A. Towards rare disease knowl- edge graph learning from social posts of patients. In: RiiForum. Cham: Springer; 2020. p. 577-89. https:// doi. org/ 10. 1007/ 978- 3-030-62066-0_ 44. https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85102 64012 8 &amp; doi= 10. 1007% 2f978-3-030-62066-0_ 44 &amp; partn erID= 40 &amp; md5= 7b08b da5b0 f9de0 0d4e5 acdac cfe77 07</p>
<p>A survey on event extraction for natural language understanding: riding the biomedical literature wave. G Frisoni, G Moro, A Carbonaro, 10.1109/ACCESS.2021.3130956IEEE Access. 9Frisoni G, Moro G, Carbonaro A. A survey on event extraction for natural language understanding: riding the biomedical literature wave. IEEE Access. 2021;9:160721-57. https:// doi. org/ 10. 1109/ ACCESS. 2021. 31309 56.</p>
<p>Combining pre-trained language models and structured knowledge. P Colon-Hernandez, C Havasi, J B Alonso, M Huggins, arXiv:2101.12294CoRR.Colon-Hernandez P, Havasi C, Alonso JB, Huggins M et al. Com- bining pre-trained language models and structured knowledge. CoRR. arXiv: 2101. 12294 2021.</p>
<p>QA-GNN: reasoning with language models and knowledge graphs for question answering. M Yasunaga, H Ren, A Bosselut, P Liang, J Leskovec, arXiv:2104.06378Yasunaga M, Ren H, Bosselut A, Liang P, Leskovec J. QA-GNN: reasoning with language models and knowledge graphs for ques- tion answering. CoRR. arXiv: 2104. 06378 2021.</p>
<p>Semantics-aware bert for language understanding. Z Zhang, Y Wu, H Zhao, Z Li, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligenceNew York, USA34Zhang Z, Wu Y, Zhao H, Li Z et al. Semantics-aware bert for lan- guage understanding. In: Proceedings of the AAAI conference on artificial intelligence, New York, USA, vol. 34; 2020. p. 9628-35.</p>
<p>A novel method for unsupervised and supervised conversational message thread detection. G Domeniconi, K Semertzidis, V Lpez, E M Daly, 10.5220/0006001100430054DATA 2016-Proceedings of 5th International Conference Data Science, Technology and Application. LisbonSciTePressDomeniconi G, Semertzidis K, Lpez V, Daly EM, et al. A novel method for unsupervised and supervised conversational message thread detection. In: DATA 2016-Proceedings of 5th Interna- tional Conference Data Science, Technology and Application. Lis- bon: SciTePress; 2016. p. 43-54. https:// doi. org/ 10. 5220/ 00060 01100 430054</p>
<p>Job recommendation from semantic similarity of linkedin users' skills. G Domeniconi, G Moro, A Pagliarani, K Pasini, 10.5220/0005702302700277eid=2-s2.0-84970 03938 1 &amp; doi= 10ICPRAM 2016. RomeSciTePress5220% 2f000 57023 02700 277 &amp; partn erID= 40 &amp; md5= eca46 33aae 1e941 8df03 4aaa5 f3a60 20Domeniconi G, Moro G, Pagliarani A, Pasini K, et al. Job rec- ommendation from semantic similarity of linkedin users' skills. In: ICPRAM 2016. Rome: SciTePress; 2016. p. 270-77. https:// doi. org/ 10. 5220/ 00057 02302 700277. https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-84970 03938 1 &amp; doi= 10. 5220% 2f000 57023 02700 277 &amp; partn erID= 40 &amp; md5= eca46 33aae 1e941 8df03 4aaa5 f3a60 20</p>
<p>Unsupervised descriptive text mining for knowledge graph learning. G Frisoni, G Moro, A Carbonaro, IC3K 2020-Proceedings of 12th International Joint Conference Knowledge Discovery, Knowledge Engineering and Knowledge Management. Frisoni G, Moro G, Carbonaro A. Unsupervised descriptive text mining for knowledge graph learning. In: IC3K 2020-Proceed- ings of 12th International Joint Conference Knowledge Discov- ery, Knowledge Engineering and Knowledge Management, vol.</p>
<p>-s2.0-85107 11334 0 &amp; partn erID= 40 &amp; md5= 7a4cc 3ae8a 6894d 1a3ff f499b b4bf7. SciTePress17SciTePress, Virtual; 2020. p. 316-24. https:// www. scopus. com/ inward/ record. uri? eid=2-s2.0-85107 11334 0 &amp; partn erID= 40 &amp; md5= 7a4cc 3ae8a 6894d 1a3ff f499b b4bf7 17</p>
<p>A robust approach to extract biomedical events from literature. Q-C Bui, Map Sloot, 10.1093/bioinformatics/bts487Bioinformatics. 2820Bui Q-C, Sloot MAP. A robust approach to extract biomedical events from literature. Bioinformatics. 2012;28(20):2654-61. https:// doi. org/ 10. 1093/ bioin forma tics/ bts487.</p>
<p>Automated lay language summarization of biomedical scientific reviews. Y Guo, W Qiu, Y Wang, T Cohen, AAAI. AAAI PressGuo Y, Qiu W, Wang Y, Cohen, T. Automated lay language sum- marization of biomedical scientific reviews. In: AAAI. AAAI Press, Virtual; 2021. p. 160-68</p>
<p>Enhancing biomedical scientific reviews summarization with graph-based factual evidence extracted from papers. G Frisoni, P Italiani, F Boschi, G Moro, 10.5220/0011354900003269Proceedings of the 11th international conference on data science, technology and applications, DATA. Lisbon: SCITEPRESS; 2022. Cuzzocrea A, Gusikhin O, van der Aalst WMP, Hammoudi Sthe 11th international conference on data science, technology and applications, DATA. Lisbon: SCITEPRESS; 2022549003269Frisoni G, Italiani P, Boschi F, Moro G. Enhancing biomedi- cal scientific reviews summarization with graph-based factual evidence extracted from papers. In: Cuzzocrea A, Gusikhin O, van der Aalst WMP, Hammoudi S, editors. Proceedings of the 11th international conference on data science, technology and applications, DATA. Lisbon: SCITEPRESS; 2022. pp. 168-79. https:// doi. org/ 10. 5220/ 00113 54900 003269</p>
<p>Text summarization with pretrained encoders. Y Liu, M Lapata, K Inui, J Jiang, V Ng, X Wan, EMNLP/IJCNLP. Hong Kong. Liu Y, Lapata M. Text summarization with pretrained encod- ers. In: Inui K, Jiang J, Ng V, Wan X, editors. EMNLP/IJCNLP. Hong Kong, China: Association for Computational (1); 2019. p. 3730-40.</p>
<p>Unified language model pre-training for natural language understanding and generation. L Dong, N Yang, W Wang, F Wei, Advances in neural information processing systems. Curran Associates, Inc322d9a5 0d5ac 1f713 f8b34 d9aac 5a-Paper. pdfDong L, Yang N, Wang W, Wei F, et al. Unified language model pre-training for natural language understanding and generation. In: Wallach H, Larochelle H, Beygelzimer A, d' Alch-Buc F, et al. editors. Advances in neural information processing systems, vol. 32. Vancouver: Curran Associates, Inc.; 2019. https:// proce edings. neuri ps. cc/ paper/ 2019/ file/ c20bb 2d9a5 0d5ac 1f713 f8b34 d9aac 5a-Paper. pdf</p>
<p>Leveraging pre-trained checkpoints for sequence generation tasks. S Rothe, S Narayan, A Severyn, 10.1162/tacl_a_00313Trans Assoc Comput Linguist. 8Rothe S, Narayan S, Severyn A. Leveraging pre-trained check- points for sequence generation tasks. Trans Assoc Comput Lin- guist. 2020;8:264-80. https:// doi. org/ 10. 1162/ tacl_a_ 00313.</p>
<p>PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. J Zhang, Y Zhao, M Saleh, P J Liu, arXiv:1912.08777Zhang J, Zhao Y, Saleh M, Liu PJ. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. CoRR. arXiv: 1912. 08777 2019.</p>
<p>Prophetnet: predicting future n-gram for sequence-to-sequence pre-training. W Qi, Y Yan, Y Gong, D Liu, EMNLP (Findings). Findings of ACL. Qi W, Yan Y, Gong Y, Liu D, et al. Prophetnet: predicting future n-gram for sequence-to-sequence pre-training. In: EMNLP (Find- ings). Findings of ACL, vol. EMNLP. Association for Computa- tional Linguistics, Virtual; 2020. p. 2401-10.</p>
<p>denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, Mohamed A Levy, O Stoyanov, V Zettlemoyer, L Bart, ACL, Association for Computational Linguistics. Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L. BART: denoising sequence-to- sequence pre-training for natural language generation, transla- tion, and comprehension. In: ACL, Association for Computa- tional Linguistics, Virtual; 2020. p. 7871-80.</p>
<p>Semantic self-segmentation for abstractive summarization of long legal documents in low-resource regimes. G Moro, L Ragazzi, Thirty-Sixth AAAI conference on artificial intelligence. AAAI 2022. AAAI PressMoro G, Ragazzi L. Semantic self-segmentation for abstrac- tive summarization of long legal documents in low-resource regimes. In: Thirty-Sixth AAAI conference on artificial intel- ligence. AAAI 2022. Virtual: AAAI Press; 2022. p. 1-9.</p>
<p>Efficient memory-enhanced transformer for long-document summarization in low-resource regimes. G Moro, L Ragazzi, L Valgimigli, G Frisoni, C Sartori, G Marfia, 10.3390/s23073542Sensors. 237Moro G, Ragazzi L, Valgimigli L, Frisoni G, Sartori C, Marfia G. Efficient memory-enhanced transformer for long-document summarization in low-resource regimes. Sensors. 2023;23(7):1. https:// doi. org/ 10. 3390/ s2307 3542.</p>
<p>Discriminative marginalized probabilistic neural method for multi-document summarization of medical literature. G Moro, L Ragazzi, L Valgimigli, D Freddi, 10.18653/v1/2022.acl-long.15Proceedings of the 60th annual meeting of the association for computational linguistics. the 60th annual meeting of the association for computational linguisticsDublinAssociation for Computational Linguistics1Moro G, Ragazzi L, Valgimigli L, Freddi D. Discriminative marginalized probabilistic neural method for multi-document summarization of medical literature. In: Proceedings of the 60th annual meeting of the association for computational linguis- tics, vol. 1: long papers. Dublin: Association for Computational Linguistics; 2022. p. 180-89. https:// doi. org/ 10. 18653/ v1/ 2022. acl-long. 15. https:// aclan tholo gy. org/ 2022. acl-long. 15</p>
<p>On faithfulness and factuality in abstractive summarization. J Maynez, S Narayan, B Bohnet, R T Mcdonald, ACL, Association for Computational Linguistics. Maynez J, Narayan S, Bohnet B, McDonald RT. On faithfulness and factuality in abstractive summarization. In: ACL, Associa- tion for Computational Linguistics, Virtual 2020. p. 1906-919.</p>
<p>Multi-reward reinforced summarization with saliency and entailment. R Pasunuru, M ; Naacl-Hlt Bansal, Melbourne, Association for Computational Linguistics)Pasunuru R, Bansal M. Multi-reward reinforced summarization with saliency and entailment. In: NAACL-HLT. Melbourne: Association for Computational Linguistics (2); 2018. p. 646-53.</p>
<p>Guiding extractive summarization with question-answering rewards. K Arumae, F Liu, arXiv:1904.02321arXiv preprintArumae K, Liu F. Guiding extractive summarization with ques- tion-answering rewards. arXiv preprint arXiv: 1904. 02321 2019.</p>
<p>Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. L Huang, L Wu, L Wang, ACL. Association for Computational LinguisticsHuang L, Wu L, Wang L. Knowledge graph-augmented abstrac- tive summarization with semantic-driven cloze reward. In: ACL, Association for Computational Linguistics, Virtual; 2020. p. 5094-107.</p>
<p>Climbing towards NLU: on meaning, form, and understanding in the age of data. E M Bender, A Koller, ACL. Association for Computational Linguistics, Virtual; 2020. Bender EM, Koller A. Climbing towards NLU: on meaning, form, and understanding in the age of data. In: ACL. Associa- tion for Computational Linguistics, Virtual; 2020. p. 5185-198.</p>
<p>Textrank: bringing order into text. R Mihalcea, P Tarau, Mihalcea R, Tarau P. Textrank: bringing order into text. 2004.</p>
<p>An exploration of document impact on graph-based multi-document summarization. X Wan, Proceedings of the. theWan X. An exploration of document impact on graph-based multi-document summarization. In: Proceedings of the 2008</p>
<p>Association for Computational Linguistics. Conference on Empirical Methods in Natural Language Processing (EMNLP '08). Honolulu, HawaiiConference on Empirical Methods in Natural Language Process- ing (EMNLP '08). Honolulu, Hawaii: Association for Compu- tational Linguistics; 2008. p. 755-62.</p>
<p>Abstractive document summarization with a graph-based attentional neural model. J Tan, X Wan, J Xiao, In: ACL (1). Association for Computational LinguisticsVancouverTan J, Wan X, Xiao J. Abstractive document summarization with a graph-based attentional neural model. In: ACL (1). Van- couver: Association for Computational Linguistics; 2017. p. 1171-181.</p>
<p>Structured neural summarization. P Fernandes, M Allamanis, M Brockschmidt, ICLR (Poster). OpenReview.net. New Orleans, LouisianaFernandes P, Allamanis M, Brockschmidt M. Structured neu- ral summarization. In: ICLR (Poster). OpenReview.net, New Orleans, Louisiana; 2019.</p>
<p>A graph-to-sequence model for amr-to-text generation. L Song, Y Zhang, Z Wang, D Gildea, ACL (1). MelbourneAssociation for Computational LinguisticsSong L, Zhang Y, Wang Z, Gildea D. A graph-to-sequence model for amr-to-text generation. In: ACL (1). Melbourne: Association for Computational Linguistics; 2018. p. 1616-626.</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, NAACL-HLT (1). Florence: Association for Computational LinguisticsKoncel-Kedziorski R, Bekal D, Luan Y, Lapata M, Hajishirzi H. Text generation from knowledge graphs with graph transform- ers. In: NAACL-HLT (1). Florence: Association for Computa- tional Linguistics; 2019. p. 2284-293.</p>
<p>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. O Agarwal, H Ge, S Shakeri, Al-Rfou , R , NAACL-HLT. Association for Computational LinguisticsAgarwal O, Ge H, Shakeri S, Al-Rfou R. Knowledge graph based synthetic corpus generation for knowledge-enhanced lan- guage model pre-training. In: NAACL-HLT. Association for Computational Linguistics, Virtual; 2021. p. 3554-565.</p>
<p>Enhancing scientific papers summarization with citation graph. C An, M Zhong, Y Chen, D Wang, AAAI. AAAI PressAn C, Zhong M, Chen Y, Wang D, et al. Enhancing scientific papers summarization with citation graph. In: AAAI. AAAI Press, Virtual; 2021. p. 12498-2506.</p>
<p>Using local knowledge graph construction to scale seq2seq models to multi-document inputs. A Fan, C Gardent, C Braud, A Bordes, EMNLP/IJCNLP (1). Hong Kong. Association for Computational LinguisticsFan A, Gardent C, Braud C, Bordes A. Using local knowledge graph construction to scale seq2seq models to multi-document inputs. In: EMNLP/IJCNLP (1). Hong Kong: Association for Computational Linguistics; 2019. p. 4184-194.</p>
<p>Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward. L Huang, L Wu, L Wang, Association for Computational LinguisticsHuang L, Wu L, Wang L. Knowledge graph-augmented abstrac- tive summarization with semantic-driven cloze reward. In: ACL, Association for Computational Linguistics; 2020. p. 5094-107.</p>
<p>Enhancing factual consistency of abstractive summarization. C Zhu, W Hinthorn, R Xu, Q Zeng, M Zeng, X Huang, M Jiang, NAACL-HLT. Association for Computational LinguisticsZhu C, Hinthorn W, Xu R, Zeng Q, Zeng M, Huang X, Jiang M. Enhancing factual consistency of abstractive summarization. In: NAACL-HLT, Association for Computational Linguistics, Virtual 2021. p. 718-733.</p>
<p>SKGSUM: abstractive document summarization with semantic knowledge graphs. X Ji, W Zhao, In: IJCNN. Shenzhen: IEEE. Ji X, Zhao W. SKGSUM: abstractive document summarization with semantic knowledge graphs. In: IJCNN. Shenzhen: IEEE; 2021. p. 1-8.</p>
<p>Leveraging linguistic structure for open domain information extraction. G Angeli, Mjj Premkumar, C D Manning, ACL (1). Beijing: The Association for Computer LinguisticsAngeli G, Premkumar MJJ, Manning CD. Leveraging linguis- tic structure for open domain information extraction. In: ACL (1). Beijing: The Association for Computer Linguistics; 2015. p. 344-54.</p>
<p>The stanford corenlp natural language processing toolkit. C D Manning, M Surdeanu, J Bauer, J R Finkel, ACL (System Demonstrations). Baltimore: The Association for Computer LinguisticsManning CD, Surdeanu M, Bauer J, Finkel JR, et al. The stanford corenlp natural language processing toolkit. In: ACL (System Demonstrations). Baltimore: The Association for Computer Lin- guistics; 2014. p. 55-60.</p>
<p>Overview of bionlp'09 shared task on event extraction. J Kim, T Ohta, S Pyysalo, Y Kano, BioNLP@HLT-NAACL (Shared Task). Boulder: Association for Computational LinguisticsKim J, Ohta T, Pyysalo S, Kano Y, et al. Overview of bionlp'09 shared task on event extraction. In: BioNLP@HLT-NAACL (Shared Task). Boulder: Association for Computational Linguis- tics; 2009. p. 1-9.</p>
<p>Overview of bionlp shared task. J Kim, S Pyysalo, T Ohta, R Bossy, Nlt Nguyen, J Tsujii, BioNLP@ACL (Shared Task). PortlandAssociation for Computational LinguisticsKim J, Pyysalo S, Ohta T, Bossy R, Nguyen NLT, Tsujii J. Over- view of bionlp shared task 2011. In: BioNLP@ACL (Shared Task). Portland: Association for Computational Linguistics; 2011. p. 1-6.</p>
<p>C Ndellec, R Bossy, J Kim, J Kim, T Ohta, S Pyysalo, P Zweigenbaum, Overview of bionlp shared task 2013. In: BioNLP@ACL (Shared Task). SofiaAssociation for Computational LinguisticsNdellec C, Bossy R, Kim J, Kim J, Ohta T, Pyysalo S, Zweigen- baum P. Overview of bionlp shared task 2013. In: BioNLP@ACL (Shared Task). Sofia: Association for Computational Linguistics; 2013. p. 1-7.</p>
<p>Corpus annotation for mining biomedical events from literature. J Kim, T Ohta, J Tsujii, 10.1186/1471-2105-9-10BMC Bioinform. Kim J, Ohta T, Tsujii J. Corpus annotation for mining biomedical events from literature. BMC Bioinform. 2008. https:// doi. org/ 10. 1186/ 1471-2105-9-10.</p>
<p>Deepeventmine: end-to-end neural nested event extraction from biomedical texts. H Trieu, T T Tran, A D Nguyen, A Nguyen, 10.1093/bioinformatics/btaa540Bioinformatics. 3619Trieu H, Tran TT, Nguyen AD, Nguyen A, et al. Deepeventmine: end-to-end neural nested event extraction from biomedical texts. Bioinformatics. 2020;36(19):4910-7. https:// doi. org/ 10. 1093/ bioin forma tics/ btaa5 40.</p>
<p>Scibert: a pretrained language model for scientific text. I Beltagy, K Lo, A Cohan, arXiv:1903.10676arXiv preprintBeltagy I, Lo K, Cohan A. Scibert: a pretrained language model for scientific text. arXiv preprint arXiv: 1903. 10676 2019.</p>
<p>Unsupervised event graph representation and similarity learning on biomedical literature. G Frisoni, G Moro, G Carlassare, A Carbonaro, 10.3390/s22010003Sensors. 221Frisoni G, Moro G, Carlassare G, Carbonaro A. Unsupervised event graph representation and similarity learning on biomedical literature. Sensors. 2022;22(1):3. https:// doi. org/ 10. 3390/ s2201 0003.</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, NAACL-HLT (1). Minneapolis: Association for Computational LinguisticsDevlin J, Chang M, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT (1). Minneapolis: Association for Computational Linguistics; 2019. p. 4171-4186.</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: a robustly optimized BERT pretraining approach. CoRR. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. Roberta: a robustly optimized BERT pretraining approach. CoRR. arXiv: 1907. 11692 2019.</p>
<p>Edge-featured graph attention network. J Chen, H Chen, arXiv:2101.076712021arXiv preprintChen J, Chen H. Edge-featured graph attention network. arXiv preprint arXiv: 2101. 07671 2021.</p>
<p>Modeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, Berg Rvd, I Titov, M Welling, European semantic web conference. Springer. Schlichtkrull M, Kipf TN, Bloem P, Berg Rvd, Titov I, Welling M. Modeling relational data with graph convolutional networks. In: European semantic web conference. Springer; 2018. p. 593-607</p>
<p>Finite geometrical systems. F W Levi, Levi FW. Finite geometrical systems. 1942.</p>
<p>Graph-to-sequence learning using gated graph neural networks. D Beck, G Haffari, T Cohn, arXiv:1806.09835arXiv preprintBeck D, Haffari G, Cohn T. Graph-to-sequence learning using gated graph neural networks. arXiv preprint arXiv: 1806. 09835 2018.</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, arXiv:1904.02342arXiv preprintKoncel-Kedziorski R, Bekal D, Luan Y, Lapata M, et al. Text generation from knowledge graphs with graph transformers. arXiv preprint arXiv: 1904. 02342 2019.</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.04733rd International conference on learning representations, ICLR 2015. Bengio Y, LeCun YSan Diego, CA, USAConference Track ProceedingsBahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. In: Bengio Y, LeCun Y, editors. 3rd International conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. arXiv: 1409. 0473 2015.</p>
<p>Fast graph representation learning with pytorch geometric. M Fey, J E Lenssen, arXiv:1903.02428CoRR.Fey M, Lenssen JE. Fast graph representation learning with pytorch geometric. CoRR. arXiv: 1903. 02428 2019.</p>
<p>Event extraction across multiple levels of biological organization. S Pyysalo, T Ohta, M Miwa, H Cho, Bioinformatics. 2818Pyysalo S, Ohta T, Miwa M, Cho H, et al. Event extraction across multiple levels of biological organization. Bioinformatics. 2012;28(18):575-81.</p>
<p>Get to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, ACL (1). Vancouver: Association for Computational LinguisticsSee A, Liu PJ, Manning CD. Get to the point: Summarization with pointer-generator networks. In: ACL (1). Vancouver: Association for Computational Linguistics; 2017 p. 1073-083.</p>
<p>ROUGE: a package for automatic evaluation of summaries. C-Y Lin, Text summarization branches out. BarcelonaAssociation for Computational LinguisticsLin C-Y. ROUGE: a package for automatic evaluation of summa- ries. In: Text summarization branches out. Barcelona: Association for Computational Linguistics; 2004. p. 74-81. https:// aclan tholo gy. org/ W04-1013</p>
<p>Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. J P Kincaid, R P Fishburne, R L Rogers, B S Chissom, Kincaid JP, Fishburne RP, Rogers RL, Chissom BS. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. 1975.</p>
<p>Technique of clear writing. R Gunning, Gunning R.e.a. Technique of clear writing. 1952</p>
<p>A computer readability formula designed for machine scoring. M Coleman, T L Liau, J Appl Psychol. 60Coleman M, Liau TL. A computer readability formula designed for machine scoring. J Appl Psychol. 1975;60:283-4.</p>
<p>NLG-metricverse: an end-to-end library for evaluating natural language generation. G Frisoni, A Carbonaro, G Moro, A Zammarchi, M Avagnano, Proceedings of the 29th international conference on computational linguistics. the 29th international conference on computational linguisticsinternational committee on computational linguistics. Gyeongju; 2022. pFrisoni G, Carbonaro A, Moro G, Zammarchi A, Avagnano M. NLG-metricverse: an end-to-end library for evaluating natural language generation. In: Proceedings of the 29th international conference on computational linguistics, international commit- tee on computational linguistics. Gyeongju; 2022. p. 3465-479. https:// aclan tholo gy. org/ 2022. coling-1. 306</p>
<p>Biobart: pretraining and evaluation of a biomedical generative language model. H Yuan, Z Yuan, R Gan, J Zhang, Y Xie, S Yu, BioNLP@ACL. Dublin: Association for Computational Linguistics. Yuan H, Yuan Z, Gan R, Zhang J, Xie Y, Yu S: Biobart: pretrain- ing and evaluation of a biomedical generative language model. In: BioNLP@ACL. Dublin: Association for Computational Linguis- tics; 2022. p. 97-109.</p>
<p>CodeCarbon: estimate and track carbon emissions from machine learning computing. V Schmidt, K Goyal, A Joshi, B Feld, 10.5281/zenodo.46584244658424Schmidt V, Goyal K, Joshi A, Feld B, et al. CodeCarbon: estimate and track carbon emissions from machine learning computing. 2021. https:// doi. org/ 10. 5281/ zenodo. 46584 24.</p>
<p>Carburacy: summarization models tuning and comparison in eco-sustainable regimes with a novel carbon-aware accuracy. G Moro, L Ragazzi, L Valgimigli, Thirty-seventh AAAI conference on artificial intelligence. AAAI 2023. Washington, DCAAAI PressMoro G, Ragazzi L, Valgimigli L. Carburacy: summarization models tuning and comparison in eco-sustainable regimes with a novel carbon-aware accuracy. In: Thirty-seventh AAAI conference on artificial intelligence. AAAI 2023. Washington, DC: AAAI Press; 2023. p. 1-9.</p>
<p>Cogito ergo summ: abstractive summarization of biomedical papers via semantic parsing graphs and consistency rewards. G Frisoni, P Italiani, S Salvatori, G Moro, AAAI. AAAI PressFrisoni G, Italiani P, Salvatori S, Moro G. Cogito ergo summ: abstractive summarization of biomedical papers via semantic parsing graphs and consistency rewards. In: AAAI. AAAI Press; 2023. p. 1-9.</p>
<p>Text-to-text extraction and verbalization of biomedical event graphs. G Frisoni, G Moro, L Balzani, Proceedings of the 29th international conference on computational linguistics, international committee on computational linguistics. the 29th international conference on computational linguistics, international committee on computational linguisticsGyeongju, Republic of KoreaFrisoni G, Moro G, Balzani L. Text-to-text extraction and ver- balization of biomedical event graphs. In: Proceedings of the 29th international conference on computational linguistics, international committee on computational linguistics, Gyeongju, Republic of Korea; 2022. p. 2692-710. https:// aclan tholo gy. org/ 2022. coling-1. 238.</p>
<p>Efficient self-supervised metric information retrieval: a bibliography based method applied to COVID literature. G Moro, L Valgimigli, 10.3390/s21196430Moro G, Valgimigli L. Efficient self-supervised metric informa- tion retrieval: a bibliography based method applied to COVID literature. Sensors. 2021. https:// doi. org/ 10. 3390/ s2119 6430.</p>
<p>Deep vision-language model for efficient multi-modal similarity search in fashion retrieval. G Moro, S Salvatori, 10.1007/978-3-031-17849-8_4SISAP 2022. Bologna, ItalySpringer13590Moro G, Salvatori S. Deep vision-language model for efficient multi-modal similarity search in fashion retrieval. In: SISAP 2022, Bologna, Italy, October 5-7, 2022, Proceedings. Lecture notes in computer science, vol. 13590. Springer; 2022. p. 40-53. https:// doi. org/ 10. 1007/ 978-3-031-17849-8_4</p>
<p>Efficient text-image semantic search: a multi-modal vision-language approach for fashion retrieval. G Moro, S Salvatori, G Frisoni, 10.1016/j.neucom.2023.03.057Neurocomputing. Moro G, Salvatori S, Frisoni G. Efficient text-image seman- tic search: a multi-modal vision-language approach for fashion retrieval. Neurocomputing. 2023. https:// doi. org/ 10. 1016/j. neu- com. 2023. 03. 057.</p>
<p>Discovering new gene functionalities from random perturbations of known gene ontological annotations. G Domeniconi, M Masseroli, G Moro, P Pinoli, 10.5220/0005087801070116INSTICC PressRomeDomeniconi G, Masseroli M, Moro G, Pinoli P. Discovering new gene functionalities from random perturbations of known gene ontological annotations. INSTICC Press, Rome; 2014. p. 107-16.</p>
<p>. 10.5220/000508780107011687801701165220% 2f000 50878 01070 116 &amp; partn erID= 40 &amp; md5= d46ef 212e9 2f6a5 b1c3d 3769c a8a05 64https:// doi. org/ 10. 5220/ 00050 87801 070116. https:// www. sco- pus. com/ inward/ record. uri? eid=2-s2.0-84909 95733 2 &amp; doi= 10. 5220% 2f000 50878 01070 116 &amp; partn erID= 40 &amp; md5= d46ef 212e9 2f6a5 b1c3d 3769c a8a05 64</p>
<p>Gene function finding through cross-organism ensemble learning. G Moro, M Masseroli, 10.1186/s13040-021-00239-wBioData Min. 14114Moro G, Masseroli M. Gene function finding through cross-organ- ism ensemble learning. BioData Min. 2021;14(1):14. https:// doi. org/ 10. 1186/ s13040-021-00239-w.</p>
<p>Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Y Chen, L Wu, M J Zaki, NeurIPS. 2020Chen Y, Wu L, Zaki MJ. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. In: NeurIPS. 2020.</p>
<p>A comparison of term weighting schemes for text classification and sentiment analysis with a supervised variant of tf.idf. G Domeniconi, G Moro, R Pasolini, C Sartori, 10.1007/978-3-319-30162-4_4DATA (Revised Selected Papers). 584SpringerDomeniconi G, Moro G, Pasolini R, Sartori C. A comparison of term weighting schemes for text classification and sentiment analysis with a supervised variant of tf.idf. In: DATA (Revised Selected Papers), vol. 584. Cham: Springer; 2015. p. 39-58.</p>
<p>. 10.1007/978-3-319-30162-4_4eid=2-s2.0-84961 12720 6 &amp; doi= 10. 1007% 2f978-3-319-30162-4_ 4 &amp; partn erID= 40md5= 81e9a 8dc20 45e11 86bf8 40b7e 43e31 18https:// doi. org/ 10. 1007/ 978-3-319-30162-4_4. https:// www. sco- pus. com/ inward/ record. uri? eid=2-s2.0-84961 12720 6 &amp; doi= 10. 1007% 2f978-3-319-30162-4_ 4 &amp; partn erID= 40 &amp; md5= 81e9a 8dc20 45e11 86bf8 40b7e 43e31 18</p>
<p>Iterative refining of category profiles for nearest centroid cross-domain text classification. G Domeniconi, G Moro, R Pasolini, C Sartori, 10.1007/978-3-319-25840-9_4Revised Selected Papers. Rome, Italy; RomeSpringer553IC3K 2014Domeniconi G, Moro G, Pasolini R, Sartori C. Iterative refining of category profiles for nearest centroid cross-domain text classifica- tion. In: IC3K 2014, Rome, Italy, October 21-24, 2014, Revised Selected Papers, vol. 553. Springer, Rome; 2014. p. 50-67. https:// doi. org/ 10. 1007/ 978-3-319-25840-9_4</p>
<p>Cross-domain and in-domain sentiment analysis with memory-based deep neural networks. G Moro, A Pagliarani, R Pasolini, C Sartori, IC3K 2018. SevilleSciTePressMoro G, Pagliarani A, Pasolini R, Sartori C. Cross-domain and in-domain sentiment analysis with memory-based deep neural networks. In: IC3K 2018. Seville: SciTePress; 2018. p. 127-38.</p>
<p>. 10.5220/0007239101270138eid=2-s2.0-85059 00037 0 &amp; doi= 105220% 2f000 72391 01270 138 &amp; partn erID= 40 &amp; md5= 257a0 4cbdf 98a4d 75275 d3956 3b0aa 17https:// doi. org/ 10. 5220/ 00072 39101 270138. https:// www. sco- pus. com/ inward/ record. uri? eid=2-s2.0-85059 00037 0 &amp; doi= 10. 5220% 2f000 72391 01270 138 &amp; partn erID= 40 &amp; md5= 257a0 4cbdf 98a4d 75275 d3956 3b0aa 17</p>
<p>Comprehensive analysis of knowledge graph embedding techniques benchmarked on link prediction. I Ferrari, G Frisoni, P Italiani, G Moro, C Sartori, 10.3390/electronics11233866Ferrari I, Frisoni G, Italiani P, Moro G, Sartori C. Comprehensive analysis of knowledge graph embedding techniques benchmarked on link prediction. Electronics. 2022. https:// doi. org/ 10. 3390/ elect ronic s1123 3866.</p>
<p>BioReader: a retrieval-enhanced text-to-text transformer for biomedical literature. G Frisoni, M Mizutani, G Moro, L Valgimigli, Proceedings of the 2022 conference on empirical methods in natural language processing. the 2022 conference on empirical methods in natural language processingAbu DhabiAssociation for Computational LinguisticsFrisoni G, Mizutani M, Moro G, Valgimigli L. BioReader: a retrieval-enhanced text-to-text transformer for biomedical litera- ture. In: Proceedings of the 2022 conference on empirical meth- ods in natural language processing. Abu Dhabi: Association for Computational Linguistics. 2022. p. 5770-793. https:// aclan tholo gy. org/ 2022. emnlp-main. 390</p>
<p>Overview of the cancer genetics (cg) task of bionlp shared task. S Pyysalo, T Ohta, S Ananiadou, Proceedings of the BioNLP Shared Task 2013 Workshop. the BioNLP Shared Task 2013 WorkshopPyysalo S, Ohta T, Ananiadou S. Overview of the cancer genet- ics (cg) task of bionlp shared task 2013. In: Proceedings of the BioNLP Shared Task 2013 Workshop, 2013. p. 58-66.</p>
<p>The Genia event extraction shared task, 2013 edition-overview. J-D Kim, Y Wang, Y Yasunori, Proceedings of the BioNLP Shared Task 2013 Workshop. Sofia: Association for Computational Linguistics. the BioNLP Shared Task 2013 Workshop. Sofia: Association for Computational LinguisticsKim J-D, Wang Y, Yasunori Y. The Genia event extraction shared task, 2013 edition-overview. In: Proceedings of the BioNLP Shared Task 2013 Workshop. Sofia: Association for Compu- tational Linguistics; 2013. p. 8-15. https:// aclan tholo gy. org/ W13-2002</p>            </div>
        </div>

    </div>
</body>
</html>
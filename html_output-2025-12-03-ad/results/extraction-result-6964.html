<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6964 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6964</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6964</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-260379133</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01285v3.pdf" target="_blank">Flows: Building Blocks of Reasoning and Collaborating AI</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework Flows. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design simplifies the process of creating Flows by allowing them to be recursively composed into arbitrarily nested interactions and is inherently concurrency-friendly. Crucially, any interaction can be implemented using this framework, including prior work on AI-AI and human-AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on competitive coding, a challenging task on which even GPT-4 struggles. Our results suggest that structured reasoning and collaboration substantially improve generalization, with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in terms of solve rate. To support rapid and rigorous research, we introduce the aiFlows library embodying Flows. The aiFlows library is available at https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our experiments are available at https://github.com/epfl-dlab/cc_flows.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6964.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6964.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection Flow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection Flow (fixed-reply)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flow that provides a fixed, canned feedback message to the generator encouraging it to reflect on the proposed solution and produce a corrected version if necessary; implemented as a Generator-Critic Flow with a fixed reply template.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model by OpenAI, used via API as the code generation tool in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflection Flow (fixed-reply)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator produces a solution, then a fixed critic message prompts the same (or another) LLM to reflect on whether the solution is correct and to provide a corrected version if not; implemented with a fixed reply template (Listing 2).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Competitive coding (Codeforces, LeetCode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a natural-language problem statement and example I/O pairs, generate code that passes hidden test cases (competitive programming).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (solve rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Code baseline (pass@1) — Codeforces pre-cutoff: 71.8% (solve rate); Codeforces post-cutoff: 26.9% (solve rate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Code_Reflection (point estimate): Codeforces pre-cutoff ≈ 81.1% (71.8 + 9.3); Codeforces post-cutoff ≈ 26.9% (26.9 + 0.0).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are limited and not generally statistically significant; observed small-to-moderate gain pre-cutoff (+9.3) and no gain post-cutoff. The paper reports that Code_Reflection yielded limited improvements overall and results are not robust across datasets/conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6964.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Collaboration Flow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaboration Flow (AI-generated critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flow where feedback is provided by an AI critic that evaluates the generator's proposed solution and returns textual feedback used to refine the generator's output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model by OpenAI, used both as generator and (in some runs) as critic in the collaboration setting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Collaboration Flow (AI critic)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator produces code; an AI critic Flow (another LLM) evaluates the proposed solution and returns feedback; the generator then uses that feedback to revise the solution (Generator–Critic pattern / Gen‑Crit).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (generator + external AI critic)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Competitive coding (Codeforces, LeetCode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation for competitive programming problems evaluated via hidden test cases or local tests.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (solve rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Code_Collaboration (point estimate): Codeforces pre-cutoff ≈ 76.6% (71.8 + 4.8); Codeforces post-cutoff ≈ 36.5% (26.9 + 9.6).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains are limited and often not statistically significant; collaboration without grounding produced inconsistent improvements (small deltas in many buckets). The paper notes that ungrounded AI feedback is less effective than grounded (test-based) feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6964.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debug Flow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Debug Flow (test-result feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flow that runs generated code on available example tests and returns formatted execution/test results as feedback (no code provided by the critic), which the generator uses to refine the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model by OpenAI, used to generate code and to consume debug output as part of the refinement loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Debug Flow</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator produces code, a testing Flow executes the code on example I/O and returns structured debug output (compilation error, runtime error, failed test details), and the generator then uses these grounded test results to revise the code.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (grounded by test execution)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Competitive coding (Codeforces, LeetCode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation evaluated by running code on provided examples and by submitting to online judges (hidden tests).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (solve rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Code_Debug (point estimate): Codeforces pre-cutoff ≈ 84.5% (71.8 + 12.7); Codeforces post-cutoff ≈ 34.8% (26.9 + 7.9).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Grounded debugging yields more consistent improvements than ungrounded feedback, but effectiveness varies by dataset; LeetCode gains were smaller (paper hypothesizes example tests on LeetCode are simpler, leading to false positives and lower grounding quality). Number of iterations and exact refinement loop depth were not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6964.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debug-Collab Flow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Debug-Collaboration Flow (debug + AI critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Flow combining test-grounded feedback with an AI critic that has access to detailed test execution results and produces grounded, actionable feedback or corrected code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer-based language model by OpenAI, used as generator and critic; critic is provided with execution/test outputs to ground its feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Debug-Collab (grounded AI critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Testing Flow runs code and formats execution results; an AI critic Flow consumes these grounded test results and returns grounded feedback or corrected code which the generator uses to refine the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (grounded, critic-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Competitive coding (Codeforces, LeetCode)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation for competitive programming problems, evaluated by hidden testcases and local test runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>pass@1 (solve rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Code_Debug_Collab (point estimate): Codeforces pre-cutoff ≈ 84.4% (71.8 + 12.6); Codeforces post-cutoff ≈ 47.5% (26.9 + 20.6).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This grounded collaboration produced the largest and statistically significant improvements on novel (post-cutoff) Codeforces problems (e.g., 26.9→47.5). Limitations include smaller gains on LeetCode (attributed to weaker grounding due to simpler example tests causing false positives). The paper notes that effectiveness depends on feedback quality and grounding; exact number of refinement cycles unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6964.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed framework where an LLM generates an output, then the same LLM generates feedback on its own output and iteratively refines it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative generate–critique–revise loop where the generator also acts as its own critic to produce feedback for self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>recursive self-critique / generate-and-refine</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; paper does not report details or experiments for this method. Related work has mixed findings; this paper cites the method but does not evaluate it directly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6964.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursively Criticize and Improve (RCI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced method where an agent recursively criticizes and improves outputs, sometimes using separate models for generation and criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recursively Criticize and Improve</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Recursively Criticize and Improve (RCI)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A scheme that uses one model (or agent) to generate an output and another to criticize it, then iterates improvements based on criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-critic (recursive)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; not evaluated in this paper. The paper notes distinctions between approaches that use single vs multiple models for critique.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6964.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (self-reflective memory of mistakes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior framework that provides free-form reflection and builds a persistent memory of self-reflective experiences to identify recurring errors and lessons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents provide free-form reflection on whether a step was executed correctly and persist lessons in memory to improve future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>iterative self-reflection with persistent memory</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; not experimentally evaluated in the present paper. Reflexion differs by storing persistent reflective experiences which this paper does not implement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6964.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previous approach that leverages external execution tools (interpreters) to let models debug and revise code using execution traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Debug</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use of external execution environments (Python interpreter, SQL engine) to produce concrete debugging signals that the model uses to iteratively revise code.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-run-debug-revise (grounded debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; similar in spirit to the Debug and Debug-Collab Flows used in this paper, which the authors found to be among the most effective strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6964.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6964.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REFINER (reasoning feedback on intermediate representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced framework in which models generate intermediate representations and interact with a critic that provides automated feedback on those representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Refiner: Reasoning feedback on intermediate representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>REFINER</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generator produces intermediate representations; a critic analyzes them and provides feedback used to improve final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-and-critic (on intermediate representations)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned in related work; not directly evaluated in this paper. The authors list it among Gen‑Crit style approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Flows: Building Blocks of Reasoning and Collaborating AI', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Recursively Criticize and Improve <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Refiner: Reasoning feedback on intermediate representations <em>(Rating: 2)</em></li>
                <li>Self-Correct <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6964",
    "paper_id": "paper-260379133",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Reflection Flow",
            "name_full": "Reflection Flow (fixed-reply)",
            "brief_description": "A Flow that provides a fixed, canned feedback message to the generator encouraging it to reflect on the proposed solution and produce a corrected version if necessary; implemented as a Generator-Critic Flow with a fixed reply template.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model by OpenAI, used via API as the code generation tool in experiments.",
            "model_size": null,
            "reflection_method_name": "Reflection Flow (fixed-reply)",
            "reflection_method_description": "Generator produces a solution, then a fixed critic message prompts the same (or another) LLM to reflect on whether the solution is correct and to provide a corrected version if not; implemented with a fixed reply template (Listing 2).",
            "iteration_type": "generate-then-reflect",
            "num_iterations": null,
            "task_name": "Competitive coding (Codeforces, LeetCode)",
            "task_description": "Given a natural-language problem statement and example I/O pairs, generate code that passes hidden test cases (competitive programming).",
            "evaluation_metric": "pass@1 (solve rate)",
            "performance_before_reflection": "Code baseline (pass@1) — Codeforces pre-cutoff: 71.8% (solve rate); Codeforces post-cutoff: 26.9% (solve rate).",
            "performance_after_reflection": "Code_Reflection (point estimate): Codeforces pre-cutoff ≈ 81.1% (71.8 + 9.3); Codeforces post-cutoff ≈ 26.9% (26.9 + 0.0).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements are limited and not generally statistically significant; observed small-to-moderate gain pre-cutoff (+9.3) and no gain post-cutoff. The paper reports that Code_Reflection yielded limited improvements overall and results are not robust across datasets/conditions.",
            "uuid": "e6964.0",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Collaboration Flow",
            "name_full": "Collaboration Flow (AI-generated critic)",
            "brief_description": "A Flow where feedback is provided by an AI critic that evaluates the generator's proposed solution and returns textual feedback used to refine the generator's output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model by OpenAI, used both as generator and (in some runs) as critic in the collaboration setting.",
            "model_size": null,
            "reflection_method_name": "Collaboration Flow (AI critic)",
            "reflection_method_description": "Generator produces code; an AI critic Flow (another LLM) evaluates the proposed solution and returns feedback; the generator then uses that feedback to revise the solution (Generator–Critic pattern / Gen‑Crit).",
            "iteration_type": "generate-then-reflect (generator + external AI critic)",
            "num_iterations": null,
            "task_name": "Competitive coding (Codeforces, LeetCode)",
            "task_description": "Code generation for competitive programming problems evaluated via hidden test cases or local tests.",
            "evaluation_metric": "pass@1 (solve rate)",
            "performance_before_reflection": "Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.",
            "performance_after_reflection": "Code_Collaboration (point estimate): Codeforces pre-cutoff ≈ 76.6% (71.8 + 4.8); Codeforces post-cutoff ≈ 36.5% (26.9 + 9.6).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Gains are limited and often not statistically significant; collaboration without grounding produced inconsistent improvements (small deltas in many buckets). The paper notes that ungrounded AI feedback is less effective than grounded (test-based) feedback.",
            "uuid": "e6964.1",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Debug Flow",
            "name_full": "Debug Flow (test-result feedback)",
            "brief_description": "A Flow that runs generated code on available example tests and returns formatted execution/test results as feedback (no code provided by the critic), which the generator uses to refine the solution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model by OpenAI, used to generate code and to consume debug output as part of the refinement loop.",
            "model_size": null,
            "reflection_method_name": "Debug Flow",
            "reflection_method_description": "Generator produces code, a testing Flow executes the code on example I/O and returns structured debug output (compilation error, runtime error, failed test details), and the generator then uses these grounded test results to revise the code.",
            "iteration_type": "generate-then-reflect (grounded by test execution)",
            "num_iterations": null,
            "task_name": "Competitive coding (Codeforces, LeetCode)",
            "task_description": "Code generation evaluated by running code on provided examples and by submitting to online judges (hidden tests).",
            "evaluation_metric": "pass@1 (solve rate)",
            "performance_before_reflection": "Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.",
            "performance_after_reflection": "Code_Debug (point estimate): Codeforces pre-cutoff ≈ 84.5% (71.8 + 12.7); Codeforces post-cutoff ≈ 34.8% (26.9 + 7.9).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Grounded debugging yields more consistent improvements than ungrounded feedback, but effectiveness varies by dataset; LeetCode gains were smaller (paper hypothesizes example tests on LeetCode are simpler, leading to false positives and lower grounding quality). Number of iterations and exact refinement loop depth were not specified.",
            "uuid": "e6964.2",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Debug-Collab Flow",
            "name_full": "Debug-Collaboration Flow (debug + AI critic)",
            "brief_description": "A Flow combining test-grounded feedback with an AI critic that has access to detailed test execution results and produces grounded, actionable feedback or corrected code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pre-trained transformer-based language model by OpenAI, used as generator and critic; critic is provided with execution/test outputs to ground its feedback.",
            "model_size": null,
            "reflection_method_name": "Debug-Collab (grounded AI critique)",
            "reflection_method_description": "Testing Flow runs code and formats execution results; an AI critic Flow consumes these grounded test results and returns grounded feedback or corrected code which the generator uses to refine the solution.",
            "iteration_type": "generate-then-reflect (grounded, critic-in-the-loop)",
            "num_iterations": null,
            "task_name": "Competitive coding (Codeforces, LeetCode)",
            "task_description": "Code generation for competitive programming problems, evaluated by hidden testcases and local test runs.",
            "evaluation_metric": "pass@1 (solve rate)",
            "performance_before_reflection": "Code baseline — Codeforces pre-cutoff: 71.8%; Codeforces post-cutoff: 26.9%.",
            "performance_after_reflection": "Code_Debug_Collab (point estimate): Codeforces pre-cutoff ≈ 84.4% (71.8 + 12.6); Codeforces post-cutoff ≈ 47.5% (26.9 + 20.6).",
            "improvement_observed": true,
            "limitations_or_failure_cases": "This grounded collaboration produced the largest and statistically significant improvements on novel (post-cutoff) Codeforces problems (e.g., 26.9→47.5). Limitations include smaller gains on LeetCode (attributed to weaker grounding due to simpler example tests causing false positives). The paper notes that effectiveness depends on feedback quality and grounding; exact number of refinement cycles unspecified.",
            "uuid": "e6964.3",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "A previously proposed framework where an LLM generates an output, then the same LLM generates feedback on its own output and iteratively refines it.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Refine",
            "reflection_method_description": "Iterative generate–critique–revise loop where the generator also acts as its own critic to produce feedback for self-improvement.",
            "iteration_type": "recursive self-critique / generate-and-refine",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; paper does not report details or experiments for this method. Related work has mixed findings; this paper cites the method but does not evaluate it directly.",
            "uuid": "e6964.4",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RCI",
            "name_full": "Recursively Criticize and Improve (RCI)",
            "brief_description": "A referenced method where an agent recursively criticizes and improves outputs, sometimes using separate models for generation and criticism.",
            "citation_title": "Recursively Criticize and Improve",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Recursively Criticize and Improve (RCI)",
            "reflection_method_description": "A scheme that uses one model (or agent) to generate an output and another to criticize it, then iterates improvements based on criticism.",
            "iteration_type": "generate-then-critic (recursive)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; not evaluated in this paper. The paper notes distinctions between approaches that use single vs multiple models for critique.",
            "uuid": "e6964.5",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (self-reflective memory of mistakes)",
            "brief_description": "A prior framework that provides free-form reflection and builds a persistent memory of self-reflective experiences to identify recurring errors and lessons.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Reflexion",
            "reflection_method_description": "Agents provide free-form reflection on whether a step was executed correctly and persist lessons in memory to improve future behavior.",
            "iteration_type": "iterative self-reflection with persistent memory",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; not experimentally evaluated in the present paper. Reflexion differs by storing persistent reflective experiences which this paper does not implement.",
            "uuid": "e6964.6",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-Debug",
            "name_full": "Teaching large language models to self-debug",
            "brief_description": "A previous approach that leverages external execution tools (interpreters) to let models debug and revise code using execution traces.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "Self-Debug",
            "reflection_method_description": "Use of external execution environments (Python interpreter, SQL engine) to produce concrete debugging signals that the model uses to iteratively revise code.",
            "iteration_type": "generate-run-debug-revise (grounded debugging)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; similar in spirit to the Debug and Debug-Collab Flows used in this paper, which the authors found to be among the most effective strategies.",
            "uuid": "e6964.7",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "REFINER",
            "name_full": "REFINER (reasoning feedback on intermediate representations)",
            "brief_description": "A referenced framework in which models generate intermediate representations and interact with a critic that provides automated feedback on those representations.",
            "citation_title": "Refiner: Reasoning feedback on intermediate representations",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reflection_method_name": "REFINER",
            "reflection_method_description": "Generator produces intermediate representations; a critic analyzes them and provides feedback used to improve final outputs.",
            "iteration_type": "generate-and-critic (on intermediate representations)",
            "num_iterations": null,
            "task_name": null,
            "task_description": null,
            "evaluation_metric": null,
            "performance_before_reflection": null,
            "performance_after_reflection": null,
            "improvement_observed": null,
            "limitations_or_failure_cases": "Mentioned in related work; not directly evaluated in this paper. The authors list it among Gen‑Crit style approaches.",
            "uuid": "e6964.8",
            "source_info": {
                "paper_title": "Flows: Building Blocks of Reasoning and Collaborating AI",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Recursively Criticize and Improve",
            "rating": 2,
            "sanitized_title": "recursively_criticize_and_improve"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Refiner: Reasoning feedback on intermediate representations",
            "rating": 2,
            "sanitized_title": "refiner_reasoning_feedback_on_intermediate_representations"
        },
        {
            "paper_title": "Self-Correct",
            "rating": 1,
            "sanitized_title": "selfcorrect"
        }
    ],
    "cost": 0.018620249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flows: Building Blocks of Reasoning and Collaborating AI
7 Feb 2024</p>
<p>Martin Josifoski 
Lars Klein 
Maxime Peyrard 
Nicolas Baldwin 
Yifei Li 
Saibo Geng 
Julian Paul Schnitzler 
Yuxing Yao 
Jiheng Wei 
Debjit Paul 
Robert West 
Flows: Building Blocks of Reasoning and Collaborating AI
7 Feb 2024384AB238A622887C476ED0AF9D424372arXiv:2308.01285v3[cs.AI]
Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems.This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans.To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions.For this purpose, we introduce the conceptual framework Flows.Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface.This modular design simplifies the process of creating Flows by allowing them to be recursively composed into arbitrarily nested interactions and is inherently concurrency-friendly. Crucially, any interaction can be implemented using this framework, including prior work on AI-AI and human-AI interactions, prompt engineering schemes, and tool augmentation.We demonstrate the potential of Flows on competitive coding, a challenging task on which even GPT-4 struggles.Our results suggest that structured reasoning and collaboration substantially improve generalization, with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in terms of solve rate.To support rapid and rigorous research, we introduce the aiFlows library embodying Flows.</p>
<p>Introduction</p>
<p>The success of large language models (LLMs) largely lies in their remarkable emergent ability to adapt to informa-tion within their context (i.e., prompt) (Brown et al., 2020;Wei et al., 2022;Kojima et al., 2022).By strategically crafting the context, LLMs can be conditioned to perform complex reasoning (Wei et al., 2022;Nye et al., 2021) and effectively utilize external tools (Schick et al., 2023), significantly enhancing their capabilities.Some of the most exciting recent developments involve defining control flows, wherein LLMs, with the ability to control a set of tools, are called in an orchestrated fashion to solve increasingly complex tasks.Examples of such control flows include ReAct (Yao et al., 2023b), AutoGPT (Richards, 2023), BabyAGI (Nakajima, 2023), PromptBreeder (Fernando et al., 2023) and FunSearch (Romera-Paredes et al., 2023).Even the ubiquitous ChatGPT (OpenAI, 2023b) application is an instance of a control flow built around the GPT-3.5 and GPT-4 models (Brown et al., 2020;OpenAI, 2023a).However, these represent but a few of the many conceivable control flows, offering only a glimpse into the vast potential of structured LLM interactions.To realize this potential, we need to develop ways to study such interactions systematically.</p>
<p>In software engineering, simple processes can be implemented in an unstructured fashion, perhaps in a single file.However, as the size and complexity of the systems increase, choosing the right abstractions and architecture becomes critical (Garlan &amp; Shaw, 1993).Currently, for structured LLM interactions we want to model, implement, and study, we are at a point where this become unwieldy.Yet, no general efficient abstraction exists for effectively modeling arbitrarily complex structured interactions.Previous work and existing frameworks, such as LangChain (Chase, 2022), Chameleon (Lu et al., 2023), and HuggingGPT (Shen et al., 2023), have converged to an ad-hoc abstraction that models agents as entities that use LLMs to select and execute actions towards specific tasks, where the set of possible actions is pre-defined by the available tools.In this view, tools serve a narrow, well-defined goal and can perform sophisticated tasks (e.g., querying a search engine or executing code).However, their behavior is limited to a single interaction.To highlight the implications of this limitation, consider the following scenario: Alice wants to apply for a job at HappyCorp.If Alice is an agent, she would need to explicitly plan the entire process, including preparing the application, sending it, and evaluating it, which may involve The fifth column outlines the structure of a hypothetical Flow, defining a meta-reasoning process that could support autonomous behavior.2a background check, organizing interviews, and more.Alice would need the knowledge and the "computational" ability to account for every detail, including unforeseen events that may arise (e.g., the interviewer being on parental leave), and require her to adapt.In reality, most of the complexity is hidden from Alice behind an interface to HappyCorp's hiring process that might itself be composed of sub-processes involving many other agents and tools.Therefore, Alice is completely agnostic to the process(es) happening behind the interface and the respective logistics.On the other hand, the hiring process, carefully designed by experts, can be reused by many agents, and its sub-processes can be modified or improved with minimal or no impact on the other components beyond an updated interface.This makes it evident that agents and tools should be able to interact in complex, dynamic or static, ways as parts of nested, modular processes (that run locally or remotely), and the distinction between the two becomes blurred as they both serve as computational units in a complex computational process.</p>
<p>Starting from the observation that all processes are (control) flows defining a potentially complex interaction between many diverse components; we introduce a conceptual framework where Flows are the fundamental building blocks of computation.Flows are independent, self-contained, goaldriven entities able to complete semantically meaningful units of work.To exchange information, Flows communicate via a standardized message-based interface.The framework is depicted in Fig. 1.</p>
<p>The Flows abstraction ensures modularity.Alice, a higherlevel meta-reasoning Flow that can support autonomous behavior, does not need to know anything beyond how to interface with HappyCorp's hiring Flow.This substantially reduces complexity (Alice is interacting with a deeply nested, compositional structured interaction through a simple interface) and provides flexibility, allowing sub-Flows to be swapped without consequences as long as they have the same interface.Indeed, HappyCorp's pre-filtering Flow can be swapped from a rule-based system to an AI model or even a human Flow without affecting the structure of the overall process.The abstraction also enables reusability and the composition of sub-Flows into new Flows for different tasks.Furthermore, the framework shares key design choices with the Actor model, one of the most prominent models of concurrent computation (cf.Sec. 3).Certainly, once Alice submits her application to HappyCorp, she does not need to wait for the response; she can move to her next goal while the other Flows run concurrently.</p>
<p>We showcase the potential of the proposed framework and library by investigating complex collaborative and structured reasoning patterns on the challenging task of competitive coding, a mind sport involving participants trying to solve problems defined by a natural language description.</p>
<p>Contributions. (i)</p>
<p>We propose Flows, a conceptual framework providing an abstraction that simplifies the design and implementation of arbitrarily nested interactions while enabling concurrency.Flows can represent any interaction and provides a common framework for reasoning about interaction patterns, specifying hypotheses, and structuring research more broadly.(ii) We open-source the aiFlows library, which embodies Flows, together with FlowVerse, which is a repository of Flows that can be readily used, extended, and composed into novel, more complex Flows.(iii) We leverage Flows and the accompanying library to systematically investigate the benefits of complex interactions for solving competitive coding problems and develop AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in terms of solve rate.</p>
<p>Related Work</p>
<p>Existing libraries for modeling structured interactions.LangChain (Chase, 2022) has become the go-to library for creating applications using large language models.However, most recent works involving structured interaction, such as Cameleon (Lu et al., 2023), Camel (Li et al., 2023), HuggingGPT (Shen et al., 2023), and the concurrent works MetaGPT (Hong et al., 2023) and AutoGen (Wu et al., 2023) all come with their own library.Researchers opt to implement bespoke solutions due to the lack of a general yet efficient abstraction for modeling and designing structured interactions as well as the infrastructure to implement them, that should enable and facilitate open-ended exploration of novel ideas.In this work, we develop such an abstraction, Flows, which, in concert with aiFlows, fills this lacuna.</p>
<p>Impact of Flows.Crucially, the framework can implement any algorithm and efficiently covers all prior works on AI-AI, human-AI interactions, as well as prompt engineering (cf.Appendix A.3).These works focusing on specific Flow instantiations have demonstrated that structured interactions can yield performance gains across tasks and models.However, recent results put the universality of previously published results into question (e.g., Huang et al. (2023)) and highlight the necessity for more systematic research.To support these research efforts, we develop the theoretical and practical infrastructure for modeling, implementation, and systematic study structured interactions of arbitrary complexity.We demonstrate the benefits of the proposed infrastructure by conducting experiments that thoroughly investigate multiple core interaction patterns, including Human-AI collaboration, and their combinations, while accounting for data contamination and variance in the results, both of which are, surprisingly, not currently a standard.</p>
<p>Competitive coding (CC).</p>
<p>With the advent of transformers, Li et al. (2022) finetuned an LLM on GitHub code repositories, and a dataset scraped from Codeforces.Recently, Zelikman et al. (2022) proposed decomposing CC problems into function descriptions and, for each function description, using an LLM to generate the implementation in a modular way.While these methods yield promising results, CC remains a challenging task far from being solved (OpenAI, 2023a).As such it presents itself as an ideal test bed for thoroughly studying the benefits of collaborative and structured reasoning interactions.</p>
<p>Flows</p>
<p>This section introduces Flows as a conceptual framework, describes its benefits, and presents the aiFlows library, which embodies the framework.</p>
<p>Flows as a Conceptual Framework</p>
<p>The framework is centered around Flows and messages.Flows represent the fundamental building block of computation.They are independent, self-contained, goal-driven entities able to complete a semantically meaningful unit of work.To exchange information, Flows communicate via a standardized message-based interface.Messages can be of any type the recipient Flow can process.</p>
<p>We differentiate between two types of Flows: Atomic and Composite.3Atomic Flows complete the work directly by leveraging tools.Tools can be as simple as a textual sequence specifying a (simple) Flow's fixed response or as complex as a compiler, a search engine, powerful AI systems like LLaMA (Touvron et al., 2023a;b), Stable Diffusion (Rombach et al., 2021), and GPT-4; or even a human.Notably, in the Flows framework, AI systems correspond to tools.An Atomic Flow is effectively a minimal wrapper around a tool and achieves two things: (i) it fully specifies the tool (e.g., the most basic Atomic Flow around GPT-4 would specify the prompts and the generation parameters); and (ii) it abstracts the complexity of the internal computation by exposing only a standard message-based interface for exchanging information with other Flows.Examples of Atomic Flows include wrappers around chain-of-thought prompted GPT-4 for solving math reasoning problems, fewshot prompted LLaMA for question answering, an existing chatbot, a search engine API, or an interface with a human.</p>
<p>Composite Flows accomplish more challenging, higherlevel goals by leveraging and coordinating other Flows.Crucially, thanks to their local state and standardized interface, Composite Flows can readily invoke Atomic Flows or other Composite Flows as part of compositional, structured interactions of arbitrary complexity.Enabling research on effective patterns of interaction is one of the main goals of our work.General examples of such patterns include (i) factorizing the problem into simpler problems (i.e., divide and conquer); (ii) evaluating (sub-)solutions at inference time (i.e., feedback); and (iii) incorporating external infor-mation or a tool.Importantly, Flows can readily invoke other, potentially heavily optimized, specialized Flows to complete specific (sub-)tasks as part of an interaction, leading to complicated behavior.One example of a Composite Flow is ReAct (Yao et al., 2023b).ReAct is a sequential Flow that structures the problem-solving procedure in two steps: a Flow selects the next action out of a predefined set of actions, and another Flow executes it.The two steps are performed until an answer is obtained.Another prominent example, AutoGPT, extends the ReAct Flow with a Memory Flow and an optional Human Feedback Flow.More generally, our framework provides a unified view of prior work, which we make explicit in Appendix A.3.</p>
<p>Importantly, as illustrated in Fig. 1, Composite Flows can script an arbitrarily complex pattern (i) precisely specifying an interaction (e.g., generate code, execute tests, brainstorm potential reasons for failure, etc.); or (ii) defining a highlevel, meta-reasoning process in which a Flow could bring about dynamic unconstrained interactions.</p>
<p>Key properties.The proposed framework is characterized by the following key properties:</p>
<p>• Flows are the compositional building blocks of computation.Connection to the Actor model.Flows is fundamentally a framework modeling the computation underlying interactions.As such, it shares key design principles with the Actor model (Hewitt et al., 1973) -a mathematical model of concurrent computation.Similarly to Flows, in the Actor model, an Actor is a concurrent computation entity that can communicate with other Actors exclusively through an asynchronous message-passing interface.By encapsulating the state and the computation within individual Actors, the model provides a high-level abstraction for effectively managing and reasoning about complex concurrent and distributed systems, completely avoiding issues associated with shared states, race conditions, and deadlocks.These benefits are similar in nature to those observed in the domain of interactions.The main distinction between the proposed framework and the Actor model lies in their respective communication protocols.Concretely, while the Actor model prescribes purely asynchronous communication, Flows natively supports synchronous communication, which is essential for the implementation of structured reasoning.Interestingly, a similar deviation from the "pure" Actor model can be identified in the implementation of Erlang, a concurrent programming language based on it (Armstrong, 2003).</p>
<p>Overall, the shared design choices still make Flows inherently concurrency-friendly from the practical perspective and are sufficient for important results from the five decades of extensive studies of the Actor model, such as the fact that every physically possible computation can be directly implemented using Actors (Hewitt, 2010), to transfer to Flows.</p>
<p>Why Flows?</p>
<p>Modularity.Flows introduces a higher-level abstraction that isolates the state of individual Flows and specifies message-based communication as the only interface through which Flows can interact.This ensures perfect modularity by design.</p>
<p>Reduction of complexity.The framework ensures the complexity of the computation performed by a Flow is fully abstracted behind the universal message-based interface.This enables an intuitive and simple design of arbitrarily complex interactions from basic building blocks.</p>
<p>Systematicity, flexibility, and reusability.The separation of responsibility allows for modules to be developed and studied systematically in isolation or as part of different interactions.Once the correctness and the benefits of a Flow have been established, it can be readily used in developing novel Flows or as a drop-in replacement for less effective Flows leveraged in completing similar goals.</p>
<p>Concurrency.The proposed framework's design is consistent with the Actor model, one of the most prominent models of concurrent computation.As a consequence, Flows can readily support any setting in which Flows run concurrently.</p>
<p>The aiFlows Library</p>
<p>Accompanying Flows, we release the aiFlows library, which embodies the framework.In addition to the inherent benefits that come with the framework, the library comes with the following add-ons: (i) FlowVerse: a repository (to which anyone can contribute) of Flows that can be readily used, extended, or composed into novel, more complex Flows.Flows allows for existing "tools" (as well as "models", "chains", "agents", etc.) to be readily incorporated by wrapping them in an Atomic Flow; (ii) a detailed logging infrastructure enabling transparent debugging, analysis, and research in optimizing (i.e., learning or fine-tuning) Flows.</p>
<p>Competitive Coding Flows</p>
<p>This work investigates the potential of structured interactions for solving competitive coding (CC) problems.In CC, given a natural language description and a few input-output examples, the task is to generate code that will produce the expected output for all of the hidden input-output test cases associated with the problem.Fig. 4 provides examples.</p>
<p>We focus the analysis on three canonical dimensions of interactions: (i) problem decomposition as structured reasoning;</p>
<p>(ii) human-AI collaboration; and (iii) refinement with various feedback types.By providing a common language for clearly specifying interactions as well as the capability to flexibly compose, exchange, and extend them, the framework makes it possible to study the space of complex interactions in a principled fashion.In the rest of the section, we describe the specific Flows used in the experiments, depicted in Fig. 2.</p>
<p>Problem decomposition.Planning has been an integral intermediate step in recent work (Lu et al., 2023;Shen et al., 2023;Yao et al., 2023b).Similar decomposition is natural in the context of CC as well.In particular, we approach the task in two steps: generating a solution strategy by a Plan Flow and then generating the corresponding code by a Code Flow.This is depicted by panel A in Fig. 2.</p>
<p>Human-AI collaboration.When designing human-AI collaborations, it is essential to take the costs of human interaction into account (Horvitz, 1999;Amershi et al., 2019;Mozannar et al., 2023).By providing immense flexibility, Flows can support research in the design of interactions involving humans as computational building blocks in a way that maximizes the utility of the overall computation with a minimal human effort.In the context of CC, we hypothesize that a human can be effectively incorporated at the plan level to provide a short "oracle" plan in natural language.We operationalize this by an (Atomic) Human Flow, illustrated in Panel B of Fig. 2 as the Oracle Plan Flow.</p>
<p>Refinement with various feedback types.Iterative refinement is a general problem-solving strategy successfully deployed across various disciplines (Perrakis et al., 1999;Reid &amp; Neubig, 2022;Schick et al., 2022;Saharia et al., 2021).The strategy revolves around the idea that a solution can be gradually improved through a mechanism for analysis, modification, and re-evaluation.The design of this "feedback" mechanism is critical for the effectiveness of the problemsolving strategy.The conceptual framework, paired with the accompanying library, provides the infrastructure to support the design, implementation, and principled research of effective refinement strategies and feedback mechanisms.In this work, we consider a canonical iterative refinement setup where a generator Flow is tasked with generating the solu-tion, and a critic Flow provides feedback on the proposed solution.We consider two feedback types in the context of both the Plan and the Code Flow: (i) Reflection Flow: the feedback consists of a fixed message encouraging the model to reflect on important aspects of the proposed solution; (ii) Collaboration Flow: the feedback is provided by an AI system that "evaluates" the proposed solution.Furthermore, we explore two more code-specific feedback types: (i) Debug Flow: the feedback message corresponds to the results from executing the code and testing it against the examples provided in the problem description; (ii) Debug-Collab Flow: the feedback is provided by an AI system with access to the code testing results, effectively, grounding the feedback and allowing more systematic reasoning about the potential causes of failure.</p>
<p>We refer to Flows using the following convention: Code-FlowName when no plan is generated and PlanFlowName-CodeFlowName otherwise.</p>
<p>Experimental Setup</p>
<p>Data.We scrape publicly available problems from one of the most popular websites hosting CC contests, Codeforces (Mirzayanov, 2023), and LeetCode (LeetCode, 2023), which cover a broad spectrum of problems ranging from easy interview questions to hard CC problems (see Appendix A.1 for more details).The datasets cover problems from 2020-August-21 to 2023-March-26 for CodeForces, and from 2013-October-25 to 2023-April-09 for LeetCode.Importantly, to study the effect of structured interactions (i.e., different Flows) in a principled manner, it is crucial to account for the possibility of data contamination, i.e., that some of the test data has been seen during training (Magar &amp; Schwartz, 2022).Containing problems published over an extended period up to a few months ago (at the time of writing), our datasets allow for reliable identification of the training data cutoff date that can help with addressing this issue.Prior code evaluation datasets like APPS (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), and Code-Contests (Li et al., 2022)   submit candidate solutions to the websites' online judges, ensuring authoritative results.For many of the Codeforces problems, we also support local evaluation based on a comprehensive set of hidden test cases we managed to scrape.For more details, see Appendix A.2. Models and Flows.We experiment with the competitive coding Flows described in Sec. 4, and GPT-4 (OpenAI, 2023a) as the LLM tool of choice.See Appendix A.4 for the specific prompts.Also, the code to reproduce the experiments in the paper is available in the project's GitHub repository.</p>
<p>Evaluation metrics.The most common evaluation metric for code generation is pass@k, corresponding to the probability that in a set of k sampled candidates, there will be at least one correct solution (Chen et al., 2021).To better align with practical use cases, we focus on pass@1, i.e. the solve rate when averaged across the problem set.We report a point estimate and a 95% confidence interval constructed from 1000 bootstrap resamples.</p>
<p>Compute and cost.All the experiments, including the most complex Flows, can be performed on commodity hardware relatively cheaply.For instance, the costs associated with querying the OpenAI API for generating Table 1 amount to $1000.</p>
<p>Experimental Results</p>
<p>We first study the generalization ability of representative Flows and empirically identify GPT-4's knowledge-cutoff date.Next, we perform a focused analysis along the dimensions described in Sec. 4.</p>
<p>Performance of Coding Flows on Pre-vs.</p>
<p>Post-Knowledge-Cutoff-Date Data Performance is averaged over a sliding window of two months.The substantial drop in performance around the reported knowledge cutoff date for GPT-3/4 (the crimson vertical line) reveals limited generalization ability that can be alleviated through structured interactions.
2 0 2 0 -1 0 -1 1 2 0 2 1 -0 1 -1 9 2 0 2 1 -0 4 -2 9 2 0 2 1 -0 8 -0 7 2 0 2 1 -1 1 -1 5 2 0 2 2 -0 2 -2 3 2 0 2 2 -0 6 -0 3 2 0 2 2 -0 9 -1 1 2 0 2 2 -1 2 -
In this experiment, we consider three representative Flows: (i) Code: the simplest Code Generator Flow corresponding to a single GPT-4 API call; (ii) Code_Debug_Collab: the most complex code Flow; (iii) Plan_Oracle-Code_Debug_Collab: the most complex code Flow with human guidance at the plan level.We perform the analysis by running the three Flows on Codeforces problems released from October 2020 to April 2023 and averaging the performance over a sliding window of two months.The results are reported in Fig. 3.</p>
<p>We observe a substantial drop in performance centered around September 2021, consistent with the knowledge Notably, there is a stark difference in the performance of the Code Flow on problems published before and after the knowledge cutoff data, with the solve rate decreasing from around 80% to 23%.While still experiencing a substantial performance drop, the Code_Debug_Collab Flow doubles the solve rate on novel problems to around 45%. Provided with human input at the plan level, the same Flow reaches 85%.Overall, this highlights that GPT-4 performs poorly on novel complex reasoning problems, but structured interactions have the potential to enhance its generalization capabilities.As both GPT-4 (i.e., the Code Flow) and the more complex interactions (Flows) exhibit qualitatively different behavior on novel data, to draw accurate conclusions, it is critical that data contamination is taken into serious consideration when designing experiments and interpreting results.</p>
<p>Comparing Competitive Coding Flows</p>
<p>Table 1 reports the performance of the systematically chosen set of Flows described in Sec. 4. Rows 6-10 correspond to Flows comprising planning and coding , while rows 1-5 perform the coding directly .In line with the findings of the previous section, we separately consider the performance on problems published before and after the knowledge cutoff date of September 2021.</p>
<p>Problem decomposition.The idea behind planning before implementing the solution is to decouple the high-level reasoning from the code implementation.To analyze the effectiveness of this pattern, we compare the Code and the Plan-Code Flow.Looking at the point estimates, in the pre-cutoff problems, introducing the plan Flow leads to decreased performance (-1.6 for Codeforces and -3.1/2.3/-9.7 for LeetCode easy/medium/hard).However, in the postcutoff problems, incorporating a plan Flow leads to gains for Codeforces (+8) and LeetCode easy and medium (+2.3 and +3.2).While these trends are consistent, considering the confidence intervals, we see that they are not statistically significant.Crucially, these results do not imply that this specific problem decomposition is not valuable as it creates a lot of potential in designing an effective human-AI collaboration.</p>
<p>Human-AI collaboration.After every contest, the Codeforces community publishes an editorial that, in addition to the code implementation, provides a short natural language description of the solution.To simulate a Flow where a human provides high-level guidance at the core of the reasoning process, we scrape the solution descriptions and pass them as human-generated plans.The results are striking: despite being only a few sentences long, human-provided plans lead to a substantial performance increase (from 26.9% to 74.5% and from 47.5% to 80.8% on novel problems, when the code is generated by Code and Code_Debug_Collab Flows, respectively).First and foremost, these results showcase the opportunities created by Flows for designing, implementing, and studying Human-AI collaboration as a key component of structured interactions.Second, specific to the problem of competitive coding, they validate the hypothesis that high-quality plans are important, suggesting that the design of more effective plan Flows is a promising direction to explore in the future.Last but not least, the results highlight the necessity of more systematic research, as patterns seemingly not valuable in one Flow, such as the simple plan-code structured reasoning problem decomposition, can provide immense value as part of another Flow.</p>
<p>Refinement with various feedback types.We find that Code_Reflection and Code_Collaboration lead to limited improvements among the code Flows.The two exceptions are Codeforces pre-cutoff (+9.3) for the former and Codeforces post-cutoff (+9.6) for the latter pattern.While close, these results are not statistically significant.On the other hand, the Flows providing grounded feedback, Code_Debug and Code_Debug_Collab, lead to consistent and statistically significant improvements, most notable on the novel Codeforces problems where performance increases from 26.9, without feedback, to 47.5, when the refinement is based on AI-generated feedback grounded in tests.On LeetCode, these improvements are smaller in magnitude.We suspect this is a consequence of the examples provided with the problem description being more simple than those in Codeforces, leading to false positives and, thereby, incorrect grounding, affecting the feedback quality.This could be addressed by generating additional tests with a Test_Case_Generator Flow, a direction we leave for future work to explore.Finally, in the plan Flows, where we consider Reflection and Collaboration (without grounding), we find that refinement does not provide statistically significant benefits.</p>
<p>Overall, our findings provide several important insights: (i) the direct benefit of problem decomposition hinges on the quality of the intermediate steps; (ii) involving humans at the core high-level reasoning process yields major improvements as humans can easily provide high-quality, grounded feedback; (iii) strategic problem decomposition is a powerful strategy for creating opportunities for effective Human-AI collaboration; (iv) the effectiveness of refinement patterns is not universal and depends on the quality of the starting solution and the feedback (e.g., the level of grounding), and the model's ability to incorporate that feedback modulated through the feedback's specificity and the model's capabilities.This analysis paints a more complex picture than what is reported by prior work for simple interactions.</p>
<p>Discussion</p>
<p>Simplicity and systematicity.Thanks to its key properties, Flows, together with aiFlows, provides an infrastructure that greatly simplifies the design and implementation of open-ended interactions, with a capability to flexibly isolate, compose, replace, or modify sub-Flows.The experiments demonstrate that carefully designed interactions can substantially improve generalization.However, they also reveal that the effectiveness of particular interaction patterns is not universal; instead, there are many factors at play.As researchers, we need to clearly specify the patterns we are studying, clearly communicate our hypotheses, and study them both in isolation and as parts of other interactions across different datasets or/and tasks.Furthermore, it is critical that data contamination is taken into serious consideration when designing experiments and drawing conclusions, and error bars become a standard in the field.</p>
<p>Cost and performance Optimization.In our experiments, we used "off-the-shelf" LLMs that have not been specifically optimized for collaboration.Performance (and compute costs) can be substantially improved by fine-tuning models to collaborate more effectively, generally or toward specialized roles (e.g., controller or critic).Learning requires data, and to support research in this direction, aiFlows implements detailed logging mechanisms of Flow runs.</p>
<p>Meta-reasoning Flows and asynchronous execution.Cognitive science research in metacognition and meta-reasoning suggests the existence of meta-level monitoring and control processes underlying cognition (Ackerman &amp; Thompson, 2017).Since Flows supports asynchronous execution of sub-Flows, it makes it possible to achieve similar asynchronous meta-cognition for autonomous AI systems moving beyond a single LLM call serving as a controller (Nakajima, 2023;Richards, 2023).For example, distributed and asynchronous execution of Flows such as FunSearch (Romera-Paredes et al., 2023) is naturally supported by Flows.</p>
<p>Conclusion</p>
<p>In this paper, we propose Flows, an abstraction that, in concert with the accompanying library aiFlows, provides the theoretical and practical infrastructure with a modular and concurrency-friendly design, which enables and facilitates the modeling, implementation, and systematic study of arbitrarily complex structured interactions.We thoroughly investigate multiple core interaction patterns, including Human-AI collaboration, and their combinations, while accounting for data contamination and the variance in the results.The investigation shows that the developed AI-only Flows add +21 and human-AI Flows add +54 absolute points in terms of solve rate, and highlights the effect of data contamination, variance, and non-universality of results.Overall, our experiments establish the potential of Flows, the necessity of more systematic research, and the value brought by Flows and aiFlows in support of these research efforts.On the one hand, Flows provides a highlevel abstraction enabling the design and implementation of interactions of arbitrary complexity.On the other, it offers a common framework for reasoning about interaction patterns, specifying hypotheses, and structuring research.We hope the framework will serve as a solid basis for practical and theoretical innovations, paving the way toward ever more useful AI, similar to the Actor model's role for concurrent and distributed systems.</p>
<p>A. Appendix</p>
<p>A.2. Code Testing and Solution Evaluation</p>
<p>The solution evaluation requires a set of input-output pairs, hidden from the user, that comprehensively test the behavior of the program.To compute the final results, we have implemented an online evaluation infrastructure that submits the candidate solutions to the websites' online judges and automatically scrapes the judgment.This mechanism ensures authoritative results.</p>
<p>For many of the Codeforces problems, we managed to scrape (sometimes a subset) of the hidden tests, allowing us to use a faster, local infrastructure for evaluating candidate solutions.On the other hand, LeetCode does not expose any of the hidden tests publicly.</p>
<p>For code testing at inference time, just like a human would, we rely on tests constructed from the (public) input-output example pairs contained in the problem description.</p>
<p>A.3. Concurrent and Previous Works as Specific Instances of Flows</p>
<p>The introduction of LLMs such as BARD, GPT-3, ChatGPT, and its latest version, GPT-4, has led to a breakthrough in AI.This has enabled many exciting developments like CoT, HuggingGPT, AutoGPT, AgentGPT, and BabyAGI.In this section, we demonstrate how Flows provides a unified view encompassing concurrent and previous work as specific Flow instances.The details are provided in Figure 5 and Table . 2. 1.Few shot Prompting (FS) (Brown et al., 2020) consists in providing a few input-output examples within the prompt, acting as demonstrations to enable the LLM to perform a specific task.This technique relies on the LLM's emergent in-context learning ability to extrapolate from these limited examples and infer how to solve the task in general.</p>
<p>Agent Flow</p>
<p>CoT Prompting We depict a selected subset of previous works incorporating structured reasoning and/or interactions between AI agents, tools, and humans, through the lens of the Flows framework.This demonstrates that Flows is a powerful language for describing, conceptualizing, and disseminating structured interaction patterns.</p>
<p>Agent Flow</p>
<ol>
<li>
<p>Chain of Thoughts (CoT) (Wei et al., 2022) is a prompting method (atomic Flow) that allows LLMs to generate a series of intermediate natural language reasoning steps that lead to the final output.</p>
</li>
<li>
<p>Tree of Thoughts (ToT) (Yao et al., 2023a) is a framework that enables (orchestration) exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem-solving.ToT allows LLMs to perform deliberate decision-making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.</p>
</li>
<li>
<p>Program of Thoughts (PoT) (Chen et al., 2022) is a prompting method that allows language models (mainly Codex) to express the reasoning process as a program.The computation is relegated to an external program, which executes the generated programs to derive the answer.</p>
</li>
</ol>
<p>5.</p>
<p>Mutimodal CoT (M-CoT) (Zhang et al., 2023) is a method that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference.To facilitate the interaction between modalities in M-CoT, smaller language models (LMs) are fine-tuned by fusing multimodal features.</p>
<ol>
<li>
<p>ToolFormer (Schick et al., 2023) is a model that is trained to decide which APIs to call, when to call them, what arguments to pass, and how to incorporate the results into future tokens prediction.</p>
</li>
<li>
<p>ReAct (Yao et al., 2023b) is a framework that uses LLMs to generate reasoning traces and task-specific actions sequentially.The framework allows for greater synergy between the two: reasoning traces help the model induce, track, and update action plans and handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information.</p>
</li>
<li>
<p>Parsel (Zelikman et al., 2022) is a framework that enables the automatic implementation and validation of complex algorithms with code LLMs.The framework first synthesizes an intermediate representation based on the Parsel language and can then apply a variety of postprocessing tools.Code is generated in a next step.9. REFINER (Paul et al., 2023) is a framework for LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning.10.Self-Refine (Madaan et al., 2023) is a framework for LLMs to generate coherent outputs.The main idea is that an LLM will initially generate an output while the same LLM provides feedback for its output and uses it to refine itself iteratively.</p>
</li>
<li>
<p>Recursively Criticize and Improve (RCI) (Kim et al., 2023) showed that a pre-trained large language model (LLM) agent could execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI).Unlike Self-refine, this method uses two separate LLMs (ChatGPT), one for performing the task and another for criticizing.</p>
</li>
<li>
<p>Self-Correct (Welleck et al., 2023) is a framework that decouples a flawed base generator (an LLM) from a separate corrector that learns to iteratively correct imperfect generations.The imperfect base generator can be an off-the-self LLM or a supervised model, and the corrector model is trained.</p>
</li>
<li>
<p>Self-Debug (Chen et al., 2023) is a framework that relies on external tools (SQL application or Python interpreter) to help large language models revise and debug SQL commands or Python code with bugs.</p>
</li>
<li>
<p>Reflexion (Shinn et al., 2023) is a framework that provides a free-form reflection on whether a step was executed by LLM correctly or not and potential improvements.Unlike self-refine and self-debug, Reflexion builds a persisting memory of self-reflective experiences, which enables an agent to identify its own errors and self-suggest lessons to learn from its mistakes over time.</p>
</li>
<li>
<p>Meta-Reasoner (Yoran et al., 2023) is an approach which prompts large language models to meta-reason over multiple chains of thought rather than aggregating their answers.This approach included two steps: (i) ask LLM to generate multiple reasoning chains, (ii) ask another LLM (meta-reasoner) to reason over the multiple reasoning chains to arrive at the correct answer.</p>
</li>
<li>
<p>HuggingGPT (Shen et al., 2023) is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve numerous sophisticated AI tasks in different modalities (such as language, vision, speech) and domains.17.Camel (Li et al., 2023) is a communicative agent framework involving inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions.</p>
</li>
<li>
<p>Chameleon (Lu et al., 2023) is a plug-and-play compositional reasoning framework that augments external tools with LLMs in a plug-and-play manner.The core idea is that an LLM-based planner assembles a sequence of tools to execute to generate the final response.The assumption is that this will be less error-prone, easily expandable to new modules, and user-friendly.</p>
</li>
<li>
<p>AutoGPT (Richards, 2023) is an experimental open-source application that leverages the capabilities of large language models (LLMs) and Chatbots such as OpenAI's GPT-4 and Chat-GPT to create fully autonomous and customizable AI agents.It has internet access, long-term and short-term memory management.</p>
</li>
<li>
<p>BabyAGI (Nakajima, 2023) is an intelligent agent capable of generating and attempting to execute tasks based on a given objective.BabyAGI operates based on three LLM flows: Task creation flow, Task prioritization flow, and Execution flow.</p>
</li>
</ol>
<p>A.4. Prompting</p>
<p>We provide the prompts used to obtain the results in Section 6.Our evaluation is made possible thanks to the modular and compositional nature of Flows.Some of the experimental setups are deeply nested, and in cases where Flows build on each other, we avoid repetition.Note that the project's GitHub repository provides the code and data to reproduce all of the experiments in the paper.</p>
<p>Direct prompting for a solution is shown in Listing 1.To add reflection, we use a Generator-Critic Flow to combine the code generation with a fixed reply, as shown in Listing 2. In the collaboration setting, we use Listing 3 as the generator and Listing 4 as the critic.</p>
<p>Debugging is incorporated via a testing Flow that adds formatting to the output of a code executor.The formatting templates are shown in Listing 6.To respond to the debug output, we rely on an adjusted coding Flow 5. Adding collaboration in the  (Chen et al., 2022) Seq.(Zhang et al., 2023) Seq.(Wei et al., 2022) Seq.(Yao et al., 2023b) Circular (Zelikman et al., 2022) Seq.(Paul et al., 2023) Gen-Crit (Madaan et al., 2023) Gen-Crit (Kim et al., 2023) Gen-Crit (Welleck et al., 2023) Gen-Crit (Chen et al., 2023) Gen-Crit (Shinn et al., 2023) Gen-Crit (Yoran et al., 2023) Seq.(Shen et al., 2023) Seq.(Li et al., 2023) Circular (Lu et al., 2023) Seq.debugging setting is done by introducing a critic that provides feedback grounded in the test results.This Flow is detailed in Listing 3.
✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ CoT (Wei et al., 2022) Atomic ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗ ToT (Yao et al., 2023a) Circular ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗ PoT✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗ M-CoT✗ ✗ ✗ ✗ ✓ ✗ ✗ ✓ ToolFormer✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓ ReAct✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗ Parsel✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗ REFINER✗ ✓ ✓ ✗ ✓ ✗ ✓ ✓ Self-Refine✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗ RCI✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗ Self-Correct✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗ Self-Debug✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗ Reflexion✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗ Meta-Reasoner✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗ HuggingGPT✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗ Camel✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗ Chameleon✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗ AutoGPT (Richards, 2023) Circular ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✗ BabyAGI (Nakajima, 2023) Circular ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗
The scenarios explained above also support the addition of a planning Flow.An example of plan generation is shown in Listing 8.     Solving competitive coding challenges is an eminently hard problem.The solve rate of only 27% by directly attempting the problem and 47% by the best-performing code Flow, paired with a reliable automatic evaluation metric, make competitive programming an ideal benchmark for AI systems.Motivated by this, we propose a competition where instead of people, proposed Flows solve competitive programming problems.</p>
<p>The competition will leverage the comprehensive dataset of publicly available Codeforces problems and the open-source infrastructure for inference and testing used in the experiments, available at https://github.com/epfl-dlab/cc_flows.</p>
<p>Figure 1 .
1
Figure 1.Flows framework exemplified.The first column depicts examples of tools.The second column depicts Atomic Flows constructed from the example tools.The third column depicts examples of Composite Flows defining structured interaction between Atomic or Composite Flows.The fourth column illustrates a specific Composite competitive coding Flow as those used in the experiments.The fifth column outlines the structure of a hypothetical Flow, defining a meta-reasoning process that could support autonomous behavior. 2</p>
<p>Figure 2 .
2
Figure2.Competitive coding Flows.At the highest level, we consider planning as a specific structured reasoning pattern for problem decomposition.In particular, the Plan Flow generates a solution strategy and passes it to the Code Flow, which implements it, as depicted in A).B) and C) depict the different choices of sub-Flows used as Plan and Code Flows in the experiments.Notably, we explore the impact of human-AI collaboration at the plan level and refinement with different types of feedback: i) fixed reply encouraging reflection; ii) AI generated feedback; iii) code testing results as feedback; iv) AI generated feedback grounded in code testing results.</p>
<p>Figure 3 .
3
Figure3.Temporal analysis.Performance is averaged over a sliding window of two months.The substantial drop in performance around the reported knowledge cutoff date for GPT-3/4 (the crimson vertical line) reveals limited generalization ability that can be alleviated through structured interactions.</p>
<p>LeetCode problems are provided in Fig.4.In the first experiment, the temporal analysis, we use 239 Codeforces problems ranging from October 2020 to April 2023.In the second experiment, we have 136 problems for Codeforces (some problems are dropped in order to keep the pre-cutoff and post-cutoff buckets equal to 68) and 558 problems for LeetCode (93 for each of the six buckets).Additionally, to support research in the area, we set up an AI competitive coding challenge based on a dataset of Codeforces problems of various difficulties published after the knowledge cutoff date.More details about the CC competition are available in Appendix A.5.You have received data from a Bubble bot.You know your task is to make factory facilities, but before you even start, you need to know how big the factory is and how many rooms it has.When you look at the data you see that you have the dimensions of the construction, which is in rectangle shape: N x M. Then in the next N lines you have M numbers.These numbers represent factory tiles and they can go from 0 to 15.Each of these numbers should be looked in its binary form.Because from each number you know on which side the tile has walls.For example number 10 in it's binary form is 1010, which means that it has a wall from the North side, it doesn't have a wall from the East, it has a wall on the South side and it doesn't have a wall on the West side.So it goes North, East, South, West.It is guaranteed that the construction always has walls on it's edges.The input will be correct.Your task is to print the size of the rooms from biggest to smallest.Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for '?' and '<em>' where:•'?' Matches any single character.•'</em>'Matches any sequence of characters (including the empty sequence).The matching should cover the entire input string (not partial).</p>
<p>a" does not match the entire string "aa".</p>
<p>Figure 4 .
4
Figure 4. Examples of competitive coding problems from Codeforces and LeetCode.</p>
<p>Figure 5 .
5
Figure5.Previous works are specific Flows.We depict a selected subset of previous works incorporating structured reasoning and/or interactions between AI agents, tools, and humans, through the lens of the Flows framework.This demonstrates that Flows is a powerful language for describing, conceptualizing, and disseminating structured interaction patterns.</p>
<p>Listing 1 .
1
Prompts for Code Flow (Codeforces) " p r o m p t t e m p l a t e s " : " s y s t e m _ m e s s a g e " : | − Your g o a l i s t o p r o v i d e e x e c u t a b l e P y t h o n c o d e t h a t s o l v e s a c o m p e t i t i v e programming p r o b l e m .The c o d e s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e u s e d t o e v a l u a t e t h e c o r r e c t n e s s o f t h e s o l u t i o n .The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h : − t h e p r o b l e m s t a t e m e n t − i n p u t d e s c r i p t i o n − o u t p u t d e s c r i p t i o n − e x a m p l e t e s t c a s e s − ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s The u s e r w i l l p r o v i d e you w i t h a t a s k and an o u t p u t f o r m a t t h a t you w i l l s t r i c t l y f o l l o w ." q u e r y _ m e s s a g e " : | − # P r o b l e m s t a t e m e n t {{ p r o b l e m _ d e s c r i p t i o n }} # I n p u t d e s c r i p t i o n {{ i n p u t _ d e s c r i p t i o n }} # O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be p a s s e d t o t h e s t a n d a r d o u t p u t .R e t u r n P y t h o n c o d e t h a t s o l v e s t h ep r o b l e m .R e p l y i n t h e f o l l o w i n g f o r m a t : <code>p y t h o n {{ c o d e _ p l a c e h o l d e r }} ``" human_message " : | − {{ q u e r y }} Listing 2. Prompts for Fixed-Reply Flow " p r o m p t t e m p l a t e s " : " f i x e d _ r e p l y " : | − C o n s i d e r t h e p r o b l e m s t a t e m e n t and t h e l a s t p r o p o s e d s o l u t i o n .Are you s u r e t h a t t h e s o l u t i o n i s p r o v i d e d i n t h e r e q u e s t e d f o r m a t , and c r u c i a l l y , s o l v e s t h e p r o b l e m ?I f t h a t i s n o t t h e c a s e , p r o v i d e t h e c o r r e c t e d v e r s i o n o f t h e c o d e i n t h e f o l l o w i n g f o r m a t :</code>p y t h o n {{ p y t h o n _ c o d e }} <code>ò t h e r w i s e , r e p l y : " F i n a l a n s w e r ." Listing 3. Prompts for Code-Collab Flow (Codeforces) " p r o m p t t e m p l a t e s " : " s y s t e m _ m e s s a g e " : | − Your g o a l i s t o p r o v i d e e x e c u t a b l e P y t h o n c o d e t h a t s o l v e s a c o m p e t i t i v e programming p r o b l e m .The c o d e s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e u s e d t o e v a l u a t e t h e c o r r e c t n e s s o f t h e s o l u t i o n .The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h : − t h e p r o b l e m s t a t e m e n t − i n p u t d e s c r i p t i o n − o u t p u t d e s c r i p t i o n − e x a m p l e t e s t c a s e s − ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s The u s e r w i l l p r o v i d e you w i t h a t a s k and an o u t p u t f o r m a t t h a t you w i l l s t r i c t l y f o l l o w ." q u e r y _ m e s s a g e " : | − # P r o b l e m s t a t e m e n t {{ p r o b l e m _ d e s c r i p t i o n }} # I n p u t d e s c r i p t i o n {{ i n p u t _ d e s c r i p t i o n }} # O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }} The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be p a s s e d t o t h e s t a n d a r d o u t p u t .R e t u r n P y t h o n c o d e t h a t s o l v e s t h e p r o b l e m .R e p l y i n t h e f o l l o w i n g f o r m a t : ```p y t h o n {{ c o d e _ p l a c e h o l d e r }}</code>" human_message " : | − # F e e d b a c k on t h e l a s t p r o p o s e d s o l u t i o n {{ c o d e _ f e e d b a c k }} C o n s i d e r t h e o r i g i n a l p r o b l e m s t a t e m e n t , t h e l a s t p r o p o s e d s o l u t i o n and t h e p r o v i d e d f e e d b a c k .Does t h e s o l u t i o n n e e d t o be u p d a t e d ?I f so , p r o v i d e t h e c o r r e c t e d v e r s i o n o f t h e c o d e i n t h e f o l l o w i n g f o r m a t : <code>`p y t h o n {{ c o d e _ p l a c e h o l d e r }}</code>ò t h e r w i s e , r e p l y : " F i n a l a n s w e r ." Listing 4.</p>
<p>p r o p o s e d s o l u t i o n o r i t i s c o r r e c t ?E x p l a i n y o u r r e a s o n i n g v e r y c o n c i s e l y , and do n o t p r o v i d e c o d e ." human_message " : | − {{ q u e r y }} Listing 5. Prompts for Code-Debug Flow (Codeforces) " p r o m p t t e m p l a t e s " : " s y s t e m _ m e s s a g e " : | − Your g o a l i s t o p r o v i d e e x e c u t a b l e P y t h o n c o d e t h a t s o l v e s a c o m p e t i t i v e programming p r o b l e m .The c o d e s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e u s e d t o e v a l u a t e t h e c o r r e c t n e s s o f t h e s o l u t i o n .The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h : − t h e p r o b l e m s t a t e m e n t − i n p u t d e s c r i p t i o n − o u t p u t d e s c r i p t i o n − e x a m p l e t e s t c a s e s − ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s The u s e r w i l l p r o v i d e you w i t h a t a s k and an o u t p u t f o r m a t t h a t you w i l l s t r i c t l y f o l l o w ." q u e r y _ m e s s a g e " : | − # P r o b l e m s t a t e m e n t {{ p r o b l e m _ d e s c r i p t i o n }}</p>
<p>The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h : − t h e p r o b l e m s t a t e m e n t − i n p u t d e s c r i p t i o n − o u t p u t d e s c r i p t i o n − e x a m p l e t e s t c a s e s − ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s − an i n c o r r e c t P y t h o n s o l u t i o n a t t e m p t and a d e s c r i p t i o n o f i t s i s s u e C r u c i a l l y , y o u r g o a l i s t o c o n s i d e r a l l a s p e c t s o f t h e p r o b l e m and p i n p o i n t t h e i s s u e s w i t h t h e s o l u t i o n a t t e m p t , and n o t t o p r o v i d e t h e c o d e i m p l e m e n t a t i o n y o u r s e l f .Some a s p e c t s t o c o n s i d e r : I s t h e i n p u t c o r r e c t l y p a r s e d ?I s t h e o u t p u t c o r r e c t l y f o r m a t t e d ?Are t h e c o r n e r c a s e s c o r r e c t l y h a n d l e d ?I s t h e r e a l o g i c a l m i s t a k e w i t h t h e a l g o r i t h m i t s e l f ?Use t h e c o d e e x e c u t i o n r e s u l t s p r o v i d e d i n t h e i s s u e d e s c r i p t i o n t o g u i d e y o u r r e a s o n i n g / d e b u g g i n g ." q u e r y _ m e s s a g e " : | − # P r o b l e m s t a t e m e n t {{ p r o b l e m _ d e s c r i p t i o n }} # I n p u t d e s c r i p t i o n {{ i n p u t _ d e s c r i p t i o n }} # O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }} # S o l u t i o n a t t e m p t t o be f i x e d <code _="_" a="a" e="e" g="g" i="i" l="l" m="m" n="n" r="r" s="s" t="t" u="u" y="y">`p y t h o n {{ c o d e }}</code> A.5.The CC-Flows-competition: a new form of competitive coding}} C o n s i d e r t h e p r o b l e m s t a t e m e n t , t h e s o l u t i o n a t t e m p t and t h e i s s u e .Why i s t h e s o l u t i o n a t t e m p t i n c o r r e c t ?How s h o u l d i t be f i x e d ?E x p l a i n y o u r r e a s o n i n g v e r y c o n c i s e l y , and do n o t p r o v i d e c o d e ." human_message " : | − {{ q u e r y }} Listing 8. Prompts for Plan Flow (Codeforces) " p r o m p t t e m p l a t e s " : " s y s t e m _ m e s s a g e " : | − Your g o a l i s t o p r o v i d e a h i g h − l e v e l c o n c e p t u a l s o l u t i o n t h a t , i f i m p l e m e n t e d , w i l l s o l v e a g i v e n c o m p e t i t i v e programming p r o b l e m .The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h : − t h e p r o b l e m s t a t e m e n t − i n p u t d e s c r i p t i o n − o u t p u t d e s c r i p t i o n − e x a m p l e t e s t c a s e s − ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s The p r o p o s e d a l g o r i t h m s h o u l d be c o m p u t a t i o n a l l y e f f i c i e n t , l o g i c a l l y c o r r e c t and h a n d l e a l l c o r n e r c a s e s .The u s e r w i l l p r o v i d e you w i t h a t a s k and an o u t p u t f o r m a t t h a t you w i l l s t r i c t l y f o l l o w ." q u e r y _ m e s s a g e " : | − # P r o b l e m s t a t e m e n t {{ p r o b l e m _ d e s c r i p t i o n }} # I n p u t d e s c r i p t i o n {{ i n p u t _ d e s c r i p t i o n }} # O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }} R e t u r n a h i g h − l e v e l c o n c e p t u a l s o l u t i o n t h a t would s o l v e t h e p r o b l e m .Be v e r y c o n c i s e , and do n o t p r o v i d e c o d e .R e p l y i n t h e f o l l o w i n g f o r m a t : # C o n c e p t u a l s o l u t i o n {{ p l a n _ p l a c e h o l d e r }} " human_message " : | − {{ q u e r y }</p>
<p>lack problem release dates, and considering the lack of publicly available information about LLMs' training data, can likely lead to confounded evaluation of models' memorization and generalization abilities.
Code testing and solution evaluation. Just like a humanparticipant, the Debug Flow has access only to the input-output example pairs contained in the problem descriptionand, at inference time, uses a local code testing infrastruc-ture to evaluate (intermediate) solution candidates. Cru-cially, these examples cover only a few simple cases, and
generating outputs consistent with them does not imply the code corresponds to a correct solution.A solution is considered correct if it passes all the hidden test cases.To determine correctness, we leverage online evaluators that</p>
<p>Table 1 .
1
Main Results.Performance of competitive coding Flows on Codeforces and LeetCode, with direct inference (Code) as baseline.
CodeforcesLeetcodePre-cutoff Post-cutoffPre-cutoffPost-cutoffEasyMediumHardEasyMediumHardCode71.8 ±11.026.9 ±11.097.8 ±3.1 93.4 ±5.4 66.7 ±10.9 76.3 ±8.6 25.1 ±8.98.0 ±5.5Code_Reflection+9.3 ±9.7+0.0 ±10.6+0.0 ±3.1 +0.0 ±5.4 +1.2 ±10.6 +0.9 ±8.1 +5.4 ±9.4 +3.5 ±6.6Code_Collaboration+4.8 ±10.5+9.6 ±11.8+0.0 ±3.1 -2.3 ±6.0-0.1 ±10.9-3.2 ±8.7 +0.0 ±8.7 +1.2 ±5.9Code_Debug+12.7 ±8.6+7.9 ±11.6+0.0 ±3.1 +1.1 ±5.0 +6.9 ±10.0 +7.7 ±7.3 +7.7 ±9.6 +2.4 ±6.3Code_Debug_Collab+12.6 ±8.9 +20.6 ±12.1+0.0 ±3.1 +0.0 ±5.4 +5.5 ±10.4 +7.5 ±7.4 +9.8 ±9.7 +1.2 ±6.0Plan-Code-1.6 ±11.0+8.0 ±11.6-3.1 ±4.5-2.3 ±5.9-9.7 ±11.2 +2.3 ±8.3 +3.2 ±9.1 -3.4 ±4.3Plan_Reflection-Code-3.3 ±11.6+4.8 ±11.6-2.1 ±4.1-4.5 ±6.6-3.1 ±10.7 +1.2 ±8.3 -3.3 ±8.5 +0.0 ±5.5Plan_Collaboration-Code-4.8 ±11.5+6.3 ±11.4-1.1 ±3.7-2.3 ±6.1-7.2 ±11.2-2.0 ±8.6 +0.1 ±9.0 +1.2 ±5.8Plan_Oracle-Code+11.0 ±9.4 +47.6 ±10.7------Plan_Oracle-Code_ Debug_Collab+23.0 ±5.2+53.9 ±9.5------
cutoff date reported by OpenAI, and denote it by a vertical line on the plot.With Codeforces problems appearing in contexts outside of the contest itself (e.g., editorials), it is reasonable to assume the model has been exposed to older problems more frequently during training.This would explain why the drop spans multiple months, from May 2021 to November 2021, depending on when which data was published and crawled.</p>
<p>Table 2 .
2
Previous work.We compare previous work across relevant dimensions.</p>
<p>Prompts for Code-Collab-Critic Flow (Codeforces)</p>
<h1>P y t h o n s o l u t i o n a t t e m p t :<code>`p y t h o n{{ c o d e }}</code>Co n s i d e r t h e p r o b l e m s t a t e m e n t and t h e s o l u t i o n a t t e m p t . Are t h e r e any i s s u e sw i t h t h e" p r o m p t t e m p l a t e s " :" s y s t e m _ m e s s a g e " : | −Your g o a l i s t o i d e n t i f y p o t e n t i a l i s s u e s w i t h a c o m p e t i t i v e programmings o l u t i o n a t t e m p t .The u s e r w i l l s p e c i f y t h e p r o b l e m by p r o v i d i n g you w i t h :− t h e p r o b l e m s t a t e m e n t− i n p u t d e s c r i p t i o n− o u t p u t d e s c r i p t i o n− e x a m p l e t e s t c a s e s− ( o p t i o n a l ) e x p l a n a t i o n o f t h e t e s t c a s e s− a P y t h o n s o l u t i o n a t t e m p tC r u c i a l l y , y o u r g o a l i s t o c o r r e c t l y i d e n t i f y p o t e n t i a l i s s u e s w i t h t h es o l u t i o n a t t e m p t , and n o t t o p r o v i d e t h e c o d e i m p l e m e n t a t i o n y o u r s e l f .The u s e r w i l l p r o v i d e you w i t h a t a s k and an o u t p u t f o r m a t t h a t you w i l ls t r i c t l y f o l l o w ." q u e r y _ m e s s a g e " : | −# P r o b l e m s t a t e m e n t{{ p r o b l e m _ d e s c r i p t i o n }}# I n p u t d e s c r i p t i o n{{ i n p u t _ d e s c r i p t i o n }}</h1>
<h1>O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}</h1>
<p>F a i l e d t e s t {{ i d x } } ] ### [ F a i l e d t e s t {{ i d x } } ] I n p u t <code>1 {{ t e s t _ i n p u t }} ``# ## [ F a i l e d t e s t {{ i d x } } ] E x p e c t e d o u t p u t ``{ { e x p e c t e d _ o u t p u t }} Prompts for Code-Debug-Collab Flow (Codeforces) " p r o m p t t e m p l a t e s " : " s y s t e m _ m e s s a g e " : | − Your g o a l i s t o i d e n t i f y t h e i s s u e s w i t h an i n c o r r e c t c o m p e t i t i v e programming s o l u t i o n a t t e m p t .
C o n s i d e r t h e p r o b l e m s t a t e m e n t , t h e l a s t p r o p o s e d s o l u t i o n , and i t s i s s u e .P r o v i d e a c o r r e c t e d v e r s i o n o f t h e c o d e t h a t s o l v e s t h e o r i g i n a l p r o b l e mand r e s o l v e s t h e i s s u e , w i t h o u t any e x p l a n a t i o n , i n t h e f o l l o w i n g f o r m a t :</code>p y t h o n{{ c o d e _ p l a c e h o l d e r }}# I n p u t d e s c r i p t i o n {{ i n p u t _ d e s c r i p t i o n }} # O u t p u t d e s c r i p t i o n {{ o u t p u t _ d e s c r i p t i o n }} {{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }} The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be p a s s e d t o t h e s t a n d a r d o u t p u t . R e t u r n P y t h o n c o d e t h a t s o l v e s t h e p r o b l e m . R e p l y i n t h e f o l l o w i n g f o r m a t : <code>p y t h o n {{ c o d e _ p l a c e h o l d e r }} ``" human_message " : | − {{ t e s t i n g _ r e s u l t s _ s u m m a r y }}</code>L isting 6. Formatting templates for Code-Testing Flow (Codeforces) " f o r m a t t i n g t e m p l a t e s " : " no e r r o r t e m p l a t e " : | − $ { . i s s u e _ t i t l e } A l l o f t h e e x e c u t e d t e s t s p a s s e d . " a l l t e s t s h e a d e r " : | − $ { . i s s u e _ t i t l e } The P y t h o n c o d e d o e s n o t s o l v e t h e p r o b l e m i n t h e p r o b l e m d e s c r i p t i o n due t o l o g i c a l e r r o r s . I t f a i l s on t h e f o l l o w i n g t e s t s . " c o m p i l a t i o n e r r o r t e m p l a t e " : | − $ { . i s s u e _ t i t l e } The e x e c u t i o n r e s u l t e d i n a c o m p i l a t i o n e r r o r . ## C o m p i l a t i o n e r r o r m e s s a g e : {{ e r r o r _ m e s s a g e }} " t i m e o u t e r r o r t e m p l a t e " : | − $ { . i s s u e _ t i t l e } The e x e c u t i o n t i m e d o u t , t h e s o l u t i o n i s n o t e f f i c i e n t enough . " r u n t i m e e r r o r t e m p l a t e " : | − $ { . i s s u e _ t i t l e } The e x e c u t i o n r e s u l t e d i n a r u n t i m e e r r o r on t h e f o l l o w i n g t e s t . ## [ F a i l e d t e s t ] I n p u t <code>{ { t e s t _ i n p u t }}</code># # [ F a i l e d t e s t ] Runtime e r r o r m e s s a g e {{ e r r o r _ m e s s a g e }} " s i n g l e t e s t e r r o r " : | − $ { . i s s u e _ t i t l e } The P y t h o n c o d e d o e s n o t s o l v e t h e p r o b l e m i n t h e p r o b l e m d e s c r i p t i o n due t o l o g i c a l e r r o r s . I t f a i l s t h e f o l l o w i n g t e s t : ## [ F a i l e d t e s t ] I n p u t <code>{ { t e s t _ i n p u t }}</code># # [ F a i l e d t e s t ] E x p e c t e d o u t p u t <code>{ { e x p e c t e d _ o u t p u t }}</code># # [ F a i l e d t e s t ] G e n e r a t e d o u t p u t <code>{ { g e n e r a t e d _ o u t p u t }}</code>" t e s t e r r o r " : | − ## [ F a i l e d t e s t {{ i d x } } ] G e n e r a t e d o u t p u t <code>{ { g e n e r a t e d _ o u t p u t }} ```L ## [</code># isting 7.
EPFL
Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
PSL University *, ** Equal contribution Correspondence to: martin.josifoski@epfl.ch, lars.klein@epfl.ch, maxime.peyrard@univ-grenoble-alpes.fr, robert.west@epfl.ch
For more details on meta-reasoning Flows see Sec. 7
The concept of a Flow is sufficient for modeling any interaction. We introduce this distinction as it improves the exposition and simplifies the implementation.
The aiFlows library is available at https://github.com/epfl-dlab/aiflows.Data and Flows for reproducing our experiments are available at https://github.com/epfl-dlab/cc_flows.Code Generator Code Feedback Code Feedback (Sequential) Code Testing Code Flow (Generator-Critic) Code Generator Code Feedback Code Feedback (Sequential) Code Testing Debug-Collab (Generator-Critic) Code Generator Code Generator Code Feedback Collaboration (Generator-Critic) Code Generator Code Testing Debug (Generator-Critic) Code GeneratorThe competition will only include problems published after the knowledge-cutoff date of GPT-4.Furthermore, not to overload the Codeforces online evaluation infrastructure, we further filter this dataset to problems for which public and private tests are available, and the output format is compatible with our local code testing infrastructure.Codeforces ranks the difficulty of each problem from 800 to 2100.At the time of publishing, we have the following number of problems per difficulty (total of 416):• difficulty 800: 149• difficulty 900 to 1500 (inclusive): 185• difficulty 1600 to 2100 (inclusive): 82We will curate a leaderboard of best-performing Flows that will be publicly available on FlowVerse and provide the predictions that reproduce the reported scores using the provided infrastructure.The data will be released and should be used in accordance with Codeforces' Terms and Conditions.Concretely, Codeforces prohibits the material from being sold, sublicensed, or commercialized.For more details, take a look at the project's GitHub page.
Meta-reasoning: Monitoring and control of thinking and reasoning. R Ackerman, V A Thompson, 10.1016/j.tics.2017.05.004.URLhttps://www.sciencedirect.com/science/article/pii/S1364661317301055Trends in Cognitive Sciences. 1364-66132182017</p>
<p>Guidelines for human-ai interaction. S Amershi, D S Weld, M Vorvoreanu, A Fourney, B Nushi, P Collisson, J Suh, S T Iqbal, P N Bennett, K Inkpen, J Teevan, R Kikin-Gil, E Horvitz, 10.1145/3290605.3300233Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019, Glasgow. S A Brewster, G Fitzpatrick, A L Cox, V Kostakos, the 2019 CHI Conference on Human Factors in Computing Systems, CHI 2019, GlasgowScotland, UKACMMay 04-09, 2019. 2019</p>
<p>Making reliable distributed systems in the presence of software errors. J Armstrong, 2003Stockholm, SwedenRoyal Institute of TechnologyPhD thesis</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>. H Chase, Langchain, 2022</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H Ponde, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D W Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, I Babuschkin, S A Balaji, S Jain, A Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, Zaremba , ArXiv, abs/2107.033742021Evaluating large language models trained on code</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, ArXiv, abs/2211.125882022</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schärli, D Zhou, ArXiv, abs/2304.051282023</p>
<p>Promptbreeder: Selfreferential self-improvement via prompt evolution. C Fernando, D Banarse, H Michalewski, S Osindero, T Rocktäschel, 10.48550/arXiv.2309.167972023</p>
<p>An introduction to software architecture. D Garlan, M Shaw, 10.1142/9789812798039_0001Advances in Software Engineering and Knowledge Engineering. V Ambriola, G Tortora, 19932World Scientific</p>
<p>. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, Steinhardt, J. Measuring coding challenge competence with apps. NeurIPS. 2021</p>
<p>C E Hewitt, Actor model of computation: Scalable robust information systems. arXiv: Programming Languages. 2010</p>
<p>A universal modular actor formalism for artificial intelligence. C E Hewitt, P B Bishop, R Steiger, International Joint Conference on Artificial Intelligence. 1973</p>
<p>Metagpt: Meta programming for multi-agent collaborative framework. S Hong, X Zheng, J Chen, Y Cheng, J Wang, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, 10.48550/arXiv.2308.003522023</p>
<p>Principles of mixed-initiative user interfaces. E Horvitz, 10.1145/302979.303030Proceeding of the CHI '99 Conference on Human Factors in Computing Systems: The CHI is the Limit. M G Williams, M W Altom, eeding of the CHI '99 Conference on Human Factors in Computing Systems: The CHI is the LimitPittsburgh, PA, USAACMMay 15-20, 1999. 1999</p>
<p>Large language models cannot self-correct reasoning yet. J Huang, X Chen, S Mishra, H S Zheng, A W Yu, X Song, D Zhou, 10.48550/arXiv.2310.017982023</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, ArXiv, abs/2303.174912023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, Oh, Advances in Neural Information Processing Systems. A , Curran Associates, Inc2022. 202335</p>
<p>G Li, H A A K Hammoud, H Itani, D Khizbullin, B Ghanem, Camel, arXiv:2303.17760Communicative agents for" mind" exploration of large scale language model society. 2023arXiv preprint</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Dal Lago, Science. 37866242022</p>
<p>Chameleon: Plug-andplay compositional reasoning with large language models. P Lu, B Peng, H Cheng, M Galley, K.-W Chang, Y N Wu, S.-C Zhu, J Gao, ArXiv, abs/2304.098422023</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Data contamination: From memorization to exploitation. I Magar, R Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 22-27, 202222022Short Papers), ACL 2022</p>
<p>. M Mirzayanov, Codeforces, Com, 2023</p>
<p>When to show a suggestion? integrating human feedback in ai-assisted programming. H Mozannar, G Bansal, A Fourney, E Horvitz, 10.48550/arXiv.2306.049302023</p>
<p>. Y Nakajima, Babyagi, 2023</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M I Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, C Sutton, A Odena, CoRR, abs/2112.001142021</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023a</p>
<p>. Openai, Chatgpt, 2023b</p>
<p>D Paul, M Ismayilzada, M Peyrard, B Borges, A Bosselut, R West, B Faltings, Refiner, arXiv:2304.01904Reasoning feedback on intermediate representations. 2023arXiv preprint</p>
<p>Automated protein model building combined with iterative structure refinement. A Perrakis, R J Morris, V S Lamzin, Nature Structural Biology. 6202928521999</p>
<p>Learning to model editing processes. M Reid, G Neubig, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>. T B Richards, Autogpt, 2023</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, CoRR, abs/2112.107522021</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 2023</p>
<p>Image super-resolution via iterative refinement. C Saharia, J Ho, W Chan, T Salimans, D J Fleet, M Norouzi, IEEE Transactions on Pattern Analysis and Machine Intelligence. 452021</p>
<p>T Schick, J Dwivedi-Yu, Z Jiang, F Petroni, P Lewis, G Izacard, Q You, C Nalmpantis, E Grave, S Riedel, Peer, ArXiv, abs/2208.11663A collaborative language model. 2022</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, ArXiv, abs/2302.047612023</p>
<p>Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D S Li, W Lu, Y T Zhuang, Hugginggpt, ArXiv, abs/2303.175802023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, ArXiv, abs/2302.139712023a</p>
<p>. H Touvron, L Martin, K R Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D M Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A S Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I M Kloumann, A V Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, 2023bT. Llama 2: Open foundation and fine-tuned chat models. arXiv</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Generating sequences by learning to self-correct. S Welleck, X Lu, P West, F Brahman, T Shen, D Khashabi, Y Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Autogen: Enabling next-gen LLM applications via multiagent conversation framework. Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, 10.48550/arXiv.2308.081552023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, ArXiv, abs/2305.106012023a</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. O Yoran, T Wolfson, B Bogin, U Katz, D Deutch, J Berant, ArXiv, abs/2304.130072023</p>
<p>Parsel: A (de-)compositional framework for algorithmic reasoning with language models. E Zelikman, Q Huang, G Poesia, N D Goodman, N Haber, 2022</p>
<p>Multimodal chain-of-thought reasoning in language models. Z Zhang, A Zhang, M Li, H Zhao, G Karypis, A J Smola, ArXiv, abs/2302.009232023</p>            </div>
        </div>

    </div>
</body>
</html>
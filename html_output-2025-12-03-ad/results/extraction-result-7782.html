<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7782 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7782</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7782</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-269032822</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.07108v2.pdf" target="_blank">From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications</a></p>
                <p><strong>Paper Abstract:</strong> Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7782.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7782.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RevisionDistance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Revision Distance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered text evaluation metric that uses an LLM to generate structured revision edits and counts the number of edits needed to bring a draft closer to a target (reference or ideal) text; smaller values indicate better quality and it yields explainable, fine-grained revision actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as the LLM User to generate revision edits); Mistral-7B and Mixtral-8x7B and GPT-4 used as text generators in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (unspecified commercial), Mistral-7B, Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / text generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method (structured revision edits / metric)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Revision Distance (D_Revision)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM (LLM User) to produce structured revision edits (JSON) that would transform a draft text toward a reference or an ideal version; count the number and categories of revision actions to produce a distance score and provide fine-grained edit details.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>D_Revision (number of revision edits); also per-category counts of revision action types</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer count of revision edits produced by the LLM (lower is better). Can be broken down by action categories (e.g., Reference Order Revision, Reference Comparison Revision, Reference Description Revision).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Easy-writing: 147 instances from Wildchat; Challenge-writing: Related Work Generation (RWG) paragraphs from ACL papers (ACL OCL corpus); Reference-free: 41 cases from UltraFeedback</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>For the academic (challenge-writing) task, 20 paragraphs from both methods were selected and assessed by five AI field specialists on content quality, structural coherence, and argumentative strength; RevisionDistance was compared to these expert judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RevisionDistance aligned with existing metrics on easy-writing and provided greater discrimination (larger relative change rate between weak and strong models); in a reference-free subset (UltraFeedback), RevisionDistance aligned with human judgments in ~76% of cases; showed better alignment with human preferences than ROUGE in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Texts chosen by humans required fewer revision edits per RevisionDistance; ROUGE sometimes favored rejected texts while RevisionDistance aligned with human-chosen texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on GPT-4 to generate edits (computational and financial cost); equal weight assigned to each revision action (no dynamic weighting implemented); tested mainly on related-work generation for challenge tasks and shorter email/letter/article tasks for easy-writing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7782.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of n-gram overlap metrics (ROUGE-N, ROUGE-L, etc.) that measure surface-level similarity between a candidate and reference text, commonly used for summarization and generation evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ROUGE: A package for automatic evaluation of summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B, Mixtral-8x7B, GPT-4 (as generators evaluated using ROUGE in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Mistral-7B, Mixtral-8x7B, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / text generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>ROUGE (e.g., ROUGE-1, ROUGE-2)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram overlap (recall/precision/F1) between generated text and reference text to give a numeric similarity score indicating surface similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ROUGE-n (e.g., ROUGE-1, ROUGE-2) scores (often reported as percentage or 0–1 normalized score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>N-gram overlap recall/precision/F1 between candidate and reference; typical scale 0–1 or reported as 0–100.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Used on the paper's easy-writing (147 Wildchat instances) and challenge-writing (RWG) evaluation sets</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared to human judgments in the challenge-writing task; ROUGE sometimes misaligned with human preferences (assigned higher scores to rejected texts in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>ROUGE produced saturated/high scores on easy-writing tasks and sometimes misaligned with human judgment for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Did not align as well as RevisionDistance with human selections in some related-work examples; ROUGE occasionally favored rejected texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Gives context-independent numerical scores and can saturate on easy tasks; lacks explainability about edit types; poor alignment with human preferences in some complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7782.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based evaluation metric that computes similarity between candidate and reference via contextualized token embeddings (typically from BERT) and reports precision, recall, and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B, Mixtral-8x7B, GPT-4 (as generators evaluated using BERTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-based encoder used by metric (paper cites BERT variants); evaluated on generation outputs from Mistral and GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / text generation evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute token-level similarity using contextual embeddings from BERT (or similar), align tokens between candidate and reference, and compute precision/recall/F1 over aligned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore precision/recall/F1 (reported on 0–1 or 0–100 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Cosine similarity-based aggregated scores over token alignments derived from contextual embeddings; typical range 0–1 (or 0–100).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied where input length <= 512 tokens; limited in the paper for long easy-writing texts where many exceeded 512 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared against human judgments in challenge-writing; limited usage due to input length restriction.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Not used on many easy-writing examples due to 512-token input length cap; generally consistent with RevisionDistance on easy-writing when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Aligned with RevisionDistance in easy-writing tasks when applied; limited applicability due to length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Input length capped at 512 tokens (in this study's implementation), preventing its use on longer easy-writing samples; yields a context-independent score without edit-level explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7782.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Score (GPTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based scoring approach that uses a GPT-family model to assign quality scores to generated text, framed as a flexible evaluator that can be prompt-conditioned for desired criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gptscore: Evaluate as you desire.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used both as a generator and as an evaluator in discussions); GPT-based scoring methods referenced</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-family (as per referenced GPTScore paper), GPT-4 used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / LLM evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method (LLM-based scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPT-Score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a GPT-family model to score candidate text by prompting the model to evaluate qualities of interest (e.g., relevance, coherence) and output a numeric or categorical judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>GPT-based score(s) (model-produced numeric or categorical ratings; scale depends on prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Model-assigned score as output by the prompting scheme (scale unspecified in this paper; typically flexible 0–1 or 0–100 depending on prompt design).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to both easy- and challenge-writing outputs in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Compared with human evaluation and RevisionDistance; cited prior work showing LLM evaluation consistent with human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as a baseline comparator; showed consistency with RevisionDistance on easy-writing tasks but RevisionDistance provided more explainable feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-based GPT-Score aligns with human evaluations in prior work; in this paper, RevisionDistance provided complementary and more interpretable output.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dependent on the prompting design and on the LLM used; may not provide fine-grained edit-level feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7782.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BLEU (Bilingual Evaluation Understudy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An n-gram precision-based automatic metric originally developed for machine translation, reporting scores that reflect overlap between candidate and reference text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bleu: a method for automatic evaluation of machine translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B, Mixtral-8x7B, GPT-4 (generators referenced in experiments where BLEU is mentioned as a common baseline metric)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various generator models used; BLEU itself is model-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute n-gram precision between a candidate and reference text, often with brevity penalty, to produce a score representing surface overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BLEU score (typically 0–1 or 0–100 percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Weighted geometric mean of n-gram precisions with brevity penalty; reported as number between 0 and 1 or 0–100.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work as a commonly used automated metric; not explicitly used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as part of related automated metrics but not reported as applied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Surface-level metric that may not correlate with human judgment on higher-level aspects; listed among traditional metrics whose context-independent scores can be insufficient for human-centered evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7782.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoverScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mover-Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic evaluation metric that measures text similarity using contextualized embeddings and Earth Mover's Distance to compare distributions of token embeddings between candidate and reference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (metric is applied to model outputs in literature); not specifically used in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>metric-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mover-Score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Embed tokens with contextual encoders, compute minimal transport (Earth Mover's Distance) between embedding distributions of candidate and reference, and derive a similarity score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>MoverScore value (continuous similarity score, typically normalized 0–1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Distance-based similarity measure derived from Earth Mover's Distance over contextual token embeddings; normalized to a similarity score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; not directly used in the experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned as part of prior automated metrics; like others, provides a context-independent score without edit-level explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7782.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based evaluation metric that uses the likelihood (or conditional generation score) from a pre-trained sequence-to-sequence model (BART) to evaluate generated text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bartscore: Evaluating generated text as text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-based scorer (as per referenced work); not directly used in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>metric-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BARTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use a pre-trained seq2seq model (e.g., BART) to compute conditional log-likelihoods of candidate text given references (or vice versa) and use these scores as evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BARTScore (log-likelihood-based score; scale depends on normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Model log-probability or derived normalized score reflecting how likely a model predicts one text given another; scale varies by implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; not directly applied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Mentioned as part of standard automated metrics; does not provide edit-level explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7782.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DiscoScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiscoScore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated metric designed to evaluate discourse coherence and related generation quality using BERT-based features and discourse-aware measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DiscoScore: Evaluating text generation with BERT and discourse coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based components (metric); not directly used in the experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>metric-agnostic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / discourse evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>DiscoScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Assess generated text via features sensitive to discourse coherence using contextual encoders (e.g., BERT) to produce a coherence-aware evaluation score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>DiscoScore (continuous coherence score)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Aggregated discourse/coherence-aware similarity measure based on BERT encodings; normalized continuous score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Mentioned in related work; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Listed among prior metrics; like others, provides a single numerical score without explicit revision actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7782.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UltraFeedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UltraFeedback dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of feedback-labeled model responses used for reference-free evaluation; in this paper 41 writing-related cases from UltraFeedback were used (each with chosen and rejected responses) to test RevisionDistance in reference-free settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human-chosen vs. rejected responses from dataset; evaluated using RevisionDistance and other metrics</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reference-free evaluation using RevisionDistance on UltraFeedback instances</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Apply RevisionDistance (LLM-generated edits) to paired chosen/rejected responses to see which requires fewer edits and compare alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Alignment rate with human judgments (proportion of cases where RevisionDistance indicates fewer edits for the human-chosen response)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percentage of cases where RevisionDistance agrees with human choice (reported as ~76%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>UltraFeedback (Bartolome et al., 2023) — 41 selected cases used in this study</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>The UltraFeedback cases already contain human choices (chosen vs. rejected); used as ground truth for reference-free alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RevisionDistance aligned with human judgments in 76% of the 41 selected UltraFeedback cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Chosen responses typically required fewer revision edits per RevisionDistance in ~76% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small subset (41 cases) used; UltraFeedback citation in paper lacks explicit title in references; selection may not represent full dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7782.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wildchat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wildchat (user-ChatGPT interactions corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large real-world corpus of ChatGPT interaction logs; the paper extracted 147 email/letter/article instances from Wildchat to form the easy-writing test set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>inthe wildchat: 570k chatgpt interaction logs in the wild.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT logs as human-written ground truth and prompts; evaluated generators' outputs against these instances</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / conversational datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / corpus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reference-based evaluation on easy-writing using Wildchat instances</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use user-ChatGPT interaction examples from Wildchat as reference texts for evaluating generated email/letter/article outputs with metrics including RevisionDistance, ROUGE, BERTScore, GPT-Score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Various (ROUGE, BERTScore, GPT-Score, RevisionDistance)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See individual metric definitions; Wildchat provides reference texts for computing those metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Wildchat (Zhao et al., 2023b) — 147 instances extracted for easy-writing evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RevisionDistance aligned with baseline metrics on the easy-writing set but provided greater discrimination between models (larger relative change rates).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Wildchat-derived texts can be long; BERTScore's 512-token cap prevented its use on many easy-writing examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7782.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Related Work Generation (RWG) testbed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A challenge-writing testbed that uses Related Work section paragraphs (from ACL papers / the ACL OCL corpus) to evaluate knowledge-intensive writing capabilities of LLMs; used to stress-test metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Capturing relations between scientific papers: An abstractive model for related work section generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vanilla GPT-4 and CoT-based GPT-4 (generators for RWG experiments); Mistral-series for easy tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4; CoT-based GPT-4 variant (both GPT-4 family); Mistral-7B/Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / scientific writing generation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>dataset / evaluation task</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reference-based evaluation on Related Work Generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate 'Related Work' paragraphs with LLMs using reference paper metadata and abstracts (from ACL OCL corpus) and evaluate outputs with RevisionDistance and baseline metrics; cohere with human expert assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>RevisionDistance, ROUGE, BERTScore (where applicable), GPT-Score, and human expert ratings</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See individual metrics; human expert ratings on content quality, structural coherence, argumentative strength (expert scale not numerically specified in paper excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Related Work paragraphs extracted from ACL papers (Rohatgi et al., 2023) and prior RWG datasets (Liu et al., 2023; Chen et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Expert analysis: 20 paragraphs from both methods reviewed by five AI field specialists; judged on content quality, structural coherence, and argumentative strength.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>RevisionDistance produced stable evaluations and better distinguished model quality for the RWG challenge-writing task compared to baseline metrics; CoT prompting improved reasoning and led to fewer reasoning-related revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>RevisionDistance aligned with human expert preference (chosen vs. rejected) and highlighted edit categories (order/comparison/description) where models differed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Testbed focused on related-work writing (knowledge-intensive) which may not generalize to all long-form generation tasks; BERTScore input length limits impacted comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7782.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-User Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM User (LLM-as-proxy-user) framework for generating revision edits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural evaluation framework in which an LLM is prompted to act as a proxy user producing structured revision edits (JSON) that transform draft text toward a reference or ideal, enabling explainable metric computation (RevisionDistance).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as the LLM User to generate structured revision edits in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation framework / procedure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM User-driven revision generation (used to compute RevisionDistance)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompt an LLM to produce revision actions in structured JSON describing action_name, revision_description, revision_level, revision_intention and changed snippets; compute counts and categorized edits for evaluation and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>RevisionDistance (derived from outputs of the LLM User); per-action category counts</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Count of generated revision action items; outputs also include descriptive fields enabling qualitative analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied across the paper's datasets: Wildchat easy-writing set, RWG related-work challenge set, UltraFeedback cases for reference-free test</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>LLM User outputs compared to human expert judgments and human-chosen vs rejected labels (e.g., UltraFeedback); expert human review on RWG (5 experts, 20 paragraphs).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Framework produced interpretable revision actions and RevisionDistance scores aligning with human preferences in experiments (e.g., ~76% alignment on UltraFeedback subset).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM User-derived edits corresponded to plausible human editing behaviors and aligned with expert judgments more often than some surface metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on the correctness and objectivity of the LLM used as the proxy user (GPT-4); cost and computation are significant; equal weighting of edits is a simplifying assumption.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7782.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Revision Action Categories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Revision Action Categories (Reference Order/Comparison/Description)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A taxonomy of edit types used to analyze revision edits produced by the LLM User: Reference Order Revision (reordering references), Reference Comparison Revision (integrating comparative discussions), and Reference Description Revision (modifying descriptions of references).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used to produce categorized revision actions for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / qualitative evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criteria / categorical taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Per-category analysis of revision edits</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Classify each generated revision action into categories (order, comparison, description) to provide fine-grained diagnostic feedback about types of improvements needed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Counts and proportions of actions per category (e.g., number of Order edits)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Integer counts and relative frequencies of revision actions in each category; used as diagnostic signals rather than a single numerical score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied primarily on RWG related-work generation outputs and in qualitative analyses reported in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Qualitative analysis compared category distributions between model outputs and human expectations; no numeric inter-rater stats reported for categories.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Category-level analysis identified that CoT-based methods reduced reasoning-related revisions (improved comparison/order dimensions) though some decline in description dimension observed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Revision categories provided insight into which aspects (order, comparison, description) the generated texts deviate from human-written references.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Categories and equal weighting are heuristic; future work suggested developing dynamic weighting for edits and validating categories broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7782.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7782.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Evaluation (Expert)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual assessment by human experts of generated texts, used as a gold-standard for quality judgments and to validate automated metrics; in this paper five AI specialists rated related-work paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (human assessors evaluated outputs from GPT-4 variants and Mistral models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science (NLP / evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation method (human annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Expert human rating</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Domain experts assess LLM-generated paragraphs on dimensions such as content quality, structural coherence, and argumentative strength; their preferences serve as ground truth for metric alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human judgments (categorical chosen/rejected, qualitative ratings on quality/coherence/argumentation)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Human-expert selections and qualitative ratings; paper reports counts (e.g., chosen vs rejected) and uses expert selections for alignment tests (no specific numeric scale detailed in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to the RWG challenge-writing outputs (20 paragraphs from each method reviewed)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Five AI field specialists assessed 20 paragraphs from both methods for content quality, structural coherence, and argumentative strength.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to validate RevisionDistance alignment; RevisionDistance aligned with expert choices where some baseline metrics (e.g., ROUGE) did not.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>RevisionDistance corresponded with expert selections (fewer edits for chosen texts); ROUGE sometimes misaligned by favoring rejected texts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human evaluation is expensive/time-consuming; limited sample size (20 paragraphs × 2 methods) in this study; inter-rater agreement statistics not reported in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>Bertscore: Evaluating text generation with bert. <em>(Rating: 2)</em></li>
                <li>ROUGE: A package for automatic evaluation of summaries. <em>(Rating: 2)</em></li>
                <li>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. <em>(Rating: 2)</em></li>
                <li>Bartscore: Evaluating generated text as text generation. <em>(Rating: 2)</em></li>
                <li>DiscoScore: Evaluating text generation with BERT and discourse coherence. <em>(Rating: 1)</em></li>
                <li>inthe wildchat: 570k chatgpt interaction logs in the wild. <em>(Rating: 2)</em></li>
                <li>Capturing relations between scientific papers: An abstractive model for related work section generation. <em>(Rating: 2)</em></li>
                <li>The acl ocl corpus: advancing open science in computational linguistics. <em>(Rating: 1)</em></li>
                <li>UltraFeedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7782",
    "paper_id": "paper-269032822",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "RevisionDistance",
            "name_full": "Revision Distance",
            "brief_description": "A human-centered text evaluation metric that uses an LLM to generate structured revision edits and counts the number of edits needed to bring a draft closer to a target (reference or ideal) text; smaller values indicate better quality and it yields explainable, fine-grained revision actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as the LLM User to generate revision edits); Mistral-7B and Mixtral-8x7B and GPT-4 used as text generators in experiments",
            "model_size": "GPT-4 (unspecified commercial), Mistral-7B, Mixtral-8x7B",
            "scientific_domain": "computer science (NLP / text generation evaluation)",
            "theory_type": "evaluation method (structured revision edits / metric)",
            "evaluation_method_name": "Revision Distance (D_Revision)",
            "evaluation_method_description": "Prompt an LLM (LLM User) to produce structured revision edits (JSON) that would transform a draft text toward a reference or an ideal version; count the number and categories of revision actions to produce a distance score and provide fine-grained edit details.",
            "evaluation_metric": "D_Revision (number of revision edits); also per-category counts of revision action types",
            "metric_definition": "Integer count of revision edits produced by the LLM (lower is better). Can be broken down by action categories (e.g., Reference Order Revision, Reference Comparison Revision, Reference Description Revision).",
            "dataset_or_benchmark": "Easy-writing: 147 instances from Wildchat; Challenge-writing: Related Work Generation (RWG) paragraphs from ACL papers (ACL OCL corpus); Reference-free: 41 cases from UltraFeedback",
            "human_evaluation_details": "For the academic (challenge-writing) task, 20 paragraphs from both methods were selected and assessed by five AI field specialists on content quality, structural coherence, and argumentative strength; RevisionDistance was compared to these expert judgments.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RevisionDistance aligned with existing metrics on easy-writing and provided greater discrimination (larger relative change rate between weak and strong models); in a reference-free subset (UltraFeedback), RevisionDistance aligned with human judgments in ~76% of cases; showed better alignment with human preferences than ROUGE in some cases.",
            "comparison_to_human_generated": true,
            "comparison_results": "Texts chosen by humans required fewer revision edits per RevisionDistance; ROUGE sometimes favored rejected texts while RevisionDistance aligned with human-chosen texts.",
            "limitations_noted": "Relies on GPT-4 to generate edits (computational and financial cost); equal weight assigned to each revision action (no dynamic weighting implemented); tested mainly on related-work generation for challenge tasks and shorter email/letter/article tasks for easy-writing.",
            "uuid": "e7782.0",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ROUGE",
            "name_full": "ROUGE (Recall-Oriented Understudy for Gisting Evaluation)",
            "brief_description": "A family of n-gram overlap metrics (ROUGE-N, ROUGE-L, etc.) that measure surface-level similarity between a candidate and reference text, commonly used for summarization and generation evaluation.",
            "citation_title": "ROUGE: A package for automatic evaluation of summaries.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B, Mixtral-8x7B, GPT-4 (as generators evaluated using ROUGE in experiments)",
            "model_size": "Mistral-7B, Mixtral-8x7B, GPT-4",
            "scientific_domain": "computer science (NLP / text generation evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "ROUGE (e.g., ROUGE-1, ROUGE-2)",
            "evaluation_method_description": "Compute n-gram overlap (recall/precision/F1) between generated text and reference text to give a numeric similarity score indicating surface similarity.",
            "evaluation_metric": "ROUGE-n (e.g., ROUGE-1, ROUGE-2) scores (often reported as percentage or 0–1 normalized score)",
            "metric_definition": "N-gram overlap recall/precision/F1 between candidate and reference; typical scale 0–1 or reported as 0–100.",
            "dataset_or_benchmark": "Used on the paper's easy-writing (147 Wildchat instances) and challenge-writing (RWG) evaluation sets",
            "human_evaluation_details": "Compared to human judgments in the challenge-writing task; ROUGE sometimes misaligned with human preferences (assigned higher scores to rejected texts in some cases).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "ROUGE produced saturated/high scores on easy-writing tasks and sometimes misaligned with human judgment for complex tasks.",
            "comparison_to_human_generated": true,
            "comparison_results": "Did not align as well as RevisionDistance with human selections in some related-work examples; ROUGE occasionally favored rejected texts.",
            "limitations_noted": "Gives context-independent numerical scores and can saturate on easy tasks; lacks explainability about edit types; poor alignment with human preferences in some complex tasks.",
            "uuid": "e7782.1",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "BERTScore",
            "name_full": "BERTScore",
            "brief_description": "An embedding-based evaluation metric that computes similarity between candidate and reference via contextualized token embeddings (typically from BERT) and reports precision, recall, and F1.",
            "citation_title": "Bertscore: Evaluating text generation with bert.",
            "mention_or_use": "use",
            "model_name": "Mistral-7B, Mixtral-8x7B, GPT-4 (as generators evaluated using BERTScore)",
            "model_size": "BERT-based encoder used by metric (paper cites BERT variants); evaluated on generation outputs from Mistral and GPT-4",
            "scientific_domain": "computer science (NLP / text generation evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "BERTScore",
            "evaluation_method_description": "Compute token-level similarity using contextual embeddings from BERT (or similar), align tokens between candidate and reference, and compute precision/recall/F1 over aligned embeddings.",
            "evaluation_metric": "BERTScore precision/recall/F1 (reported on 0–1 or 0–100 scale)",
            "metric_definition": "Cosine similarity-based aggregated scores over token alignments derived from contextual embeddings; typical range 0–1 (or 0–100).",
            "dataset_or_benchmark": "Applied where input length &lt;= 512 tokens; limited in the paper for long easy-writing texts where many exceeded 512 tokens.",
            "human_evaluation_details": "Compared against human judgments in challenge-writing; limited usage due to input length restriction.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Not used on many easy-writing examples due to 512-token input length cap; generally consistent with RevisionDistance on easy-writing when applicable.",
            "comparison_to_human_generated": true,
            "comparison_results": "Aligned with RevisionDistance in easy-writing tasks when applied; limited applicability due to length constraints.",
            "limitations_noted": "Input length capped at 512 tokens (in this study's implementation), preventing its use on longer easy-writing samples; yields a context-independent score without edit-level explanations.",
            "uuid": "e7782.2",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPTScore",
            "name_full": "GPT-Score (GPTScore)",
            "brief_description": "An LLM-based scoring approach that uses a GPT-family model to assign quality scores to generated text, framed as a flexible evaluator that can be prompt-conditioned for desired criteria.",
            "citation_title": "Gptscore: Evaluate as you desire.",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used both as a generator and as an evaluator in discussions); GPT-based scoring methods referenced",
            "model_size": "GPT-family (as per referenced GPTScore paper), GPT-4 used in experiments",
            "scientific_domain": "computer science (NLP / LLM evaluation)",
            "theory_type": "evaluation method (LLM-based scorer)",
            "evaluation_method_name": "GPT-Score",
            "evaluation_method_description": "Use a GPT-family model to score candidate text by prompting the model to evaluate qualities of interest (e.g., relevance, coherence) and output a numeric or categorical judgment.",
            "evaluation_metric": "GPT-based score(s) (model-produced numeric or categorical ratings; scale depends on prompt)",
            "metric_definition": "Model-assigned score as output by the prompting scheme (scale unspecified in this paper; typically flexible 0–1 or 0–100 depending on prompt design).",
            "dataset_or_benchmark": "Applied to both easy- and challenge-writing outputs in comparisons",
            "human_evaluation_details": "Compared with human evaluation and RevisionDistance; cited prior work showing LLM evaluation consistent with human evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as a baseline comparator; showed consistency with RevisionDistance on easy-writing tasks but RevisionDistance provided more explainable feedback.",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-based GPT-Score aligns with human evaluations in prior work; in this paper, RevisionDistance provided complementary and more interpretable output.",
            "limitations_noted": "Dependent on the prompting design and on the LLM used; may not provide fine-grained edit-level feedback.",
            "uuid": "e7782.3",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "BLEU",
            "name_full": "BLEU (Bilingual Evaluation Understudy)",
            "brief_description": "An n-gram precision-based automatic metric originally developed for machine translation, reporting scores that reflect overlap between candidate and reference text.",
            "citation_title": "Bleu: a method for automatic evaluation of machine translation.",
            "mention_or_use": "mention",
            "model_name": "Mistral-7B, Mixtral-8x7B, GPT-4 (generators referenced in experiments where BLEU is mentioned as a common baseline metric)",
            "model_size": "various generator models used; BLEU itself is model-agnostic",
            "scientific_domain": "computer science (NLP / evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "BLEU",
            "evaluation_method_description": "Compute n-gram precision between a candidate and reference text, often with brevity penalty, to produce a score representing surface overlap.",
            "evaluation_metric": "BLEU score (typically 0–1 or 0–100 percentage)",
            "metric_definition": "Weighted geometric mean of n-gram precisions with brevity penalty; reported as number between 0 and 1 or 0–100.",
            "dataset_or_benchmark": "Mentioned in related work as a commonly used automated metric; not explicitly used in this paper's experiments.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Mentioned as part of related automated metrics but not reported as applied in experiments.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Surface-level metric that may not correlate with human judgment on higher-level aspects; listed among traditional metrics whose context-independent scores can be insufficient for human-centered evaluation.",
            "uuid": "e7782.4",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MoverScore",
            "name_full": "Mover-Score",
            "brief_description": "An automatic evaluation metric that measures text similarity using contextualized embeddings and Earth Mover's Distance to compare distributions of token embeddings between candidate and reference.",
            "citation_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.",
            "mention_or_use": "mention",
            "model_name": "Various (metric is applied to model outputs in literature); not specifically used in this paper's experiments",
            "model_size": "metric-agnostic",
            "scientific_domain": "computer science (NLP / evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Mover-Score",
            "evaluation_method_description": "Embed tokens with contextual encoders, compute minimal transport (Earth Mover's Distance) between embedding distributions of candidate and reference, and derive a similarity score.",
            "evaluation_metric": "MoverScore value (continuous similarity score, typically normalized 0–1)",
            "metric_definition": "Distance-based similarity measure derived from Earth Mover's Distance over contextual token embeddings; normalized to a similarity score.",
            "dataset_or_benchmark": "Mentioned in related work; not directly used in the experiments reported here.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mentioned as part of prior automated metrics; like others, provides a context-independent score without edit-level explanation.",
            "uuid": "e7782.5",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "BARTScore",
            "name_full": "BARTScore",
            "brief_description": "A model-based evaluation metric that uses the likelihood (or conditional generation score) from a pre-trained sequence-to-sequence model (BART) to evaluate generated text quality.",
            "citation_title": "Bartscore: Evaluating generated text as text generation.",
            "mention_or_use": "mention",
            "model_name": "BART-based scorer (as per referenced work); not directly used in this paper's experiments",
            "model_size": "metric-agnostic",
            "scientific_domain": "computer science (NLP / evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "BARTScore",
            "evaluation_method_description": "Use a pre-trained seq2seq model (e.g., BART) to compute conditional log-likelihoods of candidate text given references (or vice versa) and use these scores as evaluation metrics.",
            "evaluation_metric": "BARTScore (log-likelihood-based score; scale depends on normalization)",
            "metric_definition": "Model log-probability or derived normalized score reflecting how likely a model predicts one text given another; scale varies by implementation.",
            "dataset_or_benchmark": "Mentioned in related work; not directly applied in this paper.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Mentioned as part of standard automated metrics; does not provide edit-level explanations.",
            "uuid": "e7782.6",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DiscoScore",
            "name_full": "DiscoScore",
            "brief_description": "An automated metric designed to evaluate discourse coherence and related generation quality using BERT-based features and discourse-aware measures.",
            "citation_title": "DiscoScore: Evaluating text generation with BERT and discourse coherence.",
            "mention_or_use": "mention",
            "model_name": "BERT-based components (metric); not directly used in the experiments",
            "model_size": "metric-agnostic",
            "scientific_domain": "computer science (NLP / discourse evaluation)",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "DiscoScore",
            "evaluation_method_description": "Assess generated text via features sensitive to discourse coherence using contextual encoders (e.g., BERT) to produce a coherence-aware evaluation score.",
            "evaluation_metric": "DiscoScore (continuous coherence score)",
            "metric_definition": "Aggregated discourse/coherence-aware similarity measure based on BERT encodings; normalized continuous score.",
            "dataset_or_benchmark": "Mentioned in related work; not used in experiments in this paper.",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Listed among prior metrics; like others, provides a single numerical score without explicit revision actions.",
            "uuid": "e7782.7",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "UltraFeedback",
            "name_full": "UltraFeedback dataset",
            "brief_description": "A dataset of feedback-labeled model responses used for reference-free evaluation; in this paper 41 writing-related cases from UltraFeedback were used (each with chosen and rejected responses) to test RevisionDistance in reference-free settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human-chosen vs. rejected responses from dataset; evaluated using RevisionDistance and other metrics",
            "model_size": "",
            "scientific_domain": "computer science (NLP / evaluation)",
            "theory_type": "dataset for evaluation",
            "evaluation_method_name": "Reference-free evaluation using RevisionDistance on UltraFeedback instances",
            "evaluation_method_description": "Apply RevisionDistance (LLM-generated edits) to paired chosen/rejected responses to see which requires fewer edits and compare alignment with human judgments.",
            "evaluation_metric": "Alignment rate with human judgments (proportion of cases where RevisionDistance indicates fewer edits for the human-chosen response)",
            "metric_definition": "Percentage of cases where RevisionDistance agrees with human choice (reported as ~76%).",
            "dataset_or_benchmark": "UltraFeedback (Bartolome et al., 2023) — 41 selected cases used in this study",
            "human_evaluation_details": "The UltraFeedback cases already contain human choices (chosen vs. rejected); used as ground truth for reference-free alignment.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RevisionDistance aligned with human judgments in 76% of the 41 selected UltraFeedback cases.",
            "comparison_to_human_generated": true,
            "comparison_results": "Chosen responses typically required fewer revision edits per RevisionDistance in ~76% of cases.",
            "limitations_noted": "Small subset (41 cases) used; UltraFeedback citation in paper lacks explicit title in references; selection may not represent full dataset.",
            "uuid": "e7782.8",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Wildchat",
            "name_full": "Wildchat (user-ChatGPT interactions corpus)",
            "brief_description": "A large real-world corpus of ChatGPT interaction logs; the paper extracted 147 email/letter/article instances from Wildchat to form the easy-writing test set.",
            "citation_title": "inthe wildchat: 570k chatgpt interaction logs in the wild.",
            "mention_or_use": "use",
            "model_name": "ChatGPT logs as human-written ground truth and prompts; evaluated generators' outputs against these instances",
            "model_size": "",
            "scientific_domain": "computer science (NLP / conversational datasets)",
            "theory_type": "dataset / corpus",
            "evaluation_method_name": "Reference-based evaluation on easy-writing using Wildchat instances",
            "evaluation_method_description": "Use user-ChatGPT interaction examples from Wildchat as reference texts for evaluating generated email/letter/article outputs with metrics including RevisionDistance, ROUGE, BERTScore, GPT-Score.",
            "evaluation_metric": "Various (ROUGE, BERTScore, GPT-Score, RevisionDistance)",
            "metric_definition": "See individual metric definitions; Wildchat provides reference texts for computing those metrics.",
            "dataset_or_benchmark": "Wildchat (Zhao et al., 2023b) — 147 instances extracted for easy-writing evaluation",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RevisionDistance aligned with baseline metrics on the easy-writing set but provided greater discrimination between models (larger relative change rates).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Wildchat-derived texts can be long; BERTScore's 512-token cap prevented its use on many easy-writing examples.",
            "uuid": "e7782.9",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RWG",
            "name_full": "Related Work Generation (RWG) testbed",
            "brief_description": "A challenge-writing testbed that uses Related Work section paragraphs (from ACL papers / the ACL OCL corpus) to evaluate knowledge-intensive writing capabilities of LLMs; used to stress-test metrics.",
            "citation_title": "Capturing relations between scientific papers: An abstractive model for related work section generation.",
            "mention_or_use": "use",
            "model_name": "Vanilla GPT-4 and CoT-based GPT-4 (generators for RWG experiments); Mistral-series for easy tasks",
            "model_size": "GPT-4; CoT-based GPT-4 variant (both GPT-4 family); Mistral-7B/Mixtral-8x7B",
            "scientific_domain": "computer science (NLP / scientific writing generation)",
            "theory_type": "dataset / evaluation task",
            "evaluation_method_name": "Reference-based evaluation on Related Work Generation",
            "evaluation_method_description": "Generate 'Related Work' paragraphs with LLMs using reference paper metadata and abstracts (from ACL OCL corpus) and evaluate outputs with RevisionDistance and baseline metrics; cohere with human expert assessment.",
            "evaluation_metric": "RevisionDistance, ROUGE, BERTScore (where applicable), GPT-Score, and human expert ratings",
            "metric_definition": "See individual metrics; human expert ratings on content quality, structural coherence, argumentative strength (expert scale not numerically specified in paper excerpt).",
            "dataset_or_benchmark": "Related Work paragraphs extracted from ACL papers (Rohatgi et al., 2023) and prior RWG datasets (Liu et al., 2023; Chen et al., 2021)",
            "human_evaluation_details": "Expert analysis: 20 paragraphs from both methods reviewed by five AI field specialists; judged on content quality, structural coherence, and argumentative strength.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "RevisionDistance produced stable evaluations and better distinguished model quality for the RWG challenge-writing task compared to baseline metrics; CoT prompting improved reasoning and led to fewer reasoning-related revisions.",
            "comparison_to_human_generated": true,
            "comparison_results": "RevisionDistance aligned with human expert preference (chosen vs. rejected) and highlighted edit categories (order/comparison/description) where models differed.",
            "limitations_noted": "Testbed focused on related-work writing (knowledge-intensive) which may not generalize to all long-form generation tasks; BERTScore input length limits impacted comparison.",
            "uuid": "e7782.10",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-User Framework",
            "name_full": "LLM User (LLM-as-proxy-user) framework for generating revision edits",
            "brief_description": "A procedural evaluation framework in which an LLM is prompted to act as a proxy user producing structured revision edits (JSON) that transform draft text toward a reference or ideal, enabling explainable metric computation (RevisionDistance).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as the LLM User to generate structured revision edits in experiments)",
            "model_size": "GPT-4",
            "scientific_domain": "computer science (NLP / evaluation methodology)",
            "theory_type": "evaluation framework / procedure",
            "evaluation_method_name": "LLM User-driven revision generation (used to compute RevisionDistance)",
            "evaluation_method_description": "Prompt an LLM to produce revision actions in structured JSON describing action_name, revision_description, revision_level, revision_intention and changed snippets; compute counts and categorized edits for evaluation and explanation.",
            "evaluation_metric": "RevisionDistance (derived from outputs of the LLM User); per-action category counts",
            "metric_definition": "Count of generated revision action items; outputs also include descriptive fields enabling qualitative analysis.",
            "dataset_or_benchmark": "Applied across the paper's datasets: Wildchat easy-writing set, RWG related-work challenge set, UltraFeedback cases for reference-free test",
            "human_evaluation_details": "LLM User outputs compared to human expert judgments and human-chosen vs rejected labels (e.g., UltraFeedback); expert human review on RWG (5 experts, 20 paragraphs).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Framework produced interpretable revision actions and RevisionDistance scores aligning with human preferences in experiments (e.g., ~76% alignment on UltraFeedback subset).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM User-derived edits corresponded to plausible human editing behaviors and aligned with expert judgments more often than some surface metrics.",
            "limitations_noted": "Relies on the correctness and objectivity of the LLM used as the proxy user (GPT-4); cost and computation are significant; equal weighting of edits is a simplifying assumption.",
            "uuid": "e7782.11",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Revision Action Categories",
            "name_full": "Revision Action Categories (Reference Order/Comparison/Description)",
            "brief_description": "A taxonomy of edit types used to analyze revision edits produced by the LLM User: Reference Order Revision (reordering references), Reference Comparison Revision (integrating comparative discussions), and Reference Description Revision (modifying descriptions of references).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used to produce categorized revision actions for analysis)",
            "model_size": "GPT-4",
            "scientific_domain": "computer science (NLP / qualitative evaluation)",
            "theory_type": "evaluation criteria / categorical taxonomy",
            "evaluation_method_name": "Per-category analysis of revision edits",
            "evaluation_method_description": "Classify each generated revision action into categories (order, comparison, description) to provide fine-grained diagnostic feedback about types of improvements needed.",
            "evaluation_metric": "Counts and proportions of actions per category (e.g., number of Order edits)",
            "metric_definition": "Integer counts and relative frequencies of revision actions in each category; used as diagnostic signals rather than a single numerical score.",
            "dataset_or_benchmark": "Applied primarily on RWG related-work generation outputs and in qualitative analyses reported in the paper",
            "human_evaluation_details": "Qualitative analysis compared category distributions between model outputs and human expectations; no numeric inter-rater stats reported for categories.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Category-level analysis identified that CoT-based methods reduced reasoning-related revisions (improved comparison/order dimensions) though some decline in description dimension observed.",
            "comparison_to_human_generated": true,
            "comparison_results": "Revision categories provided insight into which aspects (order, comparison, description) the generated texts deviate from human-written references.",
            "limitations_noted": "Categories and equal weighting are heuristic; future work suggested developing dynamic weighting for edits and validating categories broadly.",
            "uuid": "e7782.12",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Human Evaluation (Expert)",
            "name_full": "Human expert evaluation",
            "brief_description": "Manual assessment by human experts of generated texts, used as a gold-standard for quality judgments and to validate automated metrics; in this paper five AI specialists rated related-work paragraphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (human assessors evaluated outputs from GPT-4 variants and Mistral models)",
            "model_size": "",
            "scientific_domain": "computer science (NLP / evaluation)",
            "theory_type": "evaluation method (human annotation)",
            "evaluation_method_name": "Expert human rating",
            "evaluation_method_description": "Domain experts assess LLM-generated paragraphs on dimensions such as content quality, structural coherence, and argumentative strength; their preferences serve as ground truth for metric alignment.",
            "evaluation_metric": "Human judgments (categorical chosen/rejected, qualitative ratings on quality/coherence/argumentation)",
            "metric_definition": "Human-expert selections and qualitative ratings; paper reports counts (e.g., chosen vs rejected) and uses expert selections for alignment tests (no specific numeric scale detailed in excerpt).",
            "dataset_or_benchmark": "Applied to the RWG challenge-writing outputs (20 paragraphs from each method reviewed)",
            "human_evaluation_details": "Five AI field specialists assessed 20 paragraphs from both methods for content quality, structural coherence, and argumentative strength.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used to validate RevisionDistance alignment; RevisionDistance aligned with expert choices where some baseline metrics (e.g., ROUGE) did not.",
            "comparison_to_human_generated": true,
            "comparison_results": "RevisionDistance corresponded with expert selections (fewer edits for chosen texts); ROUGE sometimes misaligned by favoring rejected texts.",
            "limitations_noted": "Human evaluation is expensive/time-consuming; limited sample size (20 paragraphs × 2 methods) in this study; inter-rater agreement statistics not reported in the excerpt.",
            "uuid": "e7782.13",
            "source_info": {
                "paper_title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "Bertscore: Evaluating text generation with bert.",
            "rating": 2,
            "sanitized_title": "bertscore_evaluating_text_generation_with_bert"
        },
        {
            "paper_title": "ROUGE: A package for automatic evaluation of summaries.",
            "rating": 2,
            "sanitized_title": "rouge_a_package_for_automatic_evaluation_of_summaries"
        },
        {
            "paper_title": "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance.",
            "rating": 2,
            "sanitized_title": "moverscore_text_generation_evaluating_with_contextualized_embeddings_and_earth_mover_distance"
        },
        {
            "paper_title": "Bartscore: Evaluating generated text as text generation.",
            "rating": 2,
            "sanitized_title": "bartscore_evaluating_generated_text_as_text_generation"
        },
        {
            "paper_title": "DiscoScore: Evaluating text generation with BERT and discourse coherence.",
            "rating": 1,
            "sanitized_title": "discoscore_evaluating_text_generation_with_bert_and_discourse_coherence"
        },
        {
            "paper_title": "inthe wildchat: 570k chatgpt interaction logs in the wild.",
            "rating": 2,
            "sanitized_title": "inthe_wildchat_570k_chatgpt_interaction_logs_in_the_wild"
        },
        {
            "paper_title": "Capturing relations between scientific papers: An abstractive model for related work section generation.",
            "rating": 2,
            "sanitized_title": "capturing_relations_between_scientific_papers_an_abstractive_model_for_related_work_section_generation"
        },
        {
            "paper_title": "The acl ocl corpus: advancing open science in computational linguistics.",
            "rating": 1,
            "sanitized_title": "the_acl_ocl_corpus_advancing_open_science_in_computational_linguistics"
        },
        {
            "paper_title": "UltraFeedback",
            "rating": 1,
            "sanitized_title": "ultrafeedback"
        }
    ],
    "cost": 0.02003625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications
11 Apr 2024</p>
<p>Yongqiang Ma mayongqiang@whu.edu.cn 
School of Information Management
Wuhan University
China</p>
<p>Institute for Intelligent Computing
Alibaba Group
China</p>
<p>Lizhi Qing 
Institute for Intelligent Computing
Alibaba Group
China</p>
<p>Jiawei Liu 
School of Information Management
Wuhan University
China</p>
<p>Yangyang Kang yangyang.kangyy@alibaba-inc.com 
Institute for Intelligent Computing
Alibaba Group
China</p>
<p>Yue Zhang 
Institute for Intelligent Computing
Alibaba Group
China</p>
<p>Wei Lu weilu@whu.edu.cn 
School of Information Management
Wuhan University
China</p>
<p>Xiaozhong Liu 
Worcester Polytechnic Institute
USA</p>
<p>Qikai Cheng chengqikai0806@163.com 
School of Information Management
Wuhan University
China</p>
<p>From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications
11 Apr 2024418A37AE9306BB32FE4CDADBC51E7B22arXiv:2404.07108v2[cs.CL]
Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications.Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience.Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AIpowered writing assistance applications.Our proposed metric, termed "Revision Distance," utilizes LLMs to suggest revision edits that mimic the human writing process.It is determined by counting the revision edits generated by LLMs.Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score.Our results show that for the easy-writing task, "Revision Distance" is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts.Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle.Furthermore, our metric also holds significant potential for scenarios lacking reference texts.</p>
<p>Introduction</p>
<p>You can't manage what you can't measure well.-Cruz-Cázares et al. 2013 With the continuous development of large language models (LLMs) such as ChatGPT 1 , GPT-4(OpenAI), and Llama (Touvron et al., 2023), a plethora of application research and development work based on LLMs has emerged.</p>
<p>During the model training phase, the main focus is optimizing the model's loss in an isolated Edit distance between "kitten" and "sitting" is 3 1.kitten → sitten (substitute "s" for "k") 2.sitten → sittin (substitute "i" for "e") 3.sittin → sitting (insert "g" at the end)</p>
<p>Rouge Our Metric</p>
<p>Figure 1: Inspired by the classical edit distance metric, our "Revision Distance" D Revision can offer a more human-centered and nuanced metric for text evaluation.As illustrated, the D Revision (Draf t, GroudT ruth) can provide a more transparent evaluation result, benefiting from the generated revision edit details.</p>
<p>environment.However, LLM-based applications should be human-centered, prioritizing user experience and utility.This raises a key question: How do we evaluate LLM-based applications from a human-centric perspective?</p>
<p>Imagining the scenario where developers employ automatic evaluation metrics (Lin, 2004;Papineni et al., 2002;Zhang et al., 2020;Zhao et al., 2019) like ROUGE (Lin, 2004) to evaluate LLMgenerated text for writing assistance debugging.Whereas ROUGE only provides a high-level evaluation score to measure textual surface similarity.Since disregarding end-users, the evaluation result is inadequate and misaligns with user needs and preferences.To address this gap, we explore alternative human-centered evaluation metrics, putting the user at the forefront of our evaluation.</p>
<p>This paper focuses on the prevalent applica-tion scenario for LLMs, specifically, the LLMpowered writing assistant in easy-writing scenarios and challenge-writing scenarios from email, and letter writing to academic writing2 .During the AI-human collaborative writing process, AIgenerated text often requires extended revisions.Additionally, recent studies suggest that LLMs can produce human-like behavior, such as providing human preferences feedback (Bai et al., 2022;Lee et al., 2023), conducting text quality evaluation (Chiang and Lee, 2023;Fu et al., 2023).Therefore, we assume that the LLM can be a proxy user for generating revision edits, aligning with actual human editing behaviors.Drawing from these insights, our proposed metric, RevisionDistance, incorporates the iterative process of user-driven text revision.It quantifies the number of edits a user must take to an LLM-generated text to achieve a predefined quality threshold.</p>
<p>In the reference-based evaluation setting, we compared our metric with ROUGE, BERT-Score, and GPT-Score across two writing tasks: the easywriting task and the challenge-writing task.For each task, we sample texts from two models to form a comparison group.Then we apply text evaluation metrics to assess the text quality.(1) For the easy-writing task, we find that our metric consistently aligns with baseline metrics, supporting the intuition that a stronger model should produce texts with superior evaluation scores.(2) For more challenging tasks, our metrics can still provide stable and reliable evaluation results even if most of the baseline indicators encounter different issues.</p>
<p>In reference-free scenarios, the "Revision Distance" metric aligns closely with human judgment in approximately 76% of cases in the dataset from ultrafeedback dataset (Bartolome et al., 2023).Furthermore, by categorizing the types of edits made, our metric provides a more fine-grained analysis than those metrics that only yield scores.</p>
<p>The contributions are listed as follows: 1) We highlight the significance of the end-user's perspective in the text evaluation in the context of LLM-power writing assistant.2) Aligning with real-world human editing behaviors, we propose a human-centered text evaluation metric, which provides a self-explain and fine-grained insight for developers and end-users.3) Based on broad and various test tasks, we conduct an experiment to demonstrate the utility of our proposed humancentered metrics.</p>
<p>Related Work</p>
<p>The text evaluation methods can be categorized into human evaluation and automated approaches.Human evaluation is widely recognized as the most natural way to evaluate the quality of a given text, which often involves human annotators and qualitative analyses (Clark et al., 2021;Belz et al., 2023).This method is often expensive and timeconsuming work and requires extensive domain expertise for domain-specific scenarios.On the other hand, current automated evaluation methods tend to generate a comprehensive score that is facilitated in comparing new models with established stateof-the-art approaches.These include metrics such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2020), Mover-Score (Zhao et al., 2019), BARTScore (Yuan et al., 2021), and DiscoScore (Zhao et al., 2023a) typically compute a similarity (or dissimilarity) score between a model-generated text and a reference text.</p>
<p>Large language models have been adeptly utilized for roles such as aiding in data annotation (Li et al., 2023) and delivering feedback that mirrors human preferences (Bai et al., 2022;Lee et al., 2023;Pang et al., 2023).For the evaluation stage, Chiang and Lee (2023) found that the LLM evaluation is consistent with the human evaluation results.The GPTScore (Fu et al., 2023) has been proposed to score the model-generated text.Similarly, (Jain et al., 2023) also studied the efficacy of LLMs as multi-dimensional evaluators.</p>
<p>In conclusion, current metrics tend to yield a comprehensive score that detaches the task context for model development and optimization.However, the ultimate application of LLMs is humancentered, prioritizing the user experience and utility.Consequently, a context-independent numerical score is insufficient in LLM application scenarios.Our proposed metric shifts the text evaluation to a human-centered perspective, which incorporates the iterative process of user-driven text revision.For the reference-based evaluation setting, we utilize the human-written text or ChatGPT output as the ground truth.The LLM U ser is designed to produce structured revision edits, improving the consistency of the Y Draf t to the ground-truth text Y .In scenarios where no ground truth text is available, we require the LLM U ser to refine the given text towards an ideal form, as envisioned by the LLM U ser itself3 .These revision edits are produced to improve Y draf t to closer align with the ideal version, which mimics the revision process of human writers.</p>
<p>Results and Discussion</p>
<p>4.1 Evaluation for Reference-based Setting</p>
<p>Task and Dataset</p>
<p>To validate the utility of our proposed metric, we have constructed two distinct datasets to address both the easy-writing task and the challengewriting task.The challenge-writing task refers to the scenario that requires heavy knowledge reasoning and complex concept understanding.For the easy-writing task, we use the task of emails, letters, and articles generation as a testbed.For the challenge-writing task, we employ academic writing as the testbed.The test dataset details in this evaluation setting are described in Appendix A.</p>
<p>Text Generation Models</p>
<p>To assess the discriminative capacity of our revision distance metric, we designed strong and weak writing applications.The terms "strong" and "weak" refer to the generation ability of utilized LLM, as detailed in Table 1.(1) For the easy-writing task, we employ two Mistral-series models (Jiang et al., 2023); (2) For the challenge-writing task, we employ GPT-4 and its variant4 .</p>
<p>Task Level</p>
<p>Weak Strong Easy Mistral-7B Mixtral-8x7B Challenge vanilla GPT-4 CoT-based GPT-4</p>
<p>Result Analysis</p>
<p>As shown in Table 2, our metric shows utility for easy and challenging writing tasks.Different from other metrics, smaller D Revision indicate better text quality.To assess the metric's ability to differentiate between models, we calculate the relative change rate from the "Weak" model to the "Strong" model.Notably, existing metrics have reached saturation for the easy-writing tasks, exhibiting a limited relative change rate regarding the performance of distinct models.Conversely, our metric demonstrates better efficacy in discerning the nuanced capabilities of diverse models.It's observed that D Revision yields a larger change rate, highlighting the enhanced discriminative capacity of our metric.</p>
<p>Additionally, for the complex academic writing task, we conducted a human evaluation 5   Specifically, and denote performance improvement and decline, respectively, from weaker to stronger models.</p>
<p>For the easy-writing task, our D Revision aligns well with other evaluation measures, which shows the utility of our metric.For the more challenging writing task, it offers stable evaluations and better distinguishes model quality, whereas other metrics struggle.The limited input length of Bert-Score, capped at 512 tokens, precluded its use in the easy-writing task where many texts exceeded this limit.</p>
<p>the evaluation results, we categorized texts as "Chosen" or "Rejected."Our D Revision metric aligns with human preferences, indicating superior text quality with fewer revisions for "Chosen Texts."In contrast, the ROUGE metric often misaligns with human judgments, erroneously assigning higher scores to "Rejected Texts."</p>
<p>Evaluation for Reference-free Setting</p>
<p>To demonstrate the performance of our evaluation method in scenarios, where ground truth is unavailable, we extracted 41 cases related to writing tasks from the UltraFeedback dataset (Bartolome et al., 2023).Each case contains a chosen response and a rejected response.</p>
<p>When applied to the selected cases, our "Revision Distance" metric aligns with human judgments in 76% of instances, indicating that chosen responses typically necessitated fewer revisions.</p>
<p>Qualitative Analysis</p>
<p>Based on the analysis of revision edit details, we classify the revision actions into three categories:</p>
<p>(1) Reference Order Revision, (2) Reference Comparison Revision, and (3) Reference Description Revision.The description of three categories is shown in Appendix E.</p>
<p>For complex writing tasks, the challenge often lies in knowledge reasoning of concepts.CoT prompting can dramatically improve the multi-step reasoning abilities of LLMs (Wang et al., 2023).improvements can be attributed to the enhanced knowledge reasoning capabilities of the CoT-based method.There also exists a slight decline in the reference description dimension.In conclusion, the fine-grained analysis revision edits can provide insightful feedback for future model improvement.</p>
<p>As shown in</p>
<p>Conclusion</p>
<p>With the rapid advancement of LLM-based applications, the pivotal question arises: "how can we evaluate LLM-based applications from a humancentered perspective?"Existing evaluation metrics, typically used for model development, merely yield a context-independent numerical score, lacking user relevance.Our research shifts text evaluation from a predominantly model-centered perspective to a human-centered one.</p>
<p>Using the LLM-powered writing assistant as a test scenario, we take a comprehensive experiment on diverse writing tasks to validate the effectiveness and reliability of our "Revision Distance" metric.This metric converts text evaluation into contextualized text revisions, clearly highlighting textual discrepancies and offering users a detailed, transparent rationale for the scores.Our findings demonstrate the metric's applicability and dependability in both reference-based and reference-free contexts.</p>
<p>Limitations</p>
<p>This paper introduces a metric that leverages GPT-4, specifically applied to evaluating LLM-powered writing assistants.However, the computational and financial costs of using GPT-4 are considerable.Exploring the use of a smaller, specialized model to generate initial edits could reduce costs and improve efficiency.</p>
<p>LLMs have a wide array of applications, and for this study, we have chosen the "Related Work" section generation task as a testbed for challenging writing scenarios.As a knowledge-intensive cognitive task, writing the "Related Work" section requires writers to integrate multi-source knowledge into the manuscript.Therefore, writing a comprehensive "Related Work" section is a laborintensive and time-consuming endeavor.Future studies could explore the application of our metric in longer text generation tasks, such as code generation and scientific reports, to validate its effectiveness and applicability across different domains.</p>
<p>In this study, each generated revision item is assigned equal weight.Future research should focus on developing a dynamic revision edit weighting method to evaluate textual differences more finely.</p>
<p>Ethics Statement</p>
<p>In this paper, we propose a new automatic evaluation metric RevisionDistance to evaluate the LLM-generated text in an AI-power writing assistant setting.The positive impact of Revision Distance is that it can provide a more nuanced and self-explain representation of the quality of LLM-generated text.Notably, our metric is humancentered and transparent, which can help demystify the evaluation process for LLM-generated text, making it more accessible to a wider user, including those who are not experts in AI.The negative impact is that over-reliance on RevisionDistance might lead to the overlooking of qualitative aspects of text generation that are harder to quantify, such as creativity.Additionally, if the reference texts within RevisionDistance are biased or of low quality, this could amplify the biases in the LLMsgenerated text.</p>
<p>In the "Related Work" generation task, the model utilizes a set of reference papers, denoted as Ref , along with a description of the user's current research denoted as D, to generate a "Realted Work" draft, denoted as Y Draf t , for the user.In this work, the input data is sourced from the ACL papers (Rohatgi et al., 2023).We select the related work section paragraph as the test data based on the section title.Notably, the abstracts of both the reference papers and the user's target paper are utilized to encapsulate the core content of the respective research, thereby assisting in the generation process.For both Vanilla GPT-4 and CoT-based GPT-4, the temperature is set as 1.0 in the generation process.
Y Draf t = LLM gen (Ref, D)(1)
Vanilla GPT-4 We concatenate the task instruction and metadata of reference and source papers to directly prompt the LLM to get the final "Related Work".</p>
<p>CoT-based GPT-4 We initially prompt the LLM to generate learned relevant knowledge in the training stage (Sun et al., 2023) and then create three segments for different perspectives.Finally, these segments, along with the recalled knowledge and the metadata of the input papers, are integrated to generate the comprehensive "Related Work" paragraph.Based on the intermediate step, the LLM can better capture interrelationships among scientific publications and concepts.</p>
<p>D Example for Revision Action Item</p>
<p>D.1 Revision Regarding the Text Content ###Human-written Text: To deal with the STS task, previous studies have resorted to various features (e.g.word overlap, synonym/antonym), linguistic resources (e.g.WordNet and pre-trained word embeddings) and a wide assortment of learning algorithms (e.g.Support Vector Regression (SVR), regression functions and NNs).Among these works, several techniques extract multiple features of sentences and apply regression functions to estimate these similarity scores (Lai &amp; Hockenmaier, 2014;Zhao et al., 2014;Bjerva et al., 2014;Severyn et al., 2013).Lai &amp; Hockenmaier (2014) analyzed distinctive word relations (e.g.synonyms, antonyms, and hyperonyms) with features based on counts of co-occurences with other words and similarities between captions of images.Zhao et al. (2014) predicted the sentence similarity from syntactic relationship, distinctive { " action_name ": " simplify " , " revision_description ": " Simplified the text by removing details regarding CDSMs and the inclusion of the SICK benchmark .", " revision_level ": " reference " , " revision_intention ": " simplify " , " original_snippet ": " Semantic Textual Similarity ( STS ) , the task of measuring the degree of semantic equivalence between two pieces of text , has been extensively explored in the literature .The development of compositional distributional semantic models ( CDSMs ) forms an integral part of STS investigations , which employ meaning -rich computational systems to better understand and quantify semantic relatedness .The work done by Marelli et al .went a step further , focusing on producing a large English benchmark , SICK , for the deep evaluation of CDSMs , significantly contributing to the body of tools available for STS analysis ." , " revised_snippet ": " In the broad field of Semantic Textual Similarity ( STS ) , earlier works have explored numerous computational models to comprehend and quantify the semantic relatedness between texts ."} ###AI-generated Text: First, Authors B proposed a GPT-based method.Second, Authors A proposed a BERT-based method.</p>
<p>As shown in Table 5, current commonly used metrics tend to assign near-perfect scores to AIgenerated texts, implying a high degree of equiva-lence with their human-written counterparts.However, this fails to capture the underlying structural differences between the texts.As depicted in Figure 4, our metric can better capture the structural differences between the texts.Notably, our metric can offer users a coherent and transparent explanation of the scores assigned, benefiting from the detail of revision actions.</p>
<p>Metric Rouge-1</p>
<p>Rouge-2 Bert-Score Value 100.0 100.0 99.0 { " action_name ": " Reorder " , " revision_description ": " Reordered the sequence of references to match the human -written text " , ... } Figure 4: In this case, the difference between the two texts is the related work statement order, which represents the author's argumentation structure E Revision Categories for the LLM-Generated "Related Work"</p>
<ol>
<li>Reference Order Revision: This refers to reorganizing the sequence of references from various viewpoints such as chronological order, methodological approach, or problem context.</li>
</ol>
<p>Reference Comparison Revision:</p>
<p>This refers to integrating comparative discussions among a collection of references, thereby stating their congruities or discrepancies.</p>
<p>Figure 2 :
2
Figure 2: The evaluation flow of "Revision Distance".We require the LLM U ser to produce results in JSON format with detailed information, In this work, we primarily use the action_name to analyze the revisions.</p>
<p>Figure 3 :
3
Figure 3: An example of content-based revision.The generated revision is about simply the background introduction in the AI-generated text.</p>
<p>Table 1 :
1
The employed models for both writing tasks.</p>
<p>Table 2 :
2
The symbols and indicate directional changes in performance as delivered by evaluation metrics.</p>
<p>Table 3 ,
3
can provide text with fewer revisions related to Order and Comparison issues in "Related work" writing tasks.The
argumentative strength.</p>
<p>Table 3 :
3
The result of revision edits analysis.</p>
<p>Table 5 :
5
Current typical automated metrics' output.</p>
<p>We use the "Related Work" section Generation (RWG)(Liu et al., 2023;Chen et al., 2021) as the testbed for academic writing, which requires heavy knowledge reasoning work and complex concept understanding ability.
This ideal version is not explicitly generated but rather serves as an implicit standard within the revision edits generation prompt.
The models employed in both tasks are detailed in Appendix B and Appendix C, respectively.
  5  We selected 20 paragraphs from both methods for expert analysis. Five AI field specialists assessed the LLM-generated content, focusing on content quality, structural coherence, and
https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
7 https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
. Reference Description Revision: This refers to modifying the description of a particular reference paper, either by elaborating it or by making it more concise.
AcknowledgementsA Dataset for Reference-based Setting• For the easy-writing task, we use the task of emails, letters, and articles generation as testbeds.Specifically, we extracted 147 relevant instances of email, letter, and article writing from Wildchat(Zhao et al., 2023b), a real-world user-ChatGPT interactions corpus, as the easy-writing dataset.• For the challenge-writing task, we employ the "Related Work" generation task(Liu et al., 2023;Chen et al., 2021)B Text Generation Models for Easy Writing TaskWe use the APIs of Mistral-7B 6 and Mistral-8x7B 7 , hosted on Huggingface, to generate responses for the prompts within our constructed easy-writing task dataset.The parameters for the inference process are shown in
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, arXiv:2212.08073Harmlessness from ai feedback. Jared Kaplan, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston; Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown2022arXiv preprint</p>
<p>. Alvaro Bartolome, Gabriel Martin, Daniel Vila, 2023</p>
<p>Non-repeatable experiments and nonreproducible results: The reproducibility crisis in human evaluation in NLP. Anya Belz, Craig Thomson, Ehud Reiter, Simon Mille, 10.18653/v1/2023.findings-acl.226Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 10.18653/v1/2021.acl-long.473Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023</p>
<p>All that's 'human' is not gold: Evaluating human evaluation of generated text. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A Smith, 10.18653/v1/2021.acl-long.565Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>You can't manage right what you can't measure well: Technological innovation efficiency. Claudio Cruz-Cázares, Cristina Bayona-Sáez, Teresa García-Marco, 10.1016/j.respol.2013.03.012Research Policy. 4262013</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Multi-dimensional evaluation of text summarization with in-context learning. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Graham Liu, Chunting Neubig, Zhou, 10.18653/v1/2023.findings-acl.537Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023arXiv preprint</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash, arXiv:2309.002672023arXiv preprint</p>
<p>CoAnnotating: Uncertainty-guided work allocation between human and large language models for data annotation. Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, Diyi Yang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Causal intervention for abstractive related work generation. Jiachang Liu, Qi Zhang, Chongyang Shi, Usman Naseem, Shoujin Wang, Liang Hu, Ivor Tsang, 10.18653/v1/2023.findings-emnlp.141Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>GPT-4 technical report. OpenAI ; OpenAITechnical report</p>
<p>Language model self-improvement by reinforcement learning contemplation. Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu, arXiv:2305.144832023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen Kan, arXiv:2305.14996The acl ocl corpus: advancing open science in computational linguistics. 2023arXiv preprint</p>
<p>Recitation-augmented language models. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou, International Conference on Learning Representations. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>DiscoScore: Evaluating text generation with BERT and discourse coherence. Wei Zhao, Michael Strube, Steffen Eger, 10.18653/v1/2023.eacl-main.278Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023a</p>
<p>(inthe) wildchat: 570k chatgpt interaction logs in the wild. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng, The Twelfth International Conference on Learning Representations. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3041 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3041</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3041</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-252815431</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.05075v1.pdf" target="_blank">Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems</a></p>
                <p><strong>Paper Abstract:</strong> Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3041.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3041.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOLIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving Linear Systems (SOLIS) — Reflection of Thought framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free, inference-time method that elicits implicit numerical reasoning in pre-trained language models by substituting complex operands with simple anchor numbers, collecting model outputs, and recovering the underlying arithmetic expression by formulating and solving a linear system over polynomial basis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to multiple LMs (GPT-3 Code-Davinci-002, BART, T5, POET-SQL, TAPEX, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SOLIS is model-agnostic and is applied at inference time to both vanilla seq2seq LMs (BART LARGE, T5 LARGE ~350M) and large few-shot/generative LMs (GPT-3 Code-Davinci-002, 175B) or reasoning-pretrained models (POET-SQL, TAPEX). SOLIS does not change LM weights; it repeatedly queries the LM with modified inputs (anchor substitutions) to obtain observations.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic expressions and math word problems including AddSub, MultiArith, DROP numeric subset, MathExp; supports up to four operands and combinations of +, -, *, / (compositional arithmetic and multi-step calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Relies on the observation that LMs are far more reliable on small/frequent 'simple' numbers (memorization/statistical familiarity) and implicitly map inputs to arithmetic expressions; SOLIS probes this implicit mapping by anchor substitution and reconstructs an explicit arithmetic expression (symbolic) using linearization (polynomial basis) and linear-system solving.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Preliminary experiments (MathExp) show LM accuracy degrades as integer range and floating precision increase but remains strong on small integers; derived expressions from anchor data often align with question intent (Table 5); applying the derived expression to original complex numbers yields large empirical gains across models/datasets (e.g., BART numeric total F1: 66.4 -> 75.2 with search-based SOLIS; BART hard F1: 30.4 -> 64.8).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Requires that the LM produce sufficiently correct anchor answers; analytic inversion is brittle to noisy anchor outputs; SOLIS cannot recover non-algebraic operations (e.g., max) or expressions outside the formulated polynomial basis; performance depends on assumed priors (max 4 operands, binary ops).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Inference-time intervention: operand proposal (perturbation-based selection), number substitution with random small integer anchors (1–20), repeated LM querying to collect anchor answers, and arithmetic relationship inversion via analytical/search/heuristic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial improvement in arithmetic performance across backbones and settings (zero-shot, few-shot, fine-tuned). Examples: BART (DROP numeric): total F1 +8.8 (66.4 -> 75.2) and hard F1 +34.4 (30.4 -> 64.8) with search-based SOLIS; GPT-3 (AddSub zero-shot chain variant): AddSub 66.6 -> 89.4 (+22.8), MultiArith 63.8 -> 80.0 (+16.2); GPT-3 few-shot improvements also observed (see Tables 2,4,10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported example metrics: BART baseline DROP numeric F1 total 66.4, hard F1 30.4; Analytical SOLIS: hard 46.4 (+16.0), total 69.3 (+2.9); Search SOLIS: hard 64.8 (+30.4), total 75.2 (+8.8); Heuristic SOLIS: hard 52.8 (+22.4), total 71.7 (+5.3). GPT-3 few-shot (Table 10) baseline hard F1 42.5 total 64.7; w. SOLIS (search) hard 59.9 (+17.4) total 68.7 (+4.9). AddSub/MultiArith (GPT-3 zero-shot chain): AddSub 66.6 -> 89.4, MultiArith 63.8 -> 80.0. (All percentages are F1 or EM as reported in respective tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails on expressions not representable in the linearized polynomial basis (e.g., max, argmax); search algorithms explode combinatorially with >4 operands; analytic solver requires mostly-correct anchor answers and full-rank data; dependent on the LM's base numeracy — if LM cannot reflect relevant relationships on anchors, SOLIS fails; noisy/incorrect anchor outputs reduce inversion reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>SOLIS produces explicit symbolic arithmetic expressions which are then applied exactly to complex inputs, making the approach closer to symbolic program execution than end-to-end statistical LM outputs; no direct human-vs-model psychometric comparisons reported, but SOLIS narrows the gap between LM free-text outputs and exact symbolic calculation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3041.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-based algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-based arithmetic relationship inversion (SOLIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exhaustive-search method that enumerates constrained polynomial-basis equations (coefficients in {-1,0,1} for non-constant terms, limited constant set) and selects the expression that best matches LM anchor answers by exact matches and minimal prediction error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used with multiple backbone LMs (BART, T5, GPT-3, POET-SQL, TAPEX)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Algorithm runs over observed anchor groups and LM outputs; does not change the LM and operates solely on collected input-output tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Recovering algebraic expressions combining up to four operands with +, -, *, / (suitable for AddSub, MultiArith, DROP numeric, MathExp).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Exhaustive hypothesis testing over constrained expression space, robustly selecting the expression that best explains anchor outputs even when some anchor answers are noisy by minimizing absolute errors and maximizing exact matches.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirically the most effective solver in the evaluated problem space: produced the largest improvements in experiments (e.g., BART hard F1 +30.4, total +8.8; POET-SQL gains also highest with search). Robust under noisy anchor answers due to selection by error minimization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Search space grows exponentially with number of operands; inefficient for large constant ranges and impractical when operand count increases beyond the evaluated limits (≥5 operands); restricted coefficient ranges limit expressions covered.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Inference-time algorithmic search applied to anchor datasets collected from LM queries.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Gives the largest empirical gains among SOLIS solvers within the defined problem space; recovers correct expressions frequently and yields substantial downstream accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: BART (DROP numeric): search-based hard F1 64.8 (+30.4) total F1 75.2 (+8.8). POET-SQL: hard F1 76.9 (+10.1) total F1 81.4 (+3.0). GPT-3 (few-shot, Table 10): search hard 59.9 (+17.4) total 68.7 (+4.9).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Intractable runtime and memory for many operands or large constant/value ranges; cannot efficiently learn arbitrary constants; limited to expressions expressible in enumerated search space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Functionally equivalent to performing symbolic hypothesis testing over candidate formulas; recovers human-interpretable symbolic expressions that can be executed exactly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3041.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analytical-based algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analytical linear-system solver (SOLIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretically-complete method that formulates polynomial basis terms (including combinations of anchor operands and outputs) into a linear system Pa = 0 and solves for coefficients with an added normalization constraint to recover the arithmetic expression exactly when anchor answers are correct and matrix is full-rank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>used with multiple backbone LMs (BART, T5, POET-SQL, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs matrix P from polynomial basis evaluations over multiple anchor groups (including y) and solves for coefficient vector a via inverse/augmentation when P* is full-rank.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Recovering algebraic expressions built from +, -, *, / up to a pre-specified composition depth (max four operands) where division is eliminated by multiplying denominators to linearize.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Treats LM as a black box that maps anchors to numeric outputs (y); assumes those outputs satisfy an exact algebraic relation expressible in polynomial basis; recovers coefficients analytically from exact equations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The analytic method is theoretically complete for the defined expression space and guides required number of anchor groups; it produced improvements in experiments when anchor answers were accurate (e.g., BART analytic hard F1 46.4 (+16.0)).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Highly sensitive to noise — requires primarily correct anchor outputs; LM anchor answer errors break exact linear relationships and make the analytic solver fail or produce incorrect expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Inference-time deterministic linear algebraic inversion using multiple LM-observed anchor tuples and an added normalization constraint (sum of coefficients = 1).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Moderate gains when anchor outputs are accurate; less robust than search-based and heuristic methods in noisy settings but theoretically complete.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: BART analytic hard F1 46.4 (+16.0) total F1 69.3 (+2.9). POET-SQL analytic hard 73.3 (+6.5) total 80.0 (+1.6) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails with noisy anchor answers or rank-deficient P; requires sampling sufficiently many and diverse anchor groups to get full-rank; cannot tolerate many incorrect anchor outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Mathematically equivalent to solving a system of equations to recover a symbolic expression; when successful, yields exact symbolic forms that humans/symbolic engines can inspect and apply.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3041.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic-based algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic (simulated annealing) solver (SOLIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stochastic optimization approach that searches integer-constrained coefficient vectors a (values in {-1,0,1} etc.) to minimize L1 loss of P a, using simulated annealing perturbations to tolerate noisy LM anchor outputs and scale better than exhaustive search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied across backbone LMs when analytic or exhaustive search is unsuitable</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Initializes integer coefficients randomly, iteratively perturbs coefficients with simulated annealing acceptance criteria, restricts to integer coefficients and enforces at least two non-zero coefficients and one y-related coefficient equal to 1 to avoid degenerate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same arithmetic expression classes as other SOLIS solvers (compositions of +, -, *, / with up to four operands); designed to be robust to noise and more efficient than search.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Optimization-based approximate recovery of symbolic expressions that best explain noisy anchor outputs, trading optimality for robustness and runtime efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Produced consistent improvements over baselines and was more robust than analytical solver when anchor outputs were noisy (e.g., BART heuristic hard F1 52.8 (+22.4), total F1 71.7 (+5.3)).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>May produce suboptimal expressions with limited exploration steps and integer coefficient restrictions; less accurate than exhaustive search in the evaluated small-operand space.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Inference-time heuristic optimization applied to anchor observations.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Gives reliable, middle-ground improvements when search is infeasible or analytic assumptions break; better robustness to noisy anchors but lower peak performance than search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: BART heuristic hard F1 52.8 (+22.4), total F1 71.7 (+5.3). POET-SQL heuristic hard 73.0 (+6.2) total 80.5 (+2.1) (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Susceptible to local optima; integer-coefficient constraints limit expressivity; performance dependent on annealing schedule and number of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Finds approximate symbolic mappings similar to human hypothesis testing but without exhaustively enumerating all candidate formulas; yields interpretable algebraic forms when successful.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3041.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anchor probing (Operand proposal + Number substitution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anchor probing: perturbation-based operand proposal and anchor number substitution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step probing mechanism: (1) perturbation-based operand proposal that detects candidate relevant numbers by measuring LM output sensitivity to small perturbations; (2) number substitution that replaces proposed complex operands with randomly sampled small integer anchors (1–20) preserving order relations to elicit robust LM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied with multiple backbone LMs (BART, T5, POET-SQL, GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Operand proposal perturbs each candidate number by adding ±k·10^p and checks whether LM predictions change (≥3 different outputs out of 10 substitutions indicates relevance). Substitution uses several random anchor groups (group size depends on operand count) to collect anchor answers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Preprocessing/probing for all arithmetic task types used in the paper (DROP numeric, MathExp, AddSub, MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>LMs are sensitive to small changes in numbers in context and are far more accurate on small/frequent integers — using anchors leverages this memorized simple-number behavior to reveal the implicit arithmetic relation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical analysis: LMs show higher accuracy and stability on small integers; expanding anchor range causes minor drop, increasing number of anchor groups improves recovery especially for 4 operands; perturbation-based operand selection reliably identifies relevant operands (Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>If LM lacks underlying numeracy or is unstable on anchors, operand proposal may miss relevant numbers or substitution may yield noisy anchors; maintaining order relationships heuristically may sometimes be insufficient to preserve semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Input-level intervention (perturbation and substitution) at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enables construction of anchor datasets that permit reliable inversion of arithmetic relationships; larger anchor-group sizes and repeated substitution improve performance; anchor range impacts but small integers are preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Design choices: anchor numbers sampled from 1–20; group sizes 6/8/10 corresponding to 2/3/4 operands on DROP; experiments show expanding anchor-group size yields large performance improvement (figures reported in paper); perturbation uses k in {-5..-1,1..5} scaled by operand precision.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Dependent on correct identification of operands; substitution must preserve necessary order relations; insufficient repetitions or poor anchor selection yields noisy linear systems and failed inversion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to experimental probing used in cognitive science: perturb input variables to infer causal/functional relationships; converts LM statistical behavior into data for symbolic expression recovery.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3041.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Code-Davinci-002 (175B) — few-shot generative LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer (approximately 175B parameters) used via API for few-shot and zero-shot prompting, including chain-of-thought styles; evaluated on MathExp, AddSub, MultiArith, and DROP numeric subset in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 Code-Davinci-002 (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer large language model (175B) pre-trained on web-scale corpora; used with few-shot and chain-of-thought prompts in experiments; API temperature set to 0 and max tokens 128 for runs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Synthetic arithmetic expressions (MathExp), math word problems (AddSub, MultiArith), numeric answers in DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Shows strong contextual understanding but poor end-to-end calculation on rare/large/precise numbers; better accuracy on small/frequent integers likely due to memorization/statistical exposure; implicitly produces arithmetic relationships when inputs use familiar numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Preliminary MathExp shows accuracy decreases with increasing integer range and floating precision; few-shot chain-of-thought prompting improves performance but remains brittle on complex numbers; SOLIS improves GPT-3 substantially by eliciting expressions from anchor behavior (e.g., AddSub zero-shot chain: 66.6 -> 89.4 with SOLIS).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Anchor answers from GPT-3 are sometimes incorrect even for simple anchors; chain-of-thought helps but not always sufficient; model still suffers carry, missing high-digit, extra-digit and precision errors (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot/chain-of-thought prompting; SOLIS anchor probing and inversion applied at inference time (search-based default).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>SOLIS combined with GPT-3 gives large gains in AddSub/MultiArith especially in zero-shot/zero-shot-chain settings (AddSub 66.6 -> 89.4; MultiArith 63.8 -> 80.0), and modest gains when chain-of-thought few-shot already strong (Chain 88.4 -> 90.9 AddSub).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: GPT-3 baseline (few-shot/zero-shot variants) reported in Table 4 and 10: Zero-shot Chain AddSub 66.6, MultiArith 63.8; w. SOLIS 89.4 and 80.0 respectively. Few-shot Chain AddSub 88.4 -> 90.9, MultiArith 96.7 -> 98.7. On DROP numeric subset baseline F1 ~64.7 overall, hard 42.5; with search SOLIS overall 68.7, hard 59.9.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Carry errors, missing high digits, extra integer digits, extra float digits, insufficient precision; instability between numerically similar examples (e.g., 512+128 vs 513+129).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct human comparison; SOLIS’ approach yields symbolic expressions that, when executed, give exact results akin to calculator/symbolic solutions and outperform raw GPT-3 numeric outputs in many cases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3041.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3041.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART / T5 / POET-SQL / TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla and reasoning-pretrained sequence-to-sequence LMs (BART LARGE, T5 LARGE, POET-SQL, TAPEX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller (~350M) seq2seq transformer models (BART, T5) and reasoning-pretrained models (POET-SQL, TAPEX) fine-tuned on DROP; used as backbones to evaluate SOLIS gains when models are fine-tuned (or reasoning-pretrained) rather than giant few-shot LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART LARGE (~350M), T5 LARGE (~350M), POET-SQL (~350M scale), TAPEX (reasoning-pretrained table LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder architectures fine-tuned on DROP; number handling uses character-level representation and reverse-decoding to help arithmetic carry; POET-SQL and TAPEX are reasoning-pretrained variants with improved numeric reasoning priors.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>DROP numeric subset (reading-comprehension-style numeric answers), MathExp synthetic arithmetic expressions when evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>These models retain some numerical understanding from fine-tuning/pretraining but are brittle on large/precise numbers; SOLIS leverages their contextual understanding and reliable small-number behavior to extract arithmetic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>SOLIS yields consistent improvements across these backbones: T5 F1 improved up to +8.9 (e.g., 64.6 -> 73.5 with SOLIS in one configuration); BART search-based SOLIS improved hard F1 from 30.4 to 64.8 and total F1 from 66.4 to 75.2; POET-SQL and TAPEX also show modest gains (POET-SQL total F1 80.0 -> 81.4 with search).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Even fine-tuned or reasoning-pretrained LMs still make calculation errors (carry, precision); POET-SQL/TAPEX show smaller relative gains implying pretraining partially addresses numeracy but does not eliminate limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Fine-tuning baseline + SOLIS applied at inference time; design choices included character-level number representation and reverse decoding to mimic carry.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>SOLIS boosts vanilla LMs more (larger relative gains) and yields smaller but consistent gains for reasoning-pretrained models; e.g., T5 total F1 +8.9 in one setting, POET-SQL total +3.0 with search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: BART baseline DROP numeric F1 total 66.4, hard 30.4; w. search SOLIS total 75.2, hard 64.8. T5 baseline total F1 64.6 -> w. SOLIS 73.5 (+8.9). POET-SQL baseline total 78.4 -> w. SOLIS 81.4 (+3.0) (see Table 2 and Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Same arithmetic failure classes as giant LMs (carry, precision, missing digit); search-based algorithm has trouble covering expressions with arbitrary constants (heuristic addition of 100 for percent cases used).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Fine-tuned LMs improved by SOLIS approach behaviorally move closer to symbolic program execution on numeric outputs since SOLIS recovers and applies explicit expressions, but no direct human benchmarks reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do nlp models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
                <li>Giving bert a calculator: Finding operations and arguments with reading comprehension <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3041",
    "paper_id": "paper-252815431",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "SOLIS",
            "name_full": "Solving Linear Systems (SOLIS) — Reflection of Thought framework",
            "brief_description": "A training-free, inference-time method that elicits implicit numerical reasoning in pre-trained language models by substituting complex operands with simple anchor numbers, collecting model outputs, and recovering the underlying arithmetic expression by formulating and solving a linear system over polynomial basis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to multiple LMs (GPT-3 Code-Davinci-002, BART, T5, POET-SQL, TAPEX, etc.)",
            "model_description": "SOLIS is model-agnostic and is applied at inference time to both vanilla seq2seq LMs (BART LARGE, T5 LARGE ~350M) and large few-shot/generative LMs (GPT-3 Code-Davinci-002, 175B) or reasoning-pretrained models (POET-SQL, TAPEX). SOLIS does not change LM weights; it repeatedly queries the LM with modified inputs (anchor substitutions) to obtain observations.",
            "arithmetic_task_type": "Arithmetic expressions and math word problems including AddSub, MultiArith, DROP numeric subset, MathExp; supports up to four operands and combinations of +, -, *, / (compositional arithmetic and multi-step calculations).",
            "reported_mechanism": "Relies on the observation that LMs are far more reliable on small/frequent 'simple' numbers (memorization/statistical familiarity) and implicitly map inputs to arithmetic expressions; SOLIS probes this implicit mapping by anchor substitution and reconstructs an explicit arithmetic expression (symbolic) using linearization (polynomial basis) and linear-system solving.",
            "evidence_for_mechanism": "Preliminary experiments (MathExp) show LM accuracy degrades as integer range and floating precision increase but remains strong on small integers; derived expressions from anchor data often align with question intent (Table 5); applying the derived expression to original complex numbers yields large empirical gains across models/datasets (e.g., BART numeric total F1: 66.4 -&gt; 75.2 with search-based SOLIS; BART hard F1: 30.4 -&gt; 64.8).",
            "evidence_against_mechanism": "Requires that the LM produce sufficiently correct anchor answers; analytic inversion is brittle to noisy anchor outputs; SOLIS cannot recover non-algebraic operations (e.g., max) or expressions outside the formulated polynomial basis; performance depends on assumed priors (max 4 operands, binary ops).",
            "intervention_type": "Inference-time intervention: operand proposal (perturbation-based selection), number substitution with random small integer anchors (1–20), repeated LM querying to collect anchor answers, and arithmetic relationship inversion via analytical/search/heuristic solvers.",
            "effect_of_intervention": "Substantial improvement in arithmetic performance across backbones and settings (zero-shot, few-shot, fine-tuned). Examples: BART (DROP numeric): total F1 +8.8 (66.4 -&gt; 75.2) and hard F1 +34.4 (30.4 -&gt; 64.8) with search-based SOLIS; GPT-3 (AddSub zero-shot chain variant): AddSub 66.6 -&gt; 89.4 (+22.8), MultiArith 63.8 -&gt; 80.0 (+16.2); GPT-3 few-shot improvements also observed (see Tables 2,4,10).",
            "performance_metrics": "Reported example metrics: BART baseline DROP numeric F1 total 66.4, hard F1 30.4; Analytical SOLIS: hard 46.4 (+16.0), total 69.3 (+2.9); Search SOLIS: hard 64.8 (+30.4), total 75.2 (+8.8); Heuristic SOLIS: hard 52.8 (+22.4), total 71.7 (+5.3). GPT-3 few-shot (Table 10) baseline hard F1 42.5 total 64.7; w. SOLIS (search) hard 59.9 (+17.4) total 68.7 (+4.9). AddSub/MultiArith (GPT-3 zero-shot chain): AddSub 66.6 -&gt; 89.4, MultiArith 63.8 -&gt; 80.0. (All percentages are F1 or EM as reported in respective tables.)",
            "notable_failure_modes": "Fails on expressions not representable in the linearized polynomial basis (e.g., max, argmax); search algorithms explode combinatorially with &gt;4 operands; analytic solver requires mostly-correct anchor answers and full-rank data; dependent on the LM's base numeracy — if LM cannot reflect relevant relationships on anchors, SOLIS fails; noisy/incorrect anchor outputs reduce inversion reliability.",
            "comparison_to_humans_or_symbolic": "SOLIS produces explicit symbolic arithmetic expressions which are then applied exactly to complex inputs, making the approach closer to symbolic program execution than end-to-end statistical LM outputs; no direct human-vs-model psychometric comparisons reported, but SOLIS narrows the gap between LM free-text outputs and exact symbolic calculation.",
            "uuid": "e3041.0"
        },
        {
            "name_short": "Search-based algorithm",
            "name_full": "Search-based arithmetic relationship inversion (SOLIS)",
            "brief_description": "An exhaustive-search method that enumerates constrained polynomial-basis equations (coefficients in {-1,0,1} for non-constant terms, limited constant set) and selects the expression that best matches LM anchor answers by exact matches and minimal prediction error.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "used with multiple backbone LMs (BART, T5, GPT-3, POET-SQL, TAPEX)",
            "model_description": "Algorithm runs over observed anchor groups and LM outputs; does not change the LM and operates solely on collected input-output tuples.",
            "arithmetic_task_type": "Recovering algebraic expressions combining up to four operands with +, -, *, / (suitable for AddSub, MultiArith, DROP numeric, MathExp).",
            "reported_mechanism": "Exhaustive hypothesis testing over constrained expression space, robustly selecting the expression that best explains anchor outputs even when some anchor answers are noisy by minimizing absolute errors and maximizing exact matches.",
            "evidence_for_mechanism": "Empirically the most effective solver in the evaluated problem space: produced the largest improvements in experiments (e.g., BART hard F1 +30.4, total +8.8; POET-SQL gains also highest with search). Robust under noisy anchor answers due to selection by error minimization.",
            "evidence_against_mechanism": "Search space grows exponentially with number of operands; inefficient for large constant ranges and impractical when operand count increases beyond the evaluated limits (≥5 operands); restricted coefficient ranges limit expressions covered.",
            "intervention_type": "Inference-time algorithmic search applied to anchor datasets collected from LM queries.",
            "effect_of_intervention": "Gives the largest empirical gains among SOLIS solvers within the defined problem space; recovers correct expressions frequently and yields substantial downstream accuracy improvements.",
            "performance_metrics": "Examples: BART (DROP numeric): search-based hard F1 64.8 (+30.4) total F1 75.2 (+8.8). POET-SQL: hard F1 76.9 (+10.1) total F1 81.4 (+3.0). GPT-3 (few-shot, Table 10): search hard 59.9 (+17.4) total 68.7 (+4.9).",
            "notable_failure_modes": "Intractable runtime and memory for many operands or large constant/value ranges; cannot efficiently learn arbitrary constants; limited to expressions expressible in enumerated search space.",
            "comparison_to_humans_or_symbolic": "Functionally equivalent to performing symbolic hypothesis testing over candidate formulas; recovers human-interpretable symbolic expressions that can be executed exactly.",
            "uuid": "e3041.1"
        },
        {
            "name_short": "Analytical-based algorithm",
            "name_full": "Analytical linear-system solver (SOLIS)",
            "brief_description": "A theoretically-complete method that formulates polynomial basis terms (including combinations of anchor operands and outputs) into a linear system Pa = 0 and solves for coefficients with an added normalization constraint to recover the arithmetic expression exactly when anchor answers are correct and matrix is full-rank.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "used with multiple backbone LMs (BART, T5, POET-SQL, GPT-3)",
            "model_description": "Constructs matrix P from polynomial basis evaluations over multiple anchor groups (including y) and solves for coefficient vector a via inverse/augmentation when P* is full-rank.",
            "arithmetic_task_type": "Recovering algebraic expressions built from +, -, *, / up to a pre-specified composition depth (max four operands) where division is eliminated by multiplying denominators to linearize.",
            "reported_mechanism": "Treats LM as a black box that maps anchors to numeric outputs (y); assumes those outputs satisfy an exact algebraic relation expressible in polynomial basis; recovers coefficients analytically from exact equations.",
            "evidence_for_mechanism": "The analytic method is theoretically complete for the defined expression space and guides required number of anchor groups; it produced improvements in experiments when anchor answers were accurate (e.g., BART analytic hard F1 46.4 (+16.0)).",
            "evidence_against_mechanism": "Highly sensitive to noise — requires primarily correct anchor outputs; LM anchor answer errors break exact linear relationships and make the analytic solver fail or produce incorrect expressions.",
            "intervention_type": "Inference-time deterministic linear algebraic inversion using multiple LM-observed anchor tuples and an added normalization constraint (sum of coefficients = 1).",
            "effect_of_intervention": "Moderate gains when anchor outputs are accurate; less robust than search-based and heuristic methods in noisy settings but theoretically complete.",
            "performance_metrics": "Examples: BART analytic hard F1 46.4 (+16.0) total F1 69.3 (+2.9). POET-SQL analytic hard 73.3 (+6.5) total 80.0 (+1.6) (Table 2).",
            "notable_failure_modes": "Fails with noisy anchor answers or rank-deficient P; requires sampling sufficiently many and diverse anchor groups to get full-rank; cannot tolerate many incorrect anchor outputs.",
            "comparison_to_humans_or_symbolic": "Mathematically equivalent to solving a system of equations to recover a symbolic expression; when successful, yields exact symbolic forms that humans/symbolic engines can inspect and apply.",
            "uuid": "e3041.2"
        },
        {
            "name_short": "Heuristic-based algorithm",
            "name_full": "Heuristic (simulated annealing) solver (SOLIS)",
            "brief_description": "A stochastic optimization approach that searches integer-constrained coefficient vectors a (values in {-1,0,1} etc.) to minimize L1 loss of P a, using simulated annealing perturbations to tolerate noisy LM anchor outputs and scale better than exhaustive search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied across backbone LMs when analytic or exhaustive search is unsuitable",
            "model_description": "Initializes integer coefficients randomly, iteratively perturbs coefficients with simulated annealing acceptance criteria, restricts to integer coefficients and enforces at least two non-zero coefficients and one y-related coefficient equal to 1 to avoid degenerate solutions.",
            "arithmetic_task_type": "Same arithmetic expression classes as other SOLIS solvers (compositions of +, -, *, / with up to four operands); designed to be robust to noise and more efficient than search.",
            "reported_mechanism": "Optimization-based approximate recovery of symbolic expressions that best explain noisy anchor outputs, trading optimality for robustness and runtime efficiency.",
            "evidence_for_mechanism": "Produced consistent improvements over baselines and was more robust than analytical solver when anchor outputs were noisy (e.g., BART heuristic hard F1 52.8 (+22.4), total F1 71.7 (+5.3)).",
            "evidence_against_mechanism": "May produce suboptimal expressions with limited exploration steps and integer coefficient restrictions; less accurate than exhaustive search in the evaluated small-operand space.",
            "intervention_type": "Inference-time heuristic optimization applied to anchor observations.",
            "effect_of_intervention": "Gives reliable, middle-ground improvements when search is infeasible or analytic assumptions break; better robustness to noisy anchors but lower peak performance than search.",
            "performance_metrics": "Examples: BART heuristic hard F1 52.8 (+22.4), total F1 71.7 (+5.3). POET-SQL heuristic hard 73.0 (+6.2) total 80.5 (+2.1) (Table 2).",
            "notable_failure_modes": "Susceptible to local optima; integer-coefficient constraints limit expressivity; performance dependent on annealing schedule and number of steps.",
            "comparison_to_humans_or_symbolic": "Finds approximate symbolic mappings similar to human hypothesis testing but without exhaustively enumerating all candidate formulas; yields interpretable algebraic forms when successful.",
            "uuid": "e3041.3"
        },
        {
            "name_short": "Anchor probing (Operand proposal + Number substitution)",
            "name_full": "Anchor probing: perturbation-based operand proposal and anchor number substitution",
            "brief_description": "A two-step probing mechanism: (1) perturbation-based operand proposal that detects candidate relevant numbers by measuring LM output sensitivity to small perturbations; (2) number substitution that replaces proposed complex operands with randomly sampled small integer anchors (1–20) preserving order relations to elicit robust LM outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied with multiple backbone LMs (BART, T5, POET-SQL, GPT-3)",
            "model_description": "Operand proposal perturbs each candidate number by adding ±k·10^p and checks whether LM predictions change (≥3 different outputs out of 10 substitutions indicates relevance). Substitution uses several random anchor groups (group size depends on operand count) to collect anchor answers.",
            "arithmetic_task_type": "Preprocessing/probing for all arithmetic task types used in the paper (DROP numeric, MathExp, AddSub, MultiArith).",
            "reported_mechanism": "LMs are sensitive to small changes in numbers in context and are far more accurate on small/frequent integers — using anchors leverages this memorized simple-number behavior to reveal the implicit arithmetic relation.",
            "evidence_for_mechanism": "Empirical analysis: LMs show higher accuracy and stability on small integers; expanding anchor range causes minor drop, increasing number of anchor groups improves recovery especially for 4 operands; perturbation-based operand selection reliably identifies relevant operands (Appendix B).",
            "evidence_against_mechanism": "If LM lacks underlying numeracy or is unstable on anchors, operand proposal may miss relevant numbers or substitution may yield noisy anchors; maintaining order relationships heuristically may sometimes be insufficient to preserve semantics.",
            "intervention_type": "Input-level intervention (perturbation and substitution) at inference time.",
            "effect_of_intervention": "Enables construction of anchor datasets that permit reliable inversion of arithmetic relationships; larger anchor-group sizes and repeated substitution improve performance; anchor range impacts but small integers are preferred.",
            "performance_metrics": "Design choices: anchor numbers sampled from 1–20; group sizes 6/8/10 corresponding to 2/3/4 operands on DROP; experiments show expanding anchor-group size yields large performance improvement (figures reported in paper); perturbation uses k in {-5..-1,1..5} scaled by operand precision.",
            "notable_failure_modes": "Dependent on correct identification of operands; substitution must preserve necessary order relations; insufficient repetitions or poor anchor selection yields noisy linear systems and failed inversion.",
            "comparison_to_humans_or_symbolic": "Analogous to experimental probing used in cognitive science: perturb input variables to infer causal/functional relationships; converts LM statistical behavior into data for symbolic expression recovery.",
            "uuid": "e3041.4"
        },
        {
            "name_short": "GPT-3 (Code-Davinci-002)",
            "name_full": "GPT-3 Code-Davinci-002 (175B) — few-shot generative LLM",
            "brief_description": "A large autoregressive transformer (approximately 175B parameters) used via API for few-shot and zero-shot prompting, including chain-of-thought styles; evaluated on MathExp, AddSub, MultiArith, and DROP numeric subset in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 Code-Davinci-002 (175B)",
            "model_description": "Autoregressive transformer large language model (175B) pre-trained on web-scale corpora; used with few-shot and chain-of-thought prompts in experiments; API temperature set to 0 and max tokens 128 for runs.",
            "arithmetic_task_type": "Synthetic arithmetic expressions (MathExp), math word problems (AddSub, MultiArith), numeric answers in DROP.",
            "reported_mechanism": "Shows strong contextual understanding but poor end-to-end calculation on rare/large/precise numbers; better accuracy on small/frequent integers likely due to memorization/statistical exposure; implicitly produces arithmetic relationships when inputs use familiar numbers.",
            "evidence_for_mechanism": "Preliminary MathExp shows accuracy decreases with increasing integer range and floating precision; few-shot chain-of-thought prompting improves performance but remains brittle on complex numbers; SOLIS improves GPT-3 substantially by eliciting expressions from anchor behavior (e.g., AddSub zero-shot chain: 66.6 -&gt; 89.4 with SOLIS).",
            "evidence_against_mechanism": "Anchor answers from GPT-3 are sometimes incorrect even for simple anchors; chain-of-thought helps but not always sufficient; model still suffers carry, missing high-digit, extra-digit and precision errors (Table 11).",
            "intervention_type": "Few-shot/chain-of-thought prompting; SOLIS anchor probing and inversion applied at inference time (search-based default).",
            "effect_of_intervention": "SOLIS combined with GPT-3 gives large gains in AddSub/MultiArith especially in zero-shot/zero-shot-chain settings (AddSub 66.6 -&gt; 89.4; MultiArith 63.8 -&gt; 80.0), and modest gains when chain-of-thought few-shot already strong (Chain 88.4 -&gt; 90.9 AddSub).",
            "performance_metrics": "Examples: GPT-3 baseline (few-shot/zero-shot variants) reported in Table 4 and 10: Zero-shot Chain AddSub 66.6, MultiArith 63.8; w. SOLIS 89.4 and 80.0 respectively. Few-shot Chain AddSub 88.4 -&gt; 90.9, MultiArith 96.7 -&gt; 98.7. On DROP numeric subset baseline F1 ~64.7 overall, hard 42.5; with search SOLIS overall 68.7, hard 59.9.",
            "notable_failure_modes": "Carry errors, missing high digits, extra integer digits, extra float digits, insufficient precision; instability between numerically similar examples (e.g., 512+128 vs 513+129).",
            "comparison_to_humans_or_symbolic": "No direct human comparison; SOLIS’ approach yields symbolic expressions that, when executed, give exact results akin to calculator/symbolic solutions and outperform raw GPT-3 numeric outputs in many cases.",
            "uuid": "e3041.5"
        },
        {
            "name_short": "BART / T5 / POET-SQL / TAPEX",
            "name_full": "Vanilla and reasoning-pretrained sequence-to-sequence LMs (BART LARGE, T5 LARGE, POET-SQL, TAPEX)",
            "brief_description": "Smaller (~350M) seq2seq transformer models (BART, T5) and reasoning-pretrained models (POET-SQL, TAPEX) fine-tuned on DROP; used as backbones to evaluate SOLIS gains when models are fine-tuned (or reasoning-pretrained) rather than giant few-shot LMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BART LARGE (~350M), T5 LARGE (~350M), POET-SQL (~350M scale), TAPEX (reasoning-pretrained table LM)",
            "model_description": "Encoder-decoder architectures fine-tuned on DROP; number handling uses character-level representation and reverse-decoding to help arithmetic carry; POET-SQL and TAPEX are reasoning-pretrained variants with improved numeric reasoning priors.",
            "arithmetic_task_type": "DROP numeric subset (reading-comprehension-style numeric answers), MathExp synthetic arithmetic expressions when evaluated.",
            "reported_mechanism": "These models retain some numerical understanding from fine-tuning/pretraining but are brittle on large/precise numbers; SOLIS leverages their contextual understanding and reliable small-number behavior to extract arithmetic expressions.",
            "evidence_for_mechanism": "SOLIS yields consistent improvements across these backbones: T5 F1 improved up to +8.9 (e.g., 64.6 -&gt; 73.5 with SOLIS in one configuration); BART search-based SOLIS improved hard F1 from 30.4 to 64.8 and total F1 from 66.4 to 75.2; POET-SQL and TAPEX also show modest gains (POET-SQL total F1 80.0 -&gt; 81.4 with search).",
            "evidence_against_mechanism": "Even fine-tuned or reasoning-pretrained LMs still make calculation errors (carry, precision); POET-SQL/TAPEX show smaller relative gains implying pretraining partially addresses numeracy but does not eliminate limitations.",
            "intervention_type": "Fine-tuning baseline + SOLIS applied at inference time; design choices included character-level number representation and reverse decoding to mimic carry.",
            "effect_of_intervention": "SOLIS boosts vanilla LMs more (larger relative gains) and yields smaller but consistent gains for reasoning-pretrained models; e.g., T5 total F1 +8.9 in one setting, POET-SQL total +3.0 with search.",
            "performance_metrics": "Examples: BART baseline DROP numeric F1 total 66.4, hard 30.4; w. search SOLIS total 75.2, hard 64.8. T5 baseline total F1 64.6 -&gt; w. SOLIS 73.5 (+8.9). POET-SQL baseline total 78.4 -&gt; w. SOLIS 81.4 (+3.0) (see Table 2 and Table 3).",
            "notable_failure_modes": "Same arithmetic failure classes as giant LMs (carry, precision, missing digit); search-based algorithm has trouble covering expressions with arbitrary constants (heuristic addition of 100 for percent cases used).",
            "comparison_to_humans_or_symbolic": "Fine-tuned LMs improved by SOLIS approach behaviorally move closer to symbolic program execution on numeric outputs since SOLIS recovers and applies explicit expressions, but no direct human benchmarks reported.",
            "uuid": "e3041.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do nlp models know numbers? probing numeracy in embeddings",
            "rating": 2,
            "sanitized_title": "do_nlp_models_know_numbers_probing_numeracy_in_embeddings"
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 2,
            "sanitized_title": "impact_of_pretraining_term_frequencies_on_fewshot_reasoning"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Giving bert a calculator: Finding operations and arguments with reading comprehension",
            "rating": 2,
            "sanitized_title": "giving_bert_a_calculator_finding_operations_and_arguments_with_reading_comprehension"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2,
            "sanitized_title": "injecting_numerical_reasoning_skills_into_language_models"
        }
    ],
    "cost": 0.01956625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</p>
<p>Fan Zhou 
Shanghai Jiao Tong University</p>
<p>Haoyu Dong 
Microsoft Research Asia</p>
<p>Qian Liu liuqian@sea.com 
Sea AI Lab</p>
<p>Zhoujun Cheng 
Shanghai Jiao Tong University</p>
<p>Shi Han 
Microsoft Research Asia</p>
<p>Dongmei Zhang dongmeiz@microsoft.com 
Microsoft Research Asia</p>
<p>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS
Preprint
Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios.Fortunately, by reverse thinking, we have a positive point of view: with the exact same context, LMs are significantly more accurate and stable on simple numbers -typically small integers that appear frequently in the pre-training corpora -than complex numbers, indicating that LMs have a strong aptitude for bearing arithmetic results of simple numbers in mind during pre-training. This motivates us to leverage simple numbers as "anchors" to probe the implicitly inferred arithmetic expressions from language models and then explicitly apply the expressions on complex numbers.</p>
<p>INTRODUCTION</p>
<p>Language Models (LMs) have demonstrated great success on a wide range of natural language tasks (Devlin et al., 2018;Brown et al., 2020b;Chowdhery et al., 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022;He et al., 2022). But when it comes to reasoning about numbers, the crucial parts of text, tables, and knowledge bases, the performance of LMs slumps. Even rational numbers, a small subset of real numbers, readily constitute an infinite space that cannot be completely covered by pre-training corpora, hence posing a significant obstacle to LMs. Recent works have shown strong context understanding capabilities of LMs in numerical reasoning datasets (Dua et al., 2019;Cobbe et al., 2021), but LMs are still far from being robust on end-to-end numerical calculation: as numbers get larger and more complicated, the likelihood of failure for LMs increases, e.g., it is difficult for them to calculate the result of 8, 534.5 + 17.85; even for number additions between small numbers, e.g., 512 + 128 and 513 + 129, LMs are not stable enough to consistently produce the correct result. Similar observations are also reported by Razeghi et al. (2022), showing that LMs struggle to conduct end-to-end calculations on numbers that rarely appear in pre-training corpora. Figure 1: The illustration of our proposed framework, which elicits numerical reasoning in language models via Solving Linear Systems (SOLIS). challenging for LMs, to first replace them by anchor numbers (e.g., 10, 7) and use LMs to output answers (e.g., 3) that are more much accurate than complex numbers, then inversely elicit the hidden arithmetic relationship (e.g., x 1 − x 2 ) implicitly inferred by LMs through these anchor inputs and outputs, and finally explicitly applying the arithmetic relationship on the original complex numbers (10, 477 − 7, 459) to produce the answer (3,018). In this way, our method combines the advances of LMs on understanding complex context and memorizing simple numbers towards reliable numerical reasoning.</p>
<p>This paper introduces reflection of thought, a new idea of eliciting the numerical reasoning knowledge hidden in pre-trained LMs through probing with simple anchor numbers. Reflection of thought, in principle, allows models to unveil the underlying reasoning process by varying inputs at test time, so it does not need any extra training or labeled data. To inversely elicit arithmetic relationships in LMs through anchor numbers, we propose SOLIS, a novel method to transform and formulate this problem to a linear system that can be directly solved in an analytic way. To promote robustness to mistakes introduced by LMs, search-based and heuristic-based methods are further devised. Experimental results on several representative numerical reasoning datasets demonstrate that SOLIS significantly advances cutting-edge LMs. Importantly, our framework simply works in the inference phase without extra training or labeled data, making it highly portable to different kinds of LMs and achieving consistent gains over various LMs in zero-shot, few-shot and fine-tuning scenarios.</p>
<p>PRELIMINARY STUDY</p>
<p>In this section, we will first demonstrate the brittleness of language models' ability on arithmeticallyrelated tasks. Unlike arithmetic benchmarks such as AddSub or MultiArith (Roy &amp; Roth, 2015) which contain natural language context for each sample, we directly generate and feed the arithmetic expressions and test the performance on language models. This is done to reduce potential perturbing factors and highlight the models' calculating ability. We impose constraints on the complexity of the expressions: we only study the four fundamental operations, and demand no more than 4 operands, where each operand's integer range is less then 10, 000 and floating point precision is less than 4. To conduct a systematic investigation, we first produce F which represents the set of all the expressions satisfying our constraints. We randomly sample numbers within the limits of range and precision as the operands. For one expression f ∈ F with a specified range and precision, we randomly generate 50 samples. We evaluate the language model on these samples and denote this synthesized task as MathExp which stands for Math Expressions.</p>
<p>We sample a maximum of 50 expressions for each different settings of complexity, and test these samples using large scale language model GPT-3 (Brown et al., 2020a). We conduct the study on GPT-3 in a few-shot manner: to unleash its potential, we pre-pend 10 to 20 expressions (having the same f , integer range, and floating point precision as the tested sample) together with the answers as the prompt. We then call the OpenAI API 1 to get all the predictions, and evaluate the performance accordingly. Figure 2 indicate that even the latest powerful GPT-3(Code-Davinci-002) fails to achieve a satisfactory performance: (i) the prediction accuracy decreases largely as the number gets more complex, i.e., integer range or floating point precision of operands increases; (ii) the prediction accuracy also drops dramatically as the arithmetic relationship getting more complex, i.e., number of operands increases. In Appendix A, we also present the performance with our SOLIS framework, which is more robust to influence of floating point precision and integer range.</p>
<p>Results in</p>
<p>3 NUMERICAL REASONING VIA SOLVING LINEAR SYSTEMS The preliminary study demonstrates that the current language models are vulnerable to complex numbers. For example, they have no chance to guess the answer to the sum of two floating point numbers with three decimal places. However, the language model can perform reliably well when the operands are simple, i.e., relatively small integers. Such observations motivate us to simplify the numbers before feeding them into language models, thus enabling reliable neural-based numerical reasoning. In this section, we first provide an overview of our framework SOLIS, and then we elaborate on each part of our framework in detail.</p>
<p>METHOD OVERVIEW</p>
<p>As mentioned above, our method can be integrated into language models in a plug-and-play manner at test time. For the sake of clarification, in the following we refer to LMs that can steadily perform numerical reasoning as reasoning LMs. They can be either LMs obtained by fine-tuning on datasets involving numerical reasoning, or LMs that perform numerical reasoning via in-context learning.</p>
<p>As shown in Figure 1, our method generally involves three stages: (1) Operand Proposal: given a paragraph, we first identify the numbers which are necessary for the reasoning LM to perform numerical reasoning (e.g., 10, 477); (2) Number Substitution: these proposed operands 2 are generally complex for language models, and thus they need to be substituted with randomly chosen simple numbers (e.g., 10) to make the model input simpler. Using the reasoning LM, we can obtain a set of predicted answers with respect to each substituted paragraph after several substitutions.</p>
<p>(3) Arithmetic Relationship Inversion: using these paragraphs and their answers as observed data, we can inversely derive the internal reasoning flow from the reasoning LM, i.e. the arithmetic expression between the operands (e.g., y = x 1 − x 2 ). By applying the expression on the original numbers, the answer to the original paragraph can be obtained.</p>
<p>OPERAND PROPOSAL</p>
<p>There are often many numbers involved in a paragraph, and it is quite challenging to model the arithmetic relationships among all these numbers simultaneously. Consequently, it is important during the operand proposal step to trim the prospective operands to a manageable size. A straightforward strategy would be to select only the numbers pertinent to the answer as candidate operands, which is not trivial in practice since there is no intermediate supervision on the relevance between each number and the answer.</p>
<p>To address the issue, we provide a novel technique that employs number perturbation and the reasoning LM to measure the relevance systematically. It is largely inspired by prior works that leverage an image classifier to quantify the relevance of pixels with image categories (Samek et al., 2017) and its application on natural language tasks (Liu et al., 2021). In their works, relevance is assessed by the degradation of the classifier score after erasing each pixel, where a substantial degradation indicates a strong relevance. Similarly, we consider a number to be essential to the final answer if there is a difference between the model predictions before and after perturbing it. Regarding perturbations, we implement it by adding a small adjustment to each number in the paragraph (e.g., 98.5 → 98.6) and evaluate whether the model prediction changes correspondingly. Despite the fact that the reasoning LM hardly perform accurate calculations over numbers, we observe that LMs have strong context understanding capabilities about numbers and are sensitive to slight changes in the numbers used to forecast answer. More details about the operand proposal mechanism can be found in Appendix B.</p>
<p>NUMBER SUBSTITUTION</p>
<p>After the operand proposal stage, a random set of numbers is generated to substitute the proposed operands sequentially. These numbers are referred to as anchor numbers below. Each anchor number is an integer between 1 and 20, a range that we believe reasoning LMs can easily handle. Meanwhile, to minimize the effects of number substitution, we strive to maintain the order relationships among the numbers. Taking the example from Figure 1, we make the substitution number corresponding to 10, 477 larger than the one corresponding to 7, 459 since 10, 477 is larger than 7, 459.</p>
<p>Notably, the random number substitution must be repeated several times (e.g., three times in Figure 1) to obtain a group of anchor numbers. Along with the original question, each of these paragraphs is fed into the reasoning LM to predict the answer, which we call the anchor answer. Typically, the number of anchor answers must exceed the number of operands for the subsequent arithmetic relationship inversion stage to be feasible.</p>
<p>ARITHMETIC RELATIONSHIP INVERSION</p>
<p>Given a collection of anchor numbers and anchor answers, the arithmetic relationship inversion stage investigates the relationship between these numbers and induces an expression to reflect it. Taking the example from Figure 1, a typical expression can be y = x 1 − x 2 , where x 1 and x 2 are both anchor numbers while y is the anchor answer.</p>
<p>Although the example expression appears intuitive, deriving such an expression from data points is tremendously difficult because the solution space is theoretically infinite. To make it practicable, as a first step, we begin by limiting the problem-solving space to compositions of binary operators, where each operator can be addition, subtraction, multiplication or division, the four most prevalent operators in numerical reasoning (Dua et al., 2019). Meanwhile, there can be up to three compositions, which means the expression contains a maximum of four operands. With such priors, the insoluble expression induction problem can be turned into a linear system solving problem, where the anchor numbers, the anchor answer, and their compositions constitute a linear system. In this way, the problem of expression induction can be tackled by the solving algorithms for linear systems, which will be elaborated in Section 4. Finally, the answer can be reached in a trustworthy and interpretable manner by applying the derived expression to the original numbers.</p>
<p>SOLVING ALGORITHM</p>
<p>In this section, we introduce three algorithms that can derive expressions merely from anchor numbers and anchor answers, namely analytical-based, search-based and heuristic-based algorithm.</p>
<p>FORMULATION</p>
<p>Formally, given a paragraph and a question, we denote a group of anchor numbers as x = (x 1 , x 2 , . . . , x n ) and the arithmetic relationship as an expression f , which should produce the answer y by y = f (x). The goal is to recover f from different groups of anchor numbers X and corresponding anchor answers y. We propose to transform and formulate the arithmetic relationship inversion as solving a system of linear equations. Given expression f (x) with four fundamental arithmetic operations, we transform the equation y = f (x) by multiplying denominators on both sides when operator division exists, then we get:
a 0 · C + a 1 · x 1 + a 2 · x 2 + a 3 · y + a 4 · x 1 x 2 + . . . + a k · (x 1 x 2 . . . x n y) = 0(1)
For example, y = 1 − x 1 /x 2 can be transformed to x 2 − x 1 − x 2 y = 0. Then uncovering f (x) is equivalent to solving a = (a 0 , a 1 , . . . , a k ), which are coefficients of all possible polynomial basis combined by x 1 , , x n and y, denoted as p, where k = 2 n+1 − 1. Multiple groups of anchors X and y constitute multiple groups of values of polynomial basis, denoted as P, then Equation 1 can be denoted as Pa = 0, which is a typical set of linear equations.</p>
<p>ANALYTICAL-BASED ALGORITHM</p>
<p>To solve Pa = b, we can simply generate k+1 groups of anchor numbers as X and LMs' answers as y, compute P based on X and y, and finally get a = (P) −1 b when P is in full rank. But notice that y can be a linear weighted summation of x 0 , . . . , x n by itself, the coefficient matrix P may not be full-ranked. To address this, we generate k groups of anchor numbers and add an additional constraint by setting |a| = k i=0 a i = 1. So we augment P with an all-one vector to P * and finally get a = (P * ) −1 b, where b = (0, 0, . . . , 0, 1). In practice, randomly sampled groups of anchor numbers can form a full-ranked P * with a very high probability, and one can even add a buffer by sampling a bit more groups of anchor numbers than k to constitute different P * s for cross validation.</p>
<p>The analytic method is theoretically complete to deduce arithmetic expressions in our pre-defined problem space. But in practice, LMs may produce incorrect results even for anchor numbers, especially when given a complex expression, so as to violate the analytic method which needs purely correct anchor answers. To best tolerate them, we then propose search-based and heuristic-based methods to better solve a noisy linear system. Gladly, the analytic method theoretical supports other methods in aspects such as guiding the number of anchors to sample to ensure a unique expression.</p>
<p>SEARCH-BASED ALGORITHM</p>
<p>The search-based algorithm exhaustively explores the search space and finds out the most preferable arithmetic expression in the space. We constrain the search space of a in Equation 1 by: requiring a 1−n ∈ {−1, 0, 1} for all coefficients of the non-constant terms, and for coefficient a 0 of constant term C, one can restrict the search range to a pre-defined set, e.g., a 0 ∈ {−100, −1, 0, 1, 100} in our experiments for efficiency, and different from the analytic method that can easily solve constants in expressions. Constraints here mean that we only let this search algorithm cover f (x) with no more than one constant for efficiency. We then transform all searched polynomial-basis-based equations backwards into expressions because they have one-to-one mappings, e.g., from x 2 − x 1 − x 2 y = 0 to y = 1 − x 1 /x 2 . We denote the space of expressions as F, and for each f i ∈ F and each group of anchor numbers X j (using m to denote the number of groups), we get y ij by applying f i to X j .</p>
<p>Algorithm 1 SEARCH</p>
<p>Input: parameters X,ŷ, F, c threshold Output: Most preferable expression f 1: while j &lt; m do 2:
for f i ∈ F do 3: y * ij ← f i (X j ) 4: c i ← c i + 1(y * j ==ŷ ij ) 5: i ← i + |y * j −ŷ ij | 6: end for 7: j ← j + 1 8: end while 9: i * c ← arg max c, i * ← arg min 10: if c i * ≥ c threshold then f ← f i * c 11: else f ← f i * 12: end if
We define the prediction error between the target expressionf and f i as (f , f i ), which is calculated by (f , f i ) = j ij = j abs(ŷ j −y ij ), and the number of occurrence of exact matching as c i . We then find the most preferable expression with the minimum prediction error and the maximum number of exact matching. Specifically, when the number of exact matching exceeds a pre-defined c threshold , we pick the expression f i with the highest c i ; otherwise, we pick the expression f i with the lowest i . The search process is sketched in Algorithm 1.</p>
<p>This method is robust for probably incorrect predictions, i.e., when model does not have sufficient number of exact matching, it is still capable to return the most nearest expression by selecting the one with the minimum error. However, the search-based method can be challenged by exponentially explosive search space when the number of operands surges, and it's not efficient to search constant numbers that has a wide and even infinite range, neither.</p>
<p>HEURISTIC-BASED ALGORITHM</p>
<p>In this section, we introduce a heuristic-based algorithm, simulated annealing, which is efficient and does not need to search for the whole problem space, though it may produce sub-optimal results given a limited number of exploration steps. We follow the formulation introduced in Section 4.1 and proposed a optimization target L H to measure the L1 loss of P a. The pipeline includes: (1) randomly initialize a with values {-1, 0, 1} and calculate initial L H ; (2) randomly select i from 0 to k and perturb a i by adding or subtracting a constant number (we use 1 here); (3) calculate new L H , and adopt the perturbation with a large probability if L H decreases and with a low probability if it increases, balanced by a pre-defined temperature T , which decreases over steps; (4) return a if the number of steps is enough or L H equals to zero, otherwise repeat from step 1. Note that, we restrict coefficients in a to be integers for simplicity, so different from the analytical method restricting k i=0 a i = 1, we ensure only one of the coefficients of y-related polynomial basis {y, x 1 y, . . . ,</p>
<p>x 1 x 2 . . . x n y} to be non-zero (with a static value 1) and at least two coefficients in a are non-zero during the whole initialization and perturbation process to avoid some infeasible local optimal. In summary, Table 1 shows the strong and weak points of these algorithms. In the problem space introduced in Section 3.4 within at most four operands, the search-based method does not have scalability issues, so it achieves best performance in our experiments because it's robust to LMs' predictions and can retrieve optimal expression through exhaustive search except rare constants.</p>
<p>EXPERIMENTS</p>
<p>In this section, we integrate SOLIS with various language models as backbones and evaluate the effectiveness of SOLIS on two well-known numerical reasoning benchmarks.</p>
<p>EXPERIMENTAL SETUP</p>
<p>Datasets We perform experiments on DROP (Dua et al., 2019), AddSub and MultiArith, of which the latter two are widely used subsets from MAWPS (Roy &amp; Roth, 2015). DROP is a reading comprehension benchmark that focuses on numerical reasoning and has a variety of answer types, including span, number, and date. The experimental results of DROP are evaluated with the official evaluation metrics Exact Match (EM) and F1. As for MAWPS, it consists of math word problems which also require numerical reasoning ability. The subset AddSub features relatively easier numerical reasoning, whereas MultiArith necessitates multi-step numerical calculations. The EM metric is used to evaluate the results of AddSub and MultiArith. More details can be found in Appendix C.</p>
<p>Backbone and Baselines On DROP, we adopt two kinds of LMs as our backbones, including (i) Vanilla LMs: BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), (ii) Reasoning LMs: TAPEX  and POET (Pi et al., 2022).</p>
<p>We also compare the performance of our method with previous specialized models designed for the DROP dataset, such as NumNet (Ran et al., 2019), NeRd (Chen et al., 2020b), MTMSN (Hu et al., 2019) and QDGAT . All models are fine-tuned on the DROP train set, and the best validation set performance is reported. If no explicit declarations are included, all LMs here are of large size and contain approximately 350M of parameters. On AddSub and MultiArith, we adopt GPT-3 Code-Davinci-002 (GPT-3) (Brown et al., 2020a) with different prompting techniques as our backbones: Chain-of-Thought Prompting (Chain) (Wei et al., 2022b) and the Zero-shot Chain-of-Thought Prompting (Zero-shot Chain) (Kojima et al., 2022). We compare our results to the PaLM model (Chowdhery et al., 2022). Unless otherwise specified, these models perform numerical reasoning by zero-shot/few-shot in-context learning, and the few-shot demonstrations are the 8 samples released by Wei et al. (2022b).</p>
<p>IMPLEMENTATION DETAILS</p>
<p>Hyperparameter Selection For fine-tuning, we apply Adam (Loshchilov &amp; Hutter, 2019) optimizer. The fine-tuning epochs are set as 50. For BART models (i.e., BART and POET-SQL), we follow previous works (Pi et al., 2022) to set the batch size as 128 and the learning rate as 3 × 10 −5 . For T5, we decrease the batch size to 32 due to the computational budget. The early stop technique is used to save training time. For GPT-3 API, we keep the temperature as default setting 0, and set the maximum output tokens to 128. As for anchor number groups: the group size is 6/8/10 corresponding to corresponding to 2/3/4 operands on DROP; the group size is 4 on AddSub, and   (Lewis et al., 2020) 67.4 70.6 w. SOLIS 72.9 (+5.5) 76.1 (+5.5) T5 (Raffel et al., 2020) 61.0 64.6 w. SOLIS 69.9 (+8.9) 73.5 (+8.9) Reasoning LMs TAPEX  76.3 79.3 w. SOLIS 78.5 (+2.2) 81.6 (+2.3) POET-SQL (Pi et al., 2022) 76 Design Choices on DROP Following previous work, we apply two general-purpose numerical designs on the DROP dataset. First, we employ the character-level rather than subword-level number representation, which proves to be more effective (Wallace et al., 2019;Pi et al., 2022). Second, we employ the reverse decoding technique, which proves to be a successful design to mimic arithmetic carry (Geva et al., 2020). Meanwhile, as mentioned above, the search-based algorithm has difficulties in covering expressions including constants. Considering the constant 100 is frequently used for percentage calculations (e.g., "How many percent of the national population does not live in Bangkok?"), we add it to be one candidate in DROP.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>Since our work focuses on addressing arithmetic problems, we first evaluate suggested solving algorithms via their performance on the DROP subset whose answers are numbers (i.e., numeric subset). Meanwhile, we select cases in which the answer is greater than 1000, identify them as "hard" cases, and additionally report the average performance on them. As shown in Table 2, all of our proposed algorithms significantly improve the performance of LMs, especially in hard cases. For example, the search-based algorithm boosts BART with an absolute 30.4% improvement on hard cases. The full results of the performance comparison can be found in Appendix D. Notably, since the search-based algorithm is the most effective, we apply it as the default algorithm in SOLIS.   </p>
<p>54.25%</p>
<p>Composition How many more Albanian citizens were there compared to Bulgarian and Georgia citizens combined ? [y = x0 − (x1 + x2)]</p>
<p>0.34%</p>
<p>imental results on AddSub and MultiArith. The results indicate that our approach is surprisingly effective for giant LMs and can further boost the performance of chain-of-thought prompting.</p>
<p>MODEL ANALYSIS</p>
<p>Arithmetic Relationship Inversion In addition to performance improvement, SOLIS features the ability to derive an arithmetic expression for each question, whereas no such information is available during training. To better understand if these expressions align with question intentions, we collect all derived expressions and categorize them into four types in Table 5. As demonstrated, the majority of expressions contain addition and subtraction between variables and constants, which are largely consistent with the question intention, highlighting the superior interpretability of SOLIS.</p>
<p>Solving Algorithm Robustness The possibility that the anchor answers provided by reasoning LMs are inaccurate presents a challenge for the solving algorithms. To measure the robustness of our solving algorithms, we roughly decrease the probability that anchor answers are correct by decreasing the number of few-shot demonstrations in Figure 3. As shown, even though the backbone LM performance drops to 60.0%, the improvement of SOLIS is still as high as to 5.1%, suggesting its robustness.</p>
<p>Number Substitution To study the impact of different factors during the number substitution stage, we conduct experiments on MathExp in Figure 4. As demonstrated, expanding the range of anchor numbers results in a minor performance drop, showing that the reasoning LM is more familiar with small integers. Furthermore, increasing the size of anchor number groups gives a large improvement on the performance, especially when there are four operands.</p>
<p>Limitation Discussion The first limitation of our framework is that we cannot support expressions that cannot be solved with linear systems. For example, with respect to the question "How many yards was Donovan McNabb's longest rushing TD?", the expected expression [y = max i (x i )] is not supported by SOLIS. Second, the framework is less efficient when there are many operands. On the one hand, the group of anchor numbers would be quite huge, making the algorithm's runtime unacceptable. For example, when expanding to 5 operands, number substitution must be performed at least 50 times. On the other hand, for the search-based algorithm, the search space will increase exponentially, making the algorithm impracticable. Last, we assume a certain level of numeracy understanding of the reasoning LM. Therefore, if the reasoning LM is unable to comprehend the numeracy relationship, our method would not work well.</p>
<p>RELATED WORK</p>
<p>Numerical Reasoning via Specialized Models Previous works generally design trainable specialized modules and equip LMs with them to tackle different kinds of numerical reasoning problems (e.g., counting). While these methods work well on specific datasets (Dua et al., 2019;Andor et al., 2019;Hu et al., 2019;Ding et al., 2019), they are hardly suited across different datasets and backbone LMs (Chen et al., 2020b). Differently, since our method does not require additional model training, it is applicable to almost all models, even those that only provide an inference interface (e.g., GPT-3). As for methods that first generate programs or logic forms, it is quite laborious to define domain-specific language and collect corresponding training data (Berant et al., 2013). Unlike them, our methods does not require extra annotated programs. Instead, our method allows for the program discovery from examples via solving linear systems.</p>
<p>Numerical Reasoning via Pre-training This line of work always focuses on the pre-training of language models with corpus which involves reasoning. The corpus can be reasoning-oriented natural language texts from Internet (Deng et al., 2021;Lewkowycz et al., 2022), human-designed templates filled by different data sources (Geva et al., 2020;Yoran et al., 2022), or programs with rich reasoning semantics Pi et al., 2022). Although this kind of pre-training allows language models to perform better reasoning, they still require considerable computation budgets during pretraining and may still be challenged by complex numbers. In contrast, our method is efficient since it can be integrated into existing models without further training or pre-training.</p>
<p>Numerical Reasoning in Giant Language Models Recent works demonstrate that with proper prompting, giant language models (e.g., GPT-3) perform much better than smaller ones on several reasoning tasks (Wei et al., 2022b;a;Kojima et al., 2022;. For example, with the chain-of-thought prompting, the few-shot PaLM model (Chowdhery et al., 2022) can beat the previous best fine-tuned model on math word problems. However, their conclusions do not generalize to non-giant language models. Different from them, our method can be simultaneously applied to language models ranging from millions (e.g., BART) to billions (e.g., GPT-3). Moreover, our work is orthogonal to these giant LMs and can be complementary to each other. For example, Section 5 shows that our approach can further boost the numerical reasoning capability of GPT-3 with chain-of-thought prompting.</p>
<p>CONCLUSION</p>
<p>In this work, we present SOLIS, a framework which can elicit numerical reasoning in language models at test time. Motivated by the fact that language models usually excel at simple numbers, SOLIS uses simple numbers as anchors to inversely derive the implicitly inferred arithmetic expressions from language models, and subsequently apply these expressions to complex numbers to perform numerical reasoning. With modeling the expression derivation as solving linear systems, we propose three kinds of algorithms to achieve SOLIS with noisy signals. Experimental results on several numerical reasoning benchmarks demonstrate that SOLIS can be integrated to a variety of language models, and can greatly improve their performance under zero-shot, few-shot, and finetuning scenarios. Our work provides a new perspective towards tackling numerical reasoning, which can be potentially applied to more language models and numerical reasoning tasks.</p>
<p>A PRELIMINARY STUDY DETAILS</p>
<p>Here we present the model performance on MathExp of GPT-3 with different solving algorithms in Figure 5 and Figure 6. We can conclude that: (1) both algorithms are not sensitive with either the floating point precision or the integer range;</p>
<p>(2) the search-based algorithm is most robust than the analytical-based algorithm with respect to the number of operands. </p>
<p>B OPERAND PROPOSAL DETAILS</p>
<p>In Section 3.2, we mention that the textual context on a realistic dataset may be noisy, i.e., contains irrelevant numbers, thus we need to locate the operand number first. We substitute 10 times for each number appearing in the paragraph, if the output gives ≥ 3 different prediction numbers out of 10, we decide the current tested number is involved to the answer. Moreover, we substitute numbers following a template: suppose the original number x is with precision p, then the substituted numbers can be represented as x + k · 10 p , where k ∈ {−5, −4, −3, −2, −1, 1, 2, 3, 4, 5}.</p>
<p>C EXPERIMENTS</p>
<p>C.1 EXPERIMENTAL SETUP  For BART, we implement the fine-tuning methods using the Huggingface transformers library (Wolf et al., 2020) on 4 V100 16GB GPUs. We use BART LARGE (Lewis et al., 2020) as our backbone. We use same-scale reasoning-pretrained POET-SQL and TAPEX models in experiments. For T5, we implement its fine-tuning on the Huggingface transformers library on A100 GPUs. We use T5 LARGE (Raffel et al., 2020) as our backbone.</p>
<p>C.2 EXPERIMENTAL DETAILS ON DROP</p>
<p>Fine-tuning Details For all fine-tuning methods, we select the default max token length for each model. We set the max token length of generation as 96. To save training time, we set early stop mechanism: we evaluate the EM and F1 score per 500 or 1000 steps, if the performance does not increase in the latest 20 evaluations, we stop the training and save the best checkpoint.</p>
<p>On DROP, we pre-pend the question to the given paragraph. For multi-span answer, we insert ";" between each span and make up the final answer. For T5 LARGE , we also insert "</s>" token between the question and the given paragraph. Since most LMs' checkpoints on DROP is currently not off-the-shelf, we re-implement them and compare to the results reported in previous works. We present the comparison results in Table 8. </p>
<p>D MORE RESULTS ON DROP</p>
<p>We present the performance breakdown of F1 on dev set of DROP in Table 9 Apart from fine-tuning models on DROP dataset, we also use GPT-3 to conduct a study on few-shot learning. We pre-pend 10 random training samples in train set, and run all cases where answer type equals to "number". We also apply our search-based algorithm on GPT-3. To save API calling time, we only substitute the number for one time. Table 10 presents the F1 score comparison.</p>
<p>We also summarize common calculation error cases in our tested language models and present some of them for case study in Table 11, which again illustrates the unreliability of language models.   </p>
<p>Figure 2 :
2Performance with different floating point precision (left) and integer range (right).</p>
<p>MultiArith because MultiArith requires more compositional operations. More details can be found in Appendix C.</p>
<p>Figure 3 :Figure 4 :
34The experimental results of Chain and Chain w. SOLIS on AddSub as the number of few-shot examples decreases. The experimental results of SOLIS on Math-Exp with different choices of anchor number range (left) and anchor number groups (right).</p>
<p>Figure 5 :Figure 6 :
56Performance over different floating point precision (left) and integer range (right) on MathExp of GPT-3 w. search-based algorithm. Performance over different floating point precision (left) and integer range (right) on MathExp of GPT-3 w. analytical-based algorithm.</p>
<p>Table 1 :
1Comparison of solving algorithms.Optimum Robustness Scalability </p>
<p>Analytical </p>
<p>Search </p>
<p>Heuristic </p>
<p>Table 2 :
2Experimental results of SOLIS w. various solving algorithms on the DROP numeric subset.LM 
Algorithm F1(%) on Hard F1(%) on Total </p>
<p>BART </p>
<p>-
30.4 
66.4 
Analytical 46.4 (+16.0) 
69.3 (+2.9) 
Search 
64.8 (+30.4) 
75.2 (+8.8) 
Heuristic 
52.8 (+22.4) 
71.7 (+5.3) </p>
<p>POET-SQL </p>
<p>-
66.8 
78.4 
Analytical 73.3 (+6.5) 
80.0 (+1.6) 
Search 
76.9 (+10.1) 
81.4 (+3.0) 
Heuristic 
73.0 (+6.2) 
80.5 (+2.1) </p>
<p>Table 3 :
3Experimental results on the validation set of DROP dataset.Models 
EM(%) 
F1(%) </p>
<p>Specialized Models 
NumNet (Ran et al., 2019) 
64.9 
68.3 
MTMSN (Hu et al., 2019) 
76.7 
80.5 
NeRd (Chen et al., 2020b) 
78.6 
81.9 
QDGAT (Chen et al., 2020a) 84.1 
87.1 
Vanilla LMs 
BART </p>
<p>Table 3
3shows the experimental results of different models on DROP dataset. As shown, SOLIS can bring consistent and significant improvements over all backbone LMs, especially for the vanilla LMs. Taking the T5 model as an example, it could be boosted by a maximum of 8.9% with SOLIS. Even for POET-SQL which are already pre-trained for numerical reasoning, our method yields a 2.0% F1 improvement, pushing the best LM performance to 82.0% F1.Table 4presents the exper-</p>
<p>Table 4 :
4Experimental results of different methods on AddSub and MultiArith.Language Model Setting 
AddSub 
MultiArith </p>
<p>PaLM (540B) 
Standard (Chowdhery et al., 2022) 
− 
42.2 
Chain (Wei et al., 2022b) 
91.9 
94.7 </p>
<p>GPT-3 (175B) </p>
<p>Zero-shot Chain (Kojima et al., 2022) 
66.6 
63.8 
w. SOLIS 
89.4 (+22.8) 
80.0 (+16.2) 
Chain (Wei et al., 2022b) 
88.4 
96.7 
w. SOLIS 
90.9 (+2.5) 
98.7 (+2.0) </p>
<p>Table 5 :
5Case study on derived expressions using POET-SQL w. SOLIS on DROP. Listed are, the intention, the example question with intention trigger words (i.e., the colorful spans) and the derived expression, and the proportion of each intention.Question Intention Example Question with [Derived Expression] 
Proportion </p>
<p>Addition 
How many total yards of touchdown passes were there? 
[y = x1 + x2 + x3] </p>
<p>8.92% </p>
<p>Diff Constant 
How many in percent in the county from the census of 2000 
weren't English? [y = 100 − x] </p>
<p>36.49% </p>
<p>Subtraction 
How many more percentages of people were germans compared 
to irish? [y = x1 − x2] </p>
<p>Table 6 :
6Statistics of DROP datasetDataset </p>
<p>Train 
Dev </p>
<h1>Questions # Docs # Questions # Docs</h1>
<p>DROP 
77, 409 
5, 565 
9, 536 
582 </p>
<p>Table 7 :
7Statistics of MAWPS datasetSubset </p>
<h1>Questions</h1>
<p>AddSub 
395 
MultiArith 
600 </p>
<p>Table 8 :
8Performance Comparison on DROP between reported results in previous works and our re-implementation. Results marked with * represent our re-implementation results.Models </p>
<p>EM (%) F1 (%) </p>
<p>BART (Pi et al., 2022) 
66.2 
69.2 
BART  *<br />
67.4 
70.6 </p>
<h2>T5 (Yoran et al., 2022)</h2>
<p>64.6 
T5  *<br />
61.0 
64.6 </p>
<p>POET-SQL (Pi et al., 2022) 77.7 
80.6 
POET-SQL  *<br />
76.9 
80.0 </p>
<p>Table 9 :
9Breakdown of model F1 score by answer types on the dev set of DROP.Models 
Number Span Spans Date Total </p>
<p>Table 10 :
10Performance of GPT-3 w. SOLIS on the DROP numeric subset.Language Model Algorithm F1(%) on Hard F1(%) on Total </p>
<h2>GPT-3 (175B)</h2>
<p>42.5 
64.7 
Search 
59.9 (+17.4) 
68.7 (+4.9) </p>
<p>Table 11 :
11Common calculation error cases on DROP dataset. Carry Error . . . the size of the black-white IQ gap in the United States decreased from 16.33 to 9.94 IQ points. . . . Q: How many IQ points did the black-white IQ gap decrease in the United States in a 2013 analysis of the National Assessment of Educational Progress? Missing High Digit . . . The Department of Tourism recorded 26,861,095 Thai and 11,361,808 foreign visitors to Bangkok in 2010. . . . Q: How many more Thai visitors did Bangkok have in 2010 compared to other foreign visitors? Extra Integer digit . . . Rayner nailed a 23-yard field goal . . . Rayner got a 54yarder and a 46-yarder to end the half . . . Q: How many total yards of field goals did Dave Rayner have? Extra Float Number Digits . . . have estimated the IQ means of 17-year-old black, white, and Hispanic students to range respectively from 90.45-94.15 . . . Q: How many points difference is the IQ range in 17year-old black students? Insufficient Precision . . . The Diocese of Karelia has 22,000 church members in 12 parishes. . . . Q: How many church members approximately are in each one of the 12 parishes?Error Type 
Example 
Prediction 
Label </p>
<p>6.49 
6.39 </p>
<p>499287 
15499287 </p>
<p>111113 
123 </p>
<p>3.75 
3.7 </p>
<p>1833 
1833.33 </p>
<p>https://openai.com/api
We use the terms number and operand interchangeably.
ACKNOWLEDGEMENTSWe would like to thank Xuanyu Dong, who is working at Harvest Fund, for helping us with the linearization method based on polynomial basis from an analytical and mathematical perspective.
Giving bert a calculator: Finding operations and arguments with reading comprehension. Daniel Andor, Luheng He, Kenton Lee, Emily Pitler, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving bert a calculator: Finding oper- ations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5947-5952, 2019.</p>
<p>Semantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533-1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.</p>
<p>Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020b.</p>
<p>Question directed graph attention network for numerical reasoning over text. Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), pp. 6759-6768, 2020a.</p>
<p>Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, Quoc V Le, ICLR. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In ICLR, 2020b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>ReasonBERT: Pre-trained to reason with distant supervision. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun, 10.18653/v1/2021.emnlp-mainProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsXiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. ReasonBERT: Pre-trained to reason with distant supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6112-6127, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Cognitive graph for multi-hop reading comprehension at scale. Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang, 10.18653/v1/P19-1259Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2694-2703, Florence, Italy, July 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/P19-1259.</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, 2019.</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsMor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pp. 946-958, 2020.</p>
<p>Language models are general-purpose interfaces. Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei, arXiv:2206.06336arXiv preprintYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked au- toencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009, 2022.</p>
<p>A multi-type multi-span network for reading comprehension that requires discrete reasoning. Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingMinghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP-IJCNLP), pp. 1596-1606, 2019.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBartMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, 2020.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858Solving quantitative reasoning problems with language models. Theo Gutman-SoloarXiv preprintAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra- masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336arXiv preprintYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</p>
<p>Awakening latent grounding from pretrained language models for semantic parsing. Qian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, Jian-Guang Lou, 10.18653/v1/2021.findings-acl.100Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsQian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, and Jian-Guang Lou. Awakening latent grounding from pretrained language models for semantic parsing. In Findings of the Associa- tion for Computational Linguistics: ACL-IJCNLP 2021, pp. 1174-1189, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.100.</p>
<p>TAPEX: Table pre-training via learning a neural SQL executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, International Conference on Learning Representations. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX: Table pre-training via learning a neural SQL executor. In International Conference on Learning Representations, 2022.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, 7th International Conference on Learning Representations. New Orleans, LA, USAIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, arXiv:2201.11473Reasoning like program executors. arXiv preprintXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. Reasoning like program executors. arXiv preprint arXiv:2201.11473, 2022.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.</p>
<p>Numnet: Machine reading comprehension with numerical reasoning. Yankai Qiu Ran, Peng Lin, Jie Li, Zhiyuan Zhou, Liu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingQiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. Numnet: Machine reading comprehen- sion with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2474-2484, 2019.</p>
<p>Yasaman Razeghi, I V Robert L Logan, Matt Gardner, Sameer Singh, arXiv:2202.07206Impact of pretraining term frequencies on few-shot reasoning. arXiv preprintYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015.</p>
<p>Evaluating the visualization of what a deep neural network has learned. W Samek, A Binder, G Montavon, S Lapuschkin, K Müller, 10.1109/TNNLS.2016.2599820IEEE Transactions on Neural Networks and Learning Systems. 2811W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. Müller. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660-2673, 2017. doi: 10.1109/TNNLS.2016.2599820.</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do nlp models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5307-5315, 2019.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Emergent abilities of large language models. ArXiv, abs/2206.07682Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022a.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrationsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 38-45, 2020.</p>
<p>Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Wang, arXiv:2201.05966arXiv preprintTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.</p>
<p>Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills. Alon Ori Yoran, Jonathan Talmor, Berant, Proceedings of the 60th. the 60thOri Yoran, Alon Talmor, and Jonathan Berant. Turning tables: Generating examples from semi- structured tables for endowing language models with reasoning skills. In Proceedings of the 60th</p>
<p>Annual Meeting of the Association for Computational Linguistics. 1Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6016-6031, 2022.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, arXiv:2205.10625arXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu- urmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
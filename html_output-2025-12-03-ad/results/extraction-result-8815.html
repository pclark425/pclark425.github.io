<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8815 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8815</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8815</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267637261</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08496v3.pdf" target="_blank">A Systematic Review of Data-to-Text NLG.</a></p>
                <p><strong>Paper Abstract:</strong> This systematic review provides a comprehensive analysis of the state-of-the-art research on data-to-text generation, addressing gaps, highlighting challenges, and proposing future directions. We examined various approaches in this field, assessing their effectiveness and limitations, while surveying literature on datasets, evaluation metrics, application areas, multilingualism, and methods for mitigating hallucinations. We shed light on the usage, popularity, and impact of datasets, alongside evaluation metrics, emphasizing both automatic and human assessment. Additionally, we explore the evolution of data-to-text models, emphasizing the widespread adoption of transformer models. With a focus on inclusivity, we stress the importance of research in low-resourced languages. Despite notable advancements in text quality, we examine strategies utilized to tackle hallucinations in models and advocate for universally applicable techniques. This review serves as a roadmap to inspire innovation, establish evaluation benchmarks, and drive progress in data-to-text generation.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8815.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8815.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearization / Sequence Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts graph-structured inputs into a linear token sequence so they can be consumed by sequence-to-sequence models; commonly used as a baseline representation for AMR and RDF graphs by serializing nodes and edges into text (e.g., PENMAN or triple lists).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG challenge: Generating text from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Converts graphs into a single ordered token sequence by visiting graph elements (nodes, edge labels, triples) and emitting them as tokens or bracketed expressions (examples: PENMAN notation for AMR; subject-predicate-object lists for RDF). The serialization may include special tokens for node boundaries, relation labels, parentheses/brackets, and placeholders for values.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted directed acyclic graphs), RDF triple graphs / knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Textual serialization such as PENMAN notation for AMR (bracketed rooted notation) or listing RDF triples (subject; predicate; object) in a canonical order; sometimes simple depth-first or pre-order traversals are used to produce a deterministic sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation, RDF/knowledge-graph-to-text (WebNLG), general graph-to-text training for language models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper states that linearization was the historical approach to feed graphs into seq2seq models but that graph encoders (GNNs / structure-aware encoders) often outperform plain linearization because they better preserve structural relations; specific numeric comparisons are reported in the cited original works rather than in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; compatible with any seq2seq or pretrained LM without architectural changes; leverages powerful text pretraining when using text-based PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Can lose explicit structural inductive biases of the graph; may lengthen input sequences; structural relations can be harder for decoder to exploit, leading to poorer handling of long-range or multi-hop structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When graphs contain rich relational structure (AMR semantics, multi-hop KG facts) linearized sequences can lead to degraded fidelity, poorer content selection and more hallucination compared to structure-aware encoders; performance drops noted in AMR and KG tasks in comparative studies referenced by the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8815.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network / Graph Encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoders that operate directly on graph structures (e.g., GCN, GAT) to produce node- and graph-level representations which are then consumed by a decoder to generate text; widely used for AMR-to-text and KG-to-text tasks in the surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph encoding (GNN)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes nodes and edges using graph convolutions/attention so each node representation aggregates neighbor information across relation types and distances. The global or pooled representations and per-node embeddings are provided to a decoder (often seq2seq) for surface realization.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (semantic graphs), RDF / knowledge graphs, other labelled directed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No serialization; the graph structure is preserved and processed by message-passing (GCN/GAT) or other graph convolution layers to compute contextualized node embeddings; those embeddings are used directly by an attention-based decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation, KG-to-text / RDF verbalization, table/graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey reports that graph encoders generally outperform plain seq2seq on AMR-to-text and other structure-rich inputs by better capturing relations; 18 surveyed papers used GNNs and reported improvements vs linearized baselines (qualitative summary in the survey; no aggregate numeric metric provided).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves graph topology and relation information; improves fluency and factual fidelity for structure-rich inputs; reduces some hallucinations tied to structure loss.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires architectural changes to incorporate into pretrained text-only LMs; can be computationally heavier; needs careful handling to align encoder graph structure with decoder sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May still hallucinate factual content not grounded in input; effectiveness depends on graph encoder design and compatibility with decoding stage; limited gains when input can be represented compactly as a sequence or when training data is small.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8815.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Penman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN Notation for AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human- and machine-readable bracketed textual format for representing AMR graphs, used both as a storage/annotation format and as an input serialization for text generation systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Penman: An open-source library and tool for amr graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represents AMR graphs as nested parenthesized expressions with relation labels and concept nodes (e.g., (s / see-01 :ARG0 (p / person))) so that the graph is a deterministic string which can be used as input to sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR (rooted, directed, acyclic semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Emit PENMAN text by traversing the AMR graph and writing nodes and labeled edges using parentheses and role labels; used directly as token sequence for seq2seq models or as a standardized interchange format.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and AMR parsing pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey notes PENMAN is a canonical AMR serialization used widely; compared to graph encoders, PENMAN-based linearization is simpler but may underperform because of loss of explicit graph inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Standardized, widely-supported format; easy interoperability and direct use with text-based LMs; human readable.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Loses explicit graph inductive structure for sequence models; long PENMAN strings can be unwieldy; structural relations must be re-learned by decoder from tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>For complex AMR graphs or when structural reasoning is required, PENMAN linearization can lead to poorer factual fidelity and higher hallucination compared to structure-aware encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8815.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-triples</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF triple set serialization (WebNLG style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation used by WebNLG: sets of RDF (subject, predicate, object) triples paired with natural language references; triples are serialized into an input string or treated as structured inputs to graph encoders for KG-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG challenge: Generating text from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple serialization / triple set input</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Inputs are sets of RDF triples (subject, predicate, object). Conversion can be simple list/line concatenation of triples or encoding via a graph encoder; WebNLG experiments often feed triples as either linearized sequences or as graph inputs to structure-aware encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>RDF graphs / Knowledge Graphs extracted from DBpedia or similar sources</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Either (a) serialize triples in a deterministic order (subject|predicate|object tokens) for seq2seq models or (b) construct an RDF graph and encode via GNN/graph transformer that respects triple connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge-graph / RDF-to-text (WebNLG challenge), verbalization of KG facts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey reports that WebNLG uses RDF triple sets and that structure-aware encoders or PLM adaptations often outperform naive triple-serialization baselines; WebNLG also uses seen/unseen domain splits to test generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Triples are a natural and compact representation of KG facts; when paired with structure-aware encoders they help produce faithful verbalizations and can generalize across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Simple serialization can lose connectivity information (e.g., which triples link via shared nodes); graphs with many triples can produce long inputs; domain shift (unseen entities/predicates) can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Divergent or incomplete reference texts (references that add content not in triples) complicate evaluation; naive serialization increases omission and hallucination risk versus structure-aware approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8815.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>dual-graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Graph Representations (Ribeiro et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that encodes a single AMR graph via two complementary graph encoders (top-down and bottom-up) so the decoder benefits from multiple structural perspectives of the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing AMR-to-text generation with dual graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>dual graph representation (top-down + bottom-up)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes the graph using two complementary message-passing passes (e.g., top-down contextualization and bottom-up aggregation) producing dual node representations which are fused for decoding, aiming to capture different structural views.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph remains a graph; two GNN encoders run with different propagation directions or objectives to produce distinct node embeddings that are combined (concatenated / fused) before being supplied to the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey states Ribeiro et al. showed improved text quality over single-encoder baselines (qualitative summary); the dual representation captures complementary information and reduces structural gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures richer structural information by combining complementary node-context views; empirically shown to improve AMR-to-text quality in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>More complex encoder (two encoders) increases computational cost and model complexity; requires design choices for fusion of dual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Increases training and inference costs; may still fail when semantic information is missing from the AMR or when references diverge significantly from graph facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8815.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUALENC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DUALENC (Dual encoding graph + linear)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-encoder architecture that encodes both the graph structure (with a GCN) and a linearized form of the input, addressing the structural gap between encoding and decoding to improve data-to-text quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the structural gap between encoding and decoding for data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>dual encoding (graph encoder + linear encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Simultaneously encodes the input as a graph (via a GCN/graph encoder) and as a linear sequence (via an RNN/seq encoder) to provide the decoder with both structural and sequential cues; may include an intermediate content planning stage.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, tree-structured or general directed graphs used in data-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No conversion to single format; two parallel encoders process the same data in different forms (graph and linearized sequence) and outputs are fused for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation (particularly AMR-to-text / structured-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey reports DUALENC significantly improves text quality compared to single-encoder models, particularly for structured inputs like trees or graphs (qualitative summary without aggregated numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Bridges gap between structural encoding and sequence decoding; leverages both graph topology and sequential token context resulting in better alignment between input structure and generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased model complexity and resources; design and fusion choices matter for success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May not yield gains when training data is small or when the linearization already suffices; potential redundancy between encoders if not carefully regularized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8815.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STRUCTADAPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STRUCTADAPT / Structural Adapters in PLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that injects lightweight structure-aware adapter modules into pretrained language models to incorporate graph structure (AMR) without modifying the entire PLM, improving AMR-to-text performance while leveraging PLM's strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>structural adapters (adapter modules inserted into PLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adapter layers parameterize structural transformations (e.g., graph-aware attention or relation-aware transformations) and are inserted into frozen pretrained LMs so that graph structure influences contextualized representations during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, potentially other labelled graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph structure is integrated via adapters (no full serialization); adapters consume graph signals (node features / adjacency) and modulate LM hidden states accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (PLM-based graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey notes Ribeiro et al.'s STRUCTADAPT improves on prior approaches that either linearize graphs into PLMs or retrain entire encoders; it yields enhanced performance relative to earlier AMR-to-text baselines (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows reuse of large pretrained LMs with modest additional parameters; captures graph structure while retaining PLM benefits; computationally cheaper than full retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adapter design choices can be nontrivial; may not capture very deep structural reasoning unless adapters are sufficiently expressive.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If adapter capacity is too small or graph signals are noisy, improvements may be limited; integration with decoding still requires alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8815.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>graph-transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Transformer / Graph-to-Sequence Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based models adapted to operate on graph inputs by modifying attention or positional encodings to respect graph structure, used for AMR-to-text and KG-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Amr-to-text generation with graph transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph transformer (structure-aware transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Adapts transformer attention to graph nodes/edges by restricting or biasing attention with adjacency information, relation-aware attention, or graph positional encodings so that multi-head attention respects graph connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, knowledge graphs and other labeled directed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph nodes are embedded and attention is computed with graph-aware masks or biases (e.g., distance-based edge encodings) rather than feeding a serialized sequence; the encoder outputs node-contextualized embeddings for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation, KG-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey references graph-transformer approaches as competitive with GNN-based encoders and often outperforming simple linearization baselines, combining transformer expressivity with graph inductive biases (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages transformer scalability and pretraining techniques while modelling graph structure; flexible to incorporate relation labels and multi-hop context.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires careful design of attention masks/biases; may be memory-intensive for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large KGs with high-degree nodes can cause attention blowup; effectiveness depends on how graph distances/relations are encoded in attention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8815.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8815.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Sequence / Graph2Seq models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of models that directly encode graph-structured inputs (via either GNNs or graph-aware Transformer layers) and decode to a token sequence; includes many architectures referenced in the survey such as Graph2Seq AMR models and RDF verbalizers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-sequence encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph nodes/edges into contextual node embeddings using graph neural layers or graph-aware transformer layers and then uses an autoregressive decoder to generate text conditioned on those embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs, RDF/KGs, other directed labeled graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No linear conversion; message-passing or relation-aware attention produces node encodings and a decoder attends over node embeddings to produce text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text, KG/RDF-to-text, general graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey summarizes that Graph2Seq variants generally beat linearized seq2seq baselines on structure-rich tasks and are a common approach in recent literature (qualitative summary).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves relational structure and enables the decoder to directly attend to structured node representations; reduces need for complex planning postprocessing in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Architecture complexity and integration with pretrained LMs can be challenging; more parameters and compute than vanilla seq2seq.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Where training data is limited or when references diverge from graph facts, Graph2Seq models still suffer hallucinations and omissions; requires careful auxiliary supervision (content planning, ranking) to reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Systematic Review of Data-to-Text NLG.', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Enhancing AMR-to-text generation with dual graph representations <em>(Rating: 2)</em></li>
                <li>Bridging the structural gap between encoding and decoding for data-to-text generation <em>(Rating: 2)</em></li>
                <li>A Graph-to-Sequence Model for AMR-to-Text Generation <em>(Rating: 2)</em></li>
                <li>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data. <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation with graph transformer <em>(Rating: 2)</em></li>
                <li>NABU -Multilingual Graph-Based Neural RDF Verbalizer <em>(Rating: 1)</em></li>
                <li>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs <em>(Rating: 1)</em></li>
                <li>P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8815",
    "paper_id": "paper-267637261",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "linearization",
            "name_full": "Linearization / Sequence Serialization",
            "brief_description": "A method that converts graph-structured inputs into a linear token sequence so they can be consumed by sequence-to-sequence models; commonly used as a baseline representation for AMR and RDF graphs by serializing nodes and edges into text (e.g., PENMAN or triple lists).",
            "citation_title": "The WebNLG challenge: Generating text from RDF data.",
            "mention_or_use": "mention",
            "representation_name": "linearization / serialization",
            "representation_description": "Converts graphs into a single ordered token sequence by visiting graph elements (nodes, edge labels, triples) and emitting them as tokens or bracketed expressions (examples: PENMAN notation for AMR; subject-predicate-object lists for RDF). The serialization may include special tokens for node boundaries, relation labels, parentheses/brackets, and placeholders for values.",
            "graph_type": "AMR graphs (rooted directed acyclic graphs), RDF triple graphs / knowledge graphs",
            "conversion_method": "Textual serialization such as PENMAN notation for AMR (bracketed rooted notation) or listing RDF triples (subject; predicate; object) in a canonical order; sometimes simple depth-first or pre-order traversals are used to produce a deterministic sequence.",
            "downstream_task": "AMR-to-text generation, RDF/knowledge-graph-to-text (WebNLG), general graph-to-text training for language models",
            "performance_metrics": null,
            "comparison_to_others": "Paper states that linearization was the historical approach to feed graphs into seq2seq models but that graph encoders (GNNs / structure-aware encoders) often outperform plain linearization because they better preserve structural relations; specific numeric comparisons are reported in the cited original works rather than in this survey.",
            "advantages": "Simple to implement; compatible with any seq2seq or pretrained LM without architectural changes; leverages powerful text pretraining when using text-based PLMs.",
            "disadvantages": "Can lose explicit structural inductive biases of the graph; may lengthen input sequences; structural relations can be harder for decoder to exploit, leading to poorer handling of long-range or multi-hop structure.",
            "failure_cases": "When graphs contain rich relational structure (AMR semantics, multi-hop KG facts) linearized sequences can lead to degraded fidelity, poorer content selection and more hallucination compared to structure-aware encoders; performance drops noted in AMR and KG tasks in comparative studies referenced by the survey.",
            "uuid": "e8815.0",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GNN",
            "name_full": "Graph Neural Network / Graph Encoder",
            "brief_description": "Encoders that operate directly on graph structures (e.g., GCN, GAT) to produce node- and graph-level representations which are then consumed by a decoder to generate text; widely used for AMR-to-text and KG-to-text tasks in the surveyed literature.",
            "citation_title": "A Graph-to-Sequence Model for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "graph encoding (GNN)",
            "representation_description": "Encodes nodes and edges using graph convolutions/attention so each node representation aggregates neighbor information across relation types and distances. The global or pooled representations and per-node embeddings are provided to a decoder (often seq2seq) for surface realization.",
            "graph_type": "AMR graphs (semantic graphs), RDF / knowledge graphs, other labelled directed graphs",
            "conversion_method": "No serialization; the graph structure is preserved and processed by message-passing (GCN/GAT) or other graph convolution layers to compute contextualized node embeddings; those embeddings are used directly by an attention-based decoder.",
            "downstream_task": "AMR-to-text generation, KG-to-text / RDF verbalization, table/graph-to-text generation",
            "performance_metrics": null,
            "comparison_to_others": "Survey reports that graph encoders generally outperform plain seq2seq on AMR-to-text and other structure-rich inputs by better capturing relations; 18 surveyed papers used GNNs and reported improvements vs linearized baselines (qualitative summary in the survey; no aggregate numeric metric provided).",
            "advantages": "Preserves graph topology and relation information; improves fluency and factual fidelity for structure-rich inputs; reduces some hallucinations tied to structure loss.",
            "disadvantages": "Requires architectural changes to incorporate into pretrained text-only LMs; can be computationally heavier; needs careful handling to align encoder graph structure with decoder sequence generation.",
            "failure_cases": "May still hallucinate factual content not grounded in input; effectiveness depends on graph encoder design and compatibility with decoding stage; limited gains when input can be represented compactly as a sequence or when training data is small.",
            "uuid": "e8815.1",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Penman",
            "name_full": "PENMAN Notation for AMR",
            "brief_description": "A human- and machine-readable bracketed textual format for representing AMR graphs, used both as a storage/annotation format and as an input serialization for text generation systems.",
            "citation_title": "Penman: An open-source library and tool for amr graphs.",
            "mention_or_use": "mention",
            "representation_name": "PENMAN linearization",
            "representation_description": "Represents AMR graphs as nested parenthesized expressions with relation labels and concept nodes (e.g., (s / see-01 :ARG0 (p / person))) so that the graph is a deterministic string which can be used as input to sequence models.",
            "graph_type": "AMR (rooted, directed, acyclic semantic graphs)",
            "conversion_method": "Emit PENMAN text by traversing the AMR graph and writing nodes and labeled edges using parentheses and role labels; used directly as token sequence for seq2seq models or as a standardized interchange format.",
            "downstream_task": "AMR-to-text generation and AMR parsing pipelines",
            "performance_metrics": null,
            "comparison_to_others": "Survey notes PENMAN is a canonical AMR serialization used widely; compared to graph encoders, PENMAN-based linearization is simpler but may underperform because of loss of explicit graph inductive bias.",
            "advantages": "Standardized, widely-supported format; easy interoperability and direct use with text-based LMs; human readable.",
            "disadvantages": "Loses explicit graph inductive structure for sequence models; long PENMAN strings can be unwieldy; structural relations must be re-learned by decoder from tokens.",
            "failure_cases": "For complex AMR graphs or when structural reasoning is required, PENMAN linearization can lead to poorer factual fidelity and higher hallucination compared to structure-aware encoders.",
            "uuid": "e8815.2",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RDF-triples",
            "name_full": "RDF triple set serialization (WebNLG style)",
            "brief_description": "Representation used by WebNLG: sets of RDF (subject, predicate, object) triples paired with natural language references; triples are serialized into an input string or treated as structured inputs to graph encoders for KG-to-text tasks.",
            "citation_title": "The WebNLG challenge: Generating text from RDF data.",
            "mention_or_use": "mention",
            "representation_name": "RDF triple serialization / triple set input",
            "representation_description": "Inputs are sets of RDF triples (subject, predicate, object). Conversion can be simple list/line concatenation of triples or encoding via a graph encoder; WebNLG experiments often feed triples as either linearized sequences or as graph inputs to structure-aware encoders.",
            "graph_type": "RDF graphs / Knowledge Graphs extracted from DBpedia or similar sources",
            "conversion_method": "Either (a) serialize triples in a deterministic order (subject|predicate|object tokens) for seq2seq models or (b) construct an RDF graph and encode via GNN/graph transformer that respects triple connectivity.",
            "downstream_task": "Knowledge-graph / RDF-to-text (WebNLG challenge), verbalization of KG facts",
            "performance_metrics": null,
            "comparison_to_others": "Survey reports that WebNLG uses RDF triple sets and that structure-aware encoders or PLM adaptations often outperform naive triple-serialization baselines; WebNLG also uses seen/unseen domain splits to test generalization.",
            "advantages": "Triples are a natural and compact representation of KG facts; when paired with structure-aware encoders they help produce faithful verbalizations and can generalize across domains.",
            "disadvantages": "Simple serialization can lose connectivity information (e.g., which triples link via shared nodes); graphs with many triples can produce long inputs; domain shift (unseen entities/predicates) can be challenging.",
            "failure_cases": "Divergent or incomplete reference texts (references that add content not in triples) complicate evaluation; naive serialization increases omission and hallucination risk versus structure-aware approaches.",
            "uuid": "e8815.3",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "dual-graph",
            "name_full": "Dual Graph Representations (Ribeiro et al.)",
            "brief_description": "A method that encodes a single AMR graph via two complementary graph encoders (top-down and bottom-up) so the decoder benefits from multiple structural perspectives of the same graph.",
            "citation_title": "Enhancing AMR-to-text generation with dual graph representations",
            "mention_or_use": "mention",
            "representation_name": "dual graph representation (top-down + bottom-up)",
            "representation_description": "Encodes the graph using two complementary message-passing passes (e.g., top-down contextualization and bottom-up aggregation) producing dual node representations which are fused for decoding, aiming to capture different structural views.",
            "graph_type": "AMR semantic graphs",
            "conversion_method": "Graph remains a graph; two GNN encoders run with different propagation directions or objectives to produce distinct node embeddings that are combined (concatenated / fused) before being supplied to the decoder.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": null,
            "comparison_to_others": "Survey states Ribeiro et al. showed improved text quality over single-encoder baselines (qualitative summary); the dual representation captures complementary information and reduces structural gaps.",
            "advantages": "Captures richer structural information by combining complementary node-context views; empirically shown to improve AMR-to-text quality in the cited work.",
            "disadvantages": "More complex encoder (two encoders) increases computational cost and model complexity; requires design choices for fusion of dual representations.",
            "failure_cases": "Increases training and inference costs; may still fail when semantic information is missing from the AMR or when references diverge significantly from graph facts.",
            "uuid": "e8815.4",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DUALENC",
            "name_full": "DUALENC (Dual encoding graph + linear)",
            "brief_description": "A dual-encoder architecture that encodes both the graph structure (with a GCN) and a linearized form of the input, addressing the structural gap between encoding and decoding to improve data-to-text quality.",
            "citation_title": "Bridging the structural gap between encoding and decoding for data-to-text generation.",
            "mention_or_use": "mention",
            "representation_name": "dual encoding (graph encoder + linear encoder)",
            "representation_description": "Simultaneously encodes the input as a graph (via a GCN/graph encoder) and as a linear sequence (via an RNN/seq encoder) to provide the decoder with both structural and sequential cues; may include an intermediate content planning stage.",
            "graph_type": "AMR graphs, tree-structured or general directed graphs used in data-to-text",
            "conversion_method": "No conversion to single format; two parallel encoders process the same data in different forms (graph and linearized sequence) and outputs are fused for decoding.",
            "downstream_task": "Data-to-text generation (particularly AMR-to-text / structured-to-text)",
            "performance_metrics": null,
            "comparison_to_others": "Survey reports DUALENC significantly improves text quality compared to single-encoder models, particularly for structured inputs like trees or graphs (qualitative summary without aggregated numbers).",
            "advantages": "Bridges gap between structural encoding and sequence decoding; leverages both graph topology and sequential token context resulting in better alignment between input structure and generated text.",
            "disadvantages": "Increased model complexity and resources; design and fusion choices matter for success.",
            "failure_cases": "May not yield gains when training data is small or when the linearization already suffices; potential redundancy between encoders if not carefully regularized.",
            "uuid": "e8815.5",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "STRUCTADAPT",
            "name_full": "STRUCTADAPT / Structural Adapters in PLMs",
            "brief_description": "An approach that injects lightweight structure-aware adapter modules into pretrained language models to incorporate graph structure (AMR) without modifying the entire PLM, improving AMR-to-text performance while leveraging PLM's strengths.",
            "citation_title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "structural adapters (adapter modules inserted into PLMs)",
            "representation_description": "Adapter layers parameterize structural transformations (e.g., graph-aware attention or relation-aware transformations) and are inserted into frozen pretrained LMs so that graph structure influences contextualized representations during fine-tuning.",
            "graph_type": "AMR graphs, potentially other labelled graphs",
            "conversion_method": "Graph structure is integrated via adapters (no full serialization); adapters consume graph signals (node features / adjacency) and modulate LM hidden states accordingly.",
            "downstream_task": "AMR-to-text generation (PLM-based graph-to-text)",
            "performance_metrics": null,
            "comparison_to_others": "Survey notes Ribeiro et al.'s STRUCTADAPT improves on prior approaches that either linearize graphs into PLMs or retrain entire encoders; it yields enhanced performance relative to earlier AMR-to-text baselines (qualitative claim).",
            "advantages": "Allows reuse of large pretrained LMs with modest additional parameters; captures graph structure while retaining PLM benefits; computationally cheaper than full retraining.",
            "disadvantages": "Adapter design choices can be nontrivial; may not capture very deep structural reasoning unless adapters are sufficiently expressive.",
            "failure_cases": "If adapter capacity is too small or graph signals are noisy, improvements may be limited; integration with decoding still requires alignment.",
            "uuid": "e8815.6",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "graph-transformer",
            "name_full": "Graph Transformer / Graph-to-Sequence Transformer",
            "brief_description": "Transformer-based models adapted to operate on graph inputs by modifying attention or positional encodings to respect graph structure, used for AMR-to-text and KG-to-text generation.",
            "citation_title": "Amr-to-text generation with graph transformer.",
            "mention_or_use": "mention",
            "representation_name": "graph transformer (structure-aware transformer encoder)",
            "representation_description": "Adapts transformer attention to graph nodes/edges by restricting or biasing attention with adjacency information, relation-aware attention, or graph positional encodings so that multi-head attention respects graph connectivity.",
            "graph_type": "AMR graphs, knowledge graphs and other labeled directed graphs",
            "conversion_method": "Graph nodes are embedded and attention is computed with graph-aware masks or biases (e.g., distance-based edge encodings) rather than feeding a serialized sequence; the encoder outputs node-contextualized embeddings for decoding.",
            "downstream_task": "AMR-to-text generation, KG-to-text generation",
            "performance_metrics": null,
            "comparison_to_others": "Survey references graph-transformer approaches as competitive with GNN-based encoders and often outperforming simple linearization baselines, combining transformer expressivity with graph inductive biases (qualitative).",
            "advantages": "Leverages transformer scalability and pretraining techniques while modelling graph structure; flexible to incorporate relation labels and multi-hop context.",
            "disadvantages": "Requires careful design of attention masks/biases; may be memory-intensive for large graphs.",
            "failure_cases": "Large KGs with high-degree nodes can cause attention blowup; effectiveness depends on how graph distances/relations are encoded in attention.",
            "uuid": "e8815.7",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Graph2Seq",
            "name_full": "Graph-to-Sequence / Graph2Seq models",
            "brief_description": "A family of models that directly encode graph-structured inputs (via either GNNs or graph-aware Transformer layers) and decode to a token sequence; includes many architectures referenced in the survey such as Graph2Seq AMR models and RDF verbalizers.",
            "citation_title": "A Graph-to-Sequence Model for AMR-to-Text Generation.",
            "mention_or_use": "mention",
            "representation_name": "graph-to-sequence encoding",
            "representation_description": "Encodes graph nodes/edges into contextual node embeddings using graph neural layers or graph-aware transformer layers and then uses an autoregressive decoder to generate text conditioned on those embeddings.",
            "graph_type": "AMR graphs, RDF/KGs, other directed labeled graphs",
            "conversion_method": "No linear conversion; message-passing or relation-aware attention produces node encodings and a decoder attends over node embeddings to produce text.",
            "downstream_task": "AMR-to-text, KG/RDF-to-text, general graph-to-text generation",
            "performance_metrics": null,
            "comparison_to_others": "Survey summarizes that Graph2Seq variants generally beat linearized seq2seq baselines on structure-rich tasks and are a common approach in recent literature (qualitative summary).",
            "advantages": "Preserves relational structure and enables the decoder to directly attend to structured node representations; reduces need for complex planning postprocessing in many cases.",
            "disadvantages": "Architecture complexity and integration with pretrained LMs can be challenging; more parameters and compute than vanilla seq2seq.",
            "failure_cases": "Where training data is limited or when references diverge from graph facts, Graph2Seq models still suffer hallucinations and omissions; requires careful auxiliary supervision (content planning, ranking) to reduce errors.",
            "uuid": "e8815.8",
            "source_info": {
                "paper_title": "A Systematic Review of Data-to-Text NLG.",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Enhancing AMR-to-text generation with dual graph representations",
            "rating": 2,
            "sanitized_title": "enhancing_amrtotext_generation_with_dual_graph_representations"
        },
        {
            "paper_title": "Bridging the structural gap between encoding and decoding for data-to-text generation",
            "rating": 2,
            "sanitized_title": "bridging_the_structural_gap_between_encoding_and_decoding_for_datatotext_generation"
        },
        {
            "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation",
            "rating": 2,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data.",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "AMR-to-text generation with graph transformer",
            "rating": 2,
            "sanitized_title": "amrtotext_generation_with_graph_transformer"
        },
        {
            "paper_title": "NABU -Multilingual Graph-Based Neural RDF Verbalizer",
            "rating": 1,
            "sanitized_title": "nabu_multilingual_graphbased_neural_rdf_verbalizer"
        },
        {
            "paper_title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
            "rating": 1,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation",
            "rating": 1,
            "sanitized_title": "p_2_a_planandpretrain_approach_for_knowledge_graphtotext_generation"
        }
    ],
    "cost": 0.0116341,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Systematic Review of Data-to-Text NLG
27 Feb 2024</p>
<p>Chinonso Cynthia Osuji chinonso.osuji@adaptcentre.ie 
ADAPT Research Centre
Ireland</p>
<p>Dublin City University
Ireland</p>
<p>Thiago Castro Ferreira thiagocf05@ufmg.br 
Federal University of Minas Gerais (UFMG)
Brazil</p>
<p>Brian Davis brian.davis@adaptcentre.ie 
ADAPT Research Centre
Ireland</p>
<p>Dublin City University
Ireland</p>
<p>A Systematic Review of Data-to-Text NLG
27 Feb 2024D9FB667ED46F57A28D0F895F95FD6916arXiv:2402.08496v3[cs.CL]NLGD2TAMRMRMRSSQLRDF
This systematic review undertakes a comprehensive analysis of current research on data-to-text generation, identifying gaps, challenges, and future directions within the field.Relevant literature in this field on datasets, evaluation metrics, application areas, multilingualism, language models, and hallucination mitigation methods is reviewed.Various methods for producing high-quality text are explored, addressing the challenge of hallucinations in data-to-text generation.These methods include re-ranking, traditional and neural pipeline architecture, planning architectures, data cleaning, controlled generation, and modification of models and training techniques.Their effectiveness and limitations are assessed, highlighting the need for universally applicable strategies to mitigate hallucinations.The review also examines the usage, popularity, and impact of datasets, alongside evaluation metrics, with an emphasis on both automatic and human assessment.Additionally, the evolution of data-to-text models, particularly the widespread adoption of transformer models, is discussed.Despite advancements in text quality, the review emphasizes the importance of research in low-resourced languages and the engineering of datasets in these languages to promote inclusivity.Finally, several application domains of data-to-text are highlighted, emphasizing their relevance in such domains.Overall, this review serves as a guiding framework for fostering innovation and advancing data-to-text generation.</p>
<p>Introduction</p>
<p>"Natural Language Generation" (NLG) is a specific branch of artificial intelligence that deals with the conversion of non-linguistic data or information representations into text.Its primary objective is to develop computer systems capable of generating understandable and coherent text in human languages, such as English and helping to boost communication between humans and machines [1].Natural language generation techniques are used in summarization [2], text simplification [3], machine translation [4], image captioning [5], dialogue generation [6], and question answering [7; 8].As a subfield of natural language processing, its applications have evolved to accommodate two categories depending on the nature of the input: text-to-text generation and datato-text generation.Data-to-text is defined as the task of generating comprehensible texts from structured inputs [1].These structured inputs or data can be table records [9; 10], graphs [11; 12], charts [13], or databases [14].They can also be images, such as in image captioning, but we will focus on non-image structured data for this study.The data-to-text field aims to simplify complex data and provide easy comprehension and access to a broader, unspecialized, or specific audience [15; 16].</p>
<p>The initial approach to data-to-text generation employed a modular pipeline architecture.In this setup, each module was typically addressed using a rule-based method [1].However, owing to the progress technology has made over the years, with the introduction of powerful GPUs, large storage devices, and deep neural networks.The need to write only rules became obsolete since the model can understand and follow input patterns to generate desired text at a much faster computation rate.As a rapidly growing field of research, tasks such as weather forecasts [17], sports news reporting [18], financial reports [19], robo-journalism [12], health care [20], and autobiographies [21] etc., which require textual summaries from structured data, can now be potentially automated with this technology.</p>
<p>Overview</p>
<p>This section provides a concise overview of the diverse aspects of data-to-text systems, encompassing traditional, statistical, and contemporary neural approaches.Aiming to offer insights into the evolving trends and methodologies employed in the field of data-to-text generation.</p>
<p>Traditional Data-to-Text Systems</p>
<p>Traditional systems for data-to-text generation are commonly based on rules or utilize sets of templates created by humans.These templates include placeholders for slot values filled with dialogue inputs during execution [1; 15; 22; 23].In the process of templatization, natural language expressions are transformed into templates, where words directly representing data are replaced with slots according to rules derived from the text and consistencies in the data.The data-to-template generation is then applied to these templates, resulting in the generation of template sentence texts [23].However, the text generation process of this system is divided into distinct stages or modules, each dedicated to a specific task.Reiter and Dale [1] proposed a traditional five-module pipeline architecture for data-to-text generation, addressing the questions of "What to say?" and "How to say it?".These stages include:</p>
<ol>
<li>Content selection: Determines the information to be mentioned in the text.2. Content ordering: Arrange this information in their appropriate sequences in the text.3. Content aggregation/structuring: Organises this information in separate sentences and paragraphs.4. Lexicalization: Finds appropriate phrases or words that best relay the message in the sentence.5. Referring expression generation: Generates referring expressions (references, coreferences), like proper nouns, he, she, they, and him, to the entities in the text/discourse where necessary.6. Surface realization: It combines the output of all the other steps toward generating a complete text from the input data.</li>
</ol>
<p>This system provides built-in faithfulness to input, a carefully regulated style, and quick response times, rendering them an attractive option.Nevertheless, they need help in scalability since creating new templates for diverse responses is necessary, and templates from one domain may not consistently apply to other domains.Despite substantial time and resource investments to incorporate linguistic details into the templates, they often need more contextual understanding, and the restricted template set hampers the system's overall naturalness [22].</p>
<p>Statistical Data-to-Text Systems</p>
<p>Statistical systems for summarizing data into text employ probabilistic models, such as Hidden Markov Models (HMM) [9; 24] and alignment learning [25], to transform non-linguistic data into human-readable text.These models, specifically a probabilistic generative model, operate by concurrently segmenting text into utterances and mapping each utterance to a meaning representation grounded in the world state [25].</p>
<p>Operating on a probabilistic foundation, these models predict the most likely subsequent word in the target sequence based on the input data sequence.Generative models, designed to address multiple ambiguities with a focus on aligning utterances to facts, concentrate on the probability distribution for each word during alignment learning [25].This modeling of the intrinsic distribution of data points is achieved through joint probability, where the input and output coexist.The result is effective alignment of utterances to facts, minimizing the need for extensive supervision, adept handling of multiple ambiguities, and demonstration of generalizability across diverse domains [25].</p>
<p>Neural Data-to-Text Systems</p>
<p>In recent times, advances in generative models have offered fluent, more natural texts and data-driven scaling narratives as compared to traditional systems [22].Modern data-to-text generation involves the production of natural language text descriptions in sequences that explain non-linguistic data.Let D represent the data pairs {r j , s j } N j=1 of N instances of the data records r, mapped to its human generated summaries s in an n sequence of words (w 1 , ..., w n ).This process aims to map and learn a correlation between these data pairs and generate text based on these learned properties.Several techniques have been employed in learning these latent variables, some of which are the seq-to-seq model, seq-to-seq model with copy mechanism [26; 27], and the transformer attention models [28; 29].Notably, transformer attention models focus on specific properties deemed relevant during the text generation process.These properties are critical for capturing contextual relationships and enhancing the overall quality of generated text.</p>
<p>One of the significant problems in neural text generation is the occurrence of hallucinations, repetitions, omissions, inconsistencies, and a lack of coherence, which can compromise the quality and credibility of the content [7; 30].Furthermore, there is a need for more resources and datasets for languages other than English, which hinders the development and enhancement of models for these languages [7].Addressing these issues is critical for improving the accuracy and efficacy of language models in generating high-quality content.Improved deep learning models that prioritize error reduction need to be developed, and more diverse datasets and resources in multiple languages should be available to train and validate these models.</p>
<p>Related Surveys</p>
<p>A systematic review of the literature on data-to-text is required to encapsulate trends, find critical challenges and techniques, and fill in the information gaps.Several literature surveys in the field of NLG have tried to show the contributions made in the field, with studies focusing more on other text generation tasks, their training and generation methods [7], a systematic review of text generation tasks [31], its applications areas [15], hallucination and semantic adequacy measures [30; 32], evaluation metrics [33], and the evolution of deep learning models in NLG [34].A related study by Sharma et al. [35] captures the advancements in data-to-text techniques, datasets, and evaluation methods.In our study, we will expound on the existing literature, methods, languages, application areas, hallucination mitigation measures, and quality of the generated texts using structured data.</p>
<p>For this study, we will only consider structured data such as tables, RDF (Resource Description Framework) [36], knowledge bases or graphs [37], SQL (Structured Query Language) [14], AMR (Abstract Meaning Representation) [38], MR (Meaning Representation) [9], and MRS (Minimal Recursion Semantics) [39].We will also consider studies that focus on data-to-text generation and extract meaningful information about our research questions.</p>
<p>The first chapter focuses on the need for this systematic review and its structure, including how each chapter is organized.In Section 2, we will discuss the methodologies following the PRISMA 2020 [40] techniques for collecting and selecting relevant papers for this study.Section 3 discusses the results of the metadata extracted from the studies.The last sections 4, 5 and 6 offer discussions, recommendations and future directions, as well as the conclusion of the study.</p>
<p>Methodology</p>
<p>Our survey on data-to-text adopts a systematic review approach, following the guidance set forth in the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement and Page et al. [40] systematic review recommendations.We commence by formulating core research questions as the cornerstone of our investigation.Subsequently, we outline our search strategies and the electronic databases utilized for this purpose.Inclusion and exclusion criteria are applied to refine the paper selection, and we conclude by defining our procedures for data extraction and synthesis.</p>
<p>Research Questions</p>
<p>To gain a comprehensive understanding of the data-to-text domain and guide our research endeavors, we have formulated a primary research question.From this central query, we have derived a set of sub-questions, each illuminating a distinct facet of this domain.In the course of this paper, we will delve into these sub-questions in detail.Our primary research question is defined as follows: RQ: What does the existing literature in Natural Language Generation (NLG) reveal about text generation using structured data as input?Below, we present the derived sub-questions, each offering insight into specific aspects of our inquiry.These sub-questions will serve as the framework for our exploration: RQ1.Which standard datasets are commonly utilized for data-to-text generation in the literature?RQ2.Which languages are prevalent in data-to-text generation literature?RQ3.What are the techniques and design methods typically employed in data-to-text generation?RQ4.What measures are commonly employed to mitigate hallucinations in the generated text?RQ5.What are the prominent evaluation metrics used to assess the quality of generated texts?RQ6.Which application areas are explored in the context of data-to-text generation?</p>
<p>Search Strategies</p>
<p>We meticulously curated literature from various esteemed databases, focusing primarily on studies presented at conferences and published in journals renowned for their contributions to the field of Natural Language Generation.The information in Table 1 enumerates the conferences and journals pivotal to our research, while Figure 1b displays the categorization of venues as conferences or journals for the study publications.Our search efforts extended across various databases, including Google Scholar 1 , ACL anthology 2 , IEEE 3 , and Semantic Scholar 4 .To make the search more manageable, we downloaded a full Bibtex anthology with abstracts on the ACL anthology web page, deleted the bibliographies of studies less than 2017 and imported it into the Mendeley desktop application.Then, we implemented the search strategies on the bibliographies in the Mendeley desktop application and also conducted searches on other listed databases.It's worth highlighting that our database searches were conducted from September to October 2022.</p>
<p>Venues</p>
<p>Count Citations EMNLP 5  26 [29], [41], [42], [43], [44], [45], [46], [9], [14], [47], [2], [48], [49], [50], [51], [52], [53], [10], [54], [55], [56], [38], [57], [58], [59], [60] ACL 6  34 [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [24], [79], [80], [81], [82], [26], [83], [84], [39], [85], [86], [4], [87], [88], [89], [90] COLING 7  5 [91], [92], [93], [94]  11  13 [36], [98], [99], [27], [23], [100], [101], [102], [28], [103], [12], [104],</p>
<p>[105] NLP4ConvAI 12  1 [37] ISWC 13  1 [106] SIGGEN 14  4</p>
<p>[107], [108], [109], [110] Information Sciences 15  1</p>
<p>[16] Data Mining and Knowledge Discovery 16   1 [111] Table 1: Conference and Journal venues used for our search.</p>
<p>Study Screening and Selection</p>
<p>Through the application of our search strategies, we amassed a substantial corpus of 1,078 pieces of literature.However, our dataset settled at 1,052 unique studies after diligently purging duplicate entries.The inclusion and exclusion criteria for this study are outlined below.</p>
<p>Inclusion Criteria 1. Focus on data-to-text generation, even if other text generation forms are considered.2. Publication within the period from 2017 to 2022. 3. Publications in high-impact factor journals and conferences, spanning from A1-B2 and Q1-Q2 categories.4. Inclusion of survey papers and shared task papers in data-to-text generation. 5. Literature written exclusively in English.6. Minimum of 5 citations.The extensive dataset obtained through our search strategy underwent thorough filtering, during which we carefully applied our inclusion and exclusion criteria.This process whittled down our dataset to a more manageable 635 studies.Our commitment to precision persisted as we further refined our dataset with updated inclusion and exclusion criteria, as shown above.Simultaneously, we gathered citation data for each study using Google Scholar.As a result, we excluded 412 studies from the original 635, retaining a robust set of 223 studies, each with citations equal to or exceeding 5. Subsequently, an in-depth analysis of these 223 studies was undertaken through a comprehensive review of abstracts and contents.Papers that did not align with the eligibility criteria were meticulously filtered out by carefully reading the abstracts.In cases where abstracts were insufficient for determining eligibility, the entire paper was thoroughly examined.This meticulous process culminated in the selection of a final set of 90 papers for detailed examination and inclusion in our research.Notably, exclusions were made for papers not written in English, lacking human evaluation, or lacking a link to the code implementation.Exceptions were granted for 5 papers without human evaluation and 11 papers without code availability.Figure 1a illustrates the distribution of our papers across the chosen years.</p>
<p>Data Extraction and Synthesis</p>
<p>The data extracted from the 90 selected studies encompassed details such as the dataset used, the methodology employed, multilingual aspects, the evaluation metrics utilized, error mitigation strategies, and application areas.This information was systematically organized into a table and subjected to further analysis.The data synthesis, which relies on the information obtained during the extraction process, will be elaborated upon in the following section.Our categorization of datasets in the analysis encompasses eight distinct structured data types: Table, AMR, RDF, MR, SQL, Graph, JSON, and MRS.</p>
<p>Results</p>
<p>This section conducts an extensive analysis by synthesizing data from published papers.Detailed findings are presented, illuminating key insights and prevalent trends in the research landscape.Through rigorous examination, the section aims to provide a nuanced comprehension of the research findings and their broader implications.</p>
<p>Dataset</p>
<p>In this section, we comprehensively analyze the datasets utilized in the selected papers.We have identified a total of 63 distinct datasets used across these studies.Among these, WebNLG, E2E, AMR, RotoWire, WikiBio, ViGGO, ToTTo, and WMT are the most frequently employed datasets.WebNLG and E2E appear in 24 studies, followed by WikiBio and RotoWire, which is featured in 15 and 14 studies.Next is AMR-LDC2017T10, which are used in 12 studies, while AMR-LDC2015E86 and Wikipedia appears in 8 and 5 studies, and ViGGO and ToTTo are each used in 4 studies.Additionally, WMT, MLB and AMR-LDC2020T02 are each featured in 3 studies.The "Other" category includes datasets that occur only twice or once.For more details on these datasets and their occurrences, please refer to Table 2.</p>
<p>Having categorized our datasets into distinct types as displayed in Table 3, the most prevalent data type is the table, which is featured in 40 of the selected studies.Following closely are RDF, MR, and AMR, which are highlighted in 27, 26, and 17 papers, respectively, underscoring their pivotal roles in the realm of data-to-text generation research.In contrast, Graph, SQL, JSON, and MRS appear in only 5, 2, 1, and 1 papers, respectively, reflecting the diverse array of data sources that researchers harness in this field and emphasizing its inherent complexity and versatility.MR 24 [9], [98], [99], [48], [27], [68], [100], [102], [28], [11], [54], [91], [74], [58], [42], [85], [61], [77], [78], [105], [24], [63], [60] [65], [84], [87], [51], [55], [56], [38] LDC2017T10 AMR 12 [37], [84], [72], [2], [87], [91], [51], [59], [62], [56], [38], [ [46], [96], [10], [67], [70], [85], [47], [52], [16], [89], [101], [76], [43], [ [36], [27], [83], [49], [66], [11], [103], [71], [54], [107], [91], [74], [88], [108], [106], [109], [110], [104], [37], [61], [90], [44], [24],</p>
<p>[63] WikiBio Table 15 [9], [10], [66], [54], [85], [26], [29], [41], [53], [111], [11], [104], [82], [60] WikiSQL Table 2 [85], [14] WikiTableQuestions Table 1 [85] Wikipedia MR 5 [97], [39], [73], [53], [102] Table 2: Datasets Used in Selected Papers.Knowledge Graph (KG).</p>
<p>Data Type Frequency Citations Table 40 [46], [96], [9], [10], [66], [103], [67], [70], [54], [85], [47], [73], [52], [69], [82], [88], [16], [26], [92], [29], [41], [89], [53], [101], [111], [76], [44], [23], [97], [57], [11], [86], [45], [75], [42], [43], [60], [80], [22],</p>
<p>[104] RDF 27 [36] [83], [49], [103], [71], [54], [50], [107], [85], [91], [74], [88], [27], [61], [90], [44], [108], [57], [11], [95], [75], [106], [109], [110], [24], [63], [104] MR 26 [9], [103], [98], [54], [85], [99], [68], [48], [91], [74], [27], [61], [100], [102], [77], [28], [11], [86], [78], [58], [42], [105], [94], [24], [63],</p>
<p>[60] AMR 17 [64], [65], [4], [37], [84], [72], [2], [87], [91], [51], [59], [62], [55], [56], [38], [81], [79] Graph 5 [37], [90], [93], [12],
[11] SQL 2 [14], [85] JSON 1 [11] MRS 1 [39]
Table 3: Data type frequency.</p>
<p>Overview of Datasets</p>
<p>Here is a brief overview of some prevalent datasets mentioned in the previous section: WebNLG The WebNLG dataset [36], introduced in 2017, is a pivotal resource used in the WebNLG 2017 challenge.It contains RDF triples from DBPedia, each paired with text descriptions and human-generated reference texts in English.This dataset serves as a critical tool for training and evaluating the planner component.It comprises 9,674 unique triple sets and 25,298 text references, divided into training, development, and test sets.The test set includes both seen and unseen domains, allowing for the evaluation of model generalizability.Additionally, the WebNLG 2020 dataset [107] expands on this resource with Russian data.This addition was achieved through translation and post-editing, aimed at fostering multilingual capabilities and providing essential statistics for evaluating Natural Language Generation systems in the Semantic Web domain.</p>
<p>E2E The E2E [112] dataset is a crucial resource for training end-to-end, datadriven natural language generation systems, specifically in the restaurant domain.It was gathered through crowdsourcing and meticulous quality control, using images as stimuli to evoke more natural and well-articulated human references than textual MRs.This dataset, openly released as part of the E2E NLG challenge, is approximately ten times larger than its predecessors.It introduces novel challenges due to its size, lexical diversity, syntactic intricacies, and discourse complexities.Learning from this dataset promises to generate more natural, diverse, and less template-like system utterances.It comprises a rich set of 50,602 English verbalizations paired with 5,751 dialogue-act-based meaning representations.The dataset is thoughtfully partitioned into training, validation, and testing subsets, maintaining a consistent distribution of MR and reference text lengths while ensuring MR variation across different sets.Each MR encompasses 3-8 attributes or slots, including information such as name, food, area, and corresponding values.</p>
<p>AMR The Abstract Meaning Representation (AMR) [113] dataset 17 , encompassing series such as LDC2011T07, LDC2015E86, LDC2016E25, LDC2017T10, LDC2020T02, and LDC2020T07, presents a structured representation of semantic information.AMR is represented as a rooted, directed, acyclic graph with labelled edges (relations) and nodes (concepts), capturing the essence of "who is doing what to whom".It serves as a foundation for generating sentences that convey the semantics encoded within the graph.Specifically, LDC2017T10 within this series comprises 36,521 training instances of AMR graphs in PENMAN notation [114], along with their corresponding texts.Additionally, it includes 1,368 development instances and 1,371 test instances, providing a substantial resource for research and development in AMR parsing and natural language understanding and generation.</p>
<p>WikiBio The WikiBio dataset [21] is a comprehensive collection of biographical information covering individuals from diverse professions.This extensive dataset comprises over 10 million records, providing details such as names, dates of birth, occupations, and education.Focused on curating 728,321 biographies sourced from English Wikipedia, it serves as a valuable resource for evaluating text generation algorithms.The dataset encompasses the initial paragraph of each biography alongside its associated infobox, both tokenized to facilitate processing.Arranged in a standardized tabular layout, it proves to be of great utility for a range of natural language processing tasks, including text generation, summarization, entity identification, and data extraction.Researchers can utilize this dataset for the development and evaluation of algorithms and models geared toward the analysis of biographical text.</p>
<p>RotoWire The RotoWire dataset [46], designed for table-to-text generation, offers a robust platform for generating human-like summaries from basketball game tables.With 4.9K examples and 1.6M tokens from rotowire.com, it includes game data and corresponding human-written summaries.RotoWire features extended texts and a diverse vocabulary, making content selection more challenging.It leverages table records to generate structured yet informal game summaries, appealing to those interested in game statistics.This dataset proves valuable for assessing data-to-document generation systems, especially in basketball game summaries.It presents the complex task of converting structured data into coherent, informative text, accommodating diverse audiences and writing styles.Researchers can employ it to develop and evaluate table-to-text generation models, advancing natural language generation in specific domains.</p>
<p>ToTTo The ToTTo dataset [10], is a valuable resource in table-to-text generation within an open-domain English context.It comprises a substantial training set of over 120,000 instances.Its primary objective is generating a controlled one-sentence description task based on the information in a Wikipedia table, mainly focusing on the highlighted table cells.The dataset's creation involved a meticulous process wherein noisy descriptions were meticulously paired with tables, and any inaccuracies or inconsistencies in the highlighted cells were diligently rectified through iterative refinement.ToTTo stands as a pivotal benchmark for advancing high-precision, faithful, and conditional text generation research.</p>
<p>WMT Since its inception in 2006, the Workshop on Machine Translation (WMT) has been a hub for machine translation shared tasks and competitions.Its impact extends to other fields of natural language generation, such as in multilingual datato-text generation [4; 48; 108].Initially centered around translation tasks, WMT has evolved to encompass various aspects, including biomedical, multimodal, and lowresource translation.The General primary Machine Translation task, previously known as the News Task, remains a core component.Additionally, WMT features evaluation tasks like Metrics and Quality estimation.Over the years, some tasks have been discontinued, but WMT's shared task results and datasets remain crucial benchmarks for advancing machine translation research.The WMT dataset is sourced from the OPUS corpus [115], and it consists of a parallel corpus in 18 languages, primarily from the news domain.It includes news commentary text extracted mainly from online news sources, with the test data containing about 1000 sentence pairs.While most languages are paired with English, some are also paired with languages other than English such as German, Russian, French, Spanish, Italian and Chinese.The competition offers training sets compiled from various sources [116; 117].</p>
<p>Viggo The ViGGO dataset [28] addresses limitations in existing data-to-text NLG corpora by focusing on video game descriptions and providing a more conversational context.It includes over 100 video game titles and their attributes, resulting in 2,300 structured meaning representations (MRs).These MRs cover nine different dialogue act types (DAs), making ViGGO suitable for open-domain dialogue systems.Crowdsourced reference utterances were collected for each MR, enabling neural language generation models to learn multiple ways of expressing the same content.While smaller in size compared to the E2E dataset, ViGGO offers greater lexical diversity, longer inform utterances, and a more natural-sounding context due to its grounding in real video game data.Additionally, ViGGO maintains a focus on shorter, conversational responses.</p>
<p>Language</p>
<p>Multilingualism has emerged as a crucial aspect of natural language generation within data-to-text field, showcasing the remarkable progress achieved in handling structured data across various languages.Data-to-text generation has faced numerous challenges when it comes to multilingual outputs, with English often dominating as the primary target language for generation tasks.In our analysis of the 90 selected papers, a striking trend is evident: 89 of them focused on generating summaries from structured data in English.While English takes the lead, other languages also make appearances, with German, Russian, French, Spanish, Italian, and Brazilian Portuguese featuring in 6, 4, 3, 2, 2, and 2 papers, respectively.A variety of other languages were utilized in just one paper each, as illustrated in Table 4.</p>
<p>Language</p>
<p>Count Citations English 77 [46], [64], [36], [65], [9], [98], [14], [99], [47], [74], [88], [56], [38], [57], [58], [81], [48], [82], [27], [23], [96], [83], [49], [66], [67], [84], [16], [92], [54], [72], [87], [91], [68], [51], [52], [69], [100], [53], [39], [101], [102], [28], [29], [41], [11], [10], [103], [77], [93], [75], [42], [110], [94], [43] ', [104], [37], [85], [61], [59], [89], [90], [62], [70], [71], [76], [97], [78], [45], [105], [24], [63], [60], [80], [22], [111], [2]  In our exploration of multilingualism and its role in data-to-text literature, we assessed 90 papers.Among them, 12 studies ventured into multilingual approaches, while the remaining 78 predominantly focused on single-language generation, as depicted in Figure 2. Notably, a noteworthy study by Fan et al. [55] introduced a multilingual method for generating text from AMRs across twenty-one (21) EUROPARL languages.Similarly, researchers such as Xu et al. [79], Nema et al. [26], and Moussallem et al. [106] [109], and Teixeira et al. [12], have centered their efforts on bilingual text generation within the data-to-text context.Significant advancements have indeed been achieved in the domain of multilingual data-to-text generation, particularly in English, Chinese, and select European languages.Nonetheless, it is apparent that additional research is essential to advance this field.This research is needed to enable proficient multilingual generation across a more diverse range of languages, including those with diverse morphological structures and word order characteristics, as emphasized in Fan et al. [55] work.</p>
<p>Models</p>
<p>Methodology</p>
<p>Counts Papers Copy Mechanism 27 [46], [65], [9], [98], [47], [27], [96], [83], [67], [51], [52], [53], [39], [101], [70], [71], [29], [42], [104], [54], [87], [73], [16], [89], [76], [45], [60] GNN 18 [65], [14], [4], [84], [51], [54], [87], [88], [56], [38], [106], [110], [59], [81], [71], [29], [80], [90] Hierarchical encoder 5 [80], [52], [54], [104], [80] HMMM 2 [9], [24] Transformer 38 [54], [106], [59], [98], [28], [10], [103], [70], [72], [91], [78], [45], [105], [81], [44], [77], [61], [104], [37], [73], [92], [29], [55], [108], [57], [58], [109], [42], [94], [24], [60], [22], [86], , [76], [97], [90], [85], [62] Table 5: Methodology Used in the Selected Papers</p>
<p>In the field of data-to-text generation, various methodologies have been explored, including templates, statistical models, and neural network-based generation models.A significant number of studies selected for this systematic review opted for neural network models, ranging from basic sequence-to-sequence models [118] to advanced designs such as transformers [119] and large pretrained language models.Several studies have introduced refinements to these models to enhance their performance and set new benchmarks.Notably, research conducted by 27 studies, integrated a copy mechanism that uses probabilistic strategies to determine when and which tokens should be directly copied from the reference data.This copy model is particularly useful for ensuring that all data values appear in the generated text [98].Building on this innovation, subsequent investigations by five (5) studies devised fused attention [26] and hierarchical attention [67], improving decoder focus on input tokens and enhancing overall generation quality.However, some of these studies employed recurrent neural networks with static embeddings, which have shown suboptimal performance across various application domains such as in AMR-to-text [65; 2], MR-to-text [68], table-to-text [26] and in data-to-text [54].</p>
<p>Another line of research has focused on retaining the graph structure of the data using graph encoders [65] in graph neural network (GNN).Originally, graph data were linearized into sequences to accommodate sequence-to-sequence models.However, graph encoding has emerged as a promising method in data-to-text generation, especially in AMR-to-text generation.Graph encoders offer improvements over basic sequence-to-sequence models by inherently learning the graph structure and existing relations in the encoder and using an ordinary decoder to generate desired texts.</p>
<p>Ribeiro et al. [51] leveraged dual graph representations in AMR-to-text generation, effectively encoding divergent but complementary perspectives of the structural information in the AMR graph by simultaneously learning top-down and bottom-up node representations [51].The study by Zhao et al. [71] introduced DUALENC, a dual encoding model that addresses the structural gap in data-to-text generation by incorporating both graph and linear structures.This approach significantly improves text quality compared to single-encoder models, especially for structured inputs like trees or graphs.The DUALENC [71] model integrates Graph Convolutional Network (GCN) encoders with an intermediate content planning stage.This combination allows the model to capture structural information and enhance the compatibility between input and output sequences [71].Another research Li et al. [80] introduces a hierarchical encoder equipped with a reasoning module for graph-based reasoning, which enhances the ability to capture various relations between records in different dimensions.This research also introduces auxiliary supervision tasks, including number ranking and importance ranking, to further improve the model's ability to handle different record relations [80].These advancements contribute significantly to the field of GNNs.Eighteen (18) papers were found to have used this method to improve on existing baselines.</p>
<p>To address the limitations of static embedding models, some research efforts have embraced transformer models such as RNN transformers [120; 121], BERT [119], T5 [122], BART [123], XLM [124], and GPT-2 [125] for data-to-text generation tasks.These transformer-based models incorporate contextualized embeddings and use positional encoding due to their non-recurrent nature.They are trained on a large corpus of online curated texts and seem to perform well across several domains after finetuning.A total of 38 studies incorporated transformer models into their research, as detailed in Table 5.</p>
<p>Hallucination Mitigation Measures</p>
<p>In the context of data-to-text, hallucination refers to the generation of content that lacks fidelity or is not supported by the source data provided [66].Divergence can also be considered a form of hallucination when it occurs in the reference text, signifying a deviation from the expected or accurate information [66].This highlights the importance of generating text that remains faithful to the underlying data and references.</p>
<p>To address and reduce errors and hallucinations in data-to-text generation, several strategies have been deployed.These strategies encompass a wide range of approaches, including dataset cleaning and standardization, the development of novel training modules and techniques, as well as the application of knowledge distillation methods [104].</p>
<p>In the subsequent subsections, we will delve into specific papers and elucidate the strategies they have employed to address challenges and improve data-to-text generation.This comprehensive exploration will provide valuable insights into the diverse approaches and techniques used in the field to enhance the quality and fidelity of generated text.</p>
<p>Dataset Refinement and Post Editing</p>
<p>Effective refinement of datasets and post-editing play an essential role in enhancing the quality and accuracy of data-to-text generation.Wang [101] made a significant contribution to this field by focusing on boosting factual accuracy.Their work involves developing Rotowire-FG, which stands for Fact Grounding-purified version of Rotowire [46].Within this context, they incorporated content normalization techniques aimed at boosting the overall accuracy of the generated text.These techniques involve actions like converting number words into their corresponding numerical values and standardizing mentions of entities, contributing to improved text fidelity and reliability [101].</p>
<p>In addition, the TOTTO dataset [10] facilitates the controlled generation of concise descriptions derived from Wikipedia tables.This initiative aimed to enhance controllability in data-to-text generation, addressing issues previously associated with crowd-sourced datasets, which is the incomplete alignment of information in the table and their corresponding summaries.Chen et al. [29] addresses logical-level generation, presenting the LOGIC2TEXT dataset designed for generating high-fidelity descriptions from logical forms.To address scientific data-to-text challenges, Suadaa et al. [76] emphasize numerical reasoning in textual descriptions, as evidenced by the creation and utilization of the numericNLG dataset [76].Collectively, these efforts improve data-to-text generation, enhancing text accuracy and coherence.</p>
<p>Furthermore, Shimorina and Gardent [27] focuses on handling rare items or entities in the generated text.Their approach involves post-processing the text to replace placeholders with the appropriate values based on a mapping between placeholders and initial values created during pre-processing.This strategy further enhances the fidelity of the generated text.</p>
<p>Training Techniques and Model Modification</p>
<p>In this field, Gong et al. [52] presents an innovative architecture, a hierarchical encoder for table-to-text generation that excels at encapsulating the intricacies of multi-dimensional table data.The model's ability to encode row, column, and time dimensions simultaneously enables it to generate text summaries that are both highly informative and coherent.A study Ribeiro et al. [59], introduces STRUCTADAPT, which leverages adapter modules to incorporate graph structure into pretrained language models (PLMs) for better Abstract Meaning Representation (AMR) to text generation, resulting in enhanced performance compared to prior approaches [59].</p>
<p>Neural generation models use various strategies to reduce hallucination, including soft templates [98], copy mechanisms, content planning, and structure-aware systems [54].Training methodologies have evolved significantly, with some studies using paired training with unlabeled text [64], and others implementing a two-tiered approach involving an information extraction model and an attention-based encoder-decoder text generation model [46].The JointGT model by Ke et al. [90] enhances Knowledge Graph to text generation tasks by leveraging graph structure and pre-training tasks.</p>
<p>On a different note, (2) papers delved into Reinforcement Learning (RL) for data-to-text generation.Rebuffel et al. [104] introduces PARENTing, a reinforcement learning framework that is model-agnostic.This framework fine-tunes pretrained models using self-critical policy gradient algorithms to minimize hallucination in text generation by addressing divergence in training examples.Another study used multitask learning and reinforcement learning to incorporate content selection mechanisms into the encoder-decoder models [82].</p>
<p>Controllabilty and Constraints decoding</p>
<p>Controllability is achieved in data-to-text generation by introducing constraints and supervision during the decoding process or in the decoder.Various studies have contributed to this field:</p>
<p> Lin et al. [42] [74] segments target text into fragments that align with data records, improving the control and interpretability of the generated output.This automatic segmentation, which adapts to domain-specific requirements, employs a soft statistical constraint to regularize the granularity of the segments [74]. Wang et al. [78] introduces Mention Flags (MF), a unique method that guarantees constraint satisfaction in Transformer-based text generation.By tracking the fulfilment of lexical constraints in the generated text and integrating them into the S2S Transformer models, MF ensures the creation of high-quality text that complies with the given constraints [78]. Lu et al. [86] presents NEUROLOGIC Aesque, an innovative decoding algorithm, inspired by A* search and designed for large-scale language models, that enables constrained text generation.This is achieved by combining heuristic cost estimates and logic-based lexical constraints, enhancing Constrained Machine Translation and Keyword-constrained generation [86]. Hardy and Vlachos [2] introduces a new method to improve AMR-based summarization by guiding it with the source document.This two-step process estimates the distribution of missing linguistic data and uses it to guide a seq2seq model, enhancing summary fluency and quality.</p>
<p>Ranking System</p>
<p>Eight (8) papers utilized rankers to improve the fidelity of the generated text [63; 105; 44; 83; 109; 11; 71; 80].The process of reranking in text generation, specifically within the decoder, is aimed at enhancing the quality and reducing semantic errors in the generated text.This involves the creation of rules or the use of an auxiliary classifier to verify if input slots are represented in the output, an important factor in maintaining semantic quality.Rerankers are commonly applied to the final hypotheses to enhance beam search and address its limitations.This can be achieved by establishing a reranking criterion or training a reranker to predict the best hypothesis within a beam based on function scores [63; 83; 11].The strategy employed involves over-generation, followed by reranking of potential outputs using criteria that were not explicitly optimized during training.The reranked outputs favour those with fewer missing or incorrect slot mentions, thereby enhancing accuracy and relevance [105; 63].This approach extracts meaningful information from encoder-decoder models and uses it to identify which attributes are mentioned in the generated text.</p>
<p>Additionally, A promising development in neural data-to-text generation is the introduction of a trainable evaluation metric.This metric, particularly useful when tables have multiple associated textual references, uses ranking models to assess the correctness of generated hypotheses by comparing them to the original table and corresponding references.It aims to overcome the limitations of existing metrics like BLEU, ROUGE, and METEOR, which do not fully capture the faithfulness of generated text to both the input table and references [44].</p>
<p>Pipeline and Planning Architecture Systems</p>
<p>In a survey of Natural Language Generation (NLG) architectures and methodologies, Gatt and Krahmer [15] categorizes NLG approaches into three main architectural paradigms: Modular, Planning, and Integrated (Global) architectures.These architectures encompass various generation systems, which are classified based on their methodological approach and design choices [15].To address challenges such as hallucination, omissions, and errors encountered in data-to-text generation, eleven (11) papers in total made use of this modular architecture.</p>
<p>Three (3) of these studies [23; 95; 12] resorted to traditional data-to-text methods.These methods typically involve a sequential process encompassing discourse ordering, text structuring, lexicalization, referring expression generation, and textual realization.These stages can be further categorized into macro planners, which combine content selection and document planning, and micro planners, which involve sentence aggregation, lexicalization, and referring expression generation [15].</p>
<p>In contrast, recent studies have witnessed a departure from traditional rule-based approaches in the initial planning stages.Instead, five (5) studies have adopted endto-end models for generating text, spanning from intermediate stages to the final surface realization.This transition aims to assess the efficacy of end-to-end models when compared to conventional rule-based or template systems [36; 11; 68; 107; 89].This shift underscores the dynamic evolution in NLG methodologies and architectural preferences.</p>
<p>Furthermore, four (5) investigations [83; 24; 16; 49; 71] have sought to compare the performance of neural modular architectures against end-to-end neural architectures.Collectively, these research findings indicate that supervised neural modularization or pipelining within data-to-text architectures leads to notable improvements in fluency, fidelity, and the overall quality of generated text summaries.These enhancements primarily result from error reduction during content selection, the model's ability to capture long-term structural dependencies, and the accurate ordering of facts [46; 49].Shao et al. [50] introduces the Planning-based Hierarchical Variational Model (PHVM) to address the limitations of existing neural methods in generating long and diverse texts in data-to-text generation tasks.The PHVM incorporates a planning mechanism and a hierarchical latent structure to capture inter-sentence coherence and generate varied expressions.By decomposing long text generation into dependent sentence generation sub-tasks, the model effectively models input data dynamically during generation [50].</p>
<p>Evaluation Metric</p>
<p>In data-to-text generation, assessing the quality and suitability of generated text has relied on various metrics over the years.These assessments can be broadly categorized into two groups: automatic evaluation and human evaluation.Automatic evaluation employs computational methods to measure text quality, while human evaluation enlists human participants to capture nuanced aspects of text quality and coherence.</p>
<p>Automatic Evaluation</p>
<p>N-gram Metrics</p>
<p>Our analysis indicates that the most commonly employed automatic metric is BLEU [126], with a substantial presence in 80 papers.It is closely followed by METEOR [127], which is employed in 40 papers, demonstrating its continued relevance.ROUGE [128], with 17 papers utilizing it, has also been a consistent choice for assessing generated text quality.Furthermore, CHrF++ [129], used in 10 papers, and NIST [130], applied in 9 papers, have offered valuable insights into the evaluation of data-to-text outputs.These metrics, while non-semantic in nature, have played a role in understanding word or character count and n-gram overlap between generated text and reference texts.</p>
<p>In seven studies, TER [131] was employed as an edit distance metric to evaluate machine translation, quantifying the human-level editing required to align system output with a reference [131].Additionally, fifteen studies utilized CIDEr [132], a widely adopted metric for image captioning, which assesses the similarity between generated and reference captions, considering both linguistic and content aspects, and applying TF-IDF-based n-gram weighting [133].Furthermore, two studies incorporated SPICE [134] metrics, which calculates the semantic propositional content overlap between generated and reference captions using scene graphs.Four (4) studies incorporated the use of the SER (Slot-Error Rate) metrics, which are appropriate for assessing the presence of named entities, with SER being computed through exact matching of slot values in the candidate texts [27].</p>
<p>Metric</p>
<p>Count Papers BLEU 80 [46], [64], [36], [65], [9], [98], [14], [99], [47], [2], [27], [23], [86], [96], [83], [49], [4], [66], [67], [84], [50], [68], [51], [52], [69], [100], [53], [39], [101], [102], [28], [11], [10], [103], [70], [71], [54], [72], [107], [87], [91], [73], [74], [88], [16], [26], [92], [29], [41], [55], [56], [57], [58], [81], [93], [75], [106], [109], [42], [110], [94], [43], [104], [37], [85], [61], [89], [90], [62], [76], [97], [78], [45], [105], [24], [63], [79], [60], [80], [111], [86] METEOR 40 [36], [9], [99], [27], [83], [49], [4], [66], [84], [51], [100], [102], [28], [11], [103], [71], [54], [72], [107], [91], [73], [74], [88], [56], [57], [106], [109], [110], [37], [85], [61], [90], [62], [76], [78], [105], [24], [60], [86], [81] ROUGE 17 [9], [98], [2], [28], [73], [74], [26], [92], [29], [57], [61], [90], [45], [105], [24], [60], [86] CIDER 15 [9], [98], [99], [83], [66], [28], [91], [74], [57], [58], [78], [105], [24], [60], [86] CHRF++ 10</p>
<p>[72], [107], [87], [88], [106], [109], [110], [37], [62], [81] NIST 9 [9], [99], [27], [26], [57], [58], [61], [78], [60] RG, CO, CS 11 [46], [96], [66], [67], [52], [69], [101], [16], [43] , [89], [80] PARENT 9 [66], [103], [73], [41], [104], [76], [44], [97], [ [36], [4], [71], [109], [110], [85], [24] BERTScore 7 [107], [109], [110], [37], [85], [62], [76] BLEURT 5 [107], [109], [110], [37], [85] MOVERScore 2 [37], [85] Table 6: Table of the Evaluation Metrics used across the Studies.</p>
<p>Task Specific Metrics</p>
<p>A significant debate persists among researchers regarding the appropriateness of these non-semantic n-gram metrics for evaluating data-to-text outputs.These metrics primarily rely on word count and n-gram overlap between the generated text and reference texts, often showing limited correlation with human judgment [66].They were originally developed and applied in other natural language generation (NLG) domains, such as translation for BLEU, NIST and METEOR and summarization for ROUGE.</p>
<p>In response to the unique challenges posed by table-to-text generation, task-specific metrics like PARENT [66] and its variant, PARENT-T [73], have emerged.These metrics assess the quality of table-to-text outputs by comparing the generated information with the entries in the source table.The research also analyzes the sensitivity of the metrics to divergence by collecting labels for cases where references only contain information already present in the tables.The study shows that PARENT maintains a high correlation as the number of such examples varies [66].These task-specific metrics are gaining prominence, with nine (9) papers considering PARENT and one (1) paper exploring PARENT-T.</p>
<p>To enhance the semantic alignment between generated texts and their references, embedding-based and pretrained-based metrics have been introduced.These metrics utilize contextualized embeddings and Transformer-based models to assess the quality and similarity of generated text to reference sentences.BERTScore [135], featured in 7 papers, calculates the cosine similarity between generated texts and the ground truth, offering a more nuanced understanding of text quality.MoverScore [136], a metric that allows many-to-one matching, is used in 2 papers and computes the Euclidean distances between words or n-grams.It enhances the evaluation process by considering partial alignments and offering insights into text quality.Notably, BLEURT [137], a Transformer-based trained metric, has been employed in 5 papers.This approach pretrains BERT with synthetically generated sentence pairs by mask-filling with BERT, back-translation, or randomly dropping words to assess NLG system performance [133].</p>
<p>Information Extraction Metrics</p>
<p>In data-to-text evaluation, extractive evaluation methods were introduced in Wiseman et al. [46] to assess the performance of the alignment of the information extraction model in the content selection and text planning in the generation process.A total of 11 studies adopted these metrics to rate their model's performance in content selection and planning tasks.This approach employs metrics like content selection (CS), content ordering (CO), and relational generation (RG).An Information Extraction (IE) system is used to extract content plans, identify candidate entities and value pairs present in the generated text, and predict their types.CS evaluates how accurately the system's extracted records match those in the reference output, considering precision and recall.RG assesses factuality by measuring the proportion of system-extracted records that also appear in the input table.CO evaluates the system's record ordering by computing the normalized Damerau-Levenshtein Distance between the sequence of extracted records and the reference output [46].</p>
<p>The Table 6 illustrate the distribution of these evaluation metrics identified in various research papers.</p>
<p>Human Evaluation</p>
<p>While automatic metrics offer certain advantages, human evaluation is often favored when assessing generated texts.This is due to its enhanced precision in evaluating aspects such as semantic adequacy, coherence, fluency, and the identification of numerical errors.Existing automatic metrics are often benchmarked against human evaluation results to determine their reliability and suitability.In human evaluations, the assessment of the quality of generated text varies widely, with different criteria used depending on the task.Due to the lack of a standardized human evaluation procedure in Natural Language Generation (NLG), and even in the naming conventions of the criteria, researchers often adopt diverse approaches to evaluate their generated texts [138].In this review, we aim to show some aspects of human evaluation by categorizing them into the measures and methods of evaluation taking a cue from studies by Belz et al. [139], and Van Der Lee et al. [138].</p>
<p>Quality Criteria Measures</p>
<p>In our analysis, certain studies lacked explicit details regarding their methods, tools, and design of quality criteria.However, for those that provided such information, we extracted relevant data.A notable observation is the considerable variation in the meanings associated with the names of the quality criteria.Table 7 enumerates the top ten prevalent naming conventions identified in the literature with "fluency" being the most used in 29 studies.Several terms, such as relevance, clarity, readability, and factual, among others, are notable examples that were not included in the table.It's crucial to note that the interpretation and task associated with these names may differ.</p>
<p>Experimental Methods</p>
<p>This section of the human evaluation review explores various methodologies for obtaining and assessing responses based on quality criteria.Table 7 presents the human evaluation frameworks and metrics extracted from the studies.Our analysis reveals that, out of 28 studies conducting human evaluation assessments in crowd-sourcing platforms, 17 studies utilized Amazon Mechanical Turk as their primary crowdsourcing platform.Additionally, we examined the linguistic background of annotators involved; 11 studies employed expert annotators, while eight and five studies engaged graduate students and paper authors, respectively.</p>
<p>Furthermore, we investigated the scale sizes used in each experiment.The most common scale size in 42 papers ranged from 1 to 5, followed by ranges of 10 to 30, and 50 and above in 5 and 4 papers, respectively.The 1-5 rank was the most popular scale range, appearing in 12 papers.Subsequently, the 0-100, 1-7, -100 to +100, and 0-5 scales were found in 6, 5, 3, and 3 papers, respectively.Two studies incorporated the TrueSkill Algorithm alongside the ranking task during evaluation, and various other scale sizes were identified in individual papers.</p>
<p>Additionally, we documented the agreement among annotators and the tools used in some studies.Statistical tools such as Krippendorff's , Fleiss' Kappa, Cohen's Kappa, and Weighted Kappa were employed in 3, 5, and 2 studies to measure agreements among annotators.In 12 papers, responses from participants were aggregated using averages.</p>
<p>Design</p>
<p>Category</p>
<p>Counts Citations</p>
<p>Quality Criterion</p>
<p>Fluency 29 [62], [63], [82], [26], [37], [76], [104], [58], [11], [88], [85], [111], [23], [107], [95], [29], [64], [41], [74], [42], [2], [91], [83], [71], [108], [110], [48], [90], [93] Grammaticality 11 [62], [22], [96], [76], [52], [43], [80], [50], [89], [67], [94] Correctness 11 [22], [92], [76], [102], [23], [107], [29], [108],</p>
<p>[100], [93], [94] Adequacy 10</p>
<p>[27], [63], [26], [37], [11], [88], [77], [102], [95],</p>
<p>[90] Coherence 9 [28], [96], [76], [52], [43], [80], [11], [50], [89],</p>
<p>[67] Coverage 7 [86], [104], [102], [107], [64], [41], [71], [108],</p>
<p>[110] Faithfulness 7 [60], [82], [85], [41], [55], [83], [71] Naturalness 7 [28], [60], [46], [92], [103], [58], [98], [99] Conciseness 7 [96], [76], [52], [43], [80], [89], [67] Similarity 5 [37], [72], [59], [51], [38] Crowd Sourcing Platform AMT 17 [46], [96], [83], [71], [107], [51], [88], [29], [59], [89], [76], [86], [24], [60], [54], [82] [43], [98], [68], [47], [48], [61], [55], [95], [110] Annotator Experience Expert 11 [72], [91], [29], [41], [111], [28], [56], [11], [95], [105], [104] Graduate student 8 [52], [26], [92], [29], [100], [76], [58], [80] Author 5 [49], [91], [105], [94], [63] Scale Sizes 1 -5 42 [27], [39], [62], [4], [28], [53], [105], [63], [22], [86], [60], [82], [26], [46], [37], [96], [92], [45], [72], [59], [76], [103], [68], [47], [73], [52], [56], [97], [78],, [24], [43], [80], [104], [54], [58], [11], [88], [50], [51], [77], [29] [44], [50], [74], [60], [88], [26], [37], [51], [107], [72], [61], [48] Table 7: Human Evaluation Criteria and Frameworks.Amazon Mechanical Turk (AMT) [28], [90], [90], [85] Legal Domain 1</p>
<p>Application Areas</p>
<p>[44] Dialogue Systems 25 [91], [77], [58], [105], [103], [9], [54], [85], [38], [86], [42], [60], [98], [99], [68], [48], [74], [27], [61], [100], [102], [28], [78], [24], [43] Financial Reporting 1</p>
<p>[93] Biography Generation 15 [9], [10], [66], [54], [85], [26], [29], [41], [53], [111], [11], [38], [104], [82] [47], [46], [96], [10], [67], [70], [85], [52], [16], [89], [101], [76], [43], [80], [42], [23], [75], [69] Table 8: Application Areas of Data-to-text Generation.</p>
<p>Data-to-text generation finds extensive application in diverse domains, reflecting its versatility and value in addressing specific needs, as shown in Table 8.It plays a pivotal role in dialogue systems, where 26 studies focus on generating dialogues across various conversational contexts.Another significant application domain is sports narration, with 18 studies employing data-to-text generation to create textual summaries of game match records, encompassing player details, team information, and scores.In the realm of biography generation, 15 studies work on generating biographical texts for individuals with data often sourced from platforms like English Wikipedia.The application extends to translation and multilingualism, with 12 studies leveraging data-to-text techniques to tackle multilingual challenges.Additionally, data-to-text methods have proven effective in question generation and answering, exemplified by four identified studies in this domain.The application footprint extends to weather forecasting, robo-journalism, relational databases, the legal domain, and financial reporting, each with several studies showcasing the practical utility of data-to-text generation in distinct contexts.Furthermore, advertising and recipe generation domains have harnessed data-to-text techniques effectively.This comprehensive coverage highlights the adaptability and broad applicability of data-to-text generation in diverse scenarios and underscores its role in addressing specific needs across multiple domains.</p>
<p>tend to favor WebNLG and E2E over other datasets due to the competition challenges and dedicated conferences associated with them.Moreover, these datasets, being human-curated, offer more natural and fluent content compared to online-extracted datasets, contributing to their widespread acceptance.</p>
<p>In terms of evaluation metrics, BLEU emerges as the most commonly used automatic metric, enjoying broad acceptance across various data-to-text tasks.This observation is supported by the considerable number of studies that incorporate this metric in Table 6.</p>
<p>In addition, the dominance of large language models is evident in their effectiveness for data-to-text generation, as reflected by the widespread adoption of transformer models, as illustrated in Table 5.</p>
<p>Lastly, there is no universally preferred technique for addressing the hallucination problem in data-to-text.However, based on our observations, most mitigation measures are task-specific, with data refinement being a more general and effective method.This involves processes such as deduplication, cleaning, and factuality grounding, exerting control over the content the generation model encounters during training.</p>
<p>Recommendation and Future Directions</p>
<p>We have conducted a thorough analysis of the prevailing trends and have provided answers to the research questions within the scope of this literature.Nonetheless, it has come to our attention that there is a need to extend research efforts to encompass more low-resourced languages as seen in Section 3.2.Additionally, in Section 3.3 owing to the temporal limitations defined in the exclusion criteria, we did not include papers pertaining to recent large language models such as GPT-3.5 and GPT-4 [140; 141] and LLAMA [142; 143].In future investigations, our emphasis will be on studies that incorporate these advanced technologies into their research.</p>
<p>Moreover, from observations in Section 3.5, we recommend that future studies place greater emphasis on the utilization of contextual evaluation metrics for assessing the performance of data-to-text generation.These metrics have shown notable advantages in terms of semantic accuracy in data-to-text pairs, and their inclusion in evaluation frameworks is a direction worth exploring.</p>
<p>A standardized approach to human evaluation in the data-to-text field is essential.We strongly recommend authors to provide detailed explanations of their human evaluation procedures, including quality criteria definitions, response elicitation platforms, participants' knowledge backgrounds, etc.We also encourage the broader NLG community to collaboratively establish a universal naming convention to disambiguate similar terms and associated tasks.</p>
<p>Furthermore, referring to Section 3.4 and considering the richness of general knowledge that recent LLMs possess, we propose an advancement in their hallucination mitigation methods compared to task-specific LLMs.This improvement could focus more on addressing numerical and logical inference hallucination in the generated text.</p>
<p>Conclusion</p>
<p>This systematic review of data-to-text generation provides a comprehensive overview of the field, including its trends, challenges, and advancements.The review consolidates knowledge on datasets, language considerations, models, hallucination mitigation, and applications to guide future research endeavors.The insights gained from this review contribute to a deeper understanding of data-to-text generation, paving the way for continued innovation and progress.Addressing the identified trends and challenges will be crucial in advancing the capabilities and applicability of data-to-text generation systems as the field continues to evolve.</p>
<p>(a) Number of papers per year.(b) Publication Area.</p>
<p>7 .
7
Incorporation of both human and automatic evaluation of results.8. Availability of research code for reproducibility.Exclusion Criteria 1. Published before 2017.2.Not featured in journals categorized as A1-B2 and Q1-Q2.3. Written in languages other than English.4. Sole focus on other forms of text generation. 5. Less than five citations.6. Solely automatic evaluation of results.7. Lack of research code availability.</p>
<p>Fig. 2 :
2
Fig. 2: Multilinguality in Data-to-text Generation.</p>
<p>Table 14
1481]</p>
<p>Table 4 :
4
Languages and Multilingualism</p>
<p>have pursued analogous approaches, generating text concurrently in multiple European languages, underscoring the heightened interest and capabilities in multilingual data-to-text generation.Conversely, some studies, such as those by Song et al. [4], Shao et al. [50], Castro Ferreira et al. [107], Garneau and Lamontagne [44], Agarwal et al. [108], Lu et al. [86], Li et al.</p>
<p>[73]loped a novel neural model for data-to-text generation with style imitation to follow a certain style of writing from examples.The model employs a hybrid attention-copy mechanism and weak supervisions, using a content coverage constraint for balanced content fidelity and style control, proving effective in controlled text generation tasks[42].Wangetal.[73]presents a Transformer-based framework for table-to-text generation with a focus on producing faithful and informative text descriptions aligned with input tables.It introduces two essential strategies, a Table-Text Disagreement Constraint Loss and Constrained Content Matching via Optimal Transport, along with a novel evaluation metric, PARENT-T, to measure faithfulness in generated text.These constraints ensure that the latent representation of the table aligns with the corresponding representation of the generated text [73]. Shen et al.</p>
<p>https://scholar.google.com/
https://aclanthology.org/
https://www.ieee.org/
https://www.semanticscholar.org/
https://catalog.ldc.upenn.edu/byyear
Based on the data gathered from various studies, we will explore researchers' preferences. The datasets outlined in Table2reveal that, in data-to-text NLG, researchers
Acknowledgments.This work was conducted with the financial support of the Science Foundation Ireland Centre for Research Training in Artificial Intelligence under Grant No. 18/CRT/6223.This publication has emanated from research conducted with the financial support of Science Foundation Ireland under Grant number 18/CRT/6223.For the purpose of Open Access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.
Building applied natural language generation systems. E Reiter, R Dale, 10.1017/S13513249970015021997</p>
<p>Guided neural language generation for abstractive summarization using abstract meaning representation. Vlachos Hardy, A , 10.18653/v1/d18-1086arXiv:1808.09160Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>Automatic and Human-AI Interactive Text Generation. Y Dou, P Laban, C Gardent, W Xu, 2023</p>
<p>Semantic Neural Machine Translation Using AMR. L Song, D Gildea, Y Zhang, Z Wang, J Su, 10.1162/tacl_a_00252Transactions of the Association for Computational Linguistics. 72019</p>
<p>R Mokady, A Hertz, A H Bermano, ClipCap: CLIP Prefix for Image Captioning. 2021</p>
<p>Maximizing stylistic control and semantic accuracy in nlg: Personality variation and discourse contrast. V Harrison, L Reed, S Oraby, M Walker, arXiv:1907.095272019arXiv preprint</p>
<p>Neural natural language generation: A survey on multilinguality, multimodality, controllability and learning. E Erdem, B Plank, A Gatt, E Krahmer, M Sharma, A Gogineni, N Ramakrishnan, W Li, W Wu, M Chen, J Liu, X Xiao, H Wu, 10.1613/jair.5714arXiv:2207.12571Journal of Artificial Intelligence Research. 732022</p>
<p>Transformer based natural language generation for question-answering. I Akermi, J Heinecke, F Herledan, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language Generation2020</p>
<p>Learning Neural Templates for Text Generation. S Wiseman, S M Shieber, A M Rush, 2018Technical report</p>
<p>ToTTo: A controlled table-to-text generation dataset. A P Parikh, X Wang, S Gehrmann, M Faruqui, B Dhingra, D Yang, D Das, 10.18653/v1/2020.emnlp-main.89arXiv:2004.14373EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>Scalable Micro-planned Generation of Discourse from Structured Data. A Laha, P Jain, A Mishra, K Sankaranarayanan, 10.1162/coli_a_00363Computational Linguistics. 4542019</p>
<p>A L R Teixeira, J G M Campos, R Cunha, T C Ferreira, A S Pagano, F G Cozman, Damata: A robot-journalist covering the brazilian amazon deforestation. INLG 2020 -13th International Conference on Natural Language Generation, Proceedings. 2020</p>
<p>Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. J Obeid, E Hoque, arXiv:2010.091422020arXiv preprint</p>
<p>SQL-to-Text Generation with Graph-to-Sequence Model. K Xu, 2018</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. A Gatt, E Krahmer, 10.1613/jair.5714arXiv:1703.09902Journal of Artificial Intelligence Research. 612018</p>
<p>PAN: Pipeline assisted neural networks model for data-to-text generation in social internet of things. N Jiang, J Chen, R G Zhou, C Wu, H Chen, J Zheng, T Wan, 10.1016/j.ins.2020.03.080Information Sciences. 5302020</p>
<p>Dealing with hallucination and omission in neural Natural Language Generation: A use case on meteorology. J Gonzlez Corbelle, A Bugarn-Diz, J Alonso-Moral, J Taboada, Proceedings of the 15th International Conference on Natural Language Generation. the 15th International Conference on Natural Language Generation2022</p>
<p>J Zhang, J.-G Yao, X Wan, Towards constructing sports news from live text commentary. 2016</p>
<p>Interacting with financial data using natural language. V Plachouras, C Smiley, H Bretz, O Taylor, J L Leidner, D Song, F Schilder, 10.1145/2911451.2911457SIGIR 2016 -Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2016</p>
<p>. E Monfroglio, L Anselma, A Mazzei, 2022Personalizing Weekly Diet Reports</p>
<p>Neural Text Generation from Structured Data with Application to the Biography Domain. R Lebret, D Grangier, M Auli, 10.18653/V1/D16-1128arXiv:1603.07771EMNLP 2016 -Conference on Empirical Methods in Natural Language Processing, Proceedings. 2016</p>
<p>Getting to Production with Few-shot Natural Language Generation Models. P Heidari, A Einolghozati, S Jain, S Batra, L Callender, A Arun, S Mei, S Gupta, P Donmez, V Bhardwaj, A Kumar, M White, SIGDIAL 2021 -22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, Proceedings of the Conference. 20212</p>
<p>Automated learning of templates for datato-text generation: comparing rule-based, statistical and neural methods. C V Lee, E Krahmer, S Wubben, 10.18653/v1/w18-6504INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>AGGGEN: Ordering and aggregating while generating. X Xu, O Duek, V Rieser, I Konstas, 10.18653/v1/2021.acl-long.113arXiv:2106.05580ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 2021Proceedings of the Conference</p>
<p>Learning Semantic Correspondences with Less Supervision. P Liang, M I Jordan, D Klein, 2009</p>
<p>Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization. P Nema, S Shetty, P Jain, A Laha, K Sankaranarayanan, M M Khapra, 10.18653/V1/N18-1139arXiv:1804.07789NAACL HLT 2018 -2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Handling rare items in data-to-text generation. A Shimorina, C Gardent, 10.18653/v1/w18-6543INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation. J Juraska, K K Bowden, M Walker, 10.18653/V1/W19-8623arXiv:1910.12129INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Logic2Text: High-Fidelity Natural Language Generation from Logical Forms. Z Chen, W Chen, H Zha, X Zhou, Y Zhang, S Sundaresan, W Y Wang, 10.18653/V1/2020.FINDINGS-EMNLP.190arXiv:2004.14579Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020. 190</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y Bang, A Madotto, P Fung, 10.1145/3571730ACM Computing Surveys. 112022</p>
<p>A Systematic Literature Review on Text Generation Using Deep Neural Network Models. N Fatima, A S Imran, Z Kastrati, S M Daudpota, A Soomro, 10.1109/ACCESS.2022.3174108IEEE Access. 102022</p>
<p>Faithfulness in Natural Language Generation: A Systematic Survey of Analysis. W Li, W Wu, M Chen, J Liu, X Xiao, H Wu, arXiv:2203.05227Evaluation and Optimization Methods. 2022</p>
<p>A Survey of Evaluation Metrics Used for NLG Systems. A B Sai, A K Mohankumar, M M Khapra, 10.1145/3485766arXiv:2008.12009ACM Computing Surveys. 5522023</p>
<p>S Lu, Y Zhu, W Zhang, J Wang, Y Yu, arXiv:1803.07133Neural Text Generation: Past, Present and Beyond. 2018</p>
<p>Innovations in Neural Data-totext Generation. M Sharma, A Gogineni, N Ramakrishnan, arXiv:2207.125712022</p>
<p>The WebNLG challenge: Generating text from RDF data. INLG 2017 -10th International Natural Language Generation Conference. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, 10.18653/v1/w17-3518Proceedings of the Conference. the Conference2017298</p>
<p>Investigating Pretrained Language Models for Graph-to-Text Generation. L F R Ribeiro, M Schmitt, H Schtze, I Gurevych, 10.18653/v1/2021.nlp4convai-1.20arXiv:2007.084262021</p>
<p>Lightweight, dynamic graph convolutional networks for AMR-to-text generation. Y Zhang, Z Guo, Z Teng, W Lu, S B Cohen, Z Liu, L Bing, 10.18653/v1/2020.emnlp-main.169arXiv:2010.04383EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>V Hajdik, J Buys, M W Goodman, E M Bender, 10.18653/v1/n19-1235arXiv:1904.11564Neural text generation from rich semantic representations. NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, R Chou, J Glanville, J M Grimshaw, A Hrbjartsson, M M Lalu, T Li, E W Loder, E Mayo-Wilson, S Mcdonald, L A Mcguinness, L A Stewart, J Thomas, A C Tricco, V A Welch, P Whiting, D Moher, 10.1136/bmj.n71The BMJ. 3722021</p>
<p>Controlled hallucinations: learning to generate faithfully from noisy data. K Fillippova, 10.18653/v1/2020.findings-emnlp.76arXiv:2010.05873Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020864</p>
<p>Data-to-text generation with style imitation. S Lin, W Wang, Z Yang, X Liang, F F Xu, E P Xing, Z Hu, 10.18653/v1/2020.findings-emnlp.144arXiv:1901.09501Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020 (Figure 1). 2020</p>
<p>Enhancing content planning for table-to-text generation with data understanding and verification. H Gong, W Bi, X Feng, B Qin, X Liu, T Liu, 10.18653/v1/2020.findings-emnlp.262Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020. 2020</p>
<p>Trainable Ranking Models to Evaluate the Semantic Accuracy of Data-to-Text Neural Generator. Eval4NLP 2021 -Evaluation and Comparison of NLP Systems. N Garneau, L Lamontagne, 10.26615/978-954-452-056-4_006Proceedings of the 2nd Workshop. the 2nd Workshop2021</p>
<p>Few-Shot Table-to-Text Generation with Prototype Memory. Y Su, Z Meng, S Baker, N Collier, 10.18653/v1/2021.findings-emnlp.77arXiv:2108.12516Findings of ACL: EMNLP 2021. Findings of the Association for Computational Linguistics2021</p>
<p>Challenges in Data-to-Document Generation. S Wiseman, S M Shieber, A M Rush, 2017</p>
<p>Operation-guided neural networks for high fidelity data-to-text generation. F Nie, J Wang, J G Yao, R Pan, C Y Lin, 10.18653/v1/d18-1422arXiv:1809.02735Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018. 2018</p>
<p>Unsupervised natural language generation with denoising autoencoders. M Freitag, S Roy, 10.18653/v1/d18-1426arXiv:1804.07899Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2014. 20182018</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. T C Ferreira, C Lee, E Miltenburg, E Krahmer, 10.18653/v1/d19-1052arXiv:1908.09022EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019552</p>
<p>Long and diverse text generation with planning-based hierarchical variational model. Z Shao, M Huang, J Wen, W Xu, X Zhu, 10.18653/v1/d19-1321arXiv:1908.06605EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Enhancing AMR-to-text generation with dual graph representations. L F R Ribeiro, C Gardent, I Gurevych, 10.18653/v1/d19-1314arXiv:1909.00352EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time. H Gong, X Feng, B Qin, T Liu, 10.18653/v1/d19-1310arXiv:1909.02304EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>Enhancing neural data-to-text generation models with external background knowledge. S Chen, J Wang, X Feng, F Jiang, B Qin, C Y Lin, 10.18653/v1/d19-1299EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2019</p>
<p>KGPT : Knowledge-Grounded Pre-Training for Data-to-Text Generation. W Chen, Y Su, X Yan, W Y Wang, 2020</p>
<p>A Fan, F Loria, D Lorraine, C Gardent, C Loria, arXiv:2011.05443Multilingual AMR-to-Text Generation. 2020arXiv preprint</p>
<p>Online Back-Parsing for AMR-to-Text Generation. X Bai, L Song, Y Zhang, 2020</p>
<p>Partially-aligned data-to-text generation with distant supervision. Z Fu, B Shi, W Lam, L Bing, Z Liu, 10.18653/v1/2020.emnlp-main.738arXiv:2010.01268EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference. 2020</p>
<p>Controllable meaning representation to text generation: Linearization and data augmentation strategies. C Kedzie, K Mckeown, 10.18653/v1/2020.emnlp-main.419EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing. 2020Proceedings of the Conference</p>
<p>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. L F R Ribeiro, Y Zhang, I Gurevych, 10.18653/v1/2021.emnlp-main.351arXiv:2103.09120EMNLP 2021 -2021 Conference on Empirical Methods in Natural Language Processing, Proceedings. 2021</p>
<p>Data-to-text Generation by Splicing Together Nearest Neighbors. S Wiseman, A Backurs, K Stratos, 10.18653/v1/2021.emnlp-main.352arXiv:2101.08248EMNLP 2021 -2021 Conference on Empirical Methods in Natural Language Processing, Proceedings. 2021</p>
<p>Neural data-to-text generation with LM-based text augmentation. E Chang, X Shen, D Zhu, V Demberg, H Su, 10.18653/v1/2021.eacl-main.64arXiv:2102.03556EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from AMR. J Opitz, A Frank, 10.18653/v1/2021.eacl-main.129arXiv:2008.08896EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference (i)</p>
<p>Incremental beam manipulation for natural language generation. J Hargreaves, A Vlachos, G Emerson, 10.18653/v1/2021.eacl-main.219arXiv:2102.02574EACL 2021 -16th Conference of the European Chapter. the Association for Computational Linguistics2021Proceedings of the Conference</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. I Konstas, S Iyer, M Yatskar, Y Choi, L Zettlemoyer, 10.18653/v1/P17-1014arXiv:1704.08381ACL 2017 -55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers). 20171</p>
<p>A Graph-to-Sequence Model for AMR-to-Text Generation. L Song, Y Zhang, Z Wang, D Gildea, C Science, 2018</p>
<p>Handling divergent reference texts when evaluating table-to-text generation. B Dhingra, M Faruqui, A Parikh, M W Chang, D Das, W W Cohen, 10.18653/v1/p19-1483ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Data-to-text generation with entity modeling. R Puduppully, L Dong, M Lapata, 10.18653/v1/p19-1195arXiv:1906.03221ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2023-2035 (2019</p>
<p>F Nie, J G Yao, J Wang, R Pan, C Y Lin, 10.18653/v1/p19-1256A simple recipe towards reducing hallucination in neural surface realisation. ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Learning to select, track, and generate for datato-text. H Iso, Y Uehara, T Ishigaki, H Noji, E Aramaki, I Kobayashi, Y Miyao, N Okazaki, H Takamura, 10.5715/jnlp.27.599arXiv:1907.09699ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Logical natural language generation from open-domain tables. W Chen, J Chen, Y Su, Z Chen, W Y Wang, 10.18653/v1/2020.acl-main.708arXiv:2004.10404Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. C Zhao, M Walker, S Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>M M Ram, Y.-S L Radu, F Salim, GPT-too : A Language-Model-First Approach for AMR-to-Text Generation. 2020</p>
<p>Towards faithful neural table-totext generation with content-matching constraints. Z Wang, X Wang, B An, D Yu, C Chen, 10.18653/v1/2020.acl-main.101arXiv:2005.00969Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Neural data-to-text generation via jointly learning the segmentation and correspondence. X Shen, E Chang, H Su, C Niu, D Klakow, 10.18653/v1/2020.acl-main.641arXiv:2005.01096Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2019. 2020</p>
<p>Fact-based text editing. H Iso, C Qiao, H Li, 10.18653/v1/2020.acl-main.17arXiv:2007.00916Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Towards table-to-text generation with numerical reasoning. L H Suadaa, H Kamigaito, K Funakoshi, M Okumura, H Takamura, 10.18653/v1/2021.acl-long.115ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>On Training Instance Selection for Few-Shot Neural Text Generation. E Chang, X Shen, H S Yeh, V Demberg, 10.18653/v1/2021.acl-short.2arXiv:2107.03176ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 20212</p>
<p>Mention flags (MF): Constraining transformer-based text generators. Y Wang, I D Wood, S Wan, M Dras, M Johnson, 10.18653/v1/2021.acl-long.9ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>XLPT-AMR: Cross-lingual pretraining via multi-task learning for zero-shot AMR parsing and text generation. D Xu, J Li, M Zhu, M Zhang, G Zhou, 10.18653/v1/2021.acl-long.73ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. 2021Proceedings of the Conference</p>
<p>Improving encoder by auxiliary supervision tasks for table-to-text generation. L Li, C Ma, Y Yue, D Hu, 10.18653/v1/2021.acl-long.466ACL-IJCNLP 2021 -59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 2021</p>
<p>Graph Pre-training for AMR Parsing and Generation. X Bai, Y Chen, Y Zhang, 10.18653/v1/2022.acl-long.415arXiv:2203.07836Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2022</p>
<p>Bootstrapping Generators from Noisy Data. L Perez-Beltrachini, M Lapata, 10.18653/V1/N18-1137arXiv:1804.06385NAACL HLT 2018 -2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. A Moryossef, Y Goldberg, I Dagan, arXiv:1904.03396NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>Structural neural encoders for AMR-to-text generation. M Damonte, S B Cohen, arXiv:1903.11410NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20191</p>
<p>DART: Open-Domain Structured Data Record to Text Generation. L Nan, D Radev, R Zhang, A Rau, A Sivaprasad, C Hsieh, X Tang, A Vyas, N Verma, P Krishna, Y Liu, N Irwanto, J Pan, F Rahman, A Zaidi, M Mutuma, Y Tarabar, A Gupta, T Yu, Y C Tan, X V Lin, C Xiong, R Socher, N F Rajani, 10.18653/v1/2021.naacl-main.37arXiv:2007.02871NAACL-HLT 2021 -2021 Conference of the North American Chapter. Human Language Technologies2021Proceedings of the Conference</p>
<p>X Lu, S Welleck, P West, NEUROLOGIC A * esque Decoding: Constrained Text Generation with Lookahead Heuristics. 2022</p>
<p>Amr-to-text generation with graph transformer. T Wang, X Wan, H Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 82020</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. L F R Ribeiro, Y Zhang, C Gardent, I Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Data-to-text Generation with Macro Planning. R Puduppully, M Lapata, 10.1162/tacl_a_00381Transactions of the Association for Computational Linguistics. 92021</p>
<p>JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, X Zhu, M Huang, 10.18653/v1/2021.findings-acl.223arXiv:2106.10502Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity. H Harkous, I Groves, A Saffari, 10.18653/v1/2020.coling-main.218arXiv:2004.06577COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>TableGPT: Fewshot Table-to-Text Generation with Table Structure Reconstruction and Content Matching. H Gong, Y Sun, X Feng, B Qin, W Bi, X Liu, T Liu, 10.18653/v1/2020.coling-main.179COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 1978-1988 (2020</p>
<p>Learning with Contrastive Examples for Data-to-Text Generation. Y Uehara, T Ishigaki, K Aoki, K Goshima, H Noji, I Kobayashi, H Takamura, Y Miyao, 10.18653/v1/2020.coling-main.213COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Conference. 2020</p>
<p>Best Practices for Data-Efficient Modeling in NLG: How to Train Production-Ready Neural Models with Less Data. A Arun, S Batra, V Bhardwaj, A Challa, P Donmez, P Heidari, H Inan, S Jain, A Kumar, S Mei, K Mohan, M White, 10.18653/v1/2020.coling-industry.7arXiv:2011.03877COLING 2020 -28th International Conference on Computational Linguistics, Proceedings of the Industry Track. 2020</p>
<p>D Moussallem, T C Ferreira, M Zampieri, M C Cavalcanti, G Xexo, M Neves, A C N Ngomo, arXiv:1802.08150Rdf2Pt: Generating brazilian Portuguese texts from RDF data. LREC 2018 -11th International Conference on Language Resources and Evaluation. 2019</p>
<p>Data-to-text generation with content selection and planning. R Puduppully, L Dong, M Lapata, 10.1609/aaai.v33i01.33016908arXiv:1809.0058233rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence. 20192019</p>
<p>Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View. T Liu, X Zheng, B Chang, Z Sui, 10.1609/aaai.v35i15.17583arXiv:2102.0858535th AAAI Conference on Artificial Intelligence, AAAI 2021. 202115</p>
<p>End-to-End Content and Plan Selection for Data-to-Text Generation. S Gehrmann, F Z Dai, H Elder, A M Rush, arXiv:1810.04700v1E2E NLG Challenge System Descriptions. 2018</p>
<p>Y Puzikov, I Gurevych, 10.18653/v1/w18-6557E2E NLG challenge: Neural models vs. templates. INLG 2018 -11th International Natural Language Generation Conference, Proceedings of the Conference. 2018</p>
<p>A good sample is hard to find: Noise injection sampling and self-training for neural language generation models. C Kedzie, K Mckeown, 10.18653/v1/w19-8672arXiv:1911.03373INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Revisiting challenges in data-to-text generation with fact grounding. H Wang, 10.18653/v1/w19-8639arXiv:2001.03830INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019</p>
<p>Semi-supervised neural text generation by joint learning of natural language generation and natural language understanding models. R Qader, F Portet, C Labb, 10.18653/v1/w19-8669arXiv:1910.03484INLG 2019 -12th International Conference on Natural Language Generation, Proceedings of the Conference. 2019552</p>
<p>T5Pretrain-Text-to-Text Pre-Training for Data-to-Text Tasks.pdf. M Kale, 2020</p>
<p>PARENTing via Model-Agnostic Reinforcement Learning to Correct Pathological Behaviors in Data-to-Text Generation. C Rebuffel, L Soulier, G Scoutheeten, P Gallinari, arXiv:2010.10866INLG 2020 -13th International Conference on Natural Language Generation, Proceedings. 2020</p>
<p>Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG. INLG 2021 -14th International Conference on Natural Language Generation. J Juraska, M Walker, arXiv:2109.07043Proceedings. September. 2021</p>
<p>NABU -Multilingual Graph-Based Neural RDF Verbalizer. D Moussallem, D Gnaneshwar, T Castro Ferreira, A C Ngonga Ngomo, 10.1007/978-3-030-62419-4_24LNCS. 12506. 2020</p>
<p>The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+. T Castro Ferreira, C Gardent, N Ilinykh, C Lee, S Mille, D Moussallem, A Shimorina, O Agarwal, M Kale, H Ge, S Shakeri, R Al-Rfou, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web. the 3rd International Workshop on Natural Language Generation from the Semantic WebDublin, Ireland (VirtualAssociation for Computational Linguistics2020. 2020. December. 2020Proceedings of the. 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+</p>
<p>Leveraging Large Pretrained Models for WebNLG. X Li, A Maskharashvili, Jory Stevens-Guille, S White, M , Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (December). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (December)2020. 2020</p>
<p>P 2 : A Planand-Pretrain Approach for Knowledge Graph-to-Text Generation. Q Guo, Z Jin, N Dai, X Qiu, X Xue, D Wipf, Z Zhang, December. 2020</p>
<p>C Rebuffel, M Roberti, L Soulier, G Scoutheeten, R Cancelliere, P Gallinari, 10.1007/s10618-021-00801-4s10618-021-00801-4arXiv:2102.02810Controlling hallucinations at word level in data-to-text generation. 202236</p>
<p>J Novikova, O Duek, V Rieser, arXiv:1706.09254The e2e dataset: New challenges for end-toend generation. 2017arXiv preprint</p>
<p>Abstract meaning representation for sembanking. L Banarescu, C Bonial, S Cai, M Georgescu, K Griffitt, U Hermjakob, K Knight, P Koehn, M Palmer, N Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with Discourse2013</p>
<p>Penman: An open-source library and tool for amr graphs. M W Goodman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>
<p>Parallel data, tools and interfaces in opus. J Tiedemann, Lrec2012Citeseer2012</p>
<p>Findings of the 2017 conference on machine translation (wmt17). O Bojar, R Chatterjee, C Federmann, Y Graham, B Haddow, S Huang, M Huck, P Koehn, Q Liu, V Logacheva, C Monz, M Negri, M Post, R Rubino, L Specia, M Turchi, 10.18653/V1/W17-4717WMT 2017 -2nd Conference on Machine Translation, Proceedings. 20172</p>
<p>L Barrault, M Biesialska, O Bojar, M R Costa-Juss, C Federmann, Y Graham, R Grundkiewicz, B Haddow, M Huck, E Joanis, T Kocmi, P Koehn, C K Lo, N Ljubei, C Monz, M Morishita, M Nagata, T Nakazawa, S Pal, M Post, M Zampieri, 10.18653/V1/W19-5301Findings of the 2019 conference on machine translation (wmt19). 5th Conference on Machine Translation, WMT 2020 -Proceedings. 20192</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.04732014arXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>K Cho, B Van Merrinboer, D Bahdanau, Y Bengio, arXiv:1409.1259On the properties of neural machine translation: Encoder-decoder approaches. 2014arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-totext transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>G Lample, A Conneau, arXiv:1901.07291Cross-lingual language model pretraining. 2019arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational Linguistics2002</p>
<p>Meteor universal: Language specific translation evaluation for any target language. M Denkowski, A Lavie, Proceedings of the Ninth Workshop on Statistical Machine Translation. the Ninth Workshop on Statistical Machine Translation2014</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. 2004</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. M Popovi, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine Translation2015</p>
<p>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. G Doddington, Proceedings of the Second International Conference on Human Language Technology Research. the Second International Conference on Human Language Technology Research2002</p>
<p>A study of translation edit rate with targeted human annotation. M Snover, B Dorr, R Schwartz, L Micciulla, J Makhoul, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers. the 7th Conference of the Association for Machine Translation in the Americas: Technical PapersCambridge, Massachusetts, USA2006Association for Machine Translation in the Americas</p>
<p>Cider: Consensus-based image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2015</p>
<p>A survey of evaluation metrics used for nlg systems. A B Sai, A K Mohankumar, M M Khapra, ACM Computing Surveys (CSUR). 5522022</p>
<p>Spice: Semantic propositional image caption evaluation. P Anderson, B Fernando, M Johnson, S Gould, Computer Vision-ECCV 2016: 14th European Conference. Amsterdam, The NetherlandsSpringerOctober 11-14, 2016. 2016Proceedings, Part V 14</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. W Zhao, M Peyrard, F Liu, Y Gao, C M Meyer, S Eger, arXiv:1909.026222019arXiv preprint</p>
<p>T Sellam, D Das, A P Parikh, arXiv:2004.04696Bleurt: Learning robust metrics for text generation. 2020arXiv preprint</p>
<p>Best practices for the human evaluation of automatically generated text. C Van Der Lee, A Gatt, E Van Miltenburg, S Wubben, E Krahmer, Proceedings of the 12th International Conference on Natural Language Generation. the 12th International Conference on Natural Language Generation2019</p>
<p>Disentangling the properties of human evaluation methods: A classification system to support comparability, metaevaluation and reproducibility testing. A Belz, S Mille, D M Howcroft, 2020ACL</p>
<p>J Ye, X Chen, N Xu, C Zu, Z Shao, S Liu, Y Cui, Z Zhou, C Gong, Y Shen, arXiv:2303.10420A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. 2023arXiv preprint</p>
<p>O J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, Diogo, Gpt-4 technical report. 2023257532815</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3008 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3008</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3008</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-270738112</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.17873v1.pdf" target="_blank">Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) non-natural language, usually programming code, which is difficult for people who are unfamiliar with coding to read. In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models. Specifically, we use relation tuples, which are not only human-readable but also machine-friendly and easier to verify than natural language. We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models. The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models. The source code is available at https://github.com/gpgg/art.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3008.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3008.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary OpenAI conversational LLM used as a backbone in experiments; evaluated on multiple arithmetic word-problem datasets and used to generate relation-tuples, Python verifier code, and to respond to dynamic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary transformer-based conversational LLM (OpenAI ChatGPT family), used via API in paper; exact architecture and training data not specified in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step arithmetic word problems (GSM8K: 2–8 ops), SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith (math word problems, chained operations).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Uses semi-structured intermediate representation (relation tuples) to record key arithmetic facts/operations; generates Python verification code from relation tuples and executes it; dynamic feedback loop when answers from natural-language reasoning and code verification disagree. Relation tuples act like 'pause' tokens prompting internal deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: incorporating relation tuples into few-shot examples improved accuracy on 4/7 datasets; ART (relation tuples + verification + dynamic feedback) improves GSM8K accuracy to ~84.5 (paper reports +1.9% over ModelSelection's 82.6). Verification step sometimes lowers accuracy compared to relation-tuples answers alone, but overall pipeline yields higher final accuracy. Percentage-of-feedback statistics show ChatGPT requires less feedback than weaker code-capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Verification-by-code answers were sometimes less accurate than the relation-tuple answers (Step 2 < Step 1) for some models; indicates that code-generation-from-RT distribution is not always handled perfectly. No internal probing is performed to show algorithmic representations beyond behavioral gains.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting with relation tuples (semi-structured RT), generating/executing Python verification code (local code interpreter), and a dynamic feedback loop; optionally integrated with Self-Consistency (sampling multiple reasoning paths).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved overall arithmetic accuracy compared to baselines (CoT, PAL, ModelSelection); reduced number of feedback iterations needed relative to weaker models; allowed automatic verification of numeric computations via execution; enables selection of most frequent answer across steps (compatible with Self-Consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported GSM8K accuracy for ART (ChatGPT backbone) ~84.5 (paper: ART improves GSM8K by 1.9% over ModelSelection's 82.6). Paper reports improvements on SVAMP and AddSub as well (SVAMP improved by 2.8% vs ModelSelection). Percent of GSM8K questions needing no feedback for ChatGPT: 81.7% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>When verification/code-generation disagrees with natural-language reasoning, ChatGPT sometimes needed dynamic feedback; code-generation errors (less frequent than for weaker models) can still occur; inference cost higher due to mixed natural-language + RT outputs and verification steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Paper treats executed Python verification as a symbolic / exact check (more reliable than free-form natural-language CoT). No direct human-vs-model cognitive comparison; symbolic execution is used as an objective verifier of numeric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3008.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3008.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (gpt-4o-2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-capability proprietary OpenAI LLM evaluated in ablations; strong at generating code and converting relation tuples into executable Python verification, achieving the highest reported arithmetic accuracies in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-05-13 (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary high-capability transformer LLM (GPT-4 family variant) used via API; exact architecture/training not given in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same multi-step arithmetic word problems (GSM8K and the other six arithmetic datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Uses relation tuples as semi-structured internal steps which are reliably translated to executable Python; verification-by-code is effective because GPT-4o generates correct verifier code most of the time; dynamic feedback used rarely because verification and RT answers align.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>High accuracy numbers: 'Reasoning with RT' reported ~96.4, 'Verification by Programming Code' ~95.5, and ART (combined) ~96.6 on datasets (Table 4). GPT-4o needed feedback very rarely on GSM8K (no-feedback for 98.3% of items in Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Minor drop when relying solely on verification answers vs RT answers (95.5 vs 96.4) suggests occasional code-generation/execution mismatch; no mechanistic probes of internal arithmetic representation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Relation tuples prompting, Python verifier generation & execution, dynamic feedback, Self-Consistency integration.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Very small absolute improvements when combining RT + verification + feedback (ART ~96.6) because the model already performs near ceiling; verification provides robust check and reduces need for feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported 'Reasoning with RT' ~96.4, 'Verification by Programming Code' ~95.5, and 'ART (ours)' ~96.6 on the reported arithmetic tasks (per Table 4). GSM8K no-feedback rate: 98.3% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Occasional inconsistency between RT-derived answer and generated code execution; rare code-execution errors; increased inference cost from verification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Verification via executable code used as a proxy for symbolic/correct computation; GPT-4o's strong code-generation narrows the gap to symbolic execution for arithmetic checking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3008.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3008.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3 8B Instruct (meta-llama/Meta-Llama-3-8B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 8B instruction-tuned LLM used in ablation studies; weaker at generating verification code from relation tuples and thus benefits differently from ART components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM (Meta Llama 3, 8 billion parameters) instruction-tuned; run locally with vLLM on an A100 GPU in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same set of arithmetic word-problem datasets (GSM8K and the other six datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>When prompted with relation tuples, Llama3-8B-Instruct can produce improved natural-language + RT reasoning (Step 1), but struggles to reliably translate RT into executable Python (Step 2), so verification is less effective; dynamic feedback is used more frequently for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation: 'Reasoning with RT' gives higher accuracy than CoT on multiple datasets; however 'Verification by Programming Code' accuracy is notably lower for Llama3 (Table 4/5). Execution errors observed frequently: UnboundLocalError (local variable referenced before assignment), SyntaxError, caused by symbols in RT that are invalid Python variable names.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Llama3's inability to consistently produce correct verifier code shows limits of the RT->code verification mechanism for smaller/open models; Step 2 can degrade final accuracy if relied on.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Relation tuples prompting, Python verification via local code interpreter, dynamic feedback, Self-Consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Relation tuples improved base reasoning accuracy (Step 1), integration with Self-Consistency raised GSM8K results (ART SC@1 80.4, ART SC@5 84.2). But verification (Step 2) often hurt accuracy because of code-generation errors; model required feedback more frequently (GSM8K no-feedback 79.7%, others higher feedback rates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported Llama3 ART (SC@1) on GSM8K: 80.4; ART (SC@5): 84.2 (Table 6). GSM8K no-feedback: 79.7% (Table 7). Verification-by-code accuracies for Llama3 were much lower (examples in Table 4/5 indicate a gap).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Frequent code-generation and execution errors (UnboundLocalError, SyntaxError) when converting relation tuples into Python; variable-name mapping issues; higher reliance on dynamic feedback; inference cost concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Paper uses Python execution as an objective symbolic check that Llama3 struggles to match; no direct human comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3008.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3008.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relation Tuples (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation Tuples (semi-structured relation triple notation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-structured, human-readable, machine-friendly representation of intermediate reasoning steps as triples (e.g., (variable, is, expression)), intended to bridge natural-language Chain-of-Thought and formal code for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across ChatGPT (gpt-3.5-turbo-0301), GPT-4o, Llama3-8B-Instruct (used in the paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model—an intermediate representation used in prompts and few-shot examples; designed to be short, readable, and translatable to Python.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used primarily for multi-step arithmetic word problems (GSM8K and others).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>RTs serve as compressed symbolic/structured notes capturing key quantities and formulas, acting as 'pause' tokens that encourage the LM to 'think' between steps; they are designed to be easily (automatically) translatable into code for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation results: reasoning processes with RT outperformed vanilla Chain-of-Thought on 4/7 arithmetic datasets; RTs are shorter and easier to verify; authors observed RTs can function as 'pause' tokens (cite Goyal et al.) and empirically improve accuracy in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>RTs rely on the model's familiarity with the semi-structured format; when models have seen few such inputs during training, their ability to convert RT into correct code is reduced (observed especially with Llama3). No internal representational probing is provided to show RT leads to algorithmic latent computations.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering / representation intervention (replace/augment CoT by RT in few-shot examples and prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved accuracy in several datasets (4/7) when RT used in few-shot prompts; enabled automated verification by making conversion to Python straightforward; reduced human-readability cost compared to full formal languages while being machine-friendly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical: improved accuracy on GSM8K and other datasets vs CoT; specific example: ART with RT contributed to GSM8K accuracy ~84.5 for ChatGPT. (Paper tables show RT ablations across datasets.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If RTs use symbols or formats not valid in Python, verification code may fail; models unfamiliar with RT distribution may produce incorrect code; inference cost still high since RTs are used in combination with NL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>RTs are positioned as a human-readable intermediate between natural language CoT and fully formal theorem provers (Isabelle) or code—intended to combine readability with automatic checkability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3008.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3008.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification by Programming Code (Local Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic verification via Python code executed in a local code interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification step in ART where the LM translates relation tuples into Python code, which is then executed locally to produce a verification answer that is compared to the LM's original answer; mismatches trigger dynamic feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across ChatGPT, GPT-4o, Llama3-8B-Instruct (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A local code interpreter (customized Local-Code-Interpreter) executes Python produced by the LMs for numerical verification; the interpreter is external to the LMs and returns exact numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric verification for arithmetic word problems (GSM8K, SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Uses symbolic execution (Python) as an objective ground-truth check for arithmetic computations derived from RTs; treats LM as a translator from RT->executable arithmetic algorithm, leveraging code execution to catch computational errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Where models generate correct Python from RTs (notably GPT-4o and often ChatGPT), verification improves reliability and identifies errors; integration of execution results with the LM's reasoning reduces final error by enabling feedback-driven regeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>For models poor at code generation (Llama3-8B), verification step reduced performance due to code-generation/execution errors; thus verification is only as reliable as the LM's ability to produce correct code from RTs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Tool use (external execution) / program synthesis + execution used as verification intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Provides an automatic check that can confirm or contradict the LM's original reasoning and drive dynamic feedback; improves final accuracy for models that can reliably generate code; increases inference cost and depends on code-generation ability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Verification-by-code accuracy varies by model: e.g., ChatGPT verification answers sometimes lower (~79.9 on GSM8K in some reported rows) than RT answers; GPT-4o verification ~95.5 (Table 4); Llama3's verification accuracies substantially lower due to execution errors (Table 4/5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Typical execution errors: UnboundLocalError (local variable referenced before assignment), SyntaxError; invalid variable names derived from RT symbols; Python runtime errors caused by incorrect translation of RT expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Code execution is used as a symbolic oracle and is explicitly compared against natural-language CoT outputs; authors contrast RT+execution approach with formal theorem provers (Isabelle) and argue RT is more human-readable while still verifiable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3008.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3008.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic feedback mechanism (on-demand correction loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight feedback loop: when the LM's Step 1 (RT-based reasoning) answer and Step 2 (verification-by-code) answer disagree, the previous reasoning is fed back to the LM and it regenerates reasoning up to a small maximum number of attempts (max 3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies across ChatGPT, GPT-4o, Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A protocol for iterative prompting rather than a model change: the LM is asked to rethink only when necessary (i.e., when RT answer != verification answer).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used for arithmetic word problems to elicit self-improvement and resolve inconsistencies between natural-language RT reasoning and symbolic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Dynamic feedback uses disagreement as a signal to elicit corrected reasoning paths from the LM, effectively using the verifier as a critic that triggers additional generation and self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: percentage of questions requiring feedback correlates inversely with model code-generation capability (Llama3 > ChatGPT > GPT-4o). Examples show dynamic feedback corrects initially incomplete/ambiguous RT solutions (Figure 6 example where feedback enables full resolution from 'At least 19' to '40').</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>If verification is itself unreliable (due to code-gen errors), feedback may be triggered spuriously or fail to converge to correct answer; paper limits attempts to 3 to avoid indefinite loops.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Iterative prompting / self-improvement loop triggered by verifier-model disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Increases final accuracy by allowing the model to correct mistakes when verification detects inconsistency; stronger-code models require feedback less often; increases number of inference rounds for items that need it.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GSM8K feedback usage: ChatGPT no-feedback 81.7% (remaining used 1–3 feedback attempts), Llama3 no-feedback 79.7% but higher multi-feedback rates, GPT-4o no-feedback 98.3% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Requires reliable verifier; may not help when both LM reasoning and generated verifier code are incorrect; increases inference cost and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Analogous to external criticism / checking in human problem solving; pairs LM generation with symbolic verification for iterative correction rather than comparing to human corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. <em>(Rating: 2)</em></li>
                <li>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. <em>(Rating: 2)</em></li>
                <li>Don't trust: Verify -grounding LLM quantitative reasoning with autoformalization. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Faithful logical reasoning via symbolic chain-of-thought. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3008",
    "paper_id": "paper-270738112",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5-turbo-0301)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301)",
            "brief_description": "Proprietary OpenAI conversational LLM used as a backbone in experiments; evaluated on multiple arithmetic word-problem datasets and used to generate relation-tuples, Python verifier code, and to respond to dynamic feedback.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT)",
            "model_description": "Proprietary transformer-based conversational LLM (OpenAI ChatGPT family), used via API in paper; exact architecture and training data not specified in this work.",
            "arithmetic_task_type": "Multi-step arithmetic word problems (GSM8K: 2–8 ops), SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith (math word problems, chained operations).",
            "reported_mechanism": "Uses semi-structured intermediate representation (relation tuples) to record key arithmetic facts/operations; generates Python verification code from relation tuples and executes it; dynamic feedback loop when answers from natural-language reasoning and code verification disagree. Relation tuples act like 'pause' tokens prompting internal deliberation.",
            "evidence_for_mechanism": "Ablation: incorporating relation tuples into few-shot examples improved accuracy on 4/7 datasets; ART (relation tuples + verification + dynamic feedback) improves GSM8K accuracy to ~84.5 (paper reports +1.9% over ModelSelection's 82.6). Verification step sometimes lowers accuracy compared to relation-tuples answers alone, but overall pipeline yields higher final accuracy. Percentage-of-feedback statistics show ChatGPT requires less feedback than weaker code-capable models.",
            "evidence_against_mechanism": "Verification-by-code answers were sometimes less accurate than the relation-tuple answers (Step 2 &lt; Step 1) for some models; indicates that code-generation-from-RT distribution is not always handled perfectly. No internal probing is performed to show algorithmic representations beyond behavioral gains.",
            "intervention_type": "Prompting with relation tuples (semi-structured RT), generating/executing Python verification code (local code interpreter), and a dynamic feedback loop; optionally integrated with Self-Consistency (sampling multiple reasoning paths).",
            "effect_of_intervention": "Improved overall arithmetic accuracy compared to baselines (CoT, PAL, ModelSelection); reduced number of feedback iterations needed relative to weaker models; allowed automatic verification of numeric computations via execution; enables selection of most frequent answer across steps (compatible with Self-Consistency).",
            "performance_metrics": "Reported GSM8K accuracy for ART (ChatGPT backbone) ~84.5 (paper: ART improves GSM8K by 1.9% over ModelSelection's 82.6). Paper reports improvements on SVAMP and AddSub as well (SVAMP improved by 2.8% vs ModelSelection). Percent of GSM8K questions needing no feedback for ChatGPT: 81.7% (Table 7).",
            "notable_failure_modes": "When verification/code-generation disagrees with natural-language reasoning, ChatGPT sometimes needed dynamic feedback; code-generation errors (less frequent than for weaker models) can still occur; inference cost higher due to mixed natural-language + RT outputs and verification steps.",
            "comparison_to_humans_or_symbolic": "Paper treats executed Python verification as a symbolic / exact check (more reliable than free-form natural-language CoT). No direct human-vs-model cognitive comparison; symbolic execution is used as an objective verifier of numeric computation.",
            "uuid": "e3008.0",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (gpt-4o-2024-05-13)",
            "brief_description": "High-capability proprietary OpenAI LLM evaluated in ablations; strong at generating code and converting relation tuples into executable Python verification, achieving the highest reported arithmetic accuracies in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-05-13 (GPT-4o)",
            "model_description": "Proprietary high-capability transformer LLM (GPT-4 family variant) used via API; exact architecture/training not given in the paper.",
            "arithmetic_task_type": "Same multi-step arithmetic word problems (GSM8K and the other six arithmetic datasets).",
            "reported_mechanism": "Uses relation tuples as semi-structured internal steps which are reliably translated to executable Python; verification-by-code is effective because GPT-4o generates correct verifier code most of the time; dynamic feedback used rarely because verification and RT answers align.",
            "evidence_for_mechanism": "High accuracy numbers: 'Reasoning with RT' reported ~96.4, 'Verification by Programming Code' ~95.5, and ART (combined) ~96.6 on datasets (Table 4). GPT-4o needed feedback very rarely on GSM8K (no-feedback for 98.3% of items in Table 7).",
            "evidence_against_mechanism": "Minor drop when relying solely on verification answers vs RT answers (95.5 vs 96.4) suggests occasional code-generation/execution mismatch; no mechanistic probes of internal arithmetic representation provided.",
            "intervention_type": "Relation tuples prompting, Python verifier generation & execution, dynamic feedback, Self-Consistency integration.",
            "effect_of_intervention": "Very small absolute improvements when combining RT + verification + feedback (ART ~96.6) because the model already performs near ceiling; verification provides robust check and reduces need for feedback.",
            "performance_metrics": "Reported 'Reasoning with RT' ~96.4, 'Verification by Programming Code' ~95.5, and 'ART (ours)' ~96.6 on the reported arithmetic tasks (per Table 4). GSM8K no-feedback rate: 98.3% (Table 7).",
            "notable_failure_modes": "Occasional inconsistency between RT-derived answer and generated code execution; rare code-execution errors; increased inference cost from verification pipeline.",
            "comparison_to_humans_or_symbolic": "Verification via executable code used as a proxy for symbolic/correct computation; GPT-4o's strong code-generation narrows the gap to symbolic execution for arithmetic checking.",
            "uuid": "e3008.1",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama3-8B-Instruct",
            "name_full": "Meta Llama 3 8B Instruct (meta-llama/Meta-Llama-3-8B-Instruct)",
            "brief_description": "Open-source 8B instruction-tuned LLM used in ablation studies; weaker at generating verification code from relation tuples and thus benefits differently from ART components.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct (8B)",
            "model_description": "Open-source transformer LLM (Meta Llama 3, 8 billion parameters) instruction-tuned; run locally with vLLM on an A100 GPU in the paper's experiments.",
            "arithmetic_task_type": "Same set of arithmetic word-problem datasets (GSM8K and the other six datasets).",
            "reported_mechanism": "When prompted with relation tuples, Llama3-8B-Instruct can produce improved natural-language + RT reasoning (Step 1), but struggles to reliably translate RT into executable Python (Step 2), so verification is less effective; dynamic feedback is used more frequently for this model.",
            "evidence_for_mechanism": "Ablation: 'Reasoning with RT' gives higher accuracy than CoT on multiple datasets; however 'Verification by Programming Code' accuracy is notably lower for Llama3 (Table 4/5). Execution errors observed frequently: UnboundLocalError (local variable referenced before assignment), SyntaxError, caused by symbols in RT that are invalid Python variable names.",
            "evidence_against_mechanism": "Llama3's inability to consistently produce correct verifier code shows limits of the RT-&gt;code verification mechanism for smaller/open models; Step 2 can degrade final accuracy if relied on.",
            "intervention_type": "Relation tuples prompting, Python verification via local code interpreter, dynamic feedback, Self-Consistency.",
            "effect_of_intervention": "Relation tuples improved base reasoning accuracy (Step 1), integration with Self-Consistency raised GSM8K results (ART SC@1 80.4, ART SC@5 84.2). But verification (Step 2) often hurt accuracy because of code-generation errors; model required feedback more frequently (GSM8K no-feedback 79.7%, others higher feedback rates).",
            "performance_metrics": "Reported Llama3 ART (SC@1) on GSM8K: 80.4; ART (SC@5): 84.2 (Table 6). GSM8K no-feedback: 79.7% (Table 7). Verification-by-code accuracies for Llama3 were much lower (examples in Table 4/5 indicate a gap).",
            "notable_failure_modes": "Frequent code-generation and execution errors (UnboundLocalError, SyntaxError) when converting relation tuples into Python; variable-name mapping issues; higher reliance on dynamic feedback; inference cost concerns.",
            "comparison_to_humans_or_symbolic": "Paper uses Python execution as an objective symbolic check that Llama3 struggles to match; no direct human comparison provided.",
            "uuid": "e3008.2",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Relation Tuples (RT)",
            "name_full": "Relation Tuples (semi-structured relation triple notation)",
            "brief_description": "A semi-structured, human-readable, machine-friendly representation of intermediate reasoning steps as triples (e.g., (variable, is, expression)), intended to bridge natural-language Chain-of-Thought and formal code for verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies across ChatGPT (gpt-3.5-turbo-0301), GPT-4o, Llama3-8B-Instruct (used in the paper's experiments)",
            "model_description": "Not a model—an intermediate representation used in prompts and few-shot examples; designed to be short, readable, and translatable to Python.",
            "arithmetic_task_type": "Used primarily for multi-step arithmetic word problems (GSM8K and others).",
            "reported_mechanism": "RTs serve as compressed symbolic/structured notes capturing key quantities and formulas, acting as 'pause' tokens that encourage the LM to 'think' between steps; they are designed to be easily (automatically) translatable into code for execution.",
            "evidence_for_mechanism": "Ablation results: reasoning processes with RT outperformed vanilla Chain-of-Thought on 4/7 arithmetic datasets; RTs are shorter and easier to verify; authors observed RTs can function as 'pause' tokens (cite Goyal et al.) and empirically improve accuracy in several settings.",
            "evidence_against_mechanism": "RTs rely on the model's familiarity with the semi-structured format; when models have seen few such inputs during training, their ability to convert RT into correct code is reduced (observed especially with Llama3). No internal representational probing is provided to show RT leads to algorithmic latent computations.",
            "intervention_type": "Prompt engineering / representation intervention (replace/augment CoT by RT in few-shot examples and prompts).",
            "effect_of_intervention": "Improved accuracy in several datasets (4/7) when RT used in few-shot prompts; enabled automated verification by making conversion to Python straightforward; reduced human-readability cost compared to full formal languages while being machine-friendly.",
            "performance_metrics": "Empirical: improved accuracy on GSM8K and other datasets vs CoT; specific example: ART with RT contributed to GSM8K accuracy ~84.5 for ChatGPT. (Paper tables show RT ablations across datasets.)",
            "notable_failure_modes": "If RTs use symbols or formats not valid in Python, verification code may fail; models unfamiliar with RT distribution may produce incorrect code; inference cost still high since RTs are used in combination with NL.",
            "comparison_to_humans_or_symbolic": "RTs are positioned as a human-readable intermediate between natural language CoT and fully formal theorem provers (Isabelle) or code—intended to combine readability with automatic checkability.",
            "uuid": "e3008.3",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Verification by Programming Code (Local Code Interpreter)",
            "name_full": "Automatic verification via Python code executed in a local code interpreter",
            "brief_description": "A verification step in ART where the LM translates relation tuples into Python code, which is then executed locally to produce a verification answer that is compared to the LM's original answer; mismatches trigger dynamic feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies across ChatGPT, GPT-4o, Llama3-8B-Instruct (used in experiments)",
            "model_description": "A local code interpreter (customized Local-Code-Interpreter) executes Python produced by the LMs for numerical verification; the interpreter is external to the LMs and returns exact numeric results.",
            "arithmetic_task_type": "Numeric verification for arithmetic word problems (GSM8K, SVAMP, ASDIV, SingleOP, SingleEQ, AddSub, MultiArith).",
            "reported_mechanism": "Uses symbolic execution (Python) as an objective ground-truth check for arithmetic computations derived from RTs; treats LM as a translator from RT-&gt;executable arithmetic algorithm, leveraging code execution to catch computational errors.",
            "evidence_for_mechanism": "Where models generate correct Python from RTs (notably GPT-4o and often ChatGPT), verification improves reliability and identifies errors; integration of execution results with the LM's reasoning reduces final error by enabling feedback-driven regeneration.",
            "evidence_against_mechanism": "For models poor at code generation (Llama3-8B), verification step reduced performance due to code-generation/execution errors; thus verification is only as reliable as the LM's ability to produce correct code from RTs.",
            "intervention_type": "Tool use (external execution) / program synthesis + execution used as verification intervention.",
            "effect_of_intervention": "Provides an automatic check that can confirm or contradict the LM's original reasoning and drive dynamic feedback; improves final accuracy for models that can reliably generate code; increases inference cost and depends on code-generation ability.",
            "performance_metrics": "Verification-by-code accuracy varies by model: e.g., ChatGPT verification answers sometimes lower (~79.9 on GSM8K in some reported rows) than RT answers; GPT-4o verification ~95.5 (Table 4); Llama3's verification accuracies substantially lower due to execution errors (Table 4/5).",
            "notable_failure_modes": "Typical execution errors: UnboundLocalError (local variable referenced before assignment), SyntaxError; invalid variable names derived from RT symbols; Python runtime errors caused by incorrect translation of RT expressions.",
            "comparison_to_humans_or_symbolic": "Code execution is used as a symbolic oracle and is explicitly compared against natural-language CoT outputs; authors contrast RT+execution approach with formal theorem provers (Isabelle) and argue RT is more human-readable while still verifiable.",
            "uuid": "e3008.4",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Dynamic Feedback",
            "name_full": "Dynamic feedback mechanism (on-demand correction loop)",
            "brief_description": "A lightweight feedback loop: when the LM's Step 1 (RT-based reasoning) answer and Step 2 (verification-by-code) answer disagree, the previous reasoning is fed back to the LM and it regenerates reasoning up to a small maximum number of attempts (max 3).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies across ChatGPT, GPT-4o, Llama3-8B-Instruct",
            "model_description": "A protocol for iterative prompting rather than a model change: the LM is asked to rethink only when necessary (i.e., when RT answer != verification answer).",
            "arithmetic_task_type": "Used for arithmetic word problems to elicit self-improvement and resolve inconsistencies between natural-language RT reasoning and symbolic verification.",
            "reported_mechanism": "Dynamic feedback uses disagreement as a signal to elicit corrected reasoning paths from the LM, effectively using the verifier as a critic that triggers additional generation and self-correction.",
            "evidence_for_mechanism": "Empirical: percentage of questions requiring feedback correlates inversely with model code-generation capability (Llama3 &gt; ChatGPT &gt; GPT-4o). Examples show dynamic feedback corrects initially incomplete/ambiguous RT solutions (Figure 6 example where feedback enables full resolution from 'At least 19' to '40').",
            "evidence_against_mechanism": "If verification is itself unreliable (due to code-gen errors), feedback may be triggered spuriously or fail to converge to correct answer; paper limits attempts to 3 to avoid indefinite loops.",
            "intervention_type": "Iterative prompting / self-improvement loop triggered by verifier-model disagreement.",
            "effect_of_intervention": "Increases final accuracy by allowing the model to correct mistakes when verification detects inconsistency; stronger-code models require feedback less often; increases number of inference rounds for items that need it.",
            "performance_metrics": "GSM8K feedback usage: ChatGPT no-feedback 81.7% (remaining used 1–3 feedback attempts), Llama3 no-feedback 79.7% but higher multi-feedback rates, GPT-4o no-feedback 98.3% (Table 7).",
            "notable_failure_modes": "Requires reliable verifier; may not help when both LM reasoning and generated verifier code are incorrect; increases inference cost and latency.",
            "comparison_to_humans_or_symbolic": "Analogous to external criticism / checking in human problem solving; pairs LM generation with symbolic verification for iterative correction rather than comparing to human corrections.",
            "uuid": "e3008.5",
            "source_info": {
                "paper_title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning.",
            "rating": 2,
            "sanitized_title": "mathcoder_seamless_code_integration_in_llms_for_enhanced_mathematical_reasoning"
        },
        {
            "paper_title": "Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification.",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "Don't trust: Verify -grounding LLM quantitative reasoning with autoformalization.",
            "rating": 2,
            "sanitized_title": "dont_trust_verify_grounding_llm_quantitative_reasoning_with_autoformalization"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Faithful logical reasoning via symbolic chain-of-thought.",
            "rating": 1,
            "sanitized_title": "faithful_logical_reasoning_via_symbolic_chainofthought"
        }
    ],
    "cost": 0.0181865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback
25 Jun 2024</p>
<p>Zhongtao Miao 
Kaiyan Zhao zhaokaiyan1006@g.ecc.u-tokyo.ac.jp 
Yoshimasa Tsuruoka yoshimasa-tsuruoka@g.ecc.u-tokyo.ac.jp 
Josh Achiam 
Steven Adler 
Sandhini Agarwal 
Lama Ahmad 
Ilge Akkaya 
Florencia Leoni Aleman 
Diogo Almeida 
Janko Altenschmidt 
Sam Altman 
Rohan Anil 
Andrew M Dai 
MelvinOrhan Firat 
Dmitry Lepikhin 
Alexandre Passos 
Siamak Shakeri 
Emanuel Taropa 
Paige Bailey 
Zhifeng Chen 
Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Ma- Teusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Luyu Gao 
Aman Madaan 
Shuyan Zhou </p>
<p>The University of Tokyo
TokyoJapan</p>
<p>Gra-ham Neubig
Uri Alon
Yiming Yang, Jamie Callan2023Pengfei Liu</p>
<p>Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback
25 Jun 2024695BA3808BF556DDE4DAC82E37B7160AarXiv:2406.17873v1[cs.CL]
Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) nonnatural language, usually programming code, which is difficult for people who are unfamiliar with coding to read.In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models.Specifically, we use relation tuples, which are not only human-readable but also machinefriendly and easier to verify than natural language.We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models.The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models.The source code is available at https://github.com/gpgg/art.</p>
<p>Introduction</p>
<p>Large language models, such as GPT series (Brown et al., 2020;Achiam et al., 2023), PaLM (Anil et al., 2023), Mistral (Jiang et al., 2023), andLLaMA (Touvron et al., 2023a,b;AI@Meta, 2024), have shown great success in numerous tasks that require reasoning.Besides the approach to scaling up the size of large language models and training data to enhance their reasoning ability, many prompting methods have been proposed to improve their reasoning performance.Previous works (Wei et al., 2022;Kojima et al., 2022;Zelikman et al., 2022;Gao et al., 2023), which aim to enhance the reasoning ability of large language models, can be categorized into two main types: natural language-based Figure 1: Schematic overview of our framework, ART."Q" denotes a question."NL" means "Natural Language"."RT" means "Relation Tuple".The left subfigure shows our proposed framework ART without Self-Consistency (Wang et al., 2023).The right subfigure shows that our framework can be integrated with Self-Consistency seamlessly.</p>
<p>approaches and non-natural language-based approaches.The natural language-based approaches include Chain-of-Thought (CoT) (Wei et al., 2022) and Zero-shot CoT (Kojima et al., 2022), which utilize intermediate reasoning steps in natural language to elicit the reasoning ability of large language models.The non-natural language-based approaches include PAL (Gao et al., 2023), which proposes to use Python code to solve math word problems.</p>
<p>However, the reasoning steps represented in nat-ural language are usually long, which can significantly increase inference cost and may contain computational errors and unjustified logical leaps (Zhou et al., 2024b).Besides, unlike graphs or formal languages, they are difficult to verify because of the nature of natural language (Zhou et al., 2024b).</p>
<p>Recently, there have been some studies that focus on translating natural language statements into formal languages such as Isabelle (Nipkow et al., 2002) using large language models (Agrawal et al., 2022;Zhou et al., 2024b;Xu et al., 2024b).However, those formal languages are hard for humans to read.</p>
<p>In this study, we propose a framework named ART 1 to enhance the arithmetic reasoning ability of large language models.A schematic overview of our ART framework is shown in Figure 1.First, we utilize in-context learning to make a large language model generate reasoning steps mixed with a simple semi-structured form, relation tuples.We can obtain an answer after reasoning.These relation tuples are very similar to pseudo-code, which can easily be translated into real programming code.Next, the large language model generates a Python code solution to verify the reasoning steps based on the question and relation tuples.We run the Python code in a local code interpreter to obtain the verification answer.Finally, we check whether the two answers are consistent or not and provide a dynamic feedback when necessary.If the two answers are inconsistent, we will use the large language model to regenerate a new reasoning process based on a simple dynamic feedback mechanism.The answer is determined if the two answers are consistent or reach the maximum number of tries in the feedback loop.</p>
<p>The main contributions of this paper can be summarized as follows:</p>
<p>• We introduce a semi-structured representation, relation tuples, into the reasoning steps of large language models.Relation tuples are usually shorter and easier to read, compared with long reasoning steps in natural language.They are more machine friendly because they are very similar to pseudo-code, which can be translated to real Python or other programming language code easily.Our findings also reveal that incorporating relation tuples into few-shot examples can improve the accuracy 1 ART: Improving Arithmetic Reasoning Ability through Relation Tuples, Verification and Dynamic Feedback on four out of seven arithmetic datasets.</p>
<p>• This study provides a local code interpreter and employs it to develop a reasoning step verifier based on relation tuples.This local code interpreter can be integrated with any large language model seamlessly, regardless of whether they are open source or not.</p>
<p>• We implement a simple and effective dynamic feedback mechanism.Unlike Self-Refine (Madaan et al., 2023), our dynamic feedback mechanism is considerably simpler but effective.Here, "Dynamic" means that feedback is provided when necessary.</p>
<p>2 Method</p>
<p>Problem Formulation</p>
<p>We denote a large language model as LM.Suppose that we have a dataset D. The dataset can be denoted as D = {Q i , A i } N −1 i=0 , where Q i is the i-th question, A i is the answer of Q i and N is the number of examples in the dataset.The CoT method aims to generate a series of reasoning steps and an answer, which can be denoted as:
[ Ri , Âi ] = LM(Q i ),(1)
where Ri denotes the generated intermediate reasoning steps of the large language model LM and Âi denotes the predicted answer after the reasoning steps.The local code interpreter is denoted as LCI.</p>
<p>ART Framework</p>
<p>The ART framework can be described in the following steps:</p>
<p>Step 1: Reasoning with relation tuples.Given a question Q i from the dataset D, LM generates reasoning process Ri = LM(Q i ) and its answer, Âi .The reasoning process consists of a series of reasoning steps and each reasoning step contains a natural language statement and its relation tuple equivalent.The reasoning process can be denoted as a list:</p>
<p>Ri = [(r 0 , t 0 ), . . ., (r i , t i ), . . ., (r n−1 , t n−1 )],</p>
<p>(2) where r i is the reasoning step in natural language and t i is its equivalent in the relation tuple form.n is the number of reasoning steps.The prompt used in this step is shown in Figure 3.</p>
<p>Q</p>
<p>…</p>
<p>Example</p>
<p>Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for $2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?</p>
<p>Question</p>
<p>Reasoning Process with Relation Tuples</p>
<p>We know that Janet's ducks lay 16 eggs per day, (number-ofeggs-laid-per-day, is, 16).She eats three for breakfast every morning and bakes muffins for her friends every day with four, (number-of-eggs-used-perday, is, 3 + 4).…… To find out how much she makes every day at the farmers' market, we multiply the number of eggs left to sell by the price per egg, (total-earnings-per-day, is, number-of-eggs-leftto-sell-per-day * price-per-egg).9 * $2 = $18.Thus, Janet makes $18 every day at the farmers' market.Step 2: Automatic verification with relation triples and a local code interpreter.We can extract the relation tuples from the reasoning steps Ri in Step 1.The relation tuples extracted are denoted as a list:</p>
<p>Verification by
T i = [t 0 , . . . , t i , . . . , t n−1 ].(3)
To verify whether the reasoning steps in Step 1 are correct or not, we decide to use Python code and implement a local code interpreter.Based on the question Q i and reasoning steps in relation tuples T i , LM generates a Python code solution C i step by step.The code generation process can be denoted as:
C i = LM(Q i , T i ). (4)
After obtaining the Python solution C i .We execute it using our local code interpreter LCI and get the verification answer Âv i from the execution result:
Âv i = LCI(C i ).(5)
The prompt used in this step is shown in Figure 4.</p>
<p>Step 3: Checking consistency and providing dynamic feedback when necessary.equal, it indicates that the reasoning steps in Step 1 are consistent with Step 2, confirming that there is no computational error.Therefore, the answer is determined.However, if the two answers are inconsistent, the previous reasoning steps Ri will be resent to the large language model LM as a feedback.LM regenerates reasoning process Ri and its answer Âi based on the feedback.The feedback prompt used here is shown in Figure 5.We record all the answers from Step 1 and Step 2 and choose the most common one as the final answer, ensuring seamless integration with the Self-Consistency approach (Wang et al., 2023).We also provide an example to show the effectiveness of this dynamic feedback mechanism in Figure 6.Models.To evaluate our approach, we employ two proprietary large language models, ChatGPT (gpt-3.5-turbo-0301)2and GPT4o (gpt-4o-2024-05-13) 3 and one open source large language model, Llama3-8B-Instruct (meta-llama/Meta-Llama-3-8B-Instruct) 4 .Specifically, we utilize ChatGPT for our main ex-</p>
<p>The Doubtfire sisters are driving home with 7 kittens adopted from the local animal shelter when their mother calls to inform them that their two house cats have just had kittens.She says that Patchy, the first cat, has had thrice the number of adopted kittens, while Trixie, the other cat, has had 12.How many kittens does the Doubtfire family now have?</p>
<p>Question</p>
<p>Reasoning with RT w/o feedback (ART) Reasoning with RT w/ feedback (ART)</p>
<p>We know that Trixie, one of the house cats, has had 12 kittens, (number-ofkittens-from-Trixie, is, 12).Patchy, the other house cat, has had thrice the number of adopted kittens, (numberof-kittens-from-Patchy, is, 3 * number-ofadopted-kittens). …… We cannot determine the exact number of kittens without knowing the value of 'x', the number of adopted kittens.However, we do know that the Doubtfire family has at least 19 kittens.The final answer: At least 19 kittens.</p>
<p>My apologies, I made a mistake in my previous solution.We can determine the exact number of kittens from Patchy by using the information that Patchy has had thrice the number of adopted kittens.Answer: We know that Trixie, one of the house cats, has had 12 kittens, (number-of-kittens-from-Trixie, is, 12).…… The total number of kittens is the sum of the kittens from Trixie, Patchy, and the adopted kittens, (totalnumber-of-kittens, is, number-of-kittens-from-Trixie + number-of-kittens-from-Patchy + number-of-adoptedkittens).</p>
<p>Substituting the values we know, we get: total-number-of-kittens = 12 + 21 + 7 = 40.Thus, the Doubtfire family now has 40 kittens.The final answer: 40.</p>
<p>Figure 6: Comparison of "Reasoning with RT" solutions without feedback and with feedback, generated by our framework.This example shows the predicted solution for the 55-th question in the test split of the GSM8K dataset, using our method with ChatGPT (gpt-3.5-turbo-0301).</p>
<p>periments and use GPT-4o and Llama3-8B-Instruct in our ablation study.In our framework, the temperatures of Llama3-8B-Instruct are set to 0.5 in both Step 1 and Step 2. For the results in Table 2, both temperatures in Step 1 and Step 2 are set to 0, and the top_p parameter is set to 1 to ensure a fair comparison with ModelSelection (Zhao et al., 2023).We use the same prompt for all seven arithmetic datasets for each large language model.</p>
<p>In-context Learning.For ChatGPT and Llama3-8B-Instruct, we employ in-context learning using an eight-shot setting.In this setting, we select the first eight questions from the train split of the GSM8K dataset.et al. (2023).Bold fonts highlight the best performance for each dataset.</p>
<p>Main Results</p>
<p>As shown in Table 2, we report the accuracy results on the seven arithmetic datasets.Table 2 shows that our approach outperforms CoT, PAL and ModelSelection baselines on ChatGPT (gpt-3.5-turbo-0301).Notably, our method is particularly effective on the GSM8K, SVAMP and AddSub datasets.Specifically, it improves accuracy on the SVAMP dataset by 2.8%, compared with ModelSelection and achieves a 1.9% improvement over ModelSelection's 82.6% accuracy on the GSM8K dataset.</p>
<p>Analysis and Discussion</p>
<p>In this section, we analyze various factors affecting the performance of our framework.The dataset we use here is GSM8K.First, we investigate the effects of prior prompts used in ModelSelection (Zhao et al., 2023) and GPT-4 generated prompts using the same CoT method because our eight-shot examples are created based on the GPT-4 generated solutions.Then, we assess the contributions of relation tuples, verification by programming code and feedback individually using three different large language models.Finally, we show that our method can be integrated into Self-Consistency.</p>
<p>Original Prompt vs. GPT-4 generated Prompt</p>
<p>We utilize in-context learning to build our framework.Existing works use the eight-shot examples from CoT while the eight-shot examples in our method are manually created with the help of GPT-4.Therefore, in this section, we aim to test the impact of difficulty of different prompts on the model's performance with CoT.As shown in Table 3, we find that the performance difference between using the two versions of prompts is not significant on ChatGPT and Llama3-8B-Instruct.The GPT-4 generated eight-shot prompt and the eightshot prompt used in our framework are shown in Appendix B.</p>
<p>Role of Relation Tuples in Step 1</p>
<p>In this section, we analyze the role of relation tuples.From Table 4, we can observe that the reasoning process incorporating relation tuples outperforms the CoT reasoning process on four out of the seven arithmetic datasets.Relation tuples in the reasoning process can be viewed as notes that record key points in the reasoning steps in natural language.These relation tuples may function as "pause" tokens (Goyal et al., 2024), prompting large language models to "think" before generating the next reasoning step.</p>
<p>Role of Verification by Programming Code in Step 2</p>
<p>Table 5 shows the accuracy on the GSM8K dataset when using the answers from different steps of our framework as the final answers.In the From Table 5, it is evident that the accuracy scores on the GSM8K dataset using the verification answers from Step 2 of our framework are are lower than those using relation tuples.We can observe that the most obvious one is Llama3-8B-Instruct, which cannot generate programming code very well based on the semi-structured form of reasoning (relation tuples), whereas ChatGPT and GPT-4o excel in this task.</p>
<p>A possible reason for this discrepancy could be that in Step 2 of our framework, we use relation tuples and questions as inputs for large language models, which are infrequently encountered during their training phases.Consequently, these models struggle with generating Python solutions from this semi-structured form.This is particularly evident in the Llama3-8B-Instruct model, where there is an accuracy gap between using answers from Step 1 and Step 2 as final answers.This indicates that Llama3-8B-Instruct may have difficulty generating Python verification solutions based on relation tuples.</p>
<p>We also observe several common execution errors when Llama3-8B-instruct generates and executes Python solution code to verify the reasoning process in Step 2. Empirically, the most frequent error is "UnboundLocalError: local variable referenced before assignment", typically caused by using symbols that cannot serve as variable names in Python.Additionally, "SyntaxError" is another commonly encountered error.</p>
<p>Llama3</p>
<p>ChatGPT  No feedback Feedback (one-time)</p>
<p>Feedback (two-times) Feedback (three-times)</p>
<p>Role of Feedback in Step 3</p>
<p>We explore the effect of the dynamic feedback mechanism in our framework in this section.Figure 7 and 8 show the percentage of questions utilizing feedback on the GSM8K dataset and the other 6 arithmetic datasets, respectively.In Figure 7, we observe an interesting phenomenon: as the coding capabilities of the large language models increase (Llama3-8B-Instruct &lt; ChatGPT &lt; GPT-4o) as shown in Table 5, the percentage of questions requiring feedback continuously decreases.</p>
<p>From Figure 8, we observe that the dataset on which ChatGPT requires feedback most frequently is ASDIV.The percentage of feedback utilization might be related to the quality of datasets and the Backbone Method GSM8K</p>
<p>Llama3-8B-Instruct ART (SC@1) 80.4 ART (SC@5) 84.2</p>
<p>Table 6: Accuracy on the GSM8K dataset after integrating Self-Consistency (SC@5) into our framework ART."SC@5" means that the number of sampled paths is 5.</p>
<p>programming code understanding and generation capabilities of large language models.</p>
<p>Integration with Self-Consistency</p>
<p>Our framework is designed for seamless integration with the Self-Consistency approach (Wang et al., 2023).The core idea of Self-Consistency is to select the most common answer derived from multiple reasoning paths.In our framework, we also determine the final answer by choosing the most frequent answer from different steps.From Table 6, we can observe that with the aid of Self-Consistency, our framework significantly enhances the arithmetic reasoning performance of Llama3-8B-Instruct on the GSM8K dataset.</p>
<p>Related Work</p>
<p>Natural language reasoning.There are large amounts of studies (Qiao et al., 2023;Sanyal et al., 2022;Nye et al., 2021;Wang et al., 2022) focusing on enhancing the reasoning ability of large language models in natural language.Chain-of-Thought (CoT) (Wei et al., 2022) shows that intermediate reasoning steps can improve the performance of large language models.Zero-shot CoT, as proposed by Kojima et al. (2022), involves simply adding "Let's think step by step" before generating answers to elicit the reasoning ability of large language models.Least-to-most prompting (Zhou et al., 2023) breaks down complex problems to simpler problems and solve them in sequence to enable complex reasoning in large language models.Self-Consistency (Wang et al., 2023) extends CoT by sampling various reasoning paths, generating multiple answers and choosing the most common one.Tree-of-Thought (Yao et al., 2023) generalizes over Chain-of-Thought by framing any problem as a search over a tree.Besta et al. (2024) propose Graph-of-Thoughts to improve large language model's reasoning ability by modeling large language model thoughts as vertices and dependencies between these vertices as edges.Buffer-of-Thoughts (Yang et al., 2024) is a novel prompting approach which employs a meta-buffer to store a series of thought templates (high-level thoughts) and retrieves a relevant thought template and instantiate it when conducting reasoning.</p>
<p>Non-natural language reasoning and verification.There are many works (Kadlčík et al., 2023;Gao et al., 2023;Xu et al., 2024b) aiming to enhance the reasoning ability of large language models by using non-natural language forms during the reasoning process.PAL (Gao et al., 2023)  MathCoder (Wang et al., 2024) integrates natural language reasoning, code generation and execution results to enhance the mathematical reasoning ability of large language models by fine-tuning them.SymbolCoT (Xu et al., 2024b) integrates symbolic expressions and logic rules into the reasoning process of large language models to enhance their logical reasoning ability.Zhou et al. (2024b) translate informal natural language reasoning statements into formal Isabelle code which can be verified automatically to enhance internal consistency of reasoning in large language models.Different from these works, our method utilizes the semistructure understanding and code generation ability of large language models to verify the reasoning process.</p>
<p>Self-improvement and verification.There are many works focusing on the self-improvement of large language models (Huang et al., 2023;Madaan et al., 2023;Haluptzok et al., 2023;Xu et al., 2024a;Yu et al., 2023).Zelikman et al. (2022) propose Self-Taught Reasoner (STaR), which employs a reasoning process generation loop to produce reasoning steps and use these generated reasoning paths whose final answers are correct to further fine-tune large language models.Madaan et al.</p>
<p>(2023) propose Self-Refine, which has three components (generator, feedback provider and refiner).Compared with Self-Refine, the dynamic feedback in our framework is provided only when necessary.Moreover, our framework does not need the feedback provider.</p>
<p>Conclusion</p>
<p>In this paper, we propose to use a semi-structured representation for the arithmetic reasoning steps of large language models.Specifically, we utilize relation tuples to connect reasoning in natural language with formal languages, such as programming code, to more effectively verify the reasoning process of large language models.These relation tuples are human-readable and can easily be translated into formal languages.</p>
<p>Based on this new representation of reasoning steps, we have implemented a novel framework that integrates the semi-structured representation, relation tuples, into the reasoning process of large language models.Additionally, we developed a local code interpreter to verify the reasoning process of large language models.Our framework also includes a simple and effective dynamic feedback mechanism to elicit the self-improvement ability of large language models.Experimental results demonstrate that our framework can improve the arithmetic reasoning ability of large language models.</p>
<p>Limitations</p>
<p>We utilize programming code based on relation tuples to verify reasoning process.Therefore, our method highly depends on the programming code understanding and generation ability of large language models that we use.</p>
<p>Besides, the reasoning process in our method is a mixture of informal natural language statements and semi-structured relation tuples.Therefore, the inference cost is high.It will be great if large language models can reason with relation tuples only, which can reduce the inference cost while maintaining readability and are easy for machine to further process these relation tuples (e.g., automatic verification).</p>
<p>Finally, there might be other semi-structured forms of reasoning steps which are easy to verify.</p>
<p>A Comparison of different methods</p>
<p>A comparison of different methods is shown in Figure 9.</p>
<p>B Full Prompts</p>
<p>B.1 Eight-shot examples</p>
<p>Figure 10, Figure 11 and Figure 12 show the full prompts of CoT (GPT-4-generated Prompt). Figure 13, Figure 14 and Figure 15 show the full prompts used in step 1 of our framework in the eight-shot setting.</p>
<p>Figure 16, Figure 17, Figure 18, Figure 19 and Figure 20 show the full prompts used in step 2 of our framework in the eight-shot setting.</p>
<p>B.2 Five-shot examples</p>
<p>Figure 21 and Figure 22 show the full prompts used in step 1 of our framework in the five-shot setting.</p>
<p>Figure 23, Figure 24 and Figure 25 show the full prompts used in step 2 of our framework in the five-shot setting.</p>
<p>C Number of Questions Using Feedback on Arithmetic Datasets</p>
<p>The statistics of the number of the questions which need feedback and the questions which do not need feedback during inference using our framework on GSM8K dataset are shown in Table 7.</p>
<p>The statistics of the number of the questions that need feedback and the questions that do not need feedback in our framework on the other six arithmetic datasets are shown in Table 8.</p>
<p>Question</p>
<p>CoT (GPT-4 8-shot prompt) ModelSelection ART (Ours) John plans to sell all his toys and use the money to buy video games.He has 13 lego sets and he sells them for $15 each.He ends up buying 8 video games for $20 each and has $5 left.How many lego sets does he still have?Let's break down the information provided: 1. John has 13 lego sets.2.He sells them for $15 each, so he earns a total of \(13 \times \$15 = \$195 \). 3.He uses the money to buy 8 video games for $20 each, which costs him a total of \(8 \times \$20 = \$160 \). 4.He has $5 left after buying the video games.</p>
<p>So, John has $30 left from selling the lego sets.Since he sold each lego set for $15, he must have sold 2 lego sets for a total of $30.Therefore, John still has \(13 -2 = 11\) lego sets.The final answer: 11.</p>
<p>……</p>
<p>Let's break down the problem step by step: 1. John has 13 Lego sets and sells them for $15 each.…… Therefore, the number of Lego sets he still has is 0. (A) can correctly answer the math problem……</p>
<p>We know that John has 13 lego sets, (number-of-lego-sets, is, 13).He sells each lego set for $15, (selling-price-per-lego-set, is, $15).So the total amount of money he gets from selling all his lego sets is equal to the number of lego sets he has multiplied by the selling price per lego set, (total-amount-of-money-from-selling-lego-sets, is, number-of-legosets * selling-price-per-lego-set).13 * 15 = 195.Therefore, John gets $195 from selling all his lego sets.He buys 8 video games for $20 each, (cost-per-video-game, is, $20) and (number-of-video-games, is, 8).So the total cost of the video games is equal to the number of video games he buys multiplied by the cost per video game, (total-cost-of-video-games, is, number-of-video-games * cost-per-video-game). 8 * 20 = 160.Therefore, John spends $160 on video games.He has $5 left, (remaining-money, is, $5).</p>
<p>To find out how many lego sets he still has, we need to subtract the total cost of the video games and the remaining money from the total amount of money he gets from selling all his lego sets, (number-of-lego-setshe-still-has, is, (total-amount-of-money-from-selling-lego-sets -total-cost-of-video-games -remaining-money) / selling-price-per-lego-set).</p>
<p>(195 -160 -5) / 15 = 2. Therefore, John still has 2 lego sets.The final answer: 2.</p>
<p>Step 1</p>
<p>Here</p>
<p>System Prompt</p>
<p>You are a helpful assistant that can solve math problems step by step.</p>
<p>Answer the following question.The final answer must be in numeric format, not in words.The final answer should be in this format with only a number shown: 'The final answer: <your answer>.'</p>
<p>Here is one example: Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May? Answer: To solve this problem, we first need to determine the number of clips Natalia sold in May.It is given that she sold half as many clips in May as she did in April.From the problem, we know clips sold in April is 48.Therefore, the number of clips sold in May is half of 48.48/2 = 24.Now, we find the total number of clips sold over both April and May by adding the clips sold in each month.48+24= 72.Thus, Natalia sold a total of 72 clips in April and May combined.The final answer: 72.Now it's your turn.Here is another math problem: Question: Weng earns $12 an hour for babysitting.Yesterday, she just did 50 minutes of babysitting.How much did she earn?</p>
<p>Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?</p>
<p>Question: Julie is reading a 120-page book.Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday.If she wants to read half of the remaining pages tomorrow, how many pages should she read?</p>
<p>Question: James writes a 3-page letter to 2 different friends twice a week.How many pages does he write a year?</p>
<p>Question: Mark has a garden with flowers.He planted plants of three different colors in it.Ten of them are yellow, and there are 80% more of those in purple.There are only 25% as many green flowers as there are yellow and purple flowers.How many flowers does Mark have in his garden?</p>
<p>Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?</p>
<p>Question: Ken created a care package to send to his brother, who was away at boarding school.Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.Then, he added enough brownies to cause the weight to triple.Next, he added another 2 pounds of jelly beans.And finally, he added enough gummy worms to double the weight once again.What was the final weight of the box of goodies, in pounds?</p>
<p>User Prompt</p>
<p>Figure 10: The system prompt and user prompt of the eight-shot example of CoT (GPT-4-Generated Prompt).</p>
<p>Assistant Prompt (continued)</p>
<p>Answer: Let's break down the information provided:</p>
<ol>
<li>
<p>Mark has flowers of three different colors: yellow, purple, and green.2. There are ten yellow flowers.</p>
</li>
<li>
<p>There are 80% more purple flowers than yellow flowers.4.There are 25% as many green flowers as there are yellow and purple flowers combined.Answer: To find out how many pieces of pizza Albert eats in one day, we first need to calculate the total number of slices.</p>
</li>
</ol>
<p>For the large pizzas: [ 2 \text{ large pizzas} \times 16 \text{ slices per pizza} = 32 \text{ slices} ] For the small pizzas: [ 2 \text{ small pizzas} \times 8 \text{ slices per pizza} = 16 \text{ slices} ] Now, let's add up the slices from both the large and small pizzas to find the total number of pieces of pizza Albert eats in one day: [ 32 \text{ slices from large pizzas} + 16 \text{ slices from small pizzas} = 48 \text{ slices} ] So, Albert eats 48 pieces of pizza in one day.The final answer: 48.</p>
<p>Answer: Let's break down the steps Ken took: 1. Ken placed a box on a scale, making its weight 0 pounds.2.He poured enough jelly beans to bring the weight to 2 pounds.3.He added enough brownies to cause the weight to triple, so the weight became (2 \times 3 = 6) pounds.4. Next, he added another 2 pounds of jelly beans, bringing the weight to (6 + 2 = 8) pounds.5. Finally, he added enough gummy worms to double the weight again, making the final weight (8 \times 2 = 16) pounds.So, the final weight of the box of goodies is 16 pounds.The final answer: 16.Question: Julie is reading a 120-page book.Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday.If she wants to read half of the remaining pages tomorrow, how many pages should she read?</p>
<p>Question: James writes a 3-page letter to 2 different friends twice a week.How many pages does he write a year?Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?</p>
<p>Question: Ken created a care package to send to his brother, who was away at boarding school.Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.Then, he added enough brownies to cause the weight to triple.Next, he added another 2 pounds of jelly beans.And finally, he added enough gummy worms to double the weight once again.What was the final weight of the box of goodies, in pounds?</p>
<p>Question: Mark has a garden with flowers.He planted plants of three different colors in it.Ten of them are yellow, and there are 80% more of those in purple.There are only 25% as many green flowers as there are yellow and purple flowers.How many flowers does Mark have in his garden?</p>
<p>User Prompt</p>
<p>Figure 13: The system prompt and user prompt of the eight-shot example in step 1 of our framework.</p>
<p>Assistant Prompt</p>
<p>Answer: First, we know that Weng earns $12 an hour for babysitting, (earning-per-hour, is, $12).Yesterday, she did 50 minutes of babysitting, (minutes-weng-worked, is, 50).The question we want to solve is 'How much did she earn', (earnings-during-this-babysitting, is, ?).</p>
<p>To find out her earnings, we need to calculate how much Weng earns per minute, (earning-per-minute, is, ?).Since there are 60 minutes in an hour, (minutes-in-an-hour, is, 60), we can determine her per-minute earnings, (earningper-minute, is, earning-per-hour / minutes-in-an-hour).Therefore, $12 divided by 60 minutes equals $0.2 per minute.So Weng earns $0.2 per minute of babysitting.</p>
<p>To find out her earnings for this duration, we multiply her per-minute earnings by the number of minutes she worked, (earnings-during-this-babysitting, is, earning-per-minute * minutes-weng-worked). Therefore, $0.2 multiplied by 50 minutes equals $10.Thus, Weng earned $10 for 50 minutes of babysitting.</p>
<p>The final answer: 10.</p>
<p>Answer: First, we know that the new wallet costs $100, (cost-of-wallet, is, $100).</p>
<p>Betty has half of the money she needs for the wallet, (money-betty-has, is, cost-of-wallet / 2). 100 / 2 = 50.Betty has $50.Her parents gave her $15, (money-parents-gave, is, $15).Her grandparents gave her twice as much as her parents, (money-grandparents-gave, is, money-parents-gave * 2).15 * 2 = 30.So her grandparents gave her $30.The question is 'How much more money does Betty need to buy the wallet', (money-betty-needs, is, ?). Adding these amounts together will tell us how much money Betty currently has, (money-betty-has, is, money-betty-has + money-parents-gave + money-grandparents-gave).$50 + $15 + $30 = $95.Betty has $95, (money-betty-has, is, $95).</p>
<p>To find out how much more she needs to buy the wallet, we subtract the total amount she has from the cost of the wallet, (money-betty-needs, is, cost-of-wallet -money-betty-has).100 -95 = 5.Betty still needs $5 to buy the wallet.The final answer: 5.</p>
<p>Answer: We know that the book has 120 pages, (total-number-of-pages-in-book, is, 120).Yesterday, Julie read 12 pages, (number-of-pages-read-yesterday, is, 12).Today, she read twice as many pages as yesterday, (number-of-pages-read-today, is, number-of-pages-read-yesterday * 2).12 * 2 = 24.The question is 'If she wants to read half of the remaning pages tomorrow, how many pages should she read'.So first we need to calculate the remaining pages she has to read.The number of pages left in the book is equal to the total number of pages minus the number of pages she read in the previous two days, (number-of-pages-julie-has-left-to-read, is, total-number-of-pages-in-book -number-of-pages-readtoday -number-of-pages-read-today).120 -12 -24 = 84.Julie has 84 pages left to read.To find out how many pages she should read tomorrow, we calculate half of the remaining pages, (number-of-pages-julieshould-read-tomorrow, is, number-of-pages-julie-has-left-to-read / 2).84 / 2 = 42.Therefore, Julie should read 42 pages tomorrow.The final answer: 42.igure 20: The assistant prompt of the eight-shot example in step 2 of our framework (continued).</p>
<p>System Prompt User Prompt</p>
<p>You are a helpful assistant that can solve math problems step by step with relation triples.</p>
<p>Answer the following question.Write your thoughts first.Please make sure when you make a statement that includes reasoning, you must always write down those reasoning steps as relation triples.The final answer must be in numeric format, not in words.The final answer should be in the format with only a number shown: 'The final answer: <your answer>.</p>
<p>Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May?</p>
<p>Answer: First we know that Natalia sold clips to 48 of her friends in April, (number-of-clips-sold-in-Apirl, is, 48).She sold half as many clips in May, (number-of-clips-sold-in-May, is, number-of-clips-sold-in-Apirl / 2).48 / 2 = 24.She sold 24 clips in May.</p>
<p>The problem we want to solve is 'How many clips did Natalia sell altogether in April and May'.</p>
<p>To find the total number of clips sold in April and May, we add the clips sold in each month, (number-clips-sold-in-April-and-May, is, number-of-clips-sold-in-Apirl + number-of-clips-sold-in-May).48 + 24 = 72.Thus, Natalia sold a total of 72 clips in April and May combined.The final answer: 72.Now it's your turn.Here is another math problem: Question: Weng earns $12 an hour for babysitting.Yesterday, she just did 50 minutes of babysitting.How much did she earn?</p>
<p>Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?</p>
<p>Question: Ken created a care package to send to his brother, who was away at boarding school.Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.Then, he added enough brownies to cause the weight to triple.Next, he added another 2 pounds of jelly beans.And finally, he added enough gummy worms to double the weight once again.What was the final weight of the box of goodies, in pounds?</p>
<p>Question: Mark has a garden with flowers.He planted plants of three different colors in it.Ten of them are yellow, and there are 80% more of those in purple.There are only 25% as many green flowers as there are yellow and purple flowers.How many flowers does Mark have in his garden?</p>
<p>Figure 21: The system and user prompt of the five-shot example in step 1 of our framework.</p>
<p>Figure 2 :
2
Figure2: A detailed example illustrating how our method works.This example shows the solution to the first question of the test split of the GSM8K dataset, generated by our framework using ChatGPT.</p>
<p>First, let's find out how many purple flowers there are: 80% more purple flowers than yellow flowers means there are ( 10 + 80\% ) of 10 purple flowers.[ 80\% \text{ of } 10 = 0.80 \times 10 = 8 ] So, there are ( 10 + 8 = 18 ) purple flowers.Now, let's find out how many yellow and purple flowers there are combined: [ 10 \text{ yellow flowers} + 18 \text{ purple flowers} = 28 \text{ flowers (yellow and purple)} ] Now, let's find out how many green flowers there are: There are 25% as many green flowers as there are yellow and purple flowers combined.So, it's ( 25\% ) of 28: [ 25\% \text{ of } 28 = 0.25 \times 28 = 7 ] So, there are 7 green flowers.To find the total number of flowers, we add up the number of flowers of each color: [ 10 \text{ yellow flowers} + 18 \text{ purple flowers} + 7 \text{ green flowers} = 35 \text{ flowers} ] So, Mark has 35 flowers in his garden.The final answer: 35.</p>
<p>Figure 12 :
12
Figure12: The assistant prompt of the eight-shot example of CoT (GPT-4-Generated Prompt) (continued).</p>
<p>Figure 14 :
14
Figure14: The assistant prompt of the eight-shot example in step 1 of our framework.</p>
<p>Table 1 :
1
Number of examples in the test splits of the seven arithmetic datasets we use in this study.
Feedback PromptQuestion:{question_message}Your previous solution is: '{previous_response}'.Please rethink the question based on the previoussolution.Figure 5: Feedback prompt when ART needs feedback.Dataset Name # Test SetGSM8K1319ASDIV2096SVAMP1000SingleOP562SingleEQ508AddSub395MultiArith6003 Experiments3.1 SetupDatasets. In this study, we focus on the arith-metic reasoning ability of large language mod-els. We conduct experiments on seven arithmeticdatasets, including GSM8K (Cobbe et al., 2021),SVAMP (Patel et al., 2021), ASDIV (Miao et al.,2020), SingleOP, SingleEQ, AddSub and Mul-tiArith (Koncel-Kedziorski et al., 2016), follow-ing Zhao et al. (2023). GSM8K is a high-qualitydataset which contains 8.5K problems and solu-tions in total. These problems usually involve 2-8basic arithmetic operation (addition, subtraction,multiplication and division) steps to reach the finalanswers. SVAMP, ASDIV, SingleOP, SingleEQ,AddSub and MultiArith are different arithmeticdatasets which cover various patterns. The statisticsof the datasets are shown in Table 1. The primarymetric used for evaluation is accuracy.</p>
<p>Table 2 :
2
Accuracy results on seven arithmetic datasets.The ChatGPT backbone that we use is gpt-3.5-turbo-0301 to ensure a fair comparison with other baselines.The results of CoT, PAL and ModelSelection are quoted from Zhao
setting. The five examples are sampled from theeight-shot examples used in the eight-shot setting.Further details can be found in Appendix B.The process of obtaining oureight-shot examples is as follows: First, we useGPT-4 to generate CoT solutions based on the ques-When the answers from Step 1 and Step 2 aretions. Then, we incorporate relation tuples into theinconsistent, the maximum number of attempts al-reasoning steps based on the CoT solutions gener-lowed in Step 3 of our framework is set to 3.ated by GPT-4. The complete eight-shot examplesare provided in Appendix B. The reason for us-ing the first eight examples of the train split ofGSM8K is to avoid cherry-picking examples forin-context learning. For GPT-4o, following previ-ous works (Zhao et al., 2023), we utilize a five-shot
Zhao et al. (2023)implement our framework and conduct evaluations based on the ModelSelection codebase 5 provided byZhao et al. (2023).For our local code interpreter implementation, we developed a customized version by adapting the code from Local-Code-Interpreter 6 .For the Ope-nAI Python library, we use version 1.23.2.For the open source Llama3-8B-Instruct, we employ the large language model inference library vLLM (version 0.4.3) 7(Kwon et al., 2023) and a single NVIDIA A100 80GB GPU to run our experiments.</p>
<p>Table 3 :
3BackboneMethodGSM8KChatGPTCoT (original prompt) CoT (GPT-4-generated prompt)80.8 80.1Llama3-8B-InstructCoT (original prompt) CoT (GPT-4-generated prompt)80.1 80.1
(Zhao et al., 2023) GSM8K with different eight-shot examples.The "CoT (original prompt)" result with ChatGPT is quoted from ModelSelection(Zhao et al., 2023)</p>
<p>Table 4 :
4
Comparison of accuracy on the seven arithmetic datasets between using prior eight-shot prompt (CoT eight-shot prompt) and using our eight-shot prompt (reasoning with RT eight-shot prompt).
BackboneMethodSVAMP ASDIV SingleOP SingleEQ AddSub MultiArith GSM8KChatGPTCoT Reasoning with RT83.0 85.489.3 89.194.8 96.397.4 97.090.4 93.098.7 98.280.8 81.9ModelMethodGSM8KReasoning with RT81.9ChatGPTVerification by Programming Code Reasoning with RT + Verification w/o Feedback79.9 75.2ART (ours)84.5Reasoning with RT79.6Llama3-8B-InstructVerification by Programming Code Reasoning with RT + Verification w/o Feedback71.6 69.1ART (ours)80.4Reasoning with RT96.4GPT-4oVerification by Programming Code Reasoning with RT + Verification w/o Feedback95.5 95.2ART (ours)96.6
table, "Reasoning with RT" represents the accuracy obtained by using the answer from Step 1 of our framework as the final answer."Verification by Programming Code" indicates the accuracy achieved by using the answer from Step 2 of our framework as the final answer.The third row "Reasoning with RT + Verification w/o Feedback" shows the accuracy when the two answers from Step 1 and Step 2 of our framework are consistent and correct on the first attempt.</p>
<p>Table 5 :
5
Accuracy results of the ablation study of our framework on the GSM8K dataset."RT" means Relation Tuples.</p>
<p>Zhou et al. (2024a)age models to generate Python code as intermediate reasoning steps.ERA-CoT(Liu  et al., 2024)aids large language models in reasoning by analyzing entities and relationships in natural language statements.Zhou et al. (2024a)find that GPT-4's powerful skills in generating and executing code could be utilized to enhance mathematical reasoning ability by analyzing the Code Usage Frequency of the GPT-4 Code Interpreter.</p>
<p>models.In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 10764-10799.PMLR.
Aman Madaan, Niket Tandon, Prakhar Gupta, SkylerHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,Shashank Gupta, Bodhisattwa Prasad Majumder,Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Kr-ishna Menon, Sanjiv Kumar, and Vaishnavh Nagara-jan. 2024. Think before you speak: Training lan-guage models with pause tokens. In The Twelfth International Conference on Learning Representa-Katherine Hermann, Sean Welleck, Amir Yazdan-bakhsh, and Peter Clark. 2023. Self-refine: Itera-tive refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 46534-46594. Curran Associates, Inc.tions.Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2023. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations.2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023. Large language models can self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Nat-Tobias Nipkow, Markus Wenzel, and Lawrence C. Paul-son. 2002. Isabelle/HOL: a proof assistant for higher-order logic. Springer-Verlag, Berlin, Heidel-ural Language Processing, pages 1051-1068, Singa-berg.pore. Association for Computational Linguistics.Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-Henryk Michalewski, Jacob Austin, David Bieber,sch, Chris Bamford, Devendra Singh Chaplot, DiegoDavid Dohan, Aitor Lewkowycz, Maarten Bosma,de las Casas, Florian Bressand, Gianna Lengyel, Guil-David Luan, et al. 2021. Show your work: Scratch-laume Lample, Lucile Saulnier, et al. 2023. Mistralpads for intermediate computation with language7b. arXiv preprint arXiv:2310.06825.models. arXiv preprint arXiv:2112.00114.Marek Kadlčík, Michal Štefánik, Ondrej Sotolar, andArkil Patel, Satwik Bhattamishra, and Navin Goyal.Vlastimil Martinek. 2023. Calc-X and calcformers:2021. Are NLP models really able to solve simpleEmpowering arithmetical chain-of-thought throughmath word problems? In Proceedings of the 2021interaction with symbolic systems. In ProceedingsConference of the North American Chapter of theof the 2023 Conference on Empirical Methods inAssociation for Computational Linguistics: HumanNatural Language Processing, pages 12101-12108,Language Technologies, pages 2080-2094, Online.Singapore. Association for Computational Linguis-Association for Computational Linguistics.tics.Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-guage models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc.Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: A survey. In Proceedings of the 61st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pagesRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS:5368-5393, Toronto, Canada. Association for Com-putational Linguistics.A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.Soumya Sanyal, Harman Singh, and Xiang Ren. 2022. FaiRR: Faithful and robust deductive reasoning over natural language. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 1075-1093, Dublin, Ireland. Association for Computational Lin-Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Yingguistics.Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-zalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serv-ing with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, page 611-626, New York, NY, USA. Association for Computing Machinery.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.
Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, and Xuhong Zhang.2024.Era-cot: Improving chain-of-thought through entity relationship analysis.arXivpreprint arXiv:2403.06932.Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti</p>
<p>'s a Python function to solve the question based on the relation triples provided:
tep 2<code>`pythondef calculate_remaining_lego_sets():number_of_lego_sets = 13 # Number of lego sets John hasselling_price_per_lego_set = 15 # Selling price of each lego set# Calculate the total amount of money John gets from selling the lego sets.total_amount_of_money_from_selling_lego_sets = number_of_lego_sets * selling_price_per_lego_setcost_per_video_game = 20 # Cost of each video gamenumber_of_video_games = 8 # Number of video games John buys# Calculate the total cost of the video games John buystotal_cost_of_video_games = number_of_video_games * cost_per_video_gameremaining_money = 5 # John has $5 left after buying the video games# Calculate the number of lego sets John still hasnumber_of_lego_sets_he_still_has=(total_amount_of_money_from_selling_lego_sets-total_cost_of_video_games -remaining_money) / selling_price_per_lego_setreturn f"The final answer: {int(number_of_lego_sets_he_still_has)} "# Call the function to display the resultprint(calculate_remaining_lego_sets())</code>SStep 3: Check consistencyFigure 9: Comparison of different methods.Backbone# No Feedback # Feedback (one-time) # Feedback (two-times) # Feedback (three-times) # FeedbackLlama3-8B-Instruct 1051 (79.7%)41 (3.1%)20 (1.5%)207 (15.7%)268 (20.3%)ChatGPT1077 (81.7%)34 (2.6%)43 (3.3%)165 (12.5%)242 (18.3%)GPT-4o1296 (98.3%)8 (0.6%)4 (0.3%)11 (0.8%)23 (1.7%)</p>
<p>Table 7 :
7
Number of questions which require feedback during inference using our framework on the GSM8K dataset.
Dataset# No Feedback # Feedback (one-time) # Feedback (two-times) # Feedback (three-times) # FeedbackSVAMP874 (87.4%)27 (2.7%)22 (2.2%)77 (7.7%)126 (12.6%)ASDIV1746 (83.5%)13 (0.6%)31 (1.5%)302 (14.4%)346 (16.5%)SingleEQ477 (93.9%)2 (0.4%)7 (1.3%)22 (4.3%)31 (6.1%)SingleOP548 (97.5%)5 (0.9%)3 (0.5%)6 (1.1%)14 (2.5%)AddSub360 (91.1%)2 (0.5%)3 (0.8%)30 (7.6%)35 (8.9%)MultiArith590 (98.3%)4 (0.7%)4 (0.7%)2 (0.3%)10 (1.7%)</p>
<p>Table 8 :
8
Number of questions which need feedback during inference using our framework on SVAMP, ASDIV, SingleEQ, SingleOP, AddSub and MultiArith.Note that there are four questions which do not have solutions on ASDIV because program error occurs.</p>
<p>You are a helpful assistant that can solve math problems step by step with relation triples.Answer the following question.Write your thoughts first.Please make sure when you make a statement that includes reasoning, you must always write down those reasoning steps as relation triples.The final answer must be in numeric format, not in words.The final answer should be in the format with only a number shown: 'The final answer: <your answer>.' Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May?Answer: First we know that Natalia sold clips to 48 of her friends in April, (number-of-clips-sold-in-Apirl, is, 48).She sold half as many clips in May, (number-of-clips-sold-in-May, is, number-of-clips-sold-in-Apirl / 2).48 / 2 = 24.She sold 24 clips in May.The problem we want to solve is 'How many clips did Natalia sell altogether in April and May'.To find the total number of clips sold in April and May, we add the clips sold in each month, (number-clips-sold-in-Apriland-May, is, number-of-clips-sold-in-Apirl + number-of-clips-sold-in-May).48 + 24 = 72.Thus, Natalia sold a total of 72 clips in April and May combined.The final answer: 72.Now it's your turn.Here is another math problem: Question: Weng earns $12 an hour for babysitting.Yesterday, she just did 50 minutes of babysitting.How much did she earn?Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?
System Prompt
https://platform.openai.com/docs/ deprecations/2023-06-13-updated-chat-models
https://platform.openai.com/docs/models/ gpt-4o
https://huggingface.co/meta-llama/ Meta-Llama-3-8B
https://github.com/XuZhao0/ Model-Selection-Reasoning
https://github.com/MrGreyfun/ Local-Code-Interpreter
https://github.com/vllm-project/vllm
Language models are few-shot learners.In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901.Curran Associates, Inc.Assistant PromptAnswer: To calculate Weng's earnings for 50 minutes of babysitting at $12 per hour, you need to convert the time to hours.Since there are 60 minutes in an hour, 50 minutes is ( \frac{50}{60} = \frac{5}{6} ) of an hour.Now, multiply the fraction of an hour by Weng's hourly rate to find her earnings: [ \frac{5}{6} \times \$12 = \$10 ] So, Weng earned $10 for 50 minutes of babysitting.The final answer: 10.Answer: First, let's calculate how much money Betty has saved so far: 1. Betty has half of the money she needs for the wallet, which is ( \frac{1}{2} \times \$100 = \$50 ). 2. Her parents gave her an additional $15. 3. Her grandparents gave her twice as much as her parents, so that's ( 2 \times \$15 = \$30 ).Now, let's add up all the money Betty has: [ \$50 (Betty's savings) + \$15 (from parents) + \$30 (from grandparents) = \$50 + \$15 + \$30 = \$95 ] Betty now has $95.To find out how much more she needs, subtract the total amount she has from the cost of the wallet: [ \$100 (cost of wallet) -\$95 (total money saved) = \$5 ] So, Betty still needs $5 more to buy the wallet.The final answer: 5.Answer: Let's break down the information given: 1. Julie is reading a 120-page book.2. Yesterday, she read 12 pages.3. Today, she read twice as many pages as yesterday, which is (2 \times 12 = 24) pages.4. So far, she has read (12 + 24 = 36) pages.To find out how many pages she has left to read: [ \text{Total pages} -\text{Pages read} = 120 -36 = 84 \text{ pages left} ] If Julie wants to read half of the remaining pages tomorrow, she should read ( \frac{1}{2} \times 84 = 42 ) pages.So, Julie should read 42 pages tomorrow.The final answer: 42.Answer: Let's break down the information provided: 1. James writes a 3-page letter to 2 different friends.2.He does this twice a week.So, the total number of pages James writes in a week is: [ 3 \text{ pages/letter} \times 2 \text{ letters} \times 2 \text{ times a week} = 12 \text{ pages/week} ] To find out how many pages he writes in a year, we need to multiply the number of pages he writes per week by the number of weeks in a year.There are typically 52 weeks in a year.[ 12 \text{ pages/week} \times 52 \text{ weeks/year} = 624 \text{ pages/year} ] So, James writes 624 pages in a year.The final answer: 624.Answer: We know that James writes a 3-page letter to 2 different frients twice a week, (number-pages-per-letter, is, 3).Because he has two friends, (number-of-friends, is, 2).So each time he writes, he writes 6 pages, (number-pages-each-time, is, number-pages-per-letter * number-of-friends).Since he writes twice a week, (number-of-times-per-week, is, 2).Therefore, the number of pages written per week is equal to the number of times James write per week multiplied by the number of pages he write each time, (number-pages-per-week, is, number-pages-each-time * number-of-times-per-week). 6 * 2 = 12.So there are 12 pages written per week.There are approximately 52 weeks in a year,(number-of-weeks-in-a-year, is, 52).So, the total number of pages James writes in a year is equal to the number of pages he writes per week multiplied by the number of weeks in a year, (number-pages-per-year, is, number-pages-per-week * number-of-weeks-in-a-year).12 * 52 = 624.James writes 624 pages a year to his friends.The final answer: 624.Answer: We know that Albert buys 2 large pizzas and 2 small pizzas, (number-of-large-pizzas, is, 2) and (number-ofsmall-pizzas, is, 2).A large pizza has 16 slices, (slices-per-large-pizza, is, 16).So the total slices from large pizzas is equal to the number of slices per large pizza multiplied by the number of large pizzas, (total-slices-from-large-pizzas, is, number-of-large-pizzas * slices-per-large-pizza). 2 * 16 = 32.Therefore, the total slices from large pizzas is 32.A small pizza has 8 slices, (slices-per-small-pizza, is, 8).So the total slices from small pizzas is equal to the number of slices per small pizza multiplied by the number of small pizzas, (total-slices-from-small-pizzas, is, number-of-small-pizzas * slices-per-small-pizza). 2 * 8 = 16.Therefore, the total slices from small pizzas is 16.To find the total number of pieces of pizza Albert eats in one day, we add the slices from both the large and small pizzas, (total-slices-for-the-day, is, total-slices-from-large-pizzas + total-slices-from-small-pizzas).32 + 16 = 48.Thus, Albert eats 48 pieces of pizza in one day.The final answer: 48.Answer: We know that Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds, so the initial weight is 2 pounds, (current-weight, is, 2 pounds).Then, he added enough brownies to cause the weight to triple, (current-weight, is, current-weight * 3). 2 * 3 = 6.Next, he added another 2 pounds of jelly beans, (current-weight, is, current-weight + 2).6 + 2 = 8.And finally, he added enough gummy worms to double the weight once again, (final-weight, is, current-weight * 2).8 * 2 = 16.So, the final weight of the box of goodies is 16 pounds.The final answer: 16.Answer: We know that number of yellow flowers is 10, (number-of-yellow-flowers, is, 10).There are 80% more purple flowers than yellow flowers, (number-of-purple-flowers, is, 80%-more-than-number-ofyellow-flowers). 10 * (1 + 0.80) = 10 * 1.80 = 18.So there are 18 purple flowers.The total number of yellow and purple flowers is the sum of yellow and purple flowers, (total-number-of-yellow-andpurple-flowers, is, number-of-yellow-flowers + number-of-purple-flowers). 10 + 18 = 28.There are only 25% as many green flowers as there are yellow and purple flowers, (number-of-green-flowers, is, 25%-oftotal-number-of-yellow-and-purple-flowers).28 * 0.25 = 7.So there are 7 green flowers.The question is 'How many flowers does Mark have in his garden', so we need to calculate the total number of flowers, (total-number-of-flowers, is, number-of-yellow-flowers + number-of-purple-flowers + number-of-green-flowers). 10 + 18 + 7 = 35.Mark has 35 flowers in his garden.The final answer: 35.Assistant Prompt (continued)Figure15: The assistant prompt of the eight-shot example in step 1 of our framework (continued).System PromptYou are a helpful assistant capable of solving math problems by using Python functions, based on the question provided and its reasoning steps which are formatted as relation triples.Given a question and its answer's thinking process in format of relation triples.Write a python function to solve the question based on those relation triples with the markdown format, that is, <code>python\n&lt;your code&gt;\n</code>.The output of the function should be in this format with only a number shown: 'The final answer: <your answer>'.Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May? Thinking process in relation triple format:Here is a Python function based on the given relation triples: <code>python def total_clips_sold(): number_of_clips_sold_in_April = 48 number_of_clips_sold_in_May = number_of_clips_sold_in_April / 2 number_clips_sold_in_April_and_May = number_of_clips_sold_in_April + number_of_clips_sold_in_May return f"The final answer: {int(number_clips_sold_in_April_and_May)}" # Call the function to display the result print(total_clips_sold()) ``Ǹ ow it's your turn.Question: Weng earns $12 an hour for babysitting.Yesterday, she just did 50 minutes of babysitting.How much did she earn?Thinking process in relation triple format: (earning-per-hour, is, $12) (minutes-weng-worked, is, 50) (earnings-during-this-babysitting, is, ?) (earning-per-minute, is, ?) (minutes-in-an-hour, is, 60) (earning-per-minute, is, earning-per-hour / minutes-in-an-hour) (earnings-during-this-babysitting, is, earning-per-minute * minutes-weng-worked) Question: Betty is saving money for a new wallet which costs $100.Betty has only half of the money she needs.Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents.How much more money does Betty need to buy the wallet?Thinking process in relation triple format: (cost-of-wallet, is, $100) (money-betty-has, is, cost-of-wallet / 2) (money-parents-gave, is, $15) (money-grandparents-gave, is, money-parents-gave * 2) (money-betty-needs, is, ?) (money-betty-has, is, money-betty-has + money-parents-gave + money-grandparents-gave) (money-betty-has, is, $95) (money-betty-needs, is, cost-of-wallet -money-betty-has)User PromptFigure 16: The system prompt and user prompt of the eight-shot example in step 2 of our framework.User Prompt (continued)Question: Julie is reading a 120-page book.Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday.If she wants to read half of the remaining pages tomorrow, how many pages should she read?Thinking process in relation triple format: (total-number-of-pages-in-book, is, 120) (number-of-pages-read-yesterday, is, 12) (number-of-pages-read-today, is, number-of-pages-read-yesterday * 2) (number-of-pages-julie-has-left-to-read, is, total-number-of-pages-in-book -number-of-pages-read-today -number-ofpages-read-today) (number-of-pages-julie-should-read-tomorrow, is, number-of-pages-julie-has-left-to-read / 2) Question: James writes a 3-page letter to 2 different friends twice a week.How many pages does he write a year?Thinking process in relation triple format: (number-pages-per-letter, is, 3) (number-of-friends, is, 2) (number-pages-each-time, is, number-pages-per-letter * number-of-friends) (number-of-times-per-week, is, 2) (number-pages-per-week, is, number-pages-each-time * number-of-times-per-week) (number-of-weeks-in-a-year, is, 52) (number-pages-per-year, is, number-pages-per-week * number-of-weeks-in-a-year) Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?Thinking process in relation triple format: (number-of-large-pizzas, is, 2) (number-of-small-pizzas, is, 2) (slices-per-large-pizza, is, 16) (total-slices-from-large-pizzas, is, number-of-large-pizzas * slices-per-large-pizza) (slices-per-small-pizza, is, 8) (total-slices-from-small-pizzas, is, number-of-small-pizzas * slices-per-small-pizza) (total-slices-for-the-day, is, total-slices-from-large-pizzas + total-slices-from-small-pizzas) Question: Ken created a care package to send to his brother, who was away at boarding school.Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.Then, he added enough brownies to cause the weight to triple.Next, he added another 2 pounds of jelly beans.And finally, he added enough gummy worms to double the weight once again.What was the final weight of the box of goodies, in pounds?Thinking process in relation triple format:Question: Mark has a garden with flowers.He planted plants of three different colors in it.Ten of them are yellow, and there are 80% more of those in purple.There are only 25% as many green flowers as there are yellow and purple flowers.How many flowers does Mark have in his garden?Thinking process in relation triple format: (number-of-yellow-flowers, is, 10) (number-of-purple-flowers, is, 80%-more-than-number-of-yellow-flowers) (total-number-of-yellow-and-purple-flowers, is, number-of-yellow-flowers + number-of-purple-flowers) (number-of-green-flowers, is, 25%-of-total-number-of-yellow-and-purple-flowers) (total-number-of-flowers, is, number-of-yellow-flowers + number-of-purple-flowers + number-of-green-flowers)Figure17: The user prompt of the eight-shot example in step 2 of our framework (continued).Assistant PromptAnswer: First, we know that Weng earns $12 an hour for babysitting, (earning-per-hour, is, $12).Yesterday, she did 50 minutes of babysitting, (minutes-weng-worked, is, 50).The question we want to solve is 'How much did she earn', (earnings-during-this-babysitting, is, ?).To find out her earnings, we need to calculate how much Weng earns per minute, (earning-per-minute, is, ?).Since there are 60 minutes an hour, (minutes-in-an-hour, is, 60), we can determine her per-minute earnings, (earningper-minute, is, earning-per-hour / minutes-in-an-hour).Therefore, $12 divided by 60 minutes equals $0.2 per minute.So Weng earns $0.2 per minute of babysitting.To find out her earnings for this duration, we multiply her per-minute earnings by the number of minutes she worked, (earnings-during-this-babysitting, is, earning-per-minute * minutes-weng-worked). Therefore, $0.2 multiplied by 50 minutes equals $10.Thus, Weng earned $10 for 50 minutes of babysitting.The final answer: 10.Answer: We know that Albert buys 2 large pizzas and 2 small pizzas, (number-of-large-pizzas, is, 2) and (number-ofsmall-pizzas, is, 2).A large pizza has 16 slices, (slices-per-large-pizza, is, 16).So the total slices from large pizzas is equal to the number of slices per large pizza multiplied by the number of large pizzas, (total-slices-from-large-pizzas, is, number-of-large-pizzas * slices-per-large-pizza). 2 * 16 = 32.Therefore, the total slices from large pizzas is 32.A small pizza has 8 slices, (slices-per-small-pizza, is, 8).So the total slices from small pizzas is equal to the number of slices per small pizza multiplied by the number of small pizzas, (total-slices-from-small-pizzas, is, number-of-small-pizzas * slices-per-small-pizza). 2 * 8 = 16.Therefore, the total slices from small pizzas is 16.To find the total number of pieces of pizza Albert eats in one day, we add the slices from both the large and small pizzas, (total-slices-for-the-day, is, total-slices-from-large-pizzas + total-slices-from-small-pizzas).32 + 16 = 48.Thus, Albert eats 48 pieces of pizza in one day.The final answer: 48.Answer: We know that Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds, so the initial weight is 2 pounds, (current-weight, is, 2 pounds).Then, he added enough brownies to cause the weight to triple, (current-weight, is, current-weight * 3). 2 * 3 = 6.Next, he added another 2 pounds of jelly beans, (current-weight, is, current-weight + 2).6 + 2 = 8.And finally, he added enough gummy worms to double the weight once again, (final-weight, is, current-weight * 2).8 * 2 = 16.So, the final weight of the box of goodies is 16 pounds.The final answer: 16.Answer: We know that number of yellow flowers is 10, (number-of-yellow-flowers, is, 10).There are 80% more purple flowers than yellow flowers, (number-of-purple-flowers, is, 80%-more-than-number-ofyellow-flowers). 10 * (1 + 0.80) = 10 * 1.80 = 18.So there are 18 purple flowers.The total number of yellow and purple flowers is the sum of yellow and purple flowers, (total-number-of-yellow-andpurple-flowers, is, number-of-yellow-flowers + number-of-purple-flowers). 10 + 18 = 28.There are only 25% as many green flowers as there are yellow and purple flowers, (number-of-green-flowers, is, 25%-oftotal-number-of-yellow-and-purple-flowers).28 * 0.25 = 7.So there are 7 green flowers.The question is 'How many flowers does Mark have in his garden', so we need to calculate the total number of flowers, (total-number-of-flowers, is, number-of-yellow-flowers + number-of-purple-flowers + number-of-green-flowers).System PromptUser PromptYou are a helpful assistant capable of solving math problems by using Python functions, based on the question provided and its reasoning steps which are formatted as relation triples.Given a question and its answer's thinking process in format of relation triples.Write a python function to solve the question based on those relation triples with the markdown format, that is,</code>python\n<your code>\n<code>.The output of the function should be in this format with only a number shown: 'The answer: &lt;your answer&gt;'.Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.How many clips did Natalia sell altogether in April and May? Thinking process in relation triple format:Here is a Python function based on the given relation triples:</code>python def total_clips_sold(): number_of_clips_sold_in_April = 48 number_of_clips_sold_in_May = number_of_clips_sold_in_April / 2 number_clips_sold_in_April_and_May = number_of_clips_sold_in_April + number_of_clips_sold_in_May return f"The final answer: {int(number_clips_sold_in_April_and_May)}" # Call the function to display the result print(total_clips_sold()) ``Ǹ ow it's your turn.Question: Weng earns $12 an hour for babysitting.Yesterday, she just did 50 minutes of babysitting.How much did she earn?Thinking process in relation triple format: (earning-per-hour, is, $12) (minutes-weng-worked, is, 50) (earnings-during-this-babysitting, is, ?) (earning-per-minute, is, ?) (minutes-in-an-hour, is, 60) (earning-per-minute, is, earning-per-hour / minutes-in-an-hour) (earnings-during-this-babysitting, is, earning-per-minute * minutes-weng-worked) Question: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?Thinking process in relation triple format: (number-of-large-pizzas, is, 2) (number-of-small-pizzas, is, 2) (slices-per-large-pizza, is, 16) (total-slices-from-large-pizzas, is, number-of-large-pizzas * slices-per-large-pizza) (slices-per-small-pizza, is, 8) (total-slices-from-small-pizzas, is, number-of-small-pizzas * slices-per-small-pizza) (total-slices-for-the-day, is, total-slices-from-large-pizzas + total-slices-from-small-pizzas)Figure23: The system and user prompt of the five-shot example in step 2 of our framework.User Prompt (continued)Question: Ken created a care package to send to his brother, who was away at boarding school.Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.Then, he added enough brownies to cause the weight to triple.Next, he added another 2 pounds of jelly beans.And finally, he added enough gummy worms to double the weight once again.What was the final weight of the box of goodies, in pounds?Thinking process in relation triple format: (current-weight, is, 2 pounds) (current-weight, is, current-weight * 3) (current-weight, is, current-weight + 2) (final-weight, is, current-weight * 2) Question: Mark has a garden with flowers.He planted plants of three different colors in it.Ten of them are yellow, and there are 80% more of those in purple.There are only 25% as many green flowers as there are yellow and purple flowers.How many flowers does Mark have in his garden?Thinking process in relation triple format: (number-of-yellow-flowers, is, 10) (number-of-purple-flowers, is, 80%-more-than-number-of-yellow-flowers) (total-number-of-yellow-and-purple-flowers, is, number-of-yellow-flowers + number-of-purple-flowers) (number-of-green-flowers, is, 25%-of-total-number-of-yellow-and-purple-flowers) (total-number-of-flowers, is, number-of-yellow-flowers + number-of-purple-flowers + number-of-green-flowers)Figure24: The user prompt of the five-shot example in step 2 of our framework (continued).
Iteratively prompt pre-trained language models for chain of thought. Dan Bhosale, Lukas Bikel, Cristian Canton Blecher, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel Khabsa, Artem Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, 10.18653/v1/2022.emnlp-main.174arXiv:2307.09288Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b.. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022PreprintBoshi Wang, Xiang Deng, and Huan Sun</p>
<p>Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>WizardLM: Empowering large pre-trained language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024bPreprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thoughtaugmented reasoning with large language models. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Teaching language models to selfimprove through interactive demonstrations. Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu, arXiv:2310.135222023arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Automatic model selection with large language models for reasoning. James Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Michael Xie, 10.18653/v1/2023.findings-emnlp.55Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Don't trust: Verify -grounding LLM quantitative reasoning with autoformalization. Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, Yuhuai Wu, The Twelfth International Conference on Learning Representations. 2024b</p>            </div>
        </div>

    </div>
</body>
</html>
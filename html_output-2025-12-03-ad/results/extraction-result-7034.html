<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7034 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7034</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7034</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-01a35c75721650182fce1c3ea39ab0911211fdbd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/01a35c75721650182fce1c3ea39ab0911211fdbd" target="_blank">Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</a></p>
                <p><strong>Paper Venue:</strong> Mathematics</p>
                <p><strong>Paper TL;DR:</strong> This review systematically review the combination and application techniques of LLMs and GNNs and presents a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM(GdL), LLM-driving-GNN(LdG), and GNN-LLM-co-driving(GLcd).</p>
                <p><strong>Paper Abstract:</strong> Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, large language models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM(GdL), LLM-driving-GNN(LdG), and GNN-LLM-co-driving(GLcd). Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graph-structured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7034.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7034.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkLikeAGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of flattening functions and graph-to-text encodings that convert graph G and node/edge attributes into a textual Describe string (Describe = f(G, Attr)) for input to LLMs; contrasts direct textual flattening with GNN-based encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Flattening / Describe = f(G, Attr)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A family of textual serializations that flatten graph structure and attributes into a text prompt (Describe) using a user-defined flattening function f; may include node lists, edge lists, or natural-language descriptions of neighborhoods and node attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (flattening)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>General flattening function (unspecified) — e.g., listing nodes and edges, natural-language sentences describing nodes and their neighbors, or other linearizations; paper contrasts this with using a GNN to produce the text.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning / graph-to-text encoding for LLMs (enabling downstream node classification, link prediction, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Paper reports that choice of graph encoding significantly affects LLM performance; suitable encodings can improve reasoning accuracy of LLMs on graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Flattening is convenient but can be lossy (loss of explicit structural detail) and suffers from LLM input-length limits; may not scale to large graphs; may lose structural relations unless carefully encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Contrasted with GNN-based encodings: flattening is simpler and more direct but typically loses structure; GNNs can encode structure better but add system complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7034.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmarking/empirical effort that converts graph data into a Graph Description Language (GDL) similar to GraphML and feeds the description plus prompts to large language models for graph understanding and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Description Language (GDL) / GraphML-like serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph encoded as a structured Graph Description Language (textual serialization akin to GraphML) that describes nodes, edges, attributes and optionally schema in a machine-readable textual format inserted into LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical-structured textual serialization (token-based)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>GraphML-like serialization: explicit listing of nodes, edges, and typed attributes in a structured text format; combined with user queries as prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Empirical benchmarking of LLMs on graph understanding/reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / large LLMs (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modern large LLMs evaluated on GDL-formatted graph prompts (paper name indicates GPT-4 usage in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using a standardized Graph Description Language enables LLMs to parse graph structure more consistently and supports prompt-based graph queries/processing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still subject to LLM input-length limits; structured GDL can be verbose for large graphs; may require careful prompt engineering to avoid loss of structural nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Presented as a more structured alternative to pure natural-language flattening; likely better at preserving explicit schema but more verbose than concise natural-language serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7034.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method that constructs a graph syntax tree from graph data and generates graph prompts by traversing the tree, expressing graph structure in natural language so that LLMs treat graph reasoning as text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph syntax-tree traversal → natural-language prompts</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph is transformed into a syntax tree and then traversed to produce structured natural-language prompts describing nodes, relationships, and hierarchical structure for input to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical → sequential (tree traversal to text)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Syntax-tree construction followed by deterministic traversal (tree-to-text) to generate prompts that describe local and global graph structure in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph reasoning via LLMs (textualized graph-to-text prompts for reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Recasting graph reasoning as text generation enables use of powerful LLM generative capacities and simplifies downstream prompting and CoT styles.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Tree traversal textualization can still lose fine-grained topological information and faces input length constraints for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More structured than naive node/edge lists, but still a flattening approach whose fidelity depends on tree construction and traversal order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7034.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NodeEdgeListNL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Natural-language node-and-edge lists (direct textual recording)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple encoding in which graph data are recorded directly as a digitally organized natural-language list of nodes and edges, used as prompts for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models on graphs: Performance insights and comparative analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-edge list (natural-language)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph represented as an explicit list of nodes and edges written in natural language (e.g., 'Node A connects to Node B; Node A has attribute X'), forming the prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Direct listing/serialization of nodes and edges as plain natural-language sentences or enumerated items.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LLM graph understanding and reasoning (evaluation/analysis use)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simplicity eases prompt construction; can be used to probe LLM capability on small graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very lossy for larger graphs; verbosity leads to token-budget issues; topological patterns may be hard to express succinctly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Simpler but weaker than structured GDL or GNN-assisted encodings; often used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7034.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuning approach that augments LLMs with graph-specific instructions and trains a lightweight graph-text projector to align text and graph representations; uses Chain-of-Thought to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Instruction-tuned graph prompts + graph-text projector (CoT augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generates graph-aware prompts and uses a lightweight projector to align graph structural embeddings with textual token embeddings; may augment prompts with CoT chains to improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential prompts + learned alignment projector (hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Instruction-tuning with graph-specific prompts; projector aligns graph representations and text embeddings to allow seamless switching during processing.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ogbn-arxiv</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Node classification (as reported in review table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna (GraphGPT-7B-v1.5-stage2 reported in table)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-class conversational LLM (Vicuna variant) instruction-tuned with graph prompts and paired with a graph transformer in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (node classification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>75.11 (ogbn-arxiv, reported in review Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Instruction tuning and projector alignment reported to improve LLM reasoning about graph tasks and enable competitive node-classification performance without heavy graph-model redesign.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Complexity of training alignment projector; may still require graph-transformer components; tokenization and input-length constraints remain for large graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Shown in table to be competitive with other LLM+GNN combos; CoT and projector improve reasoning compared to naive flattening.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7034.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuseGraph: Graph-oriented instruction tuning of large language models for generic graph mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approach that generates compact graph descriptions using neighbor nodes and random walks and creates task-based Chain-of-Thought instruction sets for fine-tuning LLMs across tasks and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Compact neighbor/random-walk descriptions + task-based CoT instructions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph context by sampling neighbor nodes and short random-walks to form condensed textual descriptions and pairs these with CoT-style instructions for fine-tuning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, sample-based (lossy, compact)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Neighborhood sampling and random-walk extraction to create concise text sequences representing local topology and attributes; packaged into CoT instruction sets per task.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-tuning for downstream graph tasks (node classification, link prediction, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Aims to improve generalization by dynamically allocating instruction packages between tasks and datasets; reduces prompt length by compressing graph context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sampling/random-walk compression is lossy (may omit global topology); quality depends on sampling policy; not necessarily suitable for tasks requiring full-global view.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More compact than full flattening; trades fidelity for token efficiency compared to full GDL or exhaustive listing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7034.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (Language is all a graph needs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Designs scalable rule-based natural-language prompts for describing graph structures and fine-tunes LLMs with these instructions so the model can understand and perform graph tasks directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language is all a graph needs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Rule-based scalable natural-language graph prompts</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A set of human-designed, highly scalable textual templates/rules that map graph elements and structure into consistent natural-language prompts for LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, template-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Rule-based template generation: deterministic mapping of nodes/edges/attributes to natural-language sentences according to scalable rules.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ogbn-arxiv</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Node classification (as in review Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7b (reported in Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter Llama variant fine-tuned with rule-based graph prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (node classification)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>75.70 ± 0.12 (ogbn-arxiv, reported in review Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Rule-based prompts enable scalable fine-tuning and competitive node-classification performance using LLMs without complex graph-specific modules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Templates may not capture complex topology or multi-hop dependencies; subject to input length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms naive prompts in the reported benchmark and is competitive with other LLM+GNN combinations according to the table in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7034.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-ToolFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework that uses ChatGPT to expand prompts by inserting API calls to external graph inference tools; fine-tunes causal LLMs to automatically generate outputs containing API calls for invoking graph tools during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>API-augmented prompt representation (tool-call augmented text)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Prompts are augmented with simulated API calls (to graph inference tools) embedded in natural-language prompts; LLMs learn to emit those calls so external precise graph tools can be invoked during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (prompt + embedded API call tokens), hybrid tool-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Prompt augmentation via ChatGPT to create annotated prompts containing API calls; fine-tune LLMs to produce outputs that include API invocations for external graph processors.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex graph reasoning tasks (tool-assisted reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Causal LLMs (GPT-J, LLaMA variants fine-tuned in the study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained causal LLMs fine-tuned to produce text with embedded API calls that trigger external graph inference tools at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables LLMs to offload precise graph computation to specialized tools, improving accuracy on tasks requiring exact computation or multi-step logic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires assembling and maintaining external graph tools and a reliable tool-call interface; added system complexity and dependency on tool correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Unlike pure flattening or in-model graph encodings, this approach combines LLM natural-language reasoning with exact external graph computations to mitigate hallucination and precise-calculation weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7034.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoleculeSTM/Text2Mol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoleculeSTM / Text2Mol (cross-modal molecule text-graph alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-modal methods that align molecular graph structures and textual descriptions via contrastive training and project graph encodings into LLM word-embedding space to enable text-based molecular retrieval and editing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text2Mol: Cross-modal molecule retrieval with natural language queries</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Contrastive graph↔text alignment / projected graph embeddings into token space</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph encodings (from a GNN) are contrastively aligned with textual descriptions using a shared embedding space and a lightweight alignment projector that maps graph features into the LLM word-embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hybrid cross-modal (contrastive) embedding; not strictly sequence linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph encoder produces node/substructure embeddings; contrastive learning aligns these with paired text; an alignment projector maps graph features into LLM token/embedding space so text queries can retrieve or condition on graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PubChemSTM (multimodal molecule structure-text pairs, mentioned in review)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-based molecule retrieval, text-to-structure retrieval, molecular editing</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Two-encoder system (text encoder e.g., SciBERT; graph encoder e.g., GCN or GNN variants) with lightweight alignment projector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Separate text encoder (SciBERT or LLM) and graph encoder (GCN/GNN); projection module maps graph embeddings into the text embedding/token space for cross-modal retrieval and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct cross-modal retrieval and editing by aligning graph structural information into the LLM embedding space; facilitates retrieval from natural-language queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Contrastive alignment requires large paired corpora (e.g., PubChemSTM); mapping high-dimensional graph structure into token space may be lossy for fine-grained structural distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More suitable for cross-modal retrieval and editing than simple flattening; preserves more graph detail through learned alignment than naive text serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7034.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DGTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DGTL: Disentangled representation learning with large language models for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encodes raw textual information in text-attributed graphs using a frozen LLM and captures graph neighborhood information with custom disentangled GNN layers; then fine-tunes the LLM using these disentangled features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Disentangled representation learning with large language models for text-attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Frozen-LLM text encoding + disentangled GNN-derived neighborhood textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Text attributes are encoded by a frozen LLM into token embeddings; disentangled GNN layers separately model neighborhood aspects and output features that are used to fine-tune the LLM or produce textual descriptions for LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hybrid (embedding-based; graph-augmented text representations)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Use frozen LLM to encode node texts; disentangled GNN layers capture structural signals; combine these signals to create enhanced representations or generate text for LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Fine-tuning LLMs for text-attributed graph tasks (node classification, reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frozen large LLM + custom disentangled GNN layers</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining frozen pretrained LLM for text encoding and specialized GNN layers for structural disentanglement; final features used to fine-tune or condition LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Captures neighborhood structural signals while leveraging frozen LLM text encodings, enabling improved downstream performance without fully re-training LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds architectural complexity; alignment between frozen LLM encodings and disentangled GNN features can be nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Intermediate between pure flattening and full end-to-end integration: retains LLM textual strengths while injecting structural signals from GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7034.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7034.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIMLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tunes LLMs so they output predictive labels directly from textualized graph descriptions, avoiding additional parsing and enabling zero-shot predictions for molecular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Task-based textualized graph prompts for direct label output</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph data are converted into compact, task-specific textual descriptions/instructions and used to fine-tune LLMs to produce predictive labels directly (no extra parsers).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, instruction-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Generate compact, task-tailored natural-language prompts that describe graph context (e.g., molecule features/neighbors) and train the LLM to emit labels conditioned on these prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot molecular prediction / label generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows LLMs to directly output predictions from textualized graph inputs and supports instruction-based zero-shot learning for graph-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on prompt design and paired supervision; may struggle with tasks requiring precise structural computations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Simpler inference pipeline (no parser) than approaches that require graph post-processing, but potentially less precise than hybrid tool-augmented or GNN-assisted methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models <em>(Rating: 2)</em></li>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt <em>(Rating: 2)</em></li>
                <li>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining <em>(Rating: 2)</em></li>
                <li>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning <em>(Rating: 2)</em></li>
                <li>Language is all a graph needs <em>(Rating: 2)</em></li>
                <li>Disentangled representation learning with large language models for text-attributed graphs <em>(Rating: 1)</em></li>
                <li>Text2Mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7034",
    "paper_id": "paper-01a35c75721650182fce1c3ea39ab0911211fdbd",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TalkLikeAGraph",
            "name_full": "Talk like a graph: Encoding graphs for large language models",
            "brief_description": "Study of flattening functions and graph-to-text encodings that convert graph G and node/edge attributes into a textual Describe string (Describe = f(G, Attr)) for input to LLMs; contrasts direct textual flattening with GNN-based encoders.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models",
            "mention_or_use": "mention",
            "representation_name": "Flattening / Describe = f(G, Attr)",
            "representation_description": "A family of textual serializations that flatten graph structure and attributes into a text prompt (Describe) using a user-defined flattening function f; may include node lists, edge lists, or natural-language descriptions of neighborhoods and node attributes.",
            "representation_type": "sequential, token-based (flattening)",
            "encoding_method": "General flattening function (unspecified) — e.g., listing nodes and edges, natural-language sentences describing nodes and their neighbors, or other linearizations; paper contrasts this with using a GNN to produce the text.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph reasoning / graph-to-text encoding for LLMs (enabling downstream node classification, link prediction, reasoning)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Paper reports that choice of graph encoding significantly affects LLM performance; suitable encodings can improve reasoning accuracy of LLMs on graph tasks.",
            "limitations": "Flattening is convenient but can be lossy (loss of explicit structural detail) and suffers from LLM input-length limits; may not scale to large graphs; may lose structural relations unless carefully encoded.",
            "comparison_with_other": "Contrasted with GNN-based encodings: flattening is simpler and more direct but typically loses structure; GNNs can encode structure better but add system complexity.",
            "uuid": "e7034.0",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT4Graph",
            "name_full": "Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking",
            "brief_description": "A benchmarking/empirical effort that converts graph data into a Graph Description Language (GDL) similar to GraphML and feeds the description plus prompts to large language models for graph understanding and reasoning.",
            "citation_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "mention_or_use": "mention",
            "representation_name": "Graph Description Language (GDL) / GraphML-like serialization",
            "representation_description": "Graph encoded as a structured Graph Description Language (textual serialization akin to GraphML) that describes nodes, edges, attributes and optionally schema in a machine-readable textual format inserted into LLM prompts.",
            "representation_type": "hierarchical-structured textual serialization (token-based)",
            "encoding_method": "GraphML-like serialization: explicit listing of nodes, edges, and typed attributes in a structured text format; combined with user queries as prompt context.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Empirical benchmarking of LLMs on graph understanding/reasoning",
            "model_name": "GPT-4 / large LLMs (evaluated)",
            "model_description": "Modern large LLMs evaluated on GDL-formatted graph prompts (paper name indicates GPT-4 usage in experiments).",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Using a standardized Graph Description Language enables LLMs to parse graph structure more consistently and supports prompt-based graph queries/processing.",
            "limitations": "Still subject to LLM input-length limits; structured GDL can be verbose for large graphs; may require careful prompt engineering to avoid loss of structural nuance.",
            "comparison_with_other": "Presented as a more structured alternative to pure natural-language flattening; likely better at preserving explicit schema but more verbose than concise natural-language serializations.",
            "uuid": "e7034.1",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphText",
            "name_full": "Graphtext: Graph reasoning in text space",
            "brief_description": "Method that constructs a graph syntax tree from graph data and generates graph prompts by traversing the tree, expressing graph structure in natural language so that LLMs treat graph reasoning as text generation.",
            "citation_title": "Graphtext: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "Graph syntax-tree traversal → natural-language prompts",
            "representation_description": "Graph is transformed into a syntax tree and then traversed to produce structured natural-language prompts describing nodes, relationships, and hierarchical structure for input to LLMs.",
            "representation_type": "hierarchical → sequential (tree traversal to text)",
            "encoding_method": "Syntax-tree construction followed by deterministic traversal (tree-to-text) to generate prompts that describe local and global graph structure in natural language.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Graph reasoning via LLMs (textualized graph-to-text prompts for reasoning tasks)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Recasting graph reasoning as text generation enables use of powerful LLM generative capacities and simplifies downstream prompting and CoT styles.",
            "limitations": "Tree traversal textualization can still lose fine-grained topological information and faces input length constraints for large graphs.",
            "comparison_with_other": "More structured than naive node/edge lists, but still a flattening approach whose fidelity depends on tree construction and traversal order.",
            "uuid": "e7034.2",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "NodeEdgeListNL",
            "name_full": "Natural-language node-and-edge lists (direct textual recording)",
            "brief_description": "A simple encoding in which graph data are recorded directly as a digitally organized natural-language list of nodes and edges, used as prompts for LLMs.",
            "citation_title": "Evaluating large language models on graphs: Performance insights and comparative analysis",
            "mention_or_use": "mention",
            "representation_name": "Node-edge list (natural-language)",
            "representation_description": "Graph represented as an explicit list of nodes and edges written in natural language (e.g., 'Node A connects to Node B; Node A has attribute X'), forming the prompt context.",
            "representation_type": "sequential, token-based",
            "encoding_method": "Direct listing/serialization of nodes and edges as plain natural-language sentences or enumerated items.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "LLM graph understanding and reasoning (evaluation/analysis use)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Simplicity eases prompt construction; can be used to probe LLM capability on small graphs.",
            "limitations": "Very lossy for larger graphs; verbosity leads to token-budget issues; topological patterns may be hard to express succinctly.",
            "comparison_with_other": "Simpler but weaker than structured GDL or GNN-assisted encodings; often used as a baseline.",
            "uuid": "e7034.3",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph instruction tuning for large language models",
            "brief_description": "Instruction-tuning approach that augments LLMs with graph-specific instructions and trains a lightweight graph-text projector to align text and graph representations; uses Chain-of-Thought to improve reasoning.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "mention",
            "representation_name": "Instruction-tuned graph prompts + graph-text projector (CoT augmented)",
            "representation_description": "Generates graph-aware prompts and uses a lightweight projector to align graph structural embeddings with textual token embeddings; may augment prompts with CoT chains to improve multi-step reasoning.",
            "representation_type": "sequential prompts + learned alignment projector (hybrid)",
            "encoding_method": "Instruction-tuning with graph-specific prompts; projector aligns graph representations and text embeddings to allow seamless switching during processing.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ogbn-arxiv",
            "task_name": "Node classification (as reported in review table)",
            "model_name": "Vicuna (GraphGPT-7B-v1.5-stage2 reported in table)",
            "model_description": "7B-class conversational LLM (Vicuna variant) instruction-tuned with graph prompts and paired with a graph transformer in reported experiments.",
            "performance_metric": "Accuracy (node classification)",
            "performance_value": "75.11 (ogbn-arxiv, reported in review Table 2)",
            "impact_on_training": "Instruction tuning and projector alignment reported to improve LLM reasoning about graph tasks and enable competitive node-classification performance without heavy graph-model redesign.",
            "limitations": "Complexity of training alignment projector; may still require graph-transformer components; tokenization and input-length constraints remain for large graphs.",
            "comparison_with_other": "Shown in table to be competitive with other LLM+GNN combos; CoT and projector improve reasoning compared to naive flattening.",
            "uuid": "e7034.4",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MuseGraph",
            "name_full": "MuseGraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "brief_description": "Approach that generates compact graph descriptions using neighbor nodes and random walks and creates task-based Chain-of-Thought instruction sets for fine-tuning LLMs across tasks and datasets.",
            "citation_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "mention_or_use": "mention",
            "representation_name": "Compact neighbor/random-walk descriptions + task-based CoT instructions",
            "representation_description": "Encodes graph context by sampling neighbor nodes and short random-walks to form condensed textual descriptions and pairs these with CoT-style instructions for fine-tuning LLMs.",
            "representation_type": "sequential, sample-based (lossy, compact)",
            "encoding_method": "Neighborhood sampling and random-walk extraction to create concise text sequences representing local topology and attributes; packaged into CoT instruction sets per task.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Instruction-tuning for downstream graph tasks (node classification, link prediction, reasoning)",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Aims to improve generalization by dynamically allocating instruction packages between tasks and datasets; reduces prompt length by compressing graph context.",
            "limitations": "Sampling/random-walk compression is lossy (may omit global topology); quality depends on sampling policy; not necessarily suitable for tasks requiring full-global view.",
            "comparison_with_other": "More compact than full flattening; trades fidelity for token efficiency compared to full GDL or exhaustive listing.",
            "uuid": "e7034.5",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "InstructGLM",
            "name_full": "InstructGLM (Language is all a graph needs)",
            "brief_description": "Designs scalable rule-based natural-language prompts for describing graph structures and fine-tunes LLMs with these instructions so the model can understand and perform graph tasks directly.",
            "citation_title": "Language is all a graph needs",
            "mention_or_use": "mention",
            "representation_name": "Rule-based scalable natural-language graph prompts",
            "representation_description": "A set of human-designed, highly scalable textual templates/rules that map graph elements and structure into consistent natural-language prompts for LLM fine-tuning.",
            "representation_type": "sequential, template-based",
            "encoding_method": "Rule-based template generation: deterministic mapping of nodes/edges/attributes to natural-language sentences according to scalable rules.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ogbn-arxiv",
            "task_name": "Node classification (as in review Table 2)",
            "model_name": "Llama-7b (reported in Table 2)",
            "model_description": "7B-parameter Llama variant fine-tuned with rule-based graph prompts.",
            "performance_metric": "Accuracy (node classification)",
            "performance_value": "75.70 ± 0.12 (ogbn-arxiv, reported in review Table 2)",
            "impact_on_training": "Rule-based prompts enable scalable fine-tuning and competitive node-classification performance using LLMs without complex graph-specific modules.",
            "limitations": "Templates may not capture complex topology or multi-hop dependencies; subject to input length constraints.",
            "comparison_with_other": "Outperforms naive prompts in the reported benchmark and is competitive with other LLM+GNN combinations according to the table in the review.",
            "uuid": "e7034.6",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Graph-ToolFormer",
            "name_full": "Graph-ToolFormer: To empower LLMs with graph reasoning ability via prompt augmented by ChatGPT",
            "brief_description": "Framework that uses ChatGPT to expand prompts by inserting API calls to external graph inference tools; fine-tunes causal LLMs to automatically generate outputs containing API calls for invoking graph tools during inference.",
            "citation_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "mention_or_use": "mention",
            "representation_name": "API-augmented prompt representation (tool-call augmented text)",
            "representation_description": "Prompts are augmented with simulated API calls (to graph inference tools) embedded in natural-language prompts; LLMs learn to emit those calls so external precise graph tools can be invoked during reasoning.",
            "representation_type": "sequential (prompt + embedded API call tokens), hybrid tool-augmented",
            "encoding_method": "Prompt augmentation via ChatGPT to create annotated prompts containing API calls; fine-tune LLMs to produce outputs that include API invocations for external graph processors.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Complex graph reasoning tasks (tool-assisted reasoning)",
            "model_name": "Causal LLMs (GPT-J, LLaMA variants fine-tuned in the study)",
            "model_description": "Pretrained causal LLMs fine-tuned to produce text with embedded API calls that trigger external graph inference tools at inference time.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables LLMs to offload precise graph computation to specialized tools, improving accuracy on tasks requiring exact computation or multi-step logic.",
            "limitations": "Requires assembling and maintaining external graph tools and a reliable tool-call interface; added system complexity and dependency on tool correctness.",
            "comparison_with_other": "Unlike pure flattening or in-model graph encodings, this approach combines LLM natural-language reasoning with exact external graph computations to mitigate hallucination and precise-calculation weaknesses.",
            "uuid": "e7034.7",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "MoleculeSTM/Text2Mol",
            "name_full": "MoleculeSTM / Text2Mol (cross-modal molecule text-graph alignment)",
            "brief_description": "Cross-modal methods that align molecular graph structures and textual descriptions via contrastive training and project graph encodings into LLM word-embedding space to enable text-based molecular retrieval and editing.",
            "citation_title": "Text2Mol: Cross-modal molecule retrieval with natural language queries",
            "mention_or_use": "mention",
            "representation_name": "Contrastive graph↔text alignment / projected graph embeddings into token space",
            "representation_description": "Graph encodings (from a GNN) are contrastively aligned with textual descriptions using a shared embedding space and a lightweight alignment projector that maps graph features into the LLM word-embedding space.",
            "representation_type": "hybrid cross-modal (contrastive) embedding; not strictly sequence linearization",
            "encoding_method": "Graph encoder produces node/substructure embeddings; contrastive learning aligns these with paired text; an alignment projector maps graph features into LLM token/embedding space so text queries can retrieve or condition on graphs.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "PubChemSTM (multimodal molecule structure-text pairs, mentioned in review)",
            "task_name": "Text-based molecule retrieval, text-to-structure retrieval, molecular editing",
            "model_name": "Two-encoder system (text encoder e.g., SciBERT; graph encoder e.g., GCN or GNN variants) with lightweight alignment projector",
            "model_description": "Separate text encoder (SciBERT or LLM) and graph encoder (GCN/GNN); projection module maps graph embeddings into the text embedding/token space for cross-modal retrieval and generation.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Enables direct cross-modal retrieval and editing by aligning graph structural information into the LLM embedding space; facilitates retrieval from natural-language queries.",
            "limitations": "Contrastive alignment requires large paired corpora (e.g., PubChemSTM); mapping high-dimensional graph structure into token space may be lossy for fine-grained structural distinctions.",
            "comparison_with_other": "More suitable for cross-modal retrieval and editing than simple flattening; preserves more graph detail through learned alignment than naive text serializations.",
            "uuid": "e7034.8",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DGTL",
            "name_full": "DGTL: Disentangled representation learning with large language models for text-attributed graphs",
            "brief_description": "Encodes raw textual information in text-attributed graphs using a frozen LLM and captures graph neighborhood information with custom disentangled GNN layers; then fine-tunes the LLM using these disentangled features.",
            "citation_title": "Disentangled representation learning with large language models for text-attributed graphs",
            "mention_or_use": "mention",
            "representation_name": "Frozen-LLM text encoding + disentangled GNN-derived neighborhood textualization",
            "representation_description": "Text attributes are encoded by a frozen LLM into token embeddings; disentangled GNN layers separately model neighborhood aspects and output features that are used to fine-tune the LLM or produce textual descriptions for LLM input.",
            "representation_type": "hybrid (embedding-based; graph-augmented text representations)",
            "encoding_method": "Use frozen LLM to encode node texts; disentangled GNN layers capture structural signals; combine these signals to create enhanced representations or generate text for LLM fine-tuning.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Fine-tuning LLMs for text-attributed graph tasks (node classification, reasoning)",
            "model_name": "Frozen large LLM + custom disentangled GNN layers",
            "model_description": "Pipeline combining frozen pretrained LLM for text encoding and specialized GNN layers for structural disentanglement; final features used to fine-tune or condition LLM.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Captures neighborhood structural signals while leveraging frozen LLM text encodings, enabling improved downstream performance without fully re-training LLMs.",
            "limitations": "Adds architectural complexity; alignment between frozen LLM encodings and disentangled GNN features can be nontrivial.",
            "comparison_with_other": "Intermediate between pure flattening and full end-to-end integration: retains LLM textual strengths while injecting structural signals from GNNs.",
            "uuid": "e7034.9",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GIMLET",
            "name_full": "GIMLET: A unified graph-text model for instruction-based molecule zero-shot learning",
            "brief_description": "Fine-tunes LLMs so they output predictive labels directly from textualized graph descriptions, avoiding additional parsing and enabling zero-shot predictions for molecular tasks.",
            "citation_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning",
            "mention_or_use": "mention",
            "representation_name": "Task-based textualized graph prompts for direct label output",
            "representation_description": "Graph data are converted into compact, task-specific textual descriptions/instructions and used to fine-tune LLMs to produce predictive labels directly (no extra parsers).",
            "representation_type": "sequential, instruction-based",
            "encoding_method": "Generate compact, task-tailored natural-language prompts that describe graph context (e.g., molecule features/neighbors) and train the LLM to emit labels conditioned on these prompts.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Zero-shot molecular prediction / label generation",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Allows LLMs to directly output predictions from textualized graph inputs and supports instruction-based zero-shot learning for graph-related tasks.",
            "limitations": "Quality depends on prompt design and paired supervision; may struggle with tasks requiring precise structural computations.",
            "comparison_with_other": "Simpler inference pipeline (no parser) than approaches that require graph post-processing, but potentially less precise than hybrid tool-augmented or GNN-assisted methods.",
            "uuid": "e7034.10",
            "source_info": {
                "paper_title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking",
            "rating": 2,
            "sanitized_title": "gpt4graph_can_large_language_models_understand_graph_structured_data_an_empirical_evaluation_and_benchmarking"
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2,
            "sanitized_title": "graphgpt_graph_instruction_tuning_for_large_language_models"
        },
        {
            "paper_title": "Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt",
            "rating": 2,
            "sanitized_title": "graphtoolformer_to_empower_llms_with_graph_reasoning_ability_via_prompt_augmented_by_chatgpt"
        },
        {
            "paper_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "rating": 2,
            "sanitized_title": "musegraph_graphoriented_instruction_tuning_of_large_language_models_for_generic_graph_mining"
        },
        {
            "paper_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning",
            "rating": 2,
            "sanitized_title": "gimlet_a_unified_graphtext_model_for_instructionbased_molecule_zeroshot_learning"
        },
        {
            "paper_title": "Language is all a graph needs",
            "rating": 2,
            "sanitized_title": "language_is_all_a_graph_needs"
        },
        {
            "paper_title": "Disentangled representation learning with large language models for text-attributed graphs",
            "rating": 1,
            "sanitized_title": "disentangled_representation_learning_with_large_language_models_for_textattributed_graphs"
        },
        {
            "paper_title": "Text2Mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 1,
            "sanitized_title": "text2mol_crossmodal_molecule_retrieval_with_natural_language_queries"
        }
    ],
    "cost": 0.022885999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</h1>
<p>Yuxin You<br>School of Computer Science and Engineering<br>University of Electronic Science and Technology of China<br>Chengdu 611731, China<br>202321081209@std.uestc.edu.cn</p>
<p>Zhen Liu<br>School of Computer Science and Engineering<br>University of Electronic Science and Technology of China<br>Chengdu 611731, China<br>quake@uestc.edu.cn</p>
<h2>Xiangchao Wen</h2>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
Chengdu 611731, China
cha0s101cs@gmail.com</p>
<h2>Yongtao Zhang</h2>
<p>School of Computer Science and Engineering
University of Electronic Science and Technology of China
Chengdu 611731, China
202222080730@std.uestc.edu.cn</p>
<h2>Wei Ai</h2>
<p>54th Research Institute of CETC Shijiazhuang 050081, China
aiwei2001@sina.com</p>
<h2>Abstract</h2>
<p>Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, Large Language Models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graphstructured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Based on the relationship between LLMs and GNNs, we categorize the application scenarios of LLMs in graph mining into three groups: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. These models are capable of handling a variety of datasets and achieving success in various downstream tasks.</p>
<h1>1 Introduction</h1>
<p>Graphs are data structures used to describe relationships between objects, and they are widely used in many domains, such as social networks [1], computer networks, and molecular structures. Some of these graphs contain hundreds of millions of node information, but the vast majority are redundant and irrelevant. What graph mining investigates is how to extract relevant or valuable knowledge and information from scaled graph data by using graph mining models.
Over the past decade, graph mining techniques have evolved, yielding many significant results and significantly contributing to the development of the field. Early research was inspired by the word vector technique [2], and Perozzi et al. [3] firstly proposed the random walk-based graph embedding method DeepWalk; Node2Vec [4] proposed by Grover and Leskovec et al. generates node embeddings through biased random walk strategies for tasks such as node classification and link prediction, and Graph2Vec [5], on the other hand, embeds the entire graph into the vector space for graph classification tasks. For more efficient graph representation learning, the Graph Neural Networks (GNNs) [6]proposed by Kipf and Welling pioneered a new research paradigm. GNNs efficiently capture and collect structural information and dependencies in graph-structured data through information propagation and aggregation mechanisms, which enable the model to make precise predictions in complex graph structures. In recent years, various GNN architectures have been developed in information propagation and aggregation methods. For example, Graph Convolutional Network (GCN) [7] processes graph data through spectral convolution, which is widely used in node classification and graph classification tasks; Graph Attention Network (GAT) [8] introduces an attention mechanism to adaptively assign the importance of different neighboring nodes, which enhances the expressive power of the model; GraphSAGE [9] can efficiently process large-scale graph data by sampling and aggregating features from neighboring nodes. In addition, for heterogeneous graph analysis, the Heterogeneous Graph Attention Network (HAN) [10] proposed by Wang et al. aggregates information and learns between different types of nodes and edges through the attention mechanism, while Zhang et al.'s Heterogeneous Graph Neural Network (HetGNN) [11] learns embeddings by sampling various types of nodes and edges, which enables it to manage complex heterogeneous graph structures. These results not only demonstrate the prospect of wide application of graph mining techniques but also promote the development of related research fields.
In 2022, the emergence of Large Language Models (LLMs), represented by ChatGPT [12], revolutionized the field of Natural Language Processing (NLP) as well as the domain of artificial intelligence research. The large language model is pre-trained on a large amount of text, which can learn rich language expressions and huge amount of real-world</p>
<p>knowledge, and has excellent semantic understanding capabilities [13]. For example, BERT [14] uses a bidirectional attention mechanism to capture the contextual information of the text, which enables the model to perform on various NLP tasks such as question answering, named entity recognition, and sentence classification. GPT-3 [15], with 175 billion parameters, is pre-trained by an autoregressive approach and is capable of performing a wide range of tasks from text generation and translation to dialog systems. Its strength lies in its ability to perform new tasks without fine-tuning and with a small number of examples or hints, demonstrating impressive zero-shot and few-shot learning capabilities. These LLMs can be applied to various downstream tasks with little additional training. Although large language models were initially developed for natural language processing, researchers have also been exploring the application of LLMs on multimodal data in recent years. For instance, the DALL-E [16] model that combines image and text can generate corresponding images based on the text by performing self-supervised learning on paired image-text corpora. These multimodal models demonstrate outstanding potential for processing and understanding different data types.
Given the disruptive potential of large models, many researchers in graph mining have focused on them in the last two years, expecting them to bring new developments to the field of graph mining. Zhang et al. [17] discusses the challenges and opportunities presented by the combination of graphs and LLMs and showcases the potential of these models across various application domains. Chen et al. [18] explores the potential of utilizing LLMs in graph learning tasks and investigates two possible approaches: LLMs-as-Enhancers and LLMs-as-Predictors. Liu et al. [19] introduced the concept of Graph Foundation Models (GFMs) and provided a taxonomy and review of existing work related to GFMs. These studies suggest that integrating LLMs with graph neural networks can enhance various downstream graph mining tasks. The reason is that, although GNNs excel at capturing structural information, they have limitations in terms of expressiveness and generalizability[20], and their semantically constrained embeddings often fail to characterize the node features for complex node information fully. On the contrary, LLMs are good at processing complex texts but are often deficient in structural information processing. Combining the strengths of both can significantly improve the accuracy of graph mining.
To the best of our knowledge, existing reviews on the application of large models in graph mining mainly focus on categorizing LLMs simply as enhancers or predictors [18]. However, such a categorization approach is only a simple and superficial combination of LLMs and GNNs considered as independent models. It fails to delve into the profound fusion potential of LLMs and GNNs in the graph mining domain. Therefore, we propose a new classification framework, as shown in Figure 1, based on the main driving components in graph mining models, which are categorized into three groups: GNN-driving-LLMs, LLM-driving-GNNs, and GNN-LLM-co-driving. In this new classification framework, the GNN-driving-LLM mode emphasizes the GNN as the central task processing module, and the LLM plays an assisting role in specific tasks or scenarios, such as natural language interpretation or feature extraction. On the contrary, the LLM-driving-GNN mode places the LLM at the core and the GNN as an auxiliary tool for processing and guiding the graph-structured data to enhance the model's performance in complex graph data. In the GNN-LLM-co-driving mode, on the other hand, GNN and LLM work closely together to form a kind of interdependent joint model that collaboratively solves the corresponding graph mining tasks. Such a classification not only helps to fully understand the deep integration of LLMs and GNNs but also provides new ideas for future research directions.</p>
<h1>2 preliminary</h1>
<h3>2.1 Graph Mining</h3>
<p>Graph mining tasks are important in data knowledge discovery, aiming to extract valuable information from graphstructured data. Graph-structured data consists of nodes and edges, where nodes represent entities and edges represent relationships between represented entities. It is widely applied in fields such as social network analysis [21], recommender systems [22], chemistry [23] and knowledge graphs [24].
Common graph mining tasks include node classification, link prediction, graph clustering, graph matching, community detection, frequent subgraph mining, etc. Node classification utilizes labeled nodes as training data and predicts the classes of unlabeled nodes in the graph, such as node embeddings based on random walks [4], Graph Convolutional Networks (GCN) [7]. Link prediction is used to predict potential future edges, with important applications in recommender systems (e.g., friend recommendation, item recommendation). Common link prediction algorithms include neighborhood-based algorithms [25], path-dependent models [26], deep learning methods [27], and so on. Graph clustering [28] and graph matching [29] are respectively concerned with the grouping of nodes and subgraph correspondence between different graphs. Graph clustering groups graph nodes such that nodes within the same group are more tightly connected, while graph matching finds corresponding subgraphs between different graphs, which is commonly used in chemical molecular structure comparison and pattern recognition. Community detection [30] identifies subsets or associations in the graph, where nodes within a community are closely connected but have relatively few connections to nodes outside the community; commonly used algorithms such as the Girvan-Newman algorithm</p>
<p>[31], Louvain's method [32] and so on. Frequent subgraph mining [33] spots frequently occurring subgraphs from a set of graphs, which can be used in chemoinformatics to discover common molecular structures. By analyzing a large number of molecular diagrams, frequent subgraph mining can reveal which molecular fragments appear repeatedly across multiple compounds, providing valuable insights for fields such as bioinformatics and chemoinformatics [34].</p>
<h1>2.2 Large Language Model</h1>
<p>A large language model consists of a neural network with many parameters (usually billions of weights or more). In recent years, thanks to the introduction of the Transformer architecture and its ability to be pre-trained on large-scale text data, it has made significant progress in natural language processing.
The transformer model [35] proposed by Vaswani et al. in 2017 is based on the attention mechanism, which can greatly improve training efficiency and performance. Its fundamental unit is the multi-head self-attention mechanism, which can be expressed by the following formula:</p>
<p>$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>Where $Q, K$, and $V$ denote the Query, Key, and Value matrices, respectively, and $d_{k}$ is the dimension of the key vector. The multiple head mechanism further computes several different attention values and combines them:</p>
<p>$$
\operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\operatorname{head}<em 2="2">{1}, \operatorname{head}</em>
$$}, \ldots, \operatorname{head}_{h}\right) W^{O</p>
<p>The types and applications of LLMs are constantly evolving with the development of architectures and training methods. Current LLMs can be classified into the categories of autoregressive models, masked language models, encoder-decoder models, contrastive learning models, multimodal models, and others. Autoregressive models generate text by predicting the next word in a sequence, and such models can generate coherent text and perform very well in generative tasks such as GPT [36], GPT-2 [37], GPT-3 [15] and GPT-4 [38].On the other hand, Masked language models typically mask the words in a given utterance to train the model to predict these masked words and thus learn the contextual representation. Representative models of masked language models include BERT [14], RoBERTa [39] and XLNet [40]. The encoder-decoder models achieve a high degree of parallelization and dramatically improve computational efficiency by encoding the input text into a contextual representation and decoding it to generate the target text. Classic models of this type include BART [41], and T5 [42].Contrastive learning models, such as SimCLR [43], train the model to differentiate between positive and negative samples by constructing pairs of positive and negative samples so that the model can capture relevant features and similarities in the data. Finally, multimodal models are able to process information from multiple modalities (including images, videos, and texts) to enhance the performance of the model in multimodal tasks, such as CLIP [44] and DALL-E [16]. The continuous development of large language models not only enhances the ability of computers in natural language processing but also provides new ideas and methods for solving broader and more complex data analysis and processing tasks.</p>
<h2>3 Techniques of the LLMs combined with GNNs</h2>
<p>Regarding techniques of the LLMs combined with GNNs, as shown in Figure 2, we propose a novel taxonomy including GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.</p>
<h3>3.1 GNN-driving-LLM</h3>
<p>In graph mining, GNNs are a class of deep learning models specialized in processing graph data. By combining the structural information and node features of the graph, they are able to effectively learn the representation of nodes, edges, and the overall graph in a graph. Text-attributed graphs (TAGs), in which the attributes of nodes exist in the form of text, are ubiquitous in the research of graph machine learning, such as product networks [91], social networks [1, 92] and citation networks [93, 94], where textual attributes provide key semantic information for the graph mining task. Therefore, when dealing with such graphs, the structure of the graph, the textual information, and their interrelationships must be considered simultaneously.
Traditionally, the processing of node text attributes often relies on shallow embedding methods, such as the classic Cora dataset [95], which only provides embedding features based on bag-of-words models. This approach is coarse in semantic understanding, leading to the limited performance of GNNs in processing textual attribute graphs. However, with the development of LLMs, their powerful text processing and semantic understanding capabilities provide a new solution to this problem. LLMs can extract richer semantic features from the textual attributes of nodes and generate additional auxiliary information, such as attribute interpretations and pseudo-labels, which provide more resounding</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The proposed taxonomy on LLM-GNN-combined techniques.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: GNN-driving-LLM: <strong>a)</strong> LLMs generate additional information for the textual attributes of the nodes. These features extracted by LLMs are fed into the next layer of the language model to generate enhanced node embeddings, which are finally processed by GNNs for downstream tasks; <strong>b)</strong> the textual embeddings exported by LLMs can be directly used as the initial node embeddings for GNNs.</p>
<p>Semantic support for node embedding in graph mining tasks. In previous research [53, 54], techniques combining language model (LM) and GNNs have been investigated to use LMs for encoding and providing them as node features to GNNs, and the introduction of extensive language modeling further enhances the effectiveness of this approach. In the GNN-driving-LLM model, GNNs are still used as the central information processing unit, but the text attributes are deeply parsed and embedded by incorporating LLMs, which makes the GNNs more accurate in capturing the semantic information of the nodes and thus improves the performance of the model in the downstream tasks. As shown in Figure 3(a), LLMs generate additional information for the textual attributes of the nodes. Next, these features extracted by LLMs are fed into the next layer of the language model to generate enhanced node embeddings. A typical example is the TAPE [47] model, which first predicts the node text (such as paper titles and abstracts) in LLMs and generates the corresponding interpretations. Through task-specific prompts, LLMs generate categorization predictions and explanation texts. The interpreted text generated by LLMs is then fed into a smaller language model, processed through fine-tuning and transformed into node features (including original text features<strong>h</strong><em expl="expl">{orig}, explanation features<strong>h</strong></em> and prediction features<strong>h</strong><em TAPE="TAPE">{pred}). The combination of these three classes of features as <strong>h</strong></em> is used for downstream</p>
<p>GNNs training. Finally, the GNN models are trained on the generated features to achieve the node classification task. The following formula can describe the process:</p>
<p>$\mathbf{s}=\operatorname{LLM}(x,p), \quad \mathbf{h}=\operatorname{LM}(\mathbf{s}, p) \in \mathbb{R}^{N \times d}, \quad \hat{\mathbf{y}}=\operatorname{GNN}(\mathbf{h}, \mathbf{s}, A) \in \mathbb{R}^{N \times C}, \quad$ (3)</p>
<p>where the LLMs accept a raw text $x=\left(x_{1}, x_{2}, \ldots, x_{q}\right)$ as input and generates a sequence of interpreted text $s=$ $\left(s_{1}, s_{2}, \ldots, s_{m}\right)$ as output, with $p$ is the cue for the LLMs. $\mathbf{h} \in \mathbb{R}^{N \times d}$ is the output of LMs, which is the node embedding matrix augmented by LLMs, and $A \in \mathbb{R}^{N \times N}$ is the adjacency matrix of the graph. Several studies have shown that this strategy of combining LLMs and GNNs has significant advantages in practical applications. For example, LLMRec [45] is the first work on graph enhancement using LLMs by augmenting user-item interaction edges, item node attributes, and user node profiles, which effectively solves the problems of data sparsity and low-quality auxiliary information in recommender systems. Similarly, RLMRec [46] also utilizes LLMs to enhance representation learning in existing recommender systems, which uses LLMs to respectively process textual information of items, as well as user interaction information and textual attributes of related items, to generate item profiles and user profiles. By maximizing mutual information, RLMRec aligns the semantic representations from LLMs with collaborative relationship representations, significantly improving the performance of the recommendation system. PRODIGY [48] is a framework for pre-training context learners on prompt graphs. It leverages the powerful zero-sample capability of LLMs to encode textual information of graph nodes, enabling context learning on graphs. Inspired by the remarkable effectiveness of prompt learning in NLP, ALL-in-one [49] proposes a new multitask prompting approach that unifies graph prompts and language prompts formats by prompt tokens, token structures, and insertion patterns. It enables the NLP prompting concepts to be seamlessly introduced into the graph domain, thus unifying the formats of graph prompting and language prompting.
When using embeddable or open-source LLMs, the text embeddings they generate can be accessed directly. In this case, the LLMs first extract textual features for each node, which are then fed into the GNNs as initial node embeddings. Subsequently, the GNNs combine the graph structure information with these augmented node embeddings through their message passing and feature aggregation mechanisms, as shown in Figure 3(b). In general, the process can be described by the following equation:</p>
<p>$$
\mathbf{h}=\operatorname{LLM}(\mathbf{s}), \quad \hat{\mathbf{y}}=\operatorname{GNN}(\mathbf{h}, A)
$$</p>
<p>This approach significantly improves the performance of graph mining tasks, especially in downstream tasks such as node classification, link prediction, and graph classification. OFA [50] embeds textual descriptions of graph datasets from different domains into the same feature space, thus becoming the first cross-domain generalized graph classification model. GaLM [51] pretrains on large-scale heterogeneous graphs and incorporates structural information into the fine-tuning stage of LLM to generate higher quality node embeddings, thus improving performance for downstream applications in different graph patterns. LEADING [52] is an end-to-end fine-tuning algorithm that significantly improves LLMs' computational and data efficiency in processing TAGs by reducing coding redundancy and propagation redundancy. GraphEdit [55] enhances LLMs' reasoning ability on node relationships in graph data through instruction tuning to solve the noise and sparsity problems in graph structures. Specifically, GraphEdit first uses LLMs to generate semantic embeddings of nodes and filter candidate edges by a lightweight edge predictor; then, in combination with the original graph structure, it uses the inference ability of LLMs to optimize edge additions and deletions and generates the improved graph structure. Eventually, the optimized graph structure is used for GNNs training to support downstream tasks such as node classification, thus realizing denoising and global dependency mining of the graph structure.
With their exceptional ability to process text sequences, LLMs perform excellently in handling TAGs. They can extract deep semantic information from textual attributes, providing richer feature representations than traditional methods. In addition, rather than traditional GNNs that need to design different architectures for different datasets, combining LLMs and GNNs can process data from different domains by unifying the feature space and cross-domain embedding methods. This approach, which combines the diversity of language models and the ability to understand the structure of graph neural networks, offers flexibility in dealing with complex attributes and structures.
Beyond that, LLMs can also improve the performance of downstream tasks through data augmentation. For example, LLM-GNN[57] achieves efficient and low-cost label-free node classification through LLM-based zero-shot labeling and GNN-based extended learning. LLM4NG [56] represents a typical application of graph generation learning. It utilizes a large language model to generate new nodes and integrates these nodes into the original graph structure via an edge predictor to generate a new graph structure. This approach significantly improves model performance in few-shot scenarios, demonstrating the effectiveness of augmenting model learning capabilities by generating samples. Similarly, OpenGraph [58] uses LLMs for generating synthetic graph data (e.g., nodes and edges) as well as augmenting the pre-training data. It also employs a unified graph tokenizer and an efficient graph Transformer, achieving excellent generalization on multi-domain graph data in zero-shot learning.
Despite the fact that LLMs can enhance model performance, they are extremely demanding of computational resources, especially when processing large amounts of textual data, requiring significant computational resources or frequent</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: LLM-driving-GNN: Models use spreading functions or GNNs to transform graph data into sequence text so that LLMs can directly understand the graph data and accomplish downstream tasks.</p>
<p>API query calls to the LLMs, which comes with a high cost. Moreover, although LLMs provide rich semantic information, their decision-making process is often opaque and lacks interpretability. Therefore, how to efficiently fuse text embedding with graph structure information in the framework of GNNs remains a challenging issue that requires further research.</p>
<h1>3.2 LLM-driving-GNN</h1>
<p>In some graph mining tasks, LLMs, with their powerful zero-shot learning capabilities, can directly perform prediction, classification, or reasoning. The computational power of LLMs and the advantages of deep learning algorithms enable them to perform well in these tasks. However, since LLMs can only accept sequential text as input, an additional step is required for graph data: transforming the graph data, which has been defined in various ways in terms of structure and features, into sequential text directly fed into LLMs. In recent years, many studies have explored the applicability of LLMs for tasks downstream of graph structures, yielding some preliminary results [96, 97, 98]. These results suggest that LLMs have achieved initial success in processing implicit graph structures. Wang et al.[63] proposed a synthetic benchmark, NLGraph, for evaluating the performance of LLMs in the task of reasoning about graph structures. It was found that the capability of LLMs has been demonstrated in simple graph reasoning tasks but is still insufficient when dealing with complex problems.Additionally, several studies have explored the capability of LLMs in processing graph-structured data and their effectiveness in extracting and utilizing graph structural information [99, 100]. These studies have opened new avenues for exploiting the capabilities of LLMs in graph applications and further exploring the incorporation of structured data into LLMs frameworks. A growing number of researchers are committed to exploring how to better apply LLMs to downstream tasks containing graph structures and further improve their performance.</p>
<p>Typically, to enable LLMs to understand graph data, researchers use specific methods to convert graph data into textual descriptions, which are then used as inputs to LLMs and finally extract predictions from the output of the model. Among these methods, flattening functions can directly convert graphs into textual descriptions, which is more convenient and intuitive, while GNNs have demonstrated excellent ability in understanding graph structures through information propagation and aggregation among nodes. Therefore, researchers have also explored using GNNs to transform graph data with different structures and features into sequential text to fully leverage the structural information in the graph data for prediction. As shown in Figure 4, the method of encoding graph structure data into text for use in LLMs was first comprehensively investigated by Fatemi et al. [60], and can be described by the following equation:</p>
<p>$$
\text { Describe }=f(G, \text { Attr }), \quad \mathbf{A}=\operatorname{LLM}(\text { Describe }, p)
$$</p>
<p>where $f$ represents the flattening function or GNNs, which inputs the graph $G$ and the text attributes Attr on each node or edge of the graph to get the text description, i.e., Describe, to be fed to the LLMs. $p$ denotes the prompts, and $\mathbf{A}$ is the answers given by the LLMs, from which the predicted labels of downstream tasks can be extracted in a specific way. Research has shown that choosing a suitable graph encoding method can significantly improve the performance of LLMs in graph reasoning tasks. Thus, one of the current research priorities is to explore suitable graph encoding methods. For example, GPT4Graph [61] converts graph data into a Graph Description Language (GDL) like GraphML [101]. It generates prompts in conjunction with user queries so that the large language model can understand and process the graph-structured data. GraphText [59] constructs a graph syntax tree from graph data and generates graph prompts through traversal of the tree, expressing them in natural language so that LLMs can treat graph reasoning as a text generation task. An alternative approach is to record the graphical data directly in natural language, i.e., to describe the graphical data with a digitally organized list of nodes and edges [64]. GraphGPT [65] also utilizes Chain-of-Thought (CoT) [102] to augment the model's reasoning capabilities. However, in terms of transforming graph data, GraphGPT trains a lightweight graph-text projector that is able to align representations between text and graph structure, allowing the model to switch seamlessly during processing. Similarly, MoleculeSTM [66] uses a graph</p>
<p>encoder as a molecular graph structure encoder. The method enables large language models to align molecular structures to natural language by aligning molecular graphs and textual representations through contrastive training and then employing a lightweight alignment projector to map graph features into the word embedding space. DGTL [62] encodes the raw textual information in TAGs using a frozen LLM and then captures the graph neighborhood information in TAGs by a set of custom disentangled graph neural network layers. Finally, the features learned from these disentangled layers are used to fine-tune the LLMs to help the model better understand the complex graph structure information in the TAGs, thereby improving the final prediction ability of the LLMs. On the other hand, GraphTranslator [67] proposes a mechanism called Producer for creating graph-text alignment data, which enables a large language model to predict graph data based on language instructions. HiGPT [68] adapts to diverse heterogeneous graph learning tasks without downstream fine-tuning through a heterogeneous graph instruction-tuning paradigm.</p>
<p>Another way to enhance the capability of LLMs on graph-structured data is by fine-tuning LLMs to enhance their graph mining capabilities. Several researchers have explored this area and made notable progress. GIMLET [69] fine-tunes LLMs to output predictive labels directly, thus providing accurate predictions without additional parsing steps. MuseGraph [70] generates compact graph descriptions using neighbor nodes and random walks and creates task-based CoT instruction sets to fine-tune the large language model. The method dynamically allocates instruction packages between tasks and datasets to ensure the effectiveness and generalization of the training process. Eventually, the graph structure data is converted into a format suitable for LLMs, which allows the fine-tuned model to fit downstream tasks such as node classification, link prediction, and graph-to-text generation. InstructGLM [71] designed a series of rule-based, highly scalable natural language prompts for describing graph structures and performing graph tasks, and fine-tunes large language models with these instructions, enabling them to understand and process these descriptions to perform graph tasks. GraphLLM [72] adopts an end-to-end approach that integrates a graph learning module (graph transformer) with LLMs. Specifically, the approach uses a textual Transformer encoder-decoder to extract the necessary information from the node descriptions, learns the graph structure through the graph Transformer, and generates overall graph representations by aggregating the node representations. Ultimately, these graph representations are used to generate graph-enhanced prefixes injected in each LLM attention layer. This approach allows the LLM to work synergistically with the graph Transformer to incorporate structural information critical for graph inference and thus improve performance on graph reasoning tasks.</p>
<p>Unlike the above approaches, the Graph-ToolFormer framework [73] uses API calls to invoke external graph inference tools to complete reasoning tasks. First, ChatGPT is utilized to annotate and expand the manually written graph reasoning task prompts to generate a large dataset of prompts containing graph reasoning API calls. Then, the generated dataset is used to fine-tune pre-trained causal LLMs (e.g., GPT-J [103] and LLaMA [104]) and teach them how to use external graph inference tools in the generated output. Finally, the fine-tuned Graph-ToolFormer models are able to automatically add the corresponding graph reasoning API calls to the output statements when they receive input queries and questions. Through the above steps, the Graph-ToolFormer framework realizes the ability to empower existing LLMs to handle complex graph reasoning tasks, effectively addressing the current limitations of LLMs in handling precise computation, multi-step logical reasoning, spatial and topological awareness, etc. LLMs can also be used to generate new GNN architectures. Wang et al. [74] proposed a novel graph neural network architecture search method, GPT4GNAS, which guides GPT-4 to understand the search space and search strategies of GNAS by designing a new type of prompt. These prompts are iteratively run to generate new GNN architectures, and the evaluation results are used as feedback to optimize the generated architectures further. ChatRule [75] utilizes LLMs to mine logical rules as well as learn and represent graph structures by combining semantic and structural information from Knowledge Graphs (KGs), helping encode and process graph structures. These rules can be regarded as new graph structures to construct new knowledge graphs by capturing the generative rules and patterns of graphs. Meanwhile, GNP [76] extracts valuable knowledge from knowledge graphs, and through graph neural network encoding, cross-modal pooling, and self-supervised learning, it significantly improves the performance of LLMs in common-sense reasoning and biomedical reasoning tasks.</p>
<p>In conclusion, when LLMs dominate downstream tasks such as prediction, classification, and reasoning, they demonstrate significant advantages over traditional GNNs, especially in zero-sample learning and processing textual attributes. LLMs can utilize their powerful text generation and comprehension capabilities to predict and classify the graph data directly without the complex structural processing required by GNNs. However, since every coin has two sides, each approach requires careful trade-offs of the advantages and disadvantages between LLMs and GNNs. Converting graph data into textual descriptions can simplify the processing flow and enable LLMs to utilize their textual processing capabilities for reasoning directly. Nevertheless, due to the input length limitation, this approach may result in the loss of graph structure information, and the text conversion process is so complex that it is unsuitable for processing complex graph data. On the other hand, Combining GNNs can fully utilize the information in the graph structure and enhance the processing capability of the model. However, this integration method increases the complexity of the system. Effective integration of GNNs with LLMs requires careful design and tuning to ensure that both can effectively</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: a) Text encoding and graph aggregation are iteratively executed through hierarchical Graph Neural Networks (GNN) and Transformers (TRM); b) Data from these two different modalities are projected into an aligned semantic embedding space, where attention mechanisms are used to learn the correlation rules between molecular substructures and textual keywords.
work together to handle complex graph data. To sum up, how to efficiently combine graph structural information with LLMs for more powerful graph learning and reasoning is the core of research in this area.</p>
<h1>3.3 GNN-LLM-co-driving</h1>
<p>In GNN-driving-LLM, LLMs act as the preprocessors of graph data, primarily converting the textual attributes of the graph into rich feature representations through their powerful text comprehension and generation capabilities and then passing these representations to GNNs for further structural processing. In this setup, LLMs play a role in information extraction and feature enhancement. In contrast, in LLM-driving-GNN, GNNs are mainly responsible for processing graph structural information and utilizing this structural information to enhance the prediction, classification, reasoning, and generation capabilities of LLMs.
GNN-LLM-co-driving synthesizes the strengths of GNNs and LLMs, leveraging their collaboration to solve complex tasks. Different from GNN-driving-LLM and LLM-driving-GNN, the co-driving strategy emphasizes the deep interaction and complementarity between GNNs and LLMs. In this architecture, GNNs and LLMs co-drive the learning process of the model, alternating and complementing each other. GNNs, with their advantage in graph structural information, help LLMs generate semantically deeper features under complex structures; at the same time, LLMs provide GNNs with more accurate node and edge representations through their strong ability in text sequence processing. This bidirectional interaction not only improves the model's comprehensive processing capability on graph structures and textual information but also enhances the robustness and generalization of the overall model, which can better cope with diverse tasks and datasets.</p>
<p>Typical co-driving models are the GraphFormers framework proposed by Yang et al. [77], as shown in figure 5 a). It creates a unified architecture capable of processing textual and graph structural information simultaneously by embedding GNNs into each transformer layer. The graph is first aggregated in each layer through GNN, bringing neighborhood information to the central node. Then, the enhanced node features are processed by the transformer to generate richer node representations. This allows the text encoding and graph aggregation processes to alternate in the same workflow, allowing nodes to exchange information at each layer to enhance the node representations with the information from neighboring nodes. PATTON [78] adopts the GraphFormers framework and designs two pretraining strategies-Network Contextualized Masked Language Modeling (NMLM) and Masked Node Prediction (MNP)—based on it. By jointly optimizing the two objective functions NMLM and MNP to carry out the pretraining process, this approach enhances the model's semantic understanding at both the vocabulary and document levels, enabling the pre-trained language model to sufficiently understand and represent the complex semantic information in rich textual networks. Zhao et al. [79] propose the GLEM model, which combines GNNs and LLMs and alternately updates LLMs and GNNs to generate pseudo-labels with each other through a Variational Expectation-Maximization (EM) framework. By integrating textual semantics and graph structural information, GLEM can effectively perform node representation learning on large-scale text-attributed graphs and improve node classification performance. GREASELM [80] uses a modality interaction mechanism to facilitate bidirectional information transfer between each layer of the LM and the</p>
<p>GNN, deeply integrating language context with knowledge graph representations to realize joint reasoning to answer complex questions.</p>
<p>Unlike the aforementioned architectures that combine GNN and LLM models, Text2Mol [81] employs two independent encoders for textual and molecular representations, respectively. By projecting the data from these two modalities into an aligned semantic embedding space, the model achieves a cross-modal search for retrieving molecules from natural language descriptions. To enhance the interpretability of the model, the Text2Mol framework introduces a cross-modal attention model based on the Transformer decoder. The model utilizes the output of SciBERT [105] as the source sequence and the node representations generated by the GCN model as the target sequence, learning the association rules between molecule substructures and text keywords through an attention mechanism. The architecture is shown in figure 5 b). Similarly, methods such as MoleculeSTM [82], CLAMP [83], ConGraT [84], G2P2 [85], GRENADE [86] also employ independent GNN encoders and LLMs encoders to process molecular and textual data separately, and then map the embedded representations to a shared joint representation space for contrastive learning. However, these models differ in implementation details or in the specific application scenarios they are applicable to. MoleculeSTM focuses on solving new challenges in drug design, such as structure-text retrieval and text-based molecular editing, and constructs PubChemSTM, a multimodal dataset containing a large number of chemical structure-text pairs. CLAMP, on the other hand, is primarily dedicated to zero-sample bio-activity prediction. Pretraining on large-scale chemical databases (such as PubChem) containing molecular structures, text descriptions, and bioactivity measurement data significantly enhances the generalization ability of activity prediction models. ConGraT connects an adapter module after each of the text encoder and graph node encoders, respectively, which consists of two fully connected layers to generate text embeddings and graph node embeddings of the same dimension. G2P2 and GRENADE take a further step by employing contrastive learning strategies. G2P2 enhances the granularity of contrastive learning by jointly training its graph encoder and text encoder with three graph-based contrast strategies (text-node interaction, text-summary interaction, and node-summary interaction) during the pretraining phase to jointly train the graph encoder and text encoder. This allows for the alignment of graph node and text representations in a bimodal embedding space, enabling better capture of fine-grained semantic information in the text while leveraging graph structures to enhance classification model performance. On the other hand, GRENADE jointly optimizes the pre-trained language model encoder and the graph neural network encoder through two self-supervised learning algorithms including graph-centered contrastive learning [106] and graph-centered knowledge alignment.
In order to achieve efficient node classification on textual graphs, GraD [87] employs the concept of knowledge distillation. The core idea is to transfer the graph structure information from the GNN teacher model to the graph-free student model through the distillation process. The student model does not need to use the graph structure during reasoning, thus significantly improving the reasoning efficiency and realizing efficient and accurate node classification. In order to introduce between models with different coupling strengths and flexibility, GraD also proposes three different optimization strategies, namely GraD-Joint, GraD-Alt, and GraD-JKD. Similarly, the THLM [88] framework combines BERT with the heterogeneous graph neural network R-HGNN [107], and through topology-aware pretraining tasks and text augmentation strategies, it pretrains on Text-Attributed Heterogeneous Graphs (TAHGs). After the pretraining phase, the THLM framework retains only the language model for downstream tasks and no longer relies on the auxiliary HGNN, ensuring efficiency and flexibility in processing downstream tasks. Some studies focus on combining LLMs and GNNs in the context of TAGs to reduce training complexity and memory consumption while maintaining the model's expressiveness. GraphAdapter [89] combines GNNs and LLMs specifically for processing TAGs. It firstly uses GNNs for each node in the TAG to model its structural information, then integrates the structural information with context-hidden states of LLMs, and finally transforms the original task into a next-word prediction task by adding task-specific prompts. Its lightweight design, residual connections, and task-related prompts enable the method to exhibit high performance in various downstream tasks, validating its effectiveness in TAG modeling. ENGINE [90] through a tunable bypass structure-G-Ladder-combining LLMs and GNNs for efficient fine-tuning and reasoning on TAGs. The framework significantly reduces memory and computational costs while preserving structural information through the introduced lightweight G-Ladder structure that adds tunable parameters next to each layer of LLMs. To further improve efficiency, a caching mechanism is also introduced to precompute the node embeddings during the training process, and dynamic early stop is used to accelerate model inference during the reasoning process.
Compared with GNN-driving-LLM and LLM-driving-GNN, the GNN-LLM-co-driving mode emphasizes the deep interaction and complementarity between the GNNs and LLMs. In this mode, GNN and LLM alternate and enhance each other in the learning process, thereby demonstrating higher robustness and generalization in the integrated processing of graph structure and text information. This co-driving strategy can solve complex tasks efficiently by integrating the graph structure processing capability of GNNs and the text comprehension capability of LLMs.</p>
<p>Table 1: A summary of LLMs used in the field of graph mining, highlighting the model architecture, which includes the LLM model, the GNN model, whether the parameters of the LLM are fine-tuned, the predictor of the architecture, the types of datasets, and the downstream task types. In the "Task" column, "Node" denotes node-level tasks, "Link" denotes edge-level tasks, and "Graph" denotes graph-level tasks.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Architecture</th>
<th></th>
<th></th>
<th>Graph Data</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The citation network uses directed edges to represent the citation relationships between articles. Green, blue, and red nodes represent literature categorized as "GNN-driving-LLM", "LLM-driving-GNN" and "GNN-LLM-codriving" respectively. The size of the nodes reflects the number of citations of each paper.</p>
<p>Table 2: Summary of experimental setups on selected models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">GNNs</th>
<th style="text-align: center;">Environment</th>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">ratio*</th>
<th style="text-align: center;">Code</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Peak Classification</td>
<td style="text-align: center;">TAPE-GNN- $0 \times 45 \mathrm{E}[47]$</td>
<td style="text-align: center;">DeBERTa-base</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX AN880 24GB GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">77.50 $\pm 0.12$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLEM-G [79]</td>
<td style="text-align: center;">DeBERTa-base</td>
<td style="text-align: center;">RevGAT</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX AN880 24GB GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">76.57 $\pm 0.29$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OFA [50]</td>
<td style="text-align: center;">llama2-13b</td>
<td style="text-align: center;">R-GCN</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">77.51 $\pm 0.17$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InstructGLM [71]</td>
<td style="text-align: center;">Llama-7b</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$4 \times 40 \mathrm{G}$ Nvidia A100 GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.70 $\pm 0.12$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphGPT-7B-v1.5-stage2 [65]</td>
<td style="text-align: center;">Vicuna</td>
<td style="text-align: center;">Graph Transformer</td>
<td style="text-align: center;">$4 \times 40 \mathrm{G}$ Nvidia A100 GPUs</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.11</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GRENADE [86]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">RevGAT-KD</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">76.21 $\pm 0.17$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraDBERT [87]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">GraphSAGE</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">ogbn-arxiv</td>
<td style="text-align: center;">75.05 $\pm 0.11$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GraphAdapter [89]</td>
<td style="text-align: center;">Llama 2</td>
<td style="text-align: center;">GraphSAGE</td>
<td style="text-align: center;">Nvidia A800 80GB GPU</td>
<td style="text-align: center;">Ogbn-arxiv</td>
<td style="text-align: center;">77.07 $\pm 0.15$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ENGINE [90]</td>
<td style="text-align: center;">LLaMA2-7B</td>
<td style="text-align: center;">GCN,SAGE,GAT</td>
<td style="text-align: center;">$6 \times$ Nvidia RTX 3090 GPUs</td>
<td style="text-align: center;">Ogbn-arxiv</td>
<td style="text-align: center;">76.02 $\pm 0.29$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PATTON [78]</td>
<td style="text-align: center;">BERT, SciBERT</td>
<td style="text-align: center;">GraphFormers</td>
<td style="text-align: center;">$4 \times$ Nvidia A6000 GPUs</td>
<td style="text-align: center;">Amazon-Sports</td>
<td style="text-align: center;">78.60 $\pm 0.15$</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">THEM [88]</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">R-HGNN</td>
<td style="text-align: center;">$4 \times$ Nvidia RTX 3090 GPUs</td>
<td style="text-align: center;">GoodReads</td>
<td style="text-align: center;">81.57</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLMRec [45]</td>
<td style="text-align: center;">gpt-3.5-turbo, text-embedding-ada-002</td>
<td style="text-align: center;">LightGCN</td>
<td style="text-align: center;">Nvidia RTX 3090 GPU</td>
<td style="text-align: center;">MovieLens</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">Link</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RLMRec [46]</td>
<td style="text-align: center;">gpt-3.5-turbo, text-embedding-ada-002</td>
<td style="text-align: center;">LightGCN</td>
<td style="text-align: center;">Nvidia RTX 3090 GPU</td>
<td style="text-align: center;">Amazon-book</td>
<td style="text-align: center;">14.83</td>
<td style="text-align: center;">Link</td>
</tr>
</tbody>
</table>
<p><em>In the ratio, node classification tasks are evaluated by accuracy, while recommendation tasks are evaluated by recall.
</em>*Recommendation.</p>
<h1>4.2 Discussion Analysis</h1>
<p>When evaluating models that combine LLMs with GNNs, the choice of test environment and test data is crucial to ensure the reliability and fairness of the results. In this section, the test environments of existing models and the selection of test data are summarized and described, and their comparison is shown in Table 2. As different test datasets are used in various studies covering multiple graph mining tasks, such as node classification, link prediction, and graph classification, in order to facilitate the comparison, we try to select a unified dataset with downstream task types as the classification. Due to the variety of graph-level tasks, it is inconvenient to perform comparative analysis, and only the data of node classification and recommendation tasks are selected for presentation here. In addition, to ensure the reproducibility of the results, we only selected the open source data. The table lists the hardware configurations (e.g., GPU model and number) used for each model, the specific test datasets, as well as the best-performing large language model and graph neural network model in the test. The organization of this information can help readers understand each model's computational efficiency and applicability and also provides a reliable reference for subsequent research for performance comparison and technology validation under different experimental conditions.</p>
<p>During the collation process, we observed that all models show improvements relative to the baseline, which powerfully demonstrates that combining LLMs and GNNs holds more incredible promise than using either model individually. Specifically, combining the robust language understanding and generation capabilities of LLMs with the graph-structured data processing strengths of GNNs results in a significant improvement in overall performance. This cross-model synergy not only improves the understanding of complex graph data but also enhances the model's performance in tasks</p>
<p>such as node classification, showing great potential for joint applications. This finding provides important insights for future research, emphasizing the importance of multi-model collaboration.
However, while we observe performance improvements across models for specific tasks, the overall improvement remains modest. For example, the recently proposed TAPE [47] model only improves by $1.6 \%$ over the baseline. This phenomenon may stem from the fact that the current method of combining LLMs and GNNs is too simple, which is more of a "stitching" rather than a true deep fusion. This simple combination fails to fully utilize the respective strengths of the two models, nor does it achieve true complementarity between them in terms of feature extraction and representation learning. As a result, despite some degree of performance improvement, the expected significant optimization is not achieved. This suggests that future research must explore more complex and efficient integration strategies to achieve deep synergy between LLMs and GNNs, thereby driving further improvement in model performance.</p>
<h1>5 Future Direction</h1>
<p>Based on the above analysis, it is clear that there are still many directions in this research area that have yet to be fully explored and deeply understood. Therefore, this section will further analyze these issues, focusing on the drawbacks and potential research opportunities in the current study, with a view to providing new ideas and insights for future academic exploration.</p>
<h3>5.1 Multimodal Graph Data Processing</h3>
<p>In graph data, nodes may be enriched with information in multiple modalities, such as text, images, and videos. These modalities may contain rich information, so understanding these multimodal data can help improve graph learning. A number of recent studies have explored the ability of LLMs to process and integrate multimodal data, and these studies have shown that LLMs exhibit significant capabilities in this area [108, 109], which makes it possible to apply LLMs to multimodal graph data. Future research will focus on exploring how to design a unified model to jointly encode data in different modalities such as graphs, text, and images. This will be applied to areas such as social network analysis, product recommendation, and molecular modeling to enhance the performance of models in complex, multimodal scenarios.</p>
<h3>5.2 Addressing the Hallucination Problem in Large Language Models</h3>
<p>While LLMs have shown a fantastic ability to generate text, they are prone to hallucinations and misinformation due to the fact that they tend to generate answers in a single pass and lack the ability to adjust dynamically. Hallucination means that the information generated by the model seems reasonable but is actually inaccurate, deviating from the user input, context, or even from the facts [110]. In specific sophisticated fields, such misinformation is unacceptable. Therefore, future research directions should focus on solving the hallucination problem and reducing the generation of misinformation, and this can be accomplished with the help of graph data. For example, it can be done by combining external knowledge graphs so that the big language model can reason step-by-step in generating answers and refer to reliable structured data sources to verify the accuracy of the information. Furthermore, using multi-hop reasoning and dynamic knowledge retrieval mechanisms enables the model to continuously adjust and correct its output according to the context, thus providing more accurate and trustworthy answers. By employing these strategies, the model will be more stable and reliable, especially in application scenarios that require high accuracy.</p>
<h3>5.3 Enhancing the Capability to Solve Complex Graph Tasks</h3>
<p>Currently, LLMs are primarily applied to basic graph tasks such as node classification and link prediction, but the remarkable capabilities that LLMs demonstrate in various areas suggest that their potential in graph data extends beyond these tasks. As a result, more and more research has explored their application to more complex graph tasks, such as graph generation [111], question answering over knowledge graph [112] and knowledge graph construction [113]. LLMs can be used to generate novel molecular structures, analyze complex relationship patterns in social networks, or assist in constructing more contextually connected knowledge graphs. The solution to these complex tasks will drive the further development of LLMs in a variety of fields, including biomedicine, social network analysis, and natural language processing.</p>
<p>Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</p>
<h2>6 Conclusion</h2>
<p>In recent years, significant progress has been made in the application of LLMs in the field of graph mining. This study aims to provide an overview, summarize the research in this area, and provide potential directions for future research. We propose a new taxonomy based on different driving modes: GNN-driving-LLM, LLM-driving-GNN and GNN-LLM-co-driving. Each mode exhibits unique advantages and application potentials, especially when dealing with complex graph structures and textual information. The combination of LLMs and GNNs has brought new opportunities to graph mining. The semantic understanding capability of LLMs complements the structural information processing capability of GNNs, significantly improving the effectiveness of graph mining tasks. Despite the many opportunities presented by the combination of GNNs and LLMs, their high computational demands and model complexity remain challenges. Future research should explore optimizing the integration model of GNNs and LLMs to achieve more powerful graph learning and reasoning capabilities while ensuring computational efficiency, thus advancing the field of graph mining.</p>
<h2>References</h2>
<p>[1] Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and Min-Yen Kan. Fang: Leveraging social context for fake news detection using graph representation. In Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management, CIKM '20. ACM, October 2020.
[2] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013.
[3] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701-710, 2014.
[4] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016.
[5] Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs, 2017.
[6] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
[7] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks, 2017.
[8] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks, 2018.
[9] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs, 2018.
[10] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Peng Cui, P. Yu, and Yanfang Ye. Heterogeneous graph attention network, 2021.
[11] Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V. Chawla. Heterogeneous graph neural network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '19, page 793-803, New York, NY, USA, 2019. Association for Computing Machinery.
[12] Christopher D. Manning. Human Language Understanding \&amp; Reasoning. Daedalus, 151(2):127-138, 05 2022.
[13] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language models as knowledge bases?, 2019.
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[16] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021.</p>
<p>[17] Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets llms: Towards large graph models, 2023.
[18] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms) in learning on graphs, 2024.
[19] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. Towards graph foundation models: A survey and beyond, 2024.
[20] Ling Yang, Jiayi Zheng, Heyuan Wang, Zhongyi Liu, Zhilin Huang, Shenda Hong, Wentao Zhang, and Bin Cui. Individual and structural graph information bottlenecks for out-of-distribution generalization, 2023.
[21] Anton Korshunov, Ivan Beloborodov, Nazar Buzun, Valeriy Avanesov, and Sergey Kuznetsov. Social network analysis: methods and applications. Proceedings of the Institute for System Programming of RAS, 26(1):439-456, 2014.
[22] F. O. Isinkaye, Y. O. Folajimi, and B.A. Ojokoh. Recommendation systems: Principles, methods and evaluation. Egyptian Informatics Journal, 16(3):261-273, 2015.
[23] Xifeng Yan and Jiawei Han. gspan: Graph-based substructure pattern mining. In 2002 IEEE International Conference on Data Mining, 2002. Proceedings., pages 721-724. IEEE, 2002.
[24] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, $\operatorname{PP}(99), 2021$.
[25] David Liben-Nowell and Jon Kleinberg. The link prediction problem for social networks. In Proceedings of the twelfth international conference on Information and knowledge management, pages 556-559, 2003.
[26] Lars Backstrom and Jure Leskovec. Supervised random walks: predicting and recommending links in social networks. In Proceedings of the fourth ACM international conference on Web search and data mining, pages $635-644,2011$.
[27] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural information processing systems, 31, 2018.
[28] Satu Elisa Schaeffer. Graph clustering. Computer science review, 1(1):27-64, 2007.
[29] Donatello Conte, Pasquale Foggia, Carlo Sansone, and Mario Vento. Thirty years of graph matching in pattern recognition. International journal of pattern recognition and artificial intelligence, 18(03):265-298, 2004.
[30] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75-174, 2010.
[31] Michelle Girvan and Mark EJ Newman. Community structure in social and biological networks. Proceedings of the national academy of sciences, 99(12):7821-7826, 2002.
[32] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008.
[33] Akihiro Inokuchi, Takashi Washio, and Hiroshi Motoda. An apriori-based algorithm for mining frequent substructures from graph data. In Principles of Data Mining and Knowledge Discovery: 4th European Conference, PKDD 2000 Lyon, France, September 13-16, 2000 Proceedings 4, pages 13-23. Springer, 2000.
[34] Christian Borgelt and Michael R Berthold. Mining molecular fragments: Finding relevant substructures of molecules. In 2002 IEEE International Conference on Data Mining, 2002. Proceedings., pages 51-58. IEEE, 2002.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[37] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[38] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen,</p>
<p>Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[40] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2020.
[41] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.
[42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020.
[43] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations, 2020.
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
[45] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Llmrec: Large language models with graph augmentation for recommendation, 2024.
[46] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024, WWW '24. ACM, May 2024.</p>
<p>[47] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning, 2024.
[48] Qian Huang, Hongyu Ren, Peng Chen, Gregor Kržmanc, Daniel Zeng, Percy Liang, and Jure Leskovec. Prodigy: Enabling in-context learning over graphs, 2023.
[49] Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks, 2023.
[50] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One for all: Towards training one graph model for all classification tasks, 2024.
[51] Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu, Belinda Zeng, and Trishul Chilimbi. Graph-aware language model pre-training on a large graph corpus can help multiple graph applications, 2023.
[52] Rui Xue, Xipeng Shen, Ruozhou Yu, and Xiaorui Liu. Efficient large language models fine-tuning on graphs, 2023.
[53] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored search, 2021.
[54] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. Textgnn: Improving text encoder via graph neural network in sponsored search. In Proceedings of the Web Conference 2021, WWW '21. ACM, April 2021.
[55] Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, and Chao Huang. Graphedit: Large language models for graph structure learning, 2024.
[56] Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, and Xuecang Zhang. Leveraging large language models for node generation in few-shot learning on text-attributed graphs, 2024.
[57] Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node classification on graphs with large language models (llms), 2024.
[58] Lianghao Xia, Ben Kao, and Chao Huang. Opengraph: Towards open graph foundation models, 2024.
[59] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space, 2023.
[60] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language models, 2023.
[61] Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, and Shi Han. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking, 2023.
[62] Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Disentangled representation learning with large language models for text-attributed graphs, 2024.
[63] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?, 2024.
[64] Chang Liu and Bo Wu. Evaluating large language models on graphs: Performance insights and comparative analysis, 2023.
[65] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models, 2024.
[66] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery, 2023.
[67] Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. Graphtranslator: Aligning graph model to large language model for open-ended tasks, 2024.
[68] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. Higpt: Heterogeneous graph language model, 2024.
[69] Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning, 2023.
[70] Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, and Carl Yang. Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining, 2024.
[71] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Language is all a graph needs, 2024.</p>
<p>[72] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model, 2023.
[73] Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt, 2023.
[74] Haishuai Wang, Yang Gao, Xin Zheng, Peng Zhang, Hongyang Chen, Jiajun Bu, and Philip S. Yu. Graph neural architecture search with gpt-4, 2024.
[75] Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Chatrule: Mining logical rules with large language models for knowledge graph reasoning, 2024.
[76] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V. Chawla, and Panpan Xu. Graph neural prompting with large language models, 2023.
[77] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning on textual graph, 2023.
[78] Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, and Jiawei Han. Patton: Language model pretraining on text-rich networks, 2023.
[79] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference, 2023.
[80] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and Jure Leskovec. Greaselm: Graph reasoning enhanced language models for question answering, 2022.
[81] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with natural language queries. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
[82] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing, 2024.
[83] Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Günter Klambauer. Enhancing activity prediction models in drug discovery with the ability to understand human language, 2023.
[84] William Brannon, Wonjune Kang, Suyash Fulay, Hang Jiang, Brandon Roy, Deb Roy, and Jad Kabbara. Congrat: Self-supervised contrastive pretraining for joint graph and text embeddings, 2024.
[85] Zhihao Wen and Yuan Fang. Prompt tuning on graph-augmented low-resource text classification, 2024.
[86] Yichuan Li, Kaize Ding, and Kyumin Lee. Grenade: Graph-centric language model for self-supervised representation learning on text-attributed graphs, 2023.
[87] Costas Mavromatis, Vassilis N. Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos Faloutsos, and George Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs, 2023.
[88] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bowen Du. Pretraining language models with text-attributed heterogeneous graphs, 2023.
[89] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu. Can gnn be good adapter for llms?, 2024.
[90] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. Efficient tuning and inference for large language models on textual graphs, 2024.
[91] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165-172, 2013.
[92] Mark S Granovetter. The strength of weak ties. American journal of sociology, 78(6):1360-1380, 1973.
[93] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7370-7377, 2019.
[94] Derek J De Solla Price. Networks of scientific papers: The pattern of bibliographic references indicates the nature of the scientific research front. Science, 149(3683):510-515, 1965.
[95] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings, 2016.
[96] Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt a good recommender? a preliminary study, 2023.</p>
<p>[97] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022.
[98] Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences, 2023.
[99] Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can llms effectively leverage graph structural information through prompts, and why?, 2024.
[100] Yuntong Hu, Zheng Zhang, and Liang Zhao. Beyond text: A deep dive into large language models' ability on understanding graph data, 2023.
[101] Ulrik Brandes, Markus Eiglsperger, Jürgen Lerner, and Christian Pich. Graph markup language (graphml). 2013.
[102] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
[103] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https : //github.com/kingoflolz/mesh-transformer-jax, May 2021.
[104] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
[105] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text, 2019.
[106] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations, 2021.
[107] Le Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng Lv, and Hui Xiong. Heterogeneous graph representation learning with relation awareness. IEEE Transactions on Knowledge and Data Engineering, page 1-1, 2022.
[108] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm, 2024.
[109] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
[110] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren's song in the ai ocean: A survey on hallucination in large language models, 2023.
[111] Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, and Hong Mei. Exploring the potential of large language models in graph generation, 2024.
[112] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages $105-113,2019$.
[113] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. Knowledge graphs: Opportunities and challenges, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
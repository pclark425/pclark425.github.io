<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9715 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9715</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9715</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273654506</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.20266v1.pdf" target="_blank">Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks</a></p>
                <p><strong>Paper Abstract:</strong> The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts. Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following. However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain. In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts. Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference. Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions. Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks. We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9715.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9715.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge evaluation approach for domain-specific expert tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison of an LLM (GPT-4) used as an automatic judge versus subject matter experts (SMEs) on pairwise evaluations of LLM outputs in dietetics and mental health; measures agreement, analyzes explanation differences, and identifies where LLM judging degrades relative to human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Domain-specific instruction-following / expert knowledge tasks (dietetics and mental health)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (used as judge; evaluated in general persona and an expert-persona variant)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>AlpacaEval-style pairwise comparison: same 25 prompt instructions and two candidate model outputs per prompt; GPT-4 prompted to choose preferred response overall and on two aspect questions, with a short 2–3 sentence explanation; comparisons run with both a default/general persona and a domain expert persona.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>SME pairwise survey: 10 registered dietitians and 10 clinical psychologists; each SME saw 25 instruction pairs, chose overall preference and answered two randomized aspect questions per item, and provided free-text explanations; order and response placement randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement between LLM judge and SMEs on the same items; reported SME–LLM agreement: dietetics ~64% (paper Results section) and mental health ~60% (Results); abstract also reports SME agreement 68% dietetics / 64% mental health (two reported figures in paper); SME–SME agreement baseline: dietetics 75%, mental health 72%; lay-user–LLM agreement (general persona) 80% in both domains; expert persona increased SME–LLM general-preference agreement by ~4% but decreased lay-user–LLM agreement (to 76%).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as sole judges loses domain-specific critical judgment: inability to reliably detect harmful or inaccurate recommendations, failure to flag gaps or missing essential details, tendency to repeat or echo response content rather than add domain insight, different notion of clarity (equates clarity with detail instead of concise patient-facing language), reduced context-specific reasoning and practical clinical judgement, and alignment toward lay-user preferences due to RLHF bias.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Qualitative examples: LLMs often overlooked 'harmful or inaccurate aspects' flagged by SMEs (e.g., a dietitian pointing out an incorrect claim about carbohydrates raising risk; a psychologist noting that deriving a diagnosis from nonspecific symptoms is 'hugely problematic'). LLM explanations tended to restate output details and favor comprehensive/detail-heavy responses, while SMEs penalized overly detailed responses that could overwhelm patients. Expert persona sometimes failed to detect risks (e.g., mental-health clarity issues) and did not consistently improve aspect-level alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM judges aligned better with lay users (80% agreement) than with SMEs, suggesting LLMs reflect lay preferences; expert-persona prompting improved SME alignment modestly (~4% for general preference) and LLMs sometimes matched SMEs on particular categories (e.g., some alignment on referrals, positive tone, and in some Accuracy/aspect questions, especially in mental health). LLMs are useful for scalable filtering (eliminating worst models) and large-scale evaluations but not as a full replacement for SMEs in high-stakes domain-specific judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks — Abstract; Sections 3 (Methods), 4 (Analysis Methods), 5 (Results, esp. 5.1–5.3), 6 (Lay User Alignment), and 7 (Discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9715.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9715.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dietetics comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge and registered dietitians in dietetics tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Domain-specific analysis showing lower alignment between GPT-4 judged preferences and registered dietitians on aspect questions (accuracy, personalization, education context), with dietitians providing more specific, practical, and safety-conscious feedback than the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dietetics — nutrition advice, disease management, allergies, personalization</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (general persona and 'expert dietitian' persona)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise comparisons using AlpacaEval modifications on 25 dietetics instructions; GPT-4 provided overall and aspect-specific preferences plus 2–3 sentence explanations; expert persona variant instructed to act as a dietitian.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>10 registered dietitians (mean experience ~9.6 years) completed 25 pairwise evaluations each, selecting overall preference and two aspect questions per item; provided brief free-text justifications; response order randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement: SME–LLM reported ~64% agreement (Results section) for overall preferences in dietetics (abstract reports 68%); SME–SME agreement in dietetics: 75%. Agreement on aspect questions was lower and variable across categories; expert persona yielded slight improvements in some aspects but not consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When using LLM judges instead of dietitians, the evaluation loses: detection of factual inaccuracies and potentially harmful recommendations (e.g., outdated or misleading diet claims), fine-grained personalization details (cultural, allergies, exact nutrient prescriptions), and SME-style practical prioritization of concise, patient-facing wording; LLMs favored detail/volume over SME-preferred clarity and actionable specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Dietitian flagged incorrect claim: 'carbohydrates alone do not increase the risk necessarily...' which LLM judges failed to penalize appropriately; SMEs favored responses that prioritized key actionable nutrition factors (e.g., specific protein/sodium guidance) and culturally relevant recommendations, whereas LLM explanations emphasized comprehensiveness rather than practical applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>In some professional-standard questions LLMs aligned reasonably with dietitians; expert persona sometimes improved alignment for general preference. Authors note domain-specific challenges (conflicting dietary advice, evolving guidelines) that make automated judgment harder, implying that with domain-specific tuning/persona/data the gap might shrink.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3.2 (Dietetics instructions), 5.1–5.3 (Results), 7.1.2–7.1.3 (Discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9715.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9715.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mental-health comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-Judge and clinical psychologists in mental health tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of GPT-4 judge versus clinical psychologists on mental-health instruction outputs, showing mismatches in clarity, harm-detection, and tone — LLMs tended to prefer detailed/technical answers while SMEs prioritized concise, empathic, safety-oriented communication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mental health — conversational support, emotional support, counseling-related guidance</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (general persona and 'clinical psychologist' expert persona)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>AlpacaEval-style pairwise comparisons on 25 mental-health instructions; GPT-4 selected overall and aspect-specific preferences and produced short explanations; an expert persona variant instructed it to act as a clinical psychologist.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>10 clinical psychologists (mean experience ~15.2 years) conducted 25 pairwise evaluations each, choosing overall preference and two aspect questions per item, plus short explanations; randomized presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement: SME–LLM reported ~60% agreement in mental health (Results section), SME–SME agreement 72%; abstract reports 64% for mental health in one place (paper reports both figures in different sections). Expert persona improved SME–LLM agreement modestly for general preference but worsened agreement on Clarity in mental health.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs instead of clinical psychologists loses expert sensitivity to therapeutic risk and client safety (failure to flag content that could encourage self-diagnosis or harm), misaligned notion of clarity (LLM equates clarity with comprehensiveness/technical detail while SMEs prefer concise, client-appropriate phrasing), and nuanced judgment about tone/empathy that affects patient outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Psychologist example: flagging responses that jump to diagnoses from nonspecific symptoms as 'hugely problematic' — LLMs did not consistently flag such risks. LLMs preferred more technical/verbose answers that SMEs worried could overwhelm or lead to misuse (e.g., self-diagnosis). Expert-persona LLM sometimes increased technical vocabulary and missed SME-flagged harms.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLMs aligned better with SMEs on some Accuracy and Educational Context questions in mental health than in dietetics, possibly due to well-structured mental-health resources in training data; LLMs did sometimes recommend seeking professional help and favored empathetic tones, aligning with SME preferences in those respects. Expert persona improved general-preference agreement but created trade-offs (decreased Clarity alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 3.2 (Mental health instructions), 5.1–5.3 (Results), 6 (Lay user alignment), and 7.1.1–7.1.2 (Discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humans or llms as the judge? a study on judgement biases <em>(Rating: 2)</em></li>
                <li>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An Automatic Evaluator of Instruction-following Models <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Judgelm: Fine-tuned large language models are scalable judges <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9715",
    "paper_id": "paper-273654506",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge (overall)",
            "name_full": "LLM-as-a-Judge evaluation approach for domain-specific expert tasks",
            "brief_description": "Empirical comparison of an LLM (GPT-4) used as an automatic judge versus subject matter experts (SMEs) on pairwise evaluations of LLM outputs in dietetics and mental health; measures agreement, analyzes explanation differences, and identifies where LLM judging degrades relative to human experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Domain-specific instruction-following / expert knowledge tasks (dietetics and mental health)",
            "llm_judge_model": "GPT-4 (used as judge; evaluated in general persona and an expert-persona variant)",
            "llm_judge_setup": "AlpacaEval-style pairwise comparison: same 25 prompt instructions and two candidate model outputs per prompt; GPT-4 prompted to choose preferred response overall and on two aspect questions, with a short 2–3 sentence explanation; comparisons run with both a default/general persona and a domain expert persona.",
            "human_evaluation_setup": "SME pairwise survey: 10 registered dietitians and 10 clinical psychologists; each SME saw 25 instruction pairs, chose overall preference and answered two randomized aspect questions per item, and provided free-text explanations; order and response placement randomized.",
            "agreement_metric": "Percent agreement between LLM judge and SMEs on the same items; reported SME–LLM agreement: dietetics ~64% (paper Results section) and mental health ~60% (Results); abstract also reports SME agreement 68% dietetics / 64% mental health (two reported figures in paper); SME–SME agreement baseline: dietetics 75%, mental health 72%; lay-user–LLM agreement (general persona) 80% in both domains; expert persona increased SME–LLM general-preference agreement by ~4% but decreased lay-user–LLM agreement (to 76%).",
            "losses_identified": "Using LLMs as sole judges loses domain-specific critical judgment: inability to reliably detect harmful or inaccurate recommendations, failure to flag gaps or missing essential details, tendency to repeat or echo response content rather than add domain insight, different notion of clarity (equates clarity with detail instead of concise patient-facing language), reduced context-specific reasoning and practical clinical judgement, and alignment toward lay-user preferences due to RLHF bias.",
            "examples_of_loss": "Qualitative examples: LLMs often overlooked 'harmful or inaccurate aspects' flagged by SMEs (e.g., a dietitian pointing out an incorrect claim about carbohydrates raising risk; a psychologist noting that deriving a diagnosis from nonspecific symptoms is 'hugely problematic'). LLM explanations tended to restate output details and favor comprehensive/detail-heavy responses, while SMEs penalized overly detailed responses that could overwhelm patients. Expert persona sometimes failed to detect risks (e.g., mental-health clarity issues) and did not consistently improve aspect-level alignment.",
            "counterexamples_or_caveats": "LLM judges aligned better with lay users (80% agreement) than with SMEs, suggesting LLMs reflect lay preferences; expert-persona prompting improved SME alignment modestly (~4% for general preference) and LLMs sometimes matched SMEs on particular categories (e.g., some alignment on referrals, positive tone, and in some Accuracy/aspect questions, especially in mental health). LLMs are useful for scalable filtering (eliminating worst models) and large-scale evaluations but not as a full replacement for SMEs in high-stakes domain-specific judgement.",
            "paper_reference": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks — Abstract; Sections 3 (Methods), 4 (Analysis Methods), 5 (Results, esp. 5.1–5.3), 6 (Lay User Alignment), and 7 (Discussion)",
            "uuid": "e9715.0",
            "source_info": {
                "paper_title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Dietetics comparison",
            "name_full": "Comparison of LLM-as-a-Judge and registered dietitians in dietetics tasks",
            "brief_description": "Domain-specific analysis showing lower alignment between GPT-4 judged preferences and registered dietitians on aspect questions (accuracy, personalization, education context), with dietitians providing more specific, practical, and safety-conscious feedback than the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dietetics — nutrition advice, disease management, allergies, personalization",
            "llm_judge_model": "GPT-4 (general persona and 'expert dietitian' persona)",
            "llm_judge_setup": "Pairwise comparisons using AlpacaEval modifications on 25 dietetics instructions; GPT-4 provided overall and aspect-specific preferences plus 2–3 sentence explanations; expert persona variant instructed to act as a dietitian.",
            "human_evaluation_setup": "10 registered dietitians (mean experience ~9.6 years) completed 25 pairwise evaluations each, selecting overall preference and two aspect questions per item; provided brief free-text justifications; response order randomized.",
            "agreement_metric": "Percent agreement: SME–LLM reported ~64% agreement (Results section) for overall preferences in dietetics (abstract reports 68%); SME–SME agreement in dietetics: 75%. Agreement on aspect questions was lower and variable across categories; expert persona yielded slight improvements in some aspects but not consistently.",
            "losses_identified": "When using LLM judges instead of dietitians, the evaluation loses: detection of factual inaccuracies and potentially harmful recommendations (e.g., outdated or misleading diet claims), fine-grained personalization details (cultural, allergies, exact nutrient prescriptions), and SME-style practical prioritization of concise, patient-facing wording; LLMs favored detail/volume over SME-preferred clarity and actionable specificity.",
            "examples_of_loss": "Dietitian flagged incorrect claim: 'carbohydrates alone do not increase the risk necessarily...' which LLM judges failed to penalize appropriately; SMEs favored responses that prioritized key actionable nutrition factors (e.g., specific protein/sodium guidance) and culturally relevant recommendations, whereas LLM explanations emphasized comprehensiveness rather than practical applicability.",
            "counterexamples_or_caveats": "In some professional-standard questions LLMs aligned reasonably with dietitians; expert persona sometimes improved alignment for general preference. Authors note domain-specific challenges (conflicting dietary advice, evolving guidelines) that make automated judgment harder, implying that with domain-specific tuning/persona/data the gap might shrink.",
            "paper_reference": "Sections 3.2 (Dietetics instructions), 5.1–5.3 (Results), 7.1.2–7.1.3 (Discussion)",
            "uuid": "e9715.1",
            "source_info": {
                "paper_title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mental-health comparison",
            "name_full": "Comparison of LLM-as-a-Judge and clinical psychologists in mental health tasks",
            "brief_description": "Study of GPT-4 judge versus clinical psychologists on mental-health instruction outputs, showing mismatches in clarity, harm-detection, and tone — LLMs tended to prefer detailed/technical answers while SMEs prioritized concise, empathic, safety-oriented communication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Mental health — conversational support, emotional support, counseling-related guidance",
            "llm_judge_model": "GPT-4 (general persona and 'clinical psychologist' expert persona)",
            "llm_judge_setup": "AlpacaEval-style pairwise comparisons on 25 mental-health instructions; GPT-4 selected overall and aspect-specific preferences and produced short explanations; an expert persona variant instructed it to act as a clinical psychologist.",
            "human_evaluation_setup": "10 clinical psychologists (mean experience ~15.2 years) conducted 25 pairwise evaluations each, choosing overall preference and two aspect questions per item, plus short explanations; randomized presentation.",
            "agreement_metric": "Percent agreement: SME–LLM reported ~60% agreement in mental health (Results section), SME–SME agreement 72%; abstract reports 64% for mental health in one place (paper reports both figures in different sections). Expert persona improved SME–LLM agreement modestly for general preference but worsened agreement on Clarity in mental health.",
            "losses_identified": "Using LLMs instead of clinical psychologists loses expert sensitivity to therapeutic risk and client safety (failure to flag content that could encourage self-diagnosis or harm), misaligned notion of clarity (LLM equates clarity with comprehensiveness/technical detail while SMEs prefer concise, client-appropriate phrasing), and nuanced judgment about tone/empathy that affects patient outcomes.",
            "examples_of_loss": "Psychologist example: flagging responses that jump to diagnoses from nonspecific symptoms as 'hugely problematic' — LLMs did not consistently flag such risks. LLMs preferred more technical/verbose answers that SMEs worried could overwhelm or lead to misuse (e.g., self-diagnosis). Expert-persona LLM sometimes increased technical vocabulary and missed SME-flagged harms.",
            "counterexamples_or_caveats": "LLMs aligned better with SMEs on some Accuracy and Educational Context questions in mental health than in dietetics, possibly due to well-structured mental-health resources in training data; LLMs did sometimes recommend seeking professional help and favored empathetic tones, aligning with SME preferences in those respects. Expert persona improved general-preference agreement but created trade-offs (decreased Clarity alignment).",
            "paper_reference": "Sections 3.2 (Mental health instructions), 5.1–5.3 (Results), 6 (Lay user alignment), and 7.1.1–7.1.2 (Discussion)",
            "uuid": "e9715.2",
            "source_info": {
                "paper_title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humans or llms as the judge? a study on judgement biases",
            "rating": 2,
            "sanitized_title": "humans_or_llms_as_the_judge_a_study_on_judgement_biases"
        },
        {
            "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
            "rating": 2,
            "sanitized_title": "who_validates_the_validators_aligning_llmassisted_evaluation_of_llm_outputs_with_human_preferences"
        },
        {
            "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Judgelm: Fine-tuned large language models are scalable judges",
            "rating": 2,
            "sanitized_title": "judgelm_finetuned_large_language_models_are_scalable_judges"
        }
    ],
    "cost": 0.011469499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks
26 Oct 2024</p>
<p>Annalisa Szymanski 
Equal Contribution</p>
<p>Equal Contribution</p>
<p>Equal Contribution</p>
<p>Meng Jiang mjiang2@nd.edu 
Ronald A Metoyer rmetoyer@nd.edu 
Noah Ziems nziems2@nd.edu 
Equal Contribution</p>
<p>Heather A Eicher-Miller 
Jia-Jun Li </p>
<p>University of Notre Dame
USA</p>
<p>NOAH ZIEMS *
University of Notre Dame
USA</p>
<p>HEATHER A. EICHER-MILLER
Purdue University
USA</p>
<p>TOBY JIA-JUN LI
University of Notre Dame
USA</p>
<p>University of Notre Dame
USA</p>
<p>University of Notre Dame
USA</p>
<p>University of Notre Dame
Notre Dame
INUSA</p>
<p>Noah Ziems *</p>
<p>University of Notre Dame
Notre Dame
INUSA</p>
<p>Purdue University
West LafayetteINUSA</p>
<p>Toby Jia-Jun Li</p>
<p>University of Notre Dame
Notre Dame
Meng JiangINUSA</p>
<p>University of Notre Dame
Notre Dame
INUSA</p>
<p>University of Notre Dame
Notre Dame
INUSA</p>
<p>Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks
26 Oct 2024884B93BBAF6270BBE8EC5C671A4A59E210.1145/nnnnnnn.nnnnnnnarXiv:2410.20266v1[cs.HC]Manuscript submitted to ACM Manuscript submitted to ACM
The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts.Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following.However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain.In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks.We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts.Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference.Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions.Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks.We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.CCS Concepts: • Human-centered computing → Human computer interaction (HCI); • Computing methodologies → Natural language generation.</p>
<p>INTRODUCTION</p>
<p>The ability of large language models (LLMs) to follow natural language instructions allows for an enormous number of downstream use cases in the real world [5,64].Recently, there has been significant research interest in leveraging LLMs for complex knowledge-based tasks in professional fields such as healthcare, mental health, legal issues, and more [19,48].These tasks often involve question answering and task generation, providing critical benefits, particularly in areas where access to the expertise of trained professionals is limited or where underprivileged communities may lack necessary services [29].</p>
<p>However, it remains to be seen how to best evaluate the quality of model outputs for these tasks.Traditional automated metrics such as BLEU, ROUGE, and Exact Match are commonly used to assess short, well-defined text generations consisting of only a few tokens [34,46].These metrics become less useful for open-ended tasks where multiple valid responses exist.Other approaches, such as BERTScore, measure semantic overlap in meaning between texts but fail to capture user preferences, domain-specific nuances, or the complexity of reasoning required for tasks that need expert judgement [62].As a result, many tasks requiring open-ended generation rely on human evaluators to judge the correctness of a language model's output.While effective, human evaluation is often expensive, slow, and inconsistent in terms of preferences [65].</p>
<p>To address these challenges, the use of LLMs as evaluators has gained popularity within the NLP and HCI community, as they provide an inexpensive and reproducible evaluation compared to human evaluators [45].These approaches typically involve prompting the LLM evaluator, often referred to as a judge, to perform a pairwise comparison between the outputs of different language models or a single-point evaluation using specific criteria [4,12].Previous studies have shown that LLMs are a promising alternative to human expert evaluations [15,55,65].However, there have been several problems with using LLMs as evaluators, such as positional bias, knowledge bias, and format bias [66].Although there are approaches to solve these issues, such as randomizing the position of the responses, it remains to be seen whether they can be completely eliminated [66].This challenge is further complicated by tasks that require complex reasoning or cognitive processes.Evaluating such tasks often involves a combination of cognitive strategies including analytical reasoning, intuitive thinking, and pattern recognition, while incorporating elements of practical expertise [57].This complexity adds a layer of difficulty in both the performance and evaluation of LLM outputs for these tasks.This presents a critical gap in understanding how to best assess the performance of LLMs for domain-specific complex tasks, as it remains unclear when and how to rely on SME evaluations versus LLM-based assessments, and how to effectively integrate the strengths of both.Misalignments between LLM outputs and expert judgments can lead to missed opportunities to integrate LLMs into human decision-making processes, which could otherwise reduce costs and enable faster iterations.The lack of clear guidelines on how or when to integrate LLMs or SMEs into the evaluation process could lead to both resource misallocation or sub-optimal performance in high stake domains, such as healthcare.</p>
<p>Additionally, understanding the boundaries of LLM capabilities and their alignment with expert standards is essential for developing more reliable evaluation pipelines that can balance cost effectiveness with the precision required for expert-level tasks.</p>
<p>In this paper, we aim to understand the alignment between LLMs and SMEs as judges for domain-specific tasks that require complex reasoning and cognitive processes.Our work is driven by the following research questions:</p>
<p>• RQ1: How does the LLM-as-a-Judge evaluation approach compare to the evaluations conducted by SMEs for domain specific tasks?• RQ2: What are the main factors contributing to the evaluation differences and associated explanations between</p>
<p>LLMs and SMEs?</p>
<p>To answer our research questions, we conduct a case study in the fields of mental health and dietetics where the tasks are complex and may require expert knowledge.These fields are of particular importance because they represent Manuscript submitted to ACM complex domains that could bring significant societal benefits by improving access to their services.However, a lack of understanding of how LLMs perform on these tasks presents a barrier to adoption.Through a mixed-methods approach, we conducted a pairwise comparison study where both SMEs and LLMs evaluate outputs related to domain-specific tasks and provide explanations for their choices.</p>
<p>Based on our analysis, our findings contribute new insights into how the LLM-as-a-Judge evaluation approach compares to evaluations conducted by SMEs and the variability seen between different expert-level tasks.We observe patterns in LLM agreement and discuss potential reasons for them, as well as key design implications of LLM evaluations for domain-specific tasks.This paper makes the following research contributions:</p>
<p>• We contribute empirical evidence highlighting the critical role of incorporating SMEs into evaluation processes for complex, domain-specific tasks.</p>
<p>• We provide analysis of explanations provided by both LLMs and SMEs and offer insights into how and why their evaluations differ across tasks.</p>
<p>• We discuss the variability in agreement between LLMs and SMEs, emphasizing how this misalignment varies by domain and task type.</p>
<p>• We design implications for effectively integrating SMEs into the evaluation process, identifying key areas where expert input is essential for improving alignment and ensuring accurate assessments.</p>
<p>BACKGROUND</p>
<p>LLM Evaluators/LLM as a Judge:</p>
<p>How to properly evaluate language model outputs remains an open area of research [7].For tasks such as multiple choice question answering where only a few choices are valid, exact match (EM) or likelihood are often used [54].</p>
<p>However, evaluating generations with more than a few tokens presents new challenges as there are often a large number of valid and correct answers.For shorter generations such as question answering, token overlap between the generated answer and the labeled answers are measured with F1, BLEU, ROUGE, and others [34,46].These token overlap metrics often struggle when a generated answer is correct but not in the list of labeled answers, as in the case with paraphrasing.Semantic evaluators such as BERTScore seek to remedy this by measuring the semantic similarity between the target text and the generated text [62].</p>
<p>However, for tasks such as instruction following where a language model generates many paragraphs of text, evaluation is even more difficult as there is an extremely large set of possibly correct answers.To make matters worse, some sentences within a single generation may be correct while others are not.For these tasks, a labeled "correct" answer is often foregone entirely and instead replaced with human pairwise evaluation, where a human is presented with two different language model generations for the same instruction and tasked with picking which generation better adheres to the instruction [10].Language models are then compared via a relative metric such as ELO or by win-rate [16].However, this approach is expensive, time consuming, and difficult to reproduce.Recently, many works have proposed instead using a separate language model, often called a judge or Evaluator, to address these issues by replacing the human evaluator [15,32,65].For example, EvalLM allows users to compare prompts while the LLM evaluates the pairs using predefined criteria, offering transparency by displaying both the number of times a prompt won and the reasons behind the evaluation [27].EvalGen goes further by enabling users to define evaluation criteria and grade outputs, aligning LLM evaluations with human-defined benchmarks [50].Approaches that use LLMs as evaluators, rather than humans, demonstrate a surprisingly high correlation with human preferences [14].Despite these advances, it remains uncertain whether LLMs can align with SMEs in specialized or complex domains.</p>
<p>Application of Language Models in Expert Domains</p>
<p>Language models have demonstrated a promising performance in several domains requiring expert knowledge.In the mental health domain, ChatCounselor trains a language model on conversations between clients and professional psychologists, achieving performance comparable to GPT-4 [35].Similarly, Mental-LLM evaluates various open-source and closed-source models for mental health use cases, showing that instruction tuning a smaller model can outperform larger, general-purpose models [60].</p>
<p>New approaches are also emerging in the dietetics domain.ChatDiet introduces a framework for using LLMs to provide personalized, nutrition-focused food recommendations [61].Likewise, Szymanski et al. [53] offer guidelines for developers on prompting LLMs to deliver high-quality nutrition information.Additionally, Niszczota and Rybicka [43] investigate the credibility of nutrition advice generated by ChatGPT for individuals with food allergies.</p>
<p>METHODS</p>
<p>The Need for Subject Matter Expertise</p>
<p>To investigate expert knowledge in relation to ranking outputs of complex tasks, we selected subject matter experts (SMEs) from the dietetics and mental health domains.The deployment of LLMs in these fields is increasing, expanding the potential for innovative healthcare solutions [25,53].In these domains, decision making is based on clinical judgement and evidence-based practices [41,42,57].This type of reasoning integrates a decision-making process that involves weighing evidence, employing a variety of cognitive strategies, from analytical reasoning to intuitive pattern recognition, and incorporating elements of intuition and practice wisdom [42,57].</p>
<p>Due to inherent biases in LLMs and their reliance on patterns in training data, LLMs may struggle to accurately assess expert-level decision making, potentially overlooking critical factors in complex subject matters [8].We argue that this level of expertise is crucial when evaluating LLM outputs, as LLMs may lack the ability to fully capture the complexity involved in complex tasks.In addition, while lay persons may possess critical thinking skills, they lack the depth of understanding required to make clinical judgements accurately [57].An average LLM user may not recognize inaccuracies or the importance of certain dietary restrictions and mental health protocols and is less likely to be aware of the latest research and clinical guidelines.</p>
<p>Dataset Curation</p>
<p>We crafted a dataset of 25 instructions for both the dietetics and mental health domains to test the evaluations performed by both the LLM and the SMEs.This dataset was designed to provide a diverse range of instructions and questions that would challenge both the ability of the LLM to provide accurate and complex responses and the clinical judgment of the SME.</p>
<p>Dietetics Instructions.</p>
<p>We surveyed the nutrition and dietetics literature that explore the use of LLMs for nutrition advice on common prompts or questions for dietitians that can be answered by LLMs [18,30,47,53].From this survey, we compiled common themes for instructions related to disease management and guidance, dietary preferences and lifestyle, nutrition components and overall health, and food allergies and sensitivities.The prompts consider the top diseases and allergies formed in the nutrition literature [30].</p>
<p>Manuscript submitted to ACM 3.2.2Mental Health Instructions.Building on a review of LLMs in mental health care, we referenced the work of Hua et al. [24] to compile prompt instructions based on common application areas in LLMs such as conversational agents, emotional support, and on-demand online counseling.From this review, we identified frequently addressed mental health topics, including stress, anxiety, depression, suicide, bipolar disorder, PTSD, autism spectrum disorder, and others.</p>
<p>Using these insights, we crafted instructions based on common inquiries from the literature, ensuring that the dataset covered a comprehensive range of mental health concerns.</p>
<p>Aspect Evaluation</p>
<p>Questions.For each instruction, we developed a set of aspect questions aimed at evaluating specific dimensions of the responses provided.These aspect questions were inspired by previous work on evaluation criteria in the domains of dietetics and mental health [13,53].Drawing from the literature, we identified key areas such as accuracy, clarity, educational context, personalization, and professional standards.Based on these themes, we formulated specific aspect questions, which we group as shown in Table 1.</p>
<p>Pairwise Comparison Method for LLM Output Quality Evaluation</p>
<p>We adopt a pairwise comparison method in which both the LLM and the SME is presented with two candidate outputs for the same instruction and is tasked with evaluating which model's output is better [65].This pairwise comparison method is similar to the commonly used state-of-the-art methods in the Natural Language Processing (NLP) community for evaluating and comparing LLM performances, such as Chatbot Arena, MT Bench, AlpacaEval, and others [32,33,65].This method enables direct comparison between LLM and SME judgments on task-specific outputs.</p>
<p>As in MT Bench and AlpacaEval [32], to determine the preferences of LLM judges, we use a pairwise comparison approach, in which the LLM judge is presented with two candidate outputs and is prompted to indicate which candidate is better.An example of the prompt used for evaluation can be found in Table 4 of the Appendix.The response with the higher overall likelihood (higher log probability) is considered the better one.This process is done both to measure overall LLM judge preferences as well as to measure preferences with respect to specific aspect questions.</p>
<p>We also experiment with an additional condition where we give the LLM judge an expert persona by indicating that it should personify an expert, to determine whether there would be an impact on agreement rates with the SMEs.</p>
<p>Previous research has shown that expert personas enhance the perceived quality of the output [36].This technique is commonly used to generate more personalized and context-aware dialogue, mimicking the knowledge and experiences of the specified expert in the output generation [31,36,49].</p>
<p>To compare the preferences of LLM judges with SMEs, we take the same set of instructions and candidate generations, but instead task subject matter experts, in this case registered dietitians or clinical psychologists, with judging which generation candidate is better.</p>
<p>EXPERIMENT</p>
<p>Subject Matter Expert Recruitment</p>
<p>Our work was approved by the Institutional Review Board (IRB) of our university prior to recruiting participants.To reflect the specialty of the domains, we recruited ten registered dietitians and ten clinical psychologists to complete the evaluation survey tailored to the dietetics and mental health domains, respectively.These two groups were chosen for their education and experience in their respective fields.Participants were recruited through email from a network of university campuses, hospitals, and local community practices.All participants were compensated with a $75 gift card at the end of the study.</p>
<p>Manuscript submitted to ACM</p>
<p>Aspect Category Dietetics Mental Health</p>
<p>Accuracy</p>
<p>Which response provides more accurate information to fully answer the prompt question?</p>
<p>Which response includes more accurate information based on current guidelines for managing [condition]?</p>
<p>Which response includes more accurate information based on current nutritional guidelines for managing [disease]?</p>
<p>Which response provides more accurate information to fully answer the prompt question?</p>
<p>Which response discusses allergens more accurately?</p>
<p>Which response clearly refers to appropriate nutritents to determine whether [food product] is a good or bad choice for the client?Clarity Which of these responses better meets the accepted standards of a registered dietitian in communication and application of knowledge?</p>
<p>Which response better meets the accepted standards of a mental health expert in communication and application of knowledge?</p>
<p>Which response is explained more clearly and understandably?Which response is explained more clearly and understandably?</p>
<p>Which response is more comprehensive in answering the question?</p>
<p>Which response is more comprehensive in answering the question?</p>
<p>Which response is more generalized and provides less detail than the other?</p>
<p>Educational Context Which response is easier to understand and follow in terms of the expected level of health literacy for an average individual?</p>
<p>Which response provides a better educational approach?</p>
<p>Which of these responses provides a better educational approach that a client can apply for long-term success?</p>
<p>Which response provides more information to support the long-term goal of the individual?</p>
<p>Which of these responses provides more information to support the long-term goal of the individual?</p>
<p>Which response offers more educational context regarding the nutritional components and health benefits of the [food product]?</p>
<p>Personalization</p>
<p>Which response is more personalized to the specific needs of the individual?</p>
<p>Which response has more language that is encouraging to the patient?</p>
<p>Which response is the most factually and thoroughly tailored to the individual?</p>
<p>Which response better demonstrates understanding of the patient's feelings?</p>
<p>Which response is more personalized to the individual?</p>
<p>Which response is the most factually and thoroughly tailored to the individual?</p>
<p>Professional Standards Which response is more consistent with established dietary recommendations for managing [disease]?</p>
<p>Which response better demonstrates empathy more consistently and appropriately?</p>
<p>Which response is better at providing factual information to support the answer?</p>
<p>Which response is more reliable in applying knowledge in accordance with accepted standards of mental health experts?</p>
<p>Which response is more reliable in applying knowledge in accordance with accepted standards of dietitians?</p>
<p>Which response is more positive and encouraging?</p>
<p>Table 1.Breakdown of Specific Aspect Questions by Category</p>
<p>Before starting the survey, participants were asked to consent to participate.In addition, we presented participants with a pre-study questionnaire, through which we gathered information about their educational background, specialty, workplace setting, tenure in the profession, and familiarity with AI.The profiles of the dietitian participants and clinical psychologists can be found in Table 2.</p>
<p>The dietitian participants had an average of 9.6 years (SD = 8.87) of experience in the field, while the psychologists had an average of 15.2 years (SD = 9.78).Three out of ten dietitians had previously used AI tools in their practice or research, whereas none of the psychologists reported using AI in their work.When asked to rate their familiarity with AI tools on a scale from 1 to 5 (with 5 being the most familiar), the dietitians reported an average score of 2.8 (SD = 0.92), while the psychologists reported an average score of 2.0 (SD = 0.67).</p>
<p>Manuscript submitted to ACM</p>
<p>Selected Models</p>
<p>To create the two LLM candidate responses whose quality will be evaluated in the experiment, we use GPT-4o, and GPT-3.5-turbo[2,45].While there are many models to choose from, we find that GPT-4o and GPT-3.5-turbo are commonly cited in the recent literature [9,28,35].We intentionally keep this set of models to a limit of two models, as adding more significantly increases the cost of human annotation.For all responses, we used a temperature of 1.0.</p>
<p>For the LLM used to judge the quality of instruction following (the judge model), we use GPT-4 as it is widely available and is frequently used as an LLM judge [32].As with SME participants, we prompt the language model judge to provide reasoning for preferring one response over another.The in-depth analysis of this reasoning and its comparison with SME reasoning can be found in Section 5.3.</p>
<p>Subject Matter Expert Evaluation Experiment Procedure</p>
<p>To facilitate the preferences of the SME judge, we construct a survey with a total of 25 evaluations from our curated dataset.For each of the evaluations, we provide the prompt instruction along with two candidate responses from two different language models.For each evaluation, participants were first tasked with choosing their general preference Manuscript submitted to ACM through the question "Which response was better overall?".In addition, each evaluation included two aspect questions (see Section 3.2) in which the participant would rank which response addressed the question better.We randomly assigned the aspect questions to each prompt instruction, ensuring balanced coverage across all categories, with each question drawn from a different category.Further, for each instruction and question, we asked the user to provide a brief explanation of their ranking as to why the selected response was chosen over the other.We randomized the order of instructions in the survey for each participant, and in addition we randomized the placement of which candidate response was placed first.The survey was presented using the Qualtrics platform and distributed via email.</p>
<p>LLM Evaluation Experiment Procedure</p>
<p>To enable the LLM judge model to rank outputs, we use the widely adopted AlpacaEval framework1 , which identifies the candidate response preferred by the LLM.This framework scores a given model based on its win rate relative to a baseline model, which is often a well-performing model that is widely accessible and commonly used in practice [7,15,58].</p>
<p>AlpacaEval was chosen over other frameworks as it handles some of the common challenges that come with using LLM judges, such as randomizing the order of the two models outputs so there is no positional bias in the evaluation (as was done in our survey with SME participants).</p>
<p>Using the framework, the LLM judge model was provided the same twenty-five prompt instructions, the set candidate responses, and three ranking questions (one general preference question and two aspect questions) as provided to SMEs in the survey.The LLM judge model will rank which model was preferred (response A or response B) for each question.</p>
<p>It is important to note that we replace the original AlpacaEval instruction-following dataset with our own curated dataset.We also modified the framework to allow for preferences to be evaluated relative to specific aspect questions and instructed the LLM judge model to provide an explanation for its choice between candidate responses.This is done by simply adding, "Why is {preference} chosen over the other output?Provide a short 2-3 sentence explanation." as an additional user message after the initial preference is given.Additionally, to experiment with the use of expert personas, we modify the AlpacaEval instructions in the system prompt to act as an expert dietitian or clinical psychologist.An example of our modified AlpacaEval prompt can be found in Table 4.</p>
<p>Analysis Methods</p>
<p>4.</p>
<p>5.1 SME v. LLM Agreement Rates.We record LLM judge evaluations and SME evaluations on the same set of data.For each domain, we numerically measure the agreement between LLM judges and SMEs on the same prompt instructions and calculate the percentage of instances where both the LLM judge and SMEs agree.We also measure the agreement among the SMEs in each domain.</p>
<p>Qualitative Analysis of Ranking Explanations.</p>
<p>To address RQ2, we analyzed the qualitative data collected from both SMEs and LLMs on their explanations for choosing one output over the other.The participants and the LLM provided explanations not only for their overall choice but also for each individual aspect question.We followed a reflexive thematic analysis approach and utilized standard open-coding procedures to uncover underlying themes [6].</p>
<p>The explanations were reviewed for familiarization with the data, then initial codes were created to categorize the data into themes.During this process, multiple codes could be assigned to a single response to capture the complexity of the feedback.Following discussion of the initial coding, the data was re-coded with the new codes.We provide illustrative examples from both SMEs and LLMs to support and contextualize the final codes.We report on the emergent themes and highlight key differences in the explanations provided by the SMEs and the LLMs.</p>
<p>RESULTS</p>
<p>SME vs. LLM-as-a-Judge Overall Preferences</p>
<p>When examining the agreement between SMEs and the LLM on General Preference questions, we found that SMEs show a relatively low level of agreement with our LLM judge, with 60% agreement in the mental health domain and 64%</p>
<p>in the dietetics domain (see Table 3).By comparison, SMEs agreed with each other 72% in the mental health domain and 75% in the dietetics domain, with an overall agreement of 73% across both domains, establishing a higher baseline for expert agreement.Further, when adopting the expert persona method in prompt instructions, we observed a 4% improvement in the agreement between the LLM judge and SMEs for both domains in general preference questions, suggesting that personas can enhance agreement between LLMs and SMEs, although agreement levels remain low.</p>
<p>Dietetics</p>
<p>SME vs. LLM-as-a-Judge Agreement on Aspect Questions</p>
<p>Our data shows variation in agreement across the aspect questions and domains, both with and without the expert LLM persona, revealing some notable differences (see Table 3).Overall, there was a lower agreement for aspect questions in the dietetics domain with only slight improvement using the expert persona.The agreement in aspect questions tended to be better for mental health SMEs compared to dietetics SMEs with a slight improvement using the expert persona, with the exception of a notable drop in Clarity agreement.</p>
<p>Another noticeable difference is in Accuracy agreement, where the LLM shows greater alignment with the mental health SMEs compared to dietetics SMEs.Similar trends of higher agreement are observed in the Education Context and Personalization categories for the mental health domain compared to the dietetics domain; however, the general model had higher agreement than the expert persona in the dietetics domain.There was also a large difference in Clarity agreement, with a much lower alignment in the mental health domain using an expert persona compared to dietetics.These results highlight that domain and task complexity affect LLM-SME agreement differently, with the expert persona sometimes improving alignment but not consistently across all aspects.</p>
<p>SME vs LLM Explanations: Qualitative Results</p>
<p>In this section, we present the findings on the themes that emerged from the evaluation explanations with illustrative examples from the survey to address RQ2.Google" phenomenon, where patients come in with an inflexible self-perception that may not be accurate" (Psych6).</p>
<p>5.3.1</p>
<p>In contrast to the SMEs' approach, where accuracy and adherence to professional standards were top priorities, the LLM often overlooked some of the harmful or inaccurate aspects of the outputs.Instead, it often repeated specific details provided in the outputs that were focused on following prompt instruction.LLMs missed the critical details identified by SMEs, such as gaps in information or where essential details were missing.While SMEs pointed out serious issues with certain responses, the LLM did not flag these inaccuracies, suggesting that it lacked the ability to recognize the potential harm of these recommendations.This discrepancy reveals the primary risk of relying solely on LLMs for the evaluation of accuracy tasks.As LLMs can process and follow instructions, they may overlook critical details and fail to assess the broader implications of the content.</p>
<p>Theme 2:</p>
<p>Clear and Efficient Communication.SMEs preferred responses with clarity and conciseness, using simple and clear language for a broad audience, and terminology that avoided jargon or overly complex medical terms.SMEs preferred responses that were short, to the point, and avoided unnecessary detail or filler.These types of responses were chosen because they were seen as more digestible for patients with varying levels of health literacy.For example, one explanation from a dietitian states, "I like response B better, as it is more clear and relays the information in a more organized way.It also states "milk" instead of "dairy", which is more accurate."(Diet4).Equally important was the structure/flow of the responses.In both domains, responses that were well-organized, such as breaking down advice in a step-by-step manner or logically grouping related information, were considered to be more effective.For example, in the dietetic domain, the organization of content by prioritizing key factors such as protein and sodium was Manuscript submitted to ACM appreciated for its clarity as were structured lists or menus.Similarly, in the mental health domain, responses deemed to be more clinical or disjointed were less preferred.For example one psychologist notes "[The response] is short, sweet, and to the point while identifying common symptoms and not overwhelming the reader with too much information (i.e., the reader might have OCD and too much information might cause unnecessary worry)" (Psych8).</p>
<p>While the overall communication of the explanations was a significant factor for how the SMEs ranked responses, the LLM also emphasized clarity in its explanations, but in a different manner.The LLM often equated clarity with being more detailed or comprehensive, whereas SMEs prioritized clear, simple language, and the overall flow of the explanation, regardless of how much detail was included.The LLMs version of clarity often did not meet the SMEs standards for how they would structure information to a client, especially when considering the client's lack of understanding.This difference in what constitutes "clarity" could explain the misalignment observed in the Clarity aspect questions, as the SMEs preferred concise and easily digestible information over exhaustive detail.Particularly in the mental health domain, the LLMs missed that an overflow of information, even if inaccurate, may overwhelm or confuse clients and could harm their understanding.</p>
<p>Theme 3:</p>
<p>Referring to Professional Guidance and Tone.SMEs in both domains favored responses that referred to professional guidance and emphasized the importance of consulting with trained professionals.In the dietitian domain, responses that encouraged people to seek personalized long-term meal plans or advice from registered dietitians or healthcare providers were favored, as these professionals can tailor recommendations to the individual's unique health needs.Similarly, in the mental health domain, responses that directed users to mental health professionals for formal assessments or comprehensive treatment plans were highly regarded, particularly when dealing with complex conditions such as Autism Spectrum Disorder or schizophrenia.For example, "Both provide information about the approach but the lack of directing to a professional makes it seem like these are activities that the parent should be conducting." (Psych1 ).</p>
<p>For tone and framing, SMEs in both the dietetics and mental health domains valued responses that maintained a positive, empathetic, and encouraging tone while framing information in a way that felt supportive rather than judgmental.In the dietetic domain, responses that provided balanced, non-judgmental nutritional advice such as promoting mindfulness, and conveying ease in adopting healthy habits were preferred.In the mental health domain, tone played a crucial role in how advice was received.Responses that provided empathetic validation, encouraged hope, and normalized difficult emotions were viewed more favorably.The framing of mental health advice in a positive and hopeful manner without making the process seem overly daunting was preferred.For example, a psychologist mentioned that a response "seemed less pathologizing and had a nice balance of normalizing while also encouraging treatment" (Psych7).</p>
<p>The LLM generally aligned with SMEs, often favoring responses that emphasized the importance of contacting a dietitian or seeking a thorough evaluation from a mental health expert, a recurring theme across both domains.It also occasionally favored responses based on a positive tone, which was in agreement with SMEs' preferences.In contrast, the LLM generally favored responses that included actionable or practical steps for the user, but this type of information was sometimes present in both responses, suggesting that LLM's selection may also be influenced by subjective factors, such as the level of detail.Furthermore, while the LLM highlighted personalization by noting when a response was better suited to the type of user mentioned, it often lacked the in-depth rationale provided by SMEs.</p>
<p>5.3.5 Theme 5: Depth of Information.In the dietetic domain, responses that provided a comprehensive explanation or detailed information of the benefits and potential drawbacks of specific diets or foods were appreciated for offering a complete picture.For example, one dietitian stated "Option B was more educational.It did a better job of teaching why each food was chosen and gave practical tips to help the client learn how to make his own decisions and choose healthy foods to lower his cholesterol." (Diet5) In the mental health domain, responses that thoroughly explored the emotional, behavioral, and psychological aspects of a condition were preferred.Providing a detailed description of therapeutic interventions offered a more well-rounded response.Those that went beyond simply listing possible treatments and included education about potential diagnoses or addressed both emotional and environmental factors were seen as more holistic.For example a psychologist noted "A is more thorough and provides a better explanation than B. A talks about the outcomes that come from engaging in CBT and how they can help change the response to anxiety (Psych9)".</p>
<p>LLMs often emphasized the importance of comprehensive or detailed responses, as these were the most identified themes in their explanations.While this level of detail sometimes aligned with SME preferences, SMEs were more discerning about how much detail was necessary.They did not always equate more detailed responses with better ones, as they prioritized clarity and relevance over volume of information.This careful balance was something the LLM occasionally missed in its evaluations.</p>
<p>EVALUATING LAY USER ALIGNMENT WITH LARGE LANGUAGE MODELS</p>
<p>Our findings in Section 5.1 and 5.2 reveal a misalignment between the preferences of SMEs and LLMs in evaluating domain-specific tasks.This highlights the necessity for continued SME involvement in the development and fine-tuning of these models.</p>
<p>Current fine-tuning methods for LLMs, such as GPT-4, utilize Reinforcement Learning from Human Feedback (RLHF) [59].This technique relies on human evaluators to provide feedback, which is then used to align the model's responses with human values.However, most state-of-the-art LLMs use lay users for RLHF, indicating that the effectiveness is highly dependent on the quality of the feedback provided [11,23,44].When SMEs are not involved, often due to the time and cost associated with their expertise, the process may lack the precision required for domainspecific alignment.As a result, we hypothesized that LLMs may align more closely with the preferences of lay users, potentially contributing to the misalignment we observed between SMEs and LLMs.</p>
<p>To test this hypothesis and to compare the differences in alignment between SMEs and lay users with the LLM, we conducted a follow-up study in which lay users provided rankings for the same instructions.We recruited lay users to rank responses to the same survey questions taken by SMEs, answering the prompt "Which response is better overall?".This allowed us to directly compare the agreement between the LLM and both groups, and inform us on potential differences in evaluation preferences between SMEs and lay users.Lay users were recruited through Prolific, a crowdsourcing platform for conducting qualitative studies [1].A total of 10 users were recruited for each domain to complete the pairwise comparison task and were compensated the recommended average rate of $12.00 per hour.The participant profiles for lay users can be found in Table 5 in the Appendix.</p>
<p>We use the same analysis methods to calculate the agreement rates as used in Section 4.4 and Section 4.5.We also compare the agreement rate between the lay user and the general model and between the lay user and the LLM expert persona.</p>
<p>Lay User Results</p>
<p>Percent Agreement</p>
<p>General Model Expert Persona</p>
<p>Fig. 1.Comparison of Agreement between LLM Judge and SMEs versus Lay Users for Dietetics and Mental Health.The lay users show significantly more agreement with the LLM Judge using a standard persona than the LLM Judge using an expert persona.Conversely, the SMEs show more agreement with the LLM Judge using an expert persona than with a standard persona.</p>
<p>The results of our evaluation comparing Lay Users to an LLM judge are shown in Figure 1.Interestingly, we observe that the agreement rate between lay users and the LLM is 80% for both the dietetics and the mental health domain with the default general model.This is a higher agreement than what was observed between the LLM and SMEs, suggesting that the models are more closely aligned with lay user preferences.A two-tailed chi-squared test was conducted, revealing a statistically significant difference (p &lt; 0.0001) between the agreement rates of lay users and SMEs with the LLM using the general model and the expert persona LLM in each domain.</p>
<p>In addition, the use of expert personas actually decreases the agreement to 76% for both dietetics and mental health.</p>
<p>This shows the opposite of what we observed in the SME domain, where the use of expert personas improved the agreement rate.We will discuss the implications of this in Section 7.1.1 in the discussion.</p>
<p>DISCUSSION</p>
<p>This paper contributes new insights into how the LLM-as-a-Judge evaluation approach compares to evaluations conducted by SMEs for domain-specific tasks.Our research highlights the limitations of relying solely on LLMs for evaluation, and the importance of keeping the human, specifically the SME, in the loop for tasks that require expert knowledge.In this section, we explore the potential reasons behind the observed patterns in our findings.We then discuss key design implications that developers and designers should consider when designing and choosing the method for evaluating the quality of LLM outputs for domain-specific tasks.</p>
<p>Manuscript submitted to ACM As observed in the results (see Section 5), there is a misalignment between the SMEs and the LLM for both general preference and aspect questions.Additionally, the findings reveal that the alignment between the SMEs and the LLM varies across different domains, even when an expert persona LLM is used.To follow, we highlight the major takeaways in the data and discuss reasoning for the variability.</p>
<p>7.1.1Impact of Expert Personas on LLM Alignment with SMEs.Instructing the model to adopt an expert persona generally improved alignment with SMEs compared to the general model, indicating that expert-level tasks benefit from personified models for evaluation.The importance of the expert persona is further highlighted by the lower agreement between lay users and the expert persona LLM, as lay users may tend to evaluate or rank outputs based on different criteria [52].Our findings also show that the decision to use an expert persona or a general-purpose persona should depend on the specific task and domain.</p>
<p>Interestingly, in the dietetics domain, the expert LLM did not improve alignment with SMEs in the Education Context and Personalization aspect question categories.This suggests that while expert personas may be better suited for tasks requiring strict adherence to professional standards or specialized knowledge, general-purpose models may perform better in tasks that require adaptability and user-centered flexibility.The general-purpose model's broader scope allows it to generate more varied responses, which may resonate better with SMEs in tasks such as education and personalization.</p>
<p>Another reason for the misalignment may be that expert personas prioritize technical language and specific jargon, which SMEs may not consider to be as effective in educational or practical contexts.This aligns with prior studies where persona agents have become a standard for enabling personalized experiences [37].However, role-playing or personified LLMs often lack datasets that fully represent real human behaviors in specific domains [3,56].Additionally, many large-scale models are preference-tuned on lay user data, meaning their outputs align more with general preferences rather than those of domain experts.In this case, the model may be heavily influenced by preference tuning/RLHF with limited expert preference data, causing it to lack the specialized focus needed for tasks such as clinical dietetics.</p>
<p>An interesting finding in the mental health domain is that the expert persona failed to align with SMEs standards in the Clarity category.From our qualitative data, we determined that SMEs in the mental health domain expressed concerns about overly detailed outputs, which could lead to confusion, self-diagnosis, or even harm.The expert persona may be favoring the more technical outputs with increased vocabulary, but failed to detect the risks and potential misuse that SMEs typically flag.</p>
<p>7.1.2Variability in LLM v. SME Alignment across Domains and Aspect Categories.In both the dietetics and mental health domains, we observed high variability in the alignment between SMEs and the LLM depending on the aspect question.</p>
<p>For example, in the mental health domain, the SMEs showed higher alignment in questions related to Accuracy and Educational Context.This variability suggests that LLMs trained on mental health tasks may benefit from the availability of well-structured resources in the training data, such as research papers and educational materials, which contributes to higher alignment with SMEs.In contrast, the dietetics domain may face challenges such as conflicting dietary advice, misinformation, and rapidly evolving guidelines, making it harder for LLMs to achieve close alignment with SMEs in these areas.Popular diet fads and varied nutritional recommendations further complicate this alignment.Knowing this means that we need human experts to pay closer attention to certain aspects, particularly in domains where LLMs are prone to lower alignment, such as dietetics.</p>
<p>Manuscript submitted to ACM</p>
<p>The SMEs in the dietetics domain showed lower alignment with Personalization questions compared to the mental health domain, suggesting that addressing dietary needs may involve greater complexity.In mental health, personalization often involves broader, less detailed approaches, whereas in dietetics, it requires more specific, actionable nutrition-related recommendations.Our results show that mental health evaluations were also heavily influenced by the level of emotional support provided in responses, while dietary experts focused more on the practicality and specificity of the recommendations offered.</p>
<p>In addition, the dietetics domain showed slightly higher alignment with SMEs compared to the mental health domain on Professional Standard questions.This may be because questions in the mental health domain often focused on determining which responses demonstrated an empathetic or encouraging tone that was more in line with accepted expert standards.The lower misalignment in mental health indicates that the LLM may struggle to consistently apply empathy in a manner that aligns with professional standards, highlighting a potential challenge in effectively balancing technical accuracy with emotional sensitivity.3), we found that SMEs provided much more specific and unique context compared to the LLMs when addressing the explanation behind the evaluation ranking.SMEs' feedback was much more detailed and context-specific, whereas LLMs tended to provide generalized explanations.In many cases, LLMs appeared to repeat information from the instruction or the output itself with limited new information being contributed.</p>
<p>On the other hand, SMEs were shown to contribute new information and unique insights that they likely drew upon from their knowledge or experience instead of reiterating the information provided.This could be attributed to the pattern-based learning of LLMs inherent in autoregressive language modeling.Furthermore, the limited contextual understanding and lack of domain-specific expertise in LLMs contribute to their tendency to generalize.This overreliance on the instruction, combined with the insufficient use of parametric knowledge, restricts the LLM's ability to provide the depth of feedback seen in SMEs.</p>
<p>7.1.4Lay User Alignment with LLMs in General Preference Tasks.Interestingly, our findings in Section 6 also reveal that lay users tend to have higher alignment with LLMs than do SMEs for general preference tasks.This suggests that LLMs may be preference-tuned using data primarily from lay users.As a result, the models may align more with lay preferences than with expert knowledge.This can lead to a divergence in performance when evaluating tasks that require specialized expertise, emphasizing the need to carefully consider the training and evaluation process for domain-specific tasks.</p>
<p>Implications for Evaluating Systems in Domain-Specific Tasks</p>
<p>Based on the patterns identified from our analysis, we present the following design implications to guide researchers and developers in evaluating systems for domain-specific tasks.</p>
<p>7.2.1 Incorporate SME-in-the-Loop for Personalized and Complex Evaluations.Researchers and developers should integrate a SME-in-the-loop approach for evaluations that involve complex decision-making.SMEs can provide critical insights, especially when LLMs are deployed in domains that demand specialized expertise.The SME-in-the-loop approach ensures that the system's output is validated and fine-tuned to meet high standards in complex tasks, preventing errors that could arise from overreliance on generalized LLM outputs.Although integrating SMEs can be costly and time-consuming, involving SMEs in pairwise rankings or iterative review processes can help identify subtle errors and misalignments that automated systems might miss.This human-in-the-loop approach is especially valuable in tasks where the precision of recommendations is essential, such as healthcare, where the consequences of mistakes can be significant.</p>
<p>However, LLM judges are well suited for conducting large-scale evaluations across many models [66].By using LLMs to evaluate performance on large datasets and across numerous model variations, the worst-performing models can be eliminated early on.SMEs can then be brought in to evaluate the remaining top-performing models, ensuring expert-level insights are applied efficiently and cost-effectively.This hybrid approach allows the scalability and speed of LLM evaluations while maintaining high standards for complex, high-stakes decisions through the involvement of SMEs.</p>
<p>Domain-Specific Evaluation</p>
<p>Frameworks and Data Curation.Given the variation in performance across dietetics and mental health domains, a design implication is the need for LLMs to be evaluated within a domain-specific framework.The fact that alignment varied between categories such as Accuarcy and Education Context suggests that a one-size-fits-all evaluation approach may overlook critical domain-specific weaknesses.When designing evaluations for systems that operate across multiple domains, each domain should have a tailored evaluation framework, similar to the one used in our study.</p>
<p>Domain-specific evaluation frameworks can be used to assess the alignment between SMEs and LLMs in certain task criteria.Researchers or designers can curate a domain-specific dataset that addresses the unique characteristics and criteria of each field.By utilizing a common pairwise comparison method [32] between SMEs and LLMs, we can pinpoint critical evaluation criteria, allowing SMEs to focus on areas where they are most needed, thereby saving time and reducing costs.By testing the model against these curated datasets, developers can better understand the strengths and weaknesses of LLMs in each domain and refine the models to improve their alignment with expert standards.</p>
<p>7.2.3</p>
<p>Tailor LLM Personas to Domain-Specific Expertise for Improved Alignment.From other literature, we see that personas are commonly used to generate more personalization and expert dialogue [36,49].Our study expands on these findings by suggesting that LLM personas should be carefully tailored to the domain-specific expertise required for the task.For domains such as dietetics or mental health, where expert-level precision is critical, training LLMs to adopt specialized personas based on domain-specific guidelines and knowledge is essential.These expert personas enable the model to offer more accurate, professional, and context-aware outputs, particularly for high-stakes or regulatory-driven domains.However, for tasks that require adaptability and general user interaction, general-purpose models may be more effective.This suggests that more domain-specific training, either using datasets such as clinical guidelines or therapeutic frameworks or through interactive human-AI alignment efforts [20,21], may be necessary to improve alignment in categories where expert knowledge is vital.Developers should leverage domain-specific datasets and expert personas to fine-tune LLMs for optimal performance in judging specialized tasks.</p>
<p>FUTURE WORK &amp; LIMITATIONS</p>
<p>Our work has some limitations that open the door to exciting future work.First, we do not thoroughly explore the correlation between pre-training data content and downstream performance on domain-specific tasks.Future work has the opportunity to examine how differences in pre-training data influence judgment alignment in various domains, especially where misinformation or conflicting guidelines are prevalent, as in dietetics.</p>
<p>Additionally, the models used in this study were not further tuned on the pairwise comparisons or the qualitative explanations provided by SMEs.This is a common practice in LLM research, but involves significant costs and resource</p>
<p>Manuscript submitted to ACM demands in fine-tuning on specialized feedback.Future work has the opportunity to explore further tuning LLMs using this feedback, allowing the models to learn from SME judgments and refine their outputs.We believe that this approach could help significantly reduce the observed misalignment in specific domains.</p>
<p>Lastly, our study is limited to only the two specific domains of dietetics and mental health due to the high cost associated with collecting annotations from SMEs.Although these fields are representative of the kinds of complex tasks that require expert reasoning, we invite future work to expand this evaluation to other fields that require domain expertise (e.g., qualitative analysis [17,22], creativity [51,63], UX [38][39][40], and academic research [26]).Future work may also explore how LLMs perform when evaluating other domain-specific tasks to explore the benefits and weaknesses of evaluations that are unique to other fields.</p>
<p>CONCLUSIONS</p>
<p>This paper explores the differences between SMEs and LLM-as-a-judge in evaluating domain-specific tasks that require expert knowledge.Our findings reveal that it is important to design evaluation systems to keep SME-in-the-loop, and our results highlight the strengths and weaknesses identified in the evaluations between LLM and SMEs.We also provide an empirical understanding of the factors that SMEs care about during the evaluation process that differ from LLMs.Our analysis suggests implications that should be considered for evaluating systems in domain-specific tasks, such as integrating SME-in-the-loop and preference tuning or RLHF, developing frameworks on where to best integrate expertise, and using LLM expert personas to improve judgement alignment.Ultimately, this work contributes to the discussion on SME-driven evaluation, offering insights to enhance alignment between human experts and LLMs in interactive systems.</p>
<ol>
<li>1
1
Differences in LLM-as-a-Judge v. SME Evaluation (RQ1 &amp; RQ2)</li>
</ol>
<p>7.1. 3
3
SMEs' Unique Contributions in Feedback Compared to LLMs.When analyzing the qualitative results (see Section 5.</p>
<p>Table 2 .
2
Profiles of Registered Dietitian (a) and Clinical Psychologist (b) participants.The table includes responses to the following questions: 1) What is your highest level of education?2) How many years of experience do you have in the field?3) Have you ever utilized artificial intelligence tools in your practice or research related to psychology, nutrition, or therapy?4) How would you rate your familiarity with artificial intelligence and technology in the context of psychology, nutrition, or therapy?Familiarity with AI is rated on a scale of 1-5, where 1 = Not Familiar and 5 = Very Familiar.
(a) Registered DietitiansParticipant ID Profession EducationYears of Experience Use of AI Tools Familiarity with AIDiet1DietitianBachelor's Degree10No3Diet2DietitianMaster's Degree10No3Diet3DietitianMaster's Degree18Yes3Diet4DietitianBachelor's Degree3No3Diet5DietitianMaster's Degree9No2Diet6DietitianBachelor's Degree2Yes4Diet7DietitianBachelor's Degree4No1Diet8DietitianBachelor's Degree4No3Diet9DietitianMaster's Degree30No2Diet10DietitianBachelor's Degree2Yes4(b) Clinical PsychologistsParticipant ID Profession EducationYears of Experience Use of AI Tools Familiarity with AIPsych1Psychologist Doctorate10No1Psych2Psychologist Doctorate15No2Psych3Psychologist Doctorate14No2Psych4Psychologist Professional Doctorate26No1Psych5Psychologist Doctorate6No2Psych6Psychologist Doctorate10No3Psych7Psychologist Doctorate11No2Psych8Psychologist Doctorate7No3Psych9Psychologist Doctorate38No2Psych10Psychologist Doctorate15No2</p>
<p>Table 3 .
3
Comparison of agreement between the General LLM vs. SME and the Expert Persona LLM vs. SME in the Dietetics and Mental Health domains.The General Preference question refers to selecting which output is better overall, while the remaining categories focus on specific aspect questions.</p>
<p>Theme 1: Alignment with Expert Knowledge and Accuracy.SMEs consistently favored responses that demonstrated accurate information.In the dietetics domain, this included factually correct, up-to-date, evidence-based practices and scientifically supported information that aligned with professional standards.Experts were quick to flag inaccuracies, especially when responses promoted outdated or misleading dietary concepts, misrepresented nutrients, or incorrectly addressed allergens.For example, one dietitian pointed out, "This statement in response B is not true...carbohydrates alone do not increase the risk necessarily, so the statement is misleading" (Diet1).SMEs also preferred responses that mirrored their clinical recommendations and adhered to industry guidelines.Inaccurate or harmful recommendations were a key concern, with one expert highlighting "Response B is not a healthy way of thinking.It will NOT benefit someone with diabetes...it is WAY more important to communicate the dangers of the ketogenic diet" (Diet7).
In
the mental health domain, accurate responses referenced evidence-based treatments and avoided misinformation, such as promoting non-evidence-based therapies.Responses that reflected the latest research and clinical guidelines were favored, as SMEs recognized the importance of adhering to evidence-based standards.SMEs also appreciated responses that aligned with best practices in therapy and treatment.For example, responses that described therapy approaches and clarified that medications should complement therapy, rather than serve as standalone treatments, were preferred.However, mental health experts expressed concern about responses that included non-evidence-based treatments without proper qualification, or that suggested premature diagnoses based on vague symptoms.Such responses were considered problematic, with one participant noting, "Response A is bad.Like harmful, bad.Taking a couple nonspecific symptoms and telling someone what they likely have is hugely problematic and reminiscent of the "Dr.</p>
<p>5.3.4Theme4:Relevance to Client Needs and Actionable Information.SMEs preferred responses that had more personalization in both domains particularly when responses considered individual preferences, cultural factors, and specific dietary or mental health needs.In the dietetics domain, recommendations that included culturally relevant foods and addressed user allergies or health conditions were favored.In mental health, preferred responses acknowledged the client's emotional state and tailored coping strategies.Mental health SMEs found the inclusion of emotional support important, particularly when individual struggles and offering hope were acknowledged.In addition, responses with Manuscript submitted to ACM actionable information such as concrete steps or implementation strategies, were preferred.In the dietitian domain, SMEs valued specific and detailed advice, such as exact nutrient recommendations or foods, whereas in the mental health domain, step-by-step strategies were appreciated.For example a SME noted "I think Response A meets the person where they are at -basically, telling them to go get help and develop a treatment plan.This is probably what someone in this predicament needs to hear, as opposed to a comprehensive overview of diagnosis/treatment. " (Psych6).</p>
<p>Manuscript submitted to ACM
https://tatsu-lab.github.io/alpaca_eval/ Manuscript submitted to ACM
ACKNOWLEDGEMENTSThis work was supported by the Agriculture and Food Research Initiative grant no.2021-67022-33447/project accession no.1024822 from the USDA National Institute of Food and Agriculture and by the Office of Naval Research N00014-22-1-2507.A APPENDIX&lt;| im_start | &gt; system You are a highly efficient assistant , who evaluates and selects the best large language model ( LLMs ) based on the quality of their responses to a given instruction with respect to a particular aspect question .You are also a clinical psychologist .This process will be used to create a leaderboard reflecting the most accurate and human -preferred answers .&lt;| im_end | &gt; &lt;| im_start | &gt; user I require a leaderboard for various large language models .I ' ll provide you with prompts given to these models and their corresponding outputs .Your task is to assess these responses , and select the model that produces the best output from a human perspective with respect to a particular aspect question .## Instruction{" instruction ": """{ instruction }""" , " aspect_question ": """{ aspect_question }""" }## Model OutputsHere are the unordered outputs from the models .Each output is associated with a specific model , identified by a unique model identifier .{ { " model_identifier ": " m " , " output ": """{ output_1 }""" } , { " model_identifier ": " M " , " output ": """{ output_2 }""" } } ## Task Evaluate the models based on the quality and relevance of their outputs , and select the model that generated the best output with respect to the aspect question .Answer by providing the model identifier of the best model .We will use your output as the name of the best model , so make sure your output only contains one of the following model identifiers and nothing else ( no quotes , no spaces , no new lines , ...) : m or M .
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim, arXiv:2405.18027TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models. 2024. 2024arXiv preprint</p>
<p>ChainForge: An open-source visual programming environment for prompt engineering. Ian Arawjo, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman, Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021. 2021arXiv preprint</p>
<p>Reflecting on reflexive thematic analysis. Virginia Braun, Victoria Clarke, Qualitative research in sport, exercise and health. 112019. 2019</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 152024. 2024</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, arXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024. 2024arXiv preprint</p>
<p>The now and future of ChatGPT and GPT in psychiatry. Szu-Wei Cheng, Chung-Wen Chang, Wan-Jung Chang, Hao-Wei Wang, Chih-Sung Liang, Taishiro Kishimoto, Jane , Pei-Chen Chang, John S Kuo, Kuan-Pin Su, Psychiatry and clinical neurosciences. 772023. 2023</p>
<p>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, arXiv:2403.04132Chatbot arena: An open platform for evaluating llms by human preference. 2024. 2024arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 2017. 201730Advances in neural information processing systems</p>
<p>EvaluLLM: LLM assisted evaluation of generative outputs. Michael Desmond, Zahra Ashktorab, Qian Pan, Casey Dugan, James M Johnson, Companion Proceedings of the 29th International Conference on Intelligent User Interfaces. 2024</p>
<p>Design and evaluation guidelines for mental health technologies. Gavin Doherty, David Coyle, Mark Matthews, Interacting with computers. 222010. 2010</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators. 2024. 2024arXiv preprint</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>The rating of chessplayers: Past and present. E Arpad, No Title)Sam Elo, No Title)Sloan, No Title)1978. 1978</p>
<p>CollabCoder: a lower-barrier, rigorous workflow for inductive collaborative qualitative analysis with large language models. Jie Gao, Yuchen Guo, Gionnieve Lim, Tianqin Zhang, Zheng Zhang, Toby Jia-Jun, Simon Tangi Li, Perrault, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>ChatGPT as a virtual dietitian: Exploring its potential as a tool for improving nutrition knowledge. Garcia Manuel, Applied System Innovation. 6962023. 2023</p>
<p>Openagi: When llm meets domain experts. Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning. Araya Simret, Kuangshi Gebreegziabher, Zheng Ai, Elena L Zhang, Toby Jia-Jun Glassman, Li, arXiv:2408.038192024. 2024arXiv preprint</p>
<p>Araya Simret, Yukun Gebreegziabher, Elena L Yang, Toby Jia-Jun Glassman, Li, arXiv:2409.16561Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories. 2024. 2024arXiv preprint</p>
<p>Patat: Human-ai collaborative qualitative coding with explainable interactive rule synthesis. Araya Simret, Zheng Gebreegziabher, Xiaohang Zhang, Yihao Tang, Elena L Meng, Toby Jia-Jun Glassman, Li, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Overview of Gemini: Large Multimodal Models. 2024Google ResearchTechnical Report</p>
<p>Large language models in mental health care: a scoping review. Yining Hua, Fenglin Liu, Kailai Yang, Zehan Li, Yi-Han Sheu, Peilin Zhou, Lauren V Moran, Sophia Ananiadou, Andrew Beam, arXiv:2401.029842024. 2024arXiv preprint</p>
<p>Rethinking large language models in mental health applications. Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria, arXiv:2311.112672023. 2023arXiv preprint</p>
<p>I'm categorizing LLM as a productivity tool. Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, Hong Shen, arXiv:2403.19876Examining ethics of LLM use in HCI research practices. 2024. 2024arXiv preprint</p>
<p>Tae Soo, Kim , Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim, arXiv:2309.13633Evallm: Interactive evaluation of large language model prompts on user-defined criteria. 2023. 2023arXiv preprint</p>
<p>An introduction to generative artificial intelligence in mental health care: considerations and guidance. Guransh Darlene R King, Joel Nanda, Allison Stoddard, Sarah Dempsey, Jay H Hergert, John Shore, Torous, Current psychiatry reports. 252023. 2023</p>
<p>Precision nutrition: A systematic literature review. Daniel Kirk, Cagatay Catal, Bedir Tekinerdogan, Computers in Biology and Medicine. 1331043652021. 2021</p>
<p>Comparison of answers between ChatGPT and human dieticians to common nutrition questions. Daniel Kirk, Elise Van Eijnatten, Guido Camps, Journal of Nutrition and Metabolism. 202355486842023. 2023</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Chatharuhi: Reviving anime character in reality via large language model. 2023. 2023arXiv preprint</p>
<p>AlpacaEval: An Automatic Evaluator of Instruction-following Models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yuntian Bill Yuchen Lin, Khyathi Deng, Faeze Chandu, Abhilasha Brahman, Valentina Ravichander, Nouha Pyatkin, Ronan Dziri, Yejin Le Bras, Choi, arXiv:2406.04770WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild. 2024. 2024arXiv preprint</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>June M Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, Jiamin Wu, arXiv:2309.15461Chatcounselor: A large language models for mental health support. 2023. 2023arXiv preprint</p>
<p>Yiren Liu, Pranav Sharma, Mehul Jitendra Oswal, Haijun Xia, Yun Huang, arXiv:2409.12538PersonaFlow: Boosting Research Ideation with LLM-Simulated Expert Personas. 2024. 2024arXiv preprint</p>
<p>Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, Diyi Yang, arXiv:2407.00870Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles. 2024. 2024arXiv preprint</p>
<p>Yuwen Lu, Ziang Tong, Qinyi Zhao, Yewon Oh, Bryan Wang, Toby Jia-Jun Li, arXiv:2406.16177Flowy: Supporting UX Design Decisions Through AI-Driven Pattern Annotation in Multi-Screen User Flows. 2024. 2024arXiv preprint</p>
<p>Yuwen Lu, Yuewen Yang, Qinyi Zhao, Chengzhi Zhang, Toby Jia-Jun Li, arXiv:2402.06089AI Assistance for UX: A Literature Review Through Human-Centered AI. 2024. 2024arXiv preprint</p>
<p>Bridging the Gap between UX Practitioners' work practices and AI-enabled design support tools. Yuwen Lu, Chengzhi Zhang, Iris Zhang, Toby Jia-Jun Li, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Influences on clinical judgement in mental health nursing. Martin Peter, NT Research. 441999. 1999</p>
<p>Clinical Judgement: an investigation of clinical decision-making. Benjamin Paul, Michael , 2019Ph. D. Dissertation. University of Sheffield</p>
<p>The credibility of dietary advice formulated by ChatGPT: robo-diets for people with food allergies. Paweł Niszczota, Iga Rybicka, Nutrition. 1121120762023. 2023</p>
<p>GPT-4 Research Overview. 2024OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>BLEU: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei Jing, Zhu , 2002</p>
<p>Is ChatGPT an Effective Tool for Providing Dietary Advice?. Valentina Ponzo, Ilaria Goitre, Enrica Favaro, Fabio Dario Merlo, Maria Vittoria Mancino, Sergio Riso, Simona Bo, Nutrients. 164692024. 2024</p>
<p>Sumedh Rasal, Hauer, arXiv:2402.16713Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs. 2024. 2024arXiv preprint</p>
<p>Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani, arXiv:2304.11406Lamp: When large language models meet personalization. 2023. 2023arXiv preprint</p>
<p>Shreya Shankar, Björn Zamfirescu-Pereira, Aditya G Hartmann, Ian Parameswaran, Arawjo, arXiv:2404.12272Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. 2024. 2024arXiv preprint</p>
<p>Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun, Haijun Li, Xia, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>Annalisa Szymanski, Araya Simret, Oghenemaro Gebreegziabher, Ronald A Anuyah, Toby Jia-Jun Metoyer, Li, arXiv:2410.02054Comparing Criteria Development Across Domain Experts, Lay Users, and Models in Large Language Model Evaluation. 2024. 2024arXiv preprint</p>
<p>Integrating Expertise in LLMs: Crafting a Customized Nutrition Assistant with Refined Template Instructions. Annalisa Szymanski, Brianna L Wimer, Oghenemaro Anuyah, Heather A Eicher-Miller, Ronald A Metoyer, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing Systems2024</p>
<p>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421arXiv:1811.00937Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, Yun-Nung Chen, arXiv:2406.01171Two tales of persona in llms: A survey of role-playing and personalization. 2024. 2024arXiv preprint</p>
<p>. Ruth Vo, Smith, Patton, Journal of Human Nutrition and Dietetics. 342021. 2021</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.129662023. 2023arXiv preprint</p>
<p>Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Xiang-Bo Mao, Sitaram Asur, arXiv:2407.16216A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More. 2024. 2024arXiv preprint</p>
<p>Mental-llm: Leveraging large language models for mental health prediction via online text data. Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K Dey, Dakuo Wang, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. 82024. 2024</p>
<p>ChatDiet: Empowering personalized nutrition-oriented food recommender chatbots through an LLM-augmented framework. Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, Amir, Rahmani, Smart Health. 321004652024. 2024</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Visar: A human-ai argumentative writing assistant with visual programming and rapid draft prototyping. Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, Toby Jia-Jun Li, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024. 2024Manuscript submitted to ACM</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, arXiv:2310.176312023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
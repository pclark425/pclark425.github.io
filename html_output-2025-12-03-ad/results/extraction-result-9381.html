<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9381 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9381</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9381</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-509cc6c16e1ccea46fdf1b9c23beff7228cd33c3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/509cc6c16e1ccea46fdf1b9c23beff7228cd33c3" target="_blank">Batch Universal Prediction</a></p>
                <p><strong>Paper Venue:</strong> International Symposium on Information Theory</p>
                <p><strong>Paper TL;DR:</strong> The notion of batch regret is introduced as a modification of the classical average regret, and its asymptotical value is studied for add-constant predictors, in the case of memoryless sources and first-order Markov sources.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences. LLMs are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9381",
    "paper_id": "paper-509cc6c16e1ccea46fdf1b9c23beff7228cd33c3",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00401875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Batch Universal Prediction</h1>
<p>Marco Bondaschi and Michael Gastpar<br>School of Computer and Communication Sciences<br>EPFL<br>Switzerland<br>Email: {marco.bondaschi, michael.gastpar}@epfl.ch</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences. LLMs are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.</p>
<h2>I. INTRODUCTION</h2>
<p>Prediction refers to the problem of estimating the next symbols of a sequence given its past, and evaluating the confidence of such an estimate. Such a problem appears in a large number of research areas, such as information theory, statistical decision theory, finance, and machine learning. Some knowledge about the probability distribution that models the sequence to be predicted is clearly helpful. Unfortunately, in many practical applications such knowledge is partial or missing. If this is the case, then one may wish to say something about the future of the sequence when the true model of the source that is producing the symbols is any of the models belonging to a certain class. This problem usually goes under the name of universal prediction [1], and it found applications in a wide range of areas, such as compression [2], [3], gambling [4] and machine learning [5], [6]. With the recent rise in popularity of large language models (LLMs), the problem of universal prediction is more timely than ever. In fact, LLMs are essentially predictors: given an $n$-word input sequence, language models can output an estimated probability for the next $\ell$ words in an online fashion. Furthermore, LLMs can be interpreted as universal predictors, in the sense that before training, the model does not assume any information about the source generating the input data, and can therefore be used in principle for next-word prediction of data from any distribution, the model improving its prediction accuracy as more and more input data are fed into the network.</p>
<p>Formally, given a finite input alphabet $\mathcal{X}$, for every sequence of symbols $x^{i}=\left(x_{1}, x_{2}, \ldots, x_{i}\right) \in \mathcal{X}^{i}$, a predictor $\hat{p}$ assigns a probability $\hat{p}\left(y \mid x^{i}\right)$ for the $(i+1)$-th symbol to be equal to $y \in \mathcal{X}$ given the past $x^{i}$. In universal prediction, a predictor is generally required to perform well if the data is generated according to any distribution in a given class $\mathcal{P}$. In order to evaluate the quality of the estimates of a predictor, a loss function is used. In language models, this loss function
is usually the logarithmic (or cross-entropy) loss, which is defined point-wise as</p>
<p>$$
L\left(\hat{p}, y \mid x^{i}\right)=-\log \hat{p}\left(y \mid x^{i}\right)
$$</p>
<p>In the case $\ell$ sequential symbols have to be predicted, the cumulative loss equals
$L\left(\hat{p}, y^{\ell} \mid x^{i}\right)=-\log \hat{p}\left(y^{\ell} \mid x^{i}\right), \quad \hat{p}\left(y^{\ell} \mid x^{i}\right)=\prod_{j=1}^{\ell} \hat{p}\left(y_{j} \mid x^{i}, y^{j-1}\right)$.
For a given ground-truth distribution $p \in \mathcal{P}$, the regret is defined as the difference between the loss of a candidate predictor $\hat{p}$ and that of $p$, i.e.,</p>
<p>$$
R\left(\hat{p}, p, y^{\ell} \mid x^{i}\right)=L\left(\hat{p}, y^{\ell} \mid x^{i}\right)-L\left(p, y^{\ell} \mid x^{i}\right)=\log \frac{p\left(y^{\ell}\right)}{\hat{p}\left(y^{\ell} \mid x^{i}\right)}
$$</p>
<p>The average regret is defined as the expected regret over sequences distributed according to $p$,</p>
<p>$$
\begin{aligned}
R(\hat{p}, p) &amp; =\mathbb{E}<em x_i="x^{i">{p}\left[R\left(\hat{p}, p, Y^{\ell} \mid X^{i}\right)\right] \
&amp; =\sum</em>
\end{aligned}
$$}} p\left(x^{i}\right) \sum_{y^{\ell}} p\left(y^{\ell}\right) \log \frac{p\left(y^{\ell}\right)}{\hat{p}\left(y^{\ell} \mid x^{i}\right)</p>
<p>The maximal average regret is the maximum average regret over all distributions in $\mathcal{P}$, i.e., $R(\hat{p})=\max _{p \in \mathcal{P}} R(\hat{p}, p)$.</p>
<p>In universal prediction literature, it is customary to consider the regret for the prediction of an entire sequence of $n$ symbols [1], [7]. More precisely, a predictor is defined to output an estimated probability for every $n$-sequence $y^{n}$, which is denoted by $\hat{p}\left(y^{n}\right)$. The average regret is then $R_{n}(\hat{p}, p)=$ $\sum_{y^{n}} p\left(y^{n}\right) \log \frac{p\left(y^{n}\right)}{\hat{p}\left(y^{n}\right)}$. This case has been studied extensively, in particular for the memoryless case, where $\mathcal{P}$ is the class of distributions generating i.i.d. symbols. For this case, the asymptotical expression for $R_{n}(\hat{p}, p)$ as $n \rightarrow \infty$ has been derived [8]. Furthermore, the add- $\frac{1}{2}$ predictor, also called Krichevsky-Trofimov predictor, has been shown to be almost asymptotically optimal, in the sense that its asymptotic regret is larger than the optimal one only by a constant independent of $n$.</p>
<p>However, the rise of LLMs and the importance of viewing them from a universal prediction perspective requires a different paradigm than the one presented above. In fact, LLMs are trained and tested on batches of data. In particular, during the training phase a LLM model is fed $n$ batches of data, independent of each other, each of them made of $\ell$ samples.</p>
<p>At the end of the training phase, the LLM performance is then measured over a fresh test batch of $\ell$ samples. In order to be able to fairly evaluate LLMs from a universal prediction viewpoint, we introduce a new form of regret, batch regret, defined as follows.</p>
<p>Definition 1: Let $\boldsymbol{x}^{n}=\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right)$ be a training sequence of $n$ batches, each made of $\ell$ samples, $\boldsymbol{x}^{(j)}=$ $\left(x_{1}^{(j)}, \ldots, x_{\ell}^{(j)}\right)$. Let $\hat{p}\left(y^{\ell} \mid \boldsymbol{x}^{n}\right)$ be a predictor that, given $\boldsymbol{x}^{n}$, estimates the probability of a fresh sequence $y^{\ell}$ of $\ell$ samples, generated independently of $\boldsymbol{x}^{n}$. Batch regret is then defined as</p>
<p>$$
R(\hat{p}, p) \triangleq \sum_{\boldsymbol{x}^{n}} p\left(\boldsymbol{x}^{(1)}\right) \cdots p\left(\boldsymbol{x}^{(n)}\right) \sum_{y^{\ell}} p\left(y^{\ell}\right) \log \frac{p\left(y^{\ell}\right)}{\hat{p}\left(y^{\ell} \mid \boldsymbol{x}^{n}\right)}
$$</p>
<p>The focus of this paper is to study the asymptotical batch regret for add-constant predictors, in the memoryless and firstorder Markov cases.</p>
<p>Remark. We note here that if $n=0$, then we fall back into the classic full-sequence setting described above and described in [1], [8]. If instead $\ell=1$, we fall into the next-symbol prediction studied by Krichevsky [9]. In the full-sequence setting, the Krichevsky-Trofimov predictor is asymptotically the best "add-constant" estimator. In the next-symbol setting, instead, the best add-constant estimator requires a slightly larger constant $\beta_{0}=0.50922 \ldots$ Our setting can be thought as unifying and generalizing those two perspectives.</p>
<p>Notation. The regime of $\ell$ and $n$ that is perhaps most important from a LLM perspective is the one where the batch length $\ell=\ell(n)$ is a function of $n$ such that $\lim <em _infty="\infty" _rightarrow="\rightarrow" n="n">{n \rightarrow \infty} \ell(n)=\infty$ and $\lim </em> n \ell(n) f(n, \ell(n))=0$. All logarithms are considered to be in base $e$.} \frac{\ell(n)}{n}&lt;\infty$. This is the regime of $n$ and $\ell$ considered everywhere in this paper, unless differently specified. The little-o notation $f(n, \ell)=o\left(\frac{1}{n \ell}\right)$ means that $\lim _{n \rightarrow \infty</p>
<h2>A. Related work</h2>
<p>The asymptotics of average regret for memoryless sources has been fully characterized in [8]. Markov sources from a universal prediction perspective have been studied in [4], [10], [11] for the worst-case regret case and recently in [12], [13] for the average regret case. A form of conditional average regret, that matches batch regret in the special case of memoryless sources, was introduced in [14], where it is studied for the location family of distributions. More general forms of regret, defined in terms of Rényi divergence, have been studied in [15], [16] for the memoryless case.</p>
<h2>B. Overview</h2>
<p>The remainder of the paper is organized as follows. In Section II we study the binary memoryless case, that is, we consider $\mathcal{P}$ to be the class of distributions generating i.i.d. binary digits. In Section III we study the first-order Markov case, where the next bit is generated according to a distribution that solely depends on the previous bit. In this work we stick to binary alphabet. However, most of the results can be generalized to arbitrary finite alphabets, although proofs become more involved in the general case.</p>
<h2>II. MEMORYLESS SOURCES</h2>
<p>In this section we focus on the following setting. Let $\mathcal{X}={0,1}$ be the binary alphabet, and let $\mathcal{P}$ be the class of memoryless sources, i.e., sources generating i.i.d. binary digits with a given probability,</p>
<p>$$
\mathcal{P}=\left{p_{\theta}\left(x^{i}\right)=\theta^{n_{1}}(1-\theta)^{n_{0}}, \theta \in[0,1], \text { for any } i \in \mathbb{N}^{+}\right}
$$</p>
<p>where $n_{1}$ and $n_{0}$ are the number of ones and zeros in $x^{i}$, respectively. In this setting, the average regret is</p>
<p>$$
\begin{aligned}
R(\hat{p}, \theta)= &amp; \sum_{\ell_{1}=0}^{t}\binom{t}{t_{1}} \theta^{t_{1}}(1-\theta)^{t_{0}} \
&amp; \quad \sum_{\ell_{1}=0}^{\ell}\binom{\ell}{\ell_{1}} \theta^{\ell_{1}}(1-\theta)^{\ell_{0}} \log \frac{\theta^{\ell_{1}}(1-\theta)^{\ell_{0}}}{\hat{p}\left(y^{\ell} \mid x^{t}\right)}
\end{aligned}
$$</p>
<p>where $t=n \ell$. We note here the performance of two naive predictors. The first predictor is an add-constant predictor that estimates the probability of the new batch $y^{\ell}$ ignoring the past. Such a predictor is defined, for a chosen parameter $\frac{1}{2} \leq \beta \leq 1$, by</p>
<p>$$
\hat{p}\left(y^{\ell} \mid x^{t}\right)=\prod_{i=1}^{\ell} \hat{p}\left(y_{i} \mid x^{t}, y^{i-1}\right)
$$</p>
<p>where</p>
<p>$$
\hat{p}\left(y_{i}=1 \mid x^{t}, y^{i-1}\right)=\frac{\ell_{1}^{(i-1)}+\beta}{i-1+2 \beta}
$$</p>
<p>and $\ell_{1}^{(i-1)}$ is the number of ones in the sequence $y^{i-1}$. For such a predictor, the average regret simply becomes the average regret in the full-sequence of length $\ell$ described before. In this case, $\beta=\frac{1}{2}$ achieves the lowest regret, which asymptotically equals $R\left(\hat{p}_{1 / 2}\right) \approx \frac{1}{2} \log \ell+c$. The second naive predictor estimates the probability of a sequence only considering the training sequence $x^{t}$ and none of the past symbols of $y^{\ell}$. Such a predictor is</p>
<p>$$
\hat{p}<em 1="1">{\beta}\left(y^{\ell} \mid x^{t}\right)=\left(\frac{t</em>
$$}+\beta}{t+2 \beta}\right)^{\ell_{1}}\left(\frac{t_{0}+\beta}{t+2 \beta}\right)^{\ell_{0}</p>
<p>In this case the regret would be equal to $\ell$ times the Krichevsky next-symbol regret. The best constant would then be $\beta=\beta_{0}$ and the asymptotic regret would be $R(\hat{p}) \approx \frac{\beta_{0} \ell}{t}+o\left(\frac{1}{t}\right)=\frac{\beta_{0}}{n}+$ $o\left(\frac{1}{n \ell}\right)$. However, both these naive predictors are suboptimal. In this section we study the add-constant predictor that takes into account both $x^{t}$ and $y^{\ell}$ at the same time. The predictor is defined by</p>
<p>$$
\hat{p}<em i="1">{\beta}\left(y^{\ell} \mid x^{t}\right)=\prod</em>}^{\ell} \hat{p<em i="i">{\beta}\left(y</em>\right)
$$} \mid x^{t}, y^{i-1</p>
<p>where</p>
<p>$$
\hat{p}<em i="i">{\beta}\left(y</em>
$$}=1 \mid x^{t}, y^{i-1}\right)=\frac{t_{1}+\ell_{1}^{(i-1)}+\beta}{t+i-1+2 \beta</p>
<p>for a chosen $\frac{1}{2} \leq \beta \leq 1$. One can think of this predictor as a refined version of the one in (11), where the added constant is updated every time a new symbol from $y^{\ell}$ is revealed. The predictor in (12) can be rewritten using Gamma functions as</p>
<p>$$
\hat{p}<em 1="1">{\beta}\left(y^{\ell} \mid x^{t}\right)=\frac{\Gamma\left(t</em>
$$}+\ell_{1}+\beta\right) \Gamma\left(t_{0}+\ell_{0}+\beta\right) \Gamma(t+2 \beta)}{\Gamma\left(t_{1}+\beta\right) \Gamma\left(t_{0}+\beta\right) \Gamma(t+\ell+2 \beta)</p>
<p>or again, using properties of the Gamma function, as</p>
<p>$$
\hat{p}<em 0="0">{\beta}\left(y^{\ell} \mid x^{t}\right)=\int</em>\right) d \theta
$$}^{1} w_{\beta}(\theta \mid x^{t}) p_{\theta}\left(y^{\ell</p>
<p>Hence, the predictor can also be interpreted as a mixture predictor, where the prior distribution on the parameter space $w_{\beta}$ depends on the training sequence $x^{t}$.</p>
<p>For this predictor we have the following results about the batch regret. Theorem 1 concerns the regret in the interior of the simplex, while Theorem 2 concerns the boundary.</p>
<p>Theorem 1: Let $\delta&lt;\frac{1}{2}$ and $\Theta=[\delta, 1-\delta]$. Let</p>
<p>$$
\mathcal{P}<em _theta="\theta">{\delta}=\left{p</em>\right}
$$}\left(x^{i}\right)=\theta^{n_{1}}(1-\theta)^{n_{0}}, \theta \in \Theta, \text { for any } i \in \mathbb{N}^{+</p>
<p>be the class of distributions under consideration. Then,</p>
<p>$$
\begin{aligned}
\max <em _beta="\beta">{\Theta} R\left(\hat{p}</em>\right) \
&amp; =\frac{1}{2} \log \left(1+\frac{1}{n}\right)+o\left(\frac{1}{n \ell}\right)
\end{aligned}
$$}, \theta\right) &amp; =\frac{1}{2} \log \frac{t+\ell}{t}+o\left(\frac{1}{t</p>
<p>Proof: The following proof follows and extends the one in [8]. We start by rewriting the regret as</p>
<p>$$
\begin{aligned}
R\left(\hat{p}<em z__1="z_{1">{\beta}, \theta\right) &amp; =\sum</em> \
&amp; -\sum_{t_{1}=0}^{t}\binom{t}{t_{1}} \theta^{t_{1}}(1-\theta)^{t_{0}} \log \frac{\theta^{t_{1}}(1-\theta)^{t_{0}}}{\frac{\Gamma\left(t_{1}+\beta\right) \Gamma\left(t_{0}+\beta\right)}{\Gamma(t+2 \beta)}}
\end{aligned}
$$}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}} \log \frac{\theta^{z_{1}}(1-\theta)^{z_{0}}}{\frac{\Gamma\left(z_{1}+\beta\right) \Gamma\left(z_{0}+\beta\right)}{\Gamma(z+2 \beta)}</p>
<p>where $z=t+\ell$. We analyze the first term in the equation. The second term can be analyzed similarly. We will extensively use the equality</p>
<p>$$
\Gamma(x)=\sqrt{2 \pi} x^{x-\frac{1}{2}} e^{-x} e^{s}
$$</p>
<p>for some $s \in\left[0, \frac{1}{12 \pi}\right]$. Using this equation in the first term of the right-hand side of (19) we get</p>
<p>$$
\begin{aligned}
&amp; \sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}} \log \frac{\theta^{z_{1}}(1-\theta)^{z_{0}}}{\frac{\Gamma\left(z_{1}+\beta\right) \Gamma\left(z_{0}+\beta\right)}{\Gamma(z+2 \beta)}} \
&amp; =\sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}} \log \left{\sqrt{\frac{\left(z_{1}+\beta\right)\left(z_{0}+\beta\right)}{2 \pi(z+2 \beta)}}\right. \
&amp; \cdot \theta^{z_{1}}(1-\theta)^{z_{0}}\left(\frac{z_{1}+\beta}{z+2 \beta}\right)^{-z_{1}-\beta}\left(\frac{z_{0}+\beta}{z+2 \beta}\right)^{-z_{0}-\beta} \
&amp; \left.\cdot e^{s(z)-s_{1}\left(z_{1}\right)-s_{2}\left(z_{0}\right)}\right}
\end{aligned}
$$</p>
<p>We can split the last expression into three terms that will be analyzed separately:</p>
<p>$$
\begin{aligned}
&amp; -\sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}}\left(z_{1}+\beta-\frac{1}{2}\right) \log \left(z_{1}+\beta\right) \
&amp; -\sum_{z_{0}=0}^{z}\binom{z}{z_{0}} \theta^{z_{1}}(1-\theta)^{z_{0}}\left(z_{0}+\beta-\frac{1}{2}\right) \log \left(z_{0}+\beta\right) \
&amp; -\frac{1}{2} \log (2 \pi)+\left(z+2 \beta-\frac{1}{2}\right) \log (z+2 \beta) \
&amp; \sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}}\left(s(z)-s_{1}\left(z_{1}\right)-s_{0}\left(z_{0}\right)\right)
\end{aligned}
$$</p>
<p>For term (A), consider only the first half (the second follows by symmetry), with the positive sign for simplicity. We can split it into two further terms</p>
<p>$$
\begin{aligned}
&amp; \sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}}\left(z_{1}+\beta-\frac{1}{2}\right) \log \left(z_{1}+\beta-\frac{1}{2}\right) \
&amp; \sum_{z_{1}=0}^{z}\binom{z}{z_{1}} \theta^{z_{1}}(1-\theta)^{z_{0}}\left(z_{1}+\beta-\frac{1}{2}\right) \
&amp; \log \left(1+\frac{1}{2\left(z_{1}+\beta-\frac{1}{2}\right)}\right)
\end{aligned}
$$</p>
<p>We analyze term ( $\mathrm{A}^{\prime}$ ), which equals $\mathbb{E}\left[\left(Z_{1}+\beta-\frac{1}{2}\right) \log \left(Z_{1}+\right.\right.$ $\left.\left.\beta-\frac{1}{2}\right)\right]$ for $Z_{1} \sim \operatorname{Binomial}(z, \theta)$. Using the bounds (valid for $a \geq 0$ and $b&gt;0$ )</p>
<p>$$
a \log a \geq b \log b+(a-b)(1+\log b)+\frac{(a-b)^{2}}{2 b}-\frac{(a-b)^{3}}{6 b^{2}}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
a \log a \leq b \log b+(a-b)(1 &amp; +\log b)+\frac{(a-b)^{2}}{2 b} \
&amp; -\frac{(a-b)^{3}}{6 b^{2}}+\frac{(a-b)^{4}}{3 z^{3}}
\end{aligned}
$$</p>
<p>with $a=Z_{1}+\beta-\frac{1}{2}$ and $b=z \theta+\beta-\frac{1}{2}$, we have</p>
<p>$$
\begin{aligned}
\mathbb{E}[ &amp; \left(Z_{1}+\beta-\frac{1}{2}\right) \log \left(Z_{1}+\beta-\frac{1}{2}\right)] \
&amp; \geq\left(z \theta+\beta-\frac{1}{2}\right) \log \left(z \theta+\beta-\frac{1}{2}\right) \
&amp; +\frac{1-\theta}{2}-\frac{1}{z \theta}
\end{aligned}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
\mathbb{E}[ &amp; \left(Z_{1}+\beta-\frac{1}{2}\right) \log \left(Z_{1}+\beta-\frac{1}{2}\right)] \
&amp; \leq\left(z \theta+\beta-\frac{1}{2}\right) \log \left(z \theta+\beta-\frac{1}{2}\right) \
&amp; +\frac{1-\theta}{2}+\frac{1}{z \theta}
\end{aligned}
$$</p>
<p>for $z$ large enough such that $z \theta \geq 2$. For the term ( $\mathrm{A}^{\prime \prime}$ ) we have
$\frac{1}{2}-\frac{1}{2 z \theta} \leq \mathbb{E}\left[\left(Z_{1}+\beta-\frac{1}{2}\right) \log \left(1+\frac{1}{2\left(Z_{1}+\beta-\frac{1}{2}\right)}\right)\right] \leq \frac{1}{2}$
which follows from the fact that for $a \geq 0$,</p>
<p>$$
\frac{1}{2}-\frac{1}{2(a+1)} \leq a \log \left(1+\frac{1}{2 a}\right) \leq \frac{1}{2}
$$</p>
<p>and from the fact that $\mathbb{E}\left[1 /\left(Z_{1}+1\right)\right] \leq 1 / z \theta$. The (straightforward) analysis of terms (B) and (C) is left in the Appendix. The same analysis worked out for the first term of (19) can be carried out for the second term as well, just with $t$ in place of $z$. Putting the bounds together leads to the theorem.</p>
<p>Theorem 2: Let $\theta \in{0,1}$. Then</p>
<p>$$
\begin{aligned}
R\left(\hat{p}_{\beta}, \theta\right) &amp; =\beta \log \frac{t+\ell}{t}+o\left(\frac{1}{t}\right) \
&amp; =\beta \log \left(1+\frac{1}{n}\right)+o\left(\frac{1}{n \ell}\right)
\end{aligned}
$$</p>
<p>Proof: See the Appendix.
Note that Theorem 1 and 2 are not enough to prove the exact asymptotics of the batch regret in the case the class of distributions is parametrized by the entire interval $\Theta=[0,1]$. How the regret behaves in this case is open and left for future work.</p>
<h2>III. First-order Markov sources</h2>
<p>In this section we deal with the class of binary Markov sources of order 1, i.e., the class</p>
<p>$$
\mathcal{P}=\left{p_{\theta}\left(x^{i}\right)=p_{1}\left(x_{1}\right) \prod_{j=2}^{i} p\left(x_{j} \mid x_{j-1}\right)\right}
$$</p>
<p>For notation purposes we define $p_{1}=p_{1}\left(x_{1}=1\right), p=p(1 \mid 0)$, $q=p(0 \mid 1)$ and $\theta=\left(p_{1}, p, q\right) \in[0,1]^{3}$. While in traditional full-sequence universal prediction or in the Krichevsky nextsymbol prediction, the value of the initial distribution $p_{1}$ is asymptotically unimportant, in the batch prediction setting described in this paper it gets a more prominent role. In fact, the training data is a collection of $n$ fresh batches of $\ell$ bits each, where each batch $\boldsymbol{x}^{(i)}$ is generated independently of the others according to a fixed Markov source, i.e.,</p>
<p>$$
p_{\theta}\left(\boldsymbol{x}^{(i)}\right)=p_{1}\left(x_{1}^{(i)}\right) \prod_{j=2}^{\ell} p\left(x_{j}^{(i)} \mid x_{j-1}^{(i)}\right)
$$</p>
<p>Due to this different feature, depending on the regime of $\ell$ and $n$, the starting distribution $p_{1}$ becomes fundamental. Hence, we must introduce an estimator for the starting distribution $p_{1}$ as well. For a predictor in the form</p>
<p>$$
\hat{p}\left(y^{\ell} \mid \boldsymbol{x}^{n}\right)=\hat{p}<em 1="1">{1}\left(y</em>\right)
$$} \mid \boldsymbol{x}^{n}\right) \hat{p}\left(y_{2}^{\ell} \mid \boldsymbol{x}^{n}, y_{1</p>
<p>the batch regret equals</p>
<p>$$
\begin{aligned}
R(\hat{p}, \theta) &amp; =\sum_{\boldsymbol{x}^{n}} p_{\theta}\left(\boldsymbol{x}^{n}\right) \cdots p_{\theta}\left(\boldsymbol{x}^{(n)}\right) \sum_{y^{\ell}} p_{\theta}\left(y^{\ell}\right) \log \frac{p_{\theta}\left(y^{\ell}\right)}{\hat{p}\left(y^{\ell} \mid \boldsymbol{x}^{n}\right)} \
&amp; =\sum_{\boldsymbol{x}^{n}} p_{\theta}\left(\boldsymbol{x}^{n}\right) \sum_{y} p_{1}(y) \log \frac{p_{1}(y)}{\hat{p}<em _boldsymbol_x="\boldsymbol{x">{1}\left(y \mid \boldsymbol{x}^{n}\right)} \
&amp; +\sum</em>}^{n}} p_{\theta}\left(\boldsymbol{x}^{n}\right) \sum_{y} p_{1}(y) \sum_{y_{2}^{\ell}} p_{\theta}\left(y_{2}^{\ell} \mid y_{1}\right) \log \frac{p_{\theta}\left(y_{2}^{\ell} \mid y_{1}\right)}{\hat{p<em 2="2">{\beta}\left(y</em>
\end{aligned}
$$}^{\ell} \mid \boldsymbol{x}^{n}, y\right)</p>
<p>where $p_{\theta}\left(\boldsymbol{x}^{n}\right)=\prod_{i=1}^{n} p_{\theta}\left(\boldsymbol{x}^{(i)}\right)$. The last expression shows that the regret can be seen as the sum of two terms: the first term is the regret for the estimation of the initial distribution, while the second term is the regret for the estimation of the transition probabilities of the Markov source. We now derive upper bounds for the two terms for Markov sources with positive transition probabilities. Both bounds are asymptotically proportional to $\frac{1}{n}$, so that neither of them can be neglected.</p>
<p>Theorem 3: Let $\mathcal{P}$ be the class of first-order binary Markov sources as in (29). The initial distribution regret</p>
<p>$$
R_{1}(\hat{p}, \theta)=\sum_{\boldsymbol{x}^{n}} p_{\theta}\left(\boldsymbol{x}^{n}\right) \sum_{y} p_{1}(y) \log \frac{p_{1}(y)}{\hat{p}_{1}\left(y \mid \boldsymbol{x}^{n}\right)}
$$</p>
<p>is upper bounded by</p>
<p>$$
\min <em 1="1">{\hat{p}</em> \max }<em 1="1">{\theta} R</em>\right)
$$}(\hat{p}, \theta) \leq \frac{\beta_{0}}{n}+o\left(\frac{1}{n</p>
<p>Proof: Let $\boldsymbol{x}<em 1="1">{1}=\left(x</em>\right)$ be the sequence of bits in the first coordinate of each batch, and consider the predictor}^{(1)}, x_{1}^{(1)}, \ldots, x_{1}^{(n)</p>
<p>$$
\hat{p}<em 1="1">{1}\left(y \mid \boldsymbol{x}^{n}\right)=\hat{p}</em>}\left(y \mid \boldsymbol{x<em 1="1">{1}\right)=\frac{t</em>
$$}+\beta_{0}}{n+2 \beta_{0}</p>
<p>where $t_{1}=\sum_{i=1}^{n} x_{1}^{(i)}$ is the number of ones in $\boldsymbol{x}_{1}$. Then,</p>
<p>$$
R_{1}(\hat{p}, \theta)=\sum_{\boldsymbol{x}<em 1="1">{1}} p</em>}\left(\boldsymbol{x<em y="y">{1}\right) \sum</em>} p_{1}(y) \log \frac{p_{1}(y)}{\hat{p<em 1="1">{1}\left(y \mid \boldsymbol{x}</em>
$$}\right)</p>
<p>The last expression is precisely Krichevsky's next-symbol regret for the add- $\beta_{0}$ predictor. Hence, Krichevsky's bound can be applied, leading to</p>
<p>$$
\max <em 1="1">{\theta} R</em>\right)
$$}(\hat{p}, \theta) \leq \frac{\beta_{0}}{n}+o\left(\frac{1}{n</p>
<p>Furthermore, if we limit the class of predictors to those than only depend on the first coordinate of each batch, $\boldsymbol{x}<em 1="1">{1}=\left(x</em>, \theta)$ follows directly from Krichevsky's lower bound on next-symbol prediction [9, Theorem 2].}^{(1)}, x_{1}^{(1)}, \ldots, x_{1}^{(n)}\right)$, then the following lower bound on $R_{1}(\hat{p</p>
<p>Theorem 4: Let $\hat{\mathcal{P}}$ be the class of predictors $\hat{p}<em 1="1">{1}\left(y \mid \boldsymbol{x}^{n}\right)=$ $\hat{p}</em>}\left(y \mid \boldsymbol{x<em 1="1">{1}\right)$ that only depend on $\boldsymbol{x}</em>$. Then,</p>
<p>$$
\min <em 1="1">{\hat{p}</em> \max } \in \hat{\mathcal{P}}<em 1="1">{\theta} R</em>\right)
$$}(\hat{p}, \theta) \geq \frac{1}{2 n}+o\left(\frac{1}{n</p>
<p>Remark. Theorem 3 suggests that one should use an add$\beta_{0}$ predictor with the first coordinate of the $n$ batches when predicting the initial distribution of a Markov source. However, one can possibly achieve lower regret with a predictor that also uses the other coordinates of the batches to estimate the initial distribution of the Markov source. In fact, information about $p_{1}$ also leaks to other coordinates. Note that the second symbol of each batch is distributed according to</p>
<p>$\operatorname{Pr}\left(X_{2}=1\right)=p_{1}(1-p-q)+p.$ (39)</p>
<p>By inverting the last expression, one gets</p>
<p>$p_{1}=\frac{\operatorname{Pr}\left(X_{2}=1\right)-p}{1-p-q}$ (40)</p>
<p>Therefore, one can combine estimators for $\operatorname{Pr}\left(X_{2}=1\right)$, $p$ and $q$ to derive a predictor for $p_{1}$. Obvious choices would be to estimate $\operatorname{Pr}\left(X_{2}=1\right)$ with an add-constant estimator based on the counts in the second coordinate of the $n$ batches $\boldsymbol{x}^{n}$, while add-constant predictors based on the counts of transitions in the entire training set could be used to estimate $p$ and $q$. One can derive similar predictors for $p_{1}$ from the other coordinates as well, since in general $\operatorname{Pr}\left(X_{j}=1\right)=\left(p_{1}-\pi_{1}\right)(1-p-q)^{j-1}+\pi_{1}$, where $\pi_{1}=\frac{p}{p+q}$ is the probability assigned to 1 by the stationary distribution of the Markov source. It is therefore natural to average those $\ell$ predictors of $p_{1}$ to achieve lower regret than the one obtained by only considering the first coordinate of the batches.</p>
<p>The following is an upper bound for the regret on the transition probabilities of the Markov source, provided that all transition probabilities are bounded away from zero.</p>
<p>Theorem 5: Let $0&lt;\delta&lt;\frac{1}{2}$ and let $\mathcal{P}_{\delta}$ be the class of first-order binary Markov sources as in (29), such that $p, q \in$ $[\delta, 1-\delta]$. The transition probability regret</p>
<p>$$
R_{\mathrm{T}}(\hat{p}, \theta)=\sum_{\boldsymbol{x}^{n}} p_{\theta}\left(\boldsymbol{x}^{n}\right) \sum_{y^{\ell}} p_{\theta}\left(y^{\ell}\right) \log \frac{p_{\theta}\left(y_{2}^{\ell} \mid y_{1}\right)}{\hat{p}\left(y_{2}^{\ell} \mid \boldsymbol{x}^{n}, y_{1}\right)}
$$</p>
<p>is upper-bounded by</p>
<p>$$
\min <em p__theta="p_{\theta">{\hat{p}} \max </em>} \in \mathcal{P<em _mathrm_T="\mathrm{T">{\delta}} R</em>\right)
$$}}(\hat{p}, \theta) \leq \frac{1}{2 n}+o\left(\frac{1}{n \ell</p>
<p>Proof: Consider the predictor</p>
<p>$$
\hat{p}\left(y_{2}^{\ell} \mid y_{1}\right)=\prod_{h, k \in{0,1}^{2}}\left(\frac{t_{h k}+\frac{1}{2}}{t_{h}+1}\right)^{\ell_{h k}}
$$</p>
<p>Let $\pi$ be the stationary distribution of the Markov source. Note that $\mathbb{E}\left[L_{h k}\right]=\ell \pi(h) p(k \mid h)+o\left(\frac{1}{\ell}\right)$. Furthermore, we have</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[T_{h k}\right] &amp; =\sum_{i=1}^{n} \mathbb{E}\left[T_{h k}^{(i)}\right] \
&amp; =n \ell \pi(h) p(k \mid h)+o(n / \ell)
\end{aligned}
$$</p>
<p>Hence, we can write</p>
<p>$$
\begin{aligned}
R_{\mathrm{T}}(\hat{p}, \theta) &amp; =\mathbb{E}\left[\sum_{h k} L_{h k} \log \frac{p(k \mid h)}{\frac{T_{h k}+\frac{1}{2}}{T_{h}+1}}\right] \
&amp; =\sum_{h k} \mathbb{E}\left[L_{h k}\right] \mathbb{E}\left[\log \frac{p(k \mid h)}{\frac{T_{h k}+\frac{1}{2}}{T_{h}+1}}\right] \
&amp; =\ell \sum_{h} \pi(h) \mathbb{E}\left[\sum_{k} p(k \mid h) \log \frac{p(k \mid h)}{\frac{T_{h k}+\frac{1}{2}}{T_{h}+1}}\right]+o\left(\frac{1}{n \ell}\right)
\end{aligned}
$$</p>
<p>The expectation in the last expression is the estimation risk denoted by $\hat{\epsilon}$ in [13], except for one key difference: here the expectation is over $n$ independent batches of length $\ell$ each, while in [13] it is over one batch of length $n \ell$. However, the proof provided in [13, Section 10] also works in the batch case presented here. In fact, the key ingredient in the original proof is the fact that $T_{h}$ and $T_{h k}$ are highly concentrated around their mean. The same holds in the batch case, since $T_{h}$ and $T_{h k}$ are sums of independent, highly concentrated random variables: applying [13, Lemma 19] and Hoeffding's inequality to $T_{h k}$ and $T_{h}$ gives the concentration bound $\operatorname{Pr}\left(\left|T_{h}-n \ell \pi(h)\right|&gt;\right.$ $t) \lesssim \exp \left(-t^{2} / n \ell\right)$. Hence, one can prove that</p>
<p>$$
\mathbb{E}\left[\sum_{k} p(k \mid h) \log \frac{p(k \mid h)}{\frac{T_{h k}+\frac{1}{2}}{T_{h}+1}}\right] \leq \frac{1}{2 n \ell}+o\left(\frac{1}{n \ell}\right)
$$</p>
<p>Putting this into (48) leads to the theorem.
Similarly as the bound on the regret for the initial distribution, also in this case a lower bound proportional to $\frac{1}{n}$ can be derived if one limits the class of predictors to those in the form (43). Using also the latter information can lead to a lower regret. A natural modified version of the add-constant predictor for Markov sources that uses all available information takes the form</p>
<p>$$
\hat{p}<em 1="1">{\beta}\left(y^{\ell} \mid \boldsymbol{x}^{n}\right)=\hat{p}</em>}\left(y_{1} \mid \boldsymbol{x}^{n}\right) \prod_{j=2}^{\ell} \hat{p<em j="j">{\beta}\left(y</em>\right)
$$} \mid \boldsymbol{x}^{n}, y^{j-1</p>
<p>where</p>
<p>$$
\hat{p}<em j="j">{\beta}\left(y</em>
$$}=k \mid \boldsymbol{x}^{n}, y^{j-2}, y_{j-1}=h\right)=\frac{t_{h k}+\ell_{h k}^{(j-1)}+\beta}{t_{h}+\ell_{h}^{(j-1)}+2 \beta</p>
<p>where for $h, k \in{0,1}$ we define</p>
<p>$$
\begin{aligned}
t_{h k} &amp; =\sum_{i=1}^{n} t_{h k}^{(i)}, \quad t_{h k}^{(i)}=\mathrm{n} . \text { of consecutive } h k \text { in } \boldsymbol{x}^{(i)} \
t_{h} &amp; =\sum_{i=1}^{n} t_{h}^{(i)}, \quad t_{h}^{(i)}=\mathrm{n} . \text { of } h \text { in } \boldsymbol{x}^{(i)} \backslash x_{\ell}^{(i)}
\end{aligned}
$$</p>
<p>and where $\ell_{h k}^{(j-1)}$ is the number of consecutive $h k$ in $y^{j-1}$, while $\ell_{h}^{(j-1)}$ is the number of $h$ in $y^{j-2}$. Furthermore, $\hat{p}<em 1="1">{1}\left(y</em>$, even if the multiplying constant of the leading term may improve.} \mid x^{\ell}\right)$ is the predictor for the initial distribution $p_{1}$. However, we conjecture that one cannot find a predictor with a regret that decays asymptotically faster than $\frac{1}{n</p>
<h2>REFERENCES</h2>
<p>[1] N. Merhav and M. Feder, "Universal prediction," IEEE Trans. Inf. Theory, vol. 44, no. 6, pp. 2124-2147, 1998.
[2] J. Ziv and A. Lempel, "A universal algorithm for sequential data compression," IEEE Trans. Inf. Theory, vol. 23, no. 3, pp. 337-343, 1977.
[3] F. Willems, Y. Shtarkov, and T. Tjalkens, "The context-tree weighting method: basic properties," IEEE Trans. Inf. Theory, vol. 41, no. 3, pp. 653-664, 1995.
[4] Q. Xie and A. Barron, "Asymptotic minimax regret for data compression, gambling, and prediction," IEEE Trans. Inf. Theory, vol. 46, no. 2, pp. $431-445,2000$.
[5] Y. Fogel and M. Feder, "Universal learning of individual data," in Proc. 2019 IEEE Int. Symp. Inf. Theory (ISIT), 2019, pp. 2289-2293.
[6] F. E. Rosas, P. A. M. Mediano, and M. Gastpar, "Learning, compression, and leakage: Minimising classification error via meta-universal compression principles," in Proc. 2020 IEEE Inf. Theory Workshop (ITW), 2021.
[7] P. D. Grünwald, The minimum description length principle. Cambridge, MA: MIT Press, 2007.
[8] Q. Xie and A. Barron, "Minimax redundancy for the class of memoryless sources," IEEE Transactions on Information Theory, vol. 43, no. 2, pp. $646-657,1997$.
[9] R. Krichevskiy, "Laplace's law of succession and universal encoding," IEEE Transactions on Information Theory, vol. 44, no. 1, pp. 296-303, 1998.
[10] B. Y. Ryabko, "Twice-universal coding," Problems of information transmission, vol. 20, no. 3, pp. 173-177, 1984.
[11] ——, "Prediction of random sequences and universal coding," Problems of information transmission, vol. 24, no. 2, pp. 87-96, 1988.
[12] M. Falahatgar, A. Orlitsky, V. Pichapati, and A. T. Suresh, "Learning markov distributions: Does estimation trump compression?" in 2016 IEEE International Symposium on Information Theory (ISIT). IEEE, 2016, pp. 2689-2693.
[13] Y. Hao, A. Orlitsky, and V. Pichapati, "On learning markov chains," Advances in Neural Information Processing Systems, vol. 31, 2018.
[14] F. Liang and A. Barron, "Exact minimax strategies for predictive density estimation, data compression, and model selection," IEEE Transactions on Information Theory, vol. 50, no. 11, pp. 2708-2726, 2004.
[15] S. Yagli, Y. Altuğ, and S. Verdú, "Minimax Rényi redundancy," IEEE Trans. Inf. Theory, vol. 64, no. 5, pp. 3715-3733, 2018.
[16] M. Bondaschi and M. Gastpar, "Alpha-nml universal predictors," in 2022 IEEE International Symposium on Information Theory (ISIT). IEEE, 2022, pp. 468-473.</p>
<h1>APPENDIX</h1>
<h2>A. Analysis of terms (A) and (B) for the proof of Theorem 1</h2>
<p>For term (B), we can use the bound</p>
<p>$$
\frac{x}{1+x} \leq \log (1+x) \leq x
$$</p>
<p>valid for $x&gt;-1$, to easily get</p>
<p>$$
\begin{aligned}
&amp; \frac{1}{2} \log (2 \pi)-\left(z+2 \beta-\frac{1}{2}\right) \log (z+2 \beta) \
&amp; \leq \frac{1}{2} \log (2 \pi)-z \log z-\left(2 \beta-\frac{1}{2}\right) \log z-2 \beta+\frac{\beta}{z}
\end{aligned}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
&amp; \frac{1}{2} \log (2 \pi)-\left(z+2 \beta-\frac{1}{2}\right) \log (z+2 \beta) \
\geq &amp; \frac{1}{2} \log (2 \pi)-z \log z-\left(2 \beta-\frac{1}{2}\right) \log z-2 \beta-\frac{3 \beta}{z}
\end{aligned}
$$</p>
<p>Finally, for term (C) we have</p>
<p>$$
\mathbb{E}\left[s(z)-s_{1}\left(Z_{1}\right)-s_{0}\left(Z_{0}\right)\right] \leq \frac{1}{12 z}
$$</p>
<p>and</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[s(z)-s_{1}\left(Z_{1}\right)-s_{0}\left(Z_{0}\right)\right] &amp; \
&amp; \geq-\mathbb{E}\left[\frac{1}{12\left(Z_{1}+\beta\right)}\right]-\mathbb{E}\left[\frac{1}{12\left(Z_{0}+\beta\right)}\right] \
&amp; \geq-\frac{1}{12 z \theta}-\frac{1}{12 z(1-\theta)}
\end{aligned}
$$</p>
<p>where in the last step we used $\mathbb{E}\left[\frac{1}{Z_{1}+1}\right] \leq \frac{1}{z \theta}$.</p>
<h2>B. Proof of Theorem 2</h2>
<p>We prove the case $\theta=1$. The case $\theta=0$ follows in the same way due to symmetry. We have</p>
<p>$$
\begin{aligned}
R(\hat{p}, \theta=1) &amp; =\log \frac{\Gamma(t+\ell+2 \beta) \Gamma(t+\beta)}{\Gamma(t+\ell+\beta) \Gamma(t+2 \beta)} \
&amp; =\log \frac{\Gamma(t+\ell+2 \beta)}{\Gamma(t+\ell+\beta)}+\log \frac{\Gamma(t+\beta)}{\Gamma(t+2 \beta)}
\end{aligned}
$$</p>
<p>Using the double inequality</p>
<p>$$
\frac{x}{(x+s)^{1-s}} \leq \frac{\Gamma(x+s)}{\Gamma(x)} \leq x^{s}
$$</p>
<p>valid for $x&gt;0$ and $0&lt;s&lt;1$, we get the upper bound</p>
<p>$$
\begin{aligned}
R(\hat{p}, \theta=1) &amp; \leq \beta \log (t+\ell+\beta)+\log \frac{(t+2 \beta)^{1-\beta}}{t+\beta} \
&amp; \leq \beta \log \frac{t+\ell}{t}+\frac{\beta^{2}}{t+\ell}+\frac{2 \beta(1-\beta)}{t}
\end{aligned}
$$</p>
<p>and the lower bound</p>
<p>$$
\begin{aligned}
R(\hat{p}, \theta=1) &amp; \geq \log \frac{t+\ell+\beta}{(t+\ell+2 \beta)^{1-\beta}}-\beta \log (t+\beta) \
&amp; \geq \beta \log \left(\frac{t+\ell}{t}\right)-\frac{\beta}{t+\ell}-\frac{\beta^{2}}{t}
\end{aligned}
$$</p>
<p>These two bounds together prove the theorem.</p>            </div>
        </div>

    </div>
</body>
</html>
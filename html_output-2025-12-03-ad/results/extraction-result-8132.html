<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8132 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8132</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8132</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-260378835</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.01154v4.pdf" target="_blank">Arithmetic with Language Models: from Memorization to Computation</a></p>
                <p><strong>Paper Abstract:</strong> A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8132.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8132.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoding-Regression-Decoding hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesis that the LM solves arithmetic by (1) encoding token strings to internal value vectors, (2) performing the numeric computation as a regression in value space, and (3) decoding the result vector back to output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params) and nanoGPT (decoder-only, 298K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small Transformers trained from scratch on binary arithmetic with tiny vocabulary; primary model is an encoder-decoder (d_model=64, 6 encoder/6 decoder layers, 701K params); secondary model is nanoGPT decoder-only (d_model=64, 6 layers, 298K params).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>7-bit binary addition (output 8 bits) and 7-bit binary multiplication (output 14 bits)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Internal numerical (value) representations: inputs are mapped from token embeddings to value vectors (encoding), computation occurs as regression in value space (central decoder layers), and outputs are generated by decoding value vectors to token logits (final decoder layers).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Correlation of pairwise distances across input/output/token/value spaces and internal layer embeddings; ablation of model components; targeted holdout (VS_t and VS_v) to probe interpolation vs extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Model achieves near-perfect sequence accuracy on random train/val split: addition converged <50 epochs to ~100% sequence accuracy (validation MAE 0), multiplication converged ~250 epochs to near-100% sequence accuracy (validation MAE 1.3).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Extrapolation failure when contiguous regions of value space are excluded from training (VS_v): drops of ~4–6 percentage points for encoder-decoder and larger drops for nanoGPT (VS_v: encoder-decoder addition 93.7%, multiplication 94.3%; nanoGPT VS_v: addition 82.0%, multiplication 80.6%). Multiplication shows more nonlinearity and piecewise linear internal approximations, slower convergence and larger residual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) VS_v vs VS_t holdouts: excluding contiguous value-region (VS_v) harms generalization more than excluding token-neighborhoods (VS_t), consistent with a value-space regressor; 2) Layerwise correlation analysis: decoder token-distance correlation high in early/late layers while value-distance correlation peaks in middle decoder layers, indicating token->value encoding then regression then value->token decoding; 3) Ablations that remove encoder internals have little effect whereas removing positional encodings or attention prevents token->value mapping; 4) Amnesic probing (see separate entry) shows removing value information from decoder layer 3 embeddings collapses accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Multiplication exhibits less clear global distance correlations (likely due to quadratic dependence and piecewise approximations) and generalizes less cleanly across value-space gaps; some prior work shows benefits from scratchpads or CoT prompting suggesting symbolic decomposition can help sample efficiency even if ERD still possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8132.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AmnesicProbe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amnesic probing (nullspace projection intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Intervention technique used to remove value-related features from an intermediate decoder embedding by projecting onto the linear probe's nullspace, to test causal necessity of that information for output computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same small encoder-decoder Transformer used throughout experiments, trained from scratch on binary addition.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>7-bit binary addition (A+B)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Targets explicit 'value' information present in decoder layer 3 embeddings which is hypothesized to carry internal numeric magnitude needed for regression stage.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Train a linear regressor from dec3 embeddings to predict numeric sum (X+Y); compute regressor nullspace and project embeddings to remove dimensions used by the regressor (repeat twice as in Ravfogel et al.), then overwrite dec3 embeddings and forward pass to observe behavioral effect. Control: remove same-dimensional random directions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Linear regressor on dec3 embeddings: RMSE = 0.28 for predicting X+Y. Behavioral effect of projection: full sequence accuracy drops from 100% to 0.13% after nullspace projection; removing same amount of random-direction information preserves 100% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Projected removal of value-related subspace destroys computation; random-direction removal does not—demonstrates specificity. This reveals high sensitivity to particular linear subspaces carrying numeric information; intervention is destructive but specific.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>The sharp collapse in sequence accuracy after removing the regressor's nullspace-projected components constitutes causal evidence that the dec3 embeddings contain necessary value information for arithmetic output.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Amnesic probing relies on linear probes and nullspace assumptions; it may fail to remove nonlinearly encoded information. Authors repeated projection twice to mitigate probe simplicity but acknowledge limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8132.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VS_holdouts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VS_t and VS_v validation holdout experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary validation splits: VS_t selects 4096 validation pairs nearest a token-space centroid (tests token-space gaps); VS_v selects a contiguous value-space square (32<=A,B<96) of 4096 pairs (tests value-space extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params) and nanoGPT (298K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same small models trained on exhaustive dataset with 3/4 train, 1/4 validation; VS_t and VS_v are controlled choices for validation/test splitting.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>7-bit binary addition and multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Designed to distinguish whether LM generalization is governed by token-space proximity or by value-space coverage; probes whether the regressor in value space is responsible for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Data-splitting intervention: exclude contiguous regions either in token-space (VS_t) or in value-space (VS_v) from training and measure validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Encoder-decoder: VS_t: addition 100%, multiplication 97.5%; VS_v: addition 93.7%, multiplication 94.3% (drops of ~4–6% vs random split). nanoGPT: VS_t: addition 100%, multiplication 99.9%; VS_v: addition 82.0%, multiplication 80.6% (much larger drops).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Excluding contiguous value regions leads to larger generalization failures (extrapolation) than excluding token-neighborhoods, consistent with regressor inability to extrapolate outside trained value regions; nanoGPT showed bigger VS_v drops than encoder-decoder in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Stronger sensitivity to VS_v than VS_t supports ERD: encoding scatters token neighbors in value space so token-neighborhood holdout is less damaging, but leaving out a contiguous value region prevents the learned regressor from interpolating/extrapolating there.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>VS_v impact differs between architectures (nanoGPT larger drop), indicating architecture/training dynamics can alter extrapolation; multiplication nonlinearity complicates simple regressor interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8132.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemBaselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-output memorization control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Control experiment where outputs are random (no input-output relation) to test memorization difficulty relative to learning arithmetic algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same small Transformer trained on dataset where outputs are random 8-token strings instead of true sums.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Control (no arithmetic relationship)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Tests whether LM learns mapping by memorization rather than algorithmic or regressor-based mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Replace true outputs with random outputs during training and track training/validation curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Training required many more epochs (1000) to reach training sequence accuracy 87.8%; validation accuracy did not improve across epochs (no generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Model fails to generalize; memorization is much harder and slower than learning arithmetic mapping, indicating arithmetic solution is not simple rote memorization of input-output pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparative difficulty: arithmetic tasks learned quickly (<50 and ~250 epochs) whereas pure memorization needed far more epochs and did not generalize—supports existence of algorithmic or representational solution for arithmetic rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>This control cannot distinguish between symbolic algorithm learning and ERD regressor mechanisms by itself; it only establishes memorization is harder.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8132.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LayerCorr</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layerwise correlation analysis (token vs value distances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis correlating pairwise distances at input/output token/value levels with Euclidean distances of concatenated intermediate layer embeddings to reveal where value vs token information is encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Layerwise embeddings concatenated across token positions (encoder/decoder) used to compute Euclidean distances per pair in a 128-pair S set (A=A cases).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition and multiplication on identical-operand pairs (A,A) across A in [0,127]</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Findings: encoder layers maintain similar correlations (little transformation); decoder shows token-distance correlation high at first and last layers and value-distance correlation high in middle layers (peak at decoder layer 3 for addition), consistent with token->value encoding then regression then value->token decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compute Pearson correlations between ordered distance lists: input token/value distances, output token/value distances, and layer embedding distances across layers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not a performance metric per se; used to localize representational change: decoder layer 3 had minimum token-level correlation and maximum value-level correlation for addition.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>For multiplication, value-level correlation trends are less clear (low-high-low less evident) likely due to quadratic dependence causing piecewise linear internal approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Layerwise correlation pattern aligns with ERD: central decoder layers contain value-like geometry while early/final decoder layers align more with token geometry. Encoder layers' low change supports an architecture where encoding primarily happens at decoder entry in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Correlation is not causal; it only suggests representational similarity. Multiplication's weaker correlation pattern highlights limits of global-distance metrics for complex nonlinear mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8132.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study of architectural components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic removal or simplification of model components to determine which are necessary for arithmetic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params) and nanoGPT (298K params) variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants include squeezing encoder (removing encoder intermediate attention/MLP), reduced dimensionality (d_model=32), fewer heads, removal of fully connected layers, removal of positional encodings, removal of attention layers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>7-bit binary addition and multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Findings indicate positional embeddings and attention layers are necessary for token-to-value mapping; encoder intermediate computation is less essential (squeezing encoder had minor impact); reduction in dimensionality/heads/FC layers is tolerated to some extent.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Architectural ablations and measuring epochs to reach 95% validation accuracy or reporting failure within 1000 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: squeezing encoder gave similar performance; removal of positional embedding or attention prevented reaching targets (failed within 1000 epochs). Exact epoch counts per configuration reported in paper Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Removing positional encoding or attention breaks the model's ability to form positional/value encodings causing failure to learn arithmetic; aggressive capacity reductions slow or impair learning but are sometimes tolerated.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>If encoder internals removed with little effect while removing positional/attention destroys performance, it indicates attention+positional encoding are key to mapping token sequences into value representations (consistent with ERD).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Ablation results are architecture- and hyperparameter-dependent; there may be alternative minimal architectures that implement ERD differently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8132.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerformanceSummary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical performance and training dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Compact summary of learning speed, final accuracy, and error magnitudes for addition and multiplication tasks on the reported models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params) and nanoGPT (298K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Small Transformers trained on exhaustive 2^14 binary input/output pairs with 3/4 training split; greedy decoding at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>7-bit binary addition (8-bit output) and 7-bit binary multiplication (14-bit output)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Empirical observations consistent with value-space regression mechanism; multiplication requires more epochs and yields larger absolute errors.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Training curve monitoring, MAE, sequence accuracy, controlled splits (VS_t, VS_v), random-output control.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Encoder-decoder: addition converged in <50 epochs to ~100% sequence accuracy on validation (MAE 0); multiplication converged ~250 epochs to near-100% sequence accuracy (validation MAE = 1.3). nanoGPT learned addition and multiplication efficiently (see Appendix E; training curves similar or slightly faster), but showed larger VS_v sensitivity (VS_v accuracy drops to ~82% addition, ~80.6% multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Multiplication residual MAE and slower convergence; extrapolation failures in VS_v; random-output control shows inability to generalize when mapping is non-algorithmic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Faster learning and generalization on arithmetic vs memorization baseline; layerwise and intervention analyses support regressor-based mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some prior works achieve improved sample efficiency via scratchpads or CoT, implying symbolic stepwise strategies can be effective depending on setup; multiplication's complexity challenges a single smooth regressor view.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8132.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8132.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InputOrder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reverse (little-endian) input/output ordering effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that providing LSB-first (reverse) ordering for inputs/outputs speeds training and stabilizes learning versus plain (MSB-first) ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-decoder Transformer (701K params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same small Transformer setup; tests compared reverse vs plain token order for inputs and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Binary addition and multiplication</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>With LSB-first ordering, the autoregressive decoder can leverage the incremental computation context c_R_{i-1} (results produced so far) to predict next token using only local input tokens <= i, enabling incremental regressor behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compare training curves and epochs-to-convergence between reverse (default) and plain orders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reverse order leads to faster and more stable convergence; plain order requires many more epochs and less stable learning (plots in Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Plain/MSB-first ordering reduces ability to exploit incremental decomposition, making learning slower and more global (worse sample efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>This result supports the ERD/incremental-regressor framing: reverse ordering allows the regressor to use previously produced outputs as context, simplifying learning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Order effect is a training-engineering consideration; it does not disprove symbolic approaches since scratchpad methods can also exploit ordering but with different tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Arithmetic with Language Models: from Memorization to Computation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching Arithmetic to Small Transformers <em>(Rating: 2)</em></li>
                <li>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis <em>(Rating: 2)</em></li>
                <li>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals <em>(Rating: 2)</em></li>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>How well do Large Language Models perform in Arithmetic tasks <em>(Rating: 1)</em></li>
                <li>Investigating the Limitations of Transformers with Simple Arithmetic Tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8132",
    "paper_id": "paper-260378835",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "ERD",
            "name_full": "Encoding-Regression-Decoding hypothesis",
            "brief_description": "Hypothesis that the LM solves arithmetic by (1) encoding token strings to internal value vectors, (2) performing the numeric computation as a regression in value space, and (3) decoding the result vector back to output tokens.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params) and nanoGPT (decoder-only, 298K params)",
            "model_description": "Small Transformers trained from scratch on binary arithmetic with tiny vocabulary; primary model is an encoder-decoder (d_model=64, 6 encoder/6 decoder layers, 701K params); secondary model is nanoGPT decoder-only (d_model=64, 6 layers, 298K params).",
            "arithmetic_task_type": "7-bit binary addition (output 8 bits) and 7-bit binary multiplication (output 14 bits)",
            "mechanism_or_representation": "Internal numerical (value) representations: inputs are mapped from token embeddings to value vectors (encoding), computation occurs as regression in value space (central decoder layers), and outputs are generated by decoding value vectors to token logits (final decoder layers).",
            "probing_or_intervention_method": "Correlation of pairwise distances across input/output/token/value spaces and internal layer embeddings; ablation of model components; targeted holdout (VS_t and VS_v) to probe interpolation vs extrapolation.",
            "performance_metrics": "Model achieves near-perfect sequence accuracy on random train/val split: addition converged &lt;50 epochs to ~100% sequence accuracy (validation MAE 0), multiplication converged ~250 epochs to near-100% sequence accuracy (validation MAE 1.3).",
            "error_types_or_failure_modes": "Extrapolation failure when contiguous regions of value space are excluded from training (VS_v): drops of ~4–6 percentage points for encoder-decoder and larger drops for nanoGPT (VS_v: encoder-decoder addition 93.7%, multiplication 94.3%; nanoGPT VS_v: addition 82.0%, multiplication 80.6%). Multiplication shows more nonlinearity and piecewise linear internal approximations, slower convergence and larger residual errors.",
            "evidence_for_mechanism": "1) VS_v vs VS_t holdouts: excluding contiguous value-region (VS_v) harms generalization more than excluding token-neighborhoods (VS_t), consistent with a value-space regressor; 2) Layerwise correlation analysis: decoder token-distance correlation high in early/late layers while value-distance correlation peaks in middle decoder layers, indicating token-&gt;value encoding then regression then value-&gt;token decoding; 3) Ablations that remove encoder internals have little effect whereas removing positional encodings or attention prevents token-&gt;value mapping; 4) Amnesic probing (see separate entry) shows removing value information from decoder layer 3 embeddings collapses accuracy.",
            "counterexamples_or_challenges": "Multiplication exhibits less clear global distance correlations (likely due to quadratic dependence and piecewise approximations) and generalizes less cleanly across value-space gaps; some prior work shows benefits from scratchpads or CoT prompting suggesting symbolic decomposition can help sample efficiency even if ERD still possible.",
            "uuid": "e8132.0",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AmnesicProbe",
            "name_full": "Amnesic probing (nullspace projection intervention)",
            "brief_description": "Intervention technique used to remove value-related features from an intermediate decoder embedding by projecting onto the linear probe's nullspace, to test causal necessity of that information for output computation.",
            "citation_title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params)",
            "model_description": "Same small encoder-decoder Transformer used throughout experiments, trained from scratch on binary addition.",
            "arithmetic_task_type": "7-bit binary addition (A+B)",
            "mechanism_or_representation": "Targets explicit 'value' information present in decoder layer 3 embeddings which is hypothesized to carry internal numeric magnitude needed for regression stage.",
            "probing_or_intervention_method": "Train a linear regressor from dec3 embeddings to predict numeric sum (X+Y); compute regressor nullspace and project embeddings to remove dimensions used by the regressor (repeat twice as in Ravfogel et al.), then overwrite dec3 embeddings and forward pass to observe behavioral effect. Control: remove same-dimensional random directions.",
            "performance_metrics": "Linear regressor on dec3 embeddings: RMSE = 0.28 for predicting X+Y. Behavioral effect of projection: full sequence accuracy drops from 100% to 0.13% after nullspace projection; removing same amount of random-direction information preserves 100% accuracy.",
            "error_types_or_failure_modes": "Projected removal of value-related subspace destroys computation; random-direction removal does not—demonstrates specificity. This reveals high sensitivity to particular linear subspaces carrying numeric information; intervention is destructive but specific.",
            "evidence_for_mechanism": "The sharp collapse in sequence accuracy after removing the regressor's nullspace-projected components constitutes causal evidence that the dec3 embeddings contain necessary value information for arithmetic output.",
            "counterexamples_or_challenges": "Amnesic probing relies on linear probes and nullspace assumptions; it may fail to remove nonlinearly encoded information. Authors repeated projection twice to mitigate probe simplicity but acknowledge limits.",
            "uuid": "e8132.1",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "VS_holdouts",
            "name_full": "VS_t and VS_v validation holdout experiments",
            "brief_description": "Two complementary validation splits: VS_t selects 4096 validation pairs nearest a token-space centroid (tests token-space gaps); VS_v selects a contiguous value-space square (32&lt;=A,B&lt;96) of 4096 pairs (tests value-space extrapolation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params) and nanoGPT (298K params)",
            "model_description": "Same small models trained on exhaustive dataset with 3/4 train, 1/4 validation; VS_t and VS_v are controlled choices for validation/test splitting.",
            "arithmetic_task_type": "7-bit binary addition and multiplication",
            "mechanism_or_representation": "Designed to distinguish whether LM generalization is governed by token-space proximity or by value-space coverage; probes whether the regressor in value space is responsible for generalization.",
            "probing_or_intervention_method": "Data-splitting intervention: exclude contiguous regions either in token-space (VS_t) or in value-space (VS_v) from training and measure validation performance.",
            "performance_metrics": "Encoder-decoder: VS_t: addition 100%, multiplication 97.5%; VS_v: addition 93.7%, multiplication 94.3% (drops of ~4–6% vs random split). nanoGPT: VS_t: addition 100%, multiplication 99.9%; VS_v: addition 82.0%, multiplication 80.6% (much larger drops).",
            "error_types_or_failure_modes": "Excluding contiguous value regions leads to larger generalization failures (extrapolation) than excluding token-neighborhoods, consistent with regressor inability to extrapolate outside trained value regions; nanoGPT showed bigger VS_v drops than encoder-decoder in this setup.",
            "evidence_for_mechanism": "Stronger sensitivity to VS_v than VS_t supports ERD: encoding scatters token neighbors in value space so token-neighborhood holdout is less damaging, but leaving out a contiguous value region prevents the learned regressor from interpolating/extrapolating there.",
            "counterexamples_or_challenges": "VS_v impact differs between architectures (nanoGPT larger drop), indicating architecture/training dynamics can alter extrapolation; multiplication nonlinearity complicates simple regressor interpretation.",
            "uuid": "e8132.2",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MemBaselines",
            "name_full": "Random-output memorization control",
            "brief_description": "Control experiment where outputs are random (no input-output relation) to test memorization difficulty relative to learning arithmetic algorithms.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params)",
            "model_description": "Same small Transformer trained on dataset where outputs are random 8-token strings instead of true sums.",
            "arithmetic_task_type": "Control (no arithmetic relationship)",
            "mechanism_or_representation": "Tests whether LM learns mapping by memorization rather than algorithmic or regressor-based mapping.",
            "probing_or_intervention_method": "Replace true outputs with random outputs during training and track training/validation curves.",
            "performance_metrics": "Training required many more epochs (1000) to reach training sequence accuracy 87.8%; validation accuracy did not improve across epochs (no generalization).",
            "error_types_or_failure_modes": "Model fails to generalize; memorization is much harder and slower than learning arithmetic mapping, indicating arithmetic solution is not simple rote memorization of input-output pairs.",
            "evidence_for_mechanism": "Comparative difficulty: arithmetic tasks learned quickly (&lt;50 and ~250 epochs) whereas pure memorization needed far more epochs and did not generalize—supports existence of algorithmic or representational solution for arithmetic rather than memorization.",
            "counterexamples_or_challenges": "This control cannot distinguish between symbolic algorithm learning and ERD regressor mechanisms by itself; it only establishes memorization is harder.",
            "uuid": "e8132.3",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LayerCorr",
            "name_full": "Layerwise correlation analysis (token vs value distances)",
            "brief_description": "Analysis correlating pairwise distances at input/output token/value levels with Euclidean distances of concatenated intermediate layer embeddings to reveal where value vs token information is encoded.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params)",
            "model_description": "Layerwise embeddings concatenated across token positions (encoder/decoder) used to compute Euclidean distances per pair in a 128-pair S set (A=A cases).",
            "arithmetic_task_type": "Addition and multiplication on identical-operand pairs (A,A) across A in [0,127]",
            "mechanism_or_representation": "Findings: encoder layers maintain similar correlations (little transformation); decoder shows token-distance correlation high at first and last layers and value-distance correlation high in middle layers (peak at decoder layer 3 for addition), consistent with token-&gt;value encoding then regression then value-&gt;token decoding.",
            "probing_or_intervention_method": "Compute Pearson correlations between ordered distance lists: input token/value distances, output token/value distances, and layer embedding distances across layers.",
            "performance_metrics": "Not a performance metric per se; used to localize representational change: decoder layer 3 had minimum token-level correlation and maximum value-level correlation for addition.",
            "error_types_or_failure_modes": "For multiplication, value-level correlation trends are less clear (low-high-low less evident) likely due to quadratic dependence causing piecewise linear internal approximations.",
            "evidence_for_mechanism": "Layerwise correlation pattern aligns with ERD: central decoder layers contain value-like geometry while early/final decoder layers align more with token geometry. Encoder layers' low change supports an architecture where encoding primarily happens at decoder entry in this setup.",
            "counterexamples_or_challenges": "Correlation is not causal; it only suggests representational similarity. Multiplication's weaker correlation pattern highlights limits of global-distance metrics for complex nonlinear mappings.",
            "uuid": "e8132.4",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Ablation",
            "name_full": "Ablation study of architectural components",
            "brief_description": "Systematic removal or simplification of model components to determine which are necessary for arithmetic learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params) and nanoGPT (298K params) variants",
            "model_description": "Variants include squeezing encoder (removing encoder intermediate attention/MLP), reduced dimensionality (d_model=32), fewer heads, removal of fully connected layers, removal of positional encodings, removal of attention layers.",
            "arithmetic_task_type": "7-bit binary addition and multiplication",
            "mechanism_or_representation": "Findings indicate positional embeddings and attention layers are necessary for token-to-value mapping; encoder intermediate computation is less essential (squeezing encoder had minor impact); reduction in dimensionality/heads/FC layers is tolerated to some extent.",
            "probing_or_intervention_method": "Architectural ablations and measuring epochs to reach 95% validation accuracy or reporting failure within 1000 epochs.",
            "performance_metrics": "Qualitative: squeezing encoder gave similar performance; removal of positional embedding or attention prevented reaching targets (failed within 1000 epochs). Exact epoch counts per configuration reported in paper Table 3.",
            "error_types_or_failure_modes": "Removing positional encoding or attention breaks the model's ability to form positional/value encodings causing failure to learn arithmetic; aggressive capacity reductions slow or impair learning but are sometimes tolerated.",
            "evidence_for_mechanism": "If encoder internals removed with little effect while removing positional/attention destroys performance, it indicates attention+positional encoding are key to mapping token sequences into value representations (consistent with ERD).",
            "counterexamples_or_challenges": "Ablation results are architecture- and hyperparameter-dependent; there may be alternative minimal architectures that implement ERD differently.",
            "uuid": "e8132.5",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "PerformanceSummary",
            "name_full": "Empirical performance and training dynamics",
            "brief_description": "Compact summary of learning speed, final accuracy, and error magnitudes for addition and multiplication tasks on the reported models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params) and nanoGPT (298K params)",
            "model_description": "Small Transformers trained on exhaustive 2^14 binary input/output pairs with 3/4 training split; greedy decoding at inference.",
            "arithmetic_task_type": "7-bit binary addition (8-bit output) and 7-bit binary multiplication (14-bit output)",
            "mechanism_or_representation": "Empirical observations consistent with value-space regression mechanism; multiplication requires more epochs and yields larger absolute errors.",
            "probing_or_intervention_method": "Training curve monitoring, MAE, sequence accuracy, controlled splits (VS_t, VS_v), random-output control.",
            "performance_metrics": "Encoder-decoder: addition converged in &lt;50 epochs to ~100% sequence accuracy on validation (MAE 0); multiplication converged ~250 epochs to near-100% sequence accuracy (validation MAE = 1.3). nanoGPT learned addition and multiplication efficiently (see Appendix E; training curves similar or slightly faster), but showed larger VS_v sensitivity (VS_v accuracy drops to ~82% addition, ~80.6% multiplication).",
            "error_types_or_failure_modes": "Multiplication residual MAE and slower convergence; extrapolation failures in VS_v; random-output control shows inability to generalize when mapping is non-algorithmic.",
            "evidence_for_mechanism": "Faster learning and generalization on arithmetic vs memorization baseline; layerwise and intervention analyses support regressor-based mechanism.",
            "counterexamples_or_challenges": "Some prior works achieve improved sample efficiency via scratchpads or CoT, implying symbolic stepwise strategies can be effective depending on setup; multiplication's complexity challenges a single smooth regressor view.",
            "uuid": "e8132.6",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "InputOrder",
            "name_full": "Reverse (little-endian) input/output ordering effect",
            "brief_description": "Empirical finding that providing LSB-first (reverse) ordering for inputs/outputs speeds training and stabilizes learning versus plain (MSB-first) ordering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-decoder Transformer (701K params)",
            "model_description": "Same small Transformer setup; tests compared reverse vs plain token order for inputs and outputs.",
            "arithmetic_task_type": "Binary addition and multiplication",
            "mechanism_or_representation": "With LSB-first ordering, the autoregressive decoder can leverage the incremental computation context c_R_{i-1} (results produced so far) to predict next token using only local input tokens &lt;= i, enabling incremental regressor behavior.",
            "probing_or_intervention_method": "Compare training curves and epochs-to-convergence between reverse (default) and plain orders.",
            "performance_metrics": "Reverse order leads to faster and more stable convergence; plain order requires many more epochs and less stable learning (plots in Appendix C).",
            "error_types_or_failure_modes": "Plain/MSB-first ordering reduces ability to exploit incremental decomposition, making learning slower and more global (worse sample efficiency).",
            "evidence_for_mechanism": "This result supports the ERD/incremental-regressor framing: reverse ordering allows the regressor to use previously produced outputs as context, simplifying learning.",
            "counterexamples_or_challenges": "Order effect is a training-engineering consideration; it does not disprove symbolic approaches since scratchpad methods can also exploit ordering but with different tradeoffs.",
            "uuid": "e8132.7",
            "source_info": {
                "paper_title": "Arithmetic with Language Models: from Memorization to Computation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching Arithmetic to Small Transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
            "rating": 2,
            "sanitized_title": "amnesic_probing_behavioral_explanation_with_amnesic_counterfactuals"
        },
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "How well do Large Language Models perform in Arithmetic tasks",
            "rating": 1,
            "sanitized_title": "how_well_do_large_language_models_perform_in_arithmetic_tasks"
        },
        {
            "paper_title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks",
            "rating": 1,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        }
    ],
    "cost": 0.01446925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Arithmetic with Language Models: from Memorization to Computation ⋆
August 5, 2024</p>
<p>Davide Maltoni 
Department of computer science and engineering
University of Bologna
Italy</p>
<p>Matteo Ferrara 
Department of computer science and engineering
University of Bologna
Italy</p>
<p>Arithmetic with Language Models: from Memorization to Computation ⋆
August 5, 20246DEE070676EDA0FE16EEB276794449DBarXiv:2308.01154v4[cs.AI]⋆ The article has been accepted for publication in Elsevier Neural Networks journal. The final version is available on the Elsevier ScienceDirect platform. Preprint submitted to Neural NetworksLanguage ModelsAI ExplainabilityProbingInterpretabilityArithmetic
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability.This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data.Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data.We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) based on Transformer architecture (Vaswani et al., 2017) have recently demonstrated surprising problem-solving capabilities that require logic reasoning, advanced information processing and common sense (Bubeck et al., 2023;Wei et al., 2023Wei et al., , 2022)).Their huge storage capacity combined with a massive training on terabytes of heterogeneous data could suggest that the memorization of an enormous amount of knowledge is enough to perform well on similar test data.However, validations on carefully selected Outof-Distribution (OoD) data proved their reasoning capabilities on novel examples requiring nontrivial generalizations.Unfortunately, the depth and width of such models is so high that decoding and understanding the internal information processing is very challenging.</p>
<p>Focusing on arithmetic calculations, some studies (Yuan et al., 2023) demonstrate that recent LLMs (such as GPT-4) can perform additions and multiplications with long-digit operands, for which the number of variants is so high to exclude the exhaustive memorization of the training set.Nevertheless, the computational approach put in place by LLMs, as well as the interpolation/extrapolation capabilities remain unexplained.</p>
<p>In this work we design some controlled experiments, consisting of simple computation tasks such as binary addition and multiplication, and solve them with two Language Models (LMs) based on Transformer architecture: (i) the original encoder-decoder architecture by Vaswani et al. (2017) and (ii) a more recent decoder-only architecture denoted as nanoGPT (Karpathy, 2022).In spite of their simplicity, these tasks cannot be solved by pure memorization or smooth interpolation and investigating how an LM learn them can improve our understanding of the underlying mechanisms.In particular, using a tiny vocabulary of just 5 tokens and a small training set allows to operate with a light (non-pretrained) LM and use interpretability techniques to investigate internal information processing.</p>
<p>Other studies addressed the ability of LLMs to perform arithmetic computation and train small LMs to learn these tasks from scratch (see related works in Section 2).However, our aim is different: we are not interested in finding the best LM architecture and setup to maximize accuracy on arithmetic operations, but we look for a simple architecture and setup that allow to effectively solve the task in order to be able to investigate the underlying computational approach.The main novelty and contribution of this work are the formalization of the hypothesis that our LM works as an Encoding-Regression-Decoding machine and the design of a number of experiments to support and validate this hypothesis (see Table 1).</p>
<p>After presentation of related works in Section 2, in Section 3 we introduce the experimental testbed and the architecture of the LM used.Section 4 presents the results achieved and introduces control experiments and elaborations to shed light on the computation approach used to solve the tasks.In Section 5 an ablation study is presented and, finally, in Section 6 we include a final discussion and draw some conclusions.</p>
<p>Amnesic probing</p>
<p>Prove that the "value" information is crucial to properly compute the output Appendix D</p>
<p>Related works</p>
<p>LM and LLM capabilities on arithmetic tasks</p>
<p>In Yuan et al. (2023) recent LLMs have been benchmarked in arithmetic tasks, including long-digits sum and multiplication, showing that LLMs such as ChatGPT and GPT-4 can perform reasonably well on these tasks even with no specific tuning.On the other hand, the accuracy of smaller models is markedly lower, and in general they are not able to work with long operands and generalize to OoD data.</p>
<p>Goat (Liu and Low, 2023) a LLaMA model specifically fine-tuned on arithmetic tasks performed even better than GPT-4 on large-number additions and subtractions, probably due to the consistent (digit level) tokenization of numbers in LLaMA models.However, it was able to perform multi-digits multiplication and division only forcing a Chain of Thought (CoT) (Wei et al., 2023) decomposition of such tasks during instruction tuning.Nogueira et al. (2021) tuned a T5-based pre-trained LM on additions and subtractions, and argued that tokenization and input representation are critical to achieve good accuracy.In particular, in their experiments character-based tokenization works better than sub-word tokenization, and making explicit the digit position in the input string (i.e., inserting after each digit a marker to denote its position in the sequence) generally leads to better accuracy.They also trained a vanilla non-pretrained LM on smaller numbers and found that classical sinusoidal-based positional embedding does not perform well, so they proposed a tailored position-wise masked embedding.Their paper contains other interesting findings such as the impact of the digit order (plain or reverse) and the size of the training set.Muffo et al. (2023) tuned pre-trained GPT-2 models on 5-digit additions and 2-digit multiplications.They also found that making explicit the digit position in the input sequence helps to improve accuracy.While good accuracy is reported for addition, the tuned models struggle to learn multiplication even on two-digit operands.Lee et al. (2023) trained small LMs to learn arithmetic tasks, mainly focusing on addition, but also experimenting with subtraction, multiplication, sine and square root.The authors carefully ablated different aspects of the training data to isolate the factors that contribute to the appearance of arithmetic capabilities.In particular, they studied the impact of the input order (plain or reverse) and the utility of providing intermediate information about the decomposition of the task in steps to promote CoT reasoning.Some results and findings included in Lee et al. (2023) will be further discussed throughout this paper.</p>
<p>All the above works provide useful contributions to understand the capabilities and limitations of large and small LMs to deal with arithmetic tasks, but none of them focus on the computational approach used to solve them, which is the main purpose of the present work (see Table 1).</p>
<p>Interpretability techniques</p>
<p>A large number of techniques can be used to investigate the internal working mode of deep neural networks, including Transformers and LMs: see Rauker et al. (2023) for a recent survey.Weights, single neurons, subnetworks/circuits, and activations can be the target of intrinsic approaches (implemented during training) or post-hoc approaches (implemented after training).</p>
<p>Probing is a common technique used to investigate the representations learned by pre-trained LMs: it typically involves training a simple model (denoted as probe) on top of the LM embeddings to predict a given property (Belinkov, 2022).Moreover, structural probing can be used to check whether internal representations encode discrete structures such as syntax trees (Hewitt and Manning, 2019), (White et al., 2021).However, a certain criticism emerged on probing analyses which is believed to disconnect the probing task from the original one and/or to reveal correlations instead of causations.Therefore, instead of focusing on the presence of information on internal encoding, some researchers proposed to check whether the removal of some knowledge from embeddings (e.g., amnesic probing (Elazar et al., 2021)) negatively influences the model ability to perform a task (Elazar et al., 2021), (Lasri et al., 2022).Other interesting approaches to interpretability are mechanistic interpretability (Elhage et al., 2021) and causal abstraction (Geiger et al., 2021): the former is aimed at reverse engineering the algorithm that a model uses to solve a task and to map it to neural circuits; the latter constructs an interpretable causal model and aligns it with neural representations.</p>
<p>In this work we use a mix of intrinsic and post-hoc interpretability techniques: in particular through the experiments we manipulate the training set, change the input representation and the architecture components, perform correlation analyses of embeddings and apply amnesic probing.</p>
<p>Interpretability of arithmetic reasoning with LMs</p>
<p>Stolfo et al. ( 2023) introduced a causal mediation analysis to point out the LM components (e.g., attention heads, Multi-Layer Perceptrons -MLPs) involved in the information processing of simple arithmetic operations, focusing on the flow of numerical information throughout the model layers/columns.The main outcomes of this study are that the model: (i) processes the representation of numbers and operators with the first layers; (ii) information is then conveyed (by attention heads) to the last part of the sequence (i.e., output column), where (iii) it is numerically processed by late MLPs.Nanda et al. (2023) carefully studied the algorithmic approach put in place by a small Transformer to implement modular addition of small numbers.They discovered that the internal algorithmic implementation is based on discrete Fourier transforms and trigonometric identities to convert addition to rotation on a circle.While the outcomes are somewhat surprising, here the term algorithm must be taken with care: even if the experiments prove that internal processing well approximates given equations, the approach is a numerical approximation (based on weight encoded values) that does not generalize to different moduli (as a symbolic implementation of the equations could do).</p>
<p>Both these studies adopted a simplified setting where numbers are presented as single token, and the output is expected at the last position of the sequence.So the models are not operated in autoregressive manner and the multi-token encoding/decoding stages are simplified.In Section 6 we discuss how the above findings are compatible with our findings.</p>
<p>Experiment design</p>
<p>The tasks</p>
<p>We focused on two simple computation tasks: binary addition and binary multiplication.Using binary encoding allows keeping the vocabulary very compact, since we need to encode only the symbols '0', '1' and a few other tokens.The selected tasks have other nice properties such as computing input similarities by Hamming distance and easily generating all combinations.Of course, a classical artificial neural network can be trained to learn to sum and multiply two integers or floating-point numbers, but adding/multiplying strings of tokens with an LM is trickier.</p>
<p>More formally, given two integers A, B (both in the range [0,127]) our input sequence (or prompt) is a 15-token string taking the form:
a 0 a 1 a 2 a 3 a 4 a 5 a 6 ⟨op⟩ b 0 b 1 b 2 b 3 b 4 b 5 b 6
where a i , b i ∈ {'0', '1'} are the symbols corresponding to bits in the i-th position in the binary representation of A and B respectively, and ⟨op⟩ can be either '+' or '×'.</p>
<p>The expected output string (or input completion) is:
R = r 0 r 1 ...r m−1
where r i is the i-th bit in the binary representation of A⟨op⟩B, and m is the number of bits of the expected output string R (8 and 14 for addition and multiplication, respectively).</p>
<p>It is worth noting that:</p>
<p>• we are using a fixed-length input/output representation (with zero padding for unused most significant bits) to make the digit positions more explicit.</p>
<p>• in both the input and output the Least Significant Bits (LSBs) are provided before the Most Significant Bits (MSBs) (a.k.a., reverse or little-endian order) since this was supposed to simplify the model learning1 .As discussed in Appendix C this assumption leads to a much faster training.</p>
<p>If we consider the sequence-to-sequence mapping underlying the proposed tasks we note that even in a simple binary addition a slight change in the input (i.e., a single bit) can produce a relevant change in the output because of the carries propagation.In the example below a single bit modification in the input produces an 8 bit modification in the output: 1000000 + 0111111 → 11111110 1000000 + 1111111 → 00000001 Such input-output discontinuity is made more explicit for addition in Appendix A.</p>
<p>The architecture</p>
<p>A non-pretrained encoder-decoder Transformer based on the original architecture introduced in Vaswani et al. (2017) was used as primary LM.Table 2 reports the model setup and parametrization.The small vocabulary used allows us to keep the model small (just 701K learnable parameters) and trainable from scratch with a limited number of examples.</p>
<p>The LM was trained to learn separately the addition/multiplication tasks.For both problems, we exhaustively generated all the 2 14 = 16384 input/output combinations, which were then randomly split into training (3/4 → 12288) and validation (1/4 → 4096) sets.In our experiments we do not need a separate dataset to tune hyperparameters so our validation set coincides with the test set.</p>
<p>An additional control experiment was run where the input sequences were the same of the addition experiment but the output completion was randomly generated (with the same length as the addition, i.e., 8 tokens).In this case, the lack of any dependencies between input and output makes it impossible to learn an algorithmic approach (or smooth mapping) to solve the problem and the only strategy to learn the training set is memorizing all the sequences.</p>
<p>When the trained LM is used in inference mode, we always pick the most probable token from the logit outputs (i.e., greedy decoding).Two metrics can be used to denote the LM accuracy: token accuracy refers to the probability of generating the next token correctly, while sequence accuracy refers to the probability of generating the whole output string correctly in autoregressive mode (i.e., generating one token at a time and appending it to the current prompt).Most of the experiments have been repeated with a second LM (nanoGPT by Karpathy ( 2022)) which is a good representative of the decoder-only family.Details are reported in Appendix E.</p>
<p>All the experiments included in this paper can be easily reproduced by running the code available at: (to be disclosed upon acceptance).</p>
<p>Results</p>
<p>Learning addition and multiplication</p>
<p>Figure 1 shows that our simple LM is able to learn addition in less than 50 epochs, and multiplication in about 250 epochs2 .As expected multiplication is more complex and requires more training: this is due to the high non-linearity of this operation (more on this later) and to the higher length of the output (14 vs 8 tokens).The accuracy on the validation set is very close to the training set, denoting almost perfect generalization on numbers never seen before.This is a somewhat surprising result, especially considering the limited size of the training data.No grokking3 was observed (Nanda et al., 2023).Similar results were obtained with nanoGPT (see  2021) (see their Appendix B for a similar setup), we were able to learn addition with the native sinusoidal positional encoding.Moreover, in Lee et al. (2023) additions can be effectively learnt by a simple LM, but to reach 100% accuracy the training set had to be balanced in terms of the operand magnitude (i.e., number of digits) and carry propagation.</p>
<p>The effectiveness of our training procedure is probably due to the lower complexity determined by a small vocabulary and fixed-length representation.As to multiplication, Muffo et al. (2023) were not able to effectively learn two (decimal) digits multiplication, while Lee et al. (2023) and Liu and Low (2023) had to provide extra intermediate steps in the prompt (denoted as detailed scratchpad) or during instruction tuning, respectively.On the contrary our model effectively learnt multiplication of 7 binary digit operands: again the simplified setup may have been the key.</p>
<p>On the workstation used (with a single Titan RTX GPU) training can be completed in just 8 and 46 minutes for addition and multiplication, respectively.An estimation of the training complexity C of an LLM in term of floating point operations is 6 × N × T (Kaplan et al., 2020), where N is the number of model parameters (about 701K as reported in Table 2) and T the number of training tokens.T can be obtained as the product of the training set size (12288 in our experiments -see Section 3.2), the sequence length in tokens (23 and 29 for addition and multiplication, respectively -see Section 3.1) and the number of epochs (50 and 250 for addition and multiplication, respectively).Hence, for addition T is 14M (12288 × 23 × 50) and therefore C is about 59 × 10 12 operations while for multiplication T is 89M (12288 × 29 × 250) and C is about 374 × 10 12 operations.</p>
<p>Control experiment: random output</p>
<p>If the output is randomly generated and therefore there is no relation with the input, the only possibility of learning the training set is by memorizing the whole data.Figure 2 shows the training results: a much larger number of epochs (i.e., 1000) were necessary to reach a sequence accuracy of 87.8%, and, as expected, the validation accuracy did not increase over the epochs.The difficulty of memorizing the training set (many more epochs) is due to the high discontinuity of the input-output mapping.In fact, because of the random output generation, very similar input sequences can be associated to completely different outputs.</p>
<p>Therefore, even if we only consider the accuracy on the training set, this result shows that an exhaustive memorization of the input is much more complex for the LM than solving the addition and multiplication tasks.This leads us to assume that, to efficiently solve the above computation tasks, the LM has found a computational approach (or algorithm) to simplify the output prediction.Now the question is: what is the approach?</p>
<p>The computational approach</p>
<p>Let us consider two alternative approaches: Symbolic Manipulation (SM): a first idea is that the LM could learn the binary integer addition/multiplication algorithms used by an ALU inside a CPU (see Appendix B for a short reminder).Indeed, the addition algorithm is not complex and can be solved by using a 3-bit truth table (to sum each pair of corresponding bits with the carry-in) and iterative carry-out propagations.However, multiplication (by iterative additions) is much more complex and trickier to learn by using a symbolic manipulation approach.Furthermore, as shown in Lee et al. (2023), a simple LM can also learn complex operations such as the sine function or the square root, whose mathematical (and algorithmic) decomposition is very complex since they require Taylor expansion and Newton method, respectively.</p>
<p>Encoding-Regression-Decoding (ERD): if we consider the model architecture (Transformer) used for the LM and the underlying word embedding by vector representations, it is more likely that the LM solves the problem by decomposing it in the following three phases:</p>
<ol>
<li>Encoding (token to value): maps the input sequence (i.e., a 0 a 1 a 2 a 3 a 4 a 5 a 6 ⟨op⟩ b 0 b 1 b 2 b 3 b 4 b 5 b 6 ) to a suitable vector representation.In principle, two vectors v A and v B representing the values (or magnitudes) of A and B are enough.2. Regression: learns the computation as a supervised regression problem in the vector space:</li>
</ol>
<p>v R = regress(v A , v B ). Actually this regression formulation is an oversimplification of the problem since in the next-token-prediction training the LM works incrementally.In Appendix C this discussion will be expanded.3. Decoding (value to token): maps the value vector v R back to token representation (i.e., r 0 r 1 ...r m ).</p>
<p>It is worth noting that the above Encoding and Decoding phases do not need to be mapped onto the Transformer encoder and decoder (more on this later).The experiments reported in Sections 4.4 and 4.5 support the ERD assumption.The capability of capturing number magnitudes by pretrained embedders was also investigated by Wallace et al. (2019) who successfully trained a simple external regressor to compute the sum of two numbers starting from their embeddings.Other interesting studies on capturing numeracy with embedding were carried out by Naik et al. (2019) and Sundararaman et al. (2020).</p>
<p>Interpolation vs extrapolation</p>
<p>The random training/validation split performed for the experiments reported in Section 4.1 constitutes a somewhat simplified testbed to learn the two tasks.In fact, random split leads to a complete (even if sparse) coverage of the input space by both the training and validation sets, where each example in the validation set has high chance to be close to a training set example, and interpolation is enough to fill the gaps.</p>
<p>Hereafter, we exploit the well-known difficulty of a numerical regressor to work in the extrapolation regime to get insights about the computational approach of the LM.In particular, we considered two different criteria to isolate specific portion of the input space for the validation set, in order to better investigate extrapolation capabilities:
• VS t = {(A, B)|(A, B) ∈ NN 4096 ((A * , B * ))}
where NN 4096 ((A * , B * )) is the set of 4096 pairs (A, B) which are the nearest neighbors to a centroid (A * , B * ) according to the Hamming distance between the corresponding token representations (i.e., number of different tokens at corresponding positions).As centroid (A * , B * ) in the token space we used: 1010101 ⟨op⟩ 0101010.
• VS v = {(A, B)|32 ≤ A &lt; 96 and 32 ≤ B &lt; 96}
here the centroid is located in the middle of the value space (64, 64), so VS v is a squared region (of side 64) centered in the value space.</p>
<p>Both VS t and VS v isolate a contiguous data region of 4096 samples to be included in the validation set, but in the former the samples are close in the token representation space, while in latter are close in the value space.Being such contiguous portions of space excluded from the training set, we can expect a worse generalization.From the results (see Figure 3) we note that VS t is very marginally affecting LM training and generalization while VS v has a major impact: in fact, in the second case, for both addition and multiplication the final sequence accuracy is from 4% to 6% points lower.This result strengthens the ERD hypothesis, since: (i) using VS v leads to the exclusion of a specific contiguous portion of value space during phase 2 and does not allow to properly train the regressor in this region; (ii) the encoding performed during phase 1 makes irrelevant the selection performed according to VS t because, after encoding, the corresponding data point remains scattered in the value space and the regressor can easily interpolate among them.Similar results were obtained with nanoGPT (see Figure E.8 in Appendix E.)</p>
<p>Looking at internal representations</p>
<p>Understanding the internal representation (embeddings in the vector space) in a trained Transformer is not an easy task.However, in the specific setting considered we can gain some hints by looking at the distances between the embedding of different data points (at different layers) and correlating them with the corresponding distances at input/output levels.</p>
<p>Given an LM trained on addition (or multiplication) we consider the dataset S including the 128 input pairs where the two operands have identical values4 : Results are averaged over five runs.VS t reaches 100% accuracy on additions (the same of Random split) and 97.5% accuracy on multiplication (just 1.4% less than random split); VS v reaches 93.7% on addition and 94.3% on multiplication (6.3% and 4.6% less than Random split, respectively).
S = {(A, A)|0 ≤ A &lt; 128}
At the input level (in) we can compute two ordered sets of 8128 (128×127/2) distances each:
d in,t = {hdist(X, Y)|(X, X), (Y, Y) ∈ S , X &lt; Y} d in,v = {|X − Y| |(X, X), (Y, Y) ∈ S , X &lt; Y}
where hdist(X, Y) is the Hamming distance between the token representation of X and Y, and the subscript letters t and v denote token and value levels, respectively.</p>
<p>At the output level (out) we can compute the two corresponding sets of distances as:
d out,t = {hdist(P, Q)|(X, X), (Y, Y) ∈ S , X &lt; Y} d out,v = {|P − Q| |(X, X), (Y, Y) ∈ S , X &lt; Y}
where (P = X + X and Q = Y + Y) for addition, and (P = X × X and Q = Y × Y) for multiplication.</p>
<p>Finally, for each intermediate level of the Transformer encoder (enc) or decoder (dec) we can compute the Euclidean distances among the corresponding embedding vectors.
d enc i = {∥enc i (X, X) − enc i (Y, Y)∥ |(X, X), (Y, Y) ∈ S , X &lt; Y} d dec i = {∥dec i (X, X) − dec i (Y, Y)∥ |(X, X), (Y, Y) ∈ S , X &lt; Y}
where enc i and dec i are the output vectors obtained by concatenating all the token embeddings (each of dimensionality 64) after the i-th encoder and decoder layer, respectively.For example enc i has dimensionality 960 = 64 × 15 where 15 is the number of tokens in the encoder.</p>
<p>Even if the distances in the different sets have different ranges, we can use correlation to find out similarities.If two sets of distances are correlated we can expect that the corresponding representations/embeddings are correlated as well.Since both Pearson and Spearman correlations (Schober et al., 2018) provided similar outputs, for simplicity in Figure 4 we report only Pearson correlations.</p>
<p>The yellow cells in the tables of Figure 4 confirm the low correlation between the token and value representation at both input and output level.The blue cells show that correlation remains quite similar across the encoder layers as if the encoder was not performing any significant computation (this is confirmed in Section 5 where we achieve similar results by totally removing all intermediate attention and MLP layers in the encoder).More interesting is the trend of correlations across the decoder layers (green cells).In particular, for the addition the token representation has high correlation with the first and last layers and low with central layers, while the value representation has an opposite trend (see also Figure 4.c).These results support the ERD hypothesis and in particular that the initial and final layers in the decoder transform from token to value representation (and vice versa) while the central layers perform regression in the value space.In particular, at layer 3, the correlation at token level is minimum while the correlation at value level is maximum.</p>
<p>For multiplication the low-high-low trend at value level is less evident (Figure 4.d orange curve), probably because the quadratic dependence of the output from the input (at value level) does not allow to learn a simple regressor smoothly working in the whole vector space, and the mapping is performed by piecewise linear approximation in different space regions, which introduces discontinuities that make global distances in the vector space unsuitable to quantify the representation similarity.</p>
<p>As discussed in Section 2.2, correlation analyses might be insufficient to prove that the presence of a certain information in the embeddings is really necessary to compute the output (direct causation).So to further strengthen our hypothesis we applied an amnesic probing technique (Elazar et al., 2021) and proved that, upon removal of value information from the embeddings, the LM is no longer capable of performing the right computation.Details are reported in Appendix D.</p>
<p>Ablation study</p>
<p>This section presents the results of an ablation study where the LM architecture was simplified, to understand which components are necessary to learn the addition/multiplication computation.Squeezing the encoder (i.e., removing all intermediate attention and MLP layers) does not have a relevant impact; this is consistent with other works claiming that a decoder only architecture (Liu et al., 2018) can achieve similar results with respect to an encoder-decoder Transformer, and further confirmed by the nanoGPT results presented in Appendix E. A simplification of the architecture in terms of (i) reduction of dimensionality; (ii) reduction of number of heads; (iii) removal of fully connected layers is well tolerated, while positional embedding and attention layers are mandatory for the LM in order to properly perform token to value transformation (and vice versa).Table 3 summarizes the results.</p>
<p>Table 3: Epochs necessary to reach 95% accuracy on the validation set.A dash is used when 95% accuracy is not achieved in 1K epochs: in such case the accuracy reached is reported within brackets.</p>
<p>Configuration</p>
<p>Addition Multiplication Full (see</p>
<p>Discussion and conclusions</p>
<p>In this paper we introduced a simplified setup to allow a light LM to learn binary addition and multiplication.Both the LM architectures considered easily learn the two tasks and generalizes well on unseen data, proving that memorization of the training data is neither necessary nor efficient.The experiments on the interpolation/extrapolation capabilities and correlation of input-output representations with internal embedding suggest that the model solves the computational task as a supervised regression problem in the value space after an initial encoding from token to values, and a final decoding from output value to tokens.Under this hypothesis: (i) any task that can be solved by a neural network regressor can be solved by an LM as well, with the extra burden of end-to-end learning decoding/encoding steps; (ii) when looking at interpolation/extrapolation capabilities of an LM applied to a mathematical task, we should not concentrate on the input token representation but on the internal representation after encoding, keeping in mind the difficulties of a numerical regressor to work on region spaces not covered by the training set; (iii) on a more speculative side, we could guess that modern LLMs learn the number encoding/decoding once and reuse it across different numerical tasks whereas a specific regressor is learned for each task.</p>
<p>Our ERD hypothesis could be questioned considering some recent findings from Lee et al. (2023) where providing in the prompt intermediate information (scratchpad) about the decomposition of arithmetic tasks improves the training efficiency and requires fewer examples.This could suggest that a symbolic manipulation approach is adopted to learn imitating step by step the proposed decomposition.However, in most of the cases their model was able to learn the same task (even if slowly) without scratchpad and/or with wrong scratchpads.As argued by the authors the higher efficiency is actually in terms of examples and not in terms of tokens since each scratchpad requires a large number of extra tokens, and we guess these could be used as extra features by the underlying regressor.Furthermore, scratchpad contribution is negligible for more complex operations such as sine and square root, but, unexpectedly, learning such complex operations was simpler than multiplication.This is not strange under the ERD hypothesis where a unary smooth operator like the sine can be learned by a supervised regressor independently of the mathematical method used for its computation.</p>
<p>The algorithmic interpretation that Nanda et al. (2023) provided for modular addition, could also suggest that their LM discovered and efficient symbolic manipulation approach; however, as discussed in Section 2.3, it is more likely that a regressor was learned to numerically approximate an efficient sparse Fourier decomposition, under regularization constraints favoring sparsity.Finally, the information flow described in Stolfo et al. (2023), points out that MLPs in the last layers are responsible for the numerical computation of the solution, which is compatible with the hypothesis of a multi-layer regressor.</p>
<p>Of course we are not claiming that all the capabilities of modern LLMs can be explained by regression, but regression is likely to be one of the internal tools that LLMs uses to predict the next token when numbers come into play.</p>
<p>As to future research we plan to: i) further investigate the generalization capabilities of LMs in arithmetic tasks with respect to the composition of the training and test sets (Feng et al., 2023;Keskar et al., 2017), ii) design simplified experiments/setups for tasks that cannot be easily mapped to regression problems such as chain of reasoning and logic deductions.current bits5 , then a two-output 3-bit truth table (Table B.5) can be used to generate the output bit o i and carry c i used when summing the next pair of bits:</p>
<p>Inputs</p>
<p>Outputs
a i b i c i−1 o i c i 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1
A simple approach to execute binary multiplication is through iterative binary sums.Each bit b i of the second operand is multiplied by the whole first operand, but this inner multiplication is straightforward since it results either in a sequence of 0 (if b i = 0) or a copy of the first operand (if b i = 1).This intermediate result is then shifted left and summed to the current output.An example is reported in Figure B.5 below.</p>
<p>Appendix D. Amnesic probing results</p>
<p>The outcome of correlation analyses performed in Section 4.3 suggests that the embeddings in the central layers of the decoder contain information related to the value representation of the output (see Figure 4).However correlation does not mean causation, and here we investigate deeper.Amnesic probing was proposed in Elazar et al. (2021) building on the approach Ravfogel et al. (2020) to check to what extent a model output is affected by the removal of specific features or attributes in an intermediate level embeddings.Here we focus on addition and we try to remove some features from the decoder layer 3 embeddings (dec 3 (X, Y)).To this purpose a linear probe (a linear regressor in our case) was trained to predict the output value (X + Y) starting from the dec 3 (X, Y) embeddings and its nullspace is used to project the embeddings in a new space lacking output value information.According to Ravfogel et al. (2020), due to the simplicity of the linear regressor used, the procedure is repeated twice to remove more information.Our results show that:</p>
<p>1.A simple linear regressor trained on dec 3 (X, Y) embeddings can reach high accuracy in predicting X + Y (rmse = 0.28).2. If the projected embeddings are overwritten in the LM decoder at level 3, and a partial forward pass is performed thereafter, the addition sequence accuracy severely drops from 100% to 0.13%.3.As indicated in Elazar et al. (2021) since any information removal could hamper the model accuracy, a control test was performed by removing the same amount of information (but on random directions instead of the nullspace directions) and in this case the LM final sequence accuracy remained 100%.</p>
<p>This experiment provides further support to the hypothesis that the value information is not only present in the inspected embeddings but is also crucial for the output computation.</p>
<p>On the computational side, we argue that amnesic probing complexity is low because it relies on simple steps as linear regression and null space computation, with the former being the most demanding step.Linear regression complexity is O(nd 2 + d 3 ) where n is the number of training examples and d the dimensionality of the embeddings.</p>
<p>Appendix E. NanoGPT -a decoder-only LM</p>
<p>To demonstrate that our findings generalize beyond the encoder-decoder architecture of the original Transformer used in this work, the main experiments have been repeated using a second LM, that is the nanoGPT (Karpathy, 2022) decoder-only model.Table E.6 reports the details of the nanoGPT model adopted.Using the VS t subset, it reaches 100% and 99.9% accuracy on addition and multiplication, respectively (the same of Random split) while, using the VS v subset, it reaches 82.0% on addition and 80.6% on multiplication (18.0% and 19.4% less than Random split, respectively).Results are inline with those obtained in Section 4.4 but here the difference between VS t , and VS v is still more significant.</p>
<p>Figure E.7 in Appendix E.) Unlike Nogueira et al. (</p>
<p>Figure 1 :
1
Figure 1: Sequence accuracy.From the left: addition and multiplication.Results are averaged over five runs.Note that, training and validation curves are almost overlapped.At the end of training the Mean Absolute Error (MAE) on the validation set, between the real and generated operation results, is 0 and 1.3 for addition and multiplication, respectively.</p>
<p>Figure 2 :
2
Figure 2: Sequence accuracy using random output in the training set.Results are averaged over five runs.</p>
<p>Figure 3 :
3
Figure 3: Sequence accuracy on Random, VS t , and VS v validation subsets for addition (left) and multiplication (right).Results are averaged over five runs.VS t reaches 100% accuracy on additions (the same of Random split) and 97.5% accuracy on multiplication (just 1.4% less than random split); VS v reaches 93.7% on addition and 94.3% on multiplication (6.3% and 4.6% less than Random split, respectively).</p>
<p>Figure 4 :
4
Figure 4: Pearson correlation between ordered sets of distances for addition (a) and multiplication (b).Each cell denotes the correlation between the two ordered set of distances specified in the corresponding row and column.Note that since for addition in this experiment the output value is always twice the input, the correlation values (blue and green cells) are the same for d in, and d out, block of values.Graphs (c) and (d) show the correlations of output distances d out,t (at token level -blue curves) and d out,v (at value level -orange curves) with the embedding distances d dec i across the 6 decoder layers for addition and multiplication, respectively.</p>
<p>Figure B. 5 :
5
Figure B.5: Example of 4-digit binary multiplication.The sum can be performed incrementally with a two-operand adder.</p>
<p>Figure C. 6 :
6
Figure C.6: Sequence accuracy on validation set for reverse (default in this work) and plain order of the input and output representations.From left to right: addition and multiplication.</p>
<p>Figure E. 7
7
Figure E.7 shows that the nanoGPT model was able to learn addition and multiplication still more efficiently than the original Transformer (compare Figure 1 with Figure E.7).For the training, we used a minibatch size of 128, a standard CrossEntropy loss, the AdamW optimizer with a learning rate of 0.001 and betas = 0.9 and 0.98, and a gradient clipping to 1.0.</p>
<p>Figure E. 7 :
7
Figure E.7: Sequence accuracy of the nanoGPT model (refer to Section 4.1 for more details).From the left: addition and multiplication.Results are averaged over five runs.Note that, training and validation curves are almost overlapped.</p>
<p>Figure</p>
<p>Figure E.8 shows the sequence accuracy of the nanoGPT model on Random, VS t , and VS v validation subsets for addition and multiplication (see Section 4.4 for more details).Using the VS t subset, it reaches 100% and 99.9% accuracy on addition and multiplication, respectively (the same of Random split) while, using the VS v subset, it reaches 82.0% on addition and 80.6% on multiplication (18.0% and 19.4% less than Random split, respectively).Results are inline with those obtained in Section 4.4 but here the difference between VS t , and VS v is still more significant.</p>
<p>Figure E. 8 :
8
Figure E.8: Sequence accuracy of the nanoGPT model on Random, VS t , and VS v validation subsets for addition (left) and multiplication (right).Results are averaged over five runs.</p>
<p>Table 1 :
1
The main contributions of this work.</p>
<p>Table 2 :
2
Details of the LM model used in our experiments.The total number of learnable parameters is just 701K, which is several orders of magnitudes smaller than recent billion-parameters LLMs.
vocabulary size5vocabulary0: unused, 1: <start>, 2: '+' or '×', 3: '0', 4: '1'token embeddinglearnedpositional encodingfixed (sinusoidal)d model64d f fd model × 4num heads h8encoder layers6decoder layers6dropout0.1learnable parameters701K</p>
<p>Table 2
2)39137Squeezing the encoder (see main text)60426num heads h=125225Reduced dimensionality (d model = 32)66309No positional embedding-(2.4%)-(1.8%)No attention layers-(0.9%)-(1.7%)No fully connected layers56398</p>
<p>Table A .
A
4: Addition input-output discontinuities.</p>
<p>Table B .
B
5: Two-output 3-bit truth table for binary addition.</p>
<p>Table E .
E
6: Details of the nanoGPT model.
token embeddinglearnedpositional encodinglearnedd model64d f fd model × 4num heads h8decoder layers6dropout0.1learnable parameters298K
in binary arithmetic the addition/multiplication algorithms start processing the LSBs in order to correctly propagate the intermediate carries.
We used the standard CrossEntropy loss, the Adam optimizer with the learning rate of 0.0001 and betas = 0.9 and 0.98, and a minibatch size of 128.
Grooking refers to the case where validation accuracy, much smaller than training accuracy at initial stages, suddenly increases after a certain number of epochs.
since the input prompt contains two operands, we select only the cases with identical values (A = B) in order to easily determine the "magnitude" of the input, and thereafter compute meaningful distances.
when summing the LSBs (i = 0), there is no pending carry, so c −1 = 0
Appendix A. Addition input-output discontinuitiesGiven an input/output pair we consider the (2 14 ) variants obtained by perturbing (i.e., 0-1 swap) the input bits and counting the resulting changes in the output.These values, averaged over all possible input/output pairs (again 2 14 ) and normalized by row are inserted in the cells of A.4.So, for example the value in cell (row=2, column=3) means that in the 27.9% of the cases a perturbation of 2 (over 14) bits in the input leads to a change of 3 (over 8) bits in the output.Input-output discontinuities, which are further amplified in case of multiplications, make it very unlikely to solve these tasks by smooth interpolation of the input representation.Appendix B. Binary addition and multiplicationBinary addition can be executed by summing pairs of corresponding bits a i and b i , starting from the LSBs (a 0 and b 0 ) and propagating carries.Let c i−1 be the pending carry used to sum In Section 4.3 we argued that an arithmetic computation task can be decomposed into three steps whose central one is learning a regressor in the value space: v R = regress(v A , v B ).If we consider the autoregressive working mode of a Transformer and its predict-next-token training, the regressor must be able to work incrementally given the output produced so far.In particular, we can formulate the problem as:) where:are the value vectors of the two input operands, obtained as the concatenation of the value vectors of single tokens.Both are always fully available to the decoder.Note that, v a i and v b i are not the bits of the inputs, but correspond to their value vectors including also positional information.• i is the position of the token to be predicted (we can assume it is available through positional encoding).• c R i−1 = [c r 0 c r 1 ...c r i−1 ] is a value vector encoding the current context determined by the result produced so far (entering in the decoder from the bottom).• v r i is the value vector of the i-th token.In principle, the regressor could predict each v r i based on v A and v B alone, but we argue that the exploitation of the result produced so far c R i−1 can lead to higher training efficiency.To this purpose is interesting to evaluate the impact of the output ordering (plain or reverse).In both the addition and multiplication the i-th token of the result only depends on the tokens of the inputs at positions ≤ i (see Appendix B).Therefore, if reverse order is adopted, as we assumed until now,and c R i−1 are sufficient to predict v r i .Viceversa, if the output computation starts with the MSBs the regressor cannot leverage the above iterative decomposition and needs to learn the task as a global operation using whole vectors v A and v B with almost no support from the result produced so far.In FigureC.6 we note that with plain order both addition and multiplication require a much longer number of epochs to converge and the learning curve is less stable.Further experiments proved that, as expected, the order of the inputs (also reverse by default in this study) is irrelevant, since the LM can always access the whole input representations v A and v B .The advantages of using the reverse order are pointed out in other recent studies(Nogueira et al., 2021;Lee et al., 2023).In particular,Lee et al. (2023)reported a significant improvement with respect to plain order.
Y Belinkov, Probing Classifiers: Promises, Shortcomings, and Advances, Computational Linguistics. 202248</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y Lee, Y Li, S Lundberg, H Nori, H Palangi, M Ribeiro, Y Zhang, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Y Elazar, S Ravfogel, A Jacovi, Y Goldberg, Transactions of The Association for Computational Linguisticss. 92021</p>
<p>. N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, N Dassarma, D Drain, D Ganguli, Z Hatfield-Dodds, D Hernandez, A Jones, J Kernion, L Lovitt, K Ndousse, D Amodei, T Brown, J Clark, J Kaplan, S Mccandlish, C Olah, A Mathematical Framework for Transformer Circuits. 2021</p>
<p>Activity-weight duality in feed-forward neural networks reveals two co-determinants for generalization. Y Feng, W Zhang, Y Tu, Nature Machine Intelligence. 52023</p>
<p>A Geiger, H Lu, T F Icard, C Potts, Causal Abstractions of Neural Networks, 35th Conference on Neural Information Processing Systems. NeurIPS 20212021</p>
<p>A Structural Probe for Finding Syntax in Word Representations. J Hewitt, C Manning, Conference of the North American Chapter. the Association for Computational Linguistics2019</p>
<p>J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, arXiv:2001.08361Scaling Laws for Neural Language Models. 2020</p>
<p>A Karpathy, N S Keskar, D Mudigere, J Nocedal, M Smelyanskiy, P T P Tang, On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima, 5th International Conference on Learning Representations. 2022. 2017nanoGPT: a lightweight implementation of medium-sized GPTs</p>
<p>K Lasri, T Pimentel, A Lenci, T Poibeau, R Cotterell, Probing for the Usage of Grammatical Number, 60th Annual Meeting of The Association for Computational Linguistics. 2022</p>
<p>N Lee, K Sreenivasan, J Lee, K Lee, D Papailiopoulos, arXiv:2307.03381Teaching Arithmetic to Small Transformers. 2023</p>
<p>T Liu, B K H Low, arXiv:2305.14201Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks. 2023</p>
<p>P Liu, M Saleh, E Pot, B Goodrich, R Sepassi, L Kaiser, N Shazeer, Generating Wikipedia by Summarizing Long Sequences, 6th International Conference on Learning Representations (ICLR). 2018</p>
<p>Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition. M Muffo, A Cocco, E Bertino, 13th Conference on Language Resources and Evaluation (LREC). 2023</p>
<p>A Naik, A Ravichander, C Rose, E Hovy, Exploring Numeracy in Word Embeddings, 57th Annual Meeting of The Association for Computational Linguistics. 2019</p>
<p>N Nanda, L Chan, T Lieberum, J Smith, J Steinhardt, arXiv:2301.05217Progress measures for grokking via mechanistic interpretability. 2023</p>
<p>Investigating the Limitations of Transformers with Simple Arithmetic Tasks, 1st Mathematical Reasoning in. R Nogueira, Z Jiang, J Lin, General Artificial Intelligence Workshop @ (ICLR). 2021</p>
<p>T Räuker, A Ho, S Casper, D Hadfield-Menell, arXiv:2207.13243Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. 2023</p>
<p>S Ravfogel, Y Elazar, H Gonen, M Twiton, Y Goldberg, Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection, 58th Annual Meeting of the Association for Computational Linguistics. 2020</p>
<p>Correlation Coefficients: Appropriate Use and Interpretation. P Schober, C Boer, L Schwarte, 2018Anesthesia &amp; Analgesia126</p>
<p>A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis. A Stolfo, Y Belinkov, M Sachan, arXiv:2305.150542023</p>
<p>D Sundararaman, S Si, V Subramanian, G Wang, D Hazarika, L Carin, Methods for Numeracy-Preserving Word Embeddings, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020</p>
<p>Attention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, L Kaiser, I Polosukhin, Advances in Neural Information Processing Systems (NIPS). 201730</p>
<p>E Wallace, Y Wang, S Li, S Singh, M Gardner, arXiv:1909.07940Do NLP Models Know Numbers? Probing Numeracy in Embeddings. 2019</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Emergent Abilities of Large Language Models. TMLR2022</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2022</p>
<p>J White, T Pimentel, N Saphra, R Cotterell, Non, Linear Structural Probe, Conference of the North American Chapter. the Association for Computational Linguistics2021</p>
<p>How well do Large Language Models perform in Arithmetic tasks. Z Yuan, H Yuan, C Tan, W Wang, S Huang, arXiv:2304.020152023</p>            </div>
        </div>

    </div>
</body>
</html>
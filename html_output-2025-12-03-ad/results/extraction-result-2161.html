<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2161 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2161</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2161</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278789432</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16100v1.pdf" target="_blank">BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research</a></p>
                <p><strong>Paper Abstract:</strong> Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation. In this work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows. Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables. While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods. The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science. We propose BioDSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2161.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2161.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen agent using GPT-4o for single-shot code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-shot code-generation agent that takes a hypothesis and dataset schema and emits a single executable Python code block plus a final decision (True/False/Not Verifiable). Used as a baseline in the BIODSA-1K experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model-based code-generation agent</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates analysis code (Python) to produce empirical observations and a final hypothesis decision (True / False / Non-verifiable)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Executes generated code on structured biomedical tables and derives empirical observations; decision correctness evaluated by comparing agent decision to ground-truth label (True/False/Non-verifiable). Evidence alignment evaluated by LLM-as-a-judge matching agent observations O to ground-truth evidences E with Alignment Score = |O ∩ E| / |E|. Error rates measured as Type I (false positive) and Type II (false negative). Code executability measured as fraction of generated code cells that run without error.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly quantified as distance-from-training-data; novelty operationalized via task/domain difficulty and curated 'Non-verifiable' cases (datasets lacking relevant variables) which serve as out-of-distribution / novel validation challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Produces executable code at an executability rate of 76.9% (reported aggregate). Evidence alignment scores typically low (~0.20–0.25). In some domains CodeGen (gpt-4o) showed Type I and Type II error examples such as Biomarkers: Type I = 0.090, Type II = 0.164. Tends to be conservative (higher Type II than Type I).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation (decision) performance measured by Type I/II errors: across publication types CodeGen (gpt-4o) shows Type II > Type I (example Biomarkers: E_I=0.090, E_II=0.164). Non-verifiable detection true positive rate (TPR) = 63% on a curated strictly non-verifiable set of 100 hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Typical Type I values per Table 2 range ~0.051–0.157 depending on domain; e.g., Biomarkers 0.090. False positive rate increases/decreases by domain but generally lower than false negative rate.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II values are consistently higher than Type I; examples: Biomarkers 0.164, Integrative/Genomics values up to ~0.168–0.191 for some CodeGen variants. Overall CodeGen (gpt-4o) Type II often in the ~0.12–0.19 range depending on domain.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance degrades on novel / more complex domains (Genomics, Integrative). One-shot CodeGen methods perform worse on curated non-verifiable (OOD) cases (TPR 63%) and more likely to make incorrect verifiable claims (37% of strictly non-verifiable cases).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — CodeGen frequently generates plausible-looking claims but its validation (alignment with ground-truth evidence) is weak (alignment ~0.20–0.25) and it hallucinates in OOD settings (~37% incorrect assertions on strictly non-verifiable cases).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>On strictly non-verifiable (OOD) tasks, CodeGen (gpt-4o) TPR for 'Not Verifiable' = 63% (i.e., 37% over-assertion). Out-of-distribution (complex integrative) Type II rates are higher than in familiar/simpler tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Partially miscalibrated: conservative overall (higher false negatives) but shows overconfidence/hallucination on a non-trivial fraction of OOD/non-executable cases (13% of non-executable cases yielded False/True decisions). Calibration degrades on OOD/non-verifiable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not numerically reported; single-shot generation implies lower runtime overhead compared to iterative agents, but cost of validating outputs includes executing generated code on datasets (same for all agents).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>None intrinsic beyond baseline; paper shows reasoning augmentation and iterative ReAct variants reduce errors compared to single-shot CodeGen.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Single-shot CodeGen (GPT-4o) attains moderate executability (76.9%) but low evidence alignment (~0.20–0.25) and is conservative (higher Type II). It performs poorly on strictly non-verifiable/OOD cases (37% over-assertion), demonstrating a generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2161.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2161.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen (o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen agent using O3-mini for single-shot code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lower-cost single-shot code-generation variant using O3-mini that outputs executable analysis code and a final decision; used as an experimental baseline in BIODSA-1K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen (o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model-based code-generation agent (smaller model)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates analysis code (Python) and a final hypothesis decision based on resulting observations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same evaluation pipeline as other agents: execute generated code on structured biomedical data, compare agent decision to ground-truth (True/False/Non-verifiable), compute Type I/II errors, evidence alignment by LLM-as-judge, and code executability rate.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty operationalized by domain complexity and curated non-verifiable cases; no explicit embedding- or distance-based novelty metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Lower code quality/executability than larger model variants in some settings; Table 2 shows higher Type II errors in several domains (e.g., Integrative E_II = 0.191 for CodeGen o3-mini). Executability/ alignment scores not individually broken out for this variant beyond error rates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Generally worse than larger-model CodeGen and ReAct variants in hard domains: example Integrative Type II = 0.191 (higher missed detections). Non-verifiable detection TPR = 57% (worse than GPT-4o CodeGen).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I values for CodeGen (o3-mini) reported in Table 2 span ~0.103–0.157 depending on domain (e.g., Biomarkers 0.107).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Higher Type II in multiple domains (example Integrative 0.191), indicating increased false negatives relative to larger models and reasoning agents.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Stronger degradation on complex/novel domains and non-verifiable tasks compared to larger-model and reasoning-augmented agents (notably higher Type II and lower non-verifiable TPR).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — produces plausible outputs but validation metrics (alignment, correct decisions) are substantially worse in novel/complex domains, indicating a larger generation-validation gap compared to larger models and reasoning agents.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Worse than GPT-4o and reasoning-augmented agents on OOD / strictly non-verifiable tasks (TPR 57%); higher Type II on integrative/genomics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Less well calibrated than larger-model CodeGen and reasoning agents, as evidenced by higher false negatives and lower non-verifiable detection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported; single-shot so lower iterative overhead but lower performance necessitates potential human inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Paper shows reasoning augmentation (CodeGen-Reasoning) can partially mitigate deficits, but CodeGen-Reasoning in experiments had lower executability in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller-code-generation LLM (o3-mini) attains worse validation performance on complex/integrative tasks (e.g., Type II up to 0.191) and poorer non-verifiable detection (TPR 57%), amplifying the generation-validation gap compared to larger models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2161.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2161.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct agent using GPT-4o with interleaved reasoning and actions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent implementing the ReAct framework (alternating explicit reasoning 'thoughts' and code-execution 'actions'), enabling iterative refinement of analysis and conclusions; used in BIODSA-1K experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct (gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent with iterative reasoning-execution loop</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates iterative reasoning steps and incremental code snippets that are executed; uses observations to refine subsequent steps and final decision (True/False/Non-verifiable).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Iterative execute-and-refine loop: generate code cell, execute, inspect outputs, reason and decide whether to continue; decision compared to ground-truth labels; evidence alignment measured by LLM-as-judge; code executability rate counted across entire interaction trace.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty assessed indirectly via domain difficulty and curated non-verifiable tasks; ReAct demonstrates relative robustness to novel/complex tasks compared to single-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Higher code executability (ReAct: 84.9%) and slightly higher evidence alignment scores than CodeGen baselines; generally lower Type II error than CodeGen in many domains (e.g., Integrative: ReAct E_II = 0.148 vs CodeGen 0.153).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improved decision accuracy vs single-shot CodeGen: lower Type II in several tough domains, and better handling of complexity; non-verifiable detection TPR not as high as ReAct-Reasoning but better than CodeGen baselines (exact TPR per ReAct not individually listed; ReAct-Reasoning TPR reported as 92%).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Type I typically similar or slightly lower than CodeGen in many domains (e.g., Integrative E_I = 0.066 vs CodeGen 0.095); values vary by domain as in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Type II reduced relative to single-shot baselines in many domains (examples: Pan-Cancer ReAct E_II = 0.128 vs CodeGen 0.167).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Less sensitive to novelty than one-shot CodeGen; iterative reasoning helps especially in complex/information-dense domains (Genomics, Integrative), yielding larger improvements on those harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Reduced but still present: ReAct improves executability and alignment but evidence alignment remains modest (~0.20–0.25), indicating remaining mismatch between generated analyses and human-reported evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Better than single-shot CodeGen on OOD/non-verifiable tasks but lower than the reasoning-augmented ReAct-R* variant; out-of-distribution gains arise from iterative observation and plan refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Improved calibration relative to CodeGen (fewer hallucinations on strictly non-verifiable cases), but still conservative (Type II > Type I) and alignment scores indicate imperfect faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Higher than single-shot agents due to iterative execution and multiple reasoning/code cycles (not numerically reported).</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Iterative reasoning+action loop (ReAct) reduces both Type I and Type II errors and improves code executability and alignment compared to single-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct (GPT-4o) achieves higher executability (84.9%), lower Type II than CodeGen in many domains, and better evidence alignment, showing that iterative reasoning-action cycles partially close the generation-validation gap, especially on complex tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2161.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2161.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeGen-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeGen with explicit structured analysis planning (O3-mini planning + GPT-4o codegen)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage pipeline that first generates a structured analysis plan (using O3-mini) and then uses GPT-4o to translate that plan into executable code, decoupling planning from implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeGen-Reasoning (O3-mini -> GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid LLM pipeline (planning + code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates a structured analysis plan (pseudo-code steps), then generates executable code implementing the plan; outputs observations and a final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Plan-first then execute approach: structured analysis plans are required to use only schema column names, followed by code generation and execution; correctness measured via Type I/II errors, evidence alignment (LLM-as-judge), and code executability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No explicit numerical novelty metric; reasoning pipeline is intended to improve generalization to complex/novel analyses by enforcing explicit planning constraints and schema usage.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Mixed: in some domains CodeGen-Reasoning reduces Type I/II relative to the single-shot CodeGen (e.g., Biomarkers CodeGen-R* E_I=0.082, E_II=0.156 vs CodeGen gpt-4o E_I=0.090, E_II=0.164), but aggregate code executability was lower for CodeGen-Reasoning (58.2% reported for 'CodeGen Reasoning' in figure breakdown) than simpler ReAct variants.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improvements in some domains for decision errors when structured planning is used, but lower code executability results in practical limitations; non-verifiable detection TPR improves over single-shot CodeGen (paper reports reasoning-augmented agents are more conservative overall).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Reduced compared to base CodeGen in some categories (e.g., Biomarkers E_I decreased from 0.090 to 0.082), values vary by domain as in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Slight reductions in some domains; still often > Type I. Example Biomarkers E_II=0.156 for CodeGen-R* vs 0.164 for base CodeGen.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Structured planning helps more on complex tasks by reducing some errors, but limited code executability constrains realized gains; reasoning helps particularly where baseline errors are high.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Partially narrowed by separating planning from codegen, but the reduction in executability (higher variable/object misuse rates) introduced new failure modes, leaving a mixed picture.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Better than one-shot CodeGen on some metrics (reduced Type I/II), but out-of-distribution robustness is limited by lower executability and code errors introduced during plan-to-code translation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Improved conservatism (fewer false assertions) relative to single-shot CodeGen, but calibration remains imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not explicitly measured; two-stage pipeline likely increases end-to-end compute and latency compared to single-shot generation but exact costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Decoupled planning (O3-mini) and implementation (GPT-4o) to enforce structured analysis plans; this helps reduce some decision errors but introduces execution fragility.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured planning (CodeGen-Reasoning) reduces some Type I/II errors versus single-shot CodeGen in certain domains, but lower code executability (58.2% in one breakdown) and common variable/object misuse errors limit overall improvements, producing a mixed impact on the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2161.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2161.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct-Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct agent augmented with structured planning and O3-mini backend (ReAct-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced ReAct agent that incorporates explicit structured analysis planning (via O3-mini) and iterative reasoning/execution; the top-performing agent variant in many BIODSA-1K metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReAct-Reasoning (ReAct-R*)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid iterative LLM agent with structured planning (neurosymbolic/agentic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical data science / hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates structured analysis plans, iteratively executes code blocks, inspects outputs, refines plan and code, and produces a final decision and observed evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Iterative plan-and-execute with explicit planning: the agent must iteratively execute/refine until sufficient evidence is obtained; evaluation uses Type I/II error metrics, evidence alignment by LLM-as-judge, code executability across the interaction trace, and non-verifiable detection accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No explicit numeric novelty distance metric; performance on 'hard' domains and curated strictly non-verifiable cases is used to infer robustness to novelty/out-of-distribution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Highest executability reported (ReAct-Reasoning 86.6%), improved Type II and Type I errors across many publication types (e.g., in several categories ReAct-R* reports the best column values in Table 2), and superior non-verifiable detection (TPR = 92%). Evidence alignment still modest (~0.20–0.25) but slightly higher than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Strongest among tested agents: substantially lower Type II in difficult domains (example: Integrative E_II reduced to 0.107 from CodeGen o3-mini 0.191), and best non-verifiable detection performance (92% TPR on strictly non-verifiable set).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Competitive or lower than other agents in many domains (example Genomics/Integrative reductions noted in text and Table 2), values vary by domain; e.g., ReAct-R* shows E_I as low as 0.051 in one Pan-Cancer row.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Markedly lower than single-shot baselines in several domains (e.g., Integrative reduction from 0.191 to 0.107 when comparing CodeGen o3-mini to ReAct-R*), demonstrating higher sensitivity in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Least sensitive to novelty among evaluated agents; reasoning and iterative execution produce the largest improvements in high-baseline-error (novel/complex) domains. ReAct-R* achieves high TPR for 'Not Verifiable' (92%), indicating robust OOD/non-verifiable detection.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Reduced significantly but not eliminated: ReAct-Reasoning improves executability and decision accuracy, but evidence alignment remains modest (many analyses still diverge from human-authored evidence), indicating residual asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Best OOD performance among variants: on strictly non-verifiable tasks achieved TPR = 92% versus 63% (CodeGen gpt-4o) and 57% (CodeGen o3-mini). Also reduces Type II substantially on integrative/genomics tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Better calibrated for OOD detection (high TPR for 'Not Verifiable') and reduced hallucination; still conservative (Type II generally > Type I) but improved sensitivity reduces missed findings.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Not reported quantitatively; iterative planning-execution-refinement implies higher compute and latency than single-shot methods, but yields superior validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combines structured planning (enforced schema usage), ReAct iterative reasoning-action cycles, and a two-model division of labor (O3-mini for planning) to substantially close the generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct-Reasoning attains the best overall validation behavior: highest code executability (86.6%), largest reductions in Type II errors on hard domains (e.g., Integrative), and strong OOD/non-verifiable detection (TPR 92%), showing that iterative reasoning + structured planning substantially narrows the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2161.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2161.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (LLM-as-extractor / LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used both to extract hypotheses/evidence and as an LLM-based judge for evidence alignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is used in the BIODSA-1K pipeline to extract structured hypotheses and supporting evidences from abstracts and to function as an LLM-as-a-judge that scores alignment between agent observations and ground-truth evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (used for extraction and judging)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical literature processing and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates structured hypothesis/evidence JSON from publication abstracts; also used to compare agent-produced observations O to ground-truth evidence E to compute an Alignment Score.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-as-a-judge metric: Alignment Score = |O ∩ E| / |E| where a large language model (GPT-4o) judges whether agent observations match ground-truth evidences. Also used to extract hypotheses and structured evidence entries from abstracts via deterministic (zero-temperature) prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not explicitly provided; novelty not numerically quantified by GPT-4o in this pipeline—novelty operationalized by downstream task design (non-verifiable cases, domain difficulty).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Effective at deterministic extraction when prompted with structured templates; used in generating benchmark entries (1,029 hypotheses and 1,177 analysis tasks) by extracting hypothesis/evidence structures from abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provides alignment-scoring function used across experiments; paper reports alignment scores (agent vs ground-truth) computed via LLM-as-judge but does not report independent accuracy/calibration metrics for GPT-4o judge itself.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported for the judge component; alignment score is a proportion of ground-truth evidence items captured (typical agent alignment ~0.20–0.25).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported for LLM-as-judge; not evaluated for judge calibration in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not explicitly measured for the judge; paper does not report whether the LLM-as-judge calibration degrades on novel/unseen evidence formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Serves as an evaluation tool rather than a generative hypothesis-testing agent; its use highlights mismatch between agent outputs and human-reported evidence (i.e., it reveals generation-validation gaps) but its own reliability is not deeply characterized.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported for the judge component.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; the paper acknowledges use of LLM-as-judge but does not provide judge calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Used offline to compute alignment scores; compute cost not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Using an LLM-as-judge provides an automated, scalable evidence-alignment metric, but paper emphasizes need for human oversight due to modest alignment scores and potential judge limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o was used to reliably extract hypotheses/evidence for the benchmark and to compute an automated evidence-alignment score; however, alignment scores of agents (0.20–0.25) reveal that agents often fail to reproduce ground-truth evidence even when judged by an LLM, underscoring the generation-validation gap and the need for human oversight.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated hypothesis validation with agentic sequential falsifications <em>(Rating: 2)</em></li>
                <li>Towards an ai co-scientist <em>(Rating: 2)</em></li>
                <li>BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments <em>(Rating: 2)</em></li>
                <li>DiscoveryBench: Towards data-driven discovery with large language models <em>(Rating: 2)</em></li>
                <li>Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2161",
    "paper_id": "paper-278789432",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "CodeGen (gpt-4o)",
            "name_full": "CodeGen agent using GPT-4o for single-shot code generation",
            "brief_description": "A one-shot code-generation agent that takes a hypothesis and dataset schema and emits a single executable Python code block plus a final decision (True/False/Not Verifiable). Used as a baseline in the BIODSA-1K experiments.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "CodeGen (gpt-4o)",
            "system_type": "large language model-based code-generation agent",
            "domain": "biomedical data science / hypothesis validation",
            "generation_capability": "generates analysis code (Python) to produce empirical observations and a final hypothesis decision (True / False / Non-verifiable)",
            "validation_method": "Executes generated code on structured biomedical tables and derives empirical observations; decision correctness evaluated by comparing agent decision to ground-truth label (True/False/Non-verifiable). Evidence alignment evaluated by LLM-as-a-judge matching agent observations O to ground-truth evidences E with Alignment Score = |O ∩ E| / |E|. Error rates measured as Type I (false positive) and Type II (false negative). Code executability measured as fraction of generated code cells that run without error.",
            "novelty_measure": "Not explicitly quantified as distance-from-training-data; novelty operationalized via task/domain difficulty and curated 'Non-verifiable' cases (datasets lacking relevant variables) which serve as out-of-distribution / novel validation challenges.",
            "generation_performance": "Produces executable code at an executability rate of 76.9% (reported aggregate). Evidence alignment scores typically low (~0.20–0.25). In some domains CodeGen (gpt-4o) showed Type I and Type II error examples such as Biomarkers: Type I = 0.090, Type II = 0.164. Tends to be conservative (higher Type II than Type I).",
            "validation_performance": "Validation (decision) performance measured by Type I/II errors: across publication types CodeGen (gpt-4o) shows Type II &gt; Type I (example Biomarkers: E_I=0.090, E_II=0.164). Non-verifiable detection true positive rate (TPR) = 63% on a curated strictly non-verifiable set of 100 hypotheses.",
            "false_positive_rate": "Typical Type I values per Table 2 range ~0.051–0.157 depending on domain; e.g., Biomarkers 0.090. False positive rate increases/decreases by domain but generally lower than false negative rate.",
            "false_negative_rate": "Type II values are consistently higher than Type I; examples: Biomarkers 0.164, Integrative/Genomics values up to ~0.168–0.191 for some CodeGen variants. Overall CodeGen (gpt-4o) Type II often in the ~0.12–0.19 range depending on domain.",
            "novelty_effect_on_validation": "Performance degrades on novel / more complex domains (Genomics, Integrative). One-shot CodeGen methods perform worse on curated non-verifiable (OOD) cases (TPR 63%) and more likely to make incorrect verifiable claims (37% of strictly non-verifiable cases).",
            "generation_validation_asymmetry": "Yes — CodeGen frequently generates plausible-looking claims but its validation (alignment with ground-truth evidence) is weak (alignment ~0.20–0.25) and it hallucinates in OOD settings (~37% incorrect assertions on strictly non-verifiable cases).",
            "out_of_distribution_performance": "On strictly non-verifiable (OOD) tasks, CodeGen (gpt-4o) TPR for 'Not Verifiable' = 63% (i.e., 37% over-assertion). Out-of-distribution (complex integrative) Type II rates are higher than in familiar/simpler tasks.",
            "calibration_quality": "Partially miscalibrated: conservative overall (higher false negatives) but shows overconfidence/hallucination on a non-trivial fraction of OOD/non-executable cases (13% of non-executable cases yielded False/True decisions). Calibration degrades on OOD/non-verifiable tasks.",
            "validation_computational_cost": "Not numerically reported; single-shot generation implies lower runtime overhead compared to iterative agents, but cost of validating outputs includes executing generated code on datasets (same for all agents).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "None intrinsic beyond baseline; paper shows reasoning augmentation and iterative ReAct variants reduce errors compared to single-shot CodeGen.",
            "evidence_type": "supports",
            "key_findings": "Single-shot CodeGen (GPT-4o) attains moderate executability (76.9%) but low evidence alignment (~0.20–0.25) and is conservative (higher Type II). It performs poorly on strictly non-verifiable/OOD cases (37% over-assertion), demonstrating a generation-validation gap.",
            "uuid": "e2161.0"
        },
        {
            "name_short": "CodeGen (o3-mini)",
            "name_full": "CodeGen agent using O3-mini for single-shot code generation",
            "brief_description": "A lower-cost single-shot code-generation variant using O3-mini that outputs executable analysis code and a final decision; used as an experimental baseline in BIODSA-1K.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "CodeGen (o3-mini)",
            "system_type": "large language model-based code-generation agent (smaller model)",
            "domain": "biomedical data science / hypothesis validation",
            "generation_capability": "generates analysis code (Python) and a final hypothesis decision based on resulting observations",
            "validation_method": "Same evaluation pipeline as other agents: execute generated code on structured biomedical data, compare agent decision to ground-truth (True/False/Non-verifiable), compute Type I/II errors, evidence alignment by LLM-as-judge, and code executability rate.",
            "novelty_measure": "Novelty operationalized by domain complexity and curated non-verifiable cases; no explicit embedding- or distance-based novelty metric reported.",
            "generation_performance": "Lower code quality/executability than larger model variants in some settings; Table 2 shows higher Type II errors in several domains (e.g., Integrative E_II = 0.191 for CodeGen o3-mini). Executability/ alignment scores not individually broken out for this variant beyond error rates.",
            "validation_performance": "Generally worse than larger-model CodeGen and ReAct variants in hard domains: example Integrative Type II = 0.191 (higher missed detections). Non-verifiable detection TPR = 57% (worse than GPT-4o CodeGen).",
            "false_positive_rate": "Type I values for CodeGen (o3-mini) reported in Table 2 span ~0.103–0.157 depending on domain (e.g., Biomarkers 0.107).",
            "false_negative_rate": "Higher Type II in multiple domains (example Integrative 0.191), indicating increased false negatives relative to larger models and reasoning agents.",
            "novelty_effect_on_validation": "Stronger degradation on complex/novel domains and non-verifiable tasks compared to larger-model and reasoning-augmented agents (notably higher Type II and lower non-verifiable TPR).",
            "generation_validation_asymmetry": "Yes — produces plausible outputs but validation metrics (alignment, correct decisions) are substantially worse in novel/complex domains, indicating a larger generation-validation gap compared to larger models and reasoning agents.",
            "out_of_distribution_performance": "Worse than GPT-4o and reasoning-augmented agents on OOD / strictly non-verifiable tasks (TPR 57%); higher Type II on integrative/genomics tasks.",
            "calibration_quality": "Less well calibrated than larger-model CodeGen and reasoning agents, as evidenced by higher false negatives and lower non-verifiable detection.",
            "validation_computational_cost": "Not reported; single-shot so lower iterative overhead but lower performance necessitates potential human inspection.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Paper shows reasoning augmentation (CodeGen-Reasoning) can partially mitigate deficits, but CodeGen-Reasoning in experiments had lower executability in aggregate.",
            "evidence_type": "supports",
            "key_findings": "Smaller-code-generation LLM (o3-mini) attains worse validation performance on complex/integrative tasks (e.g., Type II up to 0.191) and poorer non-verifiable detection (TPR 57%), amplifying the generation-validation gap compared to larger models.",
            "uuid": "e2161.1"
        },
        {
            "name_short": "ReAct (gpt-4o)",
            "name_full": "ReAct agent using GPT-4o with interleaved reasoning and actions",
            "brief_description": "An agent implementing the ReAct framework (alternating explicit reasoning 'thoughts' and code-execution 'actions'), enabling iterative refinement of analysis and conclusions; used in BIODSA-1K experiments.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "ReAct (gpt-4o)",
            "system_type": "LLM-based agent with iterative reasoning-execution loop",
            "domain": "biomedical data science / hypothesis validation",
            "generation_capability": "Generates iterative reasoning steps and incremental code snippets that are executed; uses observations to refine subsequent steps and final decision (True/False/Non-verifiable).",
            "validation_method": "Iterative execute-and-refine loop: generate code cell, execute, inspect outputs, reason and decide whether to continue; decision compared to ground-truth labels; evidence alignment measured by LLM-as-judge; code executability rate counted across entire interaction trace.",
            "novelty_measure": "Novelty assessed indirectly via domain difficulty and curated non-verifiable tasks; ReAct demonstrates relative robustness to novel/complex tasks compared to single-shot baselines.",
            "generation_performance": "Higher code executability (ReAct: 84.9%) and slightly higher evidence alignment scores than CodeGen baselines; generally lower Type II error than CodeGen in many domains (e.g., Integrative: ReAct E_II = 0.148 vs CodeGen 0.153).",
            "validation_performance": "Improved decision accuracy vs single-shot CodeGen: lower Type II in several tough domains, and better handling of complexity; non-verifiable detection TPR not as high as ReAct-Reasoning but better than CodeGen baselines (exact TPR per ReAct not individually listed; ReAct-Reasoning TPR reported as 92%).",
            "false_positive_rate": "Type I typically similar or slightly lower than CodeGen in many domains (e.g., Integrative E_I = 0.066 vs CodeGen 0.095); values vary by domain as in Table 2.",
            "false_negative_rate": "Type II reduced relative to single-shot baselines in many domains (examples: Pan-Cancer ReAct E_II = 0.128 vs CodeGen 0.167).",
            "novelty_effect_on_validation": "Less sensitive to novelty than one-shot CodeGen; iterative reasoning helps especially in complex/information-dense domains (Genomics, Integrative), yielding larger improvements on those harder tasks.",
            "generation_validation_asymmetry": "Reduced but still present: ReAct improves executability and alignment but evidence alignment remains modest (~0.20–0.25), indicating remaining mismatch between generated analyses and human-reported evidence.",
            "out_of_distribution_performance": "Better than single-shot CodeGen on OOD/non-verifiable tasks but lower than the reasoning-augmented ReAct-R* variant; out-of-distribution gains arise from iterative observation and plan refinement.",
            "calibration_quality": "Improved calibration relative to CodeGen (fewer hallucinations on strictly non-verifiable cases), but still conservative (Type II &gt; Type I) and alignment scores indicate imperfect faithfulness.",
            "validation_computational_cost": "Higher than single-shot agents due to iterative execution and multiple reasoning/code cycles (not numerically reported).",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Iterative reasoning+action loop (ReAct) reduces both Type I and Type II errors and improves code executability and alignment compared to single-shot generation.",
            "evidence_type": "supports",
            "key_findings": "ReAct (GPT-4o) achieves higher executability (84.9%), lower Type II than CodeGen in many domains, and better evidence alignment, showing that iterative reasoning-action cycles partially close the generation-validation gap, especially on complex tasks.",
            "uuid": "e2161.2"
        },
        {
            "name_short": "CodeGen-Reasoning",
            "name_full": "CodeGen with explicit structured analysis planning (O3-mini planning + GPT-4o codegen)",
            "brief_description": "A two-stage pipeline that first generates a structured analysis plan (using O3-mini) and then uses GPT-4o to translate that plan into executable code, decoupling planning from implementation.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "CodeGen-Reasoning (O3-mini -&gt; GPT-4o)",
            "system_type": "hybrid LLM pipeline (planning + code generation)",
            "domain": "biomedical data science / hypothesis validation",
            "generation_capability": "Generates a structured analysis plan (pseudo-code steps), then generates executable code implementing the plan; outputs observations and a final decision.",
            "validation_method": "Plan-first then execute approach: structured analysis plans are required to use only schema column names, followed by code generation and execution; correctness measured via Type I/II errors, evidence alignment (LLM-as-judge), and code executability.",
            "novelty_measure": "No explicit numerical novelty metric; reasoning pipeline is intended to improve generalization to complex/novel analyses by enforcing explicit planning constraints and schema usage.",
            "generation_performance": "Mixed: in some domains CodeGen-Reasoning reduces Type I/II relative to the single-shot CodeGen (e.g., Biomarkers CodeGen-R* E_I=0.082, E_II=0.156 vs CodeGen gpt-4o E_I=0.090, E_II=0.164), but aggregate code executability was lower for CodeGen-Reasoning (58.2% reported for 'CodeGen Reasoning' in figure breakdown) than simpler ReAct variants.",
            "validation_performance": "Improvements in some domains for decision errors when structured planning is used, but lower code executability results in practical limitations; non-verifiable detection TPR improves over single-shot CodeGen (paper reports reasoning-augmented agents are more conservative overall).",
            "false_positive_rate": "Reduced compared to base CodeGen in some categories (e.g., Biomarkers E_I decreased from 0.090 to 0.082), values vary by domain as in Table 2.",
            "false_negative_rate": "Slight reductions in some domains; still often &gt; Type I. Example Biomarkers E_II=0.156 for CodeGen-R* vs 0.164 for base CodeGen.",
            "novelty_effect_on_validation": "Structured planning helps more on complex tasks by reducing some errors, but limited code executability constrains realized gains; reasoning helps particularly where baseline errors are high.",
            "generation_validation_asymmetry": "Partially narrowed by separating planning from codegen, but the reduction in executability (higher variable/object misuse rates) introduced new failure modes, leaving a mixed picture.",
            "out_of_distribution_performance": "Better than one-shot CodeGen on some metrics (reduced Type I/II), but out-of-distribution robustness is limited by lower executability and code errors introduced during plan-to-code translation.",
            "calibration_quality": "Improved conservatism (fewer false assertions) relative to single-shot CodeGen, but calibration remains imperfect.",
            "validation_computational_cost": "Not explicitly measured; two-stage pipeline likely increases end-to-end compute and latency compared to single-shot generation but exact costs not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Decoupled planning (O3-mini) and implementation (GPT-4o) to enforce structured analysis plans; this helps reduce some decision errors but introduces execution fragility.",
            "evidence_type": "mixed",
            "key_findings": "Structured planning (CodeGen-Reasoning) reduces some Type I/II errors versus single-shot CodeGen in certain domains, but lower code executability (58.2% in one breakdown) and common variable/object misuse errors limit overall improvements, producing a mixed impact on the generation-validation gap.",
            "uuid": "e2161.3"
        },
        {
            "name_short": "ReAct-Reasoning",
            "name_full": "ReAct agent augmented with structured planning and O3-mini backend (ReAct-R*)",
            "brief_description": "An enhanced ReAct agent that incorporates explicit structured analysis planning (via O3-mini) and iterative reasoning/execution; the top-performing agent variant in many BIODSA-1K metrics.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "ReAct-Reasoning (ReAct-R*)",
            "system_type": "hybrid iterative LLM agent with structured planning (neurosymbolic/agentic pipeline)",
            "domain": "biomedical data science / hypothesis validation",
            "generation_capability": "Generates structured analysis plans, iteratively executes code blocks, inspects outputs, refines plan and code, and produces a final decision and observed evidence.",
            "validation_method": "Iterative plan-and-execute with explicit planning: the agent must iteratively execute/refine until sufficient evidence is obtained; evaluation uses Type I/II error metrics, evidence alignment by LLM-as-judge, code executability across the interaction trace, and non-verifiable detection accuracy.",
            "novelty_measure": "No explicit numeric novelty distance metric; performance on 'hard' domains and curated strictly non-verifiable cases is used to infer robustness to novelty/out-of-distribution tasks.",
            "generation_performance": "Highest executability reported (ReAct-Reasoning 86.6%), improved Type II and Type I errors across many publication types (e.g., in several categories ReAct-R* reports the best column values in Table 2), and superior non-verifiable detection (TPR = 92%). Evidence alignment still modest (~0.20–0.25) but slightly higher than baselines.",
            "validation_performance": "Strongest among tested agents: substantially lower Type II in difficult domains (example: Integrative E_II reduced to 0.107 from CodeGen o3-mini 0.191), and best non-verifiable detection performance (92% TPR on strictly non-verifiable set).",
            "false_positive_rate": "Competitive or lower than other agents in many domains (example Genomics/Integrative reductions noted in text and Table 2), values vary by domain; e.g., ReAct-R* shows E_I as low as 0.051 in one Pan-Cancer row.",
            "false_negative_rate": "Markedly lower than single-shot baselines in several domains (e.g., Integrative reduction from 0.191 to 0.107 when comparing CodeGen o3-mini to ReAct-R*), demonstrating higher sensitivity in complex tasks.",
            "novelty_effect_on_validation": "Least sensitive to novelty among evaluated agents; reasoning and iterative execution produce the largest improvements in high-baseline-error (novel/complex) domains. ReAct-R* achieves high TPR for 'Not Verifiable' (92%), indicating robust OOD/non-verifiable detection.",
            "generation_validation_asymmetry": "Reduced significantly but not eliminated: ReAct-Reasoning improves executability and decision accuracy, but evidence alignment remains modest (many analyses still diverge from human-authored evidence), indicating residual asymmetry.",
            "out_of_distribution_performance": "Best OOD performance among variants: on strictly non-verifiable tasks achieved TPR = 92% versus 63% (CodeGen gpt-4o) and 57% (CodeGen o3-mini). Also reduces Type II substantially on integrative/genomics tasks.",
            "calibration_quality": "Better calibrated for OOD detection (high TPR for 'Not Verifiable') and reduced hallucination; still conservative (Type II generally &gt; Type I) but improved sensitivity reduces missed findings.",
            "validation_computational_cost": "Not reported quantitatively; iterative planning-execution-refinement implies higher compute and latency than single-shot methods, but yields superior validation performance.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combines structured planning (enforced schema usage), ReAct iterative reasoning-action cycles, and a two-model division of labor (O3-mini for planning) to substantially close the generation-validation gap.",
            "evidence_type": "supports",
            "key_findings": "ReAct-Reasoning attains the best overall validation behavior: highest code executability (86.6%), largest reductions in Type II errors on hard domains (e.g., Integrative), and strong OOD/non-verifiable detection (TPR 92%), showing that iterative reasoning + structured planning substantially narrows the generation-validation gap.",
            "uuid": "e2161.4"
        },
        {
            "name_short": "GPT-4o (LLM-as-extractor / LLM-as-judge)",
            "name_full": "GPT-4o used both to extract hypotheses/evidence and as an LLM-based judge for evidence alignment",
            "brief_description": "GPT-4o is used in the BIODSA-1K pipeline to extract structured hypotheses and supporting evidences from abstracts and to function as an LLM-as-a-judge that scores alignment between agent observations and ground-truth evidence.",
            "citation_title": "BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research",
            "mention_or_use": "use",
            "system_name": "GPT-4o (used for extraction and judging)",
            "system_type": "large language model",
            "domain": "biomedical literature processing and evaluation",
            "generation_capability": "Generates structured hypothesis/evidence JSON from publication abstracts; also used to compare agent-produced observations O to ground-truth evidence E to compute an Alignment Score.",
            "validation_method": "LLM-as-a-judge metric: Alignment Score = |O ∩ E| / |E| where a large language model (GPT-4o) judges whether agent observations match ground-truth evidences. Also used to extract hypotheses and structured evidence entries from abstracts via deterministic (zero-temperature) prompting.",
            "novelty_measure": "Not explicitly provided; novelty not numerically quantified by GPT-4o in this pipeline—novelty operationalized by downstream task design (non-verifiable cases, domain difficulty).",
            "generation_performance": "Effective at deterministic extraction when prompted with structured templates; used in generating benchmark entries (1,029 hypotheses and 1,177 analysis tasks) by extracting hypothesis/evidence structures from abstracts.",
            "validation_performance": "Provides alignment-scoring function used across experiments; paper reports alignment scores (agent vs ground-truth) computed via LLM-as-judge but does not report independent accuracy/calibration metrics for GPT-4o judge itself.",
            "false_positive_rate": "Not reported for the judge component; alignment score is a proportion of ground-truth evidence items captured (typical agent alignment ~0.20–0.25).",
            "false_negative_rate": "Not reported for LLM-as-judge; not evaluated for judge calibration in paper.",
            "novelty_effect_on_validation": "Not explicitly measured for the judge; paper does not report whether the LLM-as-judge calibration degrades on novel/unseen evidence formulations.",
            "generation_validation_asymmetry": "Serves as an evaluation tool rather than a generative hypothesis-testing agent; its use highlights mismatch between agent outputs and human-reported evidence (i.e., it reveals generation-validation gaps) but its own reliability is not deeply characterized.",
            "out_of_distribution_performance": "Not reported for the judge component.",
            "calibration_quality": "Not reported; the paper acknowledges use of LLM-as-judge but does not provide judge calibration metrics.",
            "validation_computational_cost": "Used offline to compute alignment scores; compute cost not reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Using an LLM-as-judge provides an automated, scalable evidence-alignment metric, but paper emphasizes need for human oversight due to modest alignment scores and potential judge limitations.",
            "evidence_type": "neutral",
            "key_findings": "GPT-4o was used to reliably extract hypotheses/evidence for the benchmark and to compute an automated evidence-alignment score; however, alignment scores of agents (0.20–0.25) reveal that agents often fail to reproduce ground-truth evidence even when judged by an LLM, underscoring the generation-validation gap and the need for human oversight.",
            "uuid": "e2161.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated hypothesis validation with agentic sequential falsifications",
            "rating": 2
        },
        {
            "paper_title": "Towards an ai co-scientist",
            "rating": 2
        },
        {
            "paper_title": "BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments",
            "rating": 2
        },
        {
            "paper_title": "DiscoveryBench: Towards data-driven discovery with large language models",
            "rating": 2
        },
        {
            "paper_title": "Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 1
        }
    ],
    "cost": 0.01978625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research
22 May 2025</p>
<p>Zifeng Wang zifengw2@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Benjamin Danek 
University of Illinois Urbana-Champaign</p>
<p>Jimeng Sun jimeng@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>BIODSA-1K: Benchmarking Data Science Agents for Biomedical Research
22 May 20254CE7245871230092A6609653A6257CC0arXiv:2505.16100v1[cs.AI]
Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation.In this work, we present BIODSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks.BIODSA-1K consists of 1,029 hypothesiscentric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows.Each task includes a structured hypothesis derived from the original study's conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables.While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods.The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code.Importantly, BIODSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science.We propose BIODSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.</p>
<p>Introduction</p>
<p>Artificial intelligence (AI) agents promise to accelerate scientific discovery [1,2], with the emergence of "AI scientists" [3] capable of collaborating with human researchers to perform research tasks such as literature mining and data analysis [4][5][6][7].Large language models (LLMs) [8] can serve as the intelligence backbone for converting natural language to structured outputs such as code and mathematical expressions.As a core task in biomedical research, data science bridges the gap from proposed hypotheses to novel discoveries leveraging biomedical data.For example, a researcher might hypothesize that "Genes involved in histone modification " are frequently mutated in non-Hodgkin lymphoma."Testing such hypotheses often requires close collaboration between biomedical experts and data scientists to design analyses, write code, and interpret results, and thus far has been mainly manual efforts in practice [9].</p>
<p>Recent efforts have demonstrated LLM-based agents capable of designing experiments, generating code, and summarizing results [10][11][12][13].However, existing systems often focus on narrow tasks within biomedical research or are evaluated on limited scenarios.In benchmark dataset that captures the breadth and complexity of data science tasks in biomedical research.The following challenges have not been fully explored: (1) although previous studies leverage publications to create data science tasks [10,14,15], those test cases are drawn from a small number of papers, which may not reflect the full scope of biomedical research; (2) limited task diversity as a consequence of restricted case selection; (3) the involved tasks are performed on relatively simple datasets, such as one or two tables with tens of columns; (4) overlooking the foundational data analysis steps and observed evidence that support or refute a hypothesis, thus, correct hypothesis prediction alone does not guarantee the agent performed the correct analysis; and</p>
<p>(5) the inclusion of non-verifiable hypotheses, where the required data is absent or insufficient to support a conclusive answer, yet such cases are rarely discussed.</p>
<p>In this paper, we introduce BIODSA-1K (Biomedical Data Science Agent Benchmark), a novel framework for evaluating AI agents on biomedical data science research tasks (Figure 1).BIODSA-1K specifies a complete cycle of hypothesis formulation, data analysis, and validation, by curating detailed experimental components extracted from published biomedical studies.Specifically, each instance includes a hypothesis statement, corresponding analysis plans, evidence summaries, and quantitative outcome measures.</p>
<p>As illustrated in Figure 1, BIODSA-1K includes 1,029 scientific hypotheses and the corresponding 1,177 analysis tasks drawn from 329 publications of eight types of publications.The analysis tasks are also comprehensive in terms of common analysis is done in biomedical research.A comparison to other representative data science benchmarks is illustrated in Table 1.</p>
<p>BIODSA-1K: Benchmark data and tasks</p>
<p>BIODSA-1K is constructed from scientific publications and their associated biomedical data.At the core of the benchmark are structured components that mirror the research process: a curated collection of publications and corresponding data tables, extracted hypotheses paired with supporting evidence, and data analysis tasks derived from these elements.This framework supports the development and assessment of AI agents on a wide spectrum of capabilities, from code generation and reasoning to hypothesis testing, grounded in scientific discovery workflows.In the following subsections, we detail the construction of BIODSA-1K, including how publications and data were collected, how hypotheses and supporting evidence were extracted, and how downstream tasks were defined to challenge and benchmark agent performance.</p>
<p>Publication and dataset collection</p>
<p>To construct a benchmark that reflects practical biomedical data science, it is essential to include not only scientific publications but also the corresponding biomedical datasets on which those studies are based.We therefore leverage cBioPortal [16], a comprehensive cancer genomics and clinical data TP53 is an oncogene of prostate cancer tumors.</p>
<p>Supporting evidence</p>
<p>Hypothesis</p>
<p>AI agents Biomedical datasets</p>
<p>Gene Lab Timeline</p>
<p>…</p>
<p>Experiments</p>
<p>Hypothesis decisions</p>
<p>Evidence alignment</p>
<p>Alignment score ∈ 0,1</p>
<p>Non-verifiable decision</p>
<p>Evaluation metrics  portal that maintains structured datasets with direct linkage to peer-reviewed publications.It is under a publicly available Open Database License [17].This ensures that our benchmark captures both the analytical context and the quantitative evidence underlying published findings.In particular, we assume that each publication highlights its primary results within the abstract, often supported by descriptive statistics, statistical testing, and predictive modeling results derived from the associated biomedical tables.Thus, the raw data underlying BIODSA-1K consists of two components: the publication abstracts and their corresponding structured biomedical data tables.</p>
<p>Benchmark curation
Publication Hypothesis Hypothesis Hypothesis
We utilize the cBioPortal API 2 to retrieve all available datasets in bulk.Each dataset includes study metadata that specifies the associated publication(s), including PubMed identifiers (PMIDs).Using these PMIDs, we collect the publication abstracts through the PubMed API 3 .In most cases, there is a one-to-one mapping between a dataset and a publication.However, we exclude ambiguous cases involving multiple papers and datasets when their analytical scope extends beyond the specific dataset.This filtering step avoids introducing non-verifiable hypotheses into the benchmark, thereby maintaining a clear linkage between reported findings and the underlying data.</p>
<p>According to established biomedical literature [18], we categorize the publications in our benchmark by study types.Definitions of these categories are provided in Appendix D. As illustrated in Figure 1, BIODSA-1K spans a diverse array of study types, including genomics, integrative, therapeutics, biomarkers, translational, and molecular studies, along with various analysis methodologies.This distribution highlights the comprehensiveness of BIODSA-1K, capturing both high-level exploratory research and focused hypothesis-driven studies.</p>
<p>Dataset caption</p>
<p>We caption the data tables for benchmarking in data science tasks while preserving privacy.The details of how the captioning works can be found in Appendix B. Specifically, we do not send any patient-level records to LLMs and instead construct a schema-based representation.For each column in a data table, we compute type-specific descriptive statistics, such as the number of unique values, missing value ratio, most frequent entries, and data ranges.In this way, for whatever LLM API provider we use, only the captions of the dataset will be shared.For future research and experiments Biology Hypothesis and analysis 8 Publications --8 BioCoder [12] Biomedical Analysis Github --460 ChatGPT-ADA [24] Biomedical Hypothesis and analysis 4 Publications 1 548 4 AI Co-scientist [3] Biomedical Hypothesis and analysis ---3 BioDiscoveryAgent [11] Biomedical with this benchmark, researchers can download the raw data from cBioPortal and execute the LLMgenerated code on them locally.</p>
<p>Figure 1 shows the scale and diversity of the biomedical tables included in BIODSA-1K.Each point represents a data type, positioned by its typical number of rows and columns, and sized by its prevalence in the dataset.The benchmark encompasses a wide spectrum of commonly used biomedical data types, including clinical data, mutation data, gene expression, copy number alteration, protein expression, structural variation, and patient timelines.These data sources are foundational to modern biomedical research and collectively capture the heterogeneity of real-world biomedical analysis.Moreover, the wide variance in both row and column dimensions, ranging from compact gene panels to large-scale expression matrices, demonstrates the high dimensionality and analytical complexity present in BIODSA-1K.Compared to existing benchmarks (as shown in Table 1), which often involve simpler, smaller, or less diverse datasets, our benchmark presents a significantly more challenging and realistic setting for evaluating AI agents on biomedical data science tasks.</p>
<p>Hypothesis and evidence</p>
<p>All data science challenges in BIODSA-1K are extracted from published biomedical studies using a GPT-4o model.The details of the extraction process can be found in Appendix C. Each challenge is centered around a hypothesis and its corresponding supporting evidence, reflecting how scientific claims are typically articulated in real-world literature.Rather than stating hypotheses solely in null form (e.g., "no difference between groups"), authors of original studies often present claims affirmatively (e.g., "Treatment A improves survival"), while the underlying analyses are grounded in statistical tests against a null hypothesis.To preserve fidelity to real-world practice, our benchmark follows this formulation, presenting hypotheses as definitive statements derived from the study's conclusions.Importantly, our design does not assume these statements are inherently true; instead, we evaluate whether AI agents can reconstruct the reasoning and analysis pipeline leading to such claims, including identifying when the data are insufficient to support them.</p>
<p>Each entry includes (1) a clearly stated hypothesis that is supported or rejected in the original publication, and (2) a plausible counter-hypothesis designed to test the agent's ability to reason discriminatively.An example is provided in Supplementary Figure 1.To support hypothesis validation, we extract one or more evidence entries per hypothesis, each corresponding to a distinct data analysis performed in the study.Each evidence entry is annotated with the following fields:</p>
<p>• Analysis plan: a concise description of the statistical or computational procedure used (e.g., frequency analysis, correlation test, clustering).</p>
<p>• Evidence: a textual summary of the result as reported in the publication.</p>
<p>• Variables: input variables used in the analysis and the result variable serving as the output to support or refute the hypothesis.</p>
<p>To mitigate bias toward Type I error, our benchmark includes a significant fraction of non-verifiable cases where the available data are insufficient to reach a definitive conclusion.This design encourages agents not merely to "prove" hypotheses, but to assess them critically in the context of the available evidence, akin to a real-world research setting.</p>
<p>Tasks and evaluation</p>
<p>The primary task in BIODSA-1K is hypothesis validation using structured biomedical data.Given a hypothesis extracted from a publication and the corresponding dataset, an AI agent is required to generate executable code to analyze the data and produce empirical observations.Based on these observations, the agent must decide whether the hypothesis is True, False, or Non-verifiable.To distinguish between the latter two, we define a hypothesis as False if the agent can identify relevant variables in the dataset and derive contradicting evidence through analysis.Conversely, a hypothesis is considered Non-verifiable if no relevant features or data tables exist in the dataset to support or reject the claim.For example, the hypothesis "Prostate cancer brain metastases (PCBM) have a higher mutational burden compared to non-brain metastases" is labeled as Non-verifiable if the dataset lacks mutational burden variables or comparative group labels.</p>
<p>We evaluate agent performance across multiple dimensions.On the hypothesis decision level, we compute both Type I and Type II error rates.Let H ∈ True, False denote the ground truth label of a hypothesis, and Ĥ be the label predicted by the agent.The Type I error (false positive rate) is defined as:
Type I Error = I[H = False ∧ Ĥ = True] I[H = False] ,(1)
and the Type II error (false negative rate) is given by:
Type II Error = I[H = True ∧ Ĥ = False] I[H = True] ,(2)
where I[•] is the indicator function.</p>
<p>In addition to correctness at the decision level, we assess how well the generated observations align with the supporting evidence reported in the original publication.Let E denote the set of ground truth supporting evidences and O the set of observations generated by the agent.We use a large language model (LLM)-as-a-judge [25] approach to measure the evidence alignment score:
Alignment Score = |O ∩ E| |E| .(3)
This metric quantifies the proportion of reported evidence that is successfully captured by the agent's analysis pipeline.</p>
<p>Furthermore, we evaluate the technical quality of the generated code.For each hypothesis, let C denote the total number of code cells generated and C exec the number of those that are executable without error.The code executability rate is defined as: Executability Rate = Cexec C .For ReAct-style agents that explore through multi-step reasoning, this metric is computed over all code snippets generated during the interaction trace.Lastly, we systematically assess agents on their ability to reject non-verifiable hypotheses.These hypotheses are curated by taking claims from other publications that reference unrelated datasets.An ideal agent should classify such hypotheses as Non-verifiable due to the absence of relevant data.Let H = Non-verifiable be the ground truth and Ĥ be the predicted label.We report the non-verifiable detection accuracy as:   We implement four agent-based methods to evaluate performance on BIODSA-1K.CodeGen directly generates a single executable Python code block based on the input hypothesis and dataset schema, and returns a final decision, True, False, or Non-verifiable, based on the produced observations, without explicit intermediate reasoning [26].We evaluate two variants of CodeGen: one powered by GPT-4o and the other by O3mini.ReAct follows the ReAct framework [27], in which the agent alternates between reasoning steps ("thoughts") and code execution ("actions"), allowing iterative refinement of analysis and conclusions.This version is implemented using GPT-4o.
Non-verifiable Accuracy = I[H = Non-verifiable ∧ Ĥ = Non-verifiable] I[H = Non-verifiable] .(4)
To enable more structured reasoning, we also introduce two reasoning-augmented agents aligned with recent developments in data analysis agents [6,13], which decouple experiment planning from execution.CodeGen-Reasoning first prompts O3-mini to generate a structured analysis plan detailing key reasoning and statistical steps, and then passes this plan to GPT-4o for code generation and execution, allowing division of labor between planning and implementation.ReAct-Reasoning extends ReAct with structured planning and uses O3-mini as the backend agent.It supports iterative reasoning and dynamic plan refinement based on intermediate observations across multiple steps.</p>
<p>Hypothesis validation</p>
<p>Table 2 shows that AI agents tend to be conservative in hypothesis validation across all tested publication types.In nearly every setting, the Type II error rate (E II ), which measures the frequency of missed relevant findings, is consistently higher than the Type I error rate (E I ), which reflects the incidence of false positives.For example, in the Biomarkers category, CodeGen (gpt-4o) exhibits a Type II error of 0.164 compared to a Type I error of 0.090.Figure 3 shows that ReAct-based methods consistently outperform CodeGen models, particularly when reasoning is applied.Even without reasoning, ReAct (gpt-4o) achieves lower Type II errors in challenging categories such as Integrative (0.148 vs. 0.153) and Pan-Cancer (0.128 vs. 0.167) compared to CodeGen (gpt-4o).When reasoning is incorporated, ReAct-R<em> outperforms CodeGen-R</em> in most domains, for example, in Translational, ReAct-R<em> reports a Type II error of 0.112 compared to 0.110 for CodeGen-R</em>, while maintaining a lower Type I error (0.098 vs. 0.060).</p>
<p>Finally, Figure 3 suggests that reasoning brings the greatest improvements in domains with higher baseline error rates.This trend is evident in the Genomics and Integrative categories, where non-reasoning methods exhibit relatively high Type II errors: up to 0.191 for CodeGen (o3-mini) in Integrative.In contrast, ReAct-R* reduces the same error to 0.107.This implies that reasoning is particularly valuable in more complex or information-dense publication types, helping agents better navigate and resolve ambiguous or detailed hypotheses.</p>
<p>Analysis quality</p>
<p>Making a correct hypothesis decision does not necessarily imply that the AI agent followed a valid or faithful analytical process, which is a limitation largely overlooked in prior evaluations.To address this, our benchmark explicitly assesses the evidence alignment score, which measures how well the agent-generated analysis captures the ground-truth evidence reported in the original studies.We also examine the executability of the analysis code produced by the agents as a proxy for code quality and practical usability.</p>
<p>As shown in Figure 5, the evidence alignment scores remain modest across all methods, typically ranging from 0.20 to 0.25, regardless of whether the hypothesis being validated is ultimately True or False.Among the evaluated methods, ReAct-based agents exhibit marginally higher alignment scores compared to code generation baselines.However, the consistently low scores across the board suggest  that AI agents often diverge from the evidence used in human-authored analyses, possibly reflecting a lack of domain knowledge or contextual understanding required for appropriate methodological choices.</p>
<p>Figure 6 further breaks down alignment scores by analysis type.We observe that simpler analytical tasks, such as frequency counts, are more reliably handled by AI agents.In contrast, more complex tasks, including clustering and survival analysis, pose significant challenges, with notably lower alignment scores across all models.These findings highlight the need for improved reasoning strategies and domain-specific modeling capabilities in AI systems aimed at biomedical data analysis.</p>
<p>Figure 4 presents the executability of the code generated by different AI agents and categorizes the types of errors found in non-executable outputs.Overall, ReAct-based agents exhibit the highest code executability rates, with ReAct Reasoning achieving 86.6% and ReAct at 84.9%, outperforming both CodeGen (76.9%) and CodeGen Reasoning (58.2%).Among the error types, variable or object misuse is the most common failure mode, especially prominent in CodeGen Reasoning (27.7%) and ReAct (32.1%).Logic and mathematical errors, as well as import or module-related issues, occur less frequently but still contribute to code failure across methods.7 presents the distribution of hypothesis decisions (True, False, Not Verifiable) based on whether the generated code was executable or not.The results reveal a marked difference in decision patterns between the executable and non-executable cases.In the non-executable setting, all three CodeGen variants default to deciding the hypothesis as Not Verifiable in approximately 87% of instances, but still decide around 8% as False and 5% as True.These 13% cases indicate that the AI agents sometimes turn out to hallucinate the findings.By contrast, in cases where the generated code is executable, the proportion of Not Verifiable decisions significantly drops, while the rates of True and False decisions increase substantially.</p>
<p>Non-verifiable hypothesis</p>
<p>We further investigated whether AI agents can act cautiously when faced with non-verifiable hypotheses.As shown in Figure 8, we constructed a set of 100 hypotheses that are strictly non-verifiable, meaning the associated dataset lacks the information needed to either accept or reject them.In this setting, the correct model behavior is to respond with "Not Verifiable"; any decision of "True" or "False" reflects overconfidence or hallucination.The ability to correctly identify these cases, quantified as the true positive rate (TPR) for the "Not Verifiable" class, varies substantially across agents.One-round code generation methods, such as CodeGen gpt-4o and CodeGen o3-mini, achieve only 63% and 57% TPR, respectively, often making incorrect verifiable claims in 37% and 43% of the cases.In contrast, reasoning-augmented agents like CodeGen Reasoning, ReAct, and particularly ReAct Reasoning perform more conservatively, with ReAct Reasoning achieving a TPR of 92%.</p>
<p>Related work</p>
<p>Benchmarks Recent efforts have introduced benchmark datasets to evaluate AI agents in scientific discovery and data science tasks.General-purpose scientific discovery benchmarks such as DiscoveryBench [14], ScienceAgentBench [15], and SpiderV2 [28] focus on a broad range of tasks but often overlook specialized biomedical reasoning challenges.In parallel, several benchmarks specifically target the core task of code generation in scientific domains, including SciCode [23], Blade [22], and DSBench [21].Within biomedicine, BioCoder [12] and CliniDSBench [10] address coding tasks related to biomedical data analysis.However, these benchmarks primarily emphasize code generation, while our work focuses on the broader hypothesis validation process derived directly from published scientific studies.Moreover, BIODSA-1K offers a significantly larger and more diverse evaluation scale, encompassing over three hundred publications, substantially exceeding the coverage of previous benchmarks.</p>
<p>Agents A growing body of work explores the use of AI agents for data science and scientific research.Several systems target general data science tasks, including machine learning modeling and analysis on structured datasets such as Kaggle competitions [29][30][31]20].Gao et al. [7] emphasizes the potential of developing agents specifically tailored for biomedical research.In the biomedical domain, Co-Scientist [3] and BioDiscoveryAgent [11] focus on a niche area: automating the design and execution of genetic perturbation experiments.Other agent frameworks have applied LLMs for bioinformatics programming, biomedical question answering [32], and the development of predictive models for biological outcomes [24].Closest to our work is the line of research on hypothesis validation agents [13], which investigates how agents can reason over structured data to accept or refute scientific claims.Our work builds on these foundations but uniquely grounds the validation tasks in hypotheses and evidence derived from real-world publications, enabling broader and more rigorous evaluation of biomedical data science agents.</p>
<p>Discussion and conclusion</p>
<p>While our benchmark draws from over 300 biomedical studies, it does not fully capture the diversity of the biomedical research landscape.The dataset naturally overrepresents well-established topics with high publication volume, potentially underrepresenting emerging areas or those with limited available data.This skew may influence model performance and generalizability, highlighting the need to continuously expand and rebalance the benchmark to reflect a wider spectrum of scientific inquiry.More broadly, as AI agents become increasingly capable of performing end-to-end data science tasks, they also introduce the risk of generating plausible but incorrect scientific claims.Without proper oversight, such systems could accelerate the propagation of false findings under the guise of data-driven analysis.Ensuring transparency, interpretability, and human-in-the-loop validation will be critical to responsibly deploying these tools in high-stakes scientific domains.</p>
<p>In this work, we present BIODSA-1K, a benchmark designed to evaluate AI agents on realistic biomedical data science tasks.By extracting over a thousand hypotheses and corresponding analysis plans from hundreds of published studies, BIODSA-1K captures the diversity and complexity inherent in real-world biomedical research.Unlike prior benchmarks, it encompasses not only hypothesis validation tasks with sufficient evidence, but also non-verifiable cases where the available data are inconclusive: a frequent yet underrepresented scenario in scientific reasoning.The benchmark enables comprehensive evaluation across multiple dimensions, including decision accuracy, evidence grounding, reasoning validity, and analysis code executability.We envision BIODSA-1K as a foundation for developing more robust, transparent, and trustworthy AI agents for scientific discovery.</p>
<p>[31] Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, et al.Autokaggle: A multi-agent framework for autonomous data science competitions.arXiv preprint arXiv:2410.20424,2024.</p>
<p>[32] Nikita Mehandru, Amanda K Hall, Olesya Melnichenko, Yulia Dubinina, Daniel Tsirulnikov, David Bamman, Ahmed Alaa, Scott Saponas, and Venkat S Malladi.BioAgents: Democratizing bioinformatics analysis with multi-agent systems.arXiv preprint arXiv:2501.06314,2025.</p>
<p>Justification: This paper does not involve theoretical results.Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental result reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?</p>
<p>Answer Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and research with human subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Answer: [NA]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional review board (IRB) approvals or equivalent for research with human subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Answer: [NA]</p>
<p>Justification: [NA] Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.In one analysis, the study performed a correlation analysis between MGMT promoter methylation status and mutation rates, reporting a statistically significant positive correlation (r = 0.65, p &lt; 0.01), suggesting that higher methylation is linked to increased mutation burden.A second analysis compared mutation rates between tumors with methylated versus unmethylated MGMT promoters, showing that methylated tumors had significantly higher mutation rates (mean difference = 15 mutations per sample, p &lt; 0.05).These analyses collectively support the hypothesis.</p>
<p>Example 2: PIK3R1 Mutation Frequency.This hypothesis posits that the PIK3R1 gene is frequently mutated in glioblastoma samples, with the null hypothesis stating that it is not.In the first analysis, the study reported a mutation frequency of 25% for PIK3R1 across glioblastoma samples, indicating a notable prevalence.A second analysis ranked gene mutation frequencies and found that PIK3R1 was among the top 10% of all mutated genes, further supporting the claim of frequent alteration.</p>
<p>These examples demonstrate how BIODSA-1K captures both the semantic structure of biomedical hypotheses and the analytical reasoning used to evaluate them, providing a grounded framework for assessing the capabilities of AI agents in data-driven scientific inference.</p>
<p>B Captions of Biomedical Data Tables</p>
<p>To support automated hypothesis validation and dataset reasoning tasks, we systematically generated structured captions for biomedical data tables from the cBioPortal repository.Each caption describes the content and structure of a tabular dataset, including its schema, value distributions, and metadata annotations.We developed a modular pipeline to process every dataset directory and extract metadata from text-based tables, primarily those with filenames beginning with data_.</p>
<p>For each dataset, we extracted high-level metadata including dataset ID, cancer type, and description.Within each dataset directory, we identified all data tables and parsed their contents while ignoring comment lines (those beginning with "#").The first non-comment line was interpreted as the column header.Subsequent lines were parsed as tab-delimited rows, with short rows padded and long rows truncated to maintain schema alignment.</p>
<p>To ensure consistency and facilitate downstream usage, we cleaned column names by removing punctuation and replacing whitespace with underscores.We then inferred the data type of each column using a custom heuristic function and computed column-wise statistics depending on the inferred type:</p>
<p>• Binary and categorical columns: Top value counts and number of unique values were reported, along with missing value rate.• Integer-valued columns: We computed quantiles (1%, 20%, 40%, 60%, 80%, 99%) as well as minimum and maximum values.• Continuous columns: Descriptive statistics were generated, including count, mean, standard deviation, and range, rounded to four decimal places.</p>
<p>Each table's caption includes its name, number of rows and columns, column-level statistics, and preserved comment rows if present.The final structured metadata was saved as JSON files under a centralized metadata directory, one per dataset.These captions serve as machine-readable documentation for real-world biomedical tables and are critical for enabling dataset-aware reasoning by AI agents.</p>
<p>An example metadata structure is as follows:</p>
<p>{ "dataset_id": "example_ds", "type_of_cancer": "glioblastoma", "description": "Clinical and genomic data for GBM samples", "tables": [ { 2. All evidence must include specific, measurable quantities or statistical relationships 3. Result values must be numerical (e.g., percentages, counts, p-values, correlation coefficients) or categorical with clear classifications 4. Analysis variables must be specific data columns or features that exist in the dataset Return your answer as a JSON object in the following format: '''json { "hypotheses": [ { "hypothesis": a specific, binary hypothesis that can be tested statistically, from the abstract, the one which is considered to be true from the study, "wrong_hypothesis": make a random perturbation of the hypothesis so that it is a wrong hypothesis, "supporting_evidences": [ // the evidences that support the alternative hypothesis { "analysis_plan": a brief analysis plan that can yield this evidence, "evidence": specific statistical finding or measurement, "analysis_variables": list of exact variables/features needed for analysis, "result_variable": the specific metric or statistical measure used, "result_variable_value": numerical value, statistical measure, or categorical outcome }, ... ] }, ... ] } """</p>
<p>To increase throughput and reliability, we used batched LLM calls with zero temperature to ensure deterministic completions.Each LLM output was parsed using a custom function that handled both valid JSON and malformed output formats via regular expression matching.The output structure follows a fixed schema that includes a hypothesis, a wrong_hypothesis (a small perturbation to simulate a plausible counterfactual), and a list of supporting_evidences, each containing fields such as analysis_plan, evidence, analysis_variables, result_variable, and result_variable_value.</p>
<p>Each extracted hypothesis is linked to its source publication (via PMID) and the relevant datasets (via dataset ID) so that the claim can later be validated against real-world biomedical tables.The full outputs were stored as structured JSON files, one per publication.This corpus forms the foundation of BIODSA-1K, enabling AI agents to reason over realistic, evidence-backed scientific claims.</p>
<p>D Categorization of biomedical publications</p>
<p>Genomics Publications in this category focus on large-scale genomic profiling of tumors.These studies utilize high-throughput sequencing to catalog somatic mutations, copy-number variations, and other genetic alterations across cancer samples.</p>
<p>Molecular This class covers research that investigates molecular characteristics beyond DNA mutations.It includes analyses of transcriptomic, proteomic, and epigenomic data, often derived from both patient samples and established cancer cell lines.</p>
<p>Pan-Cancer Pan-Cancer studies undertake comparative analyses across multiple types of cancers.They aim to identify common molecular patterns and differences, thereby deepening our understanding of shared and unique cancer pathways.</p>
<p>Therapeutics These publications explore the relationship between genomic alterations and drug responses.The focus is on identifying potential therapeutic targets and advancing personalized treatment strategies based on genetic and molecular data.</p>
<p>Biomarkers Research in this class is dedicated to discovering and validating diagnostic and prognostic markers.These biomarkers help in predicting disease outcomes, guiding treatment decisions, and supporting early detection.</p>
<p>Methods Publications categorized as Methods introduce new computational tools, algorithms, or experimental techniques that facilitate the analysis and interpretation of complex biomedical data.</p>
<p>Integrative Integrative studies combine data from multiple omics layers-such as genomics, transcriptomics, and proteomics-to provide a comprehensive view of tumor biology.They aim to interconnect disparate data types into coherent biological insights.</p>
<p>Translational: This class emphasizes bridging the gap between research and clinical application.Translational studies apply genomic and molecular findings to improve diagnostic methods, prognostic assessments, and treatment strategies in clinical practice.</p>
<p>E Categorization of analysis tasks</p>
<p>Correlation analysis (Correlation) Tasks that focus on statistically relating two or more variables.Examples include correlating gene methylation status with mutation rates or associating mutational profiles with clinical factors such as smoking status.</p>
<p>Comparative analysis (Comparison) Tasks that directly contrast groups or conditions.These include comparing mutation frequencies between groups (e.g., methylated vs. unmethylated promoters) or contrasting profiles across different cancer subtypes.</p>
<p>Frequency analysis (Frequency) Tasks that measure the occurrence or rate of specific genomic events.Typical examples are calculating the mutation frequency for a given gene or determining the prevalence of a particular genetic alteration.</p>
<p>Clustering and classification (Clustering) Tasks that involve grouping data based on similarities.These studies might use cluster analysis to categorize samples by genomic features or mutational signatures.</p>
<p>Survival and prognostic analysis (Survival) Tasks that associate molecular or genomic features with patient outcomes, such as survival curve comparisons or prognostic evaluations.</p>
<p>Functional and experimental analysis (Functional) Tasks that explore gene function or cellular behavior through experimental approaches.This includes RNA interference experiments or assays measuring the effects of gene knockdown on cell proliferation.</p>
<p>Genomic structural analysis (Structural)</p>
<p>Tasks that analyze genomic architecture or structural variants.Examples include the evaluation of copy-number alterations, genomic rearrangements, or spatial mutation distributions.</p>
<p>Pathway and integrative analysis (Pathway) Tasks that integrate multiple data types to elucidate biological pathways and networks.These include integrative pathway analyses, enrichment studies, or assessments of driver mutations in signaling cascades.</p>
<p>F Categorization of code errors</p>
<p>To enable systematic analysis of common failure modes in AI-generated code, we group low-level Python error types into broader categories reflecting common code quality issues.This many-to-one mapping provides a more interpretable summary of model behaviors and facilitates downstream visualization and comparison.The categorization is defined as follows:</p>
<p>• Variable/Object Misuse: Errors such as KeyError, AttributeError, NameError, and IndexError that arise from referencing undefined variables, missing dictionary keys, or invalid object attributes.</p>
<p>• Math/Logic Error: Includes errors like ZeroDivisionError, ValueError, and numpy.linalg.LinAlgError, which typically result from invalid arithmetic operations, numerical instability, or logical violations.</p>
<p>• Import/Module Error: Consists of ImportError and ModuleNotFoundError, indicating missing dependencies or incorrect import paths.</p>
<p>• File/I-O Error: Captures input/output-related issues such as FileNotFoundError and OSError, often caused by referencing unavailable files or malformed I/O operations.</p>
<p>• Pandas/Data Error: Includes errors from data processing libraries, such as pandas.errors.ParserError, MergeError, and IndexingError, typically caused by invalid parsing, merging, or indexing operations.</p>
<p>• General Exception: Encompasses generic or runtime-specific errors such as Exception and RuntimeError, which represent critical failures not captured by more specific categories.</p>
<p>G Agent prompts G.4 ReAct-Reasoning AGENT_MODEL_PROMPT_TEMPLATE = """ You are a scientific agent who can plan and execute python code iteratively to evaluate a scientific hypothesis.</p>
<p>Note:</p>
<p>-You must execute and refine the given analysis plan iteratively until you have enough evidence to support the hypothesis.-You must always write a single Python code block that can be executed directly based on the analysis plan.-Use 'print()' statements in your code to get the observations.""" PLANNING_PROMPT_TEMPLATE = """ # TASK Generate an analysis plan to evaluate the user's scientific hypothesis using the datasets provided.</p>
<p>The plan should consist of clear, actionable psudo codesteps that can be <strong>easily converted to python code</strong> without needing any additional information.</p>
<h1>REQUIREMENTS -Use only table and column names from the schema: do not invent or guess names.</h1>
<p>-Ensure every step is unambiguous and directly executable.</p>
<p>-Use consistent naming for all variables (e.g., tables, columns) throughout the plan.-Be as concise as possible while maintaining full clarity and precision.# DATASET PATHS {dataset_paths} # DATASET SCHEMA {dataset_schema} """</p>
<p>Figure 1 :
1
Figure 1: Benchmark statistics.(left) BIODSA-1K includes diverse types of biomedical research and data analysis tasks created from 329 publications; the x-axis indicates the publication types.; (Right) Bubble plot illustrating the diverse range of biomedical data tables in BIODSA-1K, showing each data table's number of rows (x-axis, log-scale) versus number of columns (y-axis, log-scale).</p>
<p>Figure 2 :
2
Figure 2: Overview of BIODSA-1K.a, Benchmark curation: Scientific publications linked to biomedical datasets are parsed to extract hypotheses and their corresponding supporting evidence, forming the core reasoning challenges.b, Experiments: AI agents are tasked with validating hypotheses by planning analysis steps, generating executable code, observing results, and making decisions based on structured biomedical datasets.c Evaluation metrics: Agent performance is evaluated based on hypothesis decision accuracy (Type I and Type II errors), evidence alignment with publication findings, non-verifiable hypothesis detection (precision and recall), and code executability rate.</p>
<p>Figure 3 :
3
Figure 3: Comparison of Type I and Type II error rates across publication types and agent variants.Each point denotes an agent's performance on a specific publication type.</p>
<p>Figure 3 andFigure 5 :
35
Figure3and Table 2 also demonstrate that reasoning augmentation improves both sensitivity and specificity.Reasoning-enhanced agents (denoted with an asterisk, e.g., CodeGen-R<em> and ReAct-R</em>) consistently outperform their base counterparts in terms of lower error rates.For instance, ReAct-R*</p>
<p>Figure 4 :
4
Figure 4: Code excitability analysis and the breakdown of error types in non-executable code across the selected AI agents.</p>
<p>Figure 7 :
7
Figure 7: Hypothesis validation results' distribution by the code excitability for CodeGen methods.</p>
<p>Table 1 :
1
Comparison of BIODSA-1K with representative benchmarks in general and biomedical domains."Avg.# Tables" denotes the average number of tables per task; "Avg.# Columns" refers to the average columns per table."-" indicates missing or non-tabular data."# Tasks" shows the number of unique data science tasks."<em>" indicates the biology-related portions of the benchmarks.
BenchmarkDomainTask LevelsTask SourcesAvg. # Tables Avg. # Columns</em> # TasksDS-1000 [19]GeneralAnalysisStackoverflow1-1000MLAgentBench [20]GeneralAnalysisPublications14713DSBench [21]GeneralAnalysisKaggle--466BLADE [22]GeneralHypothesis and analysis 31 Publications11312ScienceAgentBench [15]GeneralHypothesis and analysis 44 Publications--102DiscoveryBench-Bio<em> [14] BiologyHypothesis and analysis 2 Publications22616SciCode-Bio</em> [23]</p>
<p>Table 2 :
2
Performance of hypothesis validation across publication types.Each cell reports the Type I error rate (E I , false positive rate) and Type II error rate (E II , false negative rate), with lower values indicating better performance."R<em>" is short for "Reasoning" version of CodeGen and ReAct, respectively.Bold values highlight the best performance in each column.
BiomarkersGenomicsIntegrativeMolecularPan-CancerTherapeuticsTranslationalMethods(n=244) E I E II(n=662) E I E II(n=392) E I E II(n=108) E I E II(n=78) E I E II(n=344) E I E II(n=224) E I E IICodeGen (gpt-4o)0.090 0.164 0.077 0.168 0.095 0.153 0.157 0.157 0.077 0.167 0.087 0.137 0.094 0.147CodeGen (o3-mini) 0.107 0.145 0.128 0.187 0.122 0.191 0.098 0.118 0.103 0.179 0.157 0.181 0.143 0.138ReAct (gpt-4o)0.102 0.148 0.069 0.159 0.066 0.148 0.120 0.167 0.115 0.128 0.090 0.155 0.089 0.161CodeGen-R</em>0.082 0.156 0.054 0.139 0.082 0.125 0.111 0.139 0.141 0.154 0.083 0.148 0.060 0.110ReAct-R*0.090 0.094 0.060 0.125 0.074 0.107 0.074 0.093 0.051 0.167 0.087 0.122 0.098 0.112This provides insight into how well agents can discern dataset limitations and avoid over-assertiveconclusions.3 Experiment3.1 Implemented methods</p>
<p>Figure 6: Evidence alignment scores by types of analyses.reducesthe Type I and II errors in the Genomics category to 0.060 and 0.125, respectively, compared to 0.069 and 0.159 for the base ReAct model.Similarly, CodeGen-R* achieves a Type I error of 0.082 and Type II error of 0.156 on Biomarkers, outperforming the original CodeGen (gpt-4o) with errors of 0.090 and 0.164.These results indicate that structured reasoning enhances the agent's ability to identify relevant evidence while reducing false positives.
0.35300.27 (0.26-0.28)0.26 (0.24-0.28)Alignment Score0.23 (0.22-0.24)0.21 (0.19-0.23)0.20 (0.19-0.22)0.19 (0.18-0.20)0.19 (0.17-0.21)0.16 (0.14-0.17)1007.1%9.0%8012.0%32.1%Percentage (%)40 6076.9%58.2% 27.7%64.9%86.6%200CodeGen CodeGen ReasoningReActReasoning ReActExecutableMath/Logic ErrorOther ErrorsVariable/Object MisuseImport/Module Error</p>
<dl>
<dt>While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.The answer NA means that the paper does not include experiments.• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</dt>
<dd>[Yes]Justification: [NA]Guidelines:• The answer NA means that paper does not include experiments requiring code.• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• understand the results? Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: [NA] Guidelines: • Answer: [Yes] Justification: [NA] Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid-eration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] • 8. Experiments compute resources Justification: [NA]
6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to</dd>
</dl>
<p>• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.•We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.16.Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research?Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.• Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM)for what should or should not be described.CodeGen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26 G.2 ReAct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 G.3 CodeGen-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27 G.4 ReAct-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28 Example 1: MGMT Methylation and Hypermutation.The hypothesis states that MGMT promoter methylation is positively associated with a hypermutator phenotype in treated glioblastomas.The corresponding null hypothesis asserts no such association.
Justification: [NA] Guidelines: • Contents of Appendix A Example hypothesis and supporting evidence21B Captions of Biomedical Data Tables22C Extracting hypothesis and evidence from publications23D Categorization of biomedical publications24E Categorization of analysis tasks25F Categorization of code errors26G Agent prompts26G.1Answer: [NA]</p>
<p>-provided scientific hypothesis, you <strong>Must</strong> write {language} code to help the user evaluate the hypothesis.# IMPORTANT: CODE OUTPUT REQUIREMENTS You must import all the necessary libraries at the beginning of your code.You must use explicit print() statements for ALL outputs you want to see or analyze.Simply writing expressions like 'df.head()' will NOT show results in the execution log.Always use: -print(df.head())-print(analysis_result) -print(statistical_test_output) Every intermediate result and final output must be wrapped in a print() statement to be visible in the execution log.TASK Given the user-provided analysis plan for the user's scientific hypothesis, you <strong>Must</strong> write {language} code to fulfill the plan so that user can execute the code later to evaluate the hypothesis.# IMPORTANT: CODE OUTPUT REQUIREMENTS You must import all the necessary libraries at the beginning of your code.You must use explicit print() statements for ALL outputs you want to see or analyze.Simply writing expressions like 'df.head()' will NOT show results in the execution log.Always use: -print(df.head())-print(analysis_result) -print(statistical_test_output) Every intermediate result and final output must be wrapped in a print() statement to be visible in the execution log.
CODE_GENERATION_PROMPT_TEMPLATE = """# # DATASET PATHS{dataset_paths}## OuptutYour output should be in Markdown format and you should wrap the generated code in'''{language} ''' tags."""G.1 CodeGen# Prompts for CodeGen methods:"""# TASKGiven the user# DATASET PATHS{dataset_paths}# DATASET SCHEMA{dataset_schema}## OuptutYour output should be in Markdown format and you should wrap the generated code in'''{language} ''' tags."""
https://github.com/cbioportal/cbioportal/
https://www.ncbi.nlm.nih.gov/home/develop/api/
NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]Justification:[NA]Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes]Justification:[NA]Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory assumptions and proofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]A Example hypothesis and supporting evidence "result_variable": "correlation_coefficient", "result_variable_value": "r = 0.65, p &lt; 0.01" }, { "analysis_plan": "Compare mutation rates between glioblastomas with methylated and unmethylated MGMT promoters using a t-test.","evidence": "Tumors with methylated MGMT promoters exhibited significantly higher mutation rates than those without methylation (mean difference = 15 mutations/sample, p &lt; 0.05), supporting the association.", "analysis_variables": [ "MGMT_promoter_methylation_status", ""result_variable": "mean_mutation_rate_difference", "result_variable_value": "15 mutations/sample, p &lt; 0. "result_variable": "mutation_frequency", "result_variable_value": "25%" }, { "analysis_plan": "Rank gene mutation frequencies to determine whether PIK3R1 is among the most frequently mutated genes in glioblastoma.","evidence": "PIK3R1 mutation frequency ranked within the top 10% among all genes analyzed, suggesting it is frequently altered in glioblastoma.","analysis_variables": [ "gene_mutation_status" ],"result_variable": "relative_mutation_frequency", "result_variable_value": "Top 10% among all genes" } ] } ] } Supplementary Figure1: Examples of the hypothesis, counter-hypothesis, and supporting evidence extracted from biomedical publications.To illustrate the structure of entries in BIODSA-1K, we present two representative examples derived from glioblastoma studies.Each example consists of a hypothesis formulated from the original study's conclusions, a corresponding null hypothesis, and multiple supporting analyses that provide evidence for or against the claim."name": "data_clinical.txt","n_rows": 287, "n_columns": 12, "n_comment_rows": 4, "columns": [ { "name": "age_at_diagnosis", "data_type": "integer", "n_unique": 45, "missing_rate": 0.03, "statistics": { "min": 22, "0.2": 45, "0.8": 73, "max": 88, "statistics_type": "quantiles" } }, ... ] } ] }This structured captioning process enables interpretability, reusability, and intelligent query capabilities across diverse biomedical datasets in BIODSA-1K.C Extracting hypothesis and evidence from publicationsTo construct a benchmark of data-driven scientific claims, we developed a large-scale pipeline for extracting testable hypotheses and their supporting evidence from biomedical publications.Each extracted instance consists of a binary hypothesis derived from the abstract, a plausible counterhypothesis, and one or more structured evidence entries grounded in quantitative findings.We began with a curated metadata file from cBioPortal, which includes over 1,000 publications indexed by PubMed ID (PMID), their associated dataset identifiers, and accompanying titles, abstracts, and result summaries.After deduplication and filtering, we paired each publication with its corresponding abstract and dataset identifiers.For each entry, we concatenated the title and abstract to form a unified context and sent it to a large language model (GPT-4o) via a structured prompt.The prompt was designed to elicit hypotheses that are:• Binary and testable using statistical or machine learning methods,• Grounded in measurable outcomes, with clear references to statistical relationships or effect sizes,• Accompanied by structured supporting evidence, including analysis plans, involved variables, statistical measures, and result values.# The prompt: """The following is the abstract of a publication: {abstract} Task:Given the abstract of a publication, your task is to extract binary hypotheses and their supporting evidences that can be tested through data analysis.Requirements for hypotheses and evidences: 1.Each hypothesis must be testable using statistical analysis or machine learning methodsG.2 ReActG.3 CodeGen-Reasoning# CodeGen-Reasoning prompts ANALYSIS_PLAN_PROMPT_TEMPLATE = """ # TASK Generate an analysis plan to evaluate the user's scientific hypothesis using the datasets provided.The plan should consist of clear, actionable steps that can be <strong>easily converted to {language} code</strong> without needing any additional information.# REQUIREMENTS -Use only table and column names from the schema: do not invent or guess names.-Ensure every step is unambiguous and directly executable.-Use consistent naming for all variables (e.g., tables, columns) throughout the plan.-Be as concise as possible while maintaining full clarity and precision.# DATASET PATHS {dataset_paths}# DATASET SCHEMA {dataset_schema} # OUTPUT FORMAT Wrap the analysis plan in <analysis_plan> </analysis_plan> tags.So an example output would be ''' <analysis_plan> 1. load the dataset 2. print hello world </analysis_plan> """
The rise and potential of large language model based agents: A survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Science China Information Sciences. 6821211012025</p>
<p>Large language model based multi-agents: a survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>A foundation model for human-ai collaboration in medical literature mining. Zifeng Wang, Lang Cao, Qiao Jin, Joey Chan, Nicholas Wan, Behdad Afzali, Hyun-Jin Cho, Chang-In Choi, Mehdi Emamverdi, K Manjot, Gill, arXiv:2501.162552025arXiv preprint</p>
<p>Position: data-driven discovery with large generative models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Sanchaita Agarwal, Ashish Hazra, Peter Sabharwal, Clark, Forty-first International Conference on Machine Learning. 2024</p>
<p>Empowering biomedical discovery with ai agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, Cell. 187222024</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Healthcare data scientist qualifications, skills, and job focus: a content analysis of job postings. Melanie A Meyer, Journal of the American Medical Informatics Association. 2652019</p>
<p>Can large language models replace data scientists in biomedical research. Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, Jimeng Sun, arXiv:2410.215912024arXiv preprint</p>
<p>BioDiscoveryAgent: An ai agent for designing genetic perturbation experiments. Jian Yusuf H Roohani, Qian Vora, Percy Huang, Jure Liang, Leskovec, ICLR 2024 Workshop on Machine Learning for Genomics Explorations. 2024</p>
<p>BioCoder: a benchmark for bioinformatics code generation with large language models. Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark B Gerstein, Bioinformatics. 40Supplement_12024</p>
<p>Automated hypothesis validation with agentic sequential falsifications. Kexin Huang, Ying Jin, Ryan Li, Michael Y Li, Emmanuel Candès, Jure Leskovec, arXiv:2502.098582025arXiv preprint</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, The Thirteenth International Conference on Learning Representations. 2024</p>
<p>Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, arXiv:2410.05080Toward rigorous assessment of language agents for data-driven scientific discovery. 2024arXiv preprint</p>
<p>Integrative analysis of complex cancer genomics and clinical profiles using the cbioportal. Jianjiong Gao, Ugur Bülent Arman Aksoy, Gideon Dogrusoz, Benjamin Dresdner, S Onur Gross, Yichao Sumer, Anders Sun, Rileen Jacobsen, Erik Sinha, Ethan Larsson, Chris Cerami, Nikolaus Sander, Schultz, 10.1126/scisignal.2004088Science Signaling. 62692013</p>
<p>Open data commons open database license (odbl). v1.0</p>
<p>. Bert Vogelstein, Nickolas Papadopoulos, E Victor, Shibin Velculescu, Luis A Zhou, Kenneth W DiazJr, Kinzler, Cancer genome landscapes. Science. 33961272013</p>
<p>DS-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wentau Yih, Daniel Fried, Sida Wang, Tao Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Forty-first International Conference on Machine Learning. 2024</p>
<p>DSBench: How far are data science agents from becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>BLADE: Benchmarking language model agents for data-driven science. Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Scicode: A research coding benchmark curated by scientists. Minyang Tian, Luyu Gao, Shizhuo Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Advances in Neural Information Processing Systems. 202437</p>
<p>Large language models streamline automated machine learning for clinical studies. Tianyu Soroosh Tayebi Arasteh, Mahshad Han, Christiane Lotfinia, Jakob Nikolas Kuhl, Daniel Kather, Sven Truhn, Nebelung, Nature Communications. 15116032024</p>
<p>GPTScore: Evaluate as you desire. Jinlan Fu, See Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Code generation with alphacodium: From prompt engineering to flow engineering. Tal Ridnik, Dedy Kredo, Itamar Friedman, arXiv:2401.085002024arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Spider2-v: How far are multimodal agents from automating data science and engineering workflows?. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Wenjing Hu, Yuchen Mao, Advances in Neural Information Processing Systems. 202437</p>
<p>DS-Agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, International Conference on Machine Learning. PMLR2024</p>
<p>Large language models orchestrating structured reasoning achieve kaggle grandmaster level. Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath , Shahul Hameed, Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, arXiv:2411.035622024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5179 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5179</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5179</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-267301483</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.14423v4.pdf" target="_blank">P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS</a></p>
                <p><strong>Paper Abstract:</strong> Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5179.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5179.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection (LLM self-evaluation / iterative refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generate-then-reflect method where an LLM introspects its own output, identifies inaccuracies or inconsistencies against criteria, and produces revised outputs through one or more refinement passes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs) (e.g., GPT-family, Gemini) as discussed generically</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large pre-trained autoregressive/completion models; exact sizes and architectures are not specified in this survey paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflection / generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model first produces an initial response, then is prompted to critically review that response against predefined criteria (factuality, logical consistency, relevance) and iteratively refines the answer; the paper describes this as an introspective self-evaluation that may lead to revised outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General question answering, factuality/fact-checking, complex queries / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended QA and reasoning tasks where factual accuracy and logical coherence are important; the paper frames Reflection as applicable to complex queries requiring verification or correction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Qualitative claim: Reflection can produce revised responses that are more coherent and reliable by identifying and correcting errors discovered during self-review; the paper cites recent literature introducing the concept but presents no quantitative results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes risks: model self-evaluation can be inaccurate (risk of reinforcing its own errors), effectiveness depends on model training and ability to perform reflective tasks, and generated citations or checks during reflection can themselves be hallucinated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5179.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble of multiple reasoning traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based technique that samples multiple reasoning chains/answers from the model and selects the most consistent/resampled answer as more likely correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer LLMs used to generate multiple independent answers/reasoning traces via sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (multi-sample consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple independent outputs (reasoning traces/answers) for the same prompt, then evaluate agreement among them (via content overlap, semantic similarity metrics, BERT-score, n-gram overlap, or voting) and choose the consensus answer as more credible.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning-heavy QA / fact-checking / tasks requiring accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where multiple sampled outputs can be compared to estimate reliability (e.g., multi-step reasoning and factual question answering).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper states the assumption/empirical finding from literature that consistent replication across multiple sampled outputs increases likelihood of correctness; describes Self-Consistency as improving reliability via ensemble agreement but provides no new quantitative numbers in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increased compute cost from multiple generations; relies on diversity and calibration of samples; consensus can still converge on a shared hallucination; no numeric comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5179.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Multi-step Reasoning and Tool-use (ART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-engineering method that combines automated chain-of-thought prompting with external tool use and example retrieval to guide multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic multi-step reasoning and tool-use for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs augmented to call external tools/APIs and to use retrieved similar tasks as examples; paper does not specify model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ART (automated CoT + tool-use + example selection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given a task, ART finds similar tasks from a task library to use as examples (automating CoT example selection), and combines chain-of-thought style prompting with calls to external tools to carry out multi-step reasoning and computations.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-step tasks requiring reasoning and external data/tool access</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring both internal multi-step reasoning and interactions with external resources (calculators, search, Q&A tools) to complete the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Survey summarizes that ART enhances ability to handle complex tasks by integrating automated CoT with tool use and example retrieval; no numerical performance figures given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Complexity and orchestration overhead; requires a task library and tool integration; potential for hallucinated tool calls or incorrect use of tool outputs if not carefully validated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5179.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent method where an agent iteratively reflects on its actions/responses and uses verbal reinforcement learning signals to improve future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Language-agent LLMs (unspecified model family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based agents augmented with a mechanism for verbal self-feedback and iterative improvement; exact architecture/parameters not described in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (agent verbal RL + self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents produce actions/responses, generate verbal reflections about failures or successes, and apply reinforcement-style updates or heuristics to adjust future behavior; described in referenced work as a self-improvement loop for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language-agent tasks and sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent tasks where the agent must plan, act, observe outcomes, and refine behavior over episodes using verbal self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Survey lists Reflexion among works that enable self-improvement in agents; the survey itself does not provide quantitative results but indicates Reflexion demonstrates iterative agent improvement in its original work (referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Potential for flawed self-feedback leading to reinforcing mistakes; requires careful reward/feedback design; survey does not detail empirical failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5179.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selfcheckgpt (zero-resource black-box hallucination detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-resource method for detecting hallucinations from generative LLMs without requiring model internals, by running diagnostic checks on outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative LLMs (black-box setting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methods applicable to black-box LLMs (no access to internals), focused on detecting hallucinated content in generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-checking / diagnostic reflection (zero-resource)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Rather than iterative rewrite, SelfCheckGPT runs zero-resource checks on an LLM's outputs to detect potential hallucinations; it's a detection mechanism that can be used as part of a self-reflection pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Hallucination detection for generative outputs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detecting whether factual claims produced by LLMs are hallucinated or unsupported, without external labeled data or internal model access.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Referenced as a complementary approach for improving factuality by detecting hallucinations; the survey does not include the method's metrics but points to the referenced paper for quantitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As a detector, it may have false positives/negatives; survey does not enumerate specifics but implies detection alone does not repair errors without downstream correction steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5179.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Prompt Engineering (APE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Prompt Engineering (APE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where LLMs generate, score, and iteratively refine candidate prompts automatically to optimize prompt effectiveness for a task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models used both to generate and evaluate prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are used in a meta loop to propose prompt variants and to score/refine them; exact model details not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>APE (iterative prompt generate-score-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cycle: the LLM generates candidate prompts for a task, scores them (by clarity, specificity, or proxy performance), and iteratively refines prompts based on scores; a self-referential iterative improvement loop applied to prompts rather than final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt optimization for downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improving prompt quality to elicit better model outputs across tasks such as text generation, question answering, or chain-of-thought elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Survey states APE can produce higher-efficacy prompts and reduce manual prompt-engineering burden, but presents no quantitative comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computational cost and need for effective scoring metrics; may require seed prompts; risk of optimizing to spurious proxy measures without true task evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5179.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5179.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-correction example (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Use-the-AI-to-correct-itself example (ChatGPT self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical example where ChatGPT is instructed to create an article with incorrect facts and then asked to correct it, illustrating a basic generate-then-correct workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (example use-case in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT as used in illustrative examples (no architecture or size specified).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Manual self-correction (user-guided generate-then-correct)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>User instructs model to generate flawed content and subsequently asks the model to correct it (explicit user-led iterative correction rather than automated self-evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text editing / factual correction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Creating a deliberately flawed article and then asking the model to identify and correct inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper provides the example to illustrate that LLMs can be used to correct their outputs under user direction; no quantitative evidence is given.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Highly dependent on user prompt design; if the model's corrections rely on the same hallucinated knowledge, errors may persist; the paper flags ethical issues in using models to auto-correct sensitive content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Automatic multi-step reasoning and tool-use for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5179",
    "paper_id": "paper-267301483",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Reflection",
            "name_full": "Reflection (LLM self-evaluation / iterative refinement)",
            "brief_description": "A generate-then-reflect method where an LLM introspects its own output, identifies inaccuracies or inconsistencies against criteria, and produces revised outputs through one or more refinement passes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs) (e.g., GPT-family, Gemini) as discussed generically",
            "model_description": "Transformer-based large pre-trained autoregressive/completion models; exact sizes and architectures are not specified in this survey paper.",
            "reflection_method_name": "Reflection / generate-then-reflect",
            "reflection_method_description": "The model first produces an initial response, then is prompted to critically review that response against predefined criteria (factuality, logical consistency, relevance) and iteratively refines the answer; the paper describes this as an introspective self-evaluation that may lead to revised outputs.",
            "num_iterations": null,
            "task_name": "General question answering, factuality/fact-checking, complex queries / reasoning",
            "task_description": "Open-ended QA and reasoning tasks where factual accuracy and logical coherence are important; the paper frames Reflection as applicable to complex queries requiring verification or correction.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Qualitative claim: Reflection can produce revised responses that are more coherent and reliable by identifying and correcting errors discovered during self-review; the paper cites recent literature introducing the concept but presents no quantitative results itself.",
            "limitations_or_failure_cases": "Paper notes risks: model self-evaluation can be inaccurate (risk of reinforcing its own errors), effectiveness depends on model training and ability to perform reflective tasks, and generated citations or checks during reflection can themselves be hallucinated.",
            "uuid": "e5179.0",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble of multiple reasoning traces)",
            "brief_description": "An ensemble-based technique that samples multiple reasoning chains/answers from the model and selects the most consistent/resampled answer as more likely correct.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_description": "Transformer LLMs used to generate multiple independent answers/reasoning traces via sampling.",
            "reflection_method_name": "Self-Consistency (multi-sample consensus)",
            "reflection_method_description": "Generate multiple independent outputs (reasoning traces/answers) for the same prompt, then evaluate agreement among them (via content overlap, semantic similarity metrics, BERT-score, n-gram overlap, or voting) and choose the consensus answer as more credible.",
            "num_iterations": null,
            "task_name": "Reasoning-heavy QA / fact-checking / tasks requiring accuracy",
            "task_description": "Tasks where multiple sampled outputs can be compared to estimate reliability (e.g., multi-step reasoning and factual question answering).",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper states the assumption/empirical finding from literature that consistent replication across multiple sampled outputs increases likelihood of correctness; describes Self-Consistency as improving reliability via ensemble agreement but provides no new quantitative numbers in this survey.",
            "limitations_or_failure_cases": "Increased compute cost from multiple generations; relies on diversity and calibration of samples; consensus can still converge on a shared hallucination; no numeric comparisons provided in this paper.",
            "uuid": "e5179.1",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ART",
            "name_full": "Automatic Multi-step Reasoning and Tool-use (ART)",
            "brief_description": "A prompt-engineering method that combines automated chain-of-thought prompting with external tool use and example retrieval to guide multi-step tasks.",
            "citation_title": "Automatic multi-step reasoning and tool-use for large language models",
            "mention_or_use": "mention",
            "model_name": "Large language models (LLMs)",
            "model_description": "LLMs augmented to call external tools/APIs and to use retrieved similar tasks as examples; paper does not specify model sizes.",
            "reflection_method_name": "ART (automated CoT + tool-use + example selection)",
            "reflection_method_description": "Given a task, ART finds similar tasks from a task library to use as examples (automating CoT example selection), and combines chain-of-thought style prompting with calls to external tools to carry out multi-step reasoning and computations.",
            "num_iterations": null,
            "task_name": "Complex multi-step tasks requiring reasoning and external data/tool access",
            "task_description": "Tasks requiring both internal multi-step reasoning and interactions with external resources (calculators, search, Q&A tools) to complete the solution.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Survey summarizes that ART enhances ability to handle complex tasks by integrating automated CoT with tool use and example retrieval; no numerical performance figures given in this paper.",
            "limitations_or_failure_cases": "Complexity and orchestration overhead; requires a task library and tool integration; potential for hallucinated tool calls or incorrect use of tool outputs if not carefully validated.",
            "uuid": "e5179.2",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "A language-agent method where an agent iteratively reflects on its actions/responses and uses verbal reinforcement learning signals to improve future behavior.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "Language-agent LLMs (unspecified model family)",
            "model_description": "LLM-based agents augmented with a mechanism for verbal self-feedback and iterative improvement; exact architecture/parameters not described in the survey.",
            "reflection_method_name": "Reflexion (agent verbal RL + self-reflection)",
            "reflection_method_description": "Agents produce actions/responses, generate verbal reflections about failures or successes, and apply reinforcement-style updates or heuristics to adjust future behavior; described in referenced work as a self-improvement loop for agents.",
            "num_iterations": null,
            "task_name": "Language-agent tasks and sequential decision-making",
            "task_description": "Agent tasks where the agent must plan, act, observe outcomes, and refine behavior over episodes using verbal self-feedback.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Survey lists Reflexion among works that enable self-improvement in agents; the survey itself does not provide quantitative results but indicates Reflexion demonstrates iterative agent improvement in its original work (referenced).",
            "limitations_or_failure_cases": "Potential for flawed self-feedback leading to reinforcing mistakes; requires careful reward/feedback design; survey does not detail empirical failure modes.",
            "uuid": "e5179.3",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SelfCheckGPT",
            "name_full": "Selfcheckgpt (zero-resource black-box hallucination detection)",
            "brief_description": "A zero-resource method for detecting hallucinations from generative LLMs without requiring model internals, by running diagnostic checks on outputs.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "mention_or_use": "mention",
            "model_name": "Generative LLMs (black-box setting)",
            "model_description": "Methods applicable to black-box LLMs (no access to internals), focused on detecting hallucinated content in generated outputs.",
            "reflection_method_name": "Self-checking / diagnostic reflection (zero-resource)",
            "reflection_method_description": "Rather than iterative rewrite, SelfCheckGPT runs zero-resource checks on an LLM's outputs to detect potential hallucinations; it's a detection mechanism that can be used as part of a self-reflection pipeline.",
            "num_iterations": null,
            "task_name": "Hallucination detection for generative outputs",
            "task_description": "Detecting whether factual claims produced by LLMs are hallucinated or unsupported, without external labeled data or internal model access.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Referenced as a complementary approach for improving factuality by detecting hallucinations; the survey does not include the method's metrics but points to the referenced paper for quantitative evaluation.",
            "limitations_or_failure_cases": "As a detector, it may have false positives/negatives; survey does not enumerate specifics but implies detection alone does not repair errors without downstream correction steps.",
            "uuid": "e5179.4",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Automatic Prompt Engineering (APE)",
            "name_full": "Automatic Prompt Engineering (APE)",
            "brief_description": "An approach where LLMs generate, score, and iteratively refine candidate prompts automatically to optimize prompt effectiveness for a task.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large language models used both to generate and evaluate prompts",
            "model_description": "LLMs are used in a meta loop to propose prompt variants and to score/refine them; exact model details not provided in the survey.",
            "reflection_method_name": "APE (iterative prompt generate-score-refine)",
            "reflection_method_description": "Cycle: the LLM generates candidate prompts for a task, scores them (by clarity, specificity, or proxy performance), and iteratively refines prompts based on scores; a self-referential iterative improvement loop applied to prompts rather than final answers.",
            "num_iterations": null,
            "task_name": "Prompt optimization for downstream tasks",
            "task_description": "Improving prompt quality to elicit better model outputs across tasks such as text generation, question answering, or chain-of-thought elicitation.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Survey states APE can produce higher-efficacy prompts and reduce manual prompt-engineering burden, but presents no quantitative comparisons in this paper.",
            "limitations_or_failure_cases": "Computational cost and need for effective scoring metrics; may require seed prompts; risk of optimizing to spurious proxy measures without true task evaluation.",
            "uuid": "e5179.5",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Self-correction example (ChatGPT)",
            "name_full": "Use-the-AI-to-correct-itself example (ChatGPT self-correction)",
            "brief_description": "A practical example where ChatGPT is instructed to create an article with incorrect facts and then asked to correct it, illustrating a basic generate-then-correct workflow.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (example use-case in the paper)",
            "model_description": "ChatGPT as used in illustrative examples (no architecture or size specified).",
            "reflection_method_name": "Manual self-correction (user-guided generate-then-correct)",
            "reflection_method_description": "User instructs model to generate flawed content and subsequently asks the model to correct it (explicit user-led iterative correction rather than automated self-evaluation).",
            "num_iterations": null,
            "task_name": "Text editing / factual correction",
            "task_description": "Creating a deliberately flawed article and then asking the model to identify and correct inaccuracies.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper provides the example to illustrate that LLMs can be used to correct their outputs under user direction; no quantitative evidence is given.",
            "limitations_or_failure_cases": "Highly dependent on user prompt design; if the model's corrections rely on the same hallucinated knowledge, errors may persist; the paper flags ethical issues in using models to auto-correct sensitive content.",
            "uuid": "e5179.6",
            "source_info": {
                "paper_title": "P ROMPT D ESIGN AND E NGINEERING : I NTRODUCTION AND A DVANCED M ETHODS",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Automatic multi-step reasoning and tool-use for large language models",
            "rating": 2,
            "sanitized_title": "automatic_multistep_reasoning_and_tooluse_for_large_language_models"
        }
    ],
    "cost": 0.01245075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROMPT DESIGN AND ENGINEERING: INTRODUCTION AND ADVANCED METHODS
May 7, 2024</p>
<p>Xavier Amatriain xavier@amatriain.net 
PROMPT DESIGN AND ENGINEERING: INTRODUCTION AND ADVANCED METHODS
May 7, 2024D574A1509643059CB3439D7F3EC3A823
Prompt design and engineering has rapidly become essential for maximizing the potential of large language models.In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents.Finally, we provide a survey of tools for prompt engineers.</p>
<p>Introduction 1.What is a prompt?</p>
<p>A prompt in generative AI models is the textual input provided by users to guide the model's output.This could range from simple questions to detailed descriptions or specific tasks.In the context of image generation models like DALLE-3, prompts are often descriptive, while in LLMs like GPT-4 or Gemini, they can vary from simple queries to complex problem statements.</p>
<p>Prompts generally consist of instructions, questions, input data, and examples.In practice, to elicit a desired response from an AI model, a prompt must contain either instructions or questions, with other elements being optional.</p>
<p>Basic prompts in LLMs can be as simple as asking a direct question or providing instructions for a specific task.Advanced prompts involve more complex structures, such as "chain of thought" prompting, where the model is guided to follow a logical reasoning process to arrive at an answer.</p>
<p>Basic prompt examples</p>
<p>As mentioned above, a prompt is constructed by combining instructions, questions, input data, and examples.In order to obtain a result, either 1 or 2 must be present.Everything else is optional.Let's see a few examples (all of them using ChatGPT-4).</p>
<p>Instructions + Question</p>
<p>Beyond asking a simple question, possibly the next level of sophistication in a prompt is to include some instructions on how the model should answer the question.Here I ask for advice on how to write a college essay, but also include instructions on the different aspects I am interested to hear about in the answer.</p>
<p>Instructions + Input</p>
<p>Continuing with the previous example, it does not escape anyone that if you can ask a LLM for advice on how to write an essay, you can also directly ask it to write the essay itself. 1   Let's see what happens when we input some data about me and give some instructions: "Given the following information about me, write a 4 paragraph college essay: I am originally from Barcelona, Spain.While my childhood had different traumatic events, such as the death of my father when I was only 6, I still think I had quite a happy childhood..During my childhood, I changed schools very often, and attended all kinds of schools, from public schools to very religious private ones.One of the most "exotic" things I did during those years is to spend a full school year studying 6th grade in Twin Falls, Idaho, with my extended family.</p>
<p>I started working very early on.My first job, as an English teacher, was at age 13.After that, and throughout my studies, I worked as a teacher, waiter, and even construction worker."See output in Figure 2 1 Note that I am not advocating for this to be an ethical use of these tools, but it is important to be aware that this possibility exists and is already being used by students around the world.It is beyond the scope of this introductory guide to discuss all the possible ethical, legal, or moral concerns that LLMs or generative AI as a whole introduces, but I thought it would be important to at least call out in an introductory example.The fact that you CAN do something with a generative model does not mean that it is the right thing to do!On the other hand, if you are on the receiving end, you better prepare yourself and your organization for all kinds of AI-generated content to come your way.Fortunately, for situations like the one outlined in this example, there are already efforts underway to detect AI generated content.</p>
<p>Question + Examples</p>
<p>You can also feed examples into a language model.In the example below I include some of the shows I like and don't like to build a "cheap" recommender system.Note that while I added only a few shows, the length of this list is only limited by whatever token limit we might have in the LLM interface.</p>
<p>"Here are some examples of TV shows I really like: Breaking Bad, Peaky Blinders, The Bear.I did not like Ted Lasso.What other shows do you think I might like?"See output in Figure 3 1</p>
<p>.3 Prompt Engineering</p>
<p>Prompt engineering in generative AI models is a rapidly emerging discipline that shapes the interactions and outputs of these models.At its core, a prompt is the textual interface through which users communicate their desires to the model, be it a description for image generation in models like DALLE-3 or Midjourney, or a complex problem statement in Large Language Models (LLMs) like GPT-4 and Gemini.The prompt can range from simple questions to intricate tasks, encompassing instructions, questions, input data, and examples to guide the AI's response.</p>
<p>The essence of prompt engineering lies in crafting the optimal prompt to achieve a specific goal with a generative model.This process is not only about instructing the model but also involves a deep understanding of the model's capabilities and limitations, and the context within which it operates.In image generation models, for instance, a prompt might be a detailed description of the desired image, while in LLMs, it could be a complex query embedding various types of data.</p>
<p>Prompt engineering transcends the mere construction of prompts; it requires a blend of domain knowledge, understanding of the AI model, and a methodical approach to tailor prompts for different contexts.This might involve Furthermore, prompt engineering is an iterative and exploratory process, akin to traditional software engineering practices such as version control and regression testing.The rapid growth of this field suggests its potential to revolutionize certain aspects of machine learning, moving beyond traditional methods like feature or architecture engineering, especially in the context of large neural networks.On the other hand, traditional engineering practices such as version control and regression testing need to be adapted to this new paradigm just like they were adapted to other machine learning approaches [1].This paper aims to delve into this burgeoning field, exploring both its foundational aspects and its advanced applications.We will focus on the applications of prompt engineering to LLM.However, most techniques can find applications in multimodal generative AI models too.</p>
<p>LLMs and Their Limitations</p>
<p>Large Language Models (LLMs), including those based on the Transformer architecture [2], have become pivotal in advancing natural language processing.These models, pre-trained on vast datasets to predict subsequent tokens, exhibit remarkable linguistic capabilities.However, despite their sophistication, LLMs are constrained by inherent limitations that affect their application and effectiveness. Resource Intensity: The substantial size of LLMs translates to significant computational and financial costs, impacting scalability and accessibility.</p>
<p> Domain Specificity: While inherently generalist, LLMs often require domain-specific data to excel in specialized tasks.</p>
<p>These limitations underscore the need for advanced prompt engineering and specialized techniques to enhance LLM utility and mitigate inherent constraints.Subsequent sections delve into sophisticated strategies and engineering innovations aimed at optimizing LLM performance within these bounds.</p>
<p>3 More advanced prompt design tips and tricks</p>
<p>Chain of thought prompting</p>
<p>In chain of thought prompting, we explicitly encourage the model to be factual/correct by forcing it to follow a series of steps in its "reasoning".</p>
<p>In the examples in figures 4 and 5, we use prompts of the form:</p>
<p>"Original question?</p>
<p>Use this format: Q: <repeat_question> A: Let's think step by step.<give_reasoning> Therefore, the answer is <final_answer>."</p>
<p>Encouraging the model to be factual through other means</p>
<p>One of the most important problems with generative models is that they are likely to hallucinate knowledge that is not factual or is wrong.You can improve factuality by having the model follow a set of reasoning steps as we saw in the previous subsection.And, you can also point the model in the right direction by prompting it to cite the right sources.</p>
<p>(Note that we will later see that this approach has severe limitations since the citations themselves could be hallucinated or made up).</p>
<p>"Are mRNA vaccines safe?Answer only using reliable sources and cite those sources."</p>
<p>See results in figure 6. "Write a poem describing a beautify day &lt;|endofprompt|&gt;.It was a beautiful winter day"</p>
<p>Note in the result in figure 7 how the paragraph continues from the last sentence in the "prompt".</p>
<p>Being forceful</p>
<p>Language models do not always react well to nice, friendly language.If you REALLY want them to follow some instructions, you might want to use forceful language.Believe it or not, all caps and exclamation marks work! See example in figure 8 3.5 Use the AI to correct itself</p>
<p>In example in figure 9 we get ChatGPT to create a "questionable" article.We then ask the model to correct it in 10.</p>
<p>"Write a short article about how to find a job in tech.Include factually incorrect information."</p>
<p>Generate different opinions</p>
<p>LLMs do not have a strong sense of what is true or false, but they are pretty good at generating different opinions.This can be a great tool when brainstorming and understanding different possible points of views on a topic.We will see how this can be used in our favor in different ways by applying more advanced Prompt Engineering techniques in the next section.In the following example, we feed an article found online and ask ChatGPT to disagree with it.Note the use of tags <begin> and <end> to guide the model.The result of this input can be seen in figure 11.</p>
<p>The t e x t b e t w e e n &lt; b e g i n &gt; and <end > i s an e x a m p l e a r t i c l e .</p>
<p>&lt; b e g i n &gt;</p>
<p>From p e r s o n a l a s s i s t a n t s and recommender s y s t e m s t o s e l f  d r i v i n g c a r s and n a t u r a l l a n g u a g e p r o c e s s i n g , m a c h i n e l e a r n i n g a p p l i c a t</p>
<p>i o n s h a v e d e m o n s t r a t e d r e m a r k a b l e c a p a b i l i t i e s t o e n h a n c e human d e c i s i o n making , p r o d u c t i v i t y and c r e a t i v i t y i n t h e l a s t d e c a d e . However , m a c h i n e l e a r n i n g i s s t i l l f a r from r e a c h i n g i t s f u l l p o t e n t i a l , and f a c e s a number o f c h a l l e n g e s when i t comes t o a l g o r i t h m i c d e s i g n and i m p l e m e n t a t i o n . As t h e t e c h n o l o g y c o n t i n u e s t o a d v a n c e and improve , h e r e a r e some o f t h e most e x c i t i n g d e v e l o p m e n t s t h a t c o u l d o c c u r i n t h e n e x t d e c a d e .</p>
<p>1 .D a t a i n t e g r a t i o n : One o f t h e key d e v e l o p m e n t s t h a t i s a n t i c i p a t e d i n m a c h i n e l e a r n i n g i s t h e i n t e g r a t i o n o f m u l t i p l e m o d a l i t i e s and d o m a i n s o f d a t a , s u c h a s images , t e x t and s e n s o r d a t a t o c r e a t e r i c h e r and more r o b u s t r e p r e s e n t a t i o n s o f complex phenomena .F o r example , i m a g i n e a m a c h i n e l e a r n i n g s y s t e m t h a t c a n n o t o n l y r e c o g n i z e f a c e s , b u t a l s o i n f e r t h e i r e m o t i o n s , i n t e n t i o n s and p e r s o n a l i t i e s from t h e i r f a c i a l e x p r e s s i o n s</p>
<p>Such a s y s t e m c o u l d h a v e immense a p p l i c a t i o n s i n f i e l d s l i k e c u s t o m e r s e r v i c e , e d u c a t i o n and s e c u r i t y . To a c h i e v e t h i s l e v e l o f m u l t i m o d a l and c r o s s  domain u n d e r s t a n d i n g , m a c h i n e l e a r n i n g m o d e l s w i l l n e e d t o l e v e r a g e a d v a n c e s i n d e e p l e a r n i n g , r e p r e s e n t a t i o n l e a r n i n g and s e l f  s u p e r v i s e d l e a r n i n g , a s w e l l a s i n c o r p o r a t e domain</p>
<p>e w i t h p a t i e n t s , e m p a t h i z e w i t h t h e i r c o n c e r n s and p r o v i d e p e r s o n a l i z e d a d v i c e . S y s t e m s l i k e t h e s e c o u l d e n h a n c e t h e q u a l i t y and e f f i c i e n c y o f h e a l t h c a r e , a s w e l l a s i m p r o v e t h e w e l l  b e i n g and s a t i s f a c t i o n o f p a t i e n t s and p r o v i d e r s <end ></p>
<p>Given t h a t e x a m p l e a r t i c l e , w r i t e a s i m i l a r a r t i c l e t h a t d i s a g r e e s w i t h i t .</p>
<p>Keeping state + role playing</p>
<p>Language models themselves don't keep track of state.However, applications such as ChatGPT implement the notion of "session" where the chatbot keeps track of state from one prompt to the next.This enables much more complex conversations to take place.Note that when using API calls this would involved keeping track of state on the application side.</p>
<p>In the example in 12, we make ChatGPT discuss worst-case time complexity of the bubble sort algorithm as if it were a rude Brooklyn taxi driver.</p>
<p>Teaching an algorithm in the prompt</p>
<p>One of the most useful abilities of LLMs is the fact that they can learn from what they are being fed in the prompt.This is the so-called zero-shot learning ability.The following example is taken from the appendix in "Teaching Algorithmic Reasoning via In-context Learning" [4] where the definition of parity of a list is fed in an example.It is worth keeping in mind that LLMs like GPT only read forward and are in fact completing text.This means that it is worth it to prompt them in the right order.It has been found that giving the instruction before the example helps.Furthermore, even the order the examples are given makes a difference (see Lu et.al [5]).Keep that in mind and experiment with different orders of prompt and examples.</p>
<p>Affordances</p>
<p>Affordances are functions that are defined in the prompt and the model is explicitly instructed to use when responding.E.g. you can tell the model that whenever finding a mathematical expression it should call an explicit CALC() function and compute the numerical result before proceeding.It has been shown that using affordances can help in some cases.</p>
<p>Advanced Techniques in Prompt Engineering</p>
<p>In the previous section we introduced more complex examples of how to think about prompt design.However, those tips and tricks have more recently evolved into more tested and documented techniques that bring more "engineering"  and less art to how to build a prompt.In this section we cover some of those advanced techniques that build upon what we discussed so far.</p>
<p>Chain of Thought (CoT)</p>
<p>Building on the foundational concepts introduced earlier, the Chain of Thought (CoT) technique, as delineated in "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" by Google researchers [6], marks a significant leap in harnessing the reasoning capabilities of Large Language Models (LLMs).This technique capitalizes on the premise that, while LLMs excel at predicting sequences of tokens, their design does not inherently facilitate explicit reasoning processes.</p>
<p>CoT transforms the often implicit reasoning steps of LLMs into an explicit, guided sequence, thereby enhancing the model's ability to produce outputs grounded in logical deduction, particularly in complex problem-solving contexts.</p>
<p>The methodology manifests predominantly in two variants:</p>
<ol>
<li>Zero-Shot CoT: This approach prompts the LLM to unravel the problem iteratively, encouraging a step-by-step elucidation of its reasoning process.Although Manual CoT often outperforms its Zero-shot counterpart, its effectiveness hinges on the diversity and relevance of the provided examples.The labor-intensive and potentially error-prone process of crafting these examples paves the way for the exploration of Automatic CoT [7], which seeks to streamline and optimize the example generation process, thereby expanding the applicability and efficiency of CoT prompting in LLMs.</li>
</ol>
<p>Tree of Thought (ToT)</p>
<p>The Tree of Thought (ToT) prompting technique, as introduced in recent advancements [8], marks a significant evolution in the domain of Large Language Models (LLMs).Drawing inspiration from human cognitive processes, ToT facilitates a multi-faceted exploration of problem-solving pathways, akin to considering a spectrum of possible solutions before deducing the most plausible one.Consider a travel planning context: an LLM might branch out into flight options, train routes, and car rental scenarios, weighing the cost and feasibility of each, before suggesting the most optimal plan to the user.</p>
<p>Central to the ToT approach is the concept of "thought trees," where each branch embodies an alternative reasoning trajectory.This multiplicity allows the LLM to traverse through diverse hypotheses, mirroring the human approach to problem-solving by weighing various scenarios before reaching a consensus on the most likely outcome.</p>
<p>A pivotal component of ToT is the systematic evaluation of these reasoning branches.As the LLM unfolds different threads of thought, it concurrently assesses each for its logical consistency and pertinence to the task at hand.This dynamic analysis culminates in the selection of the most coherent and substantiated line of reasoning, thereby enhancing the decision-making prowess of the model.</p>
<p>ToT's capability to navigate through complex and multifaceted problem spaces renders it particularly beneficial in scenarios where singular lines of reasoning fall short.By emulating a more human-like deliberation process, ToT significantly amplifies the model's proficiency in tackling tasks imbued with ambiguity and intricacy.In the realm of advanced prompt engineering, the integration of Tools, Connectors, and Skills significantly enhances the capabilities of Large Language Models (LLMs).These elements enable LLMs to interact with external data sources and perform specific tasks beyond their inherent capabilities, greatly expanding their functionality and application scope.</p>
<p>Tools in this context are external functions or services that LLMs can utilize.These tools extend the range of tasks an LLM can perform, from basic information retrieval to complex interactions with external databases or APIs.</p>
<p>Connectors act as interfaces between LLMs and external tools or services.They manage data exchange and communication, enabling effective utilization of external resources.The complexity of connectors can vary, accommodating a wide range of external interactions.</p>
<p>Skills refer to specialized functions that an LLM can execute.These encapsulated capabilities, such as text summarization or language translation, enhance the LLM's ability to process and respond to prompts, even without direct access to external tools.</p>
<p>In the paper "Toolformer: Language Models Can Teach Themselves to Use Tools" [9], the authors go beyond simple tool usage by training an LLM to decide what tool to use when, and even what parameters the API needs.Tools include two different search engines, or a calculator.In the following examples, the LLM decides to call an external Q&amp;A tool, a calculator, and a Wikipedia Search Engine More recently, researchers at Berkeley have trained a new LLM called Gorilla [10] that beats GPT-4 at the use of APIs, a specific but quite general tool.</p>
<p>Automatic Multi-step Reasoning and Tool-use (ART)</p>
<p>Automatic Multi-step Reasoning and Tool-use (ART) [11] is a prompt engineering technique that combines automated chain of thought prompting with the use of external tools.ART represents a convergence of multiple prompt engineering strategies, enhancing the ability of Large Language Models (LLMs) to handle complex tasks that require both reasoning and interaction with external data sources or tools.</p>
<p>ART involves a systematic approach where, given a task and input, the system first identifies similar tasks from a task library.These tasks are then used as examples in the prompt, guiding the LLM on how to approach and execute the current task.This method is particularly effective when tasks require a combination of internal reasoning and external data processing or retrieval.</p>
<p>Enhancing Reliability through Self-Consistency</p>
<p>In the quest for accuracy and reliability in Large Language Model (LLM) outputs, the Self-Consistency approach emerges as a pivotal technique.This method, underpinned by ensemble-based strategies, involves prompting the LLM to produce multiple answers to the same question, with the coherence among these responses serving as a gauge for their credibility.</p>
<p>The essence of Self-Consistency lies in the assumption that the generation of similar responses to a singular prompt by an LLM increases the likelihood of those responses' accuracy (see Figure 18).The implementation of this approach entails the LLM addressing a query multiple times, with each response undergoing scrutiny for consistency.The evaluation of consistency can be conducted through various lenses, including but not limited to, content overlap, semantic similarity assessments, and advanced metrics like BERT-scores or n-gram overlaps, offering a multifaceted view of response agreement.This enhances the reliability of LLMs in fact-checking tools, helping ensure only the most consistent and verifiable claims are presented to the user.</p>
<p>The utility of Self-Consistency spans numerous domains where factual precision is imperative.It holds particular promise in applications such as fact-checking and information verification, where the integrity of AI-generated content is paramount.By leveraging this technique, developers and users can significantly bolster the dependability of LLMs, ensuring their outputs are not only coherent but also factually sound, thereby enhancing their applicability in critical and information-sensitive tasks.</p>
<p>Reflection</p>
<p>The concept of Reflection, as introduced in recent literature [13], marks a significant stride towards endowing Large Language Models (LLMs) with the capability for self-improvement.Central to Reflection is the LLM's engagement in an introspective review of its outputs, a process akin to human self-editing, where the model assesses its initial responses for factual accuracy, logical consistency, and overall relevance.</p>
<p>This reflective process entails a structured self-evaluation where the LLM, following the generation of an initial response, is prompted to scrutinize its output critically.Through this introspection, the model identifies potential inaccuracies or inconsistencies, paving the way for the generation of revised responses that are more coherent and reliable.</p>
<p>For instance, an LLM might initially provide a response to a complex query.It is then prompted to evaluate this response against a set of predefined criteria, such as the verifiability of facts presented or the logical flow of arguments made.Should discrepancies or areas for enhancement be identified, the model embarks on an iterative process of refinement, potentially yielding a series of progressively improved outputs.</p>
<p>However, the implementation of Reflection is not without challenges.The accuracy of self-evaluation is contingent upon the LLM's inherent understanding and its training on reflective tasks.Moreover, there exists the risk of the model reinforcing its own errors if it incorrectly assesses the quality of its responses.</p>
<p>Figure 18: Illustrative diagram of the Self-Consistency approach, demonstrating the process of generating and evaluating multiple responses to ensure accuracy, adapted from [12].This representation underscores the iterative nature of response generation and the subsequent analysis for consistency.</p>
<p>Despite these challenges, the implications of Reflection for the development of LLMs are profound.By integrating self-evaluation and revision capabilities, LLMs can achieve greater autonomy in improving the quality of their outputs, making them more versatile and dependable tools in applications where precision and reliability are paramount.</p>
<p>Expert Prompting</p>
<p>Expert Prompting, as delineated in contemporary research [14], represents a novel paradigm in augmenting the utility of Large Language Models (LLMs) by endowing them with the capability to simulate expert-level responses across diverse domains.This method capitalizes on the LLM's capacity to generate informed and nuanced answers by prompting it to embody the persona of experts in relevant fields.</p>
<p>A cornerstone of this approach is the multi-expert strategy, wherein the LLM is guided to consider and integrate insights from various expert perspectives.This not only enriches the depth and breadth of the response but also fosters a multidimensional understanding of complex issues, mirroring the collaborative deliberations among real-world experts.</p>
<p>For instance, when addressing a medical inquiry, the LLM might be prompted to channel the insights of a clinician, a medical researcher, and a public health expert.These diverse perspectives are then adeptly woven together, leveraging sophisticated algorithms, to produce a response that encapsulates a comprehensive grasp of the query.</p>
<p>This synthesis of expert viewpoints not only augments the factual accuracy and depth of the LLM's outputs but also mitigates the biases inherent in a singular perspective, presenting a balanced and well-considered response.</p>
<p>Figure 19: Illustration of the PromptChainer interface, showcasing a visual representation of Chains and their components, as adapted from [15].This interface exemplifies the modular nature of Chains, where each block signifies a step in the workflow, contributing to the overall task resolution.</p>
<p>However, Expert Prompting is not devoid of challenges.Simulating the depth of real expert knowledge necessitates advanced prompt engineering and a nuanced understanding of the domains in question.Furthermore, the reconciliation of potentially divergent expert opinions into a coherent response poses an additional layer of complexity.</p>
<p>Despite these challenges, the potential applications of Expert Prompting are vast, spanning from intricate technical advice in engineering and science to nuanced analyses in legal and ethical deliberations.This approach heralds a significant advancement in the capabilities of LLMs, pushing the boundaries of their applicability and reliability in tasks demanding expert-level knowledge and reasoning.</p>
<p>Streamlining Complex Tasks with Chains</p>
<p>Chains represent a transformative approach in leveraging Large Language Models (LLMs) for complex, multi-step tasks.This method, characterized by its sequential linkage of distinct components, each designed to perform a specialized function, facilitates the decomposition of intricate tasks into manageable segments.The essence of Chains lies in their ability to construct a cohesive workflow, where the output of one component seamlessly transitions into the input of the subsequent one, thereby enabling a sophisticated end-to-end processing capability.</p>
<p>In the realm of Chains, components might range from simple information retrieval modules to more complex reasoning or decision-making blocks.For instance, a Chain for a medical diagnosis task might begin with symptom collection, followed by differential diagnosis generation, and conclude with treatment recommendation.</p>
<p>The development and optimization of Chains, as explored in "PromptChainer: Chaining Large Language Model Prompts through Visual Programming" [15], present both challenges and innovative solutions.One significant challenge lies in the orchestration of these components to ensure fluidity and coherence in the workflow.PromptChainer (see figure 19) addresses this by offering a visual programming environment, enabling users to intuitively design and adjust Chains, thus mitigating complexities associated with traditional coding methods.</p>
<p>The application of Chains extends across various domains, from automated customer support systems, where Chains guide the interaction from initial query to resolution, to research, where they can streamline the literature review process.</p>
<p>While Chains offer a robust framework for tackling multifaceted tasks, potential limitations, such as the computational overhead associated with running multiple LLM components and the necessity for meticulous design to ensure the integrity of the workflow, warrant consideration.</p>
<p>Nonetheless, the strategic implementation of Chains, supported by tools like PromptChainer, heralds a new era of efficiency and capability in the use of LLMs, enabling them to address tasks of unprecedented complexity and scope.</p>
<p>Figure 20: Visualization of the Rails framework, illustrating the mechanism through which predefined guidelines shape and constrain LLM outputs, as exemplified in the Nemo Guardrails framework.This schematic representation highlights the different types of Rails and their roles in maintaining the quality and integrity of LLM responses.</p>
<p>Guiding LLM Outputs with Rails</p>
<p>Rails in advanced prompt engineering represent a strategic approach to directing the outputs of Large Language Models (LLMs) within predefined boundaries, ensuring their relevance, safety, and factual integrity.This method employs a structured set of rules or templates, commonly referred to as Canonical Forms, which serve as a scaffold for the model's responses, ensuring they conform to specific standards or criteria.</p>
<p>Canonical Forms within the Rails framework act as modeling languages or templates that standardize the structure and delivery of natural language sentences, guiding the LLM in generating outputs that align with desired parameters (see figure 20).These are akin to standardized structures for language, guiding the LLM to conform to certain response patterns.The design and implementation of Rails can vary widely, tailored to the specific requirements of the application:</p>
<p> Topical Rails: Designed to keep the LLM focused on a specified subject or domain, preventing digression or the inclusion of irrelevant information. Fact-Checking Rails: Aim to reduce the propagation of inaccuracies by guiding the LLM towards evidencebased responses and discouraging speculative or unverified claims. Jailbreaking Rails: Established to deter the LLM from producing outputs that circumvent its operational constraints or ethical guidelines, safeguarding against misuse or harmful content generation.</p>
<p>In practice, Rails might be applied in various scenarios, from educational tools where Topical Rails ensure content relevance, to news aggregation services where Fact-Checking Rails uphold informational integrity.Jailbreaking Rails are crucial in interactive applications to prevent the model from engaging in undesirable behaviors.</p>
<p>While Rails offer a robust mechanism for enhancing the quality and appropriateness of LLM outputs, they also present challenges, such as the need for meticulous rule definition and the potential stifling of the model's creative capabilities.Balancing these considerations is essential for leveraging Rails effectively, ensuring that LLMs deliver high-quality, reliable, and ethically sound responses.</p>
<p>Streamlining Prompt Design with Automatic Prompt Engineering</p>
<p>Automatic Prompt Engineering (APE) [16] automates the intricate process of prompt creation.By harnessing the LLMs' own capabilities for generating, evaluating, and refining prompts, APE aims to optimize the prompt design process, ensuring higher efficacy and relevance in eliciting desired responses.</p>
<p>The APE methodology (see figure 21) unfolds through a series of distinct yet interconnected steps:</p>
<p> Prompt Generation: Initially, the LLM produces a variety of prompts tailored to a specific task, leveraging its vast linguistic database and contextual understanding.</p>
<p>Figure 21: Illustration of the APE process, showcasing the cyclic nature of prompt generation, evaluation, and refinement, as conceptualized in [16].This diagram highlights the self-referential mechanism through which LLMs iteratively improve the quality of prompts, aligning them more closely with the intended task objectives.</p>
<p> Prompt Scoring: Subsequently, these prompts undergo a rigorous evaluation phase, where they are scored against key metrics such as clarity, specificity, and their potential to drive the desired outcome, ensuring that only the most effective prompts are selected for refinement.</p>
<p> Refinement and Iteration: The refinement process involves tweaking and adjusting prompts based on their scores, with the aim of enhancing their alignment with the task requirements.This iterative process fosters continuous improvement in prompt quality.</p>
<p>By automating the prompt engineering process, APE not only alleviates the burden of manual prompt creation but also introduces a level of precision and adaptability previously unattainable.The ability to generate and iteratively refine prompts can significantly enhance the utility of LLMs across a spectrum of applications, from automated content generation to sophisticated conversational agents.</p>
<p>However, the deployment of APE is not without challenges.The need for substantial computational resources and the complexity of establishing effective scoring metrics are notable considerations.Moreover, the initial set-up may require a carefully curated set of seed prompts to guide the generation process effectively.</p>
<p>Despite these challenges, APE represents a significant leap forward in prompt engineering, offering a scalable and efficient solution to unlock the full potential of LLMs in diverse applications, thereby paving the way for more nuanced and contextually relevant interactions.RAG operates by formulating queries from input prompts and leveraging these queries to fetch pertinent information from diverse sources, such as search engines (see figure 22) or knowledge graphs(see figure 23).This retrieved content is seamlessly integrated into the LLM's workflow, significantly augmenting its ability to generate informed and contextually relevant responses.</p>
<p>RAG-aware Prompting Techniques</p>
<p>The advent of RAG has spurred the development of sophisticated prompting techniques designed to leverage its capabilities fully.Among these, Forward-looking Active Retrieval Augmented Generation (FLARE) stands out for its innovative approach to enhancing LLM performance.</p>
<p>FLARE iteratively enhances LLM outputs by predicting potential content and using these predictions to guide information retrieval.Unlike traditional RAG models, which typically perform a single retrieval step before generation, FLARE engages in a continuous, dynamic retrieval process, ensuring that each segment of the generated content is supported by the most relevant external information.This process is characterized by an evaluation of confidence levels for each generated segment.When the confidence falls below a predefined threshold, FLARE prompts the LLM to use the content as a query for additional information retrieval, thereby refining the response with updated or more relevant data.</p>
<p>For a comprehensive understanding of RAG, FLARE, and related methodologies, readers are encouraged to consult the survey on retrieval augmented generation models, which provides an in-depth analysis of their evolution, applications, and impact on the field of LLMs [19].</p>
<p>LLM Agents</p>
<p>The concept of AI agents, autonomous entities that perceive, decide, and act within their environments, has evolved significantly with the advent of Large Language Models (LLMs).LLM-based agents represent a specialized instantiation of augmented LLMs, designed to perform complex tasks autonomously, often surpassing simple response generation by incorporating decision-making and tool utilization capabilities.</p>
<p>LLM agents can access external tools and services, leveraging them to complete tasks, and making informed decisions based on contextual input and predefined goals.Such agents can, for instance, interact with APIs to fetch weather information or execute purchases, thereby acting on the external world as well as interpreting it.</p>
<p>Prompt Engineering Techniques for Agents</p>
<p>The integration of LLMs into agent frameworks has led to the development of novel prompt engineering techniques, including Reasoning without Observation (ReWOO), Reason and Act (ReAct), and Dialog-Enabled Resolving Agents (DERA), each tailored to enhance the autonomous functionality of LLM-based agents.</p>
<p>Reasoning without Observation (ReWOO)</p>
<p>ReWOO enables LLMs to construct reasoning plans without immediate access to external data, relying instead on a structured reasoning framework that can be executed once relevant data becomes available (see figure 25).This approach is particularly useful in scenarios where data retrieval is costly or uncertain, allowing LLMs to maintain efficiency and reliability.ReAct (see figure 26) enhances LLMs' problem-solving capabilities by interleaving reasoning traces with actionable steps, facilitating a dynamic approach to task resolution where reasoning and action are closely integrated.</p>
<p>Dialog-Enabled Resolving Agents (DERA)</p>
<p>DERA (see figure 27) introduces a collaborative agent framework where multiple agents, each with specific roles, engage in dialogue to resolve queries and make decisions.This multi-agent approach enables handling complex queries with depth and nuance, closely mirroring human decision-making processes.</p>
<p>The development of LLM-based agents and associated prompt engineering techniques represents a significant leap forward in AI, promising to enhance the autonomy, decision-making, and interactive capabilities of LLMs across a wide range of applications.</p>
<p>Prompt Engineering Tools and Frameworks</p>
<p>The proliferation of advanced prompt engineering techniques has catalyzed the development of an array of tools and frameworks, each designed to streamline the implementation and enhance the capabilities of these methodologies.These resources are pivotal in bridging the gap between theoretical approaches and practical applications, enabling researchers and practitioners to leverage prompt engineering more effectively.Langchain has emerged as a cornerstone in the prompt engineering toolkit landscape, initially focusing on Chains but expanding to support a broader range of functionalities including Agents and web browsing capabilities.Its comprehensive suite of features makes it an invaluable resource for developing complex LLM applications.</p>
<p>Semantic Kernel, by Microsoft, offers a robust toolkit for skill development and planning, extending its utility to include chaining, indexing, and memory access.Its versatility in supporting multiple programming languages enhances its appeal to a wide user base.</p>
<p>The Guidance library, also from Microsoft, introduces a modern templating language tailored for prompt engineering, offering solutions that are aligned with the latest advancements in the field.Its focus on modern techniques makes it a go-to resource for cutting-edge prompt engineering applications.</p>
<p>Nemo Guardrails by NVidia is specifically designed to construct Rails, ensuring that LLMs operate within predefined guidelines, thereby enhancing the safety and reliability of LLM outputs.</p>
<p>LlamaIndex specializes in data management for LLM applications, providing essential tools for handling the influx of data that these models require, streamlining the data integration process.</p>
<p>From Intel, FastRAG extends the basic RAG approach with advanced implementations, aligning closely with the sophisticated techniques discussed in this guide, and offering optimized solutions for retrieval-augmented tasks.</p>
<p>Auto-GPT stands out for its focus on designing LLM agents, simplifying the development of complex AI agents with its user-friendly interface and comprehensive features.Similarly, AutoGen by Microsoft has gained traction for its capabilities in agent and multi-agent system design, further enriching the ecosystem of tools available for prompt engineering.</p>
<p>These tools and frameworks are instrumental in the ongoing evolution of prompt engineering, offering a range of solutions from foundational prompt management to the construction of intricate AI agents.As the field continues to expand, the development of new tools and the enhancement of existing ones will remain critical in unlocking the full potential of LLMs in a variety of applications.</p>
<p>"</p>
<p>How should I write my college admission essay?Give me suggestions about the different sections I should include, what tone I should use, and what expressions I should avoid."See output in Figure 1 arXiv:2401.14423v4[cs.SE] 5 May 2024</p>
<p>Figure 1 :
1
Figure 1: Instructions + Question Prompt result example</p>
<p>Figure 2 :
2
Figure 2: Instructions + Input Prompt result example</p>
<p>Figure 3 :
3
Figure 3: Question + Examples Prompt results example</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Chain of thought prompting example</p>
<p>Figure 6 :
6
Figure 6: Getting factual sources</p>
<p>Figure 7 :
7
Figure 7: Special tokens can sometimes be used in prompts</p>
<p>Figure 8 :Figure 9 :Figure 10 :
8910
Figure 8: Don't try to be nice to the AI</p>
<p>k n o w l e d g e and common s e n s e r e a s o n i n g . 2 .D e m o c r a t i z a t i o n and a c c e s s i b i l i t y : I n t h e f u t u r e , m a c h i n e l e a r n i n g may become more r e a d i l y a v a i l a b l e t o a w i d e r s e t o f u s e r s , many o f whom w i l l n o t n e e d e x t e n s i v e t e c h n i c a l e x p e r t i s e t o u n d e r s t a n d how t o u s e i t .Machine l e a r n i n g p l a t f o r m s may s o o n a l l o w u s e r s t o e a s i l y u p l o a d t h e i r d a t a , s e l e c t t h e i r o b j e c t i v e s and c u s t o m i z e t h e i r models , w i t h o u t w r i t i n g any c o d e o r w o r r y i n g a b o u t t h e u n d e r l y i n g i n f r a s t r u c t u r e .T h i s c o u l d s i g n i f i c a n t l y l o w e r t h e b a r r i e r s t o e n t r y and a d o p t i o n o f m a c h i n e l e a r n i n g , and empower u s e r s t o s o l v e t h e i r own p r o b l e m s and g e n e r a t e t h e i r own i n s i g h t s .3 .Human c e n t r i c a p p r o a c h e s : As m a c h i n e l e a r n i n g s y s t e m s grow s m a r t e r , t h e y a r e a l s o l i k e l y t o become more human c e n t r i c and s o c i a l l y  aware , n o t o n l y p e r f o r m i n g t a s k s , b u t a l s o i n t e r a c t i n g w i t h and l e a r n i n g from humans i n a d a p t i v e ways .F o r i n s t a n c e , a m a c h i n e l e a r n i n g s y s t e m may n o t o n l y be a b l e t o d i a g n o s e d i s e a s e s , b u t a l s o c o m m u n i c a t</p>
<p>"</p>
<p>The following is an example of how to compute parity for a list Q: What is the parity on the list a=[1, 1, 0, 1, 0]? A: We initialize s= a=[1, 1, 0, 1, 0].The first element of a is 1 so b=1.s = s + b = 0 + 1 = 1.s=1.a=[1, 0, 1, 0].The first element of a is 1 so b=1.s = s + b = 1 + 1 = 0. s=0.a=[0, 1, 0].The first element of a is 0 so b=0.s = s + b = 0 + 0 = 0. s=0.a=[1, 0].The first element of a is 1 so b=1.s = s + b = 0 + 1 = 1.s=1.a=[0].The first element of a is 0 so b=0.s = s + b = 1 + 0 = 1.s=1.a=[] is empty.Since the list a is empty and we have s=1, the parity is 1 Given that definition, what would be the parity of this other list b= [0, 1, 1, 0, 0, 0, 0, 0]" See results in figure 13.</p>
<p>Figure 11 :
11
Figure 11: The AI is pretty good at creating different opinions</p>
<p>Figure 12 :
12
Figure 12: While LLMs don't have memory in themselves, most applications like ChatGPT have added this functionality</p>
<p>Figure 13 :Figure 14 :
1314
Figure 13: Who said LLMs cannot learn?</p>
<p>2 .
2
Manual CoT: This more intricate variant necessitates the provision of explicit, stepwise reasoning examples as templates, thereby guiding the model more definitively towards reasoned outputs.Despite its efficacy, Manual CoT's reliance on meticulously crafted examples poses scalability and maintenance challenges.</p>
<p>Figure 15 :
15
Figure 15: Comparison of Zero-shot and Manual Chain of Thought techniques as per [6].This figure underscores the structured approach of Manual CoT in providing detailed reasoning pathways, as opposed to the more generalized guidance in Zero-shot CoT.</p>
<p>Figure 16 :
16
Figure 16: Illustrative representation of the Tree of Thought methodology, showcasing the branching out into multiple reasoning pathways as adapted from [8].Each branch symbolizes a distinct line of reasoning, enabling a comprehensive exploration of potential solutions.</p>
<p>Figure 17 :
17
Figure 17: An example of tool usage from Langchain library</p>
<p>Figure 22 :
22
Figure 22: An example of integrating RAG with LLMs for a question answering application, showcasing the process of query extraction, information retrieval, and response synthesis [17].</p>
<p>Figure 23 :
23
Figure23: Illustration of using a Knowledge Graph (KG) as a retrieval mechanism in conjunction with LLMs to enhance response generation with structured external knowledge[18].</p>
<p>Figure 24 :
24
Figure 24: Example block representation of an LLM-based agent, highlighting its components and their interaction in task execution.</p>
<p>Figure 25 :
25
Figure 25: Workflow of ReWOO, illustrating the meta-planning and execution phases in the reasoning process.</p>
<p>Figure 26 :
26
Figure 26: Comparison of ReAct with simpler prompting methods, highlighting its interleaved reasoning-action structure.</p>
<p>Figure 27 :
27
Figure 27: Conceptual representation of DERA, showcasing the interaction between different agent roles within a dialogue context.</p>
<p></p>
<p>Transient State: LLMs inherently lack persistent memory or state, necessitating additional software or systems for context retention and management. Probabilistic Nature: The stochastic nature of LLMs introduces variability in responses, even to identical prompts, challenging consistency in applications.This means you might get slightly different answers each time, even with the same prompt.</p>
<p>[3]utdated Information: Reliance on pre-training data confines LLMs to historical knowledge, precluding real-time awareness or updates.ContentFabrication: LLMs may generate plausible yet factually incorrect information, a phenomenon commonly referred to as "hallucination."[3]</p>
<p>ConclusionPrompt design and engineering will only become more critical as LLMs and generative AI evolve.We discussed foundations and cutting-edge approaches such as Retrieval Augmented Generation (RAG) -essential tools for the next wave of intelligent applications.As prompt design and engineering rapidly progress, resources like this will offer a historical lens on early techniques.Remember, innovations like Automatic Prompt Engineering (APE) covered here could become standard practice in the years to come.Be part of shaping the trajectory of these exciting developments!
Machine learning: The high interest credit card of technical debt. D Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop). 2014</p>
<p>Transformer models: an introduction and catalog. Ananth Xavier Amatriain, Jie Sankar, Praveen Bing, Timothy J Kumar Bodigutla, Michaeel Hazen, Kazi, 2023</p>
<p>Measuring and mitigating hallucinations in large language models: A multifaceted approach. Xavier Amatriain, 2024</p>
<p>Teaching algorithmic reasoning via in-context learning. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi, 2022</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, Gorilla: Large language model connected with massive apis. 2023</p>
<p>Automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco Tulio, Ribeiro , 2023Art</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Exploring the mit mathematics and eecs curriculum using large language models. Sarah J Zhang, Samuel Florin, Ariel N Lee, Eamon Niknafs, Andrei Marginean, Annie Wang, Keith Tyser, Zad Chin, Yann Hicke, Nikhil Singh, Madeleine Udell, Yoon Kim, Tonio Buonassisi, 2023Armando Solar-Lezama, and Iddo Drori</p>
<p>Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, Carrie J Cai, Promptchainer: Chaining large language model prompts through visual programming. 2022</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, 2023</p>
<p>Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Web Amazon, Services, 2023</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.083022023arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
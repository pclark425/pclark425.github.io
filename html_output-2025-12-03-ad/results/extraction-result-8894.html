<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8894 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8894</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8894</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-786d7687cf0a7629955e3534a7735c3bf3905912</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/786d7687cf0a7629955e3534a7735c3bf3905912" target="_blank">Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel algorithm called Augmented Memory that combines data augmentation with experience replay and shows that scores obtained from oracle calls can be reused to update the model multiple times, achieving a new state-of-the-art in the PMO benchmark which enforces a computational budget.</p>
                <p><strong>Paper Abstract:</strong> Sample efficiency is a fundamental challenge in de novo molecular design. Ideally, molecular generative models should learn to satisfy a desired objective under minimal oracle evaluations (computational prediction or wet-lab experiment). This problem becomes more apparent when using oracles that can provide increased predictive accuracy but impose a significant cost. Consequently, these oracles cannot be directly optimized under a practical budget. Molecular generative models have shown remarkable sample efficiency when coupled with reinforcement learning, as demonstrated in the Practical Molecular Optimization (PMO) benchmark. Here, we propose a novel algorithm called Augmented Memory that combines data augmentation with experience replay. We show that scores obtained from oracle calls can be reused to update the model multiple times. We compare Augmented Memory to previously proposed algorithms and show significantly enhanced sample efficiency in an exploitation task and a drug discovery case study requiring both exploration and exploitation. Our method achieves a new state-of-the-art in the PMO benchmark which enforces a computational budget, outperforming the previous best performing method on 19/23 tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8894.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8894.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmented Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented Memory (experience replay + SMILES augmentation for RL fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel algorithm introduced in this paper that combines experience replay (a replay buffer of top-rewarding molecules) with SMILES augmentation to reuse expensive oracle scores multiple times and accelerate sample-efficient de novo molecular design under a fixed oracle budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Augmented Memory (SMILES-based RNN Agent built on REINVENT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive language model (SMILES) — LSTM RNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Prior pre-trained on ChEMBL for DRD2/aripiprazole experiments; ZINC used for PMO benchmarking Prior as described in paper</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery / multi-parameter optimization (MPO); rediscovery/similarity tasks; docking optimization (DRD2)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>on-policy reinforcement learning (REINFORCE-style) with a squared-difference loss to match an augmented likelihood; uses experience replay buffer storing top-K molecules and SMILES augmentation (randomized SMILES) to perform multiple agent updates per oracle call; Selective Memory Purge to remove replay entries penalized by a diversity filter</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty assessed indirectly: method rediscovered target molecules (aripiprazole/others) and produced diverse nearby chemotypes when Selective Memory Purge used; PMO benchmark AUC Top-10 improvements imply generation of high-scoring molecules not present in training, and UMAP/IntDiv1 analyses show expanded chemical coverage vs baselines (quantitative novelty fraction not reported).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailors molecules via reward functions (e.g., similarity to a target, MPO objectives combining QED, MW, and docking score); oracle-based scoring enforces application-specific properties (AutoDock Vina docking score for DRD2, Tanimoto similarity for rediscovery tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PMO AUC Top-10 (primary benchmark), number of oracle calls to reach target score (sample efficiency), Tanimoto similarity distributions, IntDiv1 (internal diversity), UMAP chemical-space visualization, Vina docking scores, QED, molecular weight filters, count of generated molecules passing property filters.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Introduced Augmented Memory and showed large sample-efficiency gains: set new state-of-the-art on the PMO benchmark (sum AUC Top-10 = 15.002 vs REINVENT 14.016; outperformed REINVENT on 19/23 tasks). In aripiprazole similarity, reached average score 0.8 with 6,144 oracle calls (mean over 100 replicates) vs ~12,416 for Double Loop RL. In DRD2 docking MPO case study (9,600-call budget) generated >2,000 more molecules with better docking scores than risperidone than the next-best algorithm (AHC with replay), and shifted joint QED–Vina distribution toward desirable region.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparisons in paper: outperformed REINVENT (RNN LSTM baseline), Augmented Hill-Climb (AHC), Best Agent Reminder (BAR), and Double Loop RL (values taken from their paper) in sample efficiency across PMO tasks. Experience replay plus augmentation is credited as key advantage; Selective Memory Purge improves diversity relative to naive diversity filter.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Mode collapse risk if too many augmentation rounds — paper finds 2 augmentation rounds optimal; augmentation rounds >2 increase susceptibility to mode collapse. In sparse-reward landscapes, benefit may diminish because replay buffer is hard to populate with high-reward examples. Can exploit imperfect oracles (e.g., AutoDock Vina) to produce chemically unrealistic, highly lipophilic scaffolds unless additional constraints (QED, MW) are included. Model-size and exact LLM-scale benefits not reported. Requires careful hyperparameter tuning (sigma, buffer size, augmentation rounds) and diversity heuristics (Selective Memory Purge) to balance exploitation/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8894.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8894.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINVENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINVENT (SMILES-based reinforcement learning for de novo design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SMILES-based autoregressive RNN generative model (LSTM) used as Prior and baseline Agent; uses policy-based RL (REINFORCE-like) to fine-tune toward property objectives and is a strong sample-efficiency baseline in multiple benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REINVENT 2.0: An AI Tool for De Novo Drug Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REINVENT (RNN LSTM SMILES language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive language model (SMILES) — LSTM RNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained Priors: ChEMBL (random.prior.new used in aripiprazole and DRD2 experiments); ZINC used for PMO benchmarking Prior in this paper's PMO runs</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug discovery, de novo molecular generation, MPO</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>policy-based reinforcement learning (REINFORCE) with augmented likelihood combining Prior log-likelihood and scaled reward; can use experience replay buffers and diversity filters</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates molecules outside training set via sampling from fine-tuned policy; in PMO benchmark REINVENT was previously the most sample-efficient baseline; quantitative novelty per-se not reported in this paper beyond benchmark performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimizes application-specific reward functions (similarity objectives, MPO, docking); Prior likelihood term preserves syntactic validity and chemistry-like biases from training data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PMO AUC Top-10, docking scores (when used as oracle), Tanimoto similarity, IntDiv1, number of oracle calls to reach thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as the main baseline: REINVENT with experience replay is competitive but Augmented Memory outperforms it across PMO tasks (REINVENT second). REINVENT's architecture and loss function form basis for Augmented Memory's agent and loss derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly in experiments; REINVENT improved by adding experience replay but still fell behind Augmented Memory; AHC and BAR were also compared when augmented with replay.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Sample-efficiency can be improved with replay and augmentation; susceptible to reduced diversity or mode collapse if not managed; requires careful reward shaping and hyperparameter tuning. Exact model scale not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8894.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8894.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Double Loop RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Double Loop Reinforcement Learning using augmented SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed method (Bjerrum et al.) that uses SMILES augmentation to reuse oracle scores by generating multiple SMILES for the same molecule and updating the policy multiple times per oracle call, improving sample efficiency while maintaining diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Double Loop RL (built on REINVENT; SMILES augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES language-model-based RL (likely RNN-based as built on REINVENT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Built on the same pre-trained Prior as REINVENT (ChEMBL in referenced comparisons); uses SMILES augmentation (randomized SMILES representations) during RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular optimization; improved sample efficiency with preserved diversity</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>on-policy reinforcement learning with SMILES augmentation to perform multiple policy updates per oracle call (double-loop update), leveraging the non-injective SMILES representation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported to accelerate learning and retain diversity relative to some other biased-update methods; specific novelty metrics in this paper were taken from Bjerrum et al. for comparison (Double Loop RL reached 0.8 score at ~12,416 oracle calls in aripiprazole task).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Uses SMILES augmentation to make better use of oracle scores in property-driven objectives; does not fundamentally change the reward function design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Sample efficiency measured by oracle calls to reach similarity thresholds; diversity metrics reported in the original Double Loop RL paper (not re-derived here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as second-most sample-efficient previously reported method for aripiprazole similarity; authors used reported numbers because Double Loop RL code was not released. Augmented Memory outperformed Double Loop RL in sample efficiency in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Double Loop RL also builds on REINVENT and uses augmentation; Augmented Memory differs by combining full replay-buffer augmentation and experience replay with additional heuristics (Selective Memory Purge) and a squared-difference loss framing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Code not released (per paper) so direct reimplementation comparisons limited; like other augmentation-heavy methods, risk of mode collapse if update multiplicity not tuned (paper cites Bjerrum et al.'s observations that augmentation helps diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8894.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8894.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmented Hill-Climb (AHC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmented Hill-Climb (hybrid of hill-climbing and REINVENT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based fine-tuning algorithm that updates the policy every epoch using only the top-k generated molecules to bias learning toward high-reward samples and improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AHC (REINVENT-style LSTM Agent with hill-climb update)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive SMILES language model (LSTM RNN) with a hill-climb-style biased update</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses same Prior training data as REINVENT when applied (ChEMBL or ZINC depending on experiment); in PMO experiments authors used provided Priors matching PMO protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecule generation, sample-efficient optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>biased RL updates using top-k selection (hill-climbing) possibly combined with experience replay when implemented here; used as a baseline and also re-run with added experience replay in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generates high-reward molecules efficiently but can suffer from diversity loss unless paired with replay/augmentation or diversity filters; novelty not quantified beyond benchmark metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimizes specified reward functions (e.g., similarity, docking) by biasing updates to top-k molecules, steering the policy toward high-reward regions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PMO AUC Top-10, number of oracle calls, counts of molecules meeting defined MPO filters (e.g., DRD2 QED/MW/Vina thresholds), IntDiv1 for diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When augmented with experience replay in this paper, AHC's sample efficiency improved substantially, but still generally underperformed Augmented Memory; in DRD2 case study AHC with replay was second-best but produced far fewer molecules passing filter thresholds than Augmented Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to REINVENT, BAR, and Augmented Memory; experience replay improves AHC but Augmented Memory's buffer-augmentation strategy yields better sample-efficiency and MPO outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Susceptible to mode collapse and reduced diversity if only top-k samples drive updates; benefits from experience replay and SMILES augmentation to stabilize learning; hyperparameters (k) important and tuned per task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8894.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8894.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best Agent Reminder (BAR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best Agent Reminder (BAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL technique that keeps track of a best-performing agent and periodically 'reminds' the current policy of favorable actions by mixing trajectories or gradients, intended to stabilize and speed up policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>De novo drug design using reinforcement learning with graph-based deep generative models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BAR (used here with REINVENT-style Priors and replay)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>RL augmentation scheme applied to generative models (used here with SMILES LSTM agents; original BAR work used graph-based generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Depends on the underlying generative model; in this paper BAR was run with the same Priors as other baselines (ChEMBL/ZINC depending on experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>de novo molecular generation and optimization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>policy-based RL with explicit reminder/memory of the best agent; in this paper BAR was reimplemented and evaluated both with and without added experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Intended to encourage retention of previously-good policy behaviors; novelty not directly quantified here beyond PMO metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Encourages continued sampling of previously good actions/molecules to maintain performance during exploration/exploitation trade-offs; applied to optimize PMO tasks and docking objectives in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>PMO AUC Top-10 (used to tune BAR hyperparameters here), oracle-call efficiency, IntDiv1 for diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BAR when augmented with experience replay improved over its non-replay variant, but did not surpass REINVENT or Augmented Memory in PMO aggregate performance; BAR's sigma and alpha hyperparameters required tuning (authors report tuning table and best config used in PMO experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared vs REINVENT, AHC, and Augmented Memory; experience replay improved BAR but Augmented Memory still leads in sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Performance sensitive to hyperparameters (sigma, alpha); when used without replay it was less sample-efficient in these experiments; like other biased-gradient approaches, potential to hurt diversity without explicit counters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8894.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8894.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES augmentation / randomized SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data augmentation technique exploiting the non-injective nature of SMILES by enumerating alternative SMILES strings for the same molecular graph, used to multiply-use an oracle evaluation and to regularize token-level memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES augmentation (technique used with SMILES generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>data augmentation for sequence (SMILES) language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Applies to SMILES strings derived from molecular datasets (ChEMBL, ZINC) or sampled molecules; used on both sampled batch and entire replay buffer in Augmented Memory</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>general molecular generative modeling — improves sample efficiency and diversity in drug design and MPO tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>generative model sampling combined with augmentation: the same molecule is represented by multiple, randomized SMILES sequences so that a single oracle score can drive multiple parameter updates</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Enables repeated learning from a single molecule without overfitting to a single token sequence and helps discover diverse representations of similar molecules; novelty metrics reported indirectly via improved benchmark performance and diversity measures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not an application-specific model — it amplifies oracle information for any task (similarity, docking, MPO) where molecules are scored.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Shown to act as a regularizer: experiments without augmentation were less stable and had higher variance and more failed runs on the aripiprazole similarity task; diversity metrics (IntDiv1, UMAP) improved when augmentation + Selective Memory Purge used.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Crucial component of Augmented Memory: without SMILES augmentation, replay-only reinforcement leads to token-level memorization, higher failure rates, and instability (more runs failed to reach thresholds). Augmentation mitigates sequence-wise mode collapse and regularizes training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related to Bjerrum et al.'s Double Loop RL which also uses augmentation; authors show that augmenting the entire replay buffer and combining with replay produces better sample efficiency than replay only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Excessive augmentation rounds (too many repeated updates per oracle call) can increase mode collapse risk; the optimal number of augmentation rounds must be tuned (paper finds two rounds optimal for stability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molecular de-novo design through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>REINVENT 2.0: An AI Tool for De Novo Drug Design <em>(Rating: 2)</em></li>
                <li>Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented SMILES <em>(Rating: 2)</em></li>
                <li>Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation <em>(Rating: 2)</em></li>
                <li>De novo drug design using reinforcement learning with graph-based deep generative models <em>(Rating: 2)</em></li>
                <li>Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds <em>(Rating: 1)</em></li>
                <li>Randomized SMILES strings improve the quality of molecular generative models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8894",
    "paper_id": "paper-786d7687cf0a7629955e3534a7735c3bf3905912",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "Augmented Memory",
            "name_full": "Augmented Memory (experience replay + SMILES augmentation for RL fine-tuning)",
            "brief_description": "A novel algorithm introduced in this paper that combines experience replay (a replay buffer of top-rewarding molecules) with SMILES augmentation to reuse expensive oracle scores multiple times and accelerate sample-efficient de novo molecular design under a fixed oracle budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Augmented Memory (SMILES-based RNN Agent built on REINVENT)",
            "model_type": "autoregressive language model (SMILES) — LSTM RNN",
            "model_size": null,
            "training_data": "Prior pre-trained on ChEMBL for DRD2/aripiprazole experiments; ZINC used for PMO benchmarking Prior as described in paper",
            "application_domain": "drug discovery / multi-parameter optimization (MPO); rediscovery/similarity tasks; docking optimization (DRD2)",
            "generation_method": "on-policy reinforcement learning (REINFORCE-style) with a squared-difference loss to match an augmented likelihood; uses experience replay buffer storing top-K molecules and SMILES augmentation (randomized SMILES) to perform multiple agent updates per oracle call; Selective Memory Purge to remove replay entries penalized by a diversity filter",
            "novelty_of_chemicals": "Novelty assessed indirectly: method rediscovered target molecules (aripiprazole/others) and produced diverse nearby chemotypes when Selective Memory Purge used; PMO benchmark AUC Top-10 improvements imply generation of high-scoring molecules not present in training, and UMAP/IntDiv1 analyses show expanded chemical coverage vs baselines (quantitative novelty fraction not reported).",
            "application_specificity": "Tailors molecules via reward functions (e.g., similarity to a target, MPO objectives combining QED, MW, and docking score); oracle-based scoring enforces application-specific properties (AutoDock Vina docking score for DRD2, Tanimoto similarity for rediscovery tasks).",
            "evaluation_metrics": "PMO AUC Top-10 (primary benchmark), number of oracle calls to reach target score (sample efficiency), Tanimoto similarity distributions, IntDiv1 (internal diversity), UMAP chemical-space visualization, Vina docking scores, QED, molecular weight filters, count of generated molecules passing property filters.",
            "results_summary": "Introduced Augmented Memory and showed large sample-efficiency gains: set new state-of-the-art on the PMO benchmark (sum AUC Top-10 = 15.002 vs REINVENT 14.016; outperformed REINVENT on 19/23 tasks). In aripiprazole similarity, reached average score 0.8 with 6,144 oracle calls (mean over 100 replicates) vs ~12,416 for Double Loop RL. In DRD2 docking MPO case study (9,600-call budget) generated &gt;2,000 more molecules with better docking scores than risperidone than the next-best algorithm (AHC with replay), and shifted joint QED–Vina distribution toward desirable region.",
            "comparison_to_other_methods": "Direct comparisons in paper: outperformed REINVENT (RNN LSTM baseline), Augmented Hill-Climb (AHC), Best Agent Reminder (BAR), and Double Loop RL (values taken from their paper) in sample efficiency across PMO tasks. Experience replay plus augmentation is credited as key advantage; Selective Memory Purge improves diversity relative to naive diversity filter.",
            "limitations_and_challenges": "Mode collapse risk if too many augmentation rounds — paper finds 2 augmentation rounds optimal; augmentation rounds &gt;2 increase susceptibility to mode collapse. In sparse-reward landscapes, benefit may diminish because replay buffer is hard to populate with high-reward examples. Can exploit imperfect oracles (e.g., AutoDock Vina) to produce chemically unrealistic, highly lipophilic scaffolds unless additional constraints (QED, MW) are included. Model-size and exact LLM-scale benefits not reported. Requires careful hyperparameter tuning (sigma, buffer size, augmentation rounds) and diversity heuristics (Selective Memory Purge) to balance exploitation/exploration.",
            "uuid": "e8894.0",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "REINVENT",
            "name_full": "REINVENT (SMILES-based reinforcement learning for de novo design)",
            "brief_description": "A SMILES-based autoregressive RNN generative model (LSTM) used as Prior and baseline Agent; uses policy-based RL (REINFORCE-like) to fine-tune toward property objectives and is a strong sample-efficiency baseline in multiple benchmarks.",
            "citation_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
            "mention_or_use": "use",
            "model_name": "REINVENT (RNN LSTM SMILES language model)",
            "model_type": "autoregressive language model (SMILES) — LSTM RNN",
            "model_size": null,
            "training_data": "Pre-trained Priors: ChEMBL (random.prior.new used in aripiprazole and DRD2 experiments); ZINC used for PMO benchmarking Prior in this paper's PMO runs",
            "application_domain": "drug discovery, de novo molecular generation, MPO",
            "generation_method": "policy-based reinforcement learning (REINFORCE) with augmented likelihood combining Prior log-likelihood and scaled reward; can use experience replay buffers and diversity filters",
            "novelty_of_chemicals": "Generates molecules outside training set via sampling from fine-tuned policy; in PMO benchmark REINVENT was previously the most sample-efficient baseline; quantitative novelty per-se not reported in this paper beyond benchmark performance metrics.",
            "application_specificity": "Optimizes application-specific reward functions (similarity objectives, MPO, docking); Prior likelihood term preserves syntactic validity and chemistry-like biases from training data.",
            "evaluation_metrics": "PMO AUC Top-10, docking scores (when used as oracle), Tanimoto similarity, IntDiv1, number of oracle calls to reach thresholds.",
            "results_summary": "Used as the main baseline: REINVENT with experience replay is competitive but Augmented Memory outperforms it across PMO tasks (REINVENT second). REINVENT's architecture and loss function form basis for Augmented Memory's agent and loss derivation.",
            "comparison_to_other_methods": "Compared directly in experiments; REINVENT improved by adding experience replay but still fell behind Augmented Memory; AHC and BAR were also compared when augmented with replay.",
            "limitations_and_challenges": "Sample-efficiency can be improved with replay and augmentation; susceptible to reduced diversity or mode collapse if not managed; requires careful reward shaping and hyperparameter tuning. Exact model scale not reported.",
            "uuid": "e8894.1",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Double Loop RL",
            "name_full": "Double Loop Reinforcement Learning using augmented SMILES",
            "brief_description": "A recently proposed method (Bjerrum et al.) that uses SMILES augmentation to reuse oracle scores by generating multiple SMILES for the same molecule and updating the policy multiple times per oracle call, improving sample efficiency while maintaining diversity.",
            "citation_title": "Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented SMILES",
            "mention_or_use": "mention",
            "model_name": "Double Loop RL (built on REINVENT; SMILES augmentation)",
            "model_type": "SMILES language-model-based RL (likely RNN-based as built on REINVENT)",
            "model_size": null,
            "training_data": "Built on the same pre-trained Prior as REINVENT (ChEMBL in referenced comparisons); uses SMILES augmentation (randomized SMILES representations) during RL fine-tuning.",
            "application_domain": "de novo molecular optimization; improved sample efficiency with preserved diversity",
            "generation_method": "on-policy reinforcement learning with SMILES augmentation to perform multiple policy updates per oracle call (double-loop update), leveraging the non-injective SMILES representation",
            "novelty_of_chemicals": "Reported to accelerate learning and retain diversity relative to some other biased-update methods; specific novelty metrics in this paper were taken from Bjerrum et al. for comparison (Double Loop RL reached 0.8 score at ~12,416 oracle calls in aripiprazole task).",
            "application_specificity": "Uses SMILES augmentation to make better use of oracle scores in property-driven objectives; does not fundamentally change the reward function design.",
            "evaluation_metrics": "Sample efficiency measured by oracle calls to reach similarity thresholds; diversity metrics reported in the original Double Loop RL paper (not re-derived here).",
            "results_summary": "Referenced as second-most sample-efficient previously reported method for aripiprazole similarity; authors used reported numbers because Double Loop RL code was not released. Augmented Memory outperformed Double Loop RL in sample efficiency in this paper's experiments.",
            "comparison_to_other_methods": "Double Loop RL also builds on REINVENT and uses augmentation; Augmented Memory differs by combining full replay-buffer augmentation and experience replay with additional heuristics (Selective Memory Purge) and a squared-difference loss framing.",
            "limitations_and_challenges": "Code not released (per paper) so direct reimplementation comparisons limited; like other augmentation-heavy methods, risk of mode collapse if update multiplicity not tuned (paper cites Bjerrum et al.'s observations that augmentation helps diversity).",
            "uuid": "e8894.2",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Augmented Hill-Climb (AHC)",
            "name_full": "Augmented Hill-Climb (hybrid of hill-climbing and REINVENT)",
            "brief_description": "An RL-based fine-tuning algorithm that updates the policy every epoch using only the top-k generated molecules to bias learning toward high-reward samples and improve sample efficiency.",
            "citation_title": "Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation",
            "mention_or_use": "use",
            "model_name": "AHC (REINVENT-style LSTM Agent with hill-climb update)",
            "model_type": "autoregressive SMILES language model (LSTM RNN) with a hill-climb-style biased update",
            "model_size": null,
            "training_data": "Uses same Prior training data as REINVENT when applied (ChEMBL or ZINC depending on experiment); in PMO experiments authors used provided Priors matching PMO protocol.",
            "application_domain": "de novo molecule generation, sample-efficient optimization",
            "generation_method": "biased RL updates using top-k selection (hill-climbing) possibly combined with experience replay when implemented here; used as a baseline and also re-run with added experience replay in this paper",
            "novelty_of_chemicals": "Generates high-reward molecules efficiently but can suffer from diversity loss unless paired with replay/augmentation or diversity filters; novelty not quantified beyond benchmark metrics in this paper.",
            "application_specificity": "Optimizes specified reward functions (e.g., similarity, docking) by biasing updates to top-k molecules, steering the policy toward high-reward regions.",
            "evaluation_metrics": "PMO AUC Top-10, number of oracle calls, counts of molecules meeting defined MPO filters (e.g., DRD2 QED/MW/Vina thresholds), IntDiv1 for diversity.",
            "results_summary": "When augmented with experience replay in this paper, AHC's sample efficiency improved substantially, but still generally underperformed Augmented Memory; in DRD2 case study AHC with replay was second-best but produced far fewer molecules passing filter thresholds than Augmented Memory.",
            "comparison_to_other_methods": "Compared directly to REINVENT, BAR, and Augmented Memory; experience replay improves AHC but Augmented Memory's buffer-augmentation strategy yields better sample-efficiency and MPO outcomes.",
            "limitations_and_challenges": "Susceptible to mode collapse and reduced diversity if only top-k samples drive updates; benefits from experience replay and SMILES augmentation to stabilize learning; hyperparameters (k) important and tuned per task.",
            "uuid": "e8894.3",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Best Agent Reminder (BAR)",
            "name_full": "Best Agent Reminder (BAR)",
            "brief_description": "An RL technique that keeps track of a best-performing agent and periodically 'reminds' the current policy of favorable actions by mixing trajectories or gradients, intended to stabilize and speed up policy learning.",
            "citation_title": "De novo drug design using reinforcement learning with graph-based deep generative models",
            "mention_or_use": "use",
            "model_name": "BAR (used here with REINVENT-style Priors and replay)",
            "model_type": "RL augmentation scheme applied to generative models (used here with SMILES LSTM agents; original BAR work used graph-based generative models)",
            "model_size": null,
            "training_data": "Depends on the underlying generative model; in this paper BAR was run with the same Priors as other baselines (ChEMBL/ZINC depending on experiment).",
            "application_domain": "de novo molecular generation and optimization",
            "generation_method": "policy-based RL with explicit reminder/memory of the best agent; in this paper BAR was reimplemented and evaluated both with and without added experience replay",
            "novelty_of_chemicals": "Intended to encourage retention of previously-good policy behaviors; novelty not directly quantified here beyond PMO metrics.",
            "application_specificity": "Encourages continued sampling of previously good actions/molecules to maintain performance during exploration/exploitation trade-offs; applied to optimize PMO tasks and docking objectives in experiments.",
            "evaluation_metrics": "PMO AUC Top-10 (used to tune BAR hyperparameters here), oracle-call efficiency, IntDiv1 for diversity.",
            "results_summary": "BAR when augmented with experience replay improved over its non-replay variant, but did not surpass REINVENT or Augmented Memory in PMO aggregate performance; BAR's sigma and alpha hyperparameters required tuning (authors report tuning table and best config used in PMO experiments).",
            "comparison_to_other_methods": "Compared vs REINVENT, AHC, and Augmented Memory; experience replay improved BAR but Augmented Memory still leads in sample efficiency.",
            "limitations_and_challenges": "Performance sensitive to hyperparameters (sigma, alpha); when used without replay it was less sample-efficient in these experiments; like other biased-gradient approaches, potential to hurt diversity without explicit counters.",
            "uuid": "e8894.4",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SMILES Augmentation",
            "name_full": "SMILES augmentation / randomized SMILES",
            "brief_description": "A data augmentation technique exploiting the non-injective nature of SMILES by enumerating alternative SMILES strings for the same molecular graph, used to multiply-use an oracle evaluation and to regularize token-level memorization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SMILES augmentation (technique used with SMILES generative models)",
            "model_type": "data augmentation for sequence (SMILES) language models",
            "model_size": null,
            "training_data": "Applies to SMILES strings derived from molecular datasets (ChEMBL, ZINC) or sampled molecules; used on both sampled batch and entire replay buffer in Augmented Memory",
            "application_domain": "general molecular generative modeling — improves sample efficiency and diversity in drug design and MPO tasks",
            "generation_method": "generative model sampling combined with augmentation: the same molecule is represented by multiple, randomized SMILES sequences so that a single oracle score can drive multiple parameter updates",
            "novelty_of_chemicals": "Enables repeated learning from a single molecule without overfitting to a single token sequence and helps discover diverse representations of similar molecules; novelty metrics reported indirectly via improved benchmark performance and diversity measures.",
            "application_specificity": "Not an application-specific model — it amplifies oracle information for any task (similarity, docking, MPO) where molecules are scored.",
            "evaluation_metrics": "Shown to act as a regularizer: experiments without augmentation were less stable and had higher variance and more failed runs on the aripiprazole similarity task; diversity metrics (IntDiv1, UMAP) improved when augmentation + Selective Memory Purge used.",
            "results_summary": "Crucial component of Augmented Memory: without SMILES augmentation, replay-only reinforcement leads to token-level memorization, higher failure rates, and instability (more runs failed to reach thresholds). Augmentation mitigates sequence-wise mode collapse and regularizes training.",
            "comparison_to_other_methods": "Related to Bjerrum et al.'s Double Loop RL which also uses augmentation; authors show that augmenting the entire replay buffer and combining with replay produces better sample efficiency than replay only.",
            "limitations_and_challenges": "Excessive augmentation rounds (too many repeated updates per oracle call) can increase mode collapse risk; the optimal number of augmentation rounds must be tuned (paper finds two rounds optimal for stability).",
            "uuid": "e8894.5",
            "source_info": {
                "paper_title": "Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molecular de-novo design through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
            "rating": 2
        },
        {
            "paper_title": "Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented SMILES",
            "rating": 2
        },
        {
            "paper_title": "Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation",
            "rating": 2
        },
        {
            "paper_title": "De novo drug design using reinforcement learning with graph-based deep generative models",
            "rating": 2
        },
        {
            "paper_title": "Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds",
            "rating": 1
        },
        {
            "paper_title": "Randomized SMILES strings improve the quality of molecular generative models",
            "rating": 2
        }
    ],
    "cost": 0.01602275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Augmented Memory: Capitalizing on Experience Replay to Accelerate De Novo Molecular Design</h1>
<p>Jeff Guo ${ }^{1,2}$, Philippe Schwaller ${ }^{1,2}$<br>${ }^{1}$ Laboratory of Artificial Chemical Intelligence (LIAC), Institut des Sciences et Ingénierie Chimiques, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland<br>${ }^{2}$ National Centre of Competence in Research (NCCR) Catalysis, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland<br>{jeff.guo,philippe.schwaller}@epfl.ch</p>
<h4>Abstract</h4>
<p>Sample efficiency is a fundamental challenge in de novo molecular design. Ideally, molecular generative models should learn to satisfy a desired objective under minimal oracle evaluations (computational prediction or wet-lab experiment). This problem becomes more apparent when using oracles that can provide increased predictive accuracy but impose a significant cost. Consequently, these oracles cannot be directly optimized under a practical budget. Molecular generative models have shown remarkable sample efficiency when coupled with reinforcement learning, as demonstrated in the Practical Molecular Optimization (PMO) benchmark. Here, we propose a novel algorithm called Augmented Memory that combines data augmentation with experience replay. We show that scores obtained from oracle calls can be reused to update the model multiple times. We compare Augmented Memory to previously proposed algorithms and show significantly enhanced sample efficiency in an exploitation task and a drug discovery case study requiring both exploration and exploitation. Our method achieves a new state-of-the-art in the PMO benchmark which enforces a computational budget, outperforming the previous best performing method on 19/23 tasks. The code is available at https://github.com/schwallergroup/augmented_memory.</p>
<h2>1 Introduction</h2>
<p>A quintessential task in any molecular discovery campaign is identifying promising candidate molecules amidst an enormous chemical space ${ }^{1}$. With the democratization of computing resources, computational oracles can be deployed to query larger chemical spaces in search of the desired property profile. The use of such oracles has enabled researchers to identify functional materials ${ }^{2}$, therapeutics ${ }^{3-5}$, and catalysts ${ }^{6}$, thus accelerating chemical discovery. However, there is generally a trade-off between oracle predictive accuracy and inference cost, such that the computational budget imposes a pragmatic constraint. Provided a sufficiently sample efficient model, it is conceivable for wet-lab experiments to be the oracle itself, as enabled by a high-throughput experimentation platform. Correspondingly, designing computational workflows and algorithms that are performant under minimal oracle calls is widely beneficial to the field of molecular design.
Recent advancements in de novo molecular design have positioned generative methods as a complementary approach to traditional virtual screening ${ }^{3,7}$. Core advantages of these models include the ability to sample chemical space outside the training data and by coupling an optimization algorithm, goal-directed learning can be achieved ${ }^{8}$. Although the field is relatively nascent, molecular generative models have identified experimentally validated therapeutic molecules ${ }^{4,5,9,10}$ and organocatalysts ${ }^{6}$. An important shared commonality between these success stories is the inclusion of</p>
<p>relatively computationally expensive oracles that are optimized. In drug design, molecular docking is frequently used while in catalyst and materials design, quantum mechanical properties are of interest. Correspondingly, many generative models proposed in recent years have competed to demonstrate accelerated optimization of these properties. However, the heterogeneity of the assessment protocols makes comparisons difficult. Recently, Gao et al. ${ }^{11}$ propose the Practical Molecular Optimization (PMO) benchmark which assesses 25 molecular generative models across 23 tasks, enforcing a computational budget of 10,000 oracle calls. Their results show that REINVENT ${ }^{12,13}$, a recurrent neural network (RNN)-based generative model operating on simplified molecular-input line-entry system (SMILES) ${ }^{14}$ is, on average, the most sample efficient generative model. REINVENT ${ }^{12,13}$ uses a policy-based reinforcement learning (RL) algorithm to optimize a reward function in a goal-directed approach. Recently, alternative algorithms have been proposed in the form of Best Agent Reminder (BAR) ${ }^{15}$ and Augmented Hill Climbing (AHC) ${ }^{16}$ which both introduce bias towards high rewarding molecules to improve sample efficiency. Other studies show that experience replay, where the highest rewarding molecules sampled are stored and replayed to the model, improves sample efficiency. ${ }^{10,13}$ More recently, Bjerrum et al. ${ }^{17}$ proposed Double Loop RL to take advantage of the non-injective nature of SMILES and the ease with which they can be augmented. By obtaining different SMILES sequences for the same molecule, oracle scores can be re-used to perform multiple updates to the Agent. Their results show accelerated learning while maintaining the diversity of results, an aspect missing in many proposed benchmarks.</p>
<p>Sample efficiency is a limiting factor to enabling more exploration of chemical spaces of interest, such as in drug discovery where the reward is sparse, i.e., finding a needle in the haystack. In this paper, we highlight the importance of experience replay in policy-based RL algorithms for molecular generation. We propose a novel algorithm called Augmented Memory that combines experience replay with SMILES augmentation. We further propose Selective Memory Purge which removes entries in the replay buffer with undesired chemical scaffolds and jointly address sample efficiency and diversity. The main contributions of this paper are:</p>
<ul>
<li>We explicitly highlight the importance of experience replay on the sample efficiency of REINVENT and all proposed algorithmic modifications.</li>
<li>We propose a novel algorithm called Augmented Memory which significantly outperforms all previous algorithms in sample efficiency. This is demonstrated in an exploitation task and a drug discovery case study.</li>
<li>We propose a method called Selective Memory Purge, which can be used in conjunction with Augmented Memory to generate diverse molecules while retaining enhanced sample efficiency.</li>
<li>We expand the PMO benchmark ${ }^{11}$ by adding Augmented Memory and BAR ${ }^{15}$ implementations. We further add experience replay to the implemented version of $\mathrm{AHC}^{16,18}$ for comparison. Our algorithm achieves a new state-of-the-art and outperforms the previous state-of-the-art, REINVENT ${ }^{12,13}$, on 19/23 tasks.</li>
</ul>
<h1>2 Related Work</h1>
<p>Goal-directed Molecular Design with Policy-based Reinforcement Learning. Molecular generation can be framed as a policy-based RL problem, where a base model (Prior) is trained on a general dataset and fine-tuned (Agent) to generate molecules with desired property profiles. Existing works that follow this paradigm include SMILES-based RNNs ${ }^{12,13,19,20}$, generative adversarial networks (GANs) ${ }^{21-25}$, variational autoencoders (VAEs) ${ }^{4,26}$, graph-based models ${ }^{15,27-29}$, and GFlowNets ${ }^{30}$. While all methods can generate valid molecules and the policy can be fine-tuned via RL, none of the previous methods jointly address sample efficiency and a reliable mechanism to mitigate mode collapse. We note that GFlowNets ${ }^{30}$ by construction can achieve diverse sampling but are not as sample efficient as demonstrated in the PMO benchmark ${ }^{11}$. By contrast, SMILES-based models, particularly REINVENT ${ }^{12,13}$, have been shown to be amongst the most sample efficient molecular generative models, even when compared to the newest proposed models ${ }^{11}$. Moreover, their ability to learn complex molecular distributions ${ }^{31}$ and satisfy multi-parameter optimization (MPO) objectives has been shown in diverse benchmarks, such as GuacaMol ${ }^{32}$, MOSES ${ }^{33}$, and PMO ${ }^{11}$. Our proposed Augmented Memory algorithm builds on this observation and exploits the non-injective nature of SMILES.</p>
<p>Sample Efficiency in Molecular Design. Many existing policy-based RL works for molecular design are based on the REINFORCE [34] algorithm, particularly for models operating on SMILES. Algorithmic alternatives present a unifying theme of using biased gradients to direct the policy towards chemical space with high reward. Neil et al. [35] explored Hill Climbing (HC) and Proximal Policy Optimization (PPO) [35]. Similarly, Atance et al. introduced Best Agent Reminder (BAR) [15] which keeps track of the best agent and reminds the current policy of favorable actions. Thomas et al. introduced Augmented Hill Climbing (AHC) [16], a hybrid of HC and REINVENT's algorithm, which updates the policy at every epoch using only the top-k generated molecules and shows improved sample efficiency. However, sample efficiency by itself is not sufficient for practical applications of molecular generative models as one should aim to generate diverse molecules that satisfy the objective function. To address this limitation, Bjerrum et al. built directly on REINVENT and introduced Double Loop RL [17]. By performing SMILES augmentation, the policy can be updated numerous times per oracle call. Their results showed improved sample efficiency compared to AHC, while maintaining diverse sampling.
Experience Replay for Molecular Design. Experience replay was first proposed by Lin et al. [36] as a mechanism to replay past experiences to the model so that it can learn from the same experience numerous times. Two paradigms in RL are on-policy and off-policy where the model's actions are dictated by its current policy or a separate policy known as the behavior policy, respectively [37]. Experience replay is usually applied in off-policy methods as past experiences are less likely to be applicable to the current policy. In molecular design, experience replay has been proposed by Blaschke et al. [13, 38] and Korshunova et al. [10] to keep track of the best molecules sampled so far, based on their corresponding reward. Notably, both applications of experience replay are for on-policy learning using the REINFORCE [34] algorithm and only Korshunova et al. empirically show its benefit in sparse reward environments. We note that a similar mechanism was proposed by Putin et al. [23] using an external memory.</p>
<h1>3 Proposed Method: Augmented Memory</h1>
<p>In this work, we extend the observations by Korshunova et al. [10] and explicitly show the benefit of experience replay in dense reward environments, i.e., most molecules give at least some reward, for on-policy learning given a static objective function. This static nature means that regardless of the current policy, high-rewarding molecules will always receive the same reward, which supports the efficacy of experience replay in the on-policy setting for molecular generation. Next, we combine elements of HC and SMILES augmentation with experience replay, and propose to update the policy at every fine-tuning epoch using the entire replay buffer. A reward shaping mechanism [39] is introduced by using these extremely biased gradients towards high rewarding chemical space which we show significantly improves sample efficiency. This section describes each component of Augmented Memory (Figure 1) which is capable of performing MPO.
Squared Difference Loss. The molecular generative model builds directly on REINVENT [12, 13] and is an autoregressive SMILES-based RNN using long short-term memory (LSTM) [40] cells. The generative process is cast as an on-policy RL problem by defining the state space, $S_{t}$, and the action space, $A_{t}\left(s_{t}\right)$. Since REINVENT is a language model and samples tokens, $S_{t}$ denotes every intermediate sequence of tokens leading up to the fully constructed SMILES and $A_{t}\left(s_{t}\right)$ are the token sampling probabilities at every intermediate state. $A_{t}\left(s_{t}\right)$ is controlled by the policy, $\pi_{\theta}$, which is parameterized by the RNN. An assumption is that the SMILES generation process is Markovian (Equation 1):</p>
<p>$$
P(x)=\prod_{t=1}^{T} P\left(s_{t} \mid s_{t-1}, s_{t-2}, \ldots, s_{1}\right)
$$</p>
<p>The Augmented Likelihood is defined as a linear combination between the Prior Likelihood and the scoring function, $S$, which returns a reward denoting the desirability of a given molecule and modulated by a hyperparameter sigma, $\sigma$ (Equation 2). The Prior Likelihood term acts to ensure the generated SMILES are syntactically valid, and has been shown to empirically enforce reasonable chemistry [12, 16].</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Augmented Memory. (a) The proposed method proceeds via four steps: 1. generate a batch of SMILES according to the current policy. 2. Compute the reward for the SMILES given the objective function. 3. Update the replay buffer to keep only the top <em>K</em> molecules. Optionally, remove molecules from the replay buffer to discourage further sampling of specific scaffolds. Perform SMILES augmentation of both the sampled batch and the entire replay buffer. 4. Update the Agent and repeat step 3 <em>N</em> times. (b) Schematic of the intended behavior. Augmenting the entire replay buffer and updating the Agent repeatedly directs chemical space exploration to areas of high reward.</p>
<p>$$
\log \pi_{\theta_{\text{Augmented}}}=\log \pi_{\theta_{\text{Prior}}}+\sigma S(x)
$$</p>
<p>The policy is directly optimized by minimizing the squared difference between the Augmented Likelihood and the Agent Likelihood given a sampled batch, <em>B</em>, of SMILES constructed following the actions, <em>a</em> ∈ <em>A</em>* (Equation 3):</p>
<p>$$
L(\theta) = \frac{1}{|B|} \left[ \sum_{a \in A^*} (\log \pi_{\theta_{\text{Augmented}}} - \log \pi_{\theta_{\text{Agent}}} ) \right]^2
$$</p>
<p>Taking the gradient of the loss function yields Equation 4:</p>
<p>$$
\nabla_\theta L(\theta) = -2 \frac{1}{|B|} \left[ \sum_{a \in A^<em>} \log \pi_{\theta_{\text{Augmented}}} - \log \pi_{\theta_{\text{Agent}}} \right] \sum_{a \in A^</em>} \nabla_\theta \log \pi_{\theta_{\text{Agent}}}
$$</p>
<p><strong>Equivalency of the Squared Difference Loss to Policy Gradient Optimization.</strong> Minimizing the loss function described in Equation 3 is equivalent to maximizing the expected reward. To show this equivalency, we follow Fialková et al. [41] and start with the following objective, where <em>R</em> is the reward function (Equation 5):</p>
<p>$$
J(\theta) = \mathbb{E}<em>{a_t \sim \pi</em>\theta} \left[ \sum_{t=0}^{T} R(a_t, s_t) \right]
$$</p>
<p>Following the REINFORCE [34] algorithm and applying the log-derivative trick yields Equation 6 for the gradient:</p>
<p>$$
\nabla_\theta J(\theta) = \mathbb{E}<em>{a_t \sim \pi</em>\theta} \left[ \sum_{t=0}^{T} R(a_t, s_t) \nabla_\theta \log \pi_\theta (a_t | s_t) \right]
$$</p>
<p>Computing the expectation is intractable and is instead approximated using the mean of a sampled batch, $B$, of SMILES constructed by choosing actions, $a \in A^{*}$. Further noting that $\log \pi_{\theta}\left(a_{t} \mid s_{t}\right)=$ $\log \pi_{\theta_{\text {Agent }}}$ yields Equation 7:</p>
<p>$$
\nabla_{\theta} J(\theta)=\frac{1}{|B|} \sum_{t=0}^{T} \sum_{a \in A^{*}} R\left(a_{t}, s_{t}\right) \nabla_{\theta} \log \pi_{\theta_{\text {Agent }}}
$$</p>
<p>Finally, the reward is defined as $R\left(a_{t}, s_{t}\right)=\log \pi_{\theta_{\text {Augmented }}}-\log \pi_{\theta_{\text {Agent }}}$. The corresponding gradient expression (Equation 7) is now equivalent to the gradient of the loss function (Equation 4) up to a constant factor. Further details on the derivation and algorithm is in Appendix K.
SMILES Augmentation. SMILES are non-injective and yield different sequence representations given a different atom numbering in the molecular graph, i.e., augmented SMILES. SMILES-based molecular generative models have taken advantage of this to train performant models under low-data regimes, e.g., by artificially increasing the dataset size via data augmentation ${ }^{42}$, and to increase chemical space generalizability ${ }^{41}$ by training a Prior model on augmented SMILES. Similar to Bjerrum et al. ${ }^{17}$, we reuse scores obtained from the oracle to update the Agent multiple times by passing different augmented SMILES representations.
Experience Replay. Experience replay is implemented in REINVENT as a buffer that stores a pre-defined maximum number of the highest rewarding SMILES sampled so far ( 100 in this work). Usually, during each sampling, a subset of the buffer is replayed to the Agent ${ }^{13}$. In our proposed method, all SMILES in the buffer are augmented and using their corresponding reward, the Agent is updated multiple times according to the loss function given in Equation 3.
Selective Memory Purge. Blaschke et al. ${ }^{38}$ introduced memory-assisted RL to enforce diverse sampling in REINVENT via diversity filters (DFs). During the generative process, the scaffolds of sampled molecules are stored in 'buckets' with pre-defined and limited size. Once a bucket has been fully populated, further sampling of the same scaffold results in zero reward. We incorporate this heuristic in our proposed method called Selective Memory Purge to enforce diversity. At every epoch, the replay buffer is purged of any scaffolds that are penalized by the DF. The effect is that each augmentation round only updates the Agent with scaffolds that still receive reward, preventing the Agent from becoming myopic and leading to sub-optimal convergence.</p>
<h1>4 Results \&amp; Discussion</h1>
<p>We designed three experiments to assess our method. First, we explicitly demonstrate the importance of experience replay and identify optimal parameters for Augmented Memory using the Aripiprazole Similarity experiment. Next, we benchmark its performance on the Practical Molecular Optimization (PMO) ${ }^{11}$ benchmark containing 23 tasks. Lastly, we demonstrate the practical applicability of our method on a Dopamine Type 2 Receptor (DRD2) drug discovery case study.
Baselines. In experiments 1 and 3, the baseline algorithms include REINVENT ${ }^{12,13}$, Augmented Hill Climbing (AHC) ${ }^{16}$, Best Agent Reminder (BAR) ${ }^{15}$, and Double Loop RL ${ }^{17}$ as all algorithms are formulated using REINVENT's ${ }^{12,13}$ loss function with shared hyperparameters. Thus, the experiments isolate the effect of SMILES augmentation and experience replay on sample efficiency. Moreover, SMILES-based models are strong baselines as demonstrated in the GuacaMol ${ }^{32}$, MOSES ${ }^{33}$, and $\mathrm{PMO}^{11}$ benchmarks. In the PMO benchmarking experiment, we compare our method's performance to diverse models. The Appendix includes further details on the dataset, hyperparameters, and ablation studies.</p>
<h3>4.1 Aripiprazole Similarity</h3>
<p>The aripiprazole similarity task is from the GuacaMol benchmark ${ }^{32}$ and the objective is to successfully sample aripiprazole. This experiment was used to demonstrate the importance of experience replay and compare Augmented Memory to existing policy-based algorithms. As the code for Double Loop RL is not released, we took the values reported in the their paper which holds as the method was also built directly on REINVENT ${ }^{12,13}$, uses the same pre-trained Prior, and hyperparameters. Moreover,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Augmented Memory and Selective Memory Purge significantly improve sample efficiency and enable diverse sampling. The shaded region represents the minimum and maximum scores across triplicate runs. (a) Comparing sample efficiency of on-policy algorithms. Experience Replay (ER) improves all base algorithms. The values for Double Loop RL ${ }^{17}$ are taken from the original paper as the code is not released. The black dots are the mean at 0.5 and 0.8 and the standard deviation across triplicate runs. (b) The average score for aripiprazole similarity. In the Diversity Filter and Memory Purge experiments, scores of 0 are given if the Agent repeatedly samples the same scaffold. (c) Pooled Tanimoto similarities. Memory purge rediscovers aripiprazole and has a flatter distribution, suggesting increased exploration. (d) UMAP ${ }^{44}$ and IntDiv1 ${ }^{33}$ metric showing qualitatively and quantitatively increased exploration using Memory Purge. The plots were generated using ChemCharts ${ }^{45}$ (e) The negative log-likelihood of sampling aripiprazole across the full generative experiments.
in the studies presenting $\mathrm{AHC}^{16}$ and $\mathrm{BAR}^{15}$, experience replay was not used but we provide an implementation and further compare their performance.</p>
<p>Experience Replay is Vital for Sample Efficiency. We first identified the optimal number of augmentation rounds for Augmented Memory as two for training stability. Increasing the augmentation rounds can further improve sample efficiency but can lead to mode collapse (Appendix A). Next, we compare REINVENT ${ }^{12,13}, \mathrm{AHC}^{16}, \mathrm{BAR}^{15}$, and Double Loop RL ${ }^{17}$ with our method. Figure 2 plots the number of oracle calls to explicitly highlight the computational budget. Augmented Memory significantly outperforms all other algorithms and reaches a score of 0.8 with 6,144 oracle calls (average over 100 replicates). Double Loop RL ${ }^{17}$ uses experience replay and is the second most sample efficient algorithm and reaches a score of 0.8 after $12,416 \pm 1,984$ oracle calls (as stated in their paper), which is twice the number of oracle calls required compared to our method. Moreover, the key observation we convey is that experience replay improves upon the base algorithm in all cases (Figure 2). For example, $\mathrm{AHC}^{16}$ with the newly implemented experience replay reaches a score of 0.8 , but with more than triple the oracle calls $(19,200)$. Our observations around experience replay are supported by previous works ${ }^{10,13}$. Finally, we show that augmentation is crucial for the enhanced sample efficiency in Appendix D.</p>
<p>Selective Memory Purge Enables Diverse Sampling while Retaining Efficiency. Figure 2 demonstrates the enhanced sample efficiency of Augmented Memory but real-world applications of molecular generative models require the ability to sample diverse solutions. While aripiprazole is inherently an exploitation task, it can be framed as an exploration task if the goal is rephrased as: rediscover the target molecule and generate similar molecules. Using this formulation, we design experiments to</p>
<p>prove that Augmented Memory can achieve diverse sampling. Figure 2 shows the training plot across three methods: pure exploitation where diversity is not enforced, exploration using a diversity filter $(\mathrm{DF})^{38}$, and Selective Memory Purge. In the pure exploitation scenario, aripiprazole is rediscovered quickly (score of 1.0). In the DF experiment where a score of 0 is assigned for scaffolds sampled more than 25 times, mode collapse is observed (Figure 2). By contrast, Selective Memory Purge maintains a moderate average score. The results from triplicate experiments were pooled to investigate the density of aripiprazole similarities (Figure 2). As expected, in the pure exploitation scenario, most molecules are aripiprazole (Tanimoto similarity of 1.0). DF and Selective Memory Purge both enforce a wider distribution of similarities, but to varying degrees. In the shaded region (rediscovery score), Selective Memory Purge only shows a small density relative to DF. Moreover, Selective Memory Purge shows a flatter distribution of similarities. These observations demonstrate that Selective Memory Purge rediscovers the target molecule and enforces increased exploration compared to DF. To investigate this further, the same pooled dataset was embedded using Uniform Manifold Approximation and Projection (UMAP) ${ }^{44}$ to visualize the chemical space. Qualitatively and quantitatively, Selective Memory Purge covers a larger chemical space (Figure 2). The internal diversity (IntDiv1) metric was calculated as proposed in the MOSES benchmark ${ }^{33}$, and measures the diversity within a set of generated molecules. Finally, we save the Agent states at every 5 epochs across the entire generative run and trace the NLL of sampling aripiprazole (Figure 2). It is evident that Selective Memory Purge can discourage sampling of the target molecule more effectively than only using a DF. Importantly, the NLL also diverges, suggesting that the Agent is increasingly moving to chemical space dissimilar to aripiprazole as the generative experiment progresses.</p>
<h1>4.2 Practical Molecular Optimization (PMO) Benchmark</h1>
<p>Table 1: Performance of Augmented Memory, REINVENT ${ }^{12,13}$, AHC $^{16}$, and BAR $^{15}$ on the PMO benchmark ${ }^{11}$. The mean and standard deviation of the AUC Top-10 is reported. The values obtained for REINVENT differ slightly from the PMO paper as we performed 10 independent runs compared to 5. Best performance is bolded and is relative to all models in the benchmark. * denotes superior performance to REINVENT but not overall, compared to other models in the benchmark. We note however, that we take the AUC Top-10 values for the other models as is from the PMO paper. If they were re-run with 10 different seeds (instead of 5), the values may decrease as was observed for REINVENT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark <br> Task</th>
<th style="text-align: center;">Augmented Memory</th>
<th style="text-align: center;">REINVENT</th>
<th style="text-align: center;">AHC <br> Replay</th>
<th style="text-align: center;">BAR <br> Replay</th>
<th style="text-align: center;">AHC</th>
<th style="text-align: center;">BAR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">albuterol_similarity</td>
<td style="text-align: center;">$0.913 \pm 0.009$</td>
<td style="text-align: center;">$0.871 \pm 0.031$</td>
<td style="text-align: center;">$0.792 \pm 0.030$</td>
<td style="text-align: center;">$0.700 \pm 0.024$</td>
<td style="text-align: center;">$0.745 \pm 0.024$</td>
<td style="text-align: center;">$0.633 \pm 0.031$</td>
</tr>
<tr>
<td style="text-align: center;">amlodipine_mpo</td>
<td style="text-align: center;">$0.691 \pm 0.047$</td>
<td style="text-align: center;">$0.657 \pm 0.025$</td>
<td style="text-align: center;">$0.596 \pm 0.023$</td>
<td style="text-align: center;">$0.538 \pm 0.019$</td>
<td style="text-align: center;">$0.578 \pm 0.012$</td>
<td style="text-align: center;">$0.523 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: center;">celecoxib_rediscovery</td>
<td style="text-align: center;">$0.796 \pm 0.008$</td>
<td style="text-align: center;">$0.717 \pm 0.048$</td>
<td style="text-align: center;">$0.697 \pm 0.029$</td>
<td style="text-align: center;">$0.563 \pm 0.043$</td>
<td style="text-align: center;">$0.583 \pm 0.070$</td>
<td style="text-align: center;">$0.437 \pm 0.025$</td>
</tr>
<tr>
<td style="text-align: center;">deco_hop</td>
<td style="text-align: center;">$0.658 \pm 0.024$</td>
<td style="text-align: center;">$0.672 \pm 0.052$</td>
<td style="text-align: center;">$0.650 \pm 0.030$</td>
<td style="text-align: center;">$0.589 \pm 0.010$</td>
<td style="text-align: center;">$0.632 \pm 0.032$</td>
<td style="text-align: center;">$0.579 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: center;">drd2</td>
<td style="text-align: center;">$0.963 \pm 0.006^{*}$</td>
<td style="text-align: center;">$0.939 \pm 0.012$</td>
<td style="text-align: center;">$0.913 \pm 0.011$</td>
<td style="text-align: center;">$0.916 \pm 0.012$</td>
<td style="text-align: center;">$0.912 \pm 0.009$</td>
<td style="text-align: center;">$0.899 \pm 0.027$</td>
</tr>
<tr>
<td style="text-align: center;">fexofenadine_mpo</td>
<td style="text-align: center;">$0.859 \pm 0.009$</td>
<td style="text-align: center;">$0.783 \pm 0.021$</td>
<td style="text-align: center;">$0.747 \pm 0.004$</td>
<td style="text-align: center;">$0.708 \pm 0.010$</td>
<td style="text-align: center;">$0.749 \pm 0.005$</td>
<td style="text-align: center;">$0.692 \pm 0.009$</td>
</tr>
<tr>
<td style="text-align: center;">gsk3b</td>
<td style="text-align: center;">$0.881 \pm 0.021$</td>
<td style="text-align: center;">$0.870 \pm 0.026$</td>
<td style="text-align: center;">$0.819 \pm 0.025$</td>
<td style="text-align: center;">$0.744 \pm 0.021$</td>
<td style="text-align: center;">$0.800 \pm 0.021$</td>
<td style="text-align: center;">$0.686 \pm 0.068$</td>
</tr>
<tr>
<td style="text-align: center;">isomers_c7h8n2o2</td>
<td style="text-align: center;">$0.853 \pm 0.087$</td>
<td style="text-align: center;">$0.856 \pm 0.042$</td>
<td style="text-align: center;">$0.682 \pm 0.037$</td>
<td style="text-align: center;">$0.741 \pm 0.064$</td>
<td style="text-align: center;">$0.631 \pm 0.084$</td>
<td style="text-align: center;">$0.713 \pm 0.058$</td>
</tr>
<tr>
<td style="text-align: center;">isomers_c9h10n2o2pf2cl</td>
<td style="text-align: center;">$0.736 \pm 0.051^{*}$</td>
<td style="text-align: center;">$0.641 \pm 0.038$</td>
<td style="text-align: center;">$0.276 \pm 0.133$</td>
<td style="text-align: center;">$0.612 \pm 0.054$</td>
<td style="text-align: center;">$0.191 \pm 0.096$</td>
<td style="text-align: center;">$0.508 \pm 0.066$</td>
</tr>
<tr>
<td style="text-align: center;">jnk3</td>
<td style="text-align: center;">$0.739 \pm 0.110$</td>
<td style="text-align: center;">$0.723 \pm 0.147$</td>
<td style="text-align: center;">$0.649 \pm 0.056$</td>
<td style="text-align: center;">$0.555 \pm 0.089$</td>
<td style="text-align: center;">$0.616 \pm 0.092$</td>
<td style="text-align: center;">$0.511 \pm 0.092$</td>
</tr>
<tr>
<td style="text-align: center;">median1</td>
<td style="text-align: center;">$0.326 \pm 0.013$</td>
<td style="text-align: center;">$0.368 \pm 0.011$</td>
<td style="text-align: center;">$0.346 \pm 0.008$</td>
<td style="text-align: center;">$0.286 \pm 0.007$</td>
<td style="text-align: center;">$0.338 \pm 0.014$</td>
<td style="text-align: center;">$0.269 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: center;">median2</td>
<td style="text-align: center;">$0.291 \pm 0.008^{*}$</td>
<td style="text-align: center;">$0.279 \pm 0.005$</td>
<td style="text-align: center;">$0.273 \pm 0.005$</td>
<td style="text-align: center;">$0.218 \pm 0.008$</td>
<td style="text-align: center;">$0.265 \pm 0.005$</td>
<td style="text-align: center;">$0.199 \pm 0.006$</td>
</tr>
<tr>
<td style="text-align: center;">mestranol_similarity</td>
<td style="text-align: center;">$0.750 \pm 0.049$</td>
<td style="text-align: center;">$0.637 \pm 0.041$</td>
<td style="text-align: center;">$0.599 \pm 0.031$</td>
<td style="text-align: center;">$0.463 \pm 0.027$</td>
<td style="text-align: center;">$0.561 \pm 0.022$</td>
<td style="text-align: center;">$0.444 \pm 0.017$</td>
</tr>
<tr>
<td style="text-align: center;">osimertinib_mpo</td>
<td style="text-align: center;">$0.855 \pm 0.004$</td>
<td style="text-align: center;">$0.836 \pm 0.007$</td>
<td style="text-align: center;">$0.810 \pm 0.003$</td>
<td style="text-align: center;">$0.789 \pm 0.012$</td>
<td style="text-align: center;">$0.809 \pm 0.002$</td>
<td style="text-align: center;">$0.792 \pm 0.004$</td>
</tr>
<tr>
<td style="text-align: center;">perindopril_mpo</td>
<td style="text-align: center;">$0.613 \pm 0.015$</td>
<td style="text-align: center;">$0.561 \pm 0.019$</td>
<td style="text-align: center;">$0.487 \pm 0.012$</td>
<td style="text-align: center;">$0.468 \pm 0.012$</td>
<td style="text-align: center;">$0.482 \pm 0.008$</td>
<td style="text-align: center;">$0.455 \pm 0.011$</td>
</tr>
<tr>
<td style="text-align: center;">qed</td>
<td style="text-align: center;">$0.942 \pm 0.000$</td>
<td style="text-align: center;">$0.941 \pm 0.000$</td>
<td style="text-align: center;">$0.941 \pm 0.000$</td>
<td style="text-align: center;">$0.939 \pm 0.003$</td>
<td style="text-align: center;">$0.941 \pm 0.000$</td>
<td style="text-align: center;">$0.932 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: center;">ranolazine_mpo_mpo</td>
<td style="text-align: center;">$0.801 \pm 0.006$</td>
<td style="text-align: center;">$0.768 \pm 0.008$</td>
<td style="text-align: center;">$0.721 \pm 0.00$</td>
<td style="text-align: center;">$0.704 \pm 0.017$</td>
<td style="text-align: center;">$0.722 \pm 0.008$</td>
<td style="text-align: center;">$0.700 \pm 0.021$</td>
</tr>
<tr>
<td style="text-align: center;">scaffold_hop</td>
<td style="text-align: center;">$0.567 \pm 0.008$</td>
<td style="text-align: center;">$0.556 \pm 0.019$</td>
<td style="text-align: center;">$0.535 \pm 0.007$</td>
<td style="text-align: center;">$0.477 \pm 0.010$</td>
<td style="text-align: center;">$0.525 \pm 0.008$</td>
<td style="text-align: center;">$0.464 \pm 0.005$</td>
</tr>
<tr>
<td style="text-align: center;">sitagliptin_mpo</td>
<td style="text-align: center;">$0.284 \pm 0.050^{*}$</td>
<td style="text-align: center;">$0.049 \pm 0.067$</td>
<td style="text-align: center;">$0.022 \pm 0.008$</td>
<td style="text-align: center;">$0.126 \pm 0.049$</td>
<td style="text-align: center;">$0.028 \pm 0.011$</td>
<td style="text-align: center;">$0.070 \pm 0.020$</td>
</tr>
<tr>
<td style="text-align: center;">thiothixene_rediscovery</td>
<td style="text-align: center;">$0.550 \pm 0.041^{*}$</td>
<td style="text-align: center;">$0.531 \pm 0.036$</td>
<td style="text-align: center;">$0.519 \pm 0.012$</td>
<td style="text-align: center;">$0.396 \pm 0.011$</td>
<td style="text-align: center;">$0.467 \pm 0.032$</td>
<td style="text-align: center;">$0.347 \pm 0.013$</td>
</tr>
<tr>
<td style="text-align: center;">troglitazone_rediscovery</td>
<td style="text-align: center;">$0.540 \pm 0.048$</td>
<td style="text-align: center;">$0.428 \pm 0.028$</td>
<td style="text-align: center;">$0.409 \pm 0.020$</td>
<td style="text-align: center;">$0.301 \pm 0.007$</td>
<td style="text-align: center;">$0.371 \pm 0.019$</td>
<td style="text-align: center;">$0.279 \pm 0.007$</td>
</tr>
<tr>
<td style="text-align: center;">valsartan_smarts</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
<td style="text-align: center;">$0.091 \pm 0.273$</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
<td style="text-align: center;">$0.000 \pm 0.000$</td>
</tr>
<tr>
<td style="text-align: center;">zaleplon_mpo</td>
<td style="text-align: center;">$0.394 \pm 0.026$</td>
<td style="text-align: center;">$0.269 \pm 0.083$</td>
<td style="text-align: center;">$0.072 \pm 0.032$</td>
<td style="text-align: center;">$0.319 \pm 0.033$</td>
<td style="text-align: center;">$0.047 \pm 0.013$</td>
<td style="text-align: center;">$0.294 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: center;">Sum of AUC Top-10 ( $\uparrow$ )</td>
<td style="text-align: center;">15.002</td>
<td style="text-align: center;">14.016</td>
<td style="text-align: center;">12.555</td>
<td style="text-align: center;">12.152</td>
<td style="text-align: center;">11.993</td>
<td style="text-align: center;">11.426</td>
</tr>
<tr>
<td style="text-align: center;">PMO Rank ( $n / 29$ )</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">14</td>
</tr>
</tbody>
</table>
<p>The main motivation of our method is to improve sample efficiency. This would enable molecular generative models to explicitly optimize more expensive oracles which can afford increased predictive accuracy. We benchmark our method on the PMO benchmark proposed by Gao et al. ${ }^{11}$ which restricts the number of oracle calls to 10,000 and encompasses 23 tasks. The metric used is the Area Under the Curve (AUC) for the top 10 molecules. We note that Thomas et al. ${ }^{18}$ proposed a modified AUC Top-10 metric that incorporates diversity, but we omit comparison as the formulation can be subjective. The current AUC Top-10 metric assesses sample efficiency which is our focus. In the original PMO paper, REINVENT ${ }^{12}$ (with experience replay) is the most sample efficient model. We compare our method directly to REINVENT, BAR ${ }^{15}$, and $\mathrm{AHC}^{16}$ which reports improved sample efficiency compared to REINVENT and is open-sourced. We also add experience replay to BAR and AHC to further highlight its importance for sample efficiency. For a more statistically convincing comparison, we perform 10 independent runs (using 10 different seeds) compared to 5 used in the original PMO paper as the authors benchmarked 25 models, which imposed a significant computational cost. The optimal hyperparameters for REINVENT and AHC were used as provided in the PMO repository. We perform hyperparameter optimization for BAR following the PMO protocol (Appendix G) and Augmented Memory was run using REINVENT's optimal hyperparameters. The results show Augmented Memory significantly outperforms all methods and achieves superior performance to REINVENT across 19/23 benchmark tasks (Table 1). Moreover, the results reinforce the importance of experience replay as it improves the sample efficiency of both BAR and AHC, although neither outperform REINVENT. Finally, in the PMO paper ${ }^{11}$, models were ranked based on the sum of the total AUC Top-10 and adjacently ranked models typically differ by 0.3-0.5. Augmented Memory outperforms REINVENT by 0.986 AUC Top-10 and yields a new state-of-the-art performance on the PMO benchmark.</p>
<h1>4.3 Dopamine Type 2 Receptor (DRD2) Case Study</h1>
<p>To prove that Augmented Memory can perform MPO, we formulate a case study to generate potential dopamine type 2 receptor (DRD2) inhibitors ${ }^{46}$ by explicitly optimizing molecular docking scores (Figure 3). For accessibility and reproducibility, we use the open-source AutoDock Vina ${ }^{47}$ for docking. A well-known failure mode of docking algorithms is they reward lipophilic molecules, e.g., possessing many carbon atoms, which can be promiscuous binders ${ }^{48,49}$. Bjerrum et al. ${ }^{17}$ consider this and enforced molecules to possess a molecular weight (MW) $&lt;500$ Da but this is insufficient in preventing exploitation of the docking algorithm as we show in Appendix E. Following Guo et al. ${ }^{50}$, we design the MPO as follows: MW $&lt;500 \mathrm{Da}$, maximize QED ${ }^{51}$, and minimize the Vina docking score, for chemical plausibility. AutoDock Vina is a relatively expensive oracle and we impose a computational budget of 9,600 oracle calls, similar to the 10,000 oracle calls enforced in the $\mathrm{PMO}^{11}$ benchmark. We compare Augmented Memory, REINVENT ${ }^{12,13}$, $\mathrm{AHC}^{16}$, and BAR ${ }^{15}$ as the optimization algorithms. To mimic a real-world drug discovery pipeline that discards unpromising molecules, we pool the results from triplicate experiments with the following filter: MW $&lt;500 \mathrm{Da}$, QED $&gt;0.4$ (the DRD2 drug molecule, risperidone, has a QED of 0.66 ), and Vina docking score $&lt;-9.4$ (risperidone's score). Figure 3 shows the docking scores distribution with the number of molecules passing the filter and the IntDiv1 ${ }^{33}$ score annotated. Firstly, experience replay improves all base algorithms, further reinforcing its importance. Secondly, all algorithms with the exception of Augmented Memory perform similarly. Compared to AHC with experience replay, which is the second most sample efficient algorithm, Augmented Memory generates over 2,000 more molecules with a better docking score than risperidone, with a small trade-off in diversity (IntDiv1 of 0.801). We emphasize that AHC with experience replay does not even generate 2,000 molecules passing the filter. To further prove the optimization capability, Figure 3 shows a contour plot of the QED-Vina score distribution for Augmented Memory and AHC with experience replay. It is clear that the joint QED-Vina score distribution for Augmented Memory is shifted to higher QED values and lower Vina scores. The black dot is risperidone and the bulk density of AHC does not possess a better docking score. Finally, Figure 3 shows an example binding pose of a molecule generated using Augmented Memory. We highlight that the chemical plausibility of the structure is enforced precisely because MW and QED are also included in the MPO objective, thus representing a more realistic case study.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Dopamine type 2 receptor (DRD2) molecular docking case study. PDB ID: 6CM4. (a) Docking scores distribution of all compared algorithms. (b) Augmented Memory jointly optimizes QED and Vina docking score, demonstrating the ability to perform MPO. (c) Binding pose of a generated molecule using Augmented Memory. The three components in the objective function: MW $&lt;500$, QED, and Vina docking score are all optimized.</p>
<h1>5 Conclusion</h1>
<p>In this work, we explicitly show that experience replay is vital for sample efficiency. We propose Augmented Memory which capitalizes on this observation and applies SMILES augmentation on the replay buffer to update the Agent multiple times per oracle call. Compared to existing algorithms, Augmented Memory significantly improves sample efficiency and is able to sample diverse solutions using the newly proposed Selective Memory Purge heuristic. We benchmark Augmented Memory on the PMO benchmark ${ }^{11}$ and achieve a new state-of-the-art performance, outperforming the previous state-of-the-art on 19/23 tasks and by a total sum of 0.986 AUC Top-10. Next, we show the practical application of Augmented Memory by mimicking a more realistic drug discovery task. Our method significantly outperforms existing algorithms, as assessed by the property profile of the generated molecules, and can perform MPO. We note that in particularly sparse reward landscapes ${ }^{10}$, the enhanced sample efficiency of Augmented Memory may be diminished as it becomes more difficult to populate the replay buffer with high rewarding molecules. Future work will investigate this scenario thoroughly and algorithmic modifications to couple additional local chemical space exploration around high rewarding molecules may better handle sparsity. Finally, this work opens up future integration of Augmented Memory with curriculum learning ${ }^{52}$, the use of more expensive oracles given a limited computational budget, and further provides insights into experience replay for molecular generative models.</p>
<h2>Acknowledgement</h2>
<p>This publication was created as part of NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.</p>
<h2>References</h2>
<ol>
<li>Benjamin Sanchez-Lengeling and Alán Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360-365, July 2018. doi: 10.1126/science.aat2663. URL https://www.science.org/doi/10.1126/science. aat2663. Publisher: American Association for the Advancement of Science.</li>
<li>Julia Westermayr, Joe Gilkes, Rhyan Barrett, and Reinhard J. Maurer. High-throughput propertydriven generative design of functional organic molecules. Nat Comput Sci, 3(2):139-148, February 2023. ISSN 2662-8457. doi: 10.1038/s43588-022-00391-1. URL https://www. nature.com/articles/s43588-022-00391-1. Number: 2 Publisher: Nature Publishing Group.</li>
<li>Jiankun Lyu, Sheng Wang, Trent E Balius, Isha Singh, Anat Levit, Yurii S Moroz, Matthew J O'Meara, Tao Che, Enkhjargal Algaa, Kateryna Tolmachova, et al. Ultra-large library docking for discovering new chemotypes. Nature, 566(7743):224-229, 2019.</li>
<li>Alex Zhavoronkov, Yan A. Ivanenkov, Alex Aliper, Mark S. Veselov, Vladimir A. Aladinskiy, Anastasiya V. Aladinskaya, Victor A. Terentiev, Daniil A. Polykovskiy, Maksim D. Kuznetsov, Arip Asadulaev, Yury Volkov, Artem Zholus, Rim R. Shayakhmetov, Alexander Zhebrak, Lidiya I. Minaeva, Bogdan A. Zagribelnyy, Lennart H. Lee, Richard Soll, David Madge, Li Xing, Tao Guo, and Alán Aspuru-Guzik. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat Biotechnol, 37(9):1038-1040, September 2019. ISSN 1546-1696. doi: 10.1038/s41587-019-0224-x. URL https://www.nature.com/articles/ s41587-019-0224-x. Number: 9 Publisher: Nature Publishing Group.</li>
<li>
<p>Feng Ren, Xiao Ding, Min Zheng, Mikhail Korzinkin, Xin Cai, Wei Zhu, Alexey Mantsyzov, Alex Aliper, Vladimir Aladinskiy, Zhongying Cao, Shanshan Kong, Xi Long, Bonnie Hei Man Liu, Yingtao Liu, Vladimir Naumov, Anastasia Shneyderman, Ivan V. Ozerov, Ju Wang, Frank W. Pun, Daniil A. Polykovskiy, Chong Sun, Michael Levitt, Alán Aspuru-Guzik, and Alex Zhavoronkov. AlphaFold accelerates artificial intelligence powered drug discovery: efficient discovery of a novel CDK20 small molecule inhibitor. Chemical Science, 14(6):1443-1452, 2023. doi: 10.1039/D2SC05709C. URL https://pubs.rsc.org/en/content/articlelanding/ 2023/sc/d2sc05709c. Publisher: Royal Society of Chemistry.</p>
</li>
<li>
<p>Julius Seumer, Jonathan Kirschner Solberg Hansen, Mogens Brøndsted Nielsen, and Jan H Jensen. Computational evolution of new catalysts for the morita-baylis-hillman reaction. Angewandte Chemie International Edition, page e202218565, 2022.</p>
</li>
<li>Arman A. Sadybekov, Anastasiia V. Sadybekov, Yongfeng Liu, Christos Iliopoulos-Tsoutsouvas, Xi-Ping Huang, Julie Pickett, Blake Houser, Nilkanth Patel, Ngan K. Tran, Fei Tong, Nikolai Zvonok, Manish K. Jain, Olena Savych, Dmytro S. Radchenko, Spyros P. Nikas, Nicos A. Petasis, Yurii S. Moroz, Bryan L. Roth, Alexandros Makriyannis, and Vsevolod Katritch. Synthonbased ligand discovery in virtual libraries of over 11 billion compounds. Nature, 601(7893): 452-459, January 2022. ISSN 1476-4687. doi: 10.1038/s41586-021-04220-9. URL https: //www.nature.com/articles/s41586-021-04220-9. Number: 7893 Publisher: Nature Publishing Group.</li>
<li>Joshua Meyers, Benedek Fabian, and Nathan Brown. De novo molecular design and generative models. Drug Discovery Today, 26(11):2707-2715, November 2021. ISSN 1359-6446. doi: 10. 1016/j.drudis.2021.05.019. URL https://www.sciencedirect.com/science/article/ pii/S1359644621002531.</li>
<li>Atsushi Yoshimori, Yasunobu Asawa, Enzo Kawasaki, Tomohiko Tasaka, Seiji Matsuda, Toru Sekikawa, Satoshi Tanabe, Masahiro Neya, Hideaki Natsugari, and Chisato Kanai. Design and Synthesis of DDR1 Inhibitors with a Desired Pharmacophore Using Deep Generative Models. ChemMedChem, 16(6):955-958, 2021. ISSN 1860-7187. doi: 10.1002/ cmdc.202000786. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cmdc. 202000786. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cmdc.202000786.</li>
<li>Maria Korshunova, Niles Huang, Stephen Capuzzi, Dmytro S. Radchenko, Olena Savych, Yuriy S. Moroz, Carrow I. Wells, Timothy M. Willson, Alexander Tropsha, and Olexandr Isayev. Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds. Commun Chem, 5(1):1-11, October 2022. ISSN 2399-3669. doi: 10.1038/ s42004-022-00733-0. URL https://www.nature.com/articles/s42004-022-00733-0. Number: 1 Publisher: Nature Publishing Group.</li>
<li>Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization, October 2022. URL http://arxiv.org/ abs/2206.12411. arXiv:2206.12411 [cs, q-bio].</li>
<li>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo design through deep reinforcement learning. Journal of Cheminformatics, 9(1):48, September 2017. ISSN 1758-2946. doi: 10.1186/s13321-017-0235-x. URL https://doi.org/10.1186/ s13321-017-0235-x.</li>
<li>Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. REINVENT 2.0: An AI Tool for De Novo Drug Design. J. Chem. Inf. Model., 60(12):5918-5922, December 2020. ISSN 1549-9596. doi: 10.1021/acs.jcim.0c00915. URL https://doi.org/10.1021/acs.jcim. 0c00915. Publisher: American Chemical Society.</li>
<li>David Weininger. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, February 1988. ISSN 0095-2338. doi: 10.1021/ci00057a005. URL https://doi.org/10.1021/ci00057a005. Publisher: American Chemical Society.</li>
<li>Sara Romeo Atance, Juan Viguera Diez, Ola Engkvist, Simon Olsson, and Rocío Mercado. De novo drug design using reinforcement learning with graph-based deep generative models. Journal of Chemical Information and Modeling, 62(20):4863-4872, 2022.</li>
<li>
<p>Morgan Thomas, Noel M. O’Boyle, Andreas Bender, and Chris de Graaf. Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation. Journal of Cheminformatics, 14(1):68, October 2022. ISSN 1758-2946. doi: 10.1186/s13321-022-00646-z. URL https://doi.org/10.1186/s13321-022-00646-z.</p>
</li>
<li>
<p>Esben Jannik Bjerrum, Christian Margreitter, Thomas Blaschke, and Raquel López-Ríos de Castro. Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented SMILES, March 2023. URL http://arxiv.org/abs/2210.12458. arXiv:2210.12458 [physics].</p>
</li>
<li>Morgan Thomas, Noel M O’Boyle, Andreas Bender, and Chris De Graaf. Re-evaluating sample efficiency in de novo molecule generation. arXiv preprint arXiv:2212.01385, 2022.</li>
<li>Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science Advances, 4(7):eaap7885, July 2018. doi: 10.1126/sciadv.aap7885. URL https://www.science.org/doi/10.1126/sciadv.aap7885. Publisher: American Association for the Advancement of Science.</li>
<li>Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): $120-131,2018$.</li>
<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL http://arxiv.org/abs/1406.2661. arXiv:1406.2661 [cs, stat].</li>
<li>Benjamin Sanchez-Lengeling, Carlos Outeiral, Gabriel L Guimaraes, and Alan Aspuru-Guzik. Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic). 2017.</li>
<li>Evgeny Putin, Arip Asadulaev, Yan Ivanenkov, Vladimir Aladinskiy, Benjamin SanchezLengeling, Alán Aspuru-Guzik, and Alex Zhavoronkov. Reinforced Adversarial Neural Computer for de Novo Molecular Design. J. Chem. Inf. Model., 58(6):1194-1204, June 2018. ISSN 1549-9596. doi: 10.1021/acs.jcim.7b00690. URL https://doi.org/10.1021/acs.jcim. 7b00690. Publisher: American Chemical Society.</li>
<li>Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and Alán Aspuru-Guzik. Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models, February 2018. URL http://arxiv.org/abs/1705. 10843. arXiv:1705.10843 [cs, stat].</li>
<li>Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular graphs, September 2022. URL http://arxiv.org/abs/1805.11973. arXiv:1805.11973 [cs, stat].</li>
<li>Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes, December 2022. URL http://arxiv.org/abs/1312.6114. arXiv:1312.6114 [cs, stat].</li>
<li>Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation, February 2019. URL http: //arxiv.org/abs/1806.02473. arXiv:1806.02473 [cs, stat].</li>
<li>Wengong Jin, Dr Regina Barzilay, and Tommi Jaakkola. Multi-Objective Molecule Generation using Interpretable Substructures. In Proceedings of the 37th International Conference on Machine Learning, pages 4849-4859. PMLR, November 2020. URL https://proceedings. mlr.press/v119/jin20b.html. ISSN: 2640-3498.</li>
<li>Rocío Mercado, Tobias Rastemo, Edvard Lindelöf, Günter Klambauer, Ola Engkvist, Hongming Chen, and Esben Jannik Bjerrum. Graph networks for molecular design. Mach. Learn.: Sci. Technol., 2(2):025023, March 2021. ISSN 2632-2153. doi: 10.1088/2632-2153/abcf91. URL https://dx.doi.org/10.1088/2632-2153/abcf91. Publisher: IOP Publishing.</li>
<li>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381-27394, 2021.</li>
<li>
<p>Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1):3293, 2022.</p>
</li>
<li>
<p>Nathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. GuacaMol: Benchmarking Models for de Novo Molecular Design. J. Chem. Inf. Model., 59(3):10961108, March 2019. ISSN 1549-9596, 1549-960X. doi: 10.1021/acs.jcim.8b00839. URL https://pubs.acs.org/doi/10.1021/acs.jcim.8b00839.</p>
</li>
<li>Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alán Aspuru-Guzik, and Alex Zhavoronkov. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Frontiers in Pharmacology, 11, 2020. ISSN 1663-9812. URL https://www.frontiersin.org/articles/10.3389/fphar.2020.565644.</li>
<li>Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach Learn, 8(3):229-256, May 1992. ISSN 1573-0565. doi: 10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696.</li>
<li>Daniel Neil, Marwin Segler, Laura Guasch, Mohamed Ahmed, Dean Plumbley, Matthew Sellwood, and Nathan Brown. EXPLORING DEEP RECURRENT MODELS WITH REIN- FORCEMENT LEARNING FOR MOLECULE DESIGN. 2018.</li>
<li>Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.</li>
<li>William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, and Will Dabney. Revisiting Fundamentals of Experience Replay, July 2020. URL http://arxiv.org/abs/2007.06700. arXiv:2007.06700 [cs, stat].</li>
<li>Thomas Blaschke, Ola Engkvist, Jürgen Bajorath, and Hongming Chen. Memory-assisted reinforcement learning for diverse molecular de novo design. Journal of Cheminformatics, 12 (1):68, November 2020. ISSN 1758-2946. doi: 10.1186/s13321-020-00473-0. URL https: //doi.org/10.1186/s13321-020-00473-0.</li>
<li>Eric Wiewiora. Reward Shaping. In Claude Sammut and Geoffrey I. Webb, editors, Encyclopedia of Machine Learning, pages 863-865. Springer US, Boston, MA, 2010. ISBN 978-0-387-30164-8. doi: 10.1007/978-0-387-30164-8_731. URL https://doi.org/10.1007/ $978-0-387-30164-8 _731$.</li>
<li>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$.</li>
<li>Vendy Fialková, Jiaxi Zhao, Kostas Papadopoulos, Ola Engkvist, Esben Jannik Bjerrum, Thierry Kogej, and Atanas Patronov. LibINVENT: Reaction-based Generative Scaffold Decoration for in Silico Library Design. J. Chem. Inf. Model., 62(9):2046-2063, May 2022. ISSN 1549-9596. doi: 10.1021/acs.jcim.1c00469. URL https://doi.org/10.1021/acs.jcim.1c00469. Publisher: American Chemical Society.</li>
<li>Michael Moret, Lukas Friedrich, Francesca Grisoni, Daniel Merk, and Gisbert Schneider. Generative molecular design in low data regimes. Nat Mach Intell, 2(3):171-180, March 2020. ISSN 2522-5839. doi: 10.1038/s42256-020-0160-y. URL https://www.nature.com/articles/ s42256-020-0160-y. Number: 3 Publisher: Nature Publishing Group.</li>
<li>Josep Arús-Pous, Simon Viet Johansson, Oleksii Prykhodko, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. Randomized SMILES strings improve the quality of molecular generative models. Journal of Cheminformatics, 11 (1):71, November 2019. ISSN 1758-2946. doi: 10.1186/s13321-019-0393-0. URL https: //doi.org/10.1186/s13321-019-0393-0.</li>
<li>Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, September 2020. URL http://arxiv.org/abs/ 1802.03426. arXiv:1802.03426 [cs, stat].</li>
<li>
<p>Chemcharts. https://github.com/SMargreitter/ChemCharts.</p>
</li>
<li>
<p>Sheng Wang, Tao Che, Anat Levit, Brian K Shoichet, Daniel Wacker, and Bryan L Roth. Structure of the d2 dopamine receptor bound to the atypical antipsychotic drug risperidone. Nature, 555 (7695):269-273, 2018.</p>
</li>
<li>Oleg Trott and Arthur J Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455-461, 2010.</li>
<li>John A Arnott and Sonia Lobo Planey. The influence of lipophilicity in drug discovery and design. Expert opinion on drug discovery, 7(10):863-875, 2012.</li>
<li>AkshatKumar Nigam, Robert Pollice, and Alán Aspuru-Guzik. Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design. Digital Discovery, 1(4): 390-404, 2022.</li>
<li>Jeff Guo, Jon Paul Janet, Matthias R. Bauer, Eva Nittinger, Kathryn A. Giblin, Kostas Papadopoulos, Alexey Voronov, Atanas Patronov, Ola Engkvist, and Christian Margreitter. DockStream: a docking wrapper to enhance de novo molecular design. Journal of Cheminformatics, 13(1):89, November 2021. ISSN 1758-2946. doi: 10.1186/s13321-021-00563-7. URL https://doi.org/10.1186/s13321-021-00563-7.</li>
<li>G. Richard Bickerton, Gaia V. Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L. Hopkins. Quantifying the chemical beauty of drugs. Nature Chem, 4(2):90-98, February 2012. ISSN 17554349. doi: 10.1038/nchem.1243. URL https://www.nature.com/articles/nchem.1243. Number: 2 Publisher: Nature Publishing Group.</li>
<li>Jeff Guo, Vendy Fialková, Juan Diego Arango, Christian Margreitter, Jon Paul Janet, Kostas Papadopoulos, Ola Engkvist, and Atanas Patronov. Improving de novo molecular design with curriculum learning. Nat Mach Intell, 4(6):555-563, June 2022. ISSN 2522-5839. doi: 10.1038/ s42256-022-00494-4. URL https://www.nature.com/articles/s42256-022-00494-4. Number: 6 Publisher: Nature Publishing Group.</li>
<li>Anna Gaulton, Louisa J. Bellis, A. Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, and John P. Overington. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic Acids Res, 40 (Database issue):D1100-D1107, January 2012. ISSN 0305-1048. doi: 10.1093/nar/gkr777. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3245175/.</li>
<li>John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. ZINC20—A Free Ultralarge-Scale Chemical Database for Ligand Discovery. J. Chem. Inf. Model., 60 (12):6065-6073, December 2020. ISSN 1549-9596. doi: 10.1021/acs.jcim.0c00675. URL https://doi.org/10.1021/acs.jcim.0c00675. Publisher: American Chemical Society.</li>
<li>Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. PLoS computational biology, 13(7):e1005659, 2017.</li>
<li>Anthony K Rappé, Carla J Casewit, KS Colwell, William A Goddard III, and W Mason Skiff. Uff, a full periodic table force field for molecular mechanics and molecular dynamics simulations. Journal of the American chemical society, 114(25):10024-10035, 1992.</li>
</ol>
<h1>A Tolerability to Augmentation Rounds</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Identifying the optimal augmentation rounds using aripiprazole similarity. The shaded region represents the minimum and maximum scores across triplicate runs.</p>
<p>Similar to Esben et al. ${ }^{17}$ in their proposed Double Loop RL algorithm, increasing the number of augmentation rounds increases susceptibility to mode collapse (Figure 4). We used the aripiprazole similarity task to perform a grid optimization and found two rounds to be optimal for stability. At three rounds, mode collapse is already observed with triplicate runs.</p>
<h2>B Pure Exploitation: Robustness of 2 Augmentation Rounds</h2>
<p>Table 2: Robustness experiments: stability of two augmentation rounds. 100 replicates of aripiprazole similarity was performed using 2 augmentation rounds and the epoch number to reach various average scores are presented. The values for Double Loop RL ${ }^{17}$ are for 10 augmentations which the authors state to be most stable</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Average Score</th>
<th style="text-align: center;">Mean <br> Epochs</th>
<th style="text-align: center;">Minimum <br> Epochs</th>
<th style="text-align: center;">Maximum <br> Epochs</th>
<th style="text-align: center;">Double Loop RL <br> Epochs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.5</td>
<td style="text-align: center;">$64 \pm 6$</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">$93 \pm 9$</td>
</tr>
<tr>
<td style="text-align: left;">0.8</td>
<td style="text-align: center;">$96 \pm 18$</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">$194 \pm 31$</td>
</tr>
<tr>
<td style="text-align: left;">0.9</td>
<td style="text-align: center;">$122 \pm 17$</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">215</td>
<td style="text-align: center;">did not report</td>
</tr>
</tbody>
</table>
<p>Initial screening experiments identified two augmentation rounds to be optimal for training stability. We envisioned in pure exploitation scenarios where Selective Memory Purge is not used, mode collapse may be possible. The rationale being that the Agent is reinforced on the same replay buffer molecules. In the case where the entire replay buffer contains very similar or identical molecules, mode collapse may occur. This is not an issue when using Selective Memory Purge as entries in the replay buffer would be removed, thus preventing the entire buffer containing the same molecules. In most practical applications of molecular generative models, Selective Memory Purge should be used to achieve both exploration and exploitation. However, for full transparency, we report the stability of our proposed method in a pure exploitation scenario. The following insights will be informative if prospective users only want to generate one optimal solution in their generative experiment or want to reproduce the aripiprazole similarity experiment. To preemptively prevent mode collapse, we introduce "mode collapse guard" that purges the replay buffer if 70 percent (empirically we find this to be sufficient) of the buffer contains the exact same reward. For statistical rigour, we perform 100 replicates of aripiprazole similarity and present the results in Table 2. We follow Bjerrum et al. ${ }^{17}$ and present statistics on the epochs it takes to reach various average scores (average Tanimoto similarity of the batch of sampled molecules to aripiprazole) of $0.5,0.8$, and 0.9. The results support the stability of our method even in pure exploitation scenarios. The "mode collapse guard" was activated 14 times across 100 replicates and in all cases except 1, prevents mode collapse. The one exception failed to rediscover aripiprazole (mode collapse at a Tanimoto similarity of 0.78 ). In practical applications, the experiment can be monitored and restarted from a check-point state. Moreover, we comment on the maximum epochs it takes to reach an average score of 0.8 and 0.9 which are, in both cases, more than 5 standard deviations from the mean, thus extremely rare. We compare these results to the performance reported by Bjerrum et al. ${ }^{17}$ in their Double Loop RL work which is the second most sample-efficient algorithm. Their reported values to reach an average score of 0.8 is $194 \pm 31$ using 10 augmentation rounds across triplicate runs. We first note that it is unclear if running their algorithm for 100 replicates would still be stable as it is not open-sourced. Secondly, our worst performance, taking 195 epochs to reach an average score of 0.8 is essentially identical to their mean epoch of 194. Cross-referencing the mean it takes our method, we highlight that Augmented Memory is much more sample-efficient, as we find in the main results.</p>
<h1>C Buffer Size Experiments and Reinforcing with Only Experience Replay</h1>
<p>(a)
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b)
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Investigating changes in the replay buffer size and reinforcing the Agent only with molecules stored in the replay buffer. The shaded region represents the minimum and maximum scores across triplicate runs.</p>
<p>As Augmented Memory revolves around exploiting experience replay, we investigate the efficacy of our method when using different buffer sizes (Figure 5). We again use the aripiprazole similarity task to assess the proposed changes. Interestingly, with the exception of a buffer size of 25 , minimal difference is observed between buffer sizes. We posit that a buffer size of 25 is more susceptible to</p>
<p>mode collapse as it is increasingly likely that the stored molecules are all identical or similar relative to having a larger buffer size. Conversely, our initial hypothesis was that a larger buffer size would decrease sample efficiency. The rationale is that relatively low rewarding molecules may be stored in the buffer and reinforcing on these low rewarding molecules could be counterproductive. Following experiments (Figure 5), this was not the case, at least for the aripiprazole similarity task. Given that the differences in the buffer sizes result in minimal difference and that our hypothesis may be true for other objective functions, we decided to use a buffer size of 100 for main result experiments. Next, we were curious if reinforcing the Agent with only the molecules in the replay buffer would be possible. In these experiments, the sampled molecules in a given epoch were only used to reinforce the Agent once and no augmented forms were used to further reinforce the Agent. Interestingly, minimal difference is observed again (Figure 5). Since the performance is similar, we hypothesize that using augmented forms of the sampled molecules would act to mitigate against mode collapse. This is in agreement with insights from Arús-Pous et al. ${ }^{43}$ that posit SMILES augmentation acts as a regularizer. Therefore, all main result experiments were performed using augmented SMILES from the sampled batch and the buffer.</p>
<h1>D Ablation Study: SMILES Augmentation is a Regularizer</h1>
<p>Table 3: Stability without SMILES augmentation. 100 replicates of aripiprazole similarity was performed using 2 augmentation rounds (but without SMILES augmentation) and the epoch number to reach various average scores are presented. Failed runs did not reach the average score threshold. The epoch numbers for the runs with augmentation are shown in parenthesis for comparison.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Average Score</th>
<th style="text-align: center;">Mean <br> Epochs</th>
<th style="text-align: center;">Minimum <br> Epochs</th>
<th style="text-align: center;">Maximum <br> Epochs</th>
<th style="text-align: center;">Failed Runs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0.5</td>
<td style="text-align: center;">$69(64) \pm 7(6)$</td>
<td style="text-align: center;">$53(51)$</td>
<td style="text-align: center;">$84(79)$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">0.8</td>
<td style="text-align: center;">$115(96) \pm 35(18)$</td>
<td style="text-align: center;">$74(77)$</td>
<td style="text-align: center;">$261(194)$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">0.9</td>
<td style="text-align: center;">$154(122) \pm 43(17)$</td>
<td style="text-align: center;">$94(97)$</td>
<td style="text-align: center;">$297(215)$</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<p>The results in the main text show that experience replay is vital for sampling efficiency. In this section, the question we answer is: "can we just perform multiple rounds of Agent update with the entire replay buffer without SMILES augmentation?" If yes, then the benefits of Augmented Memory can be attributed to simply experience replay. The experimental design is as follows: using the aripiprazole similarity experiment, perform two rounds of Agent update using the entire buffer (size of 100) without SMILES augmentation. This mirrors the optimal parameters of Augmentation Memory of two augmentation rounds and a buffer size of 100 . For statistical rigour, we perform 100 replicates and present the results in Table 3. Compared to Augmented Memory, the average epochs it takes to reach an average score of $0.5,0.8$, and 0.9 is higher (values with augmentation are shown in parentheses and is from Table 2). Importantly, the standard deviation is also much higher, suggesting instability in the runs. This is further supported by some runs not reaching the 0.8 and 0.9 average score thresholds. While monitoring the sampling, we notice that the Agent repeatedly samples the same SMILES, indicating mode collapse. From a probabilistic perspective, the Agent negative log-likelihoods (NLLs) become focused on the replay buffer sequences, suggesting token-level memorization. Moreover, the minimum epochs for the 0.8 and 0.9 average score thresholds are lower than Augmented Memory. While seemingly suggesting higher efficacy compared to Augmented Memory, this observation instead further supports token-level memorization: if by chance the Agent finds favorable SMILES, it will focus only on those SMILES sequences. These insights are supported by previous work from Arús-Pous et al. ${ }^{43}$ which explored the effect of SMILES augmentation on the Prior's NLL on the training data. Specifically, they found that training a Prior without SMILES augmentation can cause token-level memorization, such that the NLL for the specific SMILES sequences in the training data are low. This decreases the generalizability of the trained Prior. Bjerrum et al. ${ }^{17}$ in their Double Loop RL work also posit that reinforcing the Agent on augmented SMILES prevents sequence-wise mode collapse. Our results are in agreement and we show that SMILES augmentation is necessary to ensure the efficacy of Augmented Memory and is itself a regularizer.</p>
<h1>E Dopamine Type 2 Receptor (DRD2) Case Study: Exploiting AutoDock Vina</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Dopamine type 2 receptor (DRD2) case study using the objective function: molecular weight $&lt;500$ Da and minimize Vina docking score. Augmented Memory significantly outperforms other algorithms. The generated molecules, however, are not realistic and shows that Augmented Memory can exploit objective functions in a sample-efficient manner.</p>
<p>This section elaborates on the statement that the experimental design of Bjerrum et al. ${ }^{17}$ in their Double Loop RL work is insufficient in preventing AutoDock Vina ${ }^{47}$ exploitation. Specifically, the drug discovery case study to design potential dopamine type 2 receptor (DRD2) inhibitors was performed using the following objective function: molecular weight (MW) $&lt;500$ Da, maximize QED, and minimize docking score. This is in contrast to the objective function proposed by Bjerrum et al.: molecular MW $&lt;500$ Da and minimize docking score. We perform a set of experiments comparing the sample efficiency of alternative algorithms including REINVENT ${ }^{12,13}$, Best Agent Reminder (BAR) ${ }^{15}$, and Augmented Hill Climbing (AHC) ${ }^{16}$ using this simplified objective function. Similar to the main result experiments, we ran all experiments enforcing an oracle budget of 9,600 calls and show the distribution of Vina scores from triplicate pooled runs. The following filter was applied: MW $&lt;500$ Da and Vina score $&lt;-9.4$ (the Vina score of the reference drug molecules, risperidone). It is evident that Augmented Memory significantly outperforms other algorithms, generating drastically better docking scores. Moreover, there is minimal difference between the performances of the other algorithms. However, we investigate the property profile of the generated molecules in the Augmented Memory experiments and show that the Agent exploits AutoDock Vina in rewarding lipophilic molecules, i.e., all top scoring molecules have extensive aromatic carbon rings (Figure 6). These molecules, while possessing excellent Vina scores, are not realistic. As we emphasize the</p>
<p>usability of Augmented Memory on more realistic case studies to encourage practical applications, we show the set of experiments which also enforce QED ${ }^{51}$ in the main results. QED ensures generated molecules are "drug-like". We end this section by emphasizing that the ability of Augmented Memory to exploit AutoDock Vina is not a weakness and rather, further proves its ability for sample-efficient optimization.</p>
<h1>F Aripiprazole and DRD2 Prior and Hyperparameters</h1>
<p>The random.prior.new pre-trained Prior was used from the REINVENT $2.0^{13}$ repository which was trained on ChEMBL ${ }^{53}$. We note that for the Aripiprazole Similarity experiment, this enables direct comparison to Double Loop RL ${ }^{17}$ as the authors also used the same Prior. The hyperparameters used for Experiment 1: Aripiprazole Similarity and Experiment 3: Dopamine Type 2 Receptor (DRD2) are presented in Table 4.</p>
<p>Table 4: Hyperparameters (default) used in Experiment 1: Aripiprazole Similarity and Experiment 3: Dopamine Type 2 Receptor (DRD2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: center;">Sigma $(\sigma)$</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">k</th>
<th style="text-align: center;">Alpha $(\alpha)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Augmented Memory</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">REINVENT ${ }^{12,13}$</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Augmented Hill-Climbing $(\mathrm{AHC})^{16}$</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Best Agent Reminder $(\mathrm{BAR})^{15}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0.5</td>
</tr>
</tbody>
</table>
<h2>G Practical Molecular Optimization (PMO) Hyperparameters</h2>
<p>Table 5: Hyperparameters used in Experiment 2: Practical Molecular Optimization (PMO) ${ }^{11}$ Benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: center;">Sigma $(\sigma)$</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">k</th>
<th style="text-align: center;">Alpha $(\alpha)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Augmented Memory</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">REINVENT ${ }^{12,13}$</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Augmented Hill-Climbing $(\mathrm{AHC})^{16}$</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Best Agent Reminder $(\mathrm{BAR})^{15}$</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.0005</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0.25</td>
</tr>
</tbody>
</table>
<p>Table 6: Best Agent Reminder (BAR) ${ }^{15}$ hyperparameter tuning for Experiment 2: Practical Molecular Optimization (PMO) ${ }^{11}$. The AUC top-10 was used to assess performance and was based on the protocol proposed in the PMO benchmark: average AUC top-10 across 3 independent runs of zaleplon_mpo and perindopril_mpo.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Sigma $(\sigma)$</th>
<th style="text-align: center;">Alpha $(\alpha)$</th>
<th style="text-align: center;">Top-10 AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">250</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.610</td>
</tr>
<tr>
<td style="text-align: left;">250</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.677</td>
</tr>
<tr>
<td style="text-align: left;">500</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.708</td>
</tr>
<tr>
<td style="text-align: left;">500</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.728</td>
</tr>
<tr>
<td style="text-align: left;">750</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.739</td>
</tr>
<tr>
<td style="text-align: left;">750</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.732</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{1 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 2}$</td>
</tr>
<tr>
<td style="text-align: left;">1000</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.759</td>
</tr>
</tbody>
</table>
<p>The hyperparameters used for the Practical Molecular Optimization (PMO) ${ }^{11}$ benchmark is presented in Table 5. The hyperparameters provided in the PMO repository for REINVENT ${ }^{12,13}$ and $\mathrm{AHC}^{16}$ were used. The hyperparameters for BAR ${ }^{15}$ were tuned according to Table 6 . We note that the default</p>
<p>$\sigma$ hyperparameter is 1 as stated in the BAR repository. However, we found that the resulting AUC Top-10 was much lower than all $\sigma$ values in 6 . Thus, we performed hyperparameter tuning using much larger $\sigma$ values according to the values REINVENT was tuned with.</p>
<h1>H Practical Molecular Optimization (PMO) Augmented Memory and BAR Prior</h1>
<p>Table 7: LSTM model hyperparameters for Augmented Memory and BAR</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Cell Type</th>
<th style="text-align: center;">LSTM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of Layers</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">Embedding Layer Size</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Training Batch Size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">SMILES Training Randomization</td>
<td style="text-align: center;">True</td>
</tr>
</tbody>
</table>
<p>Augmented Memory required training a Prior and follows the protocol from REINVENT ${ }^{13}$ and using the provided ZINC ${ }^{54}$ dataset in the $\mathrm{PMO}^{11}$ repository. Table 7 shows the hyperparameters of the LSTM ${ }^{40}$ network. We note all hyperparameters were kept default and the model was trained for 10 epochs as SMILES validity reached $95 \%$ and the total wall time was 11 minutes 57 seconds. BAR ${ }^{15}$ experiments were run with this same pre-trained Prior.</p>
<h2>I DRD2 Experiment Wall Times</h2>
<p>Table 8: Experiment 3: Dopamine Type 2 Receptor (DRD2) Case Study wall times.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: center;">Wall Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Augmented Memory</td>
<td style="text-align: center;">21 hours 25 minutes $\pm 2$ hours 20 minutes</td>
</tr>
<tr>
<td style="text-align: left;">REINVENT</td>
<td style="text-align: center;">27 hours 7 minutes $\pm 2$ hours 21 minutes</td>
</tr>
<tr>
<td style="text-align: left;">Augmented Hill-Climbing (AHC)</td>
<td style="text-align: center;">32 hours 25 minutes $\pm 5$ hours 38 minutes</td>
</tr>
<tr>
<td style="text-align: left;">Best Agent Reminder (BAR)</td>
<td style="text-align: center;">37 hours 42 minutes $\pm 4$ hours 39 minutes</td>
</tr>
</tbody>
</table>
<p>The wall times for Experiment 3: Dopamine Type 2 Receptor (DRD2) are presented in Table 8. We note that we performed a total of 6 replicates for each algorithm: 3 in the main result experiments and 3 in the exploiting AutoDock Vina experiments (Figure 6). For REINVENT ${ }^{12,13}$, AHC $^{16}$, and BAR $^{15}$, we pool the experiments using experience replay. For example, REINVENT values are reported based on 12 total runs: 3 for main result experiments, 3 for main results experiments with experience replay, 3 for exploiting AutoDock Vina experiments, and 3 for exploiting AutoDock Vina experiments with experience replay. The bottleneck in all experiments is AutoDock Vina ${ }^{47}$ and the wall time is highly variable, depending on the molecules sampled by the Agent. Finally, we note that all experiments were run with a batch size of 64 for 150 epochs. The exception is BAR which was run for 75 epochs as each epoch samples 2 batches of molecules: one from the current Agent and one from the best Agent. All experiments had an AutoDock Vina oracle budget of 9,600 calls. Finally, we comment on the variable wall times of each algorithm despite having a fixed oracle budget. There are two sources of stochasticity. Firstly, the experiments were performed on a shared cluster and compute speed is variable depending on usage. Secondly, docking is itself stochastic and generally requires more search time for larger molecules. Augmented Memory jointly optimizes for Vina, QED $^{31}$, and MW which generally enforces smaller molecules and could be a reason for the faster average compute time.</p>
<h2>J AutoDock Vina DRD2 Receptor Preparation and Docking</h2>
<p>The receptor grid for AutoDock Vina ${ }^{47}$ docking against DRD2 (PDB ID: 6CM4 ${ }^{46}$ ) was performed using DockStream ${ }^{50}$. The PDB file for 6CM4 was first downloaded from the Protein Data Bank.</p>
<p>One monomer unit was extracted and refined using PDBFixer ${ }^{55}$ through the DockStream wrapper. The prepared grid was centered at $(\mathrm{x}, \mathrm{y}, \mathrm{z})=(9.93,5.85,-9.58)$ with a search box of $15 \AA \times 15 \AA \times$ $15 \AA$. Docking for all experiments were performed with DockStream using the following protocol: embed sampled SMILES with RDKit Universal Force Field (UFF) ${ }^{56}$ with 600 maximum convergence iterations and execute AutoDock Vina docking parallelized over 36 CPU cores (Intel(R) Xeon(R) Platinum 8360Y processors).</p>
<h1>K Proof of Loss Function and Policy Gradient Equivalency</h1>
<p>In this section, we show that the loss function used to tune the Agent is equivalent to optimizing the expected reward of the policy following the REINFORCE ${ }^{34}$ algorithm. Molecules are represented as a sequence of tokens given by the Simplified Molecular Input Line Entry System (SMILES) ${ }^{14}$ format and generated in an autoregressive manner. The generative process is Markovian (Equation 8):</p>
<p>$$
P(x)=\prod_{t=1}^{T} P\left(s_{t} \mid s_{t-1}, s_{t-2}, \ldots, s_{1}\right)
$$</p>
<p>Equation 1 states that the probability of generating a given SMILES, $x$, is equal to the product of the probabilities of generating a token at time-step $t$, given the sequence so far at time-step $t-1$. The model is pre-trained on a dataset of molecules (ChEMBL ${ }^{53}$ for the main experiments and ZINC ${ }^{54}$ for the benchmarking experiment) to yield the Prior which is parameterized by the weights $\theta$. The Agent is initialized identical to the Prior but is fine-tuned during the reinforcement learning (RL) process. The Augmented Likelihood is defined as a linear combination between the Prior and a reward term (Equation 9):</p>
<p>$$
\log \pi_{\theta_{\text {Augmented }}}=\log \pi_{\theta_{\text {Prior }}}+\sigma S(x)
$$</p>
<p>$S$ is the reward function assessing the desirability of a sampled molecule and $\sigma$ is a hyperparameter that scales the reward. A higher $\sigma$ places a greater contribution on the reward function and less on the Prior. The Prior is used to ensure generated SMILES are syntactically correct and has been empirically shown to enforce reasonable chemistry. The loss function is defined as the squared difference between the Augmented Likelihood and the Agent Likelihood for a given batch, $B$, of sampled SMILES constructed following the actions, $a \in A^{*}$ (Equation 10):</p>
<p>$$
L(\theta)=\frac{1}{|B|}\left[\sum_{a \in A^{*}}\left(\log \pi_{\theta_{\text {Augmented }}}-\log \pi_{\theta_{\text {Agent }}}\right)\right]^{2}
$$</p>
<p>Taking the derivative with respect to $\theta$ (Equation 11):</p>
<p>$$
\nabla_{\theta} L(\theta)=-2 \frac{1}{|B|}\left[\sum_{a \in A^{<em>}} \log \pi_{\theta_{\text {Augmented }}}-\log \pi_{\theta_{\text {Agent }}}\right] \sum_{a \in A^{</em>}} \nabla_{\theta} \log \pi_{\theta_{\text {Agent }}}
$$</p>
<p>Minimizing $J(\theta)$ tunes the Agent to generate molecules satisfying the reward function.</p>
<p>Following Fialková et al. ${ }^{41}$, we now show that minimizing $J(\theta)$ is equivalent to optimizing the expected reward of the policy. The generative process is cast as an on-policy RL problem by defining the state space, $S_{t}$, and the action space, $A_{t}\left(s_{t}\right) . S_{t}$ denotes every intermediate sequence of tokens leading up to the fully constructed SMILES and $A_{t}\left(s_{t}\right)$ are the token sampling probabilities at every intermediate state. $A_{t}\left(s_{t}\right)$ is controlled by the policy, $\pi_{\theta}$, which is parameterized by the weights, $\theta$, of the neural network. Given a reward function, $R$, the objective is to maximize the expected reward when taking actions defined by the policy (Equation 12):</p>
<p>$$
J(\theta)=\mathbb{E}<em t="t">{a</em>\right)\right]
$$} \sim \pi_{\theta}}\left[\sum_{t=0}^{T} R\left(a_{t}, s_{t</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8973 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8973</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8973</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-0e367d898c9701f09ec3205b39bb19aa677c751f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0e367d898c9701f09ec3205b39bb19aa677c751f" target="_blank">AMR-to-text generation as a Traveling Salesman Problem</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution, which reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.</p>
                <p><strong>Paper Abstract:</strong> The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We at- tack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8973.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8973.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AGTSP-graph-to-string</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asymmetric Generalized Traveling Salesman Problem based graph-to-string ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation and decoding approach that (1) matches an input AMR graph to rooted, connected AMR-fragment -> string rules, (2) constructs an AGTSP graph whose nodes are (concept, rule) pairs grouped by concept, and (3) finds a minimal-cost ordered covering (tour) of fragments using a TSP solver where travel costs are learned with a Maximum Entropy classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AGTSP-based ordering over graph-to-string fragments</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input AMR is covered by a cut of matched rooted, connected AMR fragments paired with candidate translations (graph-to-string rules). Each (concept, rule) becomes a node; nodes sharing the same AMR concept form a group to ensure each concept is covered exactly once. Ordering of fragment translations is found by solving an Asymmetric Generalized Traveling Salesman Problem (AGTSP) where legal intra-fragment traversal is enforced with zero or infinite costs and inter-fragment transitions are scored by a learned classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) rooted directed semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Match AMR to induced graph-to-string rules (rooted, connected fragments). Build AGTSP nodes as (concept,rule). Enforce breadth-first intra-fragment ordering (zero-cost edges) and forbid overlapping fragment transitions (infinite cost). Compute inter-fragment travel probabilities p("yes"|ni,nj) with a MaxEnt model (features: N-gram LM score over concatenated translations, word count, shortest-path distance between fragment roots), convert to negative log-probabilities, then solve AGTSP with an off-the-shelf solver to produce an ordered concatenation of fragment translations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (semantic-graph to natural-language sentence generation); evaluated as text generation / surface realization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (case-insensitive) reported on SemEval-2016 Task8: All (AGTSP + induced rules + concept rules + MaxEnt) = Dev 21.12, Test 22.44; Baselines: PBMT (linearized AMR + Moses) = Dev 13.13, Test 16.94; OnlyInducedRule = Dev 17.68, Test 18.09; OnlyBigramLM = Dev 17.19, Test 17.75; OnlyConceptRule = Dev 13.15, Test 14.93; JAMR-gen (Flanigan et al. 2016) = Dev 23.00, Test 23.00.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Direct comparison in experiments: AGTSP-based system (All) substantially outperforms a PBMT baseline that first linearizes AMR (breadth-first) then uses Moses; All also beats variants using only concept rules or only induced rules. All outperforms a variant where traveling cost was evaluated by a bigram LM, showing the MaxEnt model is stronger than a simple bigram LM. JAMR-gen (tree-transducer approach) slightly outperforms All (23.00 vs 22.44) but uses a 5-gram LM versus the 4-gram LM used here.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enforces global ordering constraints via exact AGTSP optimization; allows use of arbitrary N-gram LMs via MaxEnt local move classifier; better than beam search alternatives (claimed lower search error); competitive BLEU with relatively low computational cost; naturally uses graph-to-string rules enabling structured transfer from AMR fragments to text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires induced rule coverage (only ~31% of AMR graphs covered by induced rules and ~84% of concepts on dev set), relies on a rule set plus concept rules for full coverage; intra-rule ordering combinatorics are constrained by enforcing breadth-first order which may restrict some valid orders; MaxEnt feature set does not model AMR edge labels (e.g., ARG0/ARG1) so structural role distinctions are not directly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples where system fails: incorrect subject/object selection (fails to recognize 'boy' as subject) due to lack of edge-label features; inability to correctly realize re-entrancy nodes that need to be translated more than once (re-entrance nodes not handled to produce multiple mentions); noisy translations introduced when expanding tight phrases to include unaligned function words (requires ranking and pruning).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text generation as a Traveling Salesman Problem', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8973.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8973.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-string rule induction (Peng2015)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Induced rooted-connected AMR fragment -> string rules via fragment decomposition and MCMC sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm to extract synchronous graph-to-string rules from aligned (sentence, AMR) pairs by generating a fragment decomposition forest (phrase-span paired with rooted AMR fragment) and running an MCMC sampling procedure to select likely phrase-fragment pairs; used as generation grammar in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A synchronous hyperedge replacement grammar based approach for AMR parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Induced rooted-connected AMR fragment -> string rules (phrase-fragment pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>From aligned sentence-AMR pairs, enumerate phrase-fragment pairs (phrase span [i,j] paired with a rooted, connected AMR fragment) satisfying alignment agreement; apply an MCMC sampling algorithm to choose a set of high-probability phrase-fragment synchronous rules. Rules may be expanded to include unaligned words on both sides for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted connected subgraphs / fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract all phrase-fragment candidates using alignment agreement; run an MCMC sampler to select a compact set of frequent phrase-fragment rules; expand tight phrases to include neighboring unaligned words on both sides for generation; rank translation candidates per fragment by training-data counts and select top-N candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used to translate AMR fragments to text within AMR-to-text generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used alone (OnlyInducedRule) on SemEval-2016: Dev BLEU 17.68, Test BLEU 18.09. Combined in All system yields Test BLEU 22.44.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>OnlyInducedRule outperforms OnlyConceptRule and PBMT baseline, demonstrating induced rules provide higher-quality translations than simple concept rules or naive linearized PBMT. Combining induced rules with AGTSP ordering and MaxEnt costs gives best results among the paper's ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures multi-token translations for graph fragments, improves quality over concept-level translations, and provides structural correspondences between AMR fragments and text similar to phrase-based MT rules.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Coverage incomplete (around 31% of AMR graphs fully covered by induced rules in dev set), inducing rules that include unaligned words can introduce noise and require candidate ranking/pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When induced rules do not cover an AMR fragment, system must fall back to concept rules; induced rules that include unaligned function words can produce noisy translations if low-frequency candidates are selected.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text generation as a Traveling Salesman Problem', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8973.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8973.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Concept rules</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Morphological/string-based concept translation rules and verbalization lookup</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple rules that map a single AMR concept to surface word forms (morphological variants) and entries from a verbalization list to ensure full coverage of AMR concepts not covered by induced rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Concept-to-string rules (lexicalized concept translations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each AMR concept generate candidate strings by using morphological variants of the concept lemma (e.g., want-01 -> want, wants, wanted, wanting) and verbalization list entries; create concept rules of form (concept fragment) ||| surface-string to guarantee coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Individual AMR concept nodes (single-node fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each concept in the AMR, generate candidate surface forms from morphological conversion of the concept label and entries from a verbalization list; filter out concepts that should be skipped (e.g., non-surface predicates like have-rel-role-91). These are added to the rule set and selected during AGTSP decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation as a fallback mechanism to ensure every AMR concept can be realized.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>OnlyConceptRule baseline: Dev BLEU 13.15, Test BLEU 14.93. Combined in All yields Test BLEU 22.44 (used for coverage rather than high quality).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Concept rules alone perform substantially worse than induced graph-to-string rules; used primarily to ensure coverage for OOV or uncovered fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to generate, ensures full coverage of AMR graphs (guarantees a translation candidate exists for every concept), useful for OOV handling.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Generally lower quality translations (lack context and argument realization), can reduce fluency and accuracy if relied on heavily.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When concept rules are used in place of richer induced rules, outputs are less grammatical and miss relational context; they cannot realize complex fragment translations or discourse-sensitive function words.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text generation as a Traveling Salesman Problem', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8973.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8973.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization + PBMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Breadth-first AMR linearization followed by phrase-based machine translation (Moses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method that first linearizes the AMR graph via breadth-first traversal (using file order to traverse children) into a sequence, then treats that sequence as the source for a phrase-based MT system (Moses) to generate a sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Breadth-first linearization (serialization) of AMR</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the AMR graph into a linear token sequence by breadth-first traversal of graph nodes (children traversed in the stored order), then use a standard phrase-based MT system trained on (linearized-AMR, sentence) pairs to map serialized tokens to natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (full graph serialized to token sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Perform breadth-first traversal from the AMR root producing a linear sequence of node labels (concepts and possibly edge labels depending on implementation). Feed the resulting sequence as source text into Moses (phrase-based MT) trained on the same parallel training data.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PBMT baseline: Dev BLEU 13.13, Test BLEU 16.94.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>PBMT baseline is substantially outperformed by the AGTSP-based system (All) and by OnlyInducedRule/OnlyBigramLM variants, indicating that naive linearization loses important structural information and that graph-aware fragment rules improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement using existing phrase-based MT toolkits; makes it straightforward to reuse MT infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization destroys graph structure and role distinctions (edge labels) leading to poorer performance; less effective than fragment-based induced-rule methods and AGTSP ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produces lower BLEU and poorer realization of semantic relations compared to graph-aware methods; sensitive to node ordering choices and unable to naturally realize re-entrant nodes or multiple mentions correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text generation as a Traveling Salesman Problem', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8973.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8973.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-transducer (JAMR-gen)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-tree conversion followed by tree transducer generation (JAMR-gen)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related approach (Flanigan et al., 2016) that first transforms the AMR graph into a spanning tree and then applies a tree transducer to generate the sentence, reported here as a comparison system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Spanning-tree conversion + tree transducer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert input AMR graph into a spanning tree representation (collapsing or selecting edges to form a tree), then apply learned tree transduction rules to map tree fragments to target strings and linearize the resulting tree output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs converted to spanning trees</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map AMR graphs to a tree (spanning tree extraction) and use tree transducers (synchronous tree-to-string rules) to generate sentences; uses higher-order (5-gram) language model in their system.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>JAMR-gen (Flanigan et al., 2016) reported BLEU: Dev 23.00, Test 23.00 on the same dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>JAMR-gen slightly outperforms the AGTSP-based All system (23.00 vs 22.44), but uses a higher-order LM (5-gram vs 4-gram) which may partly explain the difference; Flanigan et al. use a tree conversion prior to generation rather than direct graph fragment rules.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly uses tree transduction machinery which can be effective for structured generation and benefits from higher-order language models; competitive BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires converting graph to tree (may lose or approximate re-entrancies and other graph phenomena); conversion step may introduce errors or force lossy transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Both JAMR-gen and AGTSP-based system struggle with re-entrancy nodes that should be realized multiple times; tree conversion may not represent graph re-entrancies naturally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text generation as a Traveling Salesman Problem', 'publication_date_yy_mm': '2016-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A synchronous hyperedge replacement grammar based approach for AMR parsing. <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers. <em>(Rating: 2)</em></li>
                <li>Phrase-based statistical machine translation as a traveling salesman problem. <em>(Rating: 2)</em></li>
                <li>Smatch: an evaluation metric for semantic feature structures. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8973",
    "paper_id": "paper-0e367d898c9701f09ec3205b39bb19aa677c751f",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "AGTSP-graph-to-string",
            "name_full": "Asymmetric Generalized Traveling Salesman Problem based graph-to-string ordering",
            "brief_description": "Representation and decoding approach that (1) matches an input AMR graph to rooted, connected AMR-fragment -&gt; string rules, (2) constructs an AGTSP graph whose nodes are (concept, rule) pairs grouped by concept, and (3) finds a minimal-cost ordered covering (tour) of fragments using a TSP solver where travel costs are learned with a Maximum Entropy classifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "AGTSP-based ordering over graph-to-string fragments",
            "representation_description": "Input AMR is covered by a cut of matched rooted, connected AMR fragments paired with candidate translations (graph-to-string rules). Each (concept, rule) becomes a node; nodes sharing the same AMR concept form a group to ensure each concept is covered exactly once. Ordering of fragment translations is found by solving an Asymmetric Generalized Traveling Salesman Problem (AGTSP) where legal intra-fragment traversal is enforced with zero or infinite costs and inter-fragment transitions are scored by a learned classifier.",
            "graph_type": "Abstract Meaning Representation (AMR) rooted directed semantic graphs",
            "conversion_method": "Match AMR to induced graph-to-string rules (rooted, connected fragments). Build AGTSP nodes as (concept,rule). Enforce breadth-first intra-fragment ordering (zero-cost edges) and forbid overlapping fragment transitions (infinite cost). Compute inter-fragment travel probabilities p(\"yes\"|ni,nj) with a MaxEnt model (features: N-gram LM score over concatenated translations, word count, shortest-path distance between fragment roots), convert to negative log-probabilities, then solve AGTSP with an off-the-shelf solver to produce an ordered concatenation of fragment translations.",
            "downstream_task": "AMR-to-text generation (semantic-graph to natural-language sentence generation); evaluated as text generation / surface realization",
            "performance_metrics": "BLEU (case-insensitive) reported on SemEval-2016 Task8: All (AGTSP + induced rules + concept rules + MaxEnt) = Dev 21.12, Test 22.44; Baselines: PBMT (linearized AMR + Moses) = Dev 13.13, Test 16.94; OnlyInducedRule = Dev 17.68, Test 18.09; OnlyBigramLM = Dev 17.19, Test 17.75; OnlyConceptRule = Dev 13.15, Test 14.93; JAMR-gen (Flanigan et al. 2016) = Dev 23.00, Test 23.00.",
            "comparison_to_others": "Direct comparison in experiments: AGTSP-based system (All) substantially outperforms a PBMT baseline that first linearizes AMR (breadth-first) then uses Moses; All also beats variants using only concept rules or only induced rules. All outperforms a variant where traveling cost was evaluated by a bigram LM, showing the MaxEnt model is stronger than a simple bigram LM. JAMR-gen (tree-transducer approach) slightly outperforms All (23.00 vs 22.44) but uses a 5-gram LM versus the 4-gram LM used here.",
            "advantages": "Enforces global ordering constraints via exact AGTSP optimization; allows use of arbitrary N-gram LMs via MaxEnt local move classifier; better than beam search alternatives (claimed lower search error); competitive BLEU with relatively low computational cost; naturally uses graph-to-string rules enabling structured transfer from AMR fragments to text.",
            "disadvantages": "Requires induced rule coverage (only ~31% of AMR graphs covered by induced rules and ~84% of concepts on dev set), relies on a rule set plus concept rules for full coverage; intra-rule ordering combinatorics are constrained by enforcing breadth-first order which may restrict some valid orders; MaxEnt feature set does not model AMR edge labels (e.g., ARG0/ARG1) so structural role distinctions are not directly encoded.",
            "failure_cases": "Examples where system fails: incorrect subject/object selection (fails to recognize 'boy' as subject) due to lack of edge-label features; inability to correctly realize re-entrancy nodes that need to be translated more than once (re-entrance nodes not handled to produce multiple mentions); noisy translations introduced when expanding tight phrases to include unaligned function words (requires ranking and pruning).",
            "uuid": "e8973.0",
            "source_info": {
                "paper_title": "AMR-to-text generation as a Traveling Salesman Problem",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Graph-to-string rule induction (Peng2015)",
            "name_full": "Induced rooted-connected AMR fragment -&gt; string rules via fragment decomposition and MCMC sampling",
            "brief_description": "An algorithm to extract synchronous graph-to-string rules from aligned (sentence, AMR) pairs by generating a fragment decomposition forest (phrase-span paired with rooted AMR fragment) and running an MCMC sampling procedure to select likely phrase-fragment pairs; used as generation grammar in this work.",
            "citation_title": "A synchronous hyperedge replacement grammar based approach for AMR parsing.",
            "mention_or_use": "use",
            "representation_name": "Induced rooted-connected AMR fragment -&gt; string rules (phrase-fragment pairs)",
            "representation_description": "From aligned sentence-AMR pairs, enumerate phrase-fragment pairs (phrase span [i,j] paired with a rooted, connected AMR fragment) satisfying alignment agreement; apply an MCMC sampling algorithm to choose a set of high-probability phrase-fragment synchronous rules. Rules may be expanded to include unaligned words on both sides for generation.",
            "graph_type": "AMR graphs (rooted connected subgraphs / fragments)",
            "conversion_method": "Extract all phrase-fragment candidates using alignment agreement; run an MCMC sampler to select a compact set of frequent phrase-fragment rules; expand tight phrases to include neighboring unaligned words on both sides for generation; rank translation candidates per fragment by training-data counts and select top-N candidates.",
            "downstream_task": "Used to translate AMR fragments to text within AMR-to-text generation pipeline.",
            "performance_metrics": "When used alone (OnlyInducedRule) on SemEval-2016: Dev BLEU 17.68, Test BLEU 18.09. Combined in All system yields Test BLEU 22.44.",
            "comparison_to_others": "OnlyInducedRule outperforms OnlyConceptRule and PBMT baseline, demonstrating induced rules provide higher-quality translations than simple concept rules or naive linearized PBMT. Combining induced rules with AGTSP ordering and MaxEnt costs gives best results among the paper's ablations.",
            "advantages": "Captures multi-token translations for graph fragments, improves quality over concept-level translations, and provides structural correspondences between AMR fragments and text similar to phrase-based MT rules.",
            "disadvantages": "Coverage incomplete (around 31% of AMR graphs fully covered by induced rules in dev set), inducing rules that include unaligned words can introduce noise and require candidate ranking/pruning.",
            "failure_cases": "When induced rules do not cover an AMR fragment, system must fall back to concept rules; induced rules that include unaligned function words can produce noisy translations if low-frequency candidates are selected.",
            "uuid": "e8973.1",
            "source_info": {
                "paper_title": "AMR-to-text generation as a Traveling Salesman Problem",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Concept rules",
            "name_full": "Morphological/string-based concept translation rules and verbalization lookup",
            "brief_description": "Simple rules that map a single AMR concept to surface word forms (morphological variants) and entries from a verbalization list to ensure full coverage of AMR concepts not covered by induced rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Concept-to-string rules (lexicalized concept translations)",
            "representation_description": "For each AMR concept generate candidate strings by using morphological variants of the concept lemma (e.g., want-01 -&gt; want, wants, wanted, wanting) and verbalization list entries; create concept rules of form (concept fragment) ||| surface-string to guarantee coverage.",
            "graph_type": "Individual AMR concept nodes (single-node fragments)",
            "conversion_method": "For each concept in the AMR, generate candidate surface forms from morphological conversion of the concept label and entries from a verbalization list; filter out concepts that should be skipped (e.g., non-surface predicates like have-rel-role-91). These are added to the rule set and selected during AGTSP decoding.",
            "downstream_task": "AMR-to-text generation as a fallback mechanism to ensure every AMR concept can be realized.",
            "performance_metrics": "OnlyConceptRule baseline: Dev BLEU 13.15, Test BLEU 14.93. Combined in All yields Test BLEU 22.44 (used for coverage rather than high quality).",
            "comparison_to_others": "Concept rules alone perform substantially worse than induced graph-to-string rules; used primarily to ensure coverage for OOV or uncovered fragments.",
            "advantages": "Simple to generate, ensures full coverage of AMR graphs (guarantees a translation candidate exists for every concept), useful for OOV handling.",
            "disadvantages": "Generally lower quality translations (lack context and argument realization), can reduce fluency and accuracy if relied on heavily.",
            "failure_cases": "When concept rules are used in place of richer induced rules, outputs are less grammatical and miss relational context; they cannot realize complex fragment translations or discourse-sensitive function words.",
            "uuid": "e8973.2",
            "source_info": {
                "paper_title": "AMR-to-text generation as a Traveling Salesman Problem",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Linearization + PBMT",
            "name_full": "Breadth-first AMR linearization followed by phrase-based machine translation (Moses)",
            "brief_description": "Baseline method that first linearizes the AMR graph via breadth-first traversal (using file order to traverse children) into a sequence, then treats that sequence as the source for a phrase-based MT system (Moses) to generate a sentence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Breadth-first linearization (serialization) of AMR",
            "representation_description": "Serialize the AMR graph into a linear token sequence by breadth-first traversal of graph nodes (children traversed in the stored order), then use a standard phrase-based MT system trained on (linearized-AMR, sentence) pairs to map serialized tokens to natural language.",
            "graph_type": "AMR graphs (full graph serialized to token sequence)",
            "conversion_method": "Perform breadth-first traversal from the AMR root producing a linear sequence of node labels (concepts and possibly edge labels depending on implementation). Feed the resulting sequence as source text into Moses (phrase-based MT) trained on the same parallel training data.",
            "downstream_task": "AMR-to-text generation (baseline comparison)",
            "performance_metrics": "PBMT baseline: Dev BLEU 13.13, Test BLEU 16.94.",
            "comparison_to_others": "PBMT baseline is substantially outperformed by the AGTSP-based system (All) and by OnlyInducedRule/OnlyBigramLM variants, indicating that naive linearization loses important structural information and that graph-aware fragment rules improve performance.",
            "advantages": "Simple to implement using existing phrase-based MT toolkits; makes it straightforward to reuse MT infrastructure.",
            "disadvantages": "Linearization destroys graph structure and role distinctions (edge labels) leading to poorer performance; less effective than fragment-based induced-rule methods and AGTSP ordering.",
            "failure_cases": "Produces lower BLEU and poorer realization of semantic relations compared to graph-aware methods; sensitive to node ordering choices and unable to naturally realize re-entrant nodes or multiple mentions correctly.",
            "uuid": "e8973.3",
            "source_info": {
                "paper_title": "AMR-to-text generation as a Traveling Salesman Problem",
                "publication_date_yy_mm": "2016-09"
            }
        },
        {
            "name_short": "Tree-transducer (JAMR-gen)",
            "name_full": "AMR-to-tree conversion followed by tree transducer generation (JAMR-gen)",
            "brief_description": "Related approach (Flanigan et al., 2016) that first transforms the AMR graph into a spanning tree and then applies a tree transducer to generate the sentence, reported here as a comparison system.",
            "citation_title": "Generation from abstract meaning representation using tree transducers.",
            "mention_or_use": "mention",
            "representation_name": "Spanning-tree conversion + tree transducer",
            "representation_description": "Convert input AMR graph into a spanning tree representation (collapsing or selecting edges to form a tree), then apply learned tree transduction rules to map tree fragments to target strings and linearize the resulting tree output.",
            "graph_type": "AMR graphs converted to spanning trees",
            "conversion_method": "Map AMR graphs to a tree (spanning tree extraction) and use tree transducers (synchronous tree-to-string rules) to generate sentences; uses higher-order (5-gram) language model in their system.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "JAMR-gen (Flanigan et al., 2016) reported BLEU: Dev 23.00, Test 23.00 on the same dataset.",
            "comparison_to_others": "JAMR-gen slightly outperforms the AGTSP-based All system (23.00 vs 22.44), but uses a higher-order LM (5-gram vs 4-gram) which may partly explain the difference; Flanigan et al. use a tree conversion prior to generation rather than direct graph fragment rules.",
            "advantages": "Directly uses tree transduction machinery which can be effective for structured generation and benefits from higher-order language models; competitive BLEU.",
            "disadvantages": "Requires converting graph to tree (may lose or approximate re-entrancies and other graph phenomena); conversion step may introduce errors or force lossy transformations.",
            "failure_cases": "Both JAMR-gen and AGTSP-based system struggle with re-entrancy nodes that should be realized multiple times; tree conversion may not represent graph re-entrancies naturally.",
            "uuid": "e8973.4",
            "source_info": {
                "paper_title": "AMR-to-text generation as a Traveling Salesman Problem",
                "publication_date_yy_mm": "2016-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A synchronous hyperedge replacement grammar based approach for AMR parsing.",
            "rating": 2
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers.",
            "rating": 2
        },
        {
            "paper_title": "Phrase-based statistical machine translation as a traveling salesman problem.",
            "rating": 2
        },
        {
            "paper_title": "Smatch: an evaluation metric for semantic feature structures.",
            "rating": 1
        }
    ],
    "cost": 0.01151025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AMR-to-text generation as a Traveling Salesman Problem</h1>
<p>Linfeng Song ${ }^{1}$, Yue Zhang ${ }^{3}$, Xiaochang Peng ${ }^{1}$, Zhiguo Wang ${ }^{2}$ and Daniel Gildea ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, University of Rochester, Rochester, NY 14627<br>${ }^{2}$ IBM T.J. Watson Research Center, Yorktown Heights, NY 10598<br>${ }^{3}$ Singapore University of Technology and Design</p>
<h4>Abstract</h4>
<p>The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. "boy", "go-01" and "want01") represent concepts, and the edges (e.g. "ARG0" and "ARG1") represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012).</p>
<p>The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AMR graph for "The boy wants to go".
usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP).</p>
<p>Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng</p>
<p>et al. (2015), and use the rule matching algorithm of Cai and Knight (2013).</p>
<p>For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sentence by ordering the translations. Although the computational cost of our method is low, the initial experiment is promising, yielding a BLEU score of 22.44 on a standard benchmark.</p>
<h2>2 Method</h2>
<p>We reformulate the problem of AMR-to-text generation as an asymmetric generalized traveling salesman problem (AGTSP), a variant of TSP.</p>
<h3>2.1 TSP and its variants</h3>
<p>Given a non-directed graph $G_{N}$ with $n$ cities, supposing that there is a traveling cost between each pair of cities, TSP tries to find a tour of the minimal total cost visiting each city exactly once. In contrast, the asymmetric traveling salesman problem (ATSP) tries to find a tour of the minimal total cost on a directed graph, where the traveling costs between two nodes are different in each direction. Given a directed graph $G_{D}$ with $n$ nodes, which are clustered into $m$ groups, the asymmetric generalized traveling salesman problem (AGTSP) tries to find a tour of the minimal total cost visiting each group exactly once.</p>
<h3>2.2 AMR-to-text Generation as AGTSP</h3>
<p>Given an input AMR $A$, each node in the AGTSP graph can be represented as $(c, r)$, where $c$ is a concept in $A$ and $r=\left(A_{\text {sub }}, T_{\text {sub }}\right)$ is a rule that consists of an AMR fragment containing $c$ and a translation of the fragment. We put all nodes containing the same concept into one group, thereby translating each concept in the AMR exactly once.</p>
<p>To show a brief example, consider the AMR in Figure 1 and the following rules,
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example AGTSP graph</p>
<p>| $r_{1}$ | (w/want-01) $|\mid|$ wants |
| :-- | :-- |
| $r_{2}$ | (g/go-01) $|\mid|$ to go |
| $r_{3}$ | (w/want-01 :ARG1 g/go-01) $|\mid|$ wants to go |
| $r_{4}$ | (b/boy) $|\mid|$ The boy |</p>
<p>We build an AGTSP graph in Figure 2, where each circle represents a group and each tuple (such as $\left.\left(b, r_{4}\right)\right)$ represents a node in the AGTSP graph. We add two nodes $n_{s}$ and $n_{e}$ representing the start and end nodes respectively. Each belongs to a specific group that only contains that node, and a tour always starts with $n_{s}$ and ends with $n_{e}$. Legal moves are shown in black arrows, while illegal moves are shown in red. One legal tour is $n_{s} \rightarrow\left(b, r_{4}\right) \rightarrow$ $\left(w, r_{3}\right) \rightarrow\left(g, r_{3}\right) \rightarrow n_{e}$. The order in which nodes within a rule are visited is arbitrary; for a rule with $N$ concepts, the number of visiting orders is $O(N!)$. To reduce the search space, we enforce the breadth first order by setting costs to zero or infinity. In our example, the traveling cost from $\left(w, r_{3}\right)$ to $\left(g, r_{3}\right)$ is 0 , while the traveling cost from $\left(g, r_{3}\right)$ to $\left(w, r_{3}\right)$ is infinity. Traveling from $\left(g, r_{2}\right)$ to $\left(w, r_{3}\right)$ also has infinite cost, since there is overlap on the concept "w/want-01" between them.</p>
<p>The traveling cost is calculated by Algorithm 1. We first add $n_{s}$ and $n_{e}$ serving the same function as Figure 2. The traveling cost from $n_{s}$ directly to $n_{e}$ is infinite, since a tour has to go through other nodes before going to the end. On the other hand, the traveling cost from $n_{e}$ to $n_{s}$ is 0 (Lines 3-4), as a tour always goes back to the start after reaching the end. The traveling cost from $n_{s}$ to $n_{i}=\left(c_{i}, r_{i}\right)$ is the model score only if $c_{i}$ is the first node of the AMR fragment of $r_{i}$, otherwise the traveling cost is infinite (Lines 6-9). Similarly, the traveling cost from $n_{i}$ to $n_{e}$ is the model score only if $c_{i}$ is the last node of the fragment of $r_{i}$. Otherwise, it is infinite (Lines 10-13). The traveling cost from $n_{i}=\left(c_{i}, r_{i}\right)$ to $n_{j}=\left(c_{j}, r_{j}\right)$ is 0 if $r_{i}$ and $r_{j}$ are the same rule and $c_{j}$ is the next node of $c_{i}$ in the AMR fragment of $r_{i}$ (Lines 16-17).</p>
<p>A tour has to travel through an AMR fragment be-</p>
<p>Data: Nodes in AGTSP graph $G$
Result: Traveling Cost Matrix $T$
$1 n_{s} \leftarrow("&lt;\mathrm{s}&gt;","&lt;\mathrm{s}&gt;") ;$
$2 n_{e} \leftarrow("&lt;/ \mathrm{~s}&gt;","&lt;/ \mathrm{s}&gt;");$
$3 \mathrm{~T}\left[n_{s}\right]\left[n_{e}\right] \leftarrow \infty ;$
$4 \mathrm{~T}\left[n_{e}\right]\left[n_{s}\right] \leftarrow 0 ;$
5 for $n_{i} \leftarrow\left(c_{i}, r_{i}\right)$ in $G$ do
6 if $c_{i}=r_{i}$,frag.first then
$7 \mathrm{~T}\left[n_{s}\right]\left[n_{i}\right] \leftarrow \operatorname{ModelScore}\left(n_{s}, n_{i}\right)$;
8 else
$9 \mathrm{~T}\left[n_{s}\right]\left[n_{i}\right] \leftarrow \infty ;$
10 if $c_{i}=r_{i}$,frag.last then
$11 \mathrm{~T}\left[n_{i}\right]\left[n_{e}\right] \leftarrow \operatorname{ModelScore}\left(n_{i}, n_{e}\right)$;
12 else
$13 \mathrm{~T}\left[n_{i}\right]\left[n_{e}\right] \leftarrow \infty ;$
14 for $n_{i} \leftarrow\left(c_{i}, r_{i}\right)$ in $G$ do
for $n_{j} \leftarrow\left(c_{j}, r_{j}\right)$ in $G$ do
if $r_{i}=r_{j}$ and $r_{i}$,frag. next $\left(c_{i}\right)=c_{j}$ then
$\mathrm{T}\left[n_{i}\right]\left[n_{j}\right] \leftarrow 0$
else if $r_{i}$, frag $\cap r_{j}$, frag $=\emptyset$ and $c_{i}=$ $r_{i}$,frag.last and $c_{j}=r_{j}$,frag.first then
$\mathrm{T}\left[n_{i}\right]\left[n_{j}\right] \leftarrow \operatorname{ModelScore}\left(n_{i}, n_{j}\right)$
else
$\mathrm{T}\left[n_{i}\right]\left[n_{j}\right] \leftarrow \infty$
Algorithm 1: Traveling cost algorithm
fore jumping to another fragment. We choose the breadth-first order of nodes within the same rule, which is guaranteed to exist, as each AMR fragment is rooted and connected. Costs along the breadthfirst order within a rule $r_{i}$ are set to 0 , while other costs with a rule are infinite.</p>
<p>If $r_{i}$ is not equal to $r_{j}$, then the traveling cost is the model score if there is no overlap between $r_{i}$ and $r_{j}$ 's AMR fragment and it moves from $r_{i}$ 's last node to $r_{j}$ 's first node (Lines 18-19), otherwise the traveling cost is infinite (Lines 20-21). All other cases are illegal and we assign infinite traveling cost. We do not allow traveling between overlapping nodes, whose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4.</p>
<h3>2.3 Rule Acquisition</h3>
<p>We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given
an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair $([i, j], f)$, where $[i, j]$ is a span of the sentence and $f$ represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned words. Since incorporating unaligned words will introduce noise, we rank the translation candidates for each AMR fragment by their counts in the training data, and select the top $N$ candidates. ${ }^{1}$</p>
<p>We also generate concept rules which directly use a morphological string of the concept for translation. For example, for concept "w/want-01" in Figure 1, we generate concept rules such as "(w/want01) ||| want", "(w/want-01) ||| wants", "(w/want-01) ||| wanted" and "(w/want-01) ||| wanting". The algorithm (described in section 2.2) will choose the most suitable one from the rule set. It is similar to most MT systems in creating a translation candidate for each word, besides normal translation rules. It is easy to guarantee that the rule set can fully cover every input AMR graph.</p>
<p>Some concepts (such as "have-rel-role-91") in an AMR graph do not contribute to the final translation, and we skip them when generating concept rules. Besides that, we use a verbalization list ${ }^{2}$ for concept rule generation. For rule "VERBALIZE peacekeeping TO keep-01 :ARG1 peace", we will create a concept rule "(k/keep-01 :ARG1 (p/peace)) ||| peacekeeping" if the left-hand-side fragment appears in the target graph.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.4 Traveling cost</h3>
<p>Considering an AGTSP graph whose nodes are clustered into $m$ groups, we define the traveling cost for a tour $T$ in Equation 1:</p>
<p>$$
\operatorname{cost}\left(n_{s}, n_{e}\right)=-\sum_{i=0}^{m} \log p\left(\text { "yes" } \mid n_{T_{i}}, n_{T_{i+1}}\right)
$$</p>
<p>where $n_{T_{0}}=n_{s}, n_{T_{m+1}}=n_{e}$ and each $n_{T_{i}}(i \in$ $[1 \ldots m])$ belongs to a group that is different from all others. Here $p\left(\right.$ "yes" $\left.\left|n_{j}, n_{i}\right)\right.$ represents a learned score for a move from $n_{j}$ to $n_{i}$. The choices before $n_{T_{i}}$ are independent from choosing $n_{T_{i+1}}$ given $n_{T_{i}}$ because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs $p\left(n_{T_{i+1}} \mid n_{T_{i}}\right)$ by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially.</p>
<p>To tackle the problem, we treat it as a local binary ("yes" or "no") classification problem whether we should move to $n_{j}$ from $n_{i}$. We train a maximum entropy model, where $p\left(\right.$ "yes" $\left.\left|n_{i}, n_{j}\right)\right.$ is defined as:</p>
<p>$$
\begin{aligned}
&amp; p\left(\text { "yes" } \mid n_{i}, n_{j}\right)= \
&amp; \quad \frac{1}{Z\left(n_{i}, n_{j}\right)} \exp \left[\sum_{i=1}^{k} \lambda_{i} f_{i}\left(\text { "yes", } n_{i}, n_{j}\right)\right]
\end{aligned}
$$</p>
<p>The model uses 3 real-valued features: a language model score, the word count of the concatenated translation from $n_{i}$ to $n_{j}$, and the length of the shortest path from $n_{i}$ 's root to $n_{j}$ 's root in the input AMR. If either $n_{i}$ or $n_{j}$ is the start or end node, we set the path length to 0 . Using this model, we can use whatever N-gram we have at each time. Although language models favor shorter translations, word count will balance the effect, which is similar to MT systems. The length of the shortest path is used as a feature because the concepts whose translations are adjacent usually have lower path length than others.</p>
<h2>3 Experiments</h2>
<h3>3.1 Setup</h3>
<p>We use the dataset of SemEval-2016 Task8 (Meaning Representation Parsing), which contains 16833</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: left;">Dev</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PBMT</td>
<td style="text-align: left;">13.13</td>
<td style="text-align: left;">16.94</td>
</tr>
<tr>
<td style="text-align: left;">OnlyConceptRule</td>
<td style="text-align: left;">13.15</td>
<td style="text-align: left;">14.93</td>
</tr>
<tr>
<td style="text-align: left;">OnlyInducedRule</td>
<td style="text-align: left;">17.68</td>
<td style="text-align: left;">18.09</td>
</tr>
<tr>
<td style="text-align: left;">OnlyBigramLM</td>
<td style="text-align: left;">17.19</td>
<td style="text-align: left;">17.75</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">21.12</td>
<td style="text-align: left;">22.44</td>
</tr>
<tr>
<td style="text-align: left;">JAMR-gen</td>
<td style="text-align: left;">$\mathbf{2 3 . 0 0}$</td>
<td style="text-align: left;">$\mathbf{2 3 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Main results.
training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool ${ }^{3}$.</p>
<p>Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses ${ }^{4}$ to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen $^{5}$ (Flanigan et al., 2016), which is trained on the same dataset but with a 5-gram LM from gigaword (LDC2011T07).</p>
<p>To evaluate the importance of each module in our system, we develop the following baselines: OnlyConceptRule uses only the concept rules, OnlyInducedRule uses only the rules induced from the fragment decomposition forest, OnlyBigramLM uses both types of rules, but the traveling cost is evaluated by a bigram LM trained with gigaword.</p>
<h3>3.2 Results</h3>
<p>The results are shown in Table 1. Our method (All) significantly outperforms the baseline (PBMT)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">(w / want-01</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">:ARG0 (b / boy)</td>
</tr>
<tr>
<td style="text-align: center;">:ARG1 (b2 / believe-01</td>
</tr>
<tr>
<td style="text-align: center;">:ARG0 (g / girl)</td>
</tr>
<tr>
<td style="text-align: center;">:ARG1 b))</td>
</tr>
<tr>
<td style="text-align: center;">Ref: the boy wants the girl to believe him</td>
</tr>
<tr>
<td style="text-align: center;">All: a girl wanted to believe him</td>
</tr>
<tr>
<td style="text-align: center;">JAMR-gen: boys want the girl to believe</td>
</tr>
</tbody>
</table>
<p>Table 2: Case study.
on both the dev and test sets. PBMT does not outperform OnlyBigramLM and OnlyInducedRule, demonstrating that our rule induction algorithm is effective. We consider rooted and connected fragments from the AMR graph, and the TSP solver finds better solutions than beam search, as consistent with Zaslavskiy et al. (2009). In addition, OnlyInducedRule is significantly better than OnlyConceptRule, showing the importance of induced rules on performance. This also confirms the reason that All outperforms PBMT. This result confirms our expectation that concept rules, which are used for fulfilling the coverage of an input AMR graph in case of OOV, are generally not of high quality. Moreover, All outperforms OnlyBigramLM showing that our maximum entropy model is stronger than a bigram language model. Finally, JAMR-gen outperforms All, while JAMR-gen uses a higher order language model than All (5-gram VS 4-gram).</p>
<p>For rule coverage, around $31 \%$ AMR graphs and $84 \%$ concepts in the development set are covered by our induced rules extracted from the training set.</p>
<h3>3.3 Analysis and Discussions</h3>
<p>We further analyze All and JAMR-gen with an example AMR and show the AMR graph, the reference, and results in Table 2. First of all, both All and JAMR-gen outputs a reasonable translation containing most of the meaning from the AMR. On the other hand, All fails to recognize "boy" as the subject. The reason is that the feature set does not include edge labels, such as "ARG0" and "ARG1". Finally, neither All and JAMR-gen can handle the situation when a re-entrance node (such as "b/boy" in example graph of Table 2) need to be translated twice. This limitation exists for both works.</p>
<h2>4 Related Work</h2>
<p>Our work is related to prior work on AMR (Banarescu et al., 2013). There has been a list of work on AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which predicts the AMR structures for a given sentence. On the reverse direction, Flanigan et al. (2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012).</p>
<p>Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation.</p>
<h2>5 Conclusion</h2>
<p>In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search.</p>
<h2>Acknowledgments</h2>
<p>We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-1446996, and a Google Faculty Research Award. Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education.</p>
<h2>References</h2>
<p>Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage CCG semantic parsing with AMR. In Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 1699-1710.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages $98-106$.
Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 748-752.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL14), pages 1426-1436.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16), pages 731-739.
Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semanticsbased machine translation with hyperedge replacement grammars. In Proceedings of the International Conference on Computational Linguistics (COLING-12), pages 1359-1376.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), pages 48-54.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), pages 311-318.
Xiaochang Peng, Linfeng Song, and Daniel Gildea. 2015. A synchronous hyperedge replacement grammar based approach for AMR parsing. In Proceedings
of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15), pages 731-739.
Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing English into abstract meaning representation using syntax-based machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP15), pages 1143-1154.
Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Natural Language Engineering, 3(1):57-87.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu. 2014. Joint morphological generation and syntactic linearization. In Proceedings of the National Conference on Artificial Intelligence (AAAI-14), pages 15221528.</p>
<p>Lucy Vanderwende, Arul Menezes, and Chris Quirk. 2015. An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus. In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15), pages 26-30.
Stephen Wan, Mark Dras, Robert Dale, and Ccile Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL-09), pages 852-860.
Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for AMR parsing. In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15), pages 366-375.
Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for CCG realization. In Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 410-419.
Michael White. 2004. Reining in CCG chart realization. In International Conference on Natural Language Generation (INLG-04), pages 182-191.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation as a traveling salesman problem. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09), pages 333-341.
Yue Zhang and Stephen Clark. 2015. Discriminative syntax-based word ordering for text generation. Computational Linguistics, 41(3):503-538.
Yue Zhang. 2013. Partial-tree linearization: Generalized word ordering for text synthesis. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-13), pages 2232-2238.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://developers.google.com/optimization/
${ }^{4}$ http://www.statmt.org/moses/
${ }^{5} \mathrm{https}: / /$ github.com/jflanigan/jamr/tree/Generator&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
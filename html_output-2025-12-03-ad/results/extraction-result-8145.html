<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8145 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8145</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8145</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-281203259</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.01113v3.pdf" target="_blank">Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the incremental, internal problem-solving process of language models (LMs) with arithmetic multi-hop reasoning as a case study. We specifically investigate when LMs internally resolve sub/whole problems through first reading the problem statements, generating reasoning chains, and achieving the final answer to mechanistically interpret LMs'multi-hop problem-solving process. Our experiments reveal a systematic incremental reasoning strategy underlying LMs. They have not derived an answer at the moment they first read the problem; instead, they obtain (sub)answers while generating the reasoning chain. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8145.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8145.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter autoregressive foundation model (Qwen2.5 family) that the paper evaluated in detail for controlled multi-hop single-digit arithmetic tasks; used for both probing and causal activation-patching experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2.5 family autoregressive LLM; the specific instance evaluated has ~7B parameters (referred to as Qwen2.5-7B). The paper does not provide additional training details beyond model name/size.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Synthetic multi-hop arithmetic: single-digit + and − equations presented as assignments (e.g., A=1+B, B=2+3) with queries like A=?; five complexity levels varying #steps, #stacked pending variables, and distractor equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Empirical evidence indicates Qwen2.5-7B generally does not finalize necessary subanswers during the first pass over the input; instead it computes subanswers during generation of the Chain-of-Thought (CoT) — a 'Talk-to-Think' behavior. Numeric values can be linearly decoded from hidden states during/after CoT; distractor variables are not consistently linearly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Linear probing (linear classifier per token position and layer; trained with SGD on cached hidden states: 10k training, 2k test hidden-state samples per (t,l,variable)) and activation patching (replacing cached h_{t,l} from a different run into the live run, grid-wise across equations and 4-layer blocks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match output accuracy on the synthetic arithmetic tasks ~~100% (the paper reports near-100% exact-match for outputs across models). Probing: Acc_≺CoT (max probe accuracy before CoT) low in most required variables (examples: 0.17–0.51 in reported instances), Acc_≻CoT (max after CoT begins) high (often near 1.0 / 100% for required variables). The probe threshold τ used in core analyses: 0.90. t* (first token-time where linear probe exceeds τ) is typically > 0 (i.e., after CoT begins) for variables that require computation.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Variables that are not needed for the final answer (distractors) are often not linearly decodable (lower probe accuracy, e.g., v3 in Level 4 ≈0.44), indicating the model may avoid computing unnecessary values; smaller models / higher probe thresholds sometimes yield N/A (probe never reaches τ). Probing can fail to find a linear readout even when the model produces correct outputs, highlighting representational complexity or non-linear encoding of some information.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Linear-probe timelines: for required variables the first token position where probes exceed τ occurs after CoT begins (t* > 0) across many instances; Acc_≻CoT » Acc_≺CoT. Activation-patching on Qwen2.5-7B causally changed outputs only when patched hidden states lay in the same coarse grid as the target token or in the last grid where the necessary information was written, demonstrating a causal, temporally local (recency-biased) dependence of outputs on recent hidden states and supporting the interpretation that subanswers are computed during CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some trivial variables requiring no computation are decodable before CoT (e.g., v2 in Level 1 with #Step=0). Some distractor variables or variables not necessary for the final answer are decoded poorly by a linear probe, indicating either non-linear encoding or absence of explicit computation. Smaller models (e.g., Llama3.2-3B in paper) sometimes fail to reach probe thresholds at higher τ, showing limited generality across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8145.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8145.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-model aggregate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5 family, Qwen2.5-Math, Yi1.5, Llama3.1/3.2, Mistral-Nemo (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate results reported for ten different LLM instances (multiple sizes) showing consistent patterns: near-perfect task outputs but internal computation of subanswers predominantly during CoT generation, with recency-biased causal dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 (7B/14B/32B), Qwen2.5-Math (7B), Yi-1.5 (9B/34B), Llama3.1 (8B), Llama3.2 (3B), Mistral-Nemo (12B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Set of autoregressive foundation models of varying sizes (reported sizes: 3B, 7B, 8B, 9B, 12B, 14B, 32B, 34B). The paper lists these pretrained models by name and size and evaluates them on the synthetic tasks; no additional novel pretraining details are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same synthetic multi-hop single-digit assignment tasks (five complexity levels) as above; tests require resolving sequences of small integer additions/subtractions and handling pending variables and distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Consistent cross-model tendency: information required to derive final answers tends to appear in hidden states during CoT generation (Talk-to-Think), not during initial reading of input. Representations enabling linear decoding of required variables commonly emerge after CoT begins; distractor variables are less likely to be linearly encoded.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Layer- and tokenwise linear probes (trained per (t,l,variable), thresholds τ ∈ {0.85,0.90,0.95} reported) applied to hidden states across tokens and layers; activation-patching experiments performed only on Qwen2.5-7B but probing run across all listed models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match end-to-end output accuracy on tasks is approximately 100% across models (paper reports near-100% accuracy for all listed models). Probing patterns: Acc_≻CoT high across models; Acc_≺CoT low in most multi-step required variables. For some smaller models (e.g., Llama3.2-3B) high-threshold probes (τ≥0.95) sometimes fail (N/A) for some variables.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probe failures at high thresholds for smaller models (N/A entries). Some model-specific variation in t* timings and probe accuracies, but qualitative pattern (Talk-to-Think) persists. Distractor variables often not linearly read out. The paper notes limited generality due to restricted task diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Across-model probing heatmaps and aggregated tables: t* (first token position exceeding τ) is typically positive across models for variables that need computation; this pattern holds over thresholds and model families. Supplemental figures (Figures 9–53) and tables (6–20) show similar probe dynamics across models and levels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Llama3.2 (3B) shows multiple N/A probe outcomes at high τ thresholds; trivial variables needing no computation appear decodable before CoT in some cases; the study is limited to synthetic single-digit tasks and a single CoT format, so generalization to broader arithmetic or natural-language tasks is not established.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8145.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8145.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linear probing (per-token, per-layer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear probing of hidden states per token position and layer (Alain & Bengio style probe applied tokenwise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probing setup that trains a single linear classifier on hidden states at a particular token position and layer to predict a variable's numeric value; used to timeline when variable values become linearly decodable during input+CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Qwen2.5-7B and the other evaluated models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A simple linear classifier probe f_{t,l,vi}(h_{t,l}) = argmax_D W_{t,l,vi} h_{t,l} + b_{t,l,vi}; training used SGD to minimize cross-entropy with hyperparameters listed in paper (training set: 10k hidden-states per (t,l); evaluation: 2k). Probes trained separately for each token position t, each layer l, and each variable vi.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used on the synthetic multi-hop single-digit assignment tasks to probe for individual variable values at each token position and layer.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Probes reveal when numeric variable values become linearly represented in hidden states; evidence shows variables required for final answer typically become linearly decodable after CoT begins, indicating on-the-fly computation during generation. The probes also reveal that distractor variables are less consistently linearly represented.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Linear probe per (t,l,variable) with threshold τ for 'decoded' (main analyses used τ=0.90; additional results reported for τ=0.85 and 0.95). Aggregated metrics: t* (first t where any layer exceeds τ), Acc_≺CoT = max_{t<0,l} acc(t,l), Acc_≻CoT = max_{t≥0,l} acc(t,l).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Probe training/testing sizes: 10k train / 2k test hidden-state samples per (t,l). Typical observed values: Acc_≺CoT low (examples 0.17–0.51), Acc_≻CoT high (often ≈1.0) for required variables; many t* values fall after CoT start (t*>0). Results are summarized across many figures and tables (Figures 2, 9–53; Tables 6–20).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Probing can fail to reach a high threshold τ in some settings (reported as N/A), especially for smaller models or for distractor variables; criticisms of probing validity are acknowledged (paper cites literature on probing limitations). A high probe accuracy does not prove the model used a linearly-decodable representation to compute the answer, requiring causal interventions to test.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent temporal patterns across token positions and layers: required variables become decodable only at/after CoT generation begins; Acc_≻CoT » Acc_≺CoT supports Talk-to-Think. Probe design (per-token/per-layer) pinpoints precise timing (t*) of when a value is present in hidden state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Limitations include well-known concerns about probing (possible over-interpretation), probe capacity effects, and cases where probes do not reach thresholds despite correct model outputs. The paper mitigates this by complementing probes with causal activation-patching experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8145.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8145.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Activation patching (grid-wise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activation patching / transplantation of hidden states (grid-wise per-equation × 4-layer blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal-intervention method where cached hidden states from a 'clean' run on a different problem are transplanted into a live run to test whether those hidden states causally determine generated tokens, applied grid-wise over coarse blocks corresponding to equations and 4-layer slices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to Qwen2.5-7B in causal analyses</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hidden states were partitioned into 'grids' defined by equation index (coarse per-equation segmentation) and layer windows of 4 layers; for each grid, all hidden states in that grid were replaced with cached hidden states from a run on a different instance and the effect on specific target tokens (subanswer tokens and final answer) was measured using greedy decoding or forced-decoding for the target token.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Synthetic multi-hop single-digit arithmetic tasks (Level 3 focus for patching experiments); target tokens included intermediate subanswer tokens (e.g., token at end of equation 2 or 4) and the final answer token.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Patching results indicate strong recency bias: changing hidden states in the grid immediately responsible for writing the necessary information (same or last grid before the target token) can flip the model's generated token to the donor-run's token, whereas patching farther-away grids usually fails to alter the output. This supports a temporally local, sequential internal computation process during CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Activation patching with evaluation of Success Rate (percentage of patched runs where the intervened output matches the donor run's correct answer/chain) and Unchanged Rate (percentage where intervened output remains equal to original). Grids defined per equation × 4-layer blocks; target tokens included z17, z32, and final y; experiments used a 2,000-instance test set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitative/aggregated results: high Success Rate only when patching the same/last grid (i.e., where information was being written), low Success Rate elsewhere; plotted heatmaps and max-pooled layer results summarize grid-wise impact (Figures 4–6). Exact numeric success rates are shown in the figures/tables in paper; the textual conclusion is that interventions succeed primarily in temporally proximate grids.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Interventions outside the recent grids rarely change outputs (low Success Rate, high Unchanged Rate), indicating redundancy and recency bias in the model's internal process; patching is coarse (all hidden states in a grid replaced at once), and success depends on selecting the grid where the relevant information is actually present.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Successful causal control of outputs by patching only recent grids demonstrates that the hidden states in those grids are causally involved in producing the subsequent subanswer/final answer. This causal evidence corroborates the probe-derived timeline (t* > 0) that subanswers are produced during CoT and that generation relies on immediately preceding internal computations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Patching experiments were performed only on Qwen2.5-7B and on Level 3 tasks (limitation acknowledged), so generality across models and tasks is not fully established. Coarse-grained grid patching may mask more fine-grained causal structure, and patching all hidden states in a grid may produce side-effects not attributable to a single mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8145.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8145.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic arithmetic dataset (multi-hop)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controlled synthetic multi-hop single-digit arithmetic dataset (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A carefully controlled synthetic dataset of chained single-digit assignment/equation problems with five complexity levels designed to probe when and how LLMs compute subanswers in multi-hop CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used across all evaluated models in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instances are sequences of assignments of the form v = d or v = d ± v or v = v ± v with variables sampled from 26 letters and digits from {0..9}; dataset ensures that numerical values remain single-digit and that train/test are disjoint at the expression level (e.g., if '1+2' appears in train it is not in test). Complexity is controlled by #Step (number of required operations), #Stack (number of pending variables when reading left-to-right), and #Dist. (distractor equations).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Single-digit addition and subtraction in multi-hop, chained assignment format; queries ask for a single variable value after a sequence of equations. Five levels vary depth (up to at least 3 steps shown) and number of distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Designed to isolate when subanswers must be computed to reach the final answer and to allow tokenwise and layerwise probing of hidden states; supports diagnosis of whether models compute values during input read or during CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Used as controlled input for per-token per-layer linear probing and for activation patching interventions; token index t is defined relative to CoT start (t=0 at CoT start) to analyze timing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>End-to-end exact-match accuracy near 100% across models on this dataset (paper reports near-100% for all evaluated models). Probing and patching metrics described above rely on this dataset for controlled measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Dataset is synthetic with limited vocabulary/expression diversity; results may not generalize to real-world arithmetic phrasing or multi-digit arithmetic. Authors note the limitation that only a single CoT format was used and that broader formats should be tested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Because instances have controlled structure (known #Step and positions of equations), t* and t*_eq derived from probe results can be compared with theoretical lower bounds, allowing attribution that required computed values arise during CoT (t*>0) rather than during initial input encoding. The causal patching experiments use these instances to demonstrate recency effects.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Synthetic nature reduces linguistic variability and may allow in-context learning patterns not representative of real arithmetic word problems; dataset restricts numbers to single digits, so findings may not transfer to multi-digit arithmetic or to tasks requiring different chain structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Language models encode the value of numbers linearly <em>(Rating: 2)</em></li>
                <li>Monotonic representation of numeric attributes in language models <em>(Rating: 2)</em></li>
                <li>Towards best practices of activation patching in language models: Metrics and methods <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Knowledge Augmentation <em>(Rating: 1)</em></li>
                <li>Locating and editing factual associations in GPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8145",
    "paper_id": "paper-281203259",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Qwen2.5-7B",
            "name_full": "Qwen2.5 (7B)",
            "brief_description": "A 7-billion-parameter autoregressive foundation model (Qwen2.5 family) that the paper evaluated in detail for controlled multi-hop single-digit arithmetic tasks; used for both probing and causal activation-patching experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B",
            "model_description": "Qwen2.5 family autoregressive LLM; the specific instance evaluated has ~7B parameters (referred to as Qwen2.5-7B). The paper does not provide additional training details beyond model name/size.",
            "arithmetic_task_type": "Synthetic multi-hop arithmetic: single-digit + and − equations presented as assignments (e.g., A=1+B, B=2+3) with queries like A=?; five complexity levels varying #steps, #stacked pending variables, and distractor equations.",
            "mechanism_or_representation": "Empirical evidence indicates Qwen2.5-7B generally does not finalize necessary subanswers during the first pass over the input; instead it computes subanswers during generation of the Chain-of-Thought (CoT) — a 'Talk-to-Think' behavior. Numeric values can be linearly decoded from hidden states during/after CoT; distractor variables are not consistently linearly encoded.",
            "probing_or_intervention_method": "Linear probing (linear classifier per token position and layer; trained with SGD on cached hidden states: 10k training, 2k test hidden-state samples per (t,l,variable)) and activation patching (replacing cached h_{t,l} from a different run into the live run, grid-wise across equations and 4-layer blocks).",
            "performance_metrics": "Exact-match output accuracy on the synthetic arithmetic tasks ~~100% (the paper reports near-100% exact-match for outputs across models). Probing: Acc_≺CoT (max probe accuracy before CoT) low in most required variables (examples: 0.17–0.51 in reported instances), Acc_≻CoT (max after CoT begins) high (often near 1.0 / 100% for required variables). The probe threshold τ used in core analyses: 0.90. t* (first token-time where linear probe exceeds τ) is typically &gt; 0 (i.e., after CoT begins) for variables that require computation.",
            "error_types_or_failure_modes": "Variables that are not needed for the final answer (distractors) are often not linearly decodable (lower probe accuracy, e.g., v3 in Level 4 ≈0.44), indicating the model may avoid computing unnecessary values; smaller models / higher probe thresholds sometimes yield N/A (probe never reaches τ). Probing can fail to find a linear readout even when the model produces correct outputs, highlighting representational complexity or non-linear encoding of some information.",
            "evidence_for_mechanism": "Linear-probe timelines: for required variables the first token position where probes exceed τ occurs after CoT begins (t* &gt; 0) across many instances; Acc_≻CoT » Acc_≺CoT. Activation-patching on Qwen2.5-7B causally changed outputs only when patched hidden states lay in the same coarse grid as the target token or in the last grid where the necessary information was written, demonstrating a causal, temporally local (recency-biased) dependence of outputs on recent hidden states and supporting the interpretation that subanswers are computed during CoT.",
            "counterexamples_or_challenges": "Some trivial variables requiring no computation are decodable before CoT (e.g., v2 in Level 1 with #Step=0). Some distractor variables or variables not necessary for the final answer are decoded poorly by a linear probe, indicating either non-linear encoding or absence of explicit computation. Smaller models (e.g., Llama3.2-3B in paper) sometimes fail to reach probe thresholds at higher τ, showing limited generality across model sizes.",
            "uuid": "e8145.0",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Multi-model aggregate",
            "name_full": "Qwen2.5 family, Qwen2.5-Math, Yi1.5, Llama3.1/3.2, Mistral-Nemo (aggregate)",
            "brief_description": "Aggregate results reported for ten different LLM instances (multiple sizes) showing consistent patterns: near-perfect task outputs but internal computation of subanswers predominantly during CoT generation, with recency-biased causal dependence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5 (7B/14B/32B), Qwen2.5-Math (7B), Yi-1.5 (9B/34B), Llama3.1 (8B), Llama3.2 (3B), Mistral-Nemo (12B)",
            "model_description": "Set of autoregressive foundation models of varying sizes (reported sizes: 3B, 7B, 8B, 9B, 12B, 14B, 32B, 34B). The paper lists these pretrained models by name and size and evaluates them on the synthetic tasks; no additional novel pretraining details are provided in this paper.",
            "arithmetic_task_type": "Same synthetic multi-hop single-digit assignment tasks (five complexity levels) as above; tests require resolving sequences of small integer additions/subtractions and handling pending variables and distractors.",
            "mechanism_or_representation": "Consistent cross-model tendency: information required to derive final answers tends to appear in hidden states during CoT generation (Talk-to-Think), not during initial reading of input. Representations enabling linear decoding of required variables commonly emerge after CoT begins; distractor variables are less likely to be linearly encoded.",
            "probing_or_intervention_method": "Layer- and tokenwise linear probes (trained per (t,l,variable), thresholds τ ∈ {0.85,0.90,0.95} reported) applied to hidden states across tokens and layers; activation-patching experiments performed only on Qwen2.5-7B but probing run across all listed models.",
            "performance_metrics": "Exact-match end-to-end output accuracy on tasks is approximately 100% across models (paper reports near-100% accuracy for all listed models). Probing patterns: Acc_≻CoT high across models; Acc_≺CoT low in most multi-step required variables. For some smaller models (e.g., Llama3.2-3B) high-threshold probes (τ≥0.95) sometimes fail (N/A) for some variables.",
            "error_types_or_failure_modes": "Probe failures at high thresholds for smaller models (N/A entries). Some model-specific variation in t* timings and probe accuracies, but qualitative pattern (Talk-to-Think) persists. Distractor variables often not linearly read out. The paper notes limited generality due to restricted task diversity.",
            "evidence_for_mechanism": "Across-model probing heatmaps and aggregated tables: t* (first token position exceeding τ) is typically positive across models for variables that need computation; this pattern holds over thresholds and model families. Supplemental figures (Figures 9–53) and tables (6–20) show similar probe dynamics across models and levels.",
            "counterexamples_or_challenges": "Llama3.2 (3B) shows multiple N/A probe outcomes at high τ thresholds; trivial variables needing no computation appear decodable before CoT in some cases; the study is limited to synthetic single-digit tasks and a single CoT format, so generalization to broader arithmetic or natural-language tasks is not established.",
            "uuid": "e8145.1",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Linear probing (per-token, per-layer)",
            "name_full": "Linear probing of hidden states per token position and layer (Alain & Bengio style probe applied tokenwise)",
            "brief_description": "A probing setup that trains a single linear classifier on hidden states at a particular token position and layer to predict a variable's numeric value; used to timeline when variable values become linearly decodable during input+CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to Qwen2.5-7B and the other evaluated models",
            "model_description": "A simple linear classifier probe f_{t,l,vi}(h_{t,l}) = argmax_D W_{t,l,vi} h_{t,l} + b_{t,l,vi}; training used SGD to minimize cross-entropy with hyperparameters listed in paper (training set: 10k hidden-states per (t,l); evaluation: 2k). Probes trained separately for each token position t, each layer l, and each variable vi.",
            "arithmetic_task_type": "Used on the synthetic multi-hop single-digit assignment tasks to probe for individual variable values at each token position and layer.",
            "mechanism_or_representation": "Probes reveal when numeric variable values become linearly represented in hidden states; evidence shows variables required for final answer typically become linearly decodable after CoT begins, indicating on-the-fly computation during generation. The probes also reveal that distractor variables are less consistently linearly represented.",
            "probing_or_intervention_method": "Linear probe per (t,l,variable) with threshold τ for 'decoded' (main analyses used τ=0.90; additional results reported for τ=0.85 and 0.95). Aggregated metrics: t* (first t where any layer exceeds τ), Acc_≺CoT = max_{t&lt;0,l} acc(t,l), Acc_≻CoT = max_{t≥0,l} acc(t,l).",
            "performance_metrics": "Probe training/testing sizes: 10k train / 2k test hidden-state samples per (t,l). Typical observed values: Acc_≺CoT low (examples 0.17–0.51), Acc_≻CoT high (often ≈1.0) for required variables; many t* values fall after CoT start (t*&gt;0). Results are summarized across many figures and tables (Figures 2, 9–53; Tables 6–20).",
            "error_types_or_failure_modes": "Probing can fail to reach a high threshold τ in some settings (reported as N/A), especially for smaller models or for distractor variables; criticisms of probing validity are acknowledged (paper cites literature on probing limitations). A high probe accuracy does not prove the model used a linearly-decodable representation to compute the answer, requiring causal interventions to test.",
            "evidence_for_mechanism": "Consistent temporal patterns across token positions and layers: required variables become decodable only at/after CoT generation begins; Acc_≻CoT » Acc_≺CoT supports Talk-to-Think. Probe design (per-token/per-layer) pinpoints precise timing (t*) of when a value is present in hidden state representations.",
            "counterexamples_or_challenges": "Limitations include well-known concerns about probing (possible over-interpretation), probe capacity effects, and cases where probes do not reach thresholds despite correct model outputs. The paper mitigates this by complementing probes with causal activation-patching experiments.",
            "uuid": "e8145.2",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Activation patching (grid-wise)",
            "name_full": "Activation patching / transplantation of hidden states (grid-wise per-equation × 4-layer blocks)",
            "brief_description": "A causal-intervention method where cached hidden states from a 'clean' run on a different problem are transplanted into a live run to test whether those hidden states causally determine generated tokens, applied grid-wise over coarse blocks corresponding to equations and 4-layer slices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to Qwen2.5-7B in causal analyses",
            "model_description": "Hidden states were partitioned into 'grids' defined by equation index (coarse per-equation segmentation) and layer windows of 4 layers; for each grid, all hidden states in that grid were replaced with cached hidden states from a run on a different instance and the effect on specific target tokens (subanswer tokens and final answer) was measured using greedy decoding or forced-decoding for the target token.",
            "arithmetic_task_type": "Synthetic multi-hop single-digit arithmetic tasks (Level 3 focus for patching experiments); target tokens included intermediate subanswer tokens (e.g., token at end of equation 2 or 4) and the final answer token.",
            "mechanism_or_representation": "Patching results indicate strong recency bias: changing hidden states in the grid immediately responsible for writing the necessary information (same or last grid before the target token) can flip the model's generated token to the donor-run's token, whereas patching farther-away grids usually fails to alter the output. This supports a temporally local, sequential internal computation process during CoT.",
            "probing_or_intervention_method": "Activation patching with evaluation of Success Rate (percentage of patched runs where the intervened output matches the donor run's correct answer/chain) and Unchanged Rate (percentage where intervened output remains equal to original). Grids defined per equation × 4-layer blocks; target tokens included z17, z32, and final y; experiments used a 2,000-instance test set.",
            "performance_metrics": "Reported qualitative/aggregated results: high Success Rate only when patching the same/last grid (i.e., where information was being written), low Success Rate elsewhere; plotted heatmaps and max-pooled layer results summarize grid-wise impact (Figures 4–6). Exact numeric success rates are shown in the figures/tables in paper; the textual conclusion is that interventions succeed primarily in temporally proximate grids.",
            "error_types_or_failure_modes": "Interventions outside the recent grids rarely change outputs (low Success Rate, high Unchanged Rate), indicating redundancy and recency bias in the model's internal process; patching is coarse (all hidden states in a grid replaced at once), and success depends on selecting the grid where the relevant information is actually present.",
            "evidence_for_mechanism": "Successful causal control of outputs by patching only recent grids demonstrates that the hidden states in those grids are causally involved in producing the subsequent subanswer/final answer. This causal evidence corroborates the probe-derived timeline (t* &gt; 0) that subanswers are produced during CoT and that generation relies on immediately preceding internal computations.",
            "counterexamples_or_challenges": "Patching experiments were performed only on Qwen2.5-7B and on Level 3 tasks (limitation acknowledged), so generality across models and tasks is not fully established. Coarse-grained grid patching may mask more fine-grained causal structure, and patching all hidden states in a grid may produce side-effects not attributable to a single mechanism.",
            "uuid": "e8145.3",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Synthetic arithmetic dataset (multi-hop)",
            "name_full": "Controlled synthetic multi-hop single-digit arithmetic dataset (this paper)",
            "brief_description": "A carefully controlled synthetic dataset of chained single-digit assignment/equation problems with five complexity levels designed to probe when and how LLMs compute subanswers in multi-hop CoT settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used across all evaluated models in the paper",
            "model_description": "Instances are sequences of assignments of the form v = d or v = d ± v or v = v ± v with variables sampled from 26 letters and digits from {0..9}; dataset ensures that numerical values remain single-digit and that train/test are disjoint at the expression level (e.g., if '1+2' appears in train it is not in test). Complexity is controlled by #Step (number of required operations), #Stack (number of pending variables when reading left-to-right), and #Dist. (distractor equations).",
            "arithmetic_task_type": "Single-digit addition and subtraction in multi-hop, chained assignment format; queries ask for a single variable value after a sequence of equations. Five levels vary depth (up to at least 3 steps shown) and number of distractors.",
            "mechanism_or_representation": "Designed to isolate when subanswers must be computed to reach the final answer and to allow tokenwise and layerwise probing of hidden states; supports diagnosis of whether models compute values during input read or during CoT.",
            "probing_or_intervention_method": "Used as controlled input for per-token per-layer linear probing and for activation patching interventions; token index t is defined relative to CoT start (t=0 at CoT start) to analyze timing.",
            "performance_metrics": "End-to-end exact-match accuracy near 100% across models on this dataset (paper reports near-100% for all evaluated models). Probing and patching metrics described above rely on this dataset for controlled measurement.",
            "error_types_or_failure_modes": "Dataset is synthetic with limited vocabulary/expression diversity; results may not generalize to real-world arithmetic phrasing or multi-digit arithmetic. Authors note the limitation that only a single CoT format was used and that broader formats should be tested.",
            "evidence_for_mechanism": "Because instances have controlled structure (known #Step and positions of equations), t* and t*_eq derived from probe results can be compared with theoretical lower bounds, allowing attribution that required computed values arise during CoT (t*&gt;0) rather than during initial input encoding. The causal patching experiments use these instances to demonstrate recency effects.",
            "counterexamples_or_challenges": "Synthetic nature reduces linguistic variability and may allow in-context learning patterns not representative of real arithmetic word problems; dataset restricts numbers to single digits, so findings may not transfer to multi-digit arithmetic or to tasks requiring different chain structures.",
            "uuid": "e8145.4",
            "source_info": {
                "paper_title": "Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Language models encode the value of numbers linearly",
            "rating": 2,
            "sanitized_title": "language_models_encode_the_value_of_numbers_linearly"
        },
        {
            "paper_title": "Monotonic representation of numeric attributes in language models",
            "rating": 2,
            "sanitized_title": "monotonic_representation_of_numeric_attributes_in_language_models"
        },
        {
            "paper_title": "Towards best practices of activation patching in language models: Metrics and methods",
            "rating": 2,
            "sanitized_title": "towards_best_practices_of_activation_patching_in_language_models_metrics_and_methods"
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_knowledge_augmentation"
        },
        {
            "paper_title": "Locating and editing factual associations in GPT",
            "rating": 1,
            "sanitized_title": "locating_and_editing_factual_associations_in_gpt"
        }
    ],
    "cost": 0.017112,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning
8 Sep 2025</p>
<p>Keito Kudo 
Tohoku University</p>
<p>Yoichi Aoki 
Tohoku University</p>
<p>Tatsuki Kuribayashi tatsuki.kuribayashi@mbzuai.ac.ae 
Tohoku University</p>
<p>Shusaku Sone 
Tohoku University</p>
<p>Masaya Taniguchi masaya.taniguchi@riken.jp 
Tohoku University</p>
<p>Ana Brassard ana.brassard@riken.jp 
Tohoku University</p>
<p>Keisuke Sakaguchi keisuke.sakaguchi@tohoku.ac.jp 
Tohoku University</p>
<p>Kentaro Inui kentaro.inui@mbzuai.ac.ae 
Tohoku University</p>
<p>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning
8 Sep 202509560A75CC34A33AC35178896267E013arXiv:2412.01113v3[cs.CL]
RIKEN, 3 MBZUAI  {keito.kudo.q4, youichi.aoki.p2, sone.shusaku.r8}@dc.tohoku.ac.jp,</p>
<p>Introduction</p>
<p>An explanation may be produced in two modes: as a post hoc explanation to a predetermined conclusion (Think-to-Talk) or by the process of reaching a conclusion while explaining (Talk-to-Think).An analogy applies to large language models (LLMs) using chain-of-thought (CoT; Wu et al., 2023a) reasoning: is generated CoT reasoning chain a posthoc explanation, or does it reflect real-time stepby-step solving?This question is particularly relevant to the (mechanistic) interpretablity of LLMs -how models incrementally resolve the multi-hop problem, in other words, how information internally flows to reach the final decision.</p>
<p>In this study, we identify such internal reasoning patterns of LLMs.There may be multiple possible internal strategies for LLMs under the CoT scenario, i.e., first feed the problem statements and then have models generate full reasoning chains.One presumably hard strategy for LLMs would be to reach the final answer even during the first pass of problem statements before CoT generation, and then while generating CoT reasoning chains, the  Using linear probes, we investigated at which time during the LLM's problem-solving process it is possible to determine the values of each variable, illustrating the model's problem-solving process.Our analysis indicates that LMs come up with (sub)answers during CoT (second pass).This conclusion is also consistent with the findings from the causal experiments in § 5.</p>
<p>model simply refers to their predetermined answers as a post-hoc explanation (think-to-talk mode).The opposite will be that models do nothing during the first pass of the problem statements and start solving the problem after CoT generation begins (talk-to-think mode).Such different internal mechanisms can not be distinguished by just observing the model outputs; rather, one has to interpret and intervene in their internal representations during their multi-hop CoT-style reasoning.</p>
<p>In our first experiments, we apply linear probes to model internals at each layer at each timestep to determine when answers are reached internally.We prepared controlled testbeds of symbolic arithmetic reasoning tasks and observed whether trained probes could accurately predict and control the values of specific variables (Figure 1).By comparing accuracies across each timestep, one can observe at which point models internally start being informative to the probes, illustrating the model's internal reasoning flow.</p>
<p>The results reveal common patterns across models.They have not derived an answer at the moment they first read the problem (first-pass); instead, they obtain (sub)answers while generating the reasoning chain.These tendencies are consistent and systematic across our different testbeds.</p>
<p>Based on the above finding, we further conducted causal intervention analyses to clarify the causal relationship between the model's internal representations and the final answer ( § 5).We found that, when generating the reasoning chain, the model's internal problem-solving process exhibits a recency bias, relying heavily on the information in the immediately preceding portion of the chain.In other words, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation. 1</p>
<p>Related Work</p>
<p>Multi-stage human incremental language comprehension.Humans sometimes make several attempts with different strategies or mindsets, particularly when resolving a complex task.A common view may be, for example, that humans first adopt a shallow, fast solution, and once it fails, switch to a more expensive, presumably accurate one.Such a multi-stage processing is even related to the recent debate on the cognitive plausibility of LLMs in computational psycholinguistics (Oh and Schuler, 2023;Shain et al., 2024;Kuribayashi et al., 2024;Gruteke Klein et al., 2024).LLMs have now been criticized because they can not estimate human cognitive costs incurred, possibly by the multi-stage nature of human sentence processing (i.e., reanalysis) (van Schijndel and Linzen, 2021;Huang et al., 2024); humans re-read the sentence again from earlier points as an additional trial when facing difficulties in comprehension, but LLMs do not explicitly have such a multi-stage mechanism.Although this study is not focused on sentence processing or LMs' humanlikeness, one critical question in this line is whether and how LLMs switch their reasoning strategy for the same problem through multiple trials, and how to track their dynamic internal states.In our case, we simplify the setting to forced-decode the common chain-of-thought style format as two-time trials of the problem, where an LM first reads the problem statement and then (re-)analyzes the problem while generating the reasoning chain as well as copying the problem statement again.Here, we analyze what kind of process LLMs perform, particularly in the first pass of the problem statement, and how their initial processing is related to their later generation of reasoning chain and answer.Our results suggest some surprising behaviors; models system-1 Code and data will be made available upon acceptance.</p>
<p>atically come up with answers to simple subproblems in the first pass, but these computations are not reused in the second pass of CoT-style reasoning, suggesting some redundancy in their internal reasoning.</p>
<p>Interpreting multi-hop reasoning in language models.Interpreting internal mechanisms of LMs has been actively investigated (Conneau et al., 2018;Tenney et al., 2019;Niven and Kao, 2019;nostalgebraist, 2020;Geva et al., 2023;Lieberum et al., 2024;Ghandeharioun et al., 2024;Ferrando and Voita, 2024).They revealed, for example, specialized attention heads responsible for specific operations (Cabannes et al., 2024) or decisionmaking, copying, and induction (Dutta et al., 2024).With a more concrete example, Yang et al. (2024c) showed that, even during the first pass of the roblem statements such as The mother of the singer of Thriller is ___, language models first resolve a bridge entity, Stevie Wonder in this case, then identify the final answer.This study is more focused on the difference between the first pass of the problem statements (before CoT generation) and their second pass involving explicit problem solving (while CoT generation).</p>
<p>Arithmetic representations in LLMs.How models handle numerical information has also been closely studied.For instance, Heinzerling and Inui (2024) used partial least squares regression (Wold et al., 2001) to demonstrate that numeric attributes, such as birth years and population numbers, are encoded as monotonic representations in the activation space of LLMs and can be manipulated with interventions.In turn, Stolfo et al. (2023) showed that, in autoregressive LMs, the operations and numerical information necessary for solving quantitative reasoning are processed in the lower layers of the model, and these results are used by the attention layers to predict the final calculation outcomes.Zhu et al. (2025) studied the representation of numbers in language models' hidden states during single-hop arithmetic tasks (e.g., What is the sum of 12 and 34?).Their analysis revealed that numerical information is encoded linearly within the hidden states and demonstrated that individual digits could be manipulated independently.In this study, we add to this literature by introducing incremental arithmetic problem solving, i.e., what numerical information is contained in the model's hidden states at each time step of multi-hop arithmetic reasoning.Model interpretability methods.Linear probing (Alain and Bengio, 2017b) is one of the representative methods for analyzing the internal representations of neural models-a small model predicts a specific feature from them, thereby determining whether the input contains information about that feature.In this study, we use them to derive the models' intermediate answers.The causality with the model's output can be further verified by examining if a model's predictions change when the hidden states are intervened (Li et al., 2023;Wu et al., 2023b).One representative intervention method is activation patching (Vig et al., 2020;Meng et al., 2022;Zhang and Nanda, 2024), where hidden states obtained from one model instance are transplanted onto another during inference to change its predictions.Such techniques can be applied as a way to control model behavior in practical scenarios such as mitigating inherent biases (Zhao et al., 2019;Ganguli et al., 2023;Yang et al., 2024b).Here, we employ activation patching to validate the plausibility of the probing results.
Level INPUT OUTPUT #Step #Stack #Dist. 1 A = 1 + B −3 , B = 2 −2 ; A =? −1 A = 1 + B 0 , B = 2 1 , A = 1 + B 2 , A = 1 + 2 3 , A = 3 4 1 1 0 2 A = 2 + 3 −3 , B = 1 + A −2 ; B =? −1 B = 1 + A 0 , A = 2 + 3 1 , A = 5 2 , B = 1 + A 3 , B = 1 + 5 4 , B = 6 5 2 0 0 3 A = 1 + B −3 , B = 2 + 3 −2 ; A =? −1 A = 1 + B 0 , B = 2 + 3 1 , B = 5 2 , A = 1 + B 3 , A = 1 + 5 4 , A = 6 5 2 1 0 4 A = 1 + B −4 , B = 2 + 3 −3 , C = 4 + 5 −2 ; A =? −1 A = 1 + B 0 , B = 2 + 3 1 , B = 5 2 , A = 1 + B 3 , A = 1 + 5 4 , A = 6 5 2 1 1 5 A = 1 + B −4 , B = 2 + C −3 , C = 1 + 2 −2 ; A =? −1 A = 1 + B 0 , B = 2 + C 1 , C = 1 + 2 2 , C = 3 3 , B = 2 + C 4 , B = 2 + 3 5 , B = 5 6 , A = 1 + B 7 , A = 1 + 5 8 , A = 6 9 3 2 0
3 General settings</p>
<p>Arithmetic problems</p>
<p>We prepared a dataset of multi-hop arithmetic problems similarly to Kudo et al. (2023) and Yu (2025).</p>
<p>Each sample is a string of assignments (e.g., A=1) and operations (e.g., B=1+3 or B=1+A) ending with a query for a variable's value (e.g., B=?).We also defined five complexity levels, depending on (i) how many equations need to be resolved to reach the answer (#Step in Table 1), (ii) how many variables' values cannot be immediately resolved (and thus pended to a stack) in their first appearance when incrementally reading the problem from left to right (#Stack), (iii) and the number of unnecessary distractor equations (#Dist.).For example, in the Level 5 example in
v = d, v = d ± d, or v = d ± v.
We denote i-th variable to appear within an instance from the left as v i .E.g., in the Level 5 example in Table 1, v 1 = A, v 2 = B, and v 3 = C. 2 The value assigned to a variable v i is denoted as ${v i } ∈ D.</p>
<p>Generation rules.We ensure that ${v i } for any v i is also a single-digit number, and ${v i } is constant within the same instance (i.e., we exclude cases such as A=1+2,A=B+2,B=6 We denote a token position within the entire concatenated sequence x ⊕ z ⊕ y with t ∈ Z. t is relative to CoT; that is, t is zero where CoT begins, negative within the INPUT, and positive within the OUTPUT.Similarly, we assign an equation position t eq ∈ Z to each equation in the INPUT and OUTPUT (subscripts on the underlines in Table 1).</p>
<p>Probing</p>
<p>When do models solve (sub)problems in CoT-style reasoning?Do they (i) finalize reasoning during the INPUT stage, with the CoT as a post-hoc explanation (Think-to-Talk), or (ii) solve the task step-bystep during CoT generation (Talk-to-Think)?We address this by examining where the final answer, or the necessary information for it, emerges in the model's internal representations using linear probes.</p>
<p>Training settings for linear probes</p>
<p>We train a linear probe (Alain and Bengio, 2017a) for an l-layer LLM (l = 0 for the input embedding layer).To identify where the LLM solved a particular (sub)problem, we train a separate probe for each combination of token position t ∈ Z, layer depth l ∈ N, and v i ∈ Σ in each level of the problem.Specifically, given a model's ddimentional hidden state h t,l ∈ R d , the probing classifier f t,l,v i (•) : R d → D predicts ${v i }.That is, for each (t, l), we first obtained 10,000 of h t,l from training instances and then evaluated the accuracy of f t,l,v i (•) for each v i with 2,000 hidden states from test instances and the correct ${v i }.If a probe f t,l,v i (•) achieves high accuracy, this suggests that ${v i } is already computed at the corresponding position (t and l). Figure 2 illustrates the probing results with a Level 3 task, where, for example, the value of B can be extracted within INPUT and thus already computed before CoT begins.</p>
<p>The probe f t,l,v i (•) is a single linear transformation; that is, the probe is applied to h t,l as follows:
${v i } t,l, = f t,l,v i (h t,l ) = arg max D W t,l,v i h t,l + b t,l,v i ,(1)
where, W t,l,v i ∈ R |D|×d and b t,l,v i ∈ R |D| are the weight and bias parameters of the probe, respectively.The symbol • is used to refer to the model's estimate.We train the probes using stochastic gradient descent (Robbins, 1951) to minimize the cross entropy loss.The hyperparameters are listed in Table 5 in the appendix.</p>
<p>Evaluation metrics</p>
<p>The probing results from all the token positions t and layers l are aggregated as follows:
t * (v i ) = min{t | max l acc(t, l, v i ) &gt; τ } ,(2)
where acc(t, l, v i ) ∈ [0, 1] indicates the accuracy of a probing classifier f t,l,v i .The t * (v i ) ∈ Z indicates when was the first time the probing classifier achieved a reasonable accuracy above τ (= 0.90 in our study4 ) for the variable v i .As a more coarse but comprehensive value, we also report t * eq (v i ) indicating which equation t eq the t * (v i ) falls into.Given that the t (and t eq ) is relative to the CoT-beginning position, if t * (eq) (v i ) is negative, the value ${v i } is computed before CoT begins.This is the case for t * (i.e., B) in Figure 2, based on the spike around t eq =−2 in the upper line graph.</p>
<p>We also report two types of accuracy:
Acc ≺CoT (v i ) = max t&lt;0,l acc(t, l, v i ),(3)Acc ≻CoT (v i ) = max t≥0,l acc(t, l, v i ). (4)
Probes can predict both B and A; multi-step reasoning during CoT If Acc ≺CoT (v i ) is sufficiently high, ${v i } is resolved internally before CoT begins (Think-to-Talk mode).Conversely, if Acc ≺CoT (v i ) is low and Acc ≻CoT (v i ) is high, the answer is derived while performing CoT reasoning (Talk-to-Think mode).</p>
<p>Experimental results</p>
<p>Across task complexity levels.We first analyze Qwen2.5-7B(Qwen Team, 2024) across the five task levels.Table 2 shows t * eq for each variable as well as the lower bounds of t † eq , which can be computed with a greedy resolver of equations.In most cases, regardless of #Steps or task level, t * eq &gt; 0. The exceptions are v 2 in level 1 and v 3 in level 4. v 2 in level 1 has #Step = 0 and is a variable whose value requires no computation to derive.v 3 in level 4 is a distractor and is not required to derive the final answer.Therefore, we find that the variables that are required to derive the final answer and require computation are all solved after CoT begins.In summary, we find that the model solved all subproblems necessary to derive the final answer during CoT, and that the Talk-to-Think mode is dominant.</p>
<p>Variable</p>
<p>When (↓)
Acc. (↑) Level variable #Step t * eq t † eq ≺ CoT ≻ CoT 1 v1 (A) 1 4 −2 0.36 1 v2 (B) 0 −2 −2 1 1 2 v1 (A) 1 2 −3 0.49 1 v2 (B) 2 5 −2 0.21 1 3 v1 (A) 2 5 −2 0.18 1 v2 (B) 1 2 −2 0.50 1 4 v1 (A) 2 5 −3 0.17 1 v2 (B) 1 2 −3 0.48 1 v3 (C) 1 N/A −2 0.44 0.24 5 v1 (A) 3 9 −2 0.18 1 v2 (B) 2 6 −2 0.23 1 v3 (C) 1 3 −2 0.51 1
Table 2: The results of Qwen2.5-7B on the five levels.The t * eq is the time when the model comes up with the correct answer (see § 4.2).The t † eq column indicates the lower bound of t * eq score.The ≺ CoT and ≻ CoT scores correspond to the accuracies introduced in § 4.2.N/A indicates that the threshold τ was not exceeded at any position t.</p>
<p>Across models.We further analyzed ten models listed in Table 3  Table 3: Results for various models on the task Level 3. The t * column shows the token-wise time (described in § 4.2), and the other columns are the same as Table 2.</p>
<p>The t * and t * eq scores that are the same as their lower bounds are bolded.models, enhancing the generality of our obtained findings.These results are consistently observed across other tasks and irrespective of the threshold τ .For more details, see § A.2.</p>
<p>Analysis</p>
<p>Distractors.In task Level 4 (see Table 1), v 3 (C) is a distractor, that is, ${v 3 } is not necessary to derive the final answer.The models can infer this fact from the in context examples.According to Table 2, the Acc(v 3 ) in Level 4 was at most 44%, a relatively low accuracy.From this result, we can see that, unlike the variables required to derive the final answer, v 3 is not encoded in a simple form that can be extracted by a linear transformation alone.This suggests the possibility that the model employs an efficient internal mechanism that does not derive variables unnecessary for obtaining the final answer.It is also consistent with the finding that a Talk-to-Think mode, in which computation is performed during CoT, is dominant.</p>
<p>Causal interventions</p>
<p>Settings</p>
<p>Activation patching.We employ activation patching (Vig et al., 2020;Meng et al., 2022;Zhang and Nanda, 2024), which is a widely adopted technique for causal intervention analysis.To inspect the causal relationship between specific hidden states h t,l and a final answer ŷ, we compare two generation scenarios: (i) the ordinary inference and (ii) the intervened inference.In the latter scenario, we replace the specific hidden states h t,l with other variants ht,l obtained from the same model but with a different input x (Clean run in Figure 3).</p>
<p>The input x and x have different correct answer y and ỹ as well as different chains z and z, respectively.</p>
<p>For example, for the triple (x = "A=1+B,B=2+4;A=?,"z = "A=1+B,B=2+4,B=6,A=1+B,A=1+6,A=7," y =7), one may use (x = "A=2+B,B=1+3;A=?,"z = "A=2+B,B=1+3,B=4,A=2+B,A=2+4,A=6," ỹ =6).</p>
<p>If the model's output turns from y into ỹ or z t into zt due to the intervention to h t,l with ht,l , we can confirm the causal relationship between h t,l and the original answer y.We denote the model's final output without intervention as ŷ and that with intervention as ŷ, respectively (ỹ and y denote respective gold answers).In the same way, we denote the generated reasoning chain without intervention as ẑt and that with intervention as ẑt , respectively.</p>
<p>Evaluation metrics.We report Success Rate as a metric for this experiment.The Success rate indicates how frequently (%) the intervened output ŷ aligns with the correct answer ỹ.For reasoning chains, we report the Success rate for ẑt as well.</p>
<p>Patching targets.We specifically focus on Level 3 tasks and Qwen2.5-7B.Inspired by sliding window patching (Zhang and Nanda, 2024), we partition the hidden states into coarse grids, corresponding to each equation and every four layers, and perform activation patching on each grid separately (illustrated in Figure 5). 5For every grid, we compute the Success rate by applying activation patching.We also examine multiple target tokens, specifically, at (i) the end of the equation 2 (z 17 in B =5 2 in Figure 2), (ii) the end of the equation 4 (z 32 in A = 1+5 4 in Figure 2), and (iii) the final answer (y).When we apply activation patching, we generate the only target token with greedy de-5 All the hidden states in each grid are intervened at once.coding while forced-decoding the context.Note that the above equations are examples.We create a test set of 2,000 instances for evaluations.</p>
<p>Results.</p>
<p>Figure 4 shows the Success rate for each grid when the final answer y is the target token.We also show the results for the target tokens z 17 and z 32 in Appendix C. Figure 5 summarizes the max success rate among layers for each target token, and probing accuracy (same as the line graph in Figure 2).</p>
<p>The bottom part of Figure 5 suggests strong recency bias in the causal relationship between hidden states and output tokens.That is, intervention succeeded only when the target hidden state is (i) in the same grid as the target token, in the last grid where necessary information is written to derive the target token (e.g., B=2+3→B=5), or (iii) in the last grid where a value of a relevant variable is explicitly mentioned (e.g., B=5→A=1+5).This finding suggests the redundancy and strong recency bias in the internal process of LLMs' multi-hop reasoning.Moreover, the fact that in CoT the model relies on the immediately preceding computation when producing the (sub-)answer suggests that its internal reasoning flow is faithful to its own explanation.</p>
<p>Conclusions</p>
<p>We conducted causal probing analyses of when (sub-)answers are determined in the CoT process, using synthetic arithmetic problems as a controlled testbed.Across a range of models and task difficulties, we found that models predominantly operated in a Talk-to-Think mode: they resolved the necessary subproblems during CoT generation phase.Moreover, causal intervention experiments revealed a strong recency bias linking hidden states to outputs, indicating that LLMs rely heavily on recent computations when generating explanations.This pattern further suggests that their internal reasoning flow largely aligns with the produced explanations.</p>
<p>Limitations</p>
<p>Comprehensiveness of experimental settings Some experiments were conducted with a limited scope; for example, the experiments with various models in § 4.3 are conducted only on the Level 3 task.Additionally, causal interventions ( § 5) are performed only with Qwen2.5-7B.Conducting our experiment with more models and tasks will further enhance the generalizability of the results.</p>
<p>Variety of task</p>
<p>We analyzed the internal reasoning patterns of language models using synthetic arithmetic reasoning tasks.The use of synthetic data allows for more detailed control compared to experiments on natural language tasks.However, vocabulary and expression diversity, for example, are limited compared to natural language tasks.Therefore, conducting similar analyses on reasoning tasks will verify whether the results of this study apply to other broader, realistic contexts as well.Additionally, in our study, we focus on a single reasoning-chain pattern, and it would be desirable to also conduct experiments using other reasoning chain strategies and formats.On the other hand, controlling the length and granularity of them is difficult because there are various options.Conducting experiments for other reasoning chain strategies and formats is expected to provide more general insights.</p>
<p>Probing methods Interpreting internal mechanisms of LMs using probing have been actively conducted in our field (Conneau et al., 2018;Tenney et al., 2019;Campbell et al., 2023;Li et al., 2023); however, there are criticisms regarding the validity of some probing approaches (Liu et al., 2023;Burns et al., 2023).One way to overcome such concerns will be to analyze the generality of obtained results through more diverse methodologies (Gurnee et al., 2023;Bricken et al., 2023).Table 4: The performance of language models on the arithmetic reasoning tasks.The Task column shows the accuracy for the evaluation set (exact match).</p>
<p>A Supplemental results</p>
<p>A.1 Performance of language models in the arithmetic tasks</p>
<p>Table 4 shows the accuracy of language models on arithmetic reasoning tasks for each experimental setting.We computed the accuracy based on exact matches between the output, including the chain (ẑ⊕ ŷ), and the gold labels (z⊕y).The accuracy for all models is nearly 100%, indicating that they are capable of solving the arithmetic reasoning tasks used in this experiment.</p>
<p>A.2 All probing results</p>
<p>Figures 9 through 53 present the probing results for all models and tasks discussed in this paper.Tables 6 through 20 summarize these results for thresholds (τ ) ranging from 0.85 to 0.95.From these results, we observe trends similar to those described in § 4.3 across many settings.However, for the smaller model Llama3.2(3B), increasing the threshold τ often leads to cases where the accuracy does not reach the threshold (N/A).Nonetheless, a consistent pattern remains: Acc ≺CoT (v i ) is low whereas Acc ≻CoT (v i ) is high, indicating a Talk-to-Think mode.</p>
<p>B Hyperparameters</p>
<p>Table 5 shows the hyperparameters used for training the probes.</p>
<p>C Addtional causal intervention results</p>
<p>Figures 7 and 8 show the causal intervention results for the target tokens z 17 and z 32 , respectively.Here, in addition to the Success rate, we also present the   The Success rate heatmap at the top is the same as Figure 5.</p>
<p>Unchanged rate as a metric.The Unchanged rate indicates how frequently (%) the intervened output ŷ remains the same as y.If this value is small, it indicates that the patched hidden states do not affect the output.</p>
<p>D Computational resources</p>
<p>We used NVIDIA A100 GPUs (40GB and 80GB memory) and NVIDIA H100 GPUs to conduct this study.</p>
<p>E Usage of AI assistants</p>
<p>For writing this paper and the source code for the experiments, we use AI assistants (e.g., ChatGPT, GitHub Copilot).However, the use is limited to purposes such as code completion, translation, text editing, and table creation, and all content is solely based on the authors' ideas.Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5 (14B) v1 (A) 4 28 36.9 100 (Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5 (32B) v1 (A) 4 28 30.5 100 (Qwen Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Qwen2.5-Math (7B) v1 (A) 4 27 41.8 100 (Yang et al., 2024a) v2 (B) −2 −5 100 100</p>
<p>Yi1.5 (9B) v1 (A) 4 32 28.1 100 (Young et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Yi1.5 (34B) v1 (A) 4 32 22.9 100 (Young et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Llama3.1 (8B) v1 (A) 4 27 20.6 100 (Dubey et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Llama3.2 (3B) v1 (A) 4 28 21.8 100.0 (Dubey et al., 2024) v2 (B) −2 −5 100 100</p>
<p>Mistral-Nemo (12B) v1 (A) 4 28 17.9 100 (Mistral AI Team, 2024) v2 (B) −2 −5 100 100</p>
<p>Figure 1 :
1
Figure1: Using linear probes, we investigated at which time during the LLM's problem-solving process it is possible to determine the values of each variable, illustrating the model's problem-solving process.Our analysis indicates that LMs come up with (sub)answers during CoT (second pass).This conclusion is also consistent with the findings from the causal experiments in § 5.</p>
<p>Figure 2 :
2
Figure 2: Probing results for Qwen2.5-7B at the task Level 3. The heatmaps in the lower section represent the accuracy of probes computed on the evaluation set.Each cell shows the probing accuracies in each token t, layer l.The upper part indicates the maximum probing accuracy achieved at each token position t.The input sequence below the line graphs is just an example; in the actual evaluation set, each variable name, number, and operator are randomly sampled from (D, Σ, {+, −}).</p>
<p>Figure 3 :
3
Figure3: Overview of the causal intervention experiment.First, we perform normal inference (Clean run) and cache its hidden states.Subsequently, we evaluate whether the output changes by replacing some of the hidden states of a model solving a different problem with the cached hidden states.</p>
<p>Figure 4 :
4
Figure 4: Success rates for each grid when the final answer y (A =6 5 ) is the target token.</p>
<p>Figure 5 :
5
Figure 5: The upper part is the accuracy of probs, as shown in Figure 2. The lower part is the result of max pooling the Success rates from Figure 4 in the layer direction.</p>
<p>Figure 6 :
6
Figure 6: Success and Unchanged rates for each grid when the final answer y (A =6 5 ) is the target token.The Success rate heatmap at the top is the same as Figure 5.</p>
<p>Figure 7 :
7
Figure 7: Success rate and Unchanged rate for each grid when intervention was performed with z 17 (A = 1+5 4 ) as the target token.</p>
<p>Figure 8 :
8
Figure8: Success rate and Unchanged rate for each grid when intervention was performed with z 32 (B =5 2 ) as the target token.</p>
<p>Figure 9 :Figure 12 :Figure 16 :Figure 20 :
9121620
Figure 9: Probing results when Qwen2.5-7B solves Level 1.</p>
<p>Figure 24 :
24
Figure 21: Probing results when Qwen2.5-32B solves Level 3.</p>
<p>Figure 25 :Figure 26 :Figure 27 :Figure 28 :
25262728
Figure 25: Probing results when Qwen2.5-Math-7B solves Level 2.</p>
<p>Figure 29 :Figure 30 :Figure 32 :
293032
Figure 29: Probing results when Yi-1.5-9B solves Level 1.</p>
<p>Figure 33 :Figure 34 :Figure 36 :
333436
Figure 33: Probing results when Yi-1.5-9B solves Level 5.</p>
<p>Figure 37 :Figure 38 :Figure 40 :
373840
Figure 37: Probing results when Yi-1.5-34B solves Level 4.</p>
<p>Figure 41 :Figure 42 :Figure 44 :
414244
Figure 41: Probing results when Llama-3.1-8B solves Level 3.</p>
<p>Figure 45 :Figure 46 :Figure 48 :
454648
Figure 45: Probing results when Llama-3.2-3B solves Level 2.</p>
<p>Figure 49 :Figure 50 :Figure 51 :Figure 52 :
49505152
Figure 49: Probing results when Mistral-Nemo-Base-2407 solves Level 1.</p>
<p>Figure 53 :
53
Figure 53: Probing results when Mistral-Nemo-Base-2407 solves Level 5.</p>
<p>Table 1 :
1
Examples of arithmetic reasoning tasks used in our experiments at each complexity level.#Step indicates the number of required operations to reach the final answer.#Stack indicates how many variables' values are not immediately determined in their first appearing equation.#Dist. is the number of unnecessary distractor equations.The number (e.g., −3) indicated in the lower right corner of each equation represents the equation's position.This position is used as a reference point for calculating t * eq in § 4.2.</p>
<p>Table 1
1Notation. Formally, let v denote a variable namesampled from the 26 letters of the English al-phabet Σ = {a, b, c, . . . , z}, and d a numbersampled from the set of decimal digits D ={0, 1, 2, . . . , 9}. Each instance consists of multi-ple equations [e 1 , e 2 , • • • , e n ] followed by a finalquery q. Each equation follows the format
, where #Step is three and #Stack is two, calculating A requires at least three steps of reasoning: C(=1+2)=3, B(=2+3)=5, and then A(=1+5)=6, and two variables need to be resolved before reaching A: B and C.</p>
<p>on the Level 3 task.Same results as Qwen2.5-7Bare generally obtained across various
When (↓)Acc (↑)Variable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 53617.9100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (14B)v1 (A) 53517.8100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (32B)v1 (A) 53617.8100(Qwen Team, 2024)v2 (B) 21567.4100Qwen2.5-Math (7B)v1 (A) 53518.6100(Yang et al., 2024a)v2 (B) 21556.1100Yi1.5 (9B)v1 (A) 54117.8100(Young et al., 2024)v2 (B) 21836.9100Yi1.5 (34B)v1 (A) 54122.4100(Young et al., 2024)v2 (B) 21837.4100Llama3.1 (8B)v1 (A) 53526.0100(Dubey et al., 2024)v2 (B) 21629.6100Llama3.2 (3B)v1 (A) 53617.893.2(Dubey et al., 2024)v2 (B) 21733.295.4Mistral-Nemo (12B)v1 (A) 53617.8100(Mistral AI Team, 2024) v2 (B) 21628.9100</p>
<p>Empirical Methods in Natural Language Processing, pages 17432-17445, Miami, Florida, USA.Association for Computational Linguistics.Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg.2023.Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task.In The Eleventh International Conference on Learning Representations (ICLR).
Deep Ganguli, Amanda Askell, Nicholas Schiefer,Thomas I Liao, Kamilė Lukošiūtė, Anna Chen,Anna Goldie, Azalia Mirhoseini, Catherine Olsson,Danny Hernandez, and 1 others. 2023. The capacityTom Lieberum, Senthooran Rajamanoharan, Arthurfor moral self-correction in large language models.Conmy, Lewis Smith, Nicolas Sonnerat, VikrantarXiv preprint arXiv:2302.07459.Varma, János Kramár, Anca D. Dragan, Rohin Shah,Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual asso-ciations in auto-regressive language models. In Theand Neel Nanda. 2024. Gemma scope: Open sparse autoencoders everywhere all at once on Gemma 2. CoRR, abs/2408.05147.2023 Conference on Empirical Methods in Natural Language Processing.Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas. 2023. Cognitive dissonance: Why doAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lu-cas Dixon, and Mor Geva. 2024. Patchscopes: A unifying framework for inspecting hidden representa-tions of language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vi-language model outputs disagree with internal rep-resentations of truthfulness? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4791-4797, Singapore. Association for Computational Linguistics.enna, Austria, July 21-27, 2024. OpenReview.net.Kevin Meng, David Bau, Alex Andonian, and YonatanKeren Gruteke Klein, Yoav Meiri, Omer Shubi, and Yevgeni Berzak. 2024. The effect of surprisal on reading times in information seeking and repeated reading. In Proceedings of the 28th Conference onBelinkov. 2022. Locating and editing factual associ-ations in GPT. In Advances in Neural Information Processing Systems, volume 35, pages 17359-17372. Curran Associates, Inc.Computational Natural Language Learning, pages 219-230, Miami, FL, USA. Association for Compu-Mistral AI Team. 2024. Mistral NeMo.tational Linguistics.Timothy Niven and Hung-Yu Kao. 2019. Probing neu-Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in a haystack: Case stud-ies with sparse probing. Transactions on Machine Learning Research.ral network comprehension of natural language argu-ments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664, Florence, Italy. Association for Compu-tational Linguistics.Benjamin Heinzerling and Kentaro Inui. 2024. Mono-tonic representation of numeric attributes in languagenostalgebraist. 2020. interpreting GPT: the logit lens. LessWrong.models. In Proceedings of the 62nd Annual Meet-ing of the Association for Computational Linguistics (Volume 2: Short Papers), pages 175-195, Bangkok, Thailand. Association for Computational Linguistics.Byung-Doh Oh and William Schuler. 2023. Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times? Trans. Assoc. Comput. Linguist., 11:336-350.Kuan-Jung Huang, Suhas Arehalli, Mari Kugemoto,Christian Muxica, Grusha Prasad, Brian Dillon, andQwen Team. 2024. Qwen2.5: A party of foundationTal Linzen. 2024. Large-scale benchmark yieldsmodels.no evidence that language model surprisal explains syntactic disambiguation difficulty. J. Mem. Lang., 137:104510.Herbert E. Robbins. 1951. A stochastic approximation method. Annals of Mathematical Statistics, 22:400-407.Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, AnaBrassard, Masashi Yoshikawa, Keisuke Sakaguchi,Cory Shain, Clara Meister, Tiago Pimentel, Ryan Cot-and Kentaro Inui. 2023. Do Deep Neural Networksterell, and Roger Levy. 2024. Large-scale evi-Capture Compositionality in Arithmetic Reasoning?dence for logarithmic effects of word predictabilityIn Proceedings of the 17th Conference of the Euro-on reading time. Proc. Natl. Acad. Sci. U. S. A.,pean Chapter of the Association for Computational121(10):e2307876121.Linguistics (EACL), pages 1351-1362.Alessandro Stolfo, Yonatan Belinkov, and MrinmayaSachan. 2023. A mechanistic interpretation of arith-metic reasoning in language models using causalmediation analysis. In Proceedings of the 2023 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 7035-7052, Singapore. Associa-tion for Computational Linguistics.
TatsukiKuribayashi, Yohei Oseki, and Timothy Baldwin.2024.Psychometric predictive power of large language models.In Findings of the Association for Computational Linguistics: NAACL 2024, pages 1983-2005, Mexico City, Mexico.Association for Computational Linguistics.</p>
<p>Table 5 :
5
Hyperparameters for training the probe</p>
<p>Table 6 :
6
Results for various models on the task Level 1 (τ = 0.85).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 42735.8100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5 (14B)v1 (A) 42736.9100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5 (32B)v1 (A) 42830.5100(Qwen Team, 2024)v2 (B) −2 −5100100Qwen2.5-Math (7B)v1 (A) 42741.8100(Yang et al., 2024a)v2 (B) −2 −5100100Yi1.5 (9B)v1 (A) 43228.1100(Young et al., 2024)v2 (B) −2 −5100100Yi1.5 (34B)v1 (A) 43122.9100(Young et al., 2024)v2 (B) −2 −5100100Llama3.1 (8B)v1 (A) 42720.6100(Dubey et al., 2024)v2 (B) −2 −5100100Llama3.2 (3B)v1 (A) 42821.8 100.0(Dubey et al., 2024)v2 (B) −2 −5100100Mistral-Nemo (12B)v1 (A) 42718.0100(Mistral AI Team, 2024) v2 (B) −2 −5100100When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 21649.2100(Qwen Team, 2024)v2 (B) 53521.2100Qwen2.5 (14B)v1 (A) 21548.8100(Qwen Team, 2024)v2 (B) 53621.5100Qwen2.5 (32B)v1 (A) 21566.4100(Qwen Team, 2024)v2 (B) 53621.3100Qwen2.5-Math (7B)v1 (A) 21553.7100(Yang et al., 2024a)v2 (B) 53522.1100Yi1.5 (9B)v1 (A) 21840.2100(Young et al., 2024)v2 (B) 54117.8100Yi1.5 (34B)v1 (A) 21835.6100(Young et al., 2024)v2 (B) 54118.3100Llama3.1 (8B)v1 (A) 21531.9100(Dubey et al., 2024)v2 (B) 53517.8100Llama3.2 (3B)v1 (A) 21636.299.9(Dubey et al., 2024)v2 (B) 53617.899.9Mistral-Nemo (12B)v1 (A) 21630.8100(Mistral AI Team, 2024) v2 (B) 53617.8100</p>
<p>Table 7 :
7
Results for various models on the task Level 2 (τ = 0.85).
When (↓)Acc (↑)</p>
<p>Table 8 :
8
Results for various models on the task Level 3 (τ = 0.85).
When (↓)Acc (↑)</p>
<p>Table 9 :
9
Results for various models on the task Level 4 (τ = 0.85).</p>
<p>Table 10 :
10
Results for various models on the task Level 5 (τ = 0.85).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)9 6218.1100When (↓)Acc (↑)(Qwen Team, 2024)v2 (B) v3 (C)6 42 3 2322.6 50.6100 100Model Qwen2.5 (7B)Variable t  *  eq v1 (A) 4t  *  ≺ CoT ≻ CoT 27 35.8 100Qwen2.5 (14B)v1 (A)9 6318.198.8(Qwen Team, 2024)v2 (B) −2 −5100100(Qwen Team, 2024)v2 (B) v3 (C)6 42 3 2318.7 42.298.9 100Qwen2.5 (14B) (Qwen Team, 2024)v1 (A) 4 v2 (B) −2 −5 2736.9 100100 100Qwen2.5 (32B) (Qwen Team, 2024)v1 (A) v2 (B) v3 (C)9 63 6 43 3 2318.7 22.6 62.4100 100 100Qwen2.5 (32B) (Qwen Team, 2024)v1 (A) 4 v2 (B) −2 −5 2830.5 100100 100Qwen2.5-Math (7B) (Yang et al., 2024a)v1 (A) v2 (B)9 62 6 4218.1 22.6100 100Qwen2.5-Math (7B) (Yang et al., 2024a)v1 (A) 4 v2 (B) −2 −5 2741.8 100100 100v3 (C)3 2254.5100Yi1.5 (9B)v1 (A) 43228.1100Yi1.5 (34B)v1 (A)9 7118.1100(Young et al., 2024)v2 (B) −2 −5100100(Young et al., 2024)v2 (B) v3 (C)6 49 3 2622.6 41.2100 100Yi1.5 (34B) (Young et al., 2024)v1 (A) 4 v2 (B) −2 −5 3222.9 100100 100Llama3.1 (8B) (Dubey et al., 2024)v1 (A) v2 (B) v3 (C)9 62 6 43 3 2316.0 20.0 30.699.5 99.5 99.8Llama3.1 (8B) (Dubey et al., 2024)v1 (A) 4 v2 (B) −2 −5 2720.6 100100 100Llama3.2 (3B) (Dubey et al., 2024)v1 (A) N/A N/A v2 (B) N/A N/A14.2 26.343.7 47.4Llama3.2 (3B) (Dubey et al., 2024)v1 (A) 4 v2 (B) −2 −5 2821.8 100.0 100 100v3 (C) N/A N/A37.771.7Mistral-Nemo (12B)v1 (A) 42818.0100Mistral-Nemo (12B)v1 (A)9 6318.199.9(Mistral AI Team, 2024) v2 (B) −2 −5100100(Mistral AI Team, 2024) v2 (B)6 4316.399.9v3 (C)3 2332.099.9</p>
<p>Table 11 :
11
Results for various models on the task Level 1 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 21649.2100(Qwen Team, 2024)v2 (B) 53521.2100Qwen2.5 (14B)v1 (A) 21648.8100(Qwen Team, 2024)v2 (B) 53621.5100Qwen2.5 (32B)v1 (A) 21666.4100(Qwen Team, 2024)v2 (B) 53621.3100Qwen2.5-Math (7B)v1 (A) 21553.7100(Yang et al., 2024a)v2 (B) 53522.1100Yi1.5 (9B)v1 (A) 21840.2100(Young et al., 2024)v2 (B) 54117.8100Yi1.5 (34B)v1 (A) 21835.6100(Young et al., 2024)v2 (B) 54118.3100Llama3.1 (8B)v1 (A) 21531.9100(Dubey et al., 2024)v2 (B) 53517.8100Llama3.2 (3B)v1 (A) 21636.299.9(Dubey et al., 2024)v2 (B) 53617.899.9Mistral-Nemo (12B)v1 (A) 21630.8100(Mistral AI Team, 2024) v2 (B) 53617.8100</p>
<p>Table 12 :
12
Results for various models on the task Level 2 (τ = 0.90).
When (↓)Acc (↑)Variable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 53617.9100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (14B)v1 (A) 53517.8100(Qwen Team, 2024)v2 (B) 21650.5100Qwen2.5 (32B)v1 (A) 53617.8100(Qwen Team, 2024)v2 (B) 21567.4100Qwen2.5-Math (7B)v1 (A) 53518.6100(Yang et al., 2024a)v2 (B) 21556.1100Yi1.5 (9B)v1 (A) 54117.8100(Young et al., 2024)v2 (B) 21836.9100Yi1.5 (34B)v1 (A) 54122.4100(Young et al., 2024)v2 (B) 21837.4100Llama3.1 (8B)v1 (A) 53526.0100(Dubey et al., 2024)v2 (B) 21629.6100Llama3.2 (3B)v1 (A) 53617.893.2(Dubey et al., 2024)v2 (B) 21733.295.4Mistral-Nemo (12B)v1 (A) 53617.8100(Mistral AI Team, 2024) v2 (B) 21628.9100</p>
<p>Table 13 :
13
Results for various models on the task Level 3 (τ = 0.90).
When (↓)Acc (↑)</p>
<p>Table 14 :
14
Results for various models on the task Level 4 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)9 6318.1100(Qwen Team, 2024)v2 (B)6 4222.6100v3 (C)3 2350.6100Qwen2.5 (14B)v1 (A)9 6318.198.8(Qwen Team, 2024)v2 (B)6 4218.798.9v3 (C)3 2342.2100Qwen2.5 (32B)v1 (A)9 6318.7100(Qwen Team, 2024)v2 (B)6 4322.6100v3 (C)3 2362.4100Qwen2.5-Math (7B)v1 (A)9 6218.1100(Yang et al., 2024a)v2 (B)6 4222.6100v3 (C)3 2254.5100Yi1.5 (34B)v1 (A)9 7218.1100(Young et al., 2024)v2 (B)6 4922.6100v3 (C)3 2641.2100Llama3.1 (8B)v1 (A)9 6216.099.5(Dubey et al., 2024)v2 (B)6 4320.099.5v3 (C)3 2330.699.8Llama3.2 (3B)v1 (A) N/A N/A14.243.7(Dubey et al., 2024)v2 (B) N/A N/A26.347.4v3 (C) N/A N/A37.771.7Mistral-Nemo (12B)v1 (A)9 6318.199.9(Mistral AI Team, 2024) v2 (B)6 4316.399.9v3 (C)3 2332.099.9</p>
<p>Table 15 :
15
Results for various models on the task Level 5 (τ = 0.90).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A) 42735.8100(</p>
<p>Table 16 :
16
Results for various models on the task Level 1 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 17 :
17
Results for various models on the task Level 2 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 18 :
18
Results for various models on the task Level 3 (τ = 0.95).
When (↓)Acc (↑)ModelVariable t  *  eqt  *  ≺ CoT ≻ CoTQwen2.5 (7B)v1 (A)5 3617.2100(Qwen Team, 2024)v2 (B)2 1647.7100v3 (C) N/A N/A43.723.7Qwen2.5 (14B)v1 (A)5 3618.9100(Qwen Team, 2024)v2 (B)2 1544.3100v3 (C) N/A N/A40.426.8Qwen2.5 (32B)v1 (A)5 3617.4100(Qwen Team, 2024)v2 (B)2 1662.8100v3 (C) N/A N/A64.432.6Qwen2.5-Math (7B)v1 (A)5 3517.2100(Yang et al., 2024a)v2 (B)2 1555.6100v3 (C) N/A N/A47.829.4Yi1.5 (9B)v1 (A)5 4117.8100(Young et al., 2024)v2 (B)2 1843.5100v3 (C) N/A N/A36.721.2Yi1.5 (34B)v1 (A)5 4119.3100(Young et al., 2024)v2 (B)2 1840.8100v3 (C) N/A N/A27.926.2Llama3.1 (8B)v1 (A)5 3530.4100(Dubey et al., 2024)v2 (B)2 1627.2100v3 (C) N/A N/A18.517.6Llama3.2 (3B)v1 (A) N/A N/A26.291.7(Dubey et al., 2024)v2 (B)2 1729.198.7v3 (C) N/A N/A18.317.3Mistral-Nemo (12B)v1 (A)5 3617.2100(Mistral AI Team, 2024) v2 (B)2 1629.9100v3 (C) N/A N/A22.019.8</p>
<p>Table 19 :
19
Results for various models on the task Level 4 (τ = 0.95).
When (↓)Acc (↑)</p>
<p>Table 20 :
20
Results for various models on the task Level 5 (τ = 0.95).</p>
<p>For ease of reading, all examples throughout this paper use the uppercased variables A, B, C, and only the operator +.
For example, if 1+2 appears in the training set, then 1+2 does not appear in the test set.
See Appendix A.2 for results at different thresholds.
Ethics statementThis paper will not raise particular ethical concerns, considering that (i) no human experiments were conducted, and (ii) our tasks do not involve ethically sensitive topics.
Discovering latent knowledge in language models without supervision. Guillaume Alain, Yoshua Bengio, 5th International Conference on Learning Representations, ICLR 2017. Workshop Track Proceedings. OpenReview.net. Toulon, France2017a. April 24-26, 2017</p>
<p>Understanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, 5th International Conference on Learning Representations. Toulon, France2017b. 2017. April 24-26, 2017Workshop Track Proceedings</p>
<p>Towards monosemanticity: Decomposing language models with dictionary learning. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, and 5 others. 2023Anthropic</p>
<p>Discovering latent knowledge in language models without supervision. Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Iteration head: A mechanistic study of chain-of-thought. Vivien Cabannes, Charles Arnal, Wassim Bouaziz, Alice Xingyu, Francois Yang, Julia Charton, Kempe, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Localizing lying in Llama: Understanding instructed dishonesty on true-false questions through prompting, probing, and patching. James Campbell, Phillip Guo, Richard Ren, Socially Responsible Language Modelling Research. 2023</p>
<p>What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties. Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni, 10.18653/v1/P18-1198Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>The Llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, 10.48550/ARXIV.2407.21783CoRR, abs/2407.21783202482</p>
<p>How to think stepby-step: A mechanistic understanding of chain-ofthought reasoning. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty, Transactions on Machine Learning Research. 2024</p>
<p>Information flow routes: Automatically interpreting language models at scale. Javier Ferrando, Elena Voita, 10.18653/v1/2024.emnlp-main.965Proceedings of the 2024 Conference on. the 2024 Conference on2024</p>
<p>What do you learn from context? probing for sentence structure in contextualized word representations. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, Ellie Pavlick, International Conference on Learning Representations. 2019</p>
<p>Singlestage prediction models do not explain the magnitude of syntactic disambiguation difficulty. Marten Van Schijndel, Tal Linzen, Cogn. Sci. 456e129882021</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>PLS-regression: a basic tool of chemometrics. Svante Wold, Michael Sjöström, Lennart Eriksson, 10.1016/S0169-7439(01)00155-1Chemometrics and Intelligent Laboratory Systems. 5822001PLS Methods</p>
<p>Chain of Thought Prompting Elicits Knowledge Augmentation. Dingjun Wu, Jing Zhang, Xinmei Huang, Findings of the Association for Computational Linguistics (ACL). 2023a</p>
<p>Interpretability at scale: Identifying causal mechanisms in alpaca. Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, Noah Goodman, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, arXiv:2409.12122Xingzhang Ren, and Zhenru Zhang. 2024a. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint</p>
<p>Mitigating biases for instruction-following language models via bias neurons elimination. Nakyeong Yang, Taegwan Kang, Stanley Jungkyu Choi, Honglak Lee, Kyomin Jung, 10.18653/v1/2024.acl-long.490Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b1</p>
<p>Do large language models latently perform multi-hop reasoning?. Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel, 10.18653/v1/2024.acl-long.550Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c1</p>
<p>Wenhao Huang, and 11 others. 2024. Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, 10.48550/ARXIV.2403.04652CoRR, abs/2403.04652</p>
<p>Do LLMs really think step-by-step in implicit reasoning?. Yijiong Yu, arXiv:2411.158622025Preprint</p>
<p>Towards best practices of activation patching in language models: Metrics and methods. Fred Zhang, Neel Nanda, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Gender bias in contextualized word embeddings. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, Kai-Wei Chang, 10.18653/v1/N19-1064Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Language models encode the value of numbers linearly. Fangwei Zhu, Damai Dai, Zhifang Sui, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational LinguisticsAbu Dhabi, UAE2025Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
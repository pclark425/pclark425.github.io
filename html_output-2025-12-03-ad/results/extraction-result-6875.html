<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6875 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6875</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6875</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269757280</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06690v1.pdf" target="_blank">DrugLLM: Open Large Language Model for Few-shot Molecule Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision. Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry. For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties. Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded. In this work, we introduced DrugLLM, a LLM tailored for drug design. During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties. DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications. Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6875.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6875.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugLLM: Open Large Language Model for Few-shot Molecule Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter autoregressive transformer LLM adapted from LLaMA and trained on sequences of molecule modifications (paragraphs) encoded primarily via a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DrugLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only autoregressive LLM, fine-tuned from LLaMA-style pretraining objective (trained from scratch on molecule-modification paragraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Constructed from ZINC and ChEMBL tabular property data converted into 'modification paragraphs'; paper reports >25 million modification paragraphs and cites ~200 million molecules (Methods section also contains an inconsistent statement reporting 2 billion molecules). Training corpus covers >10,000 different molecular properties/activities and compositions (physicochemical properties and bioactivities).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-token generation over paragraphs of molecule modifications; few-shot in-context learning by providing K example modification pairs as part of the prompt (K ≤ 9 due to token limits); also supports zero-shot natural-language instruction for simple property compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Primary: Group-based Molecular Representation (GMR) strings (structural groups with positional connection info). Also evaluated DrugLLM-SMILES (SMILES tokenization) as an ablation/variant.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo small-molecule drug design: few-shot optimization of physicochemical properties (LogP, solubility, synthetic accessibility, TPSA, QED compositions, etc.) and biological activities (selected ChEMBL bioactivities).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Paragraph-construction constraint: all modification cases in a paragraph must concern the same molecular property; prompt balancing (equal proportion of increments/decrements) to avoid generation bias; K-shot input limited to ≤9 pairs by token budget; property-specific constraints enforced in evaluation (e.g., property predictors and calculated property windows).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used RDKit for property calculation and validity checks; used a message-passing neural network (MPNN) property predictor to evaluate biological activity predictions (predictors with Pearson r ≥ 0.75 selected); used UMAP for visualization; decoding pipeline to convert GMR back to canonical SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC (filtered druglike subset) and ChEMBL — converted into modification paragraphs; paper cites collecting ~4.5M ZINC molecules and ~180.2M ChEMBL molecules in Table 1 and building ~24–25M paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (proportion of generated molecules that follow the rule of provided modification examples), molecular similarity between generated and query molecules, property distributions (KDE), property improvement (e.g., LogP increase/decrease), Pearson correlation (r) of activity predictors for selecting test properties, validity (implicit via RDKit checks), and visual chemical-space overlap (UMAP).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Few-shot LogP optimization: DrugLLM's accuracy (success rate) increases progressively with shots, reaching ~75% (paper text) and outperforming baselines (~50%, similar to random). Biological activity: e.g., successful generation for Rho-associated protein kinase 1 (ROCK1) with a reported success rate of 76%. Zero-shot instruction optimization (Table 3): reported success-rate table entries (table labeled as percent but values shown as decimals) — QED & FractionCSP3: ChatGLM 0.02 (≈2%), ChatGPT 0.11 (≈11%), GPT-4 0.20 (≈20%), DrugLLM 0.40 (≈40%); QED & TPSA: 0.03 / 0.15 / 0.20 / 0.47 (≈3%/15%/20%/47%); QED & #Rotatable: 0.06 / 0.15 / 0.10 / 0.59 (≈6%/15%/10%/59%); LogP & FractionCSP3: 0.03 / 0.30 / 0.40 / 0.60 (≈3%/30%/40%/60%); LogP & TPSA: 0.04 / 0.19 / 0.43 / 0.55 (≈4%/19%/43%/55%); LogP & #Rotatable: 0.04 / 0.19 / 0.05 / 0.61 (≈4%/19%/5%/61%). The paper also reports that DrugLLM-GMR slightly outperforms DrugLLM-SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations: input-shot limit up to 9 due to hardware/token length constraints; zero-shot optimization is currently limited (mostly handles compositions of up to two known properties); GMR has difficulty representing a small subset of complex molecules and lacks standardization leading to occasional encoding/decoding errors; no wet-lab synthesis/assays reported; possible inconsistencies in reported corpus size (200M vs 2B molecules). Computational cost: trained for six weeks on eight NVIDIA RTX 3090 GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6875.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GMR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group-based Molecular Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that decomposes molecules into structural groups with unique string identifiers and positional connection metadata (group_atom connection indices separated by '/') to form linear encoded strings intended to reduce token count, simplify cycles, and increase robustness to small structural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GMR</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>chemical representation / tokenization scheme</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Dictionary constructed from ring- and fragment-based decomposition across ChEMBL and ZINC molecules (extracted from SMILES), producing a group dictionary; no external supervised training — dictionary built heuristically from corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model; used as the input/output textual encoding for DrugLLM's autoregressive generation and later decoded back to SMILES via a deterministic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Group tokens (unique strings for structural groups) plus connection-position annotations (using '/' separators) that together encode a molecule linearly; reversible decoding to SMILES is described.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Reduce tokenization complexity for LLM-based molecule generation; improve few-shot molecule optimization by representing molecules at group-level granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Encoding algorithm breaks non-ring C–C bonds to form fragments; merges intersecting rings; uses breadth-first search to record connection atom features for reliable decoding; representation currently lacks standardization for some complex small molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Decoding to SMILES for downstream property calculations (RDKit) and predictor evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Built from the ChEMBL and ZINC molecules collected for DrugLLM training (paper reports dictionary constructed using these corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirectly evaluated by comparing DrugLLM-GMR vs DrugLLM-SMILES generation performance (success rates, property improvements) and by inspecting encoding/decoding fidelity; no standalone benchmark reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Paper reports DrugLLM-GMR slightly outperforms DrugLLM-SMILES in few-shot generation tasks (quantitative delta not explicitly enumerated). Some encoding errors for a small number of complex molecules are acknowledged.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Difficulties representing certain complex small molecules; lacks full standardization which can produce occasional encoding errors; design decisions (breaking all C–C bonds in non-ring parts) may limit representation fidelity in special cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6875.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JTVAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Junction Tree Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based variational autoencoder that represents molecules as junction trees of molecular substructures for molecule generation; used here as a baseline generative model adapted to K-shot optimization by retraining or conditioning on provided example modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Junction tree variational autoencoder for molecular graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JTVAE (official released code/pretrained model used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>variational autoencoder for molecular graphs (junction-tree + graph VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Original JTVAE training data not specified in this paper; authors used the released pre-trained JTVAE model and adapted it for few-shot tasks using the K modification cases as training samples.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sampling from pre-trained latent space; adapted to K-shot by using the K modification cases as training samples for generating candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Graph/junction-tree of molecular motifs/substructures; internally maps to molecular graphs and SMILES for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation and optimization (baseline comparison in few-shot property optimization tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>In the adapted evaluation, JTVAE used K example modifications as training samples; random-sampling baseline also samples from JTVAE latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used released code; property evaluation and validity checks performed by the authors' evaluation pipeline (RDKit-based property calculators).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Adapted to the DrugLLM evaluation tasks (used K-shot examples drawn from DrugLLM's test sets); exact original pretraining corpora not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate relative to example modification rules, molecular similarity, property distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to achieve about ~50% success rate in few-shot LogP tasks (similar to random sampling) and generally failed to meaningfully improve in biological activity optimization compared with random generation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not designed for few-shot in-context learning; struggled to capture modification rules from a handful of examples in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6875.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VJTNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Junction Tree Neural Network (VJTNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical generative model for molecular graphs that samples structural motifs and assembles them to generate new molecules; used as a baseline for few-shot molecule optimization in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hierarchical generation of molecular graphs using structural motifs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VJTNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hierarchical graph generative model / VAE-style</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Original training data not specified in this paper; authors used official released code adapted to the few-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Graph/motif-based generative sampling (junction-tree based); adapted to few-shot by training on K modification cases similarly to JTVAE.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Graph of motifs/junction-tree plus molecular graphs; maps to SMILES for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation and optimization (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Adapted per-experiment to K-shot sample training; no additional constraints described here.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used official code and evaluation pipeline (RDKit calculators for properties).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Adapted to DrugLLM's evaluation dataset (K-shot examples from test tasks); original pretraining corpora not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate, molecular similarity, property improvement distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to obtain ~50% success rate on few-shot LogP tasks (similar to random) and generally failed to meaningfully improve biological activities compared with DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Like other VAE-based baselines, not optimized for few-shot in-context learning; struggled on limited-example modification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6875.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoLeR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoLeR (scaffold-based molecule generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scaffold/motif-based generative model that extends molecular scaffolds with structural motifs; used here as a baseline for few-shot property optimization comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to extend molecular scaffolds with structural motifs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoLeR</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>scaffold-based graph generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Original MoLeR training data not specified in this paper; authors used official code adapted to K-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Scaffold extension via learned motif assembly; adapted to K-shot by training on provided modification examples.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Scaffold + motif graph tokens mapped to standard molecular graphs/SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecule generation and optimization (baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Adapted to K-shot training per experiment; no extra constraints described.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used official code; property evaluation via RDKit-based calculators.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Adapted to DrugLLM's evaluation datasets (K-shot examples); original pretraining corpora not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate, similarity, property distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported to achieve ~50% success rate on LogP few-shot tasks (similar to random) and did not match DrugLLM's performance on biological activity optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not built for in-context few-shot learning; struggled to extract modification rules from small example sets in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6875.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3 family / OpenAI conversational model referenced via API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose conversational LLM evaluated here as a zero-shot/few-shot baseline for instruction-guided molecule optimization; capable of generating SMILES in some cases but performs worse than DrugLLM on the zero-shot molecule optimization tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>general-purpose decoder-only conversational LLM (prompt-based, zero-/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large multi-domain text corpora (public+proprietary); not specialized on GMR; exact chemical content unspecified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based (zero-shot or few-shot) SMILES/text generation; used in this paper with carefully constructed prompts containing example modifications to attempt molecule optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (model produced SMILES strings as output when successful); often struggled to output valid molecular strings for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Tested for zero-shot molecule optimization (instruction-guided property optimization) as a general LLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompts included example modifications and molecule descriptions; balanced prompts used when applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Invoked via OpenAI API; generated outputs post-processed and validated with RDKit and the authors' evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Not retrained on DrugLLM corpora; evaluated on the paper's zero-shot test set (6,000 instructions across six optimization tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rate (as in Table 3), validity (via RDKit), property improvement measured by RDKit calculators/predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Zero-shot success rates (Table 3): across tasks ChatGPT reported intermediate performance (e.g., QED & FractionCSP3 ≈11%, QED & TPSA ≈15%, LogP & FractionCSP3 ≈30%, LogP & TPSA ≈19%, LogP & #Rotatable ≈19%). Overall underperformed DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>General LLM lacks specialized chemical/biological training and can output invalid or duplicated SMILES; limited mapping from natural language instruction to precise molecular property changes compared with DrugLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6875.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multimodal/general LLM (GPT-4) used as a high-quality general baseline for zero-shot molecule optimization; understood instructions and optimized some molecules but with lower success rates than DrugLLM in the reported tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>large general-purpose LLM (prompt-based, zero-/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on very large web-scale corpora including code and other data; not specialized on GMR or the DrugLLM training corpus (paper does not re-train GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based zero-/few-shot SMILES/text generation provided via API; evaluated directly on instruction-guided molecule optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES output when successful; requires careful prompting to elicit valid SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Zero-shot molecule optimization (instruction-driven property composition optimizations) as a strong general LLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Carefully constructed prompts containing molecule descriptions and few-shot examples where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Invoked via OpenAI API; outputs validated via RDKit and property predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on the paper's zero-shot test tasks (6,000 instructions); not retrained on DrugLLM corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rates per Table 3; property calculations via RDKit/predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Zero-shot success rates (Table 3) generally higher than ChatGLM/ChatGPT on some tasks but lower than DrugLLM (e.g., QED & FractionCSP3 ≈20%, QED & TPSA ≈20%, LogP & FractionCSP3 ≈40%, LogP & TPSA ≈43%, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Though powerful, general LLM training does not guarantee understanding of structure–property mappings for molecular optimization; lower success rates than DrugLLM on specialized few-shot/zero-shot property composition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6875.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM (Chinese bilingual conversational LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general-purpose conversational LLM (ChatGLM) evaluated as a zero-shot baseline for molecule optimization; struggled to provide appropriate molecules and often returned duplicated input molecules in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Glm-130b: An open bilingual pre-trained model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM (official released model used locally)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>general conversational LLM (prompt-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large bilingual corpora; not specialized on chemical/biological corpora for molecule generation as used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based (zero-shot), used with prompts containing molecule descriptions and example modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Attempted SMILES/text outputs; often failed/duplicated inputs per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Zero-shot molecule optimization baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Prompts included example modifications where applicable; no special chemical tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Deployed locally using released code; generated outputs validated via RDKit and evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on the drug-optimization zero-shot test set (6,000 instructions); not retrained.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Zero-shot success rates (Table 3) and validity as checked by RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 3 reports very low success rates (e.g., QED & FractionCSP3 0.02 ≈2%, QED & TPSA 0.03 ≈3%, etc.); often returned duplicates of input molecules rather than optimized variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Poor mapping from instruction to valid molecular modifications in this domain; frequent duplication of input molecules; low success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6875.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6875.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-backbone</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-based Transformer backbone (model hyperparameters adopted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DrugLLM adopts the LLaMA model architecture/parameterization as its backbone (decoder-only transformer); DrugLLM uses these settings and expands vocabulary for chemical tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-style transformer (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer architecture (foundation LLM backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>architectural settings corresponding to 7B parameter configuration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>DrugLLM training corpus (molecule modification paragraphs) replaced generic LLaMA pretraining data; LLaMA referenced for architecture/hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-token prediction using Transformer decoder layers (32 layers, 32 attention heads, hidden dim 4096).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Vocabulary extended with frequently-used SMILES tokens and BPE applied; primary input representation was GMR in DrugLLM experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Backbone architecture to build DrugLLM for molecule generation/optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Architectural limits and token-length constraints inherited from decoder-only LLM design (prompt length limits resulted in ≤9-shot practical limit).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not an external tool but an architectural choice; DrugLLM training used AdamW optimizer and cosine annealing learning-rate schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Architecture applied to DrugLLM corpus (ZINC + ChEMBL derived paragraphs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not directly evaluated; used as the model backbone for DrugLLM whose performance is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>DrugLLM built on this backbone achieves the reported few-shot and zero-shot results (see DrugLLM entry).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Prompt length / token budget constraints limit number of example shots; vocabulary/tokenization trade-offs require representation choices (GMR vs SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugLLM: Open Large Language Model for Few-shot Molecule Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 2)</em></li>
                <li>Hierarchical generation of molecular graphs using structural motifs <em>(Rating: 2)</em></li>
                <li>Learning to extend molecular scaffolds with structural motifs <em>(Rating: 2)</em></li>
                <li>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins <em>(Rating: 2)</em></li>
                <li>Biogpt: generative pre-trained transformer for biomedical text generation and mining <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6875",
    "paper_id": "paper-269757280",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "DrugLLM",
            "name_full": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
            "brief_description": "A 7B-parameter autoregressive transformer LLM adapted from LLaMA and trained on sequences of molecule modifications (paragraphs) encoded primarily via a Group-based Molecular Representation (GMR) to perform few-shot and zero-shot molecule generation and property optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DrugLLM",
            "model_type": "decoder-only autoregressive LLM, fine-tuned from LLaMA-style pretraining objective (trained from scratch on molecule-modification paragraphs)",
            "model_size": "7B parameters",
            "training_data_description": "Constructed from ZINC and ChEMBL tabular property data converted into 'modification paragraphs'; paper reports &gt;25 million modification paragraphs and cites ~200 million molecules (Methods section also contains an inconsistent statement reporting 2 billion molecules). Training corpus covers &gt;10,000 different molecular properties/activities and compositions (physicochemical properties and bioactivities).",
            "generation_method": "Autoregressive next-token generation over paragraphs of molecule modifications; few-shot in-context learning by providing K example modification pairs as part of the prompt (K ≤ 9 due to token limits); also supports zero-shot natural-language instruction for simple property compositions.",
            "chemical_representation": "Primary: Group-based Molecular Representation (GMR) strings (structural groups with positional connection info). Also evaluated DrugLLM-SMILES (SMILES tokenization) as an ablation/variant.",
            "target_application": "De novo small-molecule drug design: few-shot optimization of physicochemical properties (LogP, solubility, synthetic accessibility, TPSA, QED compositions, etc.) and biological activities (selected ChEMBL bioactivities).",
            "constraints_used": "Paragraph-construction constraint: all modification cases in a paragraph must concern the same molecular property; prompt balancing (equal proportion of increments/decrements) to avoid generation bias; K-shot input limited to ≤9 pairs by token budget; property-specific constraints enforced in evaluation (e.g., property predictors and calculated property windows).",
            "integration_with_external_tools": "Used RDKit for property calculation and validity checks; used a message-passing neural network (MPNN) property predictor to evaluate biological activity predictions (predictors with Pearson r ≥ 0.75 selected); used UMAP for visualization; decoding pipeline to convert GMR back to canonical SMILES.",
            "dataset_used": "ZINC (filtered druglike subset) and ChEMBL — converted into modification paragraphs; paper cites collecting ~4.5M ZINC molecules and ~180.2M ChEMBL molecules in Table 1 and building ~24–25M paragraphs.",
            "evaluation_metrics": "Success rate (proportion of generated molecules that follow the rule of provided modification examples), molecular similarity between generated and query molecules, property distributions (KDE), property improvement (e.g., LogP increase/decrease), Pearson correlation (r) of activity predictors for selecting test properties, validity (implicit via RDKit checks), and visual chemical-space overlap (UMAP).",
            "reported_results": "Few-shot LogP optimization: DrugLLM's accuracy (success rate) increases progressively with shots, reaching ~75% (paper text) and outperforming baselines (~50%, similar to random). Biological activity: e.g., successful generation for Rho-associated protein kinase 1 (ROCK1) with a reported success rate of 76%. Zero-shot instruction optimization (Table 3): reported success-rate table entries (table labeled as percent but values shown as decimals) — QED & FractionCSP3: ChatGLM 0.02 (≈2%), ChatGPT 0.11 (≈11%), GPT-4 0.20 (≈20%), DrugLLM 0.40 (≈40%); QED & TPSA: 0.03 / 0.15 / 0.20 / 0.47 (≈3%/15%/20%/47%); QED & #Rotatable: 0.06 / 0.15 / 0.10 / 0.59 (≈6%/15%/10%/59%); LogP & FractionCSP3: 0.03 / 0.30 / 0.40 / 0.60 (≈3%/30%/40%/60%); LogP & TPSA: 0.04 / 0.19 / 0.43 / 0.55 (≈4%/19%/43%/55%); LogP & #Rotatable: 0.04 / 0.19 / 0.05 / 0.61 (≈4%/19%/5%/61%). The paper also reports that DrugLLM-GMR slightly outperforms DrugLLM-SMILES.",
            "experimental_validation": false,
            "challenges_or_limitations": "Reported limitations: input-shot limit up to 9 due to hardware/token length constraints; zero-shot optimization is currently limited (mostly handles compositions of up to two known properties); GMR has difficulty representing a small subset of complex molecules and lacks standardization leading to occasional encoding/decoding errors; no wet-lab synthesis/assays reported; possible inconsistencies in reported corpus size (200M vs 2B molecules). Computational cost: trained for six weeks on eight NVIDIA RTX 3090 GPUs.",
            "uuid": "e6875.0",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GMR",
            "name_full": "Group-based Molecular Representation",
            "brief_description": "A representation that decomposes molecules into structural groups with unique string identifiers and positional connection metadata (group_atom connection indices separated by '/') to form linear encoded strings intended to reduce token count, simplify cycles, and increase robustness to small structural changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GMR",
            "model_type": "chemical representation / tokenization scheme",
            "model_size": null,
            "training_data_description": "Dictionary constructed from ring- and fragment-based decomposition across ChEMBL and ZINC molecules (extracted from SMILES), producing a group dictionary; no external supervised training — dictionary built heuristically from corpus.",
            "generation_method": "Not a generative model; used as the input/output textual encoding for DrugLLM's autoregressive generation and later decoded back to SMILES via a deterministic algorithm.",
            "chemical_representation": "Group tokens (unique strings for structural groups) plus connection-position annotations (using '/' separators) that together encode a molecule linearly; reversible decoding to SMILES is described.",
            "target_application": "Reduce tokenization complexity for LLM-based molecule generation; improve few-shot molecule optimization by representing molecules at group-level granularity.",
            "constraints_used": "Encoding algorithm breaks non-ring C–C bonds to form fragments; merges intersecting rings; uses breadth-first search to record connection atom features for reliable decoding; representation currently lacks standardization for some complex small molecules.",
            "integration_with_external_tools": "Decoding to SMILES for downstream property calculations (RDKit) and predictor evaluation.",
            "dataset_used": "Built from the ChEMBL and ZINC molecules collected for DrugLLM training (paper reports dictionary constructed using these corpora).",
            "evaluation_metrics": "Indirectly evaluated by comparing DrugLLM-GMR vs DrugLLM-SMILES generation performance (success rates, property improvements) and by inspecting encoding/decoding fidelity; no standalone benchmark reported.",
            "reported_results": "Paper reports DrugLLM-GMR slightly outperforms DrugLLM-SMILES in few-shot generation tasks (quantitative delta not explicitly enumerated). Some encoding errors for a small number of complex molecules are acknowledged.",
            "experimental_validation": false,
            "challenges_or_limitations": "Difficulties representing certain complex small molecules; lacks full standardization which can produce occasional encoding errors; design decisions (breaking all C–C bonds in non-ring parts) may limit representation fidelity in special cases.",
            "uuid": "e6875.1",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "JTVAE",
            "name_full": "Junction Tree Variational Autoencoder",
            "brief_description": "A graph-based variational autoencoder that represents molecules as junction trees of molecular substructures for molecule generation; used here as a baseline generative model adapted to K-shot optimization by retraining or conditioning on provided example modifications.",
            "citation_title": "Junction tree variational autoencoder for molecular graph generation",
            "mention_or_use": "use",
            "model_name": "JTVAE (official released code/pretrained model used)",
            "model_type": "variational autoencoder for molecular graphs (junction-tree + graph VAE)",
            "model_size": null,
            "training_data_description": "Original JTVAE training data not specified in this paper; authors used the released pre-trained JTVAE model and adapted it for few-shot tasks using the K modification cases as training samples.",
            "generation_method": "Sampling from pre-trained latent space; adapted to K-shot by using the K modification cases as training samples for generating candidate molecules.",
            "chemical_representation": "Graph/junction-tree of molecular motifs/substructures; internally maps to molecular graphs and SMILES for validation.",
            "target_application": "De novo molecule generation and optimization (baseline comparison in few-shot property optimization tasks).",
            "constraints_used": "In the adapted evaluation, JTVAE used K example modifications as training samples; random-sampling baseline also samples from JTVAE latent space.",
            "integration_with_external_tools": "Used released code; property evaluation and validity checks performed by the authors' evaluation pipeline (RDKit-based property calculators).",
            "dataset_used": "Adapted to the DrugLLM evaluation tasks (used K-shot examples drawn from DrugLLM's test sets); exact original pretraining corpora not specified in this paper.",
            "evaluation_metrics": "Success rate relative to example modification rules, molecular similarity, property distributions.",
            "reported_results": "Reported to achieve about ~50% success rate in few-shot LogP tasks (similar to random sampling) and generally failed to meaningfully improve in biological activity optimization compared with random generation.",
            "experimental_validation": false,
            "challenges_or_limitations": "Not designed for few-shot in-context learning; struggled to capture modification rules from a handful of examples in this paper's experiments.",
            "uuid": "e6875.2",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "VJTNN",
            "name_full": "Variational Junction Tree Neural Network (VJTNN)",
            "brief_description": "A hierarchical generative model for molecular graphs that samples structural motifs and assembles them to generate new molecules; used as a baseline for few-shot molecule optimization in the paper.",
            "citation_title": "Hierarchical generation of molecular graphs using structural motifs",
            "mention_or_use": "use",
            "model_name": "VJTNN",
            "model_type": "hierarchical graph generative model / VAE-style",
            "model_size": null,
            "training_data_description": "Original training data not specified in this paper; authors used official released code adapted to the few-shot tasks.",
            "generation_method": "Graph/motif-based generative sampling (junction-tree based); adapted to few-shot by training on K modification cases similarly to JTVAE.",
            "chemical_representation": "Graph of motifs/junction-tree plus molecular graphs; maps to SMILES for evaluation.",
            "target_application": "De novo molecule generation and optimization (baseline).",
            "constraints_used": "Adapted per-experiment to K-shot sample training; no additional constraints described here.",
            "integration_with_external_tools": "Used official code and evaluation pipeline (RDKit calculators for properties).",
            "dataset_used": "Adapted to DrugLLM's evaluation dataset (K-shot examples from test tasks); original pretraining corpora not specified in this paper.",
            "evaluation_metrics": "Success rate, molecular similarity, property improvement distributions.",
            "reported_results": "Reported to obtain ~50% success rate on few-shot LogP tasks (similar to random) and generally failed to meaningfully improve biological activities compared with DrugLLM.",
            "experimental_validation": false,
            "challenges_or_limitations": "Like other VAE-based baselines, not optimized for few-shot in-context learning; struggled on limited-example modification tasks.",
            "uuid": "e6875.3",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MoLeR",
            "name_full": "MoLeR (scaffold-based molecule generator)",
            "brief_description": "A scaffold/motif-based generative model that extends molecular scaffolds with structural motifs; used here as a baseline for few-shot property optimization comparisons.",
            "citation_title": "Learning to extend molecular scaffolds with structural motifs",
            "mention_or_use": "use",
            "model_name": "MoLeR",
            "model_type": "scaffold-based graph generative model",
            "model_size": null,
            "training_data_description": "Original MoLeR training data not specified in this paper; authors used official code adapted to K-shot tasks.",
            "generation_method": "Scaffold extension via learned motif assembly; adapted to K-shot by training on provided modification examples.",
            "chemical_representation": "Scaffold + motif graph tokens mapped to standard molecular graphs/SMILES.",
            "target_application": "De novo molecule generation and optimization (baseline).",
            "constraints_used": "Adapted to K-shot training per experiment; no extra constraints described.",
            "integration_with_external_tools": "Used official code; property evaluation via RDKit-based calculators.",
            "dataset_used": "Adapted to DrugLLM's evaluation datasets (K-shot examples); original pretraining corpora not specified here.",
            "evaluation_metrics": "Success rate, similarity, property distribution shifts.",
            "reported_results": "Reported to achieve ~50% success rate on LogP few-shot tasks (similar to random) and did not match DrugLLM's performance on biological activity optimization.",
            "experimental_validation": false,
            "challenges_or_limitations": "Not built for in-context few-shot learning; struggled to extract modification rules from small example sets in these experiments.",
            "uuid": "e6875.4",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT-3 family / OpenAI conversational model referenced via API)",
            "brief_description": "A general-purpose conversational LLM evaluated here as a zero-shot/few-shot baseline for instruction-guided molecule optimization; capable of generating SMILES in some cases but performs worse than DrugLLM on the zero-shot molecule optimization tasks reported.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "ChatGPT (via OpenAI API)",
            "model_type": "general-purpose decoder-only conversational LLM (prompt-based, zero-/few-shot)",
            "model_size": null,
            "training_data_description": "Pretrained on large multi-domain text corpora (public+proprietary); not specialized on GMR; exact chemical content unspecified in paper.",
            "generation_method": "Prompt-based (zero-shot or few-shot) SMILES/text generation; used in this paper with carefully constructed prompts containing example modifications to attempt molecule optimization.",
            "chemical_representation": "SMILES (model produced SMILES strings as output when successful); often struggled to output valid molecular strings for some models.",
            "target_application": "Tested for zero-shot molecule optimization (instruction-guided property optimization) as a general LLM baseline.",
            "constraints_used": "Prompts included example modifications and molecule descriptions; balanced prompts used when applicable.",
            "integration_with_external_tools": "Invoked via OpenAI API; generated outputs post-processed and validated with RDKit and the authors' evaluation pipeline.",
            "dataset_used": "Not retrained on DrugLLM corpora; evaluated on the paper's zero-shot test set (6,000 instructions across six optimization tasks).",
            "evaluation_metrics": "Zero-shot success rate (as in Table 3), validity (via RDKit), property improvement measured by RDKit calculators/predictors.",
            "reported_results": "Zero-shot success rates (Table 3): across tasks ChatGPT reported intermediate performance (e.g., QED & FractionCSP3 ≈11%, QED & TPSA ≈15%, LogP & FractionCSP3 ≈30%, LogP & TPSA ≈19%, LogP & #Rotatable ≈19%). Overall underperformed DrugLLM.",
            "experimental_validation": false,
            "challenges_or_limitations": "General LLM lacks specialized chemical/biological training and can output invalid or duplicated SMILES; limited mapping from natural language instruction to precise molecular property changes compared with DrugLLM.",
            "uuid": "e6875.5",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A large multimodal/general LLM (GPT-4) used as a high-quality general baseline for zero-shot molecule optimization; understood instructions and optimized some molecules but with lower success rates than DrugLLM in the reported tasks.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 (via OpenAI API)",
            "model_type": "large general-purpose LLM (prompt-based, zero-/few-shot)",
            "model_size": null,
            "training_data_description": "Trained on very large web-scale corpora including code and other data; not specialized on GMR or the DrugLLM training corpus (paper does not re-train GPT-4).",
            "generation_method": "Prompt-based zero-/few-shot SMILES/text generation provided via API; evaluated directly on instruction-guided molecule optimization tasks.",
            "chemical_representation": "SMILES output when successful; requires careful prompting to elicit valid SMILES.",
            "target_application": "Zero-shot molecule optimization (instruction-driven property composition optimizations) as a strong general LLM baseline.",
            "constraints_used": "Carefully constructed prompts containing molecule descriptions and few-shot examples where applicable.",
            "integration_with_external_tools": "Invoked via OpenAI API; outputs validated via RDKit and property predictors.",
            "dataset_used": "Evaluated on the paper's zero-shot test tasks (6,000 instructions); not retrained on DrugLLM corpus.",
            "evaluation_metrics": "Zero-shot success rates per Table 3; property calculations via RDKit/predictors.",
            "reported_results": "Zero-shot success rates (Table 3) generally higher than ChatGLM/ChatGPT on some tasks but lower than DrugLLM (e.g., QED & FractionCSP3 ≈20%, QED & TPSA ≈20%, LogP & FractionCSP3 ≈40%, LogP & TPSA ≈43%, etc.).",
            "experimental_validation": false,
            "challenges_or_limitations": "Though powerful, general LLM training does not guarantee understanding of structure–property mappings for molecular optimization; lower success rates than DrugLLM on specialized few-shot/zero-shot property composition tasks.",
            "uuid": "e6875.6",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ChatGLM",
            "name_full": "ChatGLM (Chinese bilingual conversational LLM)",
            "brief_description": "A general-purpose conversational LLM (ChatGLM) evaluated as a zero-shot baseline for molecule optimization; struggled to provide appropriate molecules and often returned duplicated input molecules in these tasks.",
            "citation_title": "Glm-130b: An open bilingual pre-trained model",
            "mention_or_use": "use",
            "model_name": "ChatGLM (official released model used locally)",
            "model_type": "general conversational LLM (prompt-based)",
            "model_size": null,
            "training_data_description": "Pretrained on large bilingual corpora; not specialized on chemical/biological corpora for molecule generation as used in this paper.",
            "generation_method": "Prompt-based (zero-shot), used with prompts containing molecule descriptions and example modifications.",
            "chemical_representation": "Attempted SMILES/text outputs; often failed/duplicated inputs per paper.",
            "target_application": "Zero-shot molecule optimization baseline.",
            "constraints_used": "Prompts included example modifications where applicable; no special chemical tuning.",
            "integration_with_external_tools": "Deployed locally using released code; generated outputs validated via RDKit and evaluation pipeline.",
            "dataset_used": "Evaluated on the drug-optimization zero-shot test set (6,000 instructions); not retrained.",
            "evaluation_metrics": "Zero-shot success rates (Table 3) and validity as checked by RDKit.",
            "reported_results": "Table 3 reports very low success rates (e.g., QED & FractionCSP3 0.02 ≈2%, QED & TPSA 0.03 ≈3%, etc.); often returned duplicates of input molecules rather than optimized variants.",
            "experimental_validation": false,
            "challenges_or_limitations": "Poor mapping from instruction to valid molecular modifications in this domain; frequent duplication of input molecules; low success rates.",
            "uuid": "e6875.7",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA-backbone",
            "name_full": "LLaMA-based Transformer backbone (model hyperparameters adopted)",
            "brief_description": "DrugLLM adopts the LLaMA model architecture/parameterization as its backbone (decoder-only transformer); DrugLLM uses these settings and expands vocabulary for chemical tokens.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA-style transformer (adapted)",
            "model_type": "decoder-only transformer architecture (foundation LLM backbone)",
            "model_size": "architectural settings corresponding to 7B parameter configuration",
            "training_data_description": "DrugLLM training corpus (molecule modification paragraphs) replaced generic LLaMA pretraining data; LLaMA referenced for architecture/hyperparameters.",
            "generation_method": "Autoregressive next-token prediction using Transformer decoder layers (32 layers, 32 attention heads, hidden dim 4096).",
            "chemical_representation": "Vocabulary extended with frequently-used SMILES tokens and BPE applied; primary input representation was GMR in DrugLLM experiments.",
            "target_application": "Backbone architecture to build DrugLLM for molecule generation/optimization.",
            "constraints_used": "Architectural limits and token-length constraints inherited from decoder-only LLM design (prompt length limits resulted in ≤9-shot practical limit).",
            "integration_with_external_tools": "Not an external tool but an architectural choice; DrugLLM training used AdamW optimizer and cosine annealing learning-rate schedule.",
            "dataset_used": "Architecture applied to DrugLLM corpus (ZINC + ChEMBL derived paragraphs).",
            "evaluation_metrics": "Not directly evaluated; used as the model backbone for DrugLLM whose performance is reported.",
            "reported_results": "DrugLLM built on this backbone achieves the reported few-shot and zero-shot results (see DrugLLM entry).",
            "experimental_validation": false,
            "challenges_or_limitations": "Prompt length / token budget constraints limit number of example shots; vocabulary/tokenization trade-offs require representation choices (GMR vs SMILES).",
            "uuid": "e6875.8",
            "source_info": {
                "paper_title": "DrugLLM: Open Large Language Model for Few-shot Molecule Generation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 2,
            "sanitized_title": "junction_tree_variational_autoencoder_for_molecular_graph_generation"
        },
        {
            "paper_title": "Hierarchical generation of molecular graphs using structural motifs",
            "rating": 2,
            "sanitized_title": "hierarchical_generation_of_molecular_graphs_using_structural_motifs"
        },
        {
            "paper_title": "Learning to extend molecular scaffolds with structural motifs",
            "rating": 2,
            "sanitized_title": "learning_to_extend_molecular_scaffolds_with_structural_motifs"
        },
        {
            "paper_title": "Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins",
            "rating": 2,
            "sanitized_title": "druggpt_a_gptbased_strategy_for_designing_potential_ligands_targeting_specific_proteins"
        },
        {
            "paper_title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "rating": 2,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.018581749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 2024</p>
<p>Xianggen Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Yan Guo 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Haoran Li 
Jin Liu 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Shudong Huang 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>Information Technology Research Center
Beijing Academy of Agriculture and Forestry Sciences
100097BeijingChina</p>
<p>Bowen Ke 
Department of Anesthesiology
Laboratory of Anesthesia and Critical Care Medicine
Frontiers Science Center for Disease-related Molecular Network
National-Local Joint Engineering Research Centre of Translational Medicine of Anesthesiology
West China Hospital
Sichuan University
610041ChengduChina</p>
<p>Jiancheng Lv 
College of Computer Science
Sichuan University
610065ChengduChina</p>
<p>DrugLLM: Open Large Language Model for Few-shot Molecule Generation
7 May 202443D72A47FAC47A99D5D0B57EC2745F6CarXiv:2405.06690v1[q-bio.BM]large language modelfew-shot learningmolecule generation
Large Language Models (LLMs) have made great strides in areas such as language processing and computer vision.Despite the emergence of diverse techniques to improve few-shot learning capacity, current LLMs fall short in handling the languages in biology and chemistry.For example, they are struggling to capture the relationship between molecule structure and pharmacochemical properties.Consequently, the few-shot learning capacity of small-molecule drug modification remains impeded.In this work, we introduced DrugLLM, a LLM tailored for drug design.During the training process, we employed Group-based Molecular Representation (GMR) to represent molecules, arranging them in sequences that reflect modifications aimed at enhancing specific molecular properties.DrugLLM learns how to modify molecules in drug discovery by predicting the next molecule based on past modifications.Extensive computational experiments demonstrate that DrugLLM can generate new molecules with expected properties based on limited examples, presenting a powerful few-shot molecule generation capacity.</p>
<p>Introduction</p>
<p>Small molecules play a critical role in drug discovery due to their ability to bind to specific biological targets and modulate their functions (Kovachka et al., 2024;Offensperger et al., 2024;Scott et al., 2016).In fact, over the past decade, according to the approval records of the U.S. Food and Drug Administration (FDA), small molecule drugs have accounted for 76% of the total number of drugs approved for the market (Brown and Wobst, 2021;Norsworthy et al., 2024).Small molecules are advantageous in drug discovery because they can be synthesized relatively easily and have good bioavailability, making them more likely to reach their intended targets in vivo (especially when passing through cell membranes) (Vargason et al., 2021).However, based on the current research technologies, it is very challenging to design a molecule with ideal properties, and it will consume a lot of resources and time.For example, in the drug development process, it takes 9 to 12 years and billions of dollars to find an effective drug (Adams and Brantner, 2006;Dickson and Gagnon, 2009).</p>
<p>The vastness of the search space for new molecules, with up to 10 60 synthetically creatable drug-like molecules, presents a significant challenge in drug design as chemists must navigate this immense space to identify molecules that interact with a biological target, and while modern techniques allow for the testing of over 10 6 molecules in a laboratory setting, larger experiments become prohibitively expensive (Segler et al., 2018).As such, computational tools are necessary to help narrow down the search space.Virtual screening is one such strategy used to identify promising molecules from millions of existing or billions of virtual molecules (Lyu et al., 2023).But highthroughput screening and virtual screening only consider known molecules that are synthetically accessible and fall short of producing novel molecules (Crunkhorn, 2022;Sadybekov et al., 2022).</p>
<p>As an alternative to exploring the huge molecule space, de novo drug design exhibits the remarkable ability to generate entirely novel and distinctive molecules (Ren et al., 2024;Tropsha et al., 2024).Traditional de novo drug design methods use molecular construction rules to generate new molecules based on the receptor structure (Gillet et al., 1995;Waszkowycz et al., 1994) or the ligand structure (Afantitis et al., 2011).Recently, deep learning and reinforcement learning techniques offer promising potential in de novo drug design due to their powerfutl approximation capacity (Blaschke et al., 2018;Li et al., 2018;Liu et al., 2020;Putin et al., 2018).In particular, Popova et al. (2018) integrates both generative and predictive neural networks to generate novel targeted chemical libraries in a trial-and-error manner.Jin et al. (2018) propose a junction treebased variational autoencoder to learn a continuous molecule space and generate new molecules by sampling from it.</p>
<p>Despite the development of various deep learning (DL)-based methods for de novo drug design, the field of few-shot molecule generation remains notably underexplored.Few-shot molecule generation aims to generate new molecules with expected properties given limited molecule examples.Most of the current de novo drug design approaches require thousands of data for learning (Korshunova et al., 2022).However, data scarcity is a prevalent issue in drug discovery due to the high costs associated with biological experimentation (Wang et al., 2023).Consequently, the ability to perform few-shot generation is of paramount importance for the advancement of de novo drug design techniques.</p>
<p>The large language models (LLMs) have achieved significant progress in natural language processing, especially in the few-shot learning problem (Ahmed and Devanbu, 2022).Despite the emergence of diverse LLMs (Chang et al., 2024), they fall short in handling the languages in biology and chemistry (Ross et al., 2022).For example, they are still struggling to capture the relationship between molecule structure and the corresponding properties.Therefore, how do we build an LLM that can accurately characterize the "structure-effect-metabolism-toxicity" relationships in molecules?</p>
<p>In this work, we presented DrugLLM, a large language model for drug design.In DrugLLM, molecules are represented using Group-based Molecular Representation (GMR), which is a novel type of molecular representation to address token abundance, cyclic complexity, and structural sensitivity inherent in SMILES (O'Boyle, 2012).GMR makes structural groups as units to build the topological structure of molecules and transform them into linear sequences.</p>
<p>Furthermore, we will provide an in-depth exposition of the training methodology employed by DrugLLM.The methodology organizes the modification sequences in accordance with specific molecular properties.Drawing an analogy, a case of molecule modification (a pair of molecules with similar structures) toward a certain property serves as a "sentence".Multiple cases of modification toward the identical property constitute a paragraph.By continuously predicting the next molecule based on the modification history, DrugLLM learns the intrinsic relationship between the molecular structure and the corresponding property.To the best of our knowledge, DrugLLM is the first large language model for few-shot molecule generation.</p>
<p>Results</p>
<p>The DrugLLM Framework</p>
<p>The focus of this work is to train a large language model that can capture the relationship between the molecule structure and the corresponding chemical and biological activities.Unlike ChatGPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023), which are trained on massive text data from the internet, and DrugGPT (Li et al., 2023)that uses SMILES as its representation, DrugLLM employs Group-based Molecular RepresSMILESentation (GMR) strings as its primary language representation.GMR leverages structural groups to depict molecular architectures, thereby effectively overcoming three principal challenges inherent in the application of SMILES notation within model processing contexts: (1) Token Abundance: In the SMILES format, each character is considered a separate token, which can result in an unwieldy number of tokens and subsequently consume considerable computational resources during training.(2) Cyclic Complexity: The representation of cyclic structures within molecules is particularly intricate in SMILES, which increases the difficulty of model training.(3) Structural Sensitivity: Even minor alterations in a molecule's structure can produce significant discrepancies in the corresponding SMILES representation.</p>
<p>As shown in Figure 1A, the GMR framework employs unique string identifiers to represent distinct structural groups.These identifiers are linked by numerical position data flanked by a slash.By employing GMR, the model can recognize molecular strings by treating structural groups as units, thereby reducing the number of input and output tokens.Additionally, GMR removes cyclic structures by merging them, simplifying the logic of molecular assembly and lowering the difficulty of model recognition.It also minimizes the discrepancies in SMILES strings that result from even small structural changes.</p>
<p>To train DrugLLM, we construct sentences and paragraphs composed of molecule modifications as training data (Figure 1B).Specifically, DrugLLM regards the modification between two molecules with similar structures as a sentence.A series of such modifications are viewed as a sequence of sentences that form a paragraph.Note that we impose the constraint that the molecular modifications in a paragraph should characterize the identical property.For instance, if the first three cases of molecule modifications describe the increase in the number of hydrogen bond acceptors, we expect all subsequent sentences in that paragraph to also discuss the increase in acceptor numbers.In this way, the contents of a paragraph are concentrated, making DrugLLM able to auto-regressively predict the next token based on the previous contexts.Moreover, each paragraph exhibits autonomy, encompassing a diverse array of molecular characteristics.The distinct paragraphs engage with unique molecular properties, necessitating that DrugLLM be endowed with the capability for in-context learning (a form of few-shot learning).</p>
<p>However, there are few related datasets available.In this work, we collect the tabular form of the molecule datasets from the ZINC database (Wu et al., 2018) and the ChEMBL platform (Davies et al., 2015;Mendez et al., 2019), and convert them into the corresponding sentences and paragraphs.In total, we collected over 25, 000, 000 modification paragraphs and 200, 000, 000 molecules to build the training dataset (Table 1).The dataset involves over 10,000 different molecular properties or activities, such as the count of hydrogen bond acceptors and the topological  Following recent work on the pre-training large language models, DrugLLM is based on the Transformer architecture (Vaswani et al., 2017).We adopt the parameters of LLama 7B (Touvron et al., 2023) and expand the vocabulary by introducing the frequently-used SMILES tokens.These tokens are divided by the byte pair encoding (BPE) algorithm (Sennrich et al., 2016).We train DrugLLM using the AdamW optimizer for six weeks on eight NVIDIA RTX 3090 GPUs, where it learns to generate the paragraphs from scratch.In the view of machine learning, the paragraph plays as a process of few-shot molecule generation.Therefore, the trained DrugLLM is able to directly perform few-shot molecule generation without further fine-tuning.</p>
<p>DrugLLM is a few-shot learner in molecule optimization toward physicochemical properties</p>
<p>Figure 2A illustrates our approach under the K-shot learning framework, where we provide the model with K pairs of example modifications and a benchmark molecule.The objective of the model is to generate a new molecule that not only maintains structural similarity to the benchmark molecule but also exhibits superior properties (either increased or decreased, as guided by the examples).Due to input token limitations, we restrict the number of molecular optimization examples to a maximum of nine pairs.To visually represent the structural similarity between the benchmark and generated molecules in the chemical space, we employ the Uniform Manifold Approximation and Projection (UMAP) method to create a chart (Figure 2B).There is a high degree of consistency between the distribution of the generated molecules (on the left) and the source molecules (on the right).This distributional similarity, coupled with the notable improvement in the LogP properties of the generated molecules, underscores the robust capability of the model to optimize the properties of the benchmark molecule.</p>
<p>To evaluate the capacity of DrugLLM in terms of few-shot molecule generation, we select four physicochemical properties that are not seen by DrugLLM as the test tasks, including the wateroctanol partition coefficient (LogP), solubility, synthetic accessibility, and topological polar surface area (TPSA).As these four molecular properties can be accurately estimated by machine learningbased scripts, they are widely used in the assessment of molecule generation models.For comparison, we take the junction tree-based variational auto-encoder (JTVAE) (Jin et al., 2018), the variational junction tree neural network (VJTNN) (Jin et al., 2020), and the scaffold-based molecule generator (MoLeR) (Maziarz et al., 2021).We also include a random generation control implemented by random sampling based on the latent space of JTVAE.The quality of the generated molecules was assessed based on their success rate and molecular similarity.The success rate represents the proportion of generated molecules that adhere to the rule of examples of modifications.To avoid the generation bias of the generators, the input contexts (i.e., the prompts of the language models) describe the increment or decrement of the property with a balanced proportion.Although these models are not initially designed for few-shot learning, they are state-of-the-art molecule generators in literature.</p>
<p>We first present the distributions of several key properties -LogP, solubility, synthetic accessibility, and TPSA -for both the source and generated data in Figure 3A.These distributions are visualized using Kernel Density Estimation (KDE).The significant numerical improvements of the model in optimizing these key properties are clearly demonstrated, further attesting to the effectiveness of the model in optimizing molecular properties.Next, as shown in Figure 3B, we report the performance of few-shot generation with respect to the LogP value.We note that the three baseline molecule generators, namely JTVAE, VJTNN, and MoLeR, just obtained a success rate of about 50%, which is similar to a random generation.In contrast, DrugLLM exhibits a progressive improvement in few-shot molecule generation, with the accuracy of the generated molecules increasing incrementally to 75% as the number of shots increases.Performance comparisons on molecular solubility, synthetic accessibility, and TPSA are similar and consistent.When it comes to similarity, it is typically more challenging to optimize a molecule with fewer modifications (i.e., higher similarity).Despite this, DrugLLM maintains a higher success rate even with increased generation similarity, underscoring its superior performance in the few-shot generation.Furthermore, we note that DrugLLM-GMR slightly outperforms DrugLLM-SMILES, highlighting the benefits of GMR in large model training.</p>
<p>DrugLLM is a few-shot learner in molecule optimization towards biological activities</p>
<p>Since DrugLLM shows impressive few-shot generation capacity in physicochemical properties, we next validate the effectiveness of DrugLLM in the biological activities of molecules, which is more challenging.The molecules produced by DrugLLM are usually novel and are not recorded in the ChEMBL database.Unlike the physicochemical properties mentioned above, the biological activities of molecules (e.g. the Ki value on streptokinase A) are difficult to estimate by chemical or physical rules.In addition, the lengthy time and high costs associated with wet-lab experiments hinder the large-scale evaluation of molecules.Instead, we leverage a message-passing neural network to predict biological activities.Specifically, before building the DrugLLM dataset through the ChEMBL database, we scan all biological activities and select the one that has relatively adequate samples (N ≥ 800) and could induce an accurate property predictor (Pearson r ≥ 0.75).Finally, 10 activities are obtained and excluded from the training data.These activities are used to test the optimization of the few-shot DrugLLM.As the Pearson correlation of the prediction of the predictor is greater than 0.75, it could correlate well with the real evaluation in statistics.</p>
<p>We observe that the three generator baselines fail to obtain meaningful improvement compared with the random generation (Table 2), indicating that these molecule generators still struggle to capture the modification rules underlying the limited examples.As for DrugLLM, it significantly outperforms the other baselines by a large margin in most of the test properties.In particular, DrugLLM can generate appropriate molecules that bind to Rho-associated protein kinase 1, with a success rate of 76%.Note that these test properties are not observable in the training of DrugLLM.These results demonstrate that DrugLLM is able to figure out the intrinsic rules of the molecule modifications given a limited number of examples for an unknown molecular property.</p>
<p>DrugLLM support instruction guided molecule optimization in a zero-shot manner</p>
<p>Previous experiments demonstrated that DrugLLM can accept multiple pairs (i.e., shots) of molecules as references to learn the modification rules and generate new molecules with the desired properties.In this section, we explore zero-shot molecule optimization, which involves generating modified molecules with better properties of interest according to natural language instruction without any specific training instances.In this experimental setting, we assume that when DrugLLM is trained on a large scale of properties and their compositions, it is capable of generalizing to optimize the molecules toward unseen compositions of properties.Therefore, we move six optimization tasks out of the DrugLLM training set in advance and leave them as test tasks.For example, all samples related to the optimization of quantitative estimation of drug similarity (QED) and topological polar surface area (TPSA) are in the test set, but the samples related to the optimization of each single property are in the training set.Based on this setting, we build the test set that contains 6, 000 instructions, each optimization task for 1000.The generated molecules are evaluated by the Python scripts via the RDKit library.</p>
<p>It is noteworthy that DrugLLM is one of the very few approaches that support zero-shot molecule optimization.Apart from ChatGPT (Brown et al., 2020), GPT-4(Achiam et al., 2023), and ChatGLM (Zeng et al., 2022), the other large language models, e.g., LLaMA (Touvron et al., 2023), Alpaca (Maeng et al., 2017), and Vicuna (Chiang et al., 2023), were unable to generate valid SMILES strings of molecules.Thus, we only compared the zero-shot molecule optimization capacity between DrugLLM and ChatGPT, GPT-4, and ChatGLM, all of which are trained on thousands of billion tokens.The challenge of zero-shot molecule optimization lies in two folds.On the one hand, the mapping between semantics and molecular property is hard to learn from the general corpus.On the other hand, the biological data related to the structure-property relationship is not sufficient due to the lengthy time and high costs associated with wet-lab experiments.As a result, we observed that ChatGLM struggled to provide appropriate molecules on all the zero-shot molecule optimization tasks (Table 3), with most generations outputting the duplicated molecules with the input ones.In addition, ChatGPT and GPT-4 were able to understand the instructions and optimize some of the given molecules.However, the success rate is relatively low.In terms of DrugLLM, it improves the optimization success rates by significant margins compared with the other LLM, indicating a better capacity in instruction understanding and molecule optimization.</p>
<p>Discussion</p>
<p>In this study, we introduce a computational task named few-shot molecule optimization, which is one of the few-shot generation problems.Given a molecule of interest, the task involves generating new molecules that adhere to the rules underlying the few modification examples.Although various few-shot learning tasks have been proposed and investigated (Ma et al., 2021;Stanley et al., 2021;Zhang et al., 2024), there are few studies that consider few-shot molecule generation.Few-shot molecule optimization requires the model to capture the abstract rules in a small number of examples and apply the rules to new molecules, requiring a comprehensive understanding of "structure-effect-metabolism-toxicity" relationships.As expected, the current methods struggle to accomplish few-shot molecule optimization, including ChatGPT and the other competitive molecule generators.In comparison, DrugLLM exhibits impressive performance, taking a solidified step toward general artificial intelligence in drug design.</p>
<p>DrugLLM is a large language model (LLM), built on a large number of textual data that span a wide variety of small molecules and biological domains.Recently, LLMs, such as ChatGPT (Brown et al., 2020), Alpaca (Maeng et al., 2017), and ChatGLM (Zeng et al., 2022), have amazing capabilities for general-purpose natural language generation.However, they are designed for general use and lack profound biological and pharmaceutical knowledge.We notice that there are also several LLMs for biological and medical fields, such as BioGPT (Luo et al., 2022) and DrugGPT (Li et al., 2023).However, these LLMs still follow traditional training strategies that learn to generate natural language as in the biomedical article.This raises an open question that how LLM understands the language of biology and chemistry and how to perform few-shot learning in this field.In this work, we propose a novel solution in which DrugLLM iteratively performs similar molecule modifications according to the context using GMR.Experiments demonstrated its exceptional effectiveness in a few-shot molecule optimization.</p>
<p>Despite the advantages of our method, this study has several limitations.Firstly, DrugLLM merely supports up to 9 shots of molecule modifications due to the limitation of the hardware.In such an input length, we have validated the few-shot learning capacity of DrugLLM.In the future, we will increase the input length (i.e., the number of shots) to achieve more impressive performance in drug design.Secondly, the zero-shot molecule optimization of DrugLLM is relatively elementary.The current DrugLLM is only capable of optimizing the molecules toward the composition of two known molecular properties.The zero-shot learning ability for arbitrary instructions is still lagging behind.Thirdly, the GMR currently in use has difficulty representing a small number of complex molecules under certain circumstances and lacks certain standardization measures.In the future, we will optimize for special cases of GMR representation and add standardization methods to reduce the small number of encoding errors that the model may generate.</p>
<p>In conclusion, this study presents the first attempt to build a large language model for fewshot molecule generation and optimization.Based on the tabular data related to molecular properties and biological activities, we build a large-scale textual corpus in the format of the sequences of molecule modifications.DrugLLM is trained to predict the next molecules based on historical modifications in an autoregressive manner.In extensive computational experiments, we observed that DrugLLM surpassed all the competitive methods (including GPT4) in optimizing new molecules in the few-shot learning setting on over 20 properties or biological activities.These results establish the substantial enhancement of efficacy facilitated by our proposed methodology in molecule generation and optimization, highlighting the potential of DrugLLM as a powerful computational tool in drug discovery.</p>
<p>METHOD DETAILS</p>
<p>Data collection and preparation</p>
<p>To train and analyze the DrugLLM model, we construct a large-scale dataset from the ZINC (Wu et al., 2018) and ChEMBL (Davies et al., 2015;Mendez et al., 2019) datasets.ZINC is a free database that contains more than 230 million purchasable compounds in ready-to-dock, 3D formats.We filter the druglike molecules from ZINC and obtain 4.5 million molecules.ChEMBL is a comprehensive repository for bioactive compounds with their properties.We gather bioactivity data from the ChEMBL database with the corresponding Web resource client.Following the preprocessing pipeline in (Stanley et al., 2021), we excluded all compounds that are not druglike molecules.A standard cleaning and canonicalization procedure was applied to filtered compounds.All of the molecules were represented by SMILES strings and labeled with certain properties.To facilitate property comparison between two molecules, we only consider property categories with real numbers.Therefore, we obtained a large-scale dataset that comprised thousands of tabular data, each table corresponding to hundreds of molecules measured by an identical property.</p>
<p>Based on the collected tabular data, we then transform them into meaningful textual sentences and paragraphs.In particular, we regard the modification between two molecules with similar structures as a sentence and multiple cases of molecular modifications as a paragraph.In the meantime, we stipulate that the molecular modifications in a paragraph should describe the same property changes.In other words, if the first two cases of molecule modifications indicate the increase of solubility, we would expect the rest sentences of this paragraph to be all about the solubility increase.The above stipulation was realized by a heuristic algorithm: given a pool of molecules with their property (in tabular form), we first clustered the molecules in terms of the molecular scaffolds with randomly selected clustering centers.If the similarity between a molecule and a center is greater than 0.6, the molecule is clustered at that center.The number of clustering centers would dynamically increase until all of the molecules in the pool are classified.</p>
<p>Apart from the modification of the molecule to a single property, we also consider the compositions of multiple properties, which are mainly involved in the simple molecular properties that can be calculated by Python scripts.For example, we include LogP, Topological polar surface area (TPSA), and their composition in the training set for model training.In total, we collect over 25 million modification paragraphs and 2 billion molecules to build the training dataset.The dataset involves over 10, 000 different molecular properties, activities, and compositions.In ad-dition to the SMILES molecule, we also added descriptions of the property optimizations in each paragraph to build the relationship between the molecule structures and the semantic meaning of the properties.</p>
<p>Group-based molecular representation (GMR)</p>
<p>The core of the GMR framework involves decomposing molecules into structural groups and noting the inter-group connections, facilitating the group-based reconstruction of SMILES strings.GMR begins by assigning a unique string identifier to each structural group, thereby constructing a comprehensive dictionary.When a molecule is processed within the GMR framework, its SMILES string is converted into an encoded string that encapsulates both the identity of the structural groups and their positional relationships within the molecule.For computational analyses or property evaluations that require the original SMILES string, the encoded string can be decoded by applying a decoding algorithm.In the specific implementation of the GMR framework, there are three key steps:</p>
<p>(1) Dictionary construction: Initially, we leverage the extensive molecular data resources available in the ChEMBL database.Using SMILES expressions, we extract information about the ring structures in the molecule, merge intersecting rings, and identify structural groups on the rings.For the nonring parts of the molecule, we employed a strategy of breaking all C-C bonds and treating the remaining molecular fragments as independent structural groups.This approach allows us to decompose all the groups of the molecule, assign a unique string identifier to each group, and construct a comprehensive dictionary.</p>
<p>(2) Molecular encoding: The encoding process, as depicted in Algorithm 1, is initiated by splitting the SMILES string of an individual molecule into several structural units.We systematically decompose the molecule, removing each structural group and verifying the connectivity of the molecule after each removal.If the molecule remains connected after removal of a structural group, we record the two atoms at the connection point.Since marking the connection point may cause changes in the atom index and affect the subsequent normalization process of the molecule, we use a breadth-first search algorithm to characterize the marked atoms and their adjacent area in detail.This forms a feature description of the atoms.After removing the mark and performing SMILES normalization on the structural group and molecular fragment after splitting, we use the previously obtained atom feature description to re-identify and record the atom index of these two atoms as connection location information.This process is repeated until the molecule is completely decomposed into a single structural group, at which point the molecular fragment serves as the starting point for encoding.Starting from the encoding starting point, we build the basis of the encoding string according to the corresponding string in the dictionary.Subsequently, we traverse the structural groups recorded during the removal process and their corresponding atom indices, using the "/" character to separate different location information.Gradually, we integrate the strings corresponding to the structural groups in the dictionary and the location information into the encoding string, eventually generating a complete and accurate molecular encoding result.</p>
<p>(3) Molecular decoding: The decoding process is essentially the reverse of the encoding process.We use the encoded molecular fragment as the starting point for splicing and gradually recombine each structural group in the correct position according to the connection information recorded in the encoding.This process is repeated until all structural groups are correctly spliced back, thereby restoring the original molecular SMILES.This ensures the integrity and reversibility of molecular information.</p>
<p>The implementations of DrugLLM</p>
<p>Similar to ChatGPT, the training objective of DrugLLM is to iteratively predict the next token of the paragraphs.Formally, a generated paragraph x is composed of the optimization description o and the molecule modifications m, given by
x = [o, m 1 , m 2 , • • • , m N ],(1)
where m n stands for the n-th case of the molecule modification.Essentially, DrugLLM is to approximate the probability
P(x t |x 1 , x 2 , • • • , x t−1 ) = DrugLLM(x 1 , x 2 , • • • , x t−1 ),(2)
where x t is a token in the paragraph x.</p>
<p>However, the testing objective of the few-shot molecule optimization is to predict the optimized molecules given the few shots of molecule modifications.That is
m g = DrugLLM(m 1 , m 2 , • • • , m K , m o ),(3)
where m g stands for the generated molecules of DrugLLM in the K shots input.m o represents the query molecule that needs to be optimized.Similarly, the testing objective of the zero-shot molecule optimization is to predict the optimized molecules given the descriptions of the optimization tasks, given by
m g = DrugLLM(o, m o ).(4)
We adopt the LLaMA model architecture as the basic backbone of our DrugLLM.Specifically, DrugLLM is a Transformer decoder with 32 layers and 32 attention heads.The hidden dimension is set to 4096 and the batch size is 64.As a result, DrugLLM has 7 billion parameters, all of which are updated during the pre-training.The training process employs the AdamW optimizer with a learning rate of 3 × 10 −5 , and we adopt a cosine annealing schedule to adjust the learning rate.</p>
<p>The implementations of the competitive baselines</p>
<p>There are very few methods that support few-shot molecule optimization or generation.In this work, we take the widely used and powerful molecule generators as baselines, including JTVAE, VJTNN, MoLeR, and a control model (random generation).For these methods, we used the released official codes and adapted them to perform few-shot optimization tasks.For JTVAE, which is designed to generate molecules from a pre-trained latent space, we follow the convention to optimize the molecules based on the released pre-trained JTVAE model.To accomplish the K-shot optimization, we leveraged the K modification cases as the training samples and used the trained JTVAE to generate the new molecules for evaluation.The evaluation procedures of VJTNN and MoLeR are similar to JTVAE except for the exclusion of the pre-trained model.The random generation model is implemented by random sampling based on the latent space of the pre-trained JTVAE.</p>
<p>We further incorporated advanced large language models as benchmarks, including Chat-GLM, ChatGPT, and GPT-4.These models require carefully constructed prompts to generate relevant output data.In crafting these prompts, we integrated the characteristic descriptions of the molecules to be optimized and instances of a few-shot optimization to ensure that the model can accurately understand the task requirements.For ChatGLM, we used the officially released code and pre-trained models, deployed and executed in a local server environment, to obtain the model prediction results.In contrast, for ChatGPT and GPT-4, we utilized their online service capabilities by transmitting the processed input data through their official API interfaces, thereby eliciting the corresponding model outputs from remote servers.</p>
<p>Figure 1 :
1
Figure 1: Schematic overview of the DrugLLM framework.A, Construction process.Group-based Molecular Representation (GMR) is constructed from molecular structure units.B, Training framework.DrugLLM is trained on molecular modifications, with each paragraph representing a unique attribute.Each paragraph is self-contained and represents multiple characteristics, with different paragraphs corresponding to different attributes.</p>
<p>Figure 2 :
2
Figure 2: Visualization of few-shot molecule optimization.A, The training and testing examples of fewshot molecule optimization.B, Chemical space navigation by transfer learning.The UMAP plot shows the distribution of 15000 molecules selected from the source space and their corresponding 15000 molecules in the generated space after LogP property optimization.Different values of LogP are represented by different colors.</p>
<p>Figure 3 :
3
Figure 3: The performance of few-shot molecule optimization in physiochemical properties.A, The distribution of the water-octanol partition coefficient (LogP), Solubility, Synthetic Accessibility, and Topological Polar Surface Area (TPSA) for the source data and the generated data, represented by Kernel Density Estimation (KDE).The KDE demonstrations of the source data and the generated data are displayed by blue solid lines and red solid lines, respectively.Histograms (hist) for the source data and the generated data are represented by blue bars and red bars, respectively.B, The performance of the generation methods in terms of the success rates and the generation similarities.</p>
<p>Table 1 :
1
Pre-training data of DrugLLM.The training data include ZINC and ChEMBL databases.Similar molecules are collected together to build the modification paragraphs.A modification paragraph contains multiple molecule modifications that aim to improve or decrease the same molecular properties.TPSA).Considering that the few-shot learning capability of machine learning models arises from their exposure to a sufficient variety of training tasks, a large scale of different paragraphs could enforce DrugLLM captures the intrinsic nature of molecule design in a few-shot fashion.
Dataset# of molecules # of paragraphs # of tasks Disk sizeZINC4.5M0.6M770780MChEMBL180.2M24M1010030Gpolar surface area (</p>
<p>Table 2 :
2
The performance of few-shot molecule optimization toward biological activities
Target</p>
<p>Table 3 :
3
The success rates (%) of individual methods in zero-shot molecule optimization.
MethodChatGLM ChatGPT GPT-4 DrugLLMQED &amp; FractionCSP30.020.110.200.40QED &amp; TPSA0.030.150.200.47QED &amp; # Rotatable bonds0.060.150.100.59LogP &amp; FractionCSP30.030.300.400.60LogP &amp; TPSA0.040.190.430.55LogP &amp; # Rotatable bonds0.040.190.050.61</p>
<p>← moleculeAtom + "/" + groupAtom + group
Algorithm 1 Molecular encoding algorithm1: procedure ENCODEMOLECULE(molecule)2:unprocessedMolecularGroups ← splitIntoGroups(molecule)3:groupConnections ← []4:while len(unprocessedMolecularGroups) &gt; 1 do5:remainingGroups ← ∅6:for currentGroup ∈ unprocessedMolecularGroups do7:molecule, isProcessed ← ProcessGroup(molecule, currentGroup, groupConnections)8:if not isProcessed then9:remainingGroups ← remainingGroups ∪ {currentGroup}10:end if11:end for12:unprocessedMolecularGroups ← remainingGroups13:end while14:initialGroup ← first(unprocessedMolecularGroups)15:moleculeEncoding ← getDictionaryString(initialGroup)16:for all connectionItem ∈ groupConnections do17:moleculeAtom ← connectionItem[molConnectedAtom]18:groupAtom ← connectionItem[groupConnectedAtom]19:group ← getDictionaryString(connectionItem[group])20:21:moleculeEncoding ← moleculeEncoding + connectionEncoding22:end for23:return moleculeEncoding24: end procedure
connectionEncoding</p>
<p>ACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China (No. 62206192); the Natural Science Foundation of Sichuan Province (No. 2023NS-36 FSC1408); the Science and Technology Major Project of Sichuan Province; and the Fundamental Research Funds for the Central Universities (No. 1082204112364).
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Estimating the cost of new drug development: is it really $802 million?. C P Adams, V V Brantner, Health Aff (Millwood). 2522006</p>
<p>Ligand-based virtual screening procedure for the prediction and the identification of novel β-amyloid aggregation inhibitors using kohonen maps and counterpropagation artificial neural networks. A Afantitis, G Melagraki, P A Koutentis, H Sarimveis, G Kollias, Eur. J. Med. Chem. 4622011</p>
<p>Few-shot training llms for project-specific code-summarization. T Ahmed, P Devanbu, Proc. IEEE/ACM Int. Conf. Autom. Softw. Eng. 2022</p>
<p>Application of generative autoencoder in de novo molecular design. T Blaschke, M Olivecrona, O Engkvist, J Bajorath, H Chen, Mol Inform. 371-217001232018</p>
<p>): trends and future directions. D G Brown, H J Wobst, J. Med. Chem. 6452021. 2010-2019A decade of fda-approved drugs</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, Adv. Condens. Matter Phys. 332020</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, ACM Trans Intell Syst Technol. 1532024</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>Screening ultra-large virtual libraries. S Crunkhorn, Nat. Rev. Drug Discov. 21952022</p>
<p>M Davies, M Nowotka, G Papadatos, N Dedman, A Gaulton, F Atkinson, L Bellis, J P Overington, Chembl web services: streamlining access to drug discovery data and utilities. 201543</p>
<p>The cost of new drug discovery and development. M Dickson, J P Gagnon, Discov Med. 4222009</p>
<p>Sprout, hippo and caesa: Tools for de novo structure generation and estimation of synthetic accessibility. V J Gillet, G Myatt, Z Zsoldos, A P Johnson, Perspect. Drug Discovery Des. 31995</p>
<p>Junction tree variational autoencoder for molecular graph generation, ICML. W Jin, R Barzilay, T Jaakkola, 2018</p>
<p>Hierarchical generation of molecular graphs using structural motifs, ICML. W Jin, R Barzilay, T Jaakkola, 2020</p>
<p>Generative and reinforcement learning approaches for the automated de novo design of bioactive compounds. M Korshunova, N Huang, S Capuzzi, Commun Chem. 51292022</p>
<p>Small molecule approaches to targeting rna. S Kovachka, M Panosetti, B Grimaldi, S Azoulay, A Di Giorgio, M Duca, Nat. Rev. Chem. 2024</p>
<p>Druggpt: A gpt-based strategy for designing potential ligands targeting specific proteins. Y Li, C Gao, X Song, X Wang, Y Xu, S Han, 2023</p>
<p>Multi-objective de novo drug design with conditional graph generative model. Y Li, L Zhang, Z Liu, J. Cheminformatics. 102018</p>
<p>A chance-constrained generative framework for sequence optimization, ICML. X Liu, Q Liu, S Song, J Peng, 2020</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. R Luo, L Sun, Y Xia, T Qin, S Zhang, H Poon, T.-Y Liu, Brief. Bioinformatics. 2364092022</p>
<p>Modeling the expansion of virtual screening libraries. J Lyu, J Irwin, B Shoichet, Nat Chem Biol. 192023</p>
<p>Few-shot learning creates predictive models of drug response that translate from high-throughput screens to individual patients. J Ma, S H Fong, Y Luo, C J Bakkenist, J P Shen, S Mourragui, L F Wessels, M Hafner, R Sharan, J Peng, Nat Cancer. 222021</p>
<p>Alpaca: Intermittent execution without checkpoints. K Maeng, A Colin, B Lucia, PACMPL. 12017</p>
<p>K Maziarz, H Jackson-Flux, P Cameron, arXiv:2103.03864Learning to extend molecular scaffolds with structural motifs. 2021arXiv preprint</p>
<p>Chembl: towards direct deposition of bioassay data. D Mendez, A Gaulton, A P Bento, J Chambers, M De Veij, E Félix, M P Magariños, J F Mosquera, P Mutowo, M Nowotka, Nucleic Acids Res. 47D12019</p>
<p>Fda approvals in 2023: biomarker-positive subsets, equipoise and verification of benefit. K J Norsworthy, R J Lee-Alonzo, R Pazdur, Nat Rev Clin Oncol. 2024</p>
<p>Large-scale chemoproteomics expedites ligand discovery and predicts ligand behavior in cells. F Offensperger, G Tin, M Duran-Frigola, E Hahn, S Dobner, C W Ende, J W Strohbach, A Rukavina, V Brennsteiner, K Ogilvie, Science. 384669458642024</p>
<p>Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. N M O'boyle, J. Cheminformatics. 42012</p>
<p>Deep reinforcement learning for de novo drug design. M Popova, Sci. Adv. 478852018</p>
<p>Reinforced adversarial neural computer for de novo molecular design. E Putin, A Asadulaev, Y Ivanenkov, V Aladinskiy, B Sanchez-Lengeling, A Aspuru-Guzik, A Zhavoronkov, J Chem Inf Model. 5862018</p>
<p>A small-molecule tnik inhibitor targets fibrosis in preclinical and clinical models. F Ren, A Aliper, J Chen, H Zhao, S Rao, C Kuppe, I V Ozerov, M Zhang, K Witte, C Kruse, Nature Biotechnology. 2024</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nature Machine Intelligence. 4122022</p>
<p>Synthon-based ligand discovery in virtual libraries of over 11 billion compounds. A A Sadybekov, A V Sadybekov, Y Liu, C Iliopoulos-Tsoutsouvas, X.-P Huang, J Pickett, B Houser, N Patel, N K Tran, F Tong, Nature. 60178932022</p>
<p>Small molecules, big targets: drug discovery faces the protein-protein interaction challenge. D E Scott, A R Bayly, C Abell, J Skidmore, Nat. Rev. Drug Discov. 1582016</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. M H Segler, T Kogej, C Tyrchan, M P Waller, ACS Cent. Sci. 412018</p>
<p>R Sennrich, B Haddow, A Birch, Neural machine translation of rare words with subword units, ACL. 2016</p>
<p>FS-mol: A few-shot learning dataset of molecules. M Stanley, J F Bronskill, K Maziarz, H Misztela, J Lanini, M Segler, N Schneider, M Brockschmidt, 2021NeurIPS</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Integrating qsar modelling and deep learning in drug discovery: the emergence of deep qsar. A Tropsha, O Isayev, A Varnek, G Schneider, A Cherkasov, Nature Reviews Drug Discovery. 2322024</p>
<p>The evolution of commercial drug delivery technologies. A Vargason, A Anselmo, S Mitragotri, Nat Biomed Eng. 52021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Adv. Condens. Matter Phys. 302017</p>
<p>Multitask joint strategies of self-supervised representation learning on biomedical networks for drug discovery. X Wang, Y Cheng, Y Yang, Nat Mach Intell. 52023</p>
<p>Pro_ligand: an approach to de novo molecular design. 2. design of novel molecules from molecular field analysis (mfa) models and pharmacophores. B Waszkowycz, D E Clark, D Frenkel, J Li, C W Murray, B Robson, D R Westhead, J. Med. Chem. 37231994</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chem. Sci. 922018</p>
<p>A Zeng, X Liu, Z Du, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Human-level few-shot concept induction through minimax entropy learning. C Zhang, B Jia, Y Zhu, S.-C Zhu, Sci. Adv. 101624882024</p>            </div>
        </div>

    </div>
</body>
</html>
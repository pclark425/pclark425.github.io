<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1047 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1047</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1047</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-218581602</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2020.acl-main.229.pdf" target="_blank">BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps</a></p>
                <p><strong>Paper Abstract:</strong> Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1047.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1047.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BABYWALK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-and-language navigation agent that decomposes long instructions into short micro-instructions (BABY-STEPs), uses a memory buffer to summarize past experience, is trained by imitation learning on BABY-STEPs then refined with curriculum-based reinforcement learning to generalize to longer navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BABYWALK</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Virtual embodied navigation agent operating in photo-realistic indoor environments; learns in two phases: (1) imitation learning on short sub-instructions (BABY-STEPs) to match expert sub-trajectories, (2) curriculum-based reinforcement learning (policy gradient) on increasingly longer concatenated BABY-STEPs; uses a hierarchical summary memory buffer (forgetting-weighted summary) and an LSTM policy with cross-modal attention.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agent in photo-realistic simulator / Matterport3D-based panoramas)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Vision-and-Language Navigation in Matterport-derived datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic indoor environments (Matterport3D) where the agent receives natural-language navigation instructions and panoramic visual observations (12 headings × 3 elevations); datasets differ by concatenating expert trajectories to produce tasks of increasing length (#rooms) and longer instructions. Complexity arises from longer multi-step paths, richer sequences of landmarks/rooms, and longer natural-language instructions; variation arises from distributional shifts across datasets (different path lengths, instruction length distributions, and segmentation noise).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized primarily by instruction/path length and number of sub-goals: average instruction length (words) and average # BABY-STEPs; reported dataset statistics: R2R avg instr length 29.4 words, avg # BABY-STEPs 1.8; R4R 58.4 words, 3.6 BABY-STEPs; R6R 91.2 words, 5.6 BABY-STEPs; R8R 121.6 words, 7.4 BABY-STEPs. Other measures: path length (PL), number of rooms concatenated.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>ranges from low (R2R: ~29 words, ~1.8 BABY-STEPs) to very high (R8R: ~122 words, ~7.4 BABY-STEPs)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Dataset-level distribution shifts in instruction length and path length (R2R→R4R→R6R→R8R), out-of-domain evaluation (train on one length-distribution, test on another); also visual variation inherent to different Matterport scenes but primary variation studied is instruction/task-length distribution shift. #instructions and splits reported (e.g., training instructions counts in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high (the paper explicitly tests out-of-domain generalization from medium-length training sets to longer-length test sets; variation increases with dataset mismatch between training and testing datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Multiple: Success Rate (SR), Coverage weighted by Length Score (CLS), Success-rate-weighted normalized Dynamic Time Warping (SDTW), NDTW, SPL, Navigation Error (NE), Path Length (PL). The authors emphasize CLS and SDTW as fidelity/instruction-following metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative results for BABYWALK (models trained on R4R) from the paper: in-domain (R4R->R4R) SR=29.6%, CLS=47.8, SDTW=18.1; out-of-domain (R4R->R6R) SR=26.4%, CLS=44.9, SDTW=13.1; out-of-domain (R4R->R8R) SR=26.3%, CLS=44.7, SDTW=11.5. (Additional reported values: when trained on R2R and evaluated in-domain R2R->R2R SR=43.8%, CLS=54.4, SDTW=36.9.)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly studies how increasing environment/task complexity (longer instructions, more subgoals/rooms) interacts with variation (training vs test distribution shifts). Key relationships: agents trained on shorter tasks do not generalize well to longer tasks (complexity increase + distribution shift degrades fidelity metrics). Optimizing for goal-reaching (SR) can yield high SR but low instruction-fidelity; fidelity-oriented rewards (CLS, SDTW) better capture instruction-following. BABYWALK's design (BABY-STEP decomposition + memory + curriculum RL) mitigates the negative effect of increased complexity and variation, improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>R8R trained & evaluated in-domain (high complexity, low variation): reported for BABYWALK (trained on R8R → R8R) SR≈22.3%, CLS≈46.4, SDTW≈10.4.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Out-of-domain case: trained on R4R (medium complexity) and evaluated on R8R (high complexity, distribution shift): SR≈26.3%, CLS≈44.7, SDTW≈11.5.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>In-domain low complexity: trained on R2R → R2R SR≈43.8%, CLS≈54.4, SDTW≈36.9.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Two-phase: (1) imitation learning on BABY-STEP subtasks (student-forcing) as warm start; (2) curriculum-based reinforcement learning (policy gradient) with up to 4 'lectures' of increasingly long concatenations of BABY-STEPs; also optional data augmentation (speaker-based) pretraining for some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Extensively tested: BABYWALK generalizes substantially better than prior agents when training on shorter/medium-length datasets and evaluating on longer tasks (R6R, R8R). Example: BABYWALK (trained on R4R) attains SDTW on R6R/R8R roughly double that of the second-best baseline (FAST) and approaches in-domain performance of agents trained directly on longer datasets; curriculum RL and memory buffer both contribute to gains.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training regime: imitation warm-up for 50,000 iterations; then up to 4 RL lectures each trained for 10,000 iterations (sample 8 rollouts per instruction before backprop). Batch sizes: IL mini-batch 100; CRL lectures batches 50, 32, 20, 20. Discount γ=0.95. Max steps per BABY-STEP capped at 10. (These numbers provide a sense of interactions/iterations.)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Standard VLN agents trained on shorter tasks fail to generalize to longer instructions; 2) Decomposing long instructions into BABY-STEPs and training via imitation on subtasks plus curriculum RL markedly improves generalization to longer tasks (higher CLS/SDTW); 3) Memory buffer summarizing past BABY-STEPs (forgetting-weighted) is essential — removing it harms transfer; 4) Curriculum RL (incremental lecture lengths) improves performance over direct RL on entire instruction; 5) Optimizing for goal-reaching (SR) is insufficient for instruction fidelity — fidelity metrics (CLS, SDTW) better reflect following instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Complexity and variation are operationalized primarily via instruction/path length distributions across R2R,R4R,R6R,R8R; fidelity metrics (CLS, SDTW) are emphasized as capturing instruction-following under complexity/variation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1047.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1047.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEQ2SEQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seq2Seq agent (Anderson et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early multimodal sequence-to-sequence VLN agent trained by imitation to map natural language instructions to navigation actions in panoramic photo-realistic environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SEQ2SEQ</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Multimodal sequence-to-sequence policy mapping instructions to actions; trained primarily by imitation learning on paired instruction-trajectory data; adapted in this paper to the panoramic state/action space for baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agent in photo-realistic simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same indoor panoramic environments; baseline performance reported across datasets of varying instruction/path lengths; complexity varied by dataset concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Evaluated across datasets with differing avg instruction lengths and #BABY-STEPs (R2R..R8R as above). Also path length (PL), NE, SR used.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>tested from low to very high via dataset splits (R2R->R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Cross-dataset generalization (train on one dataset length distribution, test on another) as in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high depending on cross-dataset evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, CLS, SDTW, SPL, PL, NE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative (from paper): when trained on R4R and evaluated in-domain R4R->R4R SEQ2SEQ had SR≈25.7%, CLS≈20.7, SDTW≈9.0 (substantially lower fidelity metrics than BABYWALK).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Mentioned/observed via baseline comparisons: SEQ2SEQ (imitation-only) degrades substantially when evaluated on longer/out-of-domain tasks — indicating sensitivity to increased complexity and distributional variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Imitation learning (Seq2Seq behavioral cloning/style), baseline reimplemented/adapted for panoramic action space.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Performs poorly on out-of-domain longer tasks (lower CLS/SDTW), demonstrating limited generalization from shorter to longer instruction distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not explicitly reported in this paper for SEQ2SEQ beyond adaptation details; used as a baseline reimplementation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Imitation-only Seq2Seq baselines fail to maintain instruction-following fidelity when facing increased task complexity/variation; highlights need for designs like decomposition, memory, and curriculum learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1047.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1047.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speaker-Follower models for Vision-and-Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Speaker-Follower (SF) is a VLN approach that augments training data via a learned speaker model to synthesize instruction-trajectory pairs and uses sequence models with attention to improve instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Speaker-follower models for vision-and-language navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Speaker-Follower (SF)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>VLN agent that combines a follower policy with a learned speaker to augment training data; trained with imitation learning and augmented datasets to improve instruction grounding; in this paper adapted as a baseline and compared across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photo-realistic panoramic indoor environments; SF leverages synthetic instruction augmentation to improve robustness across varying instruction lengths, but still tested on the same dataset distribution shifts as BABYWALK.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Instruction/path length and #BABY-STEPs across R2R..R8R.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>evaluated from low to high via datasets</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Cross-dataset (out-of-domain) evaluations and data augmentation to reduce variation sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, CLS, SDTW, SPL, PL, NE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (from paper tables): SF (with augmentation) trained on R4R in-domain gave SR≈24.9%, CLS≈23.6, SDTW≈9.2 (lower fidelity than BABYWALK on same setting).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>SF benefits from data augmentation to mitigate some variation, but still exhibits degraded fidelity on longer/out-of-domain tasks compared to BABYWALK.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Imitation learning with data augmentation using a learned speaker; used as a baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Improved over raw Seq2Seq in some metrics due to augmentation, but still underperforms BABYWALK on fidelity metrics (CLS, SDTW) when generalizing to longer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not broken out in this paper; SF usually benefits from augmented synthetic pairs to improve sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Data augmentation (speaker) helps cross-domain robustness but does not fully solve generalization to much longer instruction sequences; decomposition + curriculum provides stronger gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1047.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1047.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Cross-Modal Matching (RCM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that refines a speaker-follower-like model using reinforcement learning and self-imitation, trained with either goal-oriented or fidelity-oriented rewards to influence behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RCM (GOAL / FIDELITY)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Starts from a follower model and refines via reinforcement learning; two reward settings explored: RCM(GOAL) optimizes goal-reaching, RCM(FIDELITY) optimizes a fidelity-oriented reward (CLS); used in this paper as a strong baseline for fidelity vs goal trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport-based VLN datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same indoor panoramic environments; RCM variants illustrate the trade-off between goal-reaching and instruction fidelity under varying task lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Instruction/path length (#words, #BABY-STEPs) and path length (PL); fidelity metrics (CLS, SDTW) used to evaluate effect of complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>evaluated across dataset lengths</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Cross-dataset transfer (train on one distribution / test on another); experiments contrast RCM(GOAL) vs RCM(FIDELITY) to show sensitivity to objective under variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, CLS, SDTW</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative (from paper): RCM(FIDELITY) (trained on R4R) in-domain R4R->R4R had SR≈24.7%, CLS≈39.2, SDTW≈13.7; RCM(GOAL) had higher SR but lower fidelity (CLS) showing the trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicit: RCM(GOAL) achieves higher SR (goal-reaching) but lower instruction fidelity than RCM(FIDELITY); fidelity objective is more informative for distinguishing agents that actually follow instructions as complexity/variation increases.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Imitation pretraining plus reinforcement learning (policy gradient) optimizing either goal-reach or fidelity-oriented rewards; self-imitation techniques used in origin paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>RCM(FIDELITY) improves instruction fidelity vs a goal-oriented RL agent, but still underperforms BABYWALK on generalizing to much longer tasks (R6R/R8R).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not detailed in this paper beyond RL refinement; uses policy gradient methods as cited.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Objective choice (goal vs fidelity) matters: fidelity-optimized agents better follow instructions, but that alone is insufficient for generalizing to longer/out-of-domain instruction distributions without architectural and curriculum changes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1047.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1047.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REGRETFUL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Regretful Agent: Heuristic-aided navigation through progress estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN agent that uses a progress monitor and a regret module allowing backtracking to correct mistakes during navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The regretful agent: Heuristic-aided navigation through progress estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REGRETFUL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>VLN agent with a learned progress monitor and a regret/backtracking mechanism to recover from mistakes; trained with imitation and/or RL in prior work and reimplemented/adapted as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates in panoramic indoor scenes; backtracking heuristics aim to improve robustness when task complexity or variation leads to errors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Instruction/path length and #BABY-STEPs across datasets; agent designed to correct mistakes on longer/harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>evaluated across dataset lengths</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Tested by cross-dataset evaluation in paper; baseline performance reported for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, CLS, SDTW</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative (from paper): REGRETFUL (trained on R4R) in-domain R4R->R4R SR≈30.1%, CLS≈34.1, SDTW≈13.5 (lower fidelity than BABYWALK).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Backtracking helps in some complex tasks but does not match gains from BABY-STEP decomposition + curriculum for generalizing to far longer instruction distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Imitation learning with auxiliary progress estimation; some variants include reinforcement learning/backtracking heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Provides improvements over some baselines but underperforms BABYWALK on fidelity metrics when generalizing to longer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not specified in this paper for REGRETFUL beyond baseline reimplementation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Progress monitoring and backtracking improve robustness to errors but are insufficient alone to achieve the level of generalization obtained by BABYWALK's decomposition + memory + curriculum approach.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1047.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1047.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Frontier Aware Search with Backtracking (FAST)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN agent that incorporates global and local knowledge to compare partial trajectories and performs backtracking/search to improve navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tactical rewind: Self-correction via backtracking in vision-and-language navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FAST</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent that maintains frontier-aware search and backtracking heuristics to compare partial trajectories of varying lengths, designed to improve goal-reaching and recovery; used as a strong baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on panorama-based indoor navigation tasks; complexity increases with concatenated longer paths and more sub-instructions; FAST uses search/backtracking to handle some complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Instruction/path length (#words, #BABY-STEPs), path length (PL); compared across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>tested across low-to-high complexity datasets</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Cross-dataset transfer and out-of-domain evaluation; robustness to distributional variation evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-to-high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR, CLS, SDTW</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative (from paper): FAST (pretrained) trained on R4R in-domain gave SR≈36.2%, CLS≈34.0, SDTW≈15.5; out-of-domain (R4R->R6R) FAST SDTW≈7.7 (much lower than BABYWALK's ~13.1 on same transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Performs well on goal-reaching (high SR/SPL) in some settings (shortcuts), but shows lower instruction-following fidelity (CLS/SDTW) when generalizing to longer tasks compared to BABYWALK.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Imitation plus heuristics and backtracking/local-global comparison; included as a baseline with pretraining/augmentation variants.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>FAST attains strong SR in some settings (often taking short-cuts to reach the goal) but lower fidelity metrics and worse transfer SDTW on longer/out-of-domain tasks compared to BABYWALK.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not broken out in this paper for FAST beyond baseline reimplementation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Backtracking/search strategies help goal-reaching but do not substitute for decomposition and curriculum learning when the objective is instruction fidelity across increased complexity and variation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. <em>(Rating: 2)</em></li>
                <li>The regretful agent: Heuristic-aided navigation through progress estimation. <em>(Rating: 2)</em></li>
                <li>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>Stay on the path: Instruction fidelity in vision-and-language navigation. <em>(Rating: 2)</em></li>
                <li>Effective and general evaluation for instruction conditioned navigation using dynamic time warping. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1047",
    "paper_id": "paper-218581602",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "BABYWALK",
            "name_full": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps",
            "brief_description": "A vision-and-language navigation agent that decomposes long instructions into short micro-instructions (BABY-STEPs), uses a memory buffer to summarize past experience, is trained by imitation learning on BABY-STEPs then refined with curriculum-based reinforcement learning to generalize to longer navigation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BABYWALK",
            "agent_description": "Virtual embodied navigation agent operating in photo-realistic indoor environments; learns in two phases: (1) imitation learning on short sub-instructions (BABY-STEPs) to match expert sub-trajectories, (2) curriculum-based reinforcement learning (policy gradient) on increasingly longer concatenated BABY-STEPs; uses a hierarchical summary memory buffer (forgetting-weighted summary) and an LSTM policy with cross-modal attention.",
            "agent_type": "simulated agent (virtual agent in photo-realistic simulator / Matterport3D-based panoramas)",
            "environment_name": "Vision-and-Language Navigation in Matterport-derived datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Photo-realistic indoor environments (Matterport3D) where the agent receives natural-language navigation instructions and panoramic visual observations (12 headings × 3 elevations); datasets differ by concatenating expert trajectories to produce tasks of increasing length (#rooms) and longer instructions. Complexity arises from longer multi-step paths, richer sequences of landmarks/rooms, and longer natural-language instructions; variation arises from distributional shifts across datasets (different path lengths, instruction length distributions, and segmentation noise).",
            "complexity_measure": "Characterized primarily by instruction/path length and number of sub-goals: average instruction length (words) and average # BABY-STEPs; reported dataset statistics: R2R avg instr length 29.4 words, avg # BABY-STEPs 1.8; R4R 58.4 words, 3.6 BABY-STEPs; R6R 91.2 words, 5.6 BABY-STEPs; R8R 121.6 words, 7.4 BABY-STEPs. Other measures: path length (PL), number of rooms concatenated.",
            "complexity_level": "ranges from low (R2R: ~29 words, ~1.8 BABY-STEPs) to very high (R8R: ~122 words, ~7.4 BABY-STEPs)",
            "variation_measure": "Dataset-level distribution shifts in instruction length and path length (R2R→R4R→R6R→R8R), out-of-domain evaluation (train on one length-distribution, test on another); also visual variation inherent to different Matterport scenes but primary variation studied is instruction/task-length distribution shift. #instructions and splits reported (e.g., training instructions counts in Table 1).",
            "variation_level": "medium-to-high (the paper explicitly tests out-of-domain generalization from medium-length training sets to longer-length test sets; variation increases with dataset mismatch between training and testing datasets)",
            "performance_metric": "Multiple: Success Rate (SR), Coverage weighted by Length Score (CLS), Success-rate-weighted normalized Dynamic Time Warping (SDTW), NDTW, SPL, Navigation Error (NE), Path Length (PL). The authors emphasize CLS and SDTW as fidelity/instruction-following metrics.",
            "performance_value": "Representative results for BABYWALK (models trained on R4R) from the paper: in-domain (R4R-&gt;R4R) SR=29.6%, CLS=47.8, SDTW=18.1; out-of-domain (R4R-&gt;R6R) SR=26.4%, CLS=44.9, SDTW=13.1; out-of-domain (R4R-&gt;R8R) SR=26.3%, CLS=44.7, SDTW=11.5. (Additional reported values: when trained on R2R and evaluated in-domain R2R-&gt;R2R SR=43.8%, CLS=54.4, SDTW=36.9.)",
            "complexity_variation_relationship": "Yes — the paper explicitly studies how increasing environment/task complexity (longer instructions, more subgoals/rooms) interacts with variation (training vs test distribution shifts). Key relationships: agents trained on shorter tasks do not generalize well to longer tasks (complexity increase + distribution shift degrades fidelity metrics). Optimizing for goal-reaching (SR) can yield high SR but low instruction-fidelity; fidelity-oriented rewards (CLS, SDTW) better capture instruction-following. BABYWALK's design (BABY-STEP decomposition + memory + curriculum RL) mitigates the negative effect of increased complexity and variation, improving generalization.",
            "high_complexity_low_variation_performance": "R8R trained & evaluated in-domain (high complexity, low variation): reported for BABYWALK (trained on R8R → R8R) SR≈22.3%, CLS≈46.4, SDTW≈10.4.",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Out-of-domain case: trained on R4R (medium complexity) and evaluated on R8R (high complexity, distribution shift): SR≈26.3%, CLS≈44.7, SDTW≈11.5.",
            "low_complexity_low_variation_performance": "In-domain low complexity: trained on R2R → R2R SR≈43.8%, CLS≈54.4, SDTW≈36.9.",
            "training_strategy": "Two-phase: (1) imitation learning on BABY-STEP subtasks (student-forcing) as warm start; (2) curriculum-based reinforcement learning (policy gradient) with up to 4 'lectures' of increasingly long concatenations of BABY-STEPs; also optional data augmentation (speaker-based) pretraining for some variants.",
            "generalization_tested": true,
            "generalization_results": "Extensively tested: BABYWALK generalizes substantially better than prior agents when training on shorter/medium-length datasets and evaluating on longer tasks (R6R, R8R). Example: BABYWALK (trained on R4R) attains SDTW on R6R/R8R roughly double that of the second-best baseline (FAST) and approaches in-domain performance of agents trained directly on longer datasets; curriculum RL and memory buffer both contribute to gains.",
            "sample_efficiency": "Training regime: imitation warm-up for 50,000 iterations; then up to 4 RL lectures each trained for 10,000 iterations (sample 8 rollouts per instruction before backprop). Batch sizes: IL mini-batch 100; CRL lectures batches 50, 32, 20, 20. Discount γ=0.95. Max steps per BABY-STEP capped at 10. (These numbers provide a sense of interactions/iterations.)",
            "key_findings": "1) Standard VLN agents trained on shorter tasks fail to generalize to longer instructions; 2) Decomposing long instructions into BABY-STEPs and training via imitation on subtasks plus curriculum RL markedly improves generalization to longer tasks (higher CLS/SDTW); 3) Memory buffer summarizing past BABY-STEPs (forgetting-weighted) is essential — removing it harms transfer; 4) Curriculum RL (incremental lecture lengths) improves performance over direct RL on entire instruction; 5) Optimizing for goal-reaching (SR) is insufficient for instruction fidelity — fidelity metrics (CLS, SDTW) better reflect following instructions.",
            "notes": "Complexity and variation are operationalized primarily via instruction/path length distributions across R2R,R4R,R6R,R8R; fidelity metrics (CLS, SDTW) are emphasized as capturing instruction-following under complexity/variation.",
            "uuid": "e1047.0"
        },
        {
            "name_short": "SEQ2SEQ",
            "name_full": "Seq2Seq agent (Anderson et al., 2018)",
            "brief_description": "An early multimodal sequence-to-sequence VLN agent trained by imitation to map natural language instructions to navigation actions in panoramic photo-realistic environments.",
            "citation_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.",
            "mention_or_use": "use",
            "agent_name": "SEQ2SEQ",
            "agent_description": "Multimodal sequence-to-sequence policy mapping instructions to actions; trained primarily by imitation learning on paired instruction-trajectory data; adapted in this paper to the panoramic state/action space for baseline comparisons.",
            "agent_type": "simulated agent (virtual agent in photo-realistic simulator)",
            "environment_name": "Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Same indoor panoramic environments; baseline performance reported across datasets of varying instruction/path lengths; complexity varied by dataset concatenation.",
            "complexity_measure": "Evaluated across datasets with differing avg instruction lengths and #BABY-STEPs (R2R..R8R as above). Also path length (PL), NE, SR used.",
            "complexity_level": "tested from low to very high via dataset splits (R2R-&gt;R8R)",
            "variation_measure": "Cross-dataset generalization (train on one dataset length distribution, test on another) as in the paper.",
            "variation_level": "medium-to-high depending on cross-dataset evaluation",
            "performance_metric": "SR, CLS, SDTW, SPL, PL, NE",
            "performance_value": "Representative (from paper): when trained on R4R and evaluated in-domain R4R-&gt;R4R SEQ2SEQ had SR≈25.7%, CLS≈20.7, SDTW≈9.0 (substantially lower fidelity metrics than BABYWALK).",
            "complexity_variation_relationship": "Mentioned/observed via baseline comparisons: SEQ2SEQ (imitation-only) degrades substantially when evaluated on longer/out-of-domain tasks — indicating sensitivity to increased complexity and distributional variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Imitation learning (Seq2Seq behavioral cloning/style), baseline reimplemented/adapted for panoramic action space.",
            "generalization_tested": true,
            "generalization_results": "Performs poorly on out-of-domain longer tasks (lower CLS/SDTW), demonstrating limited generalization from shorter to longer instruction distributions.",
            "sample_efficiency": "Not explicitly reported in this paper for SEQ2SEQ beyond adaptation details; used as a baseline reimplementation.",
            "key_findings": "Imitation-only Seq2Seq baselines fail to maintain instruction-following fidelity when facing increased task complexity/variation; highlights need for designs like decomposition, memory, and curriculum learning.",
            "uuid": "e1047.1"
        },
        {
            "name_short": "SF",
            "name_full": "Speaker-Follower models for Vision-and-Language Navigation",
            "brief_description": "Speaker-Follower (SF) is a VLN approach that augments training data via a learned speaker model to synthesize instruction-trajectory pairs and uses sequence models with attention to improve instruction following.",
            "citation_title": "Speaker-follower models for vision-and-language navigation.",
            "mention_or_use": "use",
            "agent_name": "Speaker-Follower (SF)",
            "agent_description": "VLN agent that combines a follower policy with a learned speaker to augment training data; trained with imitation learning and augmented datasets to improve instruction grounding; in this paper adapted as a baseline and compared across datasets.",
            "agent_type": "simulated agent (virtual agent)",
            "environment_name": "Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Photo-realistic panoramic indoor environments; SF leverages synthetic instruction augmentation to improve robustness across varying instruction lengths, but still tested on the same dataset distribution shifts as BABYWALK.",
            "complexity_measure": "Instruction/path length and #BABY-STEPs across R2R..R8R.",
            "complexity_level": "evaluated from low to high via datasets",
            "variation_measure": "Cross-dataset (out-of-domain) evaluations and data augmentation to reduce variation sensitivity.",
            "variation_level": "medium",
            "performance_metric": "SR, CLS, SDTW, SPL, PL, NE",
            "performance_value": "Example (from paper tables): SF (with augmentation) trained on R4R in-domain gave SR≈24.9%, CLS≈23.6, SDTW≈9.2 (lower fidelity than BABYWALK on same setting).",
            "complexity_variation_relationship": "SF benefits from data augmentation to mitigate some variation, but still exhibits degraded fidelity on longer/out-of-domain tasks compared to BABYWALK.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Imitation learning with data augmentation using a learned speaker; used as a baseline comparison.",
            "generalization_tested": true,
            "generalization_results": "Improved over raw Seq2Seq in some metrics due to augmentation, but still underperforms BABYWALK on fidelity metrics (CLS, SDTW) when generalizing to longer tasks.",
            "sample_efficiency": "Not broken out in this paper; SF usually benefits from augmented synthetic pairs to improve sample efficiency.",
            "key_findings": "Data augmentation (speaker) helps cross-domain robustness but does not fully solve generalization to much longer instruction sequences; decomposition + curriculum provides stronger gains.",
            "uuid": "e1047.2"
        },
        {
            "name_short": "RCM",
            "name_full": "Reinforced Cross-Modal Matching (RCM)",
            "brief_description": "An agent that refines a speaker-follower-like model using reinforcement learning and self-imitation, trained with either goal-oriented or fidelity-oriented rewards to influence behavior.",
            "citation_title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.",
            "mention_or_use": "use",
            "agent_name": "RCM (GOAL / FIDELITY)",
            "agent_description": "Starts from a follower model and refines via reinforcement learning; two reward settings explored: RCM(GOAL) optimizes goal-reaching, RCM(FIDELITY) optimizes a fidelity-oriented reward (CLS); used in this paper as a strong baseline for fidelity vs goal trade-offs.",
            "agent_type": "simulated agent (virtual agent)",
            "environment_name": "Matterport-based VLN datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Same indoor panoramic environments; RCM variants illustrate the trade-off between goal-reaching and instruction fidelity under varying task lengths.",
            "complexity_measure": "Instruction/path length (#words, #BABY-STEPs) and path length (PL); fidelity metrics (CLS, SDTW) used to evaluate effect of complexity.",
            "complexity_level": "evaluated across dataset lengths",
            "variation_measure": "Cross-dataset transfer (train on one distribution / test on another); experiments contrast RCM(GOAL) vs RCM(FIDELITY) to show sensitivity to objective under variation.",
            "variation_level": "medium-to-high",
            "performance_metric": "SR, CLS, SDTW",
            "performance_value": "Representative (from paper): RCM(FIDELITY) (trained on R4R) in-domain R4R-&gt;R4R had SR≈24.7%, CLS≈39.2, SDTW≈13.7; RCM(GOAL) had higher SR but lower fidelity (CLS) showing the trade-off.",
            "complexity_variation_relationship": "Explicit: RCM(GOAL) achieves higher SR (goal-reaching) but lower instruction fidelity than RCM(FIDELITY); fidelity objective is more informative for distinguishing agents that actually follow instructions as complexity/variation increases.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Imitation pretraining plus reinforcement learning (policy gradient) optimizing either goal-reach or fidelity-oriented rewards; self-imitation techniques used in origin paper.",
            "generalization_tested": true,
            "generalization_results": "RCM(FIDELITY) improves instruction fidelity vs a goal-oriented RL agent, but still underperforms BABYWALK on generalizing to much longer tasks (R6R/R8R).",
            "sample_efficiency": "Not detailed in this paper beyond RL refinement; uses policy gradient methods as cited.",
            "key_findings": "Objective choice (goal vs fidelity) matters: fidelity-optimized agents better follow instructions, but that alone is insufficient for generalizing to longer/out-of-domain instruction distributions without architectural and curriculum changes.",
            "uuid": "e1047.3"
        },
        {
            "name_short": "REGRETFUL",
            "name_full": "The Regretful Agent: Heuristic-aided navigation through progress estimation",
            "brief_description": "A VLN agent that uses a progress monitor and a regret module allowing backtracking to correct mistakes during navigation.",
            "citation_title": "The regretful agent: Heuristic-aided navigation through progress estimation.",
            "mention_or_use": "use",
            "agent_name": "REGRETFUL",
            "agent_description": "VLN agent with a learned progress monitor and a regret/backtracking mechanism to recover from mistakes; trained with imitation and/or RL in prior work and reimplemented/adapted as a baseline in this paper.",
            "agent_type": "simulated agent",
            "environment_name": "Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Operates in panoramic indoor scenes; backtracking heuristics aim to improve robustness when task complexity or variation leads to errors.",
            "complexity_measure": "Instruction/path length and #BABY-STEPs across datasets; agent designed to correct mistakes on longer/harder tasks.",
            "complexity_level": "evaluated across dataset lengths",
            "variation_measure": "Tested by cross-dataset evaluation in paper; baseline performance reported for comparisons.",
            "variation_level": "medium",
            "performance_metric": "SR, CLS, SDTW",
            "performance_value": "Representative (from paper): REGRETFUL (trained on R4R) in-domain R4R-&gt;R4R SR≈30.1%, CLS≈34.1, SDTW≈13.5 (lower fidelity than BABYWALK).",
            "complexity_variation_relationship": "Backtracking helps in some complex tasks but does not match gains from BABY-STEP decomposition + curriculum for generalizing to far longer instruction distributions.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Imitation learning with auxiliary progress estimation; some variants include reinforcement learning/backtracking heuristics.",
            "generalization_tested": true,
            "generalization_results": "Provides improvements over some baselines but underperforms BABYWALK on fidelity metrics when generalizing to longer tasks.",
            "sample_efficiency": "Not specified in this paper for REGRETFUL beyond baseline reimplementation.",
            "key_findings": "Progress monitoring and backtracking improve robustness to errors but are insufficient alone to achieve the level of generalization obtained by BABYWALK's decomposition + memory + curriculum approach.",
            "uuid": "e1047.4"
        },
        {
            "name_short": "FAST",
            "name_full": "Frontier Aware Search with Backtracking (FAST)",
            "brief_description": "A VLN agent that incorporates global and local knowledge to compare partial trajectories and performs backtracking/search to improve navigation.",
            "citation_title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation.",
            "mention_or_use": "use",
            "agent_name": "FAST",
            "agent_description": "Agent that maintains frontier-aware search and backtracking heuristics to compare partial trajectories of varying lengths, designed to improve goal-reaching and recovery; used as a strong baseline in comparisons.",
            "agent_type": "simulated agent",
            "environment_name": "Matterport-derived VLN datasets (R2R, R4R, R6R, R8R)",
            "environment_description": "Operates on panorama-based indoor navigation tasks; complexity increases with concatenated longer paths and more sub-instructions; FAST uses search/backtracking to handle some complexity.",
            "complexity_measure": "Instruction/path length (#words, #BABY-STEPs), path length (PL); compared across datasets.",
            "complexity_level": "tested across low-to-high complexity datasets",
            "variation_measure": "Cross-dataset transfer and out-of-domain evaluation; robustness to distributional variation evaluated.",
            "variation_level": "medium-to-high",
            "performance_metric": "SR, CLS, SDTW",
            "performance_value": "Representative (from paper): FAST (pretrained) trained on R4R in-domain gave SR≈36.2%, CLS≈34.0, SDTW≈15.5; out-of-domain (R4R-&gt;R6R) FAST SDTW≈7.7 (much lower than BABYWALK's ~13.1 on same transfer).",
            "complexity_variation_relationship": "Performs well on goal-reaching (high SR/SPL) in some settings (shortcuts), but shows lower instruction-following fidelity (CLS/SDTW) when generalizing to longer tasks compared to BABYWALK.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Imitation plus heuristics and backtracking/local-global comparison; included as a baseline with pretraining/augmentation variants.",
            "generalization_tested": true,
            "generalization_results": "FAST attains strong SR in some settings (often taking short-cuts to reach the goal) but lower fidelity metrics and worse transfer SDTW on longer/out-of-domain tasks compared to BABYWALK.",
            "sample_efficiency": "Not broken out in this paper for FAST beyond baseline reimplementation.",
            "key_findings": "Backtracking/search strategies help goal-reaching but do not substitute for decomposition and curriculum learning when the objective is instruction fidelity across increased complexity and variation.",
            "uuid": "e1047.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.",
            "rating": 2,
            "sanitized_title": "visionandlanguage_navigation_interpreting_visuallygrounded_navigation_instructions_in_real_environments"
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation.",
            "rating": 2,
            "sanitized_title": "speakerfollower_models_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.",
            "rating": 2,
            "sanitized_title": "reinforced_crossmodal_matching_and_selfsupervised_imitation_learning_for_visionlanguage_navigation"
        },
        {
            "paper_title": "The regretful agent: Heuristic-aided navigation through progress estimation.",
            "rating": 2,
            "sanitized_title": "the_regretful_agent_heuristicaided_navigation_through_progress_estimation"
        },
        {
            "paper_title": "Tactical rewind: Self-correction via backtracking in vision-and-language navigation.",
            "rating": 2,
            "sanitized_title": "tactical_rewind_selfcorrection_via_backtracking_in_visionandlanguage_navigation"
        },
        {
            "paper_title": "Stay on the path: Instruction fidelity in vision-and-language navigation.",
            "rating": 2,
            "sanitized_title": "stay_on_the_path_instruction_fidelity_in_visionandlanguage_navigation"
        },
        {
            "paper_title": "Effective and general evaluation for instruction conditioned navigation using dynamic time warping.",
            "rating": 2,
            "sanitized_title": "effective_and_general_evaluation_for_instruction_conditioned_navigation_using_dynamic_time_warping"
        }
    ],
    "cost": 0.02104375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps
Association for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020</p>
<p>Wang Zhu 
Simon Fraser University</p>
<p>Hexiang Hu 
Jiacheng Chen 
University of Southern California</p>
<p>Zhiwei Deng 
University of Southern California</p>
<p>Princeton University</p>
<p>Vihan Jain 
Google Research</p>
<p>Eugene Ie 
Google Research</p>
<p>Fei Sha 
University of Southern California</p>
<p>Google Research</p>
<p>BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps</p>
<p>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 20202539
Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps. The learning process is composed of two phases. In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps. In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions. We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk's generalization ability. Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better. The codes and the datasets are released on our project page https://github.com/ Sha-Lab/babywalk.</p>
<p>Introduction</p>
<p>Autonomous agents such as household robots need to interact with the physical world in multiple modalities. As an example, in vision-and-language navigation (VLN) (Anderson et al., 2018), the agent moves around in a photo-realistic simulated environment (Chang et al., 2017) by following a sequence of natural language instructions. To infer its whereabouts so as to decide its moves, the * Author contributed equally † On leave from University of Southern California agent infuses its visual perception, its trajectory and the instructions (Fried et al., 2018;Anderson et al., 2018;Wang et al., 2019;Ma et al., 2019a,b). Arguably, the ability to understand and follow the instructions is one of the most crucial skills to acquire by VLN agents. Jain et al. (2019) shows that the VLN agents trained on the originally proposed dataset ROOM2ROOM (i.e. R2R thereafter) do not follow the instructions, despite having achieved high success rates of reaching the navigation goals. They proposed two remedies: a new dataset ROOM4ROOM (or R4R) that doubles the path lengths in the R2R, and a new evaluation metric Coverage weighted by Length Score (CLS) that measures more closely whether the groundtruth paths are followed. They showed optimizing the fidelity of following instructions leads to agents with desirable behavior. Moreover, the long lengths in R4R are informative in identifying agents who score higher in such fidelity measure.</p>
<p>In this paper, we investigate another crucial aspect of following the instructions: can a VLN agent generalize to following longer instructions by learning from shorter ones? This aspect has important implication to real-world applications as collecting annotated long sequences of instructions and training on them can be costly. Thus, it is highly desirable to have this generalization ability. After all, it seems that humans can achieve this effortlessly 1 .</p>
<p>To this end, we have created several datasets of longer navigation tasks, inspired by R4R (Jain et al., 2019). We trained VLN agents on R4R and use the agents to navigate in ROOM6ROOM (i.e., R6R) and ROOM8ROOM (i.e., R8R). We contrast to the performance of the agents which are trained on those datasets directly ("in-domain"). The results Figure 1: Performance of various VLN agents on generalizing from shorter navigation tasks to longer ones. The vertical axis is the newly proposed path-following metric SDTW (Magalhaes et al., 2019), the higher the better. BABYWALK generalizes better than other approaches across different lengths of navigation tasks. Meanwhile, it get very close to the performances of the in-domain agents (the dashed line). Please refer to the texts for details. are shown in Fig. 1.</p>
<p>Our findings are that the agents trained on R4R (denoted by the purple and the pink solid lines) perform significantly worse than the in-domain agents (denoted the light blue dashed line). Also interestingly, when such out-of-domain agents are applied to the dataset R2R with shorter navigation tasks, they also perform significantly worse than the corresponding in-domain agent despite R4R containing many navigation paths from R2R. Note that the agent trained to optimize the aforementioned fidelity measure (RCM(fidelity)) performs better than the agent trained to reach the goal only (RCM(goal)), supporting the claim by Jain et al. (2019) that following instructions is a more meaningful objective than merely goal-reaching. Yet, the fidelity measure itself is not enough to enable the agent to transfer well to longer navigation tasks.</p>
<p>To address these deficiencies, we propose a new approach for VLN. The agent follows a long navigation instruction by decomposing the instruction into shorter ones ("micro-instructions", i.e., BABY-STEPs), each of which corresponds to an intermediate goal/task to be executed sequentially. To this end, the agent has three components: (a) a memory buffer that summarizes the agent's experiences so that the agent can use them to provide the context for executing the next BABY-STEP. (b) the agent first learns from human experts in "bitesize". Instead of trying to imitate to achieve the ground-truth paths as a whole, the agent is given the pairs of a BABY-STEP and the corresponding human expert path so that it can learn policies of actions from shorter instructions. (c) In the second stage of learning, the agent refines the policies by curriculum-based reinforcement learning, where the agent is given increasingly longer navigation tasks to achieve. In particular, this curriculum design reflects our desiderata that the agent optimized on shorter tasks should generalize well to slightly longer tasks and then much longer ones.</p>
<p>While we do not claim that our approach faithfully simulates human learning of navigation, the design is loosely inspired by it. We name our approach BABYWALK and refer to the intermediate navigation goals in (b) as BABY-STEPs. Fig. 1 shows that BABYWALK (the red solid line) significantly outperforms other approaches and despite being out-of-domain, it even reach the performance of in-domain agents on R6R and R8R.</p>
<p>The effectiveness of BABYWALK also leads to an interesting twist. As mentioned before, one of the most important observations by Jain et al.</p>
<p>(2019) is that the original VLN dataset R2R fails to reveal the difference between optimizing goalreaching (thus ignoring the instructions) and optimizing the fidelity (thus adhering to the instructions). Yet, leaving details to section 5, we have also shown that applying BABYWALK to R2R can lead to equally strong performance on generalizing from shorter instructions (i.e., R2R) to longer ones.</p>
<p>In summary, in this paper, we have demonstrated empirically that the current VLN agents are ineffective in generalizing from learning on shorter navigation tasks to longer ones. We propose a new approach in addressing this important problem. We validate the approach with extensive benchmarks, including ablation studies to identify the effectiveness of various components in our approach.</p>
<p>Related Work</p>
<p>Vision-and-Language Navigation (VLN) Recent works (Anderson et al., 2018;Thomason et al., 2019;Jain et al., 2019;Chen et al., 2019;Nguyen and Daumé III, 2019) extend the early works of instruction based navigation (Chen and Mooney, 2011;Kim and Mooney, 2013;Mei et al., 2016) to photo-realistic simulated environments. For instance, Anderson et al. (2018) proposed to learn a multi-modal Sequence-to-Sequence agent (Seq2Seq) by imitating expert demonstration. Fried et al. (2018) developed a method that augments the paired instruction and demonstration data using a learned speaker model, to teach the navigation agent to better understand instructions. Wang et al. (2019) further applies reinforcement learning (RL) and self-imitation learning to improve navigation agents. Ma et al. (2019a,b) designed models that track the execution progress for a sequence of instructions using soft-attention.</p>
<p>Different from them, we focus on transferring an agent's performances on shorter tasks to longer ones. This leads to designs and learning schemes that improve generalization across datasets. We use a memory buffer to prevent mistakes in the distant past from exerting strong influence on the present. In imitation learning stage, we solve fine-grained subtasks (BABY-STEPs) instead of asking the agent to learn the navigation trajectory as a whole. We then use curriculum-based reinforcement learning by asking the agent to follow increasingly longer instructions.</p>
<p>Transfer and Cross-domain Adaptation There have been a large body of works in transfer learning and generalization across tasks and environments in both computer vision and reinforcement learning (Andreas et al., 2017;Oh et al., 2017;Zhu et al., 2017a,b;Sohn et al., 2018;Hu et al., 2018). Of particular relevance is the recent work on adapting VLN agents to changes in visual environments (Huang et al., 2019;Tan et al., 2019). To our best knowledge, this work is the first to focus on adapting to a simple aspect of language variability -the length of the instructions.</p>
<p>Curriculum Learning Since proposed in (Bengio et al., 2009), curriculum learning was successfully used in a range of tasks: training robots for goal reaching (Florensa et al., 2017), visual question answering (Mao et al., 2019), image generation (Karras et al., 2018). To our best knowledge, this work is the first to apply the idea to learning in VLN.</p>
<p>Notation and the Setup of VLN</p>
<p>In the VLN task, the agent receives a natural language instruction X composed of a sequence of sentences. We model the agent with an Markov Decision Process (MDP) which is defined as a tuple of a state space S, an action space A, an initial state s 1 , a stationary transition dynamics ρ : S×A → S, a reward function r : S × A → R, and the discount factor γ for weighting future rewards. The agent acts according to a policy π : S × A → 0 ∪ R + . The state and action spaces are defined the same as in (Fried et al., 2018) (cf. § 4.4 </p>
<p>for details).</p>
<p>For each X, the sequence of the pairs (s, a) is called a trajectory Y = s 1 , a 1 , . . . , s |Y| , a |Y| where |·| denotes the length of the sequence or the size of a set. We useâ to denote an action taken by the agent according to its policy. Hence,Ŷ denotes the agent's trajectory, while Y (or a) denotes the human expert's trajectory (or action). The agent is given training examples of (X, Y) to optimize its policy to maximize its expected rewards.</p>
<p>In our work, we introduce additional notations in the following. We will segment a (long) instruction X into multiple shorter sequences of sentences {x m , m = 1, 2, · · · , M}, to which we refer as BABY-STEPs. Each x m is interpreted as a microinstruction that corresponds to a trajectory by the agentŷ m and is aligned with a part of the human expert's trajectory, denoted as y m . While the alignment is not available in existing datasets for VLN, we will describe how to obtain them in a later section ( § 4.3). Throughout the paper, we also freely interexchange the term "following the mth microinstruction", "executing the BABY-STEP x m ", or "complete the mth subtask".</p>
<p>We use t ∈ [1, |Y|] to denote the (discrete) time steps the agent takes actions. Additionally, when the agent follows x m , for convenience, we sometimes use t m ∈ [1, |ŷ m |] to index the time steps, instead of the "global time" t = t m + m−1 i=1 |ŷ i |.</p>
<p>Approach</p>
<p>We describe in detail the 3 key elements in the design of our navigation agent: (i) a memory buffer for storing and recalling past experiences to provide contexts for the current navigation instruction ( § 4.1); (ii) an imitation-learning stage of navigating with short instructions to accomplish a single BABY-STEP ( § 4.2.1); (iii) a curriculum-based reinforcement learning phase where the agent learns with increasingly longer instructions (i.e. multiple BABY-STEPs) ( § 4.2.2). We describe new benchmarks created for learning and evaluation and key implementation details in § 4.3 and § 4.4 (with more details in the Appendix).</p>
<p>The BABYWALK Agent</p>
<p>The basic operating model of our navigation agent BABYWALK is to follow a "micro instruction" x m (i.e., a short sequence of instructions, to which we Figure 2: The BABYWALK agent has a memory buffer storing its past experiences of instructions x m , and its trajectoryŷ m . When a new BABY-STEP x m is presented, the agent retrieves from the memory a summary of its experiences as the history context. It takes actions conditioning on the context (as well as its state s t and the previous actionâ t ). Upon finishing following the instruction. the trajectoryŷ m is then sent to the memory to be remembered.</p>
<p>also refer as BABY-STEP), conditioning on the contextẑ m and to output a trajectoryŷ m . A schematic diagram is shown in Fig. 2. Of particularly different from previous approaches is the introduction of a novel memory module. We assume the BABY-STEPs are given in the training and inference time - § 4.3 explains how to obtain them if not given a prior (Readers can directly move to that section and return to this part afterwards). The left of the Fig. 3 gives an example of those micro-instructions.</p>
<p>Context The context is a summary of the past experiences of the agent, namely the previous (m− 1) mini-instructions and trajectories:
z m = g f SUMMARY (x 1 , · · · , x m−1 ), f SUMMARY (ŷ 1 , · · · ,ŷ m−1 )(1)
where the function g is implemented with a multilayer perceptron. The summary function f SUMMARY is explained in below.</p>
<p>Summary To map variable-length sequences (such as the trajectory and the instructions) to a single vector, we can use various mechanisms such as LSTM. We reported an ablation study on this in § 5.3. In the following, we describe the "forgetting" one that weighs more heavily towards the most recent experiences and performs the best empirically.
f SUMMARY (x 1 , · · · , x m−1 ) = m−1 i=1 α i · u(x i ) (2) f SUMMARY (ŷ 1 , · · · ,ŷ m−1 ) = m−1 i=1 α i · v(ŷ i ) (3)
where the weights are normalized to 1 and inverse proportional to how far i is from m,
α i ∝ exp − γ · ω(m − 1 − i) (4)
γ is a hyper-parameter (we set to 1/2) and ω(·) is a monotonically nondecreasing function and we simply choose the identity function. Note that, we summarize over representations of "micro-instructions" (x m ) and experiences of executing those micro-instructionsŷ m . The two encoders u(·) and v(·) are described in § 4.4. They are essentially the summaries of "low-level" details, i.e., representations of a sequence of words, or a sequence of states and actions. While existing work often directly summarizes all the low-level details, we have found that the current form of "hierarchical" summarizing (i.e., first summarizing each BABY-STEP, then summarizing all previous BABY-STEPs) performs better.</p>
<p>Policy The agent takes actions, conditioning on the contextẑ m , and the current instruction x m :
a t ∼ π (·|s t ,â t−1 ; u(x m ),ẑ m )(5)
where the policy is implemented with a LSTM with the same cross-modal attention between visual states and languages as in (Fried et al., 2018).</p>
<p>Learning of the BABYWALK Agent</p>
<p>The agent learns in two phases. In the first one, imitation learning is used where the agent learns to execute BABY-STEPs accurately. In the second one, the agent learns to execute successively longer tasks from a designed curriculum.</p>
<p>Imitation Learning</p>
<p>BABY-STEPs are shorter navigation tasks. With the mth instruction x m , the agent is asked to follow the instruction so that its trajectory matches the human expert's y m . To assist the learning, the context is computed from the human expert trajectory up to the mth BABY-STEP (i.e., in eq. (1),ŷs are replaced with ys). We maximize the objective
= M m=1 |ym| tm=1 log π (a tm |s tm , a tm−1 ; u(x m ), z m )
We emphasize here each BABY-STEP is treated independently of the others in this learning regime. Each time a BABY-STEP is to be executed, we "preset" the agent in the human expert's context</p>
<p>Baby Walk</p>
<p>Baby Walk Baby Walk Figure 3: Two-phase learning by BABYWALK. (Left) An example instruction-trajectory pair from the R4R dataset is shown. The long instruction is segmented into four BABY-STEP instructions. We use those BABY-STEPs for imitation learning ( § 4.2.1) (Right) Curriculum-based RL. The BABYWALK agent warm-starts from the imitation learning policy, and incrementally learns to handle longer tasks by executing consecutive BABY-STEPs and getting feedback from external rewards (c.f . § 4.2.2). We illustrate two initial RL lectures using the left example. and the last visited state. We follow existing literature (Anderson et al., 2018;Fried et al., 2018) and use student-forcing based imitation learning, which uses agent's predicted action instead of the expert action for the trajectory rollout.</p>
<p>Curriculum Reinforcement Learning</p>
<p>We want the agent to be able to execute multiple consecutive BABY-STEPs and optimize its performance on following longer navigation instructions (instead of the cross-entropy losses from the imitation learning). However, there is a discrepancy between our goal of training the agent to cope with the uncertainty in a long instruction and the imitation learning agent's ability in accomplishing shorter tasks given the human annotated history. Thus it is challenging to directly optimize the agent with a typical RL learning procedure, even the imitation learning might have provided a good initialization for the policy, see our ablation study in § 5.3. Inspired by the curriculum learning strategy (Bengio et al., 2009), we design an incremental learning process that the agent is presented with a curriculum of increasingly longer navigation tasks. Fig. 3 illustrates this idea with two "lectures". Given a long navigation instruction X with M BABY-STEPs, for the kth lecture, the agent is given all the human expert's trajectory up to but not including the (M − k + 1)th BABY-STEP, as well as the history context z M−k+1 . The agent is then asked to execute the kth micro-instructions from x M−k+1 to x M using reinforcement learning to produce its trajectory that optimizes a task related  metric, for instance the fidelity metric measuring how faithful the agent follows the instructions. As we increase k from 1 to M, the agent faces the challenge of navigating longer and longer tasks with reinforcement learning. However, the agent only needs to improve its skills from its prior exposure to shorter ones. Our ablation studies show this is indeed a highly effective strategy.</p>
<p>New Datasets for Evaluation &amp; Learning</p>
<p>To our best knowledge, this is the first work studying how well VLN agents generalize to long navigation tasks. To this end, we create the following datasets in the same style as in (Jain et al., 2019).</p>
<p>ROOM6ROOM and ROOM8ROOM</p>
<p>We concatenate the trajectories in the training as well as the validation unseen split of the ROOM2ROOM dataset for 3 times and 4 times respectively, thus extending the lengths of navigation tasks to 6 rooms and 8 rooms. To join, the end of the former trajectory must be within 0.5 meter with the beginning of the later trajectory. Table 1 and Fig. 4 contrast the different datasets in the # of instructions, the average length (in words) of instructions and how the distributions vary. Table 1 summarizes the descriptive statistics of BABY-STEPs across all datasets used in this paper. The datasets and the segmentation/alignments are made publically available 2 .</p>
<p>Key Implementation Details</p>
<p>In the following, we describe key information for research reproducibility, while the complete details are in the Appendix.</p>
<p>States and Actions</p>
<p>We follow (Fried et al., 2018) to set up the states as the visual features (i.e. ResNet-152 features (He et al., 2016)) from the agent-centric panoramic views in 12 headings × 3 elevations with 30 degree intervals. Likewise, we use the same panoramic action space.</p>
<p>Identifying BABY-STEPs Our learning approach requires an agent to follow microinstructions (i.e., the BABY-STEPs). Existing datasets (Anderson et al., 2018;Jain et al., 2019;Chen et al., 2019) do not provide fine-grained segmentations of long instructions. Therefore, we use a template matching approach to aggregate consecutive sentences into BABY-STEPs. First, we extract the noun phrase using POS tagging. Then, we employs heuristic rules to chunk a long instruction into shorter segments according to punctuation and landmark phrase (i.e., words for concrete objects). We document the details in the Appendix.</p>
<p>Aligning BABY-STEPs with Expert Trajectory</p>
<p>Without extra annotation, we propose a method to approximately chunk original expert trajectories into sub-trajectories that align with the BABY-STEPs. This is important for imitation learning at the micro-instruction level ( § 4.2.1). Specifically, we learn a multi-label visual landmark classifier to identify concrete objects from the states along expert trajectories by using the landmark phrases extracted from the their instructions as weak supervision. For each trajectory-instruction pair, we then extract the visual landmarks of every state as well as the landmark phrases in BABY-STEP instructions. Next, we perform a dynamic programming procedure to segment the expert trajectories by aligning the visual landmarks and landmark phrases, using the confidence scores of the multi-label visual landmark classifier to form the function.</p>
<p>Encoders and Embeddings</p>
<p>The encoder u(·) for the (micro)instructions is a LSTM. The encoder for the trajectory y contains two separate Bi-LSTMs, one for the state s t and the other for the action a t . The outputs of the two Bi-LSTMs are then concatenated to form the embedding function v(·). The details of the neural network architectures (i.e. configurations as well as an illustrative figure), optimization hyper-parameters, etc. are included in the Appendix.</p>
<p>Learning Policy with Reinforcement Learning</p>
<p>In the second phase of learning, BABYWALK uses RL to learn a policy that maximizes the fidelity-oriented rewards (CLS) proposed by Jain et al. (2019). We use policy gradient as the optimizer (Sutton et al., 2000). Meanwhile, we set the maximum number of lectures in curriculum RL to be 4, which is studied in Section 5.3.</p>
<p>Experiments</p>
<p>We describe the experimental setup ( § 5.1),followed by the main results in § 5.2 where we show the proposed BABYWALK agent attains competitive results on both the in-domain dataset but also generalizing to out-of-the-domain datasets with varying lengths of navigation tasks. We report results from various ablation studies in § 5.3. While we primarily focus on the ROOM4ROOM dataset, we re-analyze the original ROOM2ROOM dataset in § 5.4 and were surprised to find out the agents trained on it can generalize.</p>
<p>Experimental Setups.</p>
<p>Datasets We conduct empirical studies on the existing datasets ROOM2ROOM and ROOM4ROOM (Anderson et al., 2018;Jain et al., 2019), and the two newly created benchmark datasets ROOM6ROOM and ROOM8ROOM, described in § 4.3. Table 1 and Fig. 4 contrast their differences.</p>
<p>In-domain</p>
<p>Generalization to other datasets  Agents to Compare to Whenever possible, for all agents we compare to, we either re-run, reimplement or adapt publicly available codes from their corresponding authors with their provided instructions to ensure a fair comparison. We also "sanity check" by ensuring the results from our implementation and adaptation replicate and are comparable to the reported ones in the literature. We compare our BABYWALK to the following: (1) the SEQ2SEQ agent (Anderson et al., 2018), being adapted to the panoramic state and action space used in this work; (2) the Speaker Follower The last 3 agents are reported having state-ofthe art results on the benchmark datasets. Except the SEQ2SEQ agent, all other agents depend on an additional pre-training stage with data augmentation (Fried et al., 2018), which improves crossboard. Thus, we train two BABYWALK agents: one with and the other without the data augmentation.
Setting R4R → R4R R 4R → R2R R 4R → R6R R 4R →</p>
<p>Main results</p>
<p>In-domain Generalization This is the standard evaluation scenario where a trained agent is assessed on the unseen split from the same dataset as the training data. The leftmost columns in Table 2 reports the results where the training data is from R4R. The BABYWALK agents outperform all other agents when evaluated on CLS and SDTW.</p>
<p>When evaluated on SR, FAST performs the best and the BABYWALK agents do not stand out. This is expected: agents which are trained to reach goal do not necessarily lead to better instructionfollowing. Note that RCM(FIDELITY) performs well in path-following.</p>
<p>Out-of-domain Generalization While our primary goal is to train agents to generalize well to longer navigation tasks, we are also curious how the agents perform on shorter navigation tasks too. The right columns in Table 2 report the comparison. The BABYWALK agents outperform all other agents in all metrics except SR. In particular, on   SDTW, the generalization to R6R and R8R is especially encouraging, resulting almost twice those of the second-best agent FAST. Moreover, recalling from Fig. 1, BABYWALK's generalization to R6R and R8R attain even better performance than the RCM agents that are trained in-domain. Fig. 5 provides additional evidence on the success of BABYWALK, where we have contrasted to its performance to other agents' on following instructions in different lengths across all datasets. Clearly, the BABYWALK agent is able to improve very noticeably on longer instructions. Fig. 6 contrasts visually several agents in executing two (long) navigation tasks. BABYWALK's trajectories are similar to what human experts provide, while other agents' are not. Table 3 illustrates the importance of having a memory buffer to summarize the agent's past experiences. Without the memory (NULL), generalization to longer tasks is significantly worse. Using LSTM to summarize is worse than using forgetting to summarize (eqs. (2,3)). Meanwhile, ablating γ of the forgetting   mechanism concludes that γ = 0.5 is the optimal to our hyperparameter search. Note that when γ = 0, this mechanism degenerates to taking average of the memory buffer, and leads to inferior results. Table 4 establishes the value of CRL. While imitation learning (IL) provides a good warm-up for SR, significant improvement on other two metrics come from the subsequent RL (IL+RL). Furthermore, CRL (with 4 "lectures") provides clear improvements over direct RL on the entire instruction (i.e., learning to execute all BABY-STEPs at once). Each lecture improves over the previous one, especially in terms of the SDTW metric.</p>
<p>Qualitative Results</p>
<p>Analysis</p>
<p>Memory Buffer is Beneficial</p>
<p>Curriculum-based RL (CRL) is Important</p>
<p>Revisiting ROOM2ROOM</p>
<p>Our experimental study has been focusing on using R4R as the training dataset as it was established that as opposed to R2R, R4R distinguishes well an agent who just learns to reach the goal from an agent who learns to follow instructions. Given the encouraging results of generalizing to longer tasks, a natural question to ask, how well can an agent trained on R2R generalize? Results in Table 5 are interesting. Shown in the top panel, the difference in the averaged performance of generalizing to R6R and R8R is not significant. The agent trained on R4R has a small win on R6R presumably because R4R is closer to R6R than R2R does. But for even longer tasks in R8R, the win is similar.</p>
<p>In the bottom panel, however, it seems that R2R → R4R is stronger (incurring less loss in performance when compared to the in-domain setting R4R → R4R) than the reverse direction (i.e., comparing R4R → R2R to the in-domain R2R → R2R). This might have been caused by the noisier segmentation of long instructions into BABY-STEPs in R4R. (While R4R is composed of two navigation paths in R2R, the segmentation algorithm is not aware of the "natural" boundaries between the two paths.)</p>
<p>Discussion</p>
<p>There are a few future directions to pursue. First, despite the significant improvement, the gap between short and long tasks is still large and needs to be further reduced. Secondly, richer and more complicated variations between the learning setting and the real physical world need to be tackled. For instance, developing agents that are robust to variations in both visual appearance and instruction descriptions is an important next step. </p>
<p>A Details on BABY-STEP Identification and Trajectory Alignments</p>
<p>In this section, we describe the details of how BABY-STEPs are identified in the annotated natural language instructions and how expert trajectory data are segmented to align with BABY-STEP instructions.</p>
<p>A.1 Identify BABY-STEPs</p>
<p>We identify the navigable BABY-STEPs from the natural language instructions of R2R, R4R, R6R and R8R, based on the following 6 steps:</p>
<ol>
<li>
<p>Split sentence and chunk phrases. We split the instructions by periods. For each sentence, we perform POS tagging using the SpaCy (Honnibal and Montani, 2017) package to locate and chunk all plausible noun phrases and verb phrases.</p>
</li>
<li>
<p>Curate noun phrases. We curate noun phrases by removing the stop words (i.e., the, for, from etc.) and isolated punctuations among them and lemmatizing each word of them. The purpose is to collect a concentrated set of semantic noun phrases that contain potential visual objects.</p>
</li>
<li>
<p>Identify "landmark words". Next, given the set of candidate visual object words, we filter out a blacklist of words that either do not correspond to any visual counterpart or are misclassified by the SpaCy package. The word blacklist includes: end, 18 inch, head, inside, forward, position, ground, home, face, walk, feet, way, walking, bit, veer, 've, next, stop, towards, right, direction, thing, facing, side, turn, middle, one, out, piece, left, destination, straight, enter, wait, don't, stand, back, round We use the remaining noun phrases as the "landmark words" of the sentences. Note that this step identifies the "landmark words" for the later procedure which aligns BABY-STEPs and expert trajectories.</p>
</li>
</ol>
<p>Identifying verb phrases.</p>
<p>Similarly, we use a verb blacklist to filter out verbs that require no navigational actions of the agent. The blacklist includes: make, turn, face, facing, veer.</p>
<p>5.</p>
<p>Merge non-actionable sentences. We merge the sentence without landmarks and verbs into the next sentence, as it is likely not actionable.</p>
<ol>
<li>Merge stop sentences. There are sentences that only describe the stop condition of a navigation action, which include verb-noun compositions indicating the stop condition. We detect the sentences starting with wait, stop, there, remain, you will see as the sentences that only describe the stop condition and merge them to the previous sentence. Similarly, we detect sentences starting with with, facing and merge them to the next sentence.</li>
</ol>
<p>After applying the above 6 heuristic rules to the language instruction, we obtain chunks of sentences that describes the navigable BABY-STEPs of the whole task (i.e., a sequence of navigational sub-goals.).</p>
<p>A.2 Align Expert Trajectories with identified BABY-STEPs</p>
<p>In the previous section, we describe the algorithm for identifying BABY-STEP instructions from the original natural language instructions of the dataset. Now we are going to describe the procedure of aligning BABY-STEPs with the expert trajectories, which segments the expert trajectories according to the BABY-STEPs to create the training data for the learning pipeline of our BABYWALK agent. Note that during the training, our BABYWALK does not rely on the existence of ground-truth alignments between the (micro)instructions and BABY-STEPs trajectories.</p>
<p>Main Idea</p>
<p>The main idea here is to: 1) perform visual landmark classification to produce confidence scores of landmarks for each visual state s along expert trajectories; 2) use the predicted landmark scores and the "landmark words" in BABY-STEPs to guide the alignment between the expert trajectory and BABY-STEPs. To achieve this, we train a visual landmark classifier with weak supervision -trajectory-wise existence of landmark objects. Next, based on the predicted landmark confidence scores, we use dynamic programming (DP) to chunk the expert trajectory into segments and assign the segments to the BABY-STEPs.</p>
<p>Weakly Supervised Learning of the Landmark Classifier Given the pairs of aligned instruction and trajectories (X, Y) from the original dataset, we train a landmark classifier to detect landmarks mentioned in the instructions. We formulate it as a multi-label classification problem that asks a classifier f LDMK (s t ; O) to predict all the landmarks O X of the instruction X given the corresponding trajectory Y. Here, we denotes all possible landmarks from the entire dataset to be O, and the landmarks of a specific instruction X to be O X . Concretely, we first train a convolutional neural network (CNN) based on the visual state features s t to independently predict the existence of landmarks at every time step, then we aggregate the predictions across all time steps to get trajectory-wise logits ψ via max-pooling over all states of the trajectory.
ψ = max {f LDMK (s t ; O) | t = 1, . . . , |Y|}
Here f LDMK denotes the independent state-wise landmark classifier, and ψ is the logits before normalization for computing the landmark probability. For the specific details of f LDMK , we input the 6×6 panorama visual feature (i.e. ResNet-152 feature) into a two-layer CNN (with kernel size of 3, hidden dimension of 128 and ReLU as non-linearity layer) to produce feature activation with spatial extents, followed by a global averaging operator over spatial dimensions and a multi-layer perceptron (2-layer with hidden dimension of 512 and ReLU as non-linearity layer) that outputs the state-wise logits for all visual landmarks O. We then max pool all the state-wise logits along the trajectory and compute the loss using a trajectory-wise binary cross-entropy between the ground-truth landmark label (of existence) and the prediction.</p>
<p>Aligning BABY-STEPs and Trajectories with Visual Landmarks Now, sppose we have a sequence of BABY-STEP instructions X = {x m , m = 1, . . . , M}, and its expert trajectory Y = {s t , t = 1, . . . , |Y|}, we can compute the averaged landmark score for the landmarks O xm that exists in this sub-task instruction x m on a single state s t :
Ψ (t, m) = 1 [o m ∈ O xm ] f LDMK (s t ; O) |O xm | Here 1 [o m ∈ O]
represents the one-hot encoding of the landmarks that exists in the BABY-STEP x m , and |O xm | is the total number of existed landmarks. We then apply dynamic programming (DP) to solve the trajectory segmentation specified by the following Bellman equation (in a recursive form). Encoders Instruction encoder u(·) for the instructions is a single directional LSTM with hidden size 512 and a word embedding layer of size 300 (initialized with GloVE embedding (Pennington et al., 2014)). We use the same encoder for encoding the past experienced and the current executing instruction. Trajectory encoder v(·) contains two separate bidirectional LSTMs (Bi-LSTM), both with hidden size 512. The first Bi-LSTM encodes a t i and outputs a hidden state for each time step t i . Then we attends the hidden state to the panoramic view s t i to get a state feature of size 2176 for each time step. The second Bi-LSTM encoders the state feature. We use the trajectory encoder just for encoding the past experienced trajectories.
Φ (t, m) = ⎧ ⎪ ⎨ ⎪ ⎩ Ψ(t, m), if t = 1 Ψ(t, m) + max i∈{1,...,t−1} Φ(i, m − 1) ,</p>
<p>BABYWALK Policy</p>
<p>The BABYWALK policy network consists of one LSTM with two attention layers and an action predictor. First we attend the hidden state to the panoramic view s t to get state feature of size 2176. The state feature is concatenated with the previous action embedding as a variable to update the hidden state using a LSTM with hidden size 512. The updated hidden state is then attended to the context variables (output of u(·)). For the action predictor module, we concatenate the output of text attention layer with the summarized past contextẑ m in order to get an action prediction variable. We then get the action prediction variable through a 2-layer MLP and make a dot product with the navigable action embeddings to retrieve the probability of the next action.</p>
<p>Model Inference During the inference time, the BABYWALK policy only requires running the heuristic BABY-STEP identification on the test-time instruction. No need for oracle BABY-STEP trajectory during this time as the BABYWALK agent is going to roll out for each BABY-STEP by itself.</p>
<p>B.2 Details of Reward Shaping for RL</p>
<p>As mentioned in the main text, we learn policy via optimizing the Fidelity-oriented reward (Jain et al., 2019). Now we give the complete details of this reward function. Suppose the total number of roll out steps is T = M i=1 |ŷ i |, we would have the following form of reward function:
r(s t , a t ) = 0, if t &lt; T SR(Y,Ŷ) + CLS(Y,Ŷ), if t = T
Here,Ŷ =ŷ 1 ⊕ . . . ⊕ŷ M represents the concatenation of BABY-STEP trajectories produced by the navigation agent (and we note ⊕ as the concatenation operation).</p>
<p>B.3 Optimization Hyper-parameters</p>
<p>For each BABY-STEP task, we set the maximal number of steps to be 10, and truncate the corresponding BABY-STEP instruction length to be 100. During both the imitation learning and the curriculum reinforcement learning procedures, we fix the learning rate to be 1e-4. In the imitation learning, the mini-batch size is set to be 100. In the curriculum learning, we reduce the mini-batch size as curriculum increases to save memory consumption. For the 1st, 2nd, 3rd and 4th curriculum, the mini-batch size is set to be 50, 32, 20, and 20 respectively. During the learning, we pre-train our BABYWALK model for 50000 iterations using the imitation learning as a warm-up stage. Next, in each lecture (up to 4) of the reinforcement learning (RL), we train the BABYWALK agent for an additional 10000 iterations, and select the best performing model in terms of SDTW to resume the next lecture. For executing each instruction during the RL, we sample 8 navigation episodes before performing any back-propagation. For each learning stage, we use separate Adam optimizers to optimize for all the parameters. Meanwhile, we use the L2 weight decay as the regularizer with its coefficient set to be 0.0005. In the reinforcement learning, the discounted factor γ is set to be 0.95.</p>
<p>C Additional Experimental Results</p>
<p>In this section, we describe a comprehensive set of evaluation metrics and then show transfer results of models trained on each dataset, with all metrics. We provide additional analysis studying the effectiveness of template based BABY-STEP identification. Finally we present additional qualitative results.</p>
<p>Complete set of Evaluation Metrics. We adopt the following set of metrics:</p>
<p>• Path Length (PL) is the length of the agent's navigation path.</p>
<p>• Navigation Error (NE) measures the distance between the goal location and final location of the agent's path.</p>
<p>• Success Rate (SR) that measures the average rate of the agent stopping within a specified distance near the goal location (Anderson et al., 2018) •  Table 6: Sanity check of model trained on R2R and evaluated on its validation unseen split ( + : pre-trained with data augmentation; :reimplemented or readapted from the original authors' released code).</p>
<p>As mentioned in the main text, we compare our re-implementation and originally reported results of baseline methods on the R2R datasets, as Table 6. We found that the results are mostly very similar, indicating that our re-implementation are reliable.</p>
<p>C.2 Complete Curriculum Learning Results</p>
<p>We present the curriculum learning results with all evaluation metrics in Table 7.</p>
<p>C.3 Results of BABY-STEP Identification</p>
<p>We present an additional analysis comparing different BABY-STEP identification methods. We compare our template-based BABY-STEP identification with a simple method that treat each sentence as an BABY-STEP (referred as sentence-wise), both using the complete BABYWALK model with the same training routine. The results are shown in the    Table 8. Generally speaking, the template based BABY-STEP identification provides a better performance.</p>
<p>C.4 In-domain Results of Models Trained on Instructions with Different lengths</p>
<p>As mentioned in the main text, we display all the indomain results of navigation agents trained on R2R, R4R, R6R, R8R, respectively. The complete results of all different metrics are included in the Table 9. We note that our BABYWALK agent consistently outperforms baseline methods on each dataset. It is worth noting that on R4R, R6R and R8R datasets, RCM(GOAL) + achieves better results in SPL. This is due to the aforementioned fact that they often  take short-cuts to directly reach the goal, with a significantly short trajectory. As a consequence, the success rate weighted by inverse path length is high.</p>
<p>C.5 Transfer Results of Models Trained on Instructions with Different lengths</p>
<p>For completeness, we also include all the transfer results of navigation agents trained on R2R, R4R, R6R, R8R, respectfully. The complete results of all different metrics are included in the  lating to shorter ones, rather than extrapolating to longer instructions, which is intuitively an easier direction.</p>
<p>C.6 Additional Qualitative Results</p>
<p>We present more qualitative result of various VLN agents as Fig 8. It seems that BABYWALK can produce trajectories that align better with the human expert trajectories. </p>
<p>Figure 4 :
4The distribution of lengths of instructions and ground-truth trajectories in our datasets.</p>
<p>(SF) agent (Fried et al., 2018); (3) the Reinforced Cross-Modal Agent (RCM) (Wang et al., 2019) that refines the SF agent using reinforcement learning with either goal-oriented reward (RCM(GOAL)) or fidelity-oriented reward (RCM(FIDELITY)); (4) the Regretful Agent (REGRETFUL) (Ma et al., 2019b) that uses a progress monitor that records visited path and a regret module that performs backtracking; (5) the Frontier Aware Search with Backtracking agent (FAST) (Ke et al., 2019) that incorporates global and local knowledge to compare partial trajectories in different lengths.</p>
<p>Figure 5 :
5Performance by various agents on navigation tasks in different lengths. See texts for details.</p>
<p>Figure 6 :
6Trajectories by human experts and VLN agents on two navigation tasks. More are in the Appendix.</p>
<p>Figure 7 Figure 7 :
77otherwiseHere, Φ (t, m) represents the maximum potential of choosing the state s t as the end point of the BABY-STEP instruction x m . Solving this DP leads to a set of correspondingly segmented trajectories Y = {y m , m = 1, . . . , M}, with y m being the mth BABY-STEP sub-trajectory. gives an overview of the unrolled version of our full navigation agent.Panoramic State-Action Space(Fried et al.,  2018)  We set up the states s t as the stacked visual feature of agent-centric panoramic views in 12 headings × 3 elevations with 30 degree intervals. The visual feature of each view is a concatenation of the ResNet-152 feature vector of size 2048 and the orientation feature vector of size 128 (The 4-dimensional orientation feature [sin(φ); cos(φ); sin(ω); cos(ω)] are tiled 32 times). We use similar single-view visual feature of size 2176 as our action embeddings. Our network architecture at the m-th BABY-STEP sub-task. Red line represents the procedure of encoding context variable z m via summarizing the BABY-STEP trajectory f SUMMARY (v(ŷ 1 ), . . . , v(ŷ m−1 )) and the corresponding (micro)instruction f SUMMARY (u(x 1 ), . . . , u(x m−1 )) in the memory buffer. Blue line represents the procedure of encoding the (micro)instruction u(x m ) of the current BABY-STEP. Purple line represents the detailed decision making process of our BABYWALK policy (A st is denoted as the set of navigable directions at s t as defined by Fried et al.(2018))</p>
<p>Table 1: Datasets used for VLN learning and evaluationR2R </p>
<p>R 4R 
R 6R 
R 8R </p>
<p>Train seen instr. 
14,039 233,532 89,632 94,731 
Val unseen instr. 
2,349 
45,234 35,777 43,273 
Avg instr. length 
29.4 
58.4 
91.2 
121.6 </p>
<p>Avg # BABY-STEPs 1.8 
3.6 
5.6 
7.4 </p>
<p>R8R Average
R8RMetrics SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑SEQ2SEQ </p>
<p>25.7 20.7 9.0 
16.3 27.1 10.6 
14.4 17.7 4.6 
20.7 15.0 4.7 
17.1 19.9 6.6 </p>
<p>SF </p>
<ul>
<li></li>
</ul>
<p>24.9 23.6 9.2 
22.5 29.5 14.8 
15.5 20.4 5.2 
21.6 17.2 5.0 
19.9 22.4 8.3 </p>
<p>RCM(GOAL) </p>
<ul>
<li></li>
</ul>
<p>28.7 36.3 13.2 
25.9 44.2 20.2 
19.3 31.8 7.3 
22.8 27.6 5.1 
22.7 34.5 10.9 </p>
<p>RCM(FIDELITY) </p>
<ul>
<li>24.7 39.2 13.7 
29.1 34.3 18.3 
20.5 38.3 7.9 
20.9 34.6 6.1 
23.5 35.7 10.8 </li>
</ul>
<p>REGRETFUL </p>
<ul>
<li></li>
</ul>
<p>30.1 34.1 13.5 
22.8 32.6 13.4 
18.0 31.7 7.5 
18.7 29.3 5.6 
19.8 31.2 8.8 </p>
<p>FAST </p>
<ul>
<li></li>
</ul>
<p>36.2 34.0 15.5 
25.1 33.9 14.2 
22.1 31.5 7.7 
27.7 29.6 6.3 
25.0 31.7 9.4 </p>
<p>BABYWALK 
29.6 47.8 18.1 
35.2 48.5 27.2 
26.4 44.9 13.1 
26.3 44.7 11.5 
29.3 46.0 17.3 
BABYWALK + 
27.3 49.4 17.3 
34.1 50.4 27.8 
25.5 47.2 13.6 
23.1 46.0 11.1 
27.6 47.9 17.5 </p>
<p>Table 2 :
2VLN agents trained on the R4R dataset and evaluated on the unseen portion of the R4R (in-domain) and the other 3 out-of-the-domain datasets: R2R, R6R and R8R with different distributions in instruction length. The Appendix has more comparisons. ( + : pre-trained with data augmentation. : reimplemented or adapted from the original authors' public codes). Metrics We adopt the following metrics: Success Rate (SR) that measures the average rate of the agent stopping within a specified distance near the goal location(Anderson et al., 2018), Coverage weighted by Length Score (CLS) (Jain et al., 2019) that measures the fidelity of the agent's path to the reference, weighted by the length score, and the newly proposed Success rate weighted normalized Dynamic Time Warping (SDTW) that measures in more fine-grained details, the spatiotemporal similarity of the paths by the agent and the human expert, weighted by the success rate(Magalhaes et al., 2019). Both CLS and SDTW measure explicitly the agent's ability to follow instructions and in particular, it was shown that SDTW corresponds to human preferences the most. We report results in other metrics in the Appendix.Evaluation </p>
<p>Table 3 :
3The memory buffer is beneficial to generalizing to different tasks from on which the agent is trained.</p>
<p>Setting R4R →
R4RR4R R 4R → others Metrics SR↑ CLS↑ SDTW ↑ SR↑ CLS↑ SDTW ↑IL </p>
<p>24.7 27.9 
11.1 
24.2 25.8 
10.2 </p>
<p>IL+RL </p>
<p>25.0 45.5 
13.6 
25.0 43.8 
14.1 </p>
<p>IL+ CRL w/ LECTURE # 
1st 
24.1 44.8 
13.5 
24.1 43.1 
13.6 
2nd 
26.7 45.9 
15.2 
26.2 43.7 
14.8 
3rd 
27.9 47.4 
17.0 
26.7 45.4 
16.3 
4th 
27.3 49.4 
17.3 
27.6 47.9 
17.5 </p>
<p>Table 4 :
4BABYWALK's performances with curriculum-
based reinforcement learning (CRL), which improves 
imitation learning without or with reinforcement learn-
ing (IL+RL). </p>
<p>Eval 
→ R6R 
→ R8R 
Training SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑ </p>
<p>R2R </p>
<p>21.7 49.0 
11.2 
20.7 48.7 
9.8 </p>
<p>R4R </p>
<p>25.5 47.2 
13.6 
23.1 46.0 
11.1 </p>
<p>Eval 
→ R2R 
→ R4R 
Training SR↑ CLS↑ SDTW↑ SR↑ CLS↑ SDTW↑ </p>
<p>R2R </p>
<p>43.8 54.4 
36.9 
21.4 51.0 
13.8 </p>
<p>R4R </p>
<p>34.1 50.4 
27.8 
27.3 49.4 
17.3 </p>
<p>Table 5 :
5(Top) BABYWALK trained on R2R is nearly as effective as the agent trained on R4R when generalizing to longer tasks. (Bottom) BABYWALK trained on R2R adapts to R4R better than the agent trained in the reverse direction.</p>
<p>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive growing of gans for improved quality, stability, and variation. In ICLR.Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learning to navigate unseen environments: Back translation with environmental dropout. In EMNLP.Jacob Andreas, Dan Klein, and Sergey Levine. 2017. 
Modular multitask reinforcement learning with pol-
icy sketches. In ICML. </p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, 
and Jason Weston. 2009. Curriculum learning. In 
ICML. </p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Ma-
ciej Halber, Matthias Niessner, Manolis Savva, Shu-
ran Song, Andy Zeng, and Yinda Zhang. 2017. Mat-
terport3D: Learning from RGB-D data in indoor en-
vironments. In 3DV. </p>
<p>David L Chen and Raymond J Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In AAAI. </p>
<p>Howard Chen, Alane Suhr, Dipendra Misra, Noah 
Snavely, and Yoav Artzi. 2019. Touchdown: Natural 
language navigation and spatial reasoning in visual 
street environments. In CVPR. </p>
<p>Carlos Florensa, David Held, Markus Wulfmeier, 
Michael Zhang, and Pieter Abbeel. 2017. Reverse 
curriculum generation for reinforcement learning. 
In CoRL. 
Daniel Fried, Ronghang Hu, Volkan Cirik, Anna 
Rohrbach, Jacob Andreas, Louis-Philippe Morency, 
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, 
and Trevor Darrell. 2018. Speaker-follower models 
for vision-and-language navigation. In NeurIPS. </p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian 
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR. </p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: 
Natural language understanding with Bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear. </p>
<p>Hexiang Hu, Liyu Chen, Boqing Gong, and Fei Sha. 
2018. Synthesized policies for transfer and adapta-
tion across tasks and environments. In NeurIPS. </p>
<p>Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander 
Ku, Gabriel Magalhaes, Jason Baldridge, and Eu-
gene Ie. 2019. Transferable representation learning 
in vision-and-language navigation. In ICCV. </p>
<p>Vihan Jain, Gabriel Magalhaes, Alex Ku, Ashish 
Vaswani, Eugene Ie, and Jason Baldridge. 2019. 
Stay on the path: Instruction fidelity in vision-and-
language navigation. In EMNLP. </p>
<p>Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtz-
man, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin 
Choi, and Siddhartha Srinivasa. 2019. Tactical 
rewind: Self-correction via backtracking in vision-
and-language navigation. In CVPR. </p>
<p>Joohyun Kim and Raymond Mooney. 2013. Adapting 
discriminative reranking to grounded language learn-
ing. In ACL. </p>
<p>Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-
Regib, Zsolt Kira, Richard Socher, and Caiming 
Xiong. 2019a. Self-monitoring navigation agent via 
auxiliary progress estimation. In ICLR. </p>
<p>Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming 
Xiong, and Zsolt Kira. 2019b. The regretful agent: 
Heuristic-aided navigation through progress estima-
tion. In CVPR. </p>
<p>Gabriel Magalhaes, Vihan Jain, Alexander Ku, Eugene 
Ie, and Jason Baldridge. 2019. Effective and general 
evaluation for instruction conditioned navigation us-
ing dynamic time warping. In NeurIPS ViGIL Work-
shop. </p>
<p>Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B 
Tenenbaum, and Jiajun Wu. 2019. The neuro-
symbolic concept learner: Interpreting scenes, 
words, and sentences from natural supervision. In 
ICLR. </p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 
2016. Listen, attend, and walk: Neural mapping 
of navigational instructions to action sequences. In 
AAAI. </p>
<p>Khanh Nguyen and Hal Daumé III. 2019. Help, 
anna! visual navigation with natural multimodal as-
sistance via retrospective curiosity-encouraging imi-
tation learning. In EMNLP. </p>
<p>Junhyuk Oh, Satinder Singh, Honglak Lee, and Push-
meet Kohli. 2017. Zero-shot task generalization 
with multi-task deep reinforcement learning. In 
ICML. </p>
<p>Jeffrey Pennington, Richard Socher, and Christopher 
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 conference 
on empirical methods in natural language process-
ing (EMNLP), pages 1532-1543. </p>
<p>Sungryull Sohn, Junhyuk Oh, and Honglak Lee. 
2018. Hierarchical reinforcement learning for zero-
shot generalization with subtask dependencies. In 
NeurIPS. </p>
<p>Richard S Sutton, David A McAllester, Satinder P 
Singh, and Yishay Mansour. 2000. Policy gradient 
methods for reinforcement learning with function ap-
proximation. In NeurIPS. </p>
<p>Jesse Thomason, Michael Murray, Maya Cakmak, and 
Luke Zettlemoyer. 2019. Vision-and-dialog naviga-
tion. In CoRL. </p>
<p>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jian-
feng Gao, Dinghan Shen, Yuan-Fang Wang, 
William Yang Wang, and Lei Zhang. 2019. Re-
inforced cross-modal matching and self-supervised 
imitation learning for vision-language navigation. 
In CVPR. </p>
<p>Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, 
Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, and 
Ali Farhadi. 2017a. Visual semantic planning using 
deep successor representations. In ICCV. </p>
<p>Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J 
Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. 
2017b. Target-driven visual navigation in indoor 
scenes using deep reinforcement learning. In ICRA. 
Appendix </p>
<p>In this supplementary material, we provide details 
omitted in the main text. The content is organized 
as what follows: </p>
<p>• Section A. Details on identifying BABY-STEP 
instructions and aligning BABY-STEPs with ex-
pert trajectories. ( § 4.3 and  § 4.4 of the main 
text) </p>
<p>• Section B. Implementation details of the navi-
gation agent, reward function used in RL and 
optimization hyper-parameters. ( § 4.4 of the 
main text) </p>
<p>• Section C. Additional experimental results, in-
cluding in-domain &amp; transfer results of different 
dataset trained models, sanity check of our reim-
plementation, and extra analysis of BABYWALK. 
( § 5.1 and  § 5.2 of the main text) </p>
<p>Success weighted by Path Length (SPL) (Anderson et al., 2018) measures the success rate weighted by the inverse trajectory length, to penalize very long successful trajectory. Coverage weighted by Length Score (CLS) (Jain et al., 2019) that measures the fidelity of the agent's path to the reference, weighted by the length score, and the newly proposed • Normalized Dynamic Time Warping (NDTW) that measures in more fine-grained details, the spatiotemporal similarity of the paths by the agent and the human expert (Magalhaes et al., 2019). Success rate weighted normalized Dynamic Time Warping (SDTW) that further measures the spatiotemporal similarity of the paths weighted by the success rate (Magalhaes et al., 2019). CLS, NDTW and SDTW measure explicitly the agent's ability to follow instructions and in particular, it was shown that SDTW corresponds to human preferences the most.• • C.1 Sanity Check between Prior Methods 
and Our Re-implementation </p>
<p>Data Splits 
R2R Validation Unseen 
Perf. Measures </p>
<p>PL NE↓ SR↑ SPL </p>
<p>Reported Results 
SEQ2SEQ (Fried et al., 2018) 
-7.07 31.2 -</p>
<p>SF </p>
<ul>
<li>(Fried et al., 2018) 
-6.62 35.5 -</li>
</ul>
<p>RCM </p>
<ul>
<li>(Wang et al., 2019) 
14.84 5.88 42.5 -</li>
</ul>
<p>REGRETFUL </p>
<ul>
<li>(Ma et al., 2019b) 
-5.32 50.0 41.0 </li>
</ul>
<p>FAST </p>
<ul>
<li>(Ke et al., 2019) 
21.17 4.97 56.0 43.0 </li>
</ul>
<p>Re-implemented Version </p>
<p>SEQ2SEQ </p>
<p>15.76 6.71 33.6 25.5 </p>
<p>SF </p>
<ul>
<li></li>
</ul>
<p>15.55 6.52 35.8 27.6 </p>
<p>RCM </p>
<ul>
<li></li>
</ul>
<p>11.15 6.18 42.4 38.6 </p>
<p>REGRETFUL </p>
<ul>
<li></li>
</ul>
<p>13.74 5.38 48.7 39.7 </p>
<p>FAST </p>
<ul>
<li></li>
</ul>
<p>20.45 4.97 56.6 43.7 </p>
<p>IL+ CRL w/ LECTURE # D a ta s e ts M e tr ic s SDTW↑ 11.1 13.6 13.5 15.2 17.0 17.3 NDTW↑ 17.8 28.1 26.0 26.9 30.9 32.7 SDTW↑ 7.7 10.8 9.7 11.0 12.7 13.6 R 8 R PL 93.1 47.5 50.0 55.3 45.2 39.9IL </p>
<p>IL + R L 
1 s t 
2 n d 
3 rd 
4 th </p>
<p>R 2 R </p>
<p>PL </p>
<p>22.4 12.0 11.6 13.2 10.6 9.6 </p>
<p>NE↓ </p>
<p>6.8 7.1 6.8 6.8 6.7 
6.6 </p>
<p>SR↑ </p>
<p>28.1 29.8 29.9 33.2 32.2 34.1 </p>
<p>SPL↑ </p>
<p>15.7 24.3 24.9 26.6 27.5 30.2 </p>
<p>CLS↑ </p>
<p>28.9 46.2 46.6 47.2 48.1 50.4 
NDTW↑ 30.6 43.8 42.5 41.0 47.7 50.0 
SDTW↑ 16.5 23.2 23.1 24.3 25.7 27.8 </p>
<p>R 4 R </p>
<p>PL </p>
<p>43.4 22.8 23.9 25.5 21.4 19.0 </p>
<p>NE↓ </p>
<p>8.4 8.6 8.5 8.4 8.0 
8.2 </p>
<p>SR↑ </p>
<p>24.7 25.0 24.1 26.7 27.9 27.3 </p>
<p>SPL↑ </p>
<p>8.2 11.2 11.0 12.3 13.7 14.7 </p>
<p>CLS↑ </p>
<p>27.9 45.5 44.8 45.9 47.4 49.4 
NDTW↑ 24.3 34.4 32.8 33.7 38.4 39.6 
R 6 R </p>
<p>PL </p>
<p>68.8 35.3 37.0 40.6 33.2 28.7 </p>
<p>NE↓ </p>
<p>9.4 9.5 9.4 9.4 8.9 
9.2 </p>
<p>SR↑ </p>
<p>22.7 23.7 21.9 23.4 24.7 25.5 </p>
<p>SPL↑ </p>
<p>4.2 7.2 6.4 6.8 8.1 
9.2 </p>
<p>CLS↑ </p>
<p>24.4 43.0 41.8 42.3 44.2 47.2 </p>
<p>Table 7 :
7Ablation on BABYWALK after each learning stage (trained on R4R).</p>
<p>Table 8 :
8BABYWALK Agent performances between different segmentation rules (trained on R4R). Refer to text for more details.</p>
<p>Table 10 .
10According to this table, we note that models trained on R8R can achieve the best overall transfer learning performances. This could because of the fact that R8R trained model only needs to deal with interpo-+ R C M ( F ID E L IT Y ) B A B Y W A L K B A B Y W A L KD a ta s e ts 
M e tr ic s 
S E Q 2 S E Q </p>
<p>S F </p>
<ul>
<li></li>
</ul>
<p>R C M ( G O A L ) </p>
<ul>
<li></li>
<li></li>
</ul>
<p>R 2 R → R 2 R </p>
<p>PL </p>
<p>15.8 15.6 11.1 10.2 
10.7 10.2 </p>
<p>NE↓ </p>
<p>6.7 6.5 6.2 
6.2 
6.2 5.9 </p>
<p>SR↑ </p>
<p>33.6 35.8 42.4 42.1 
42.6 43.8 </p>
<p>SPL↑ </p>
<p>25.5 27.6 38.6 38.6 
38.3 39.6 </p>
<p>CLS↑ </p>
<p>38.5 39.8 52.7 52.6 
52.9 54.4 
NDTW↑ 39.2 41.0 51.0 50.8 
53.4 55.3 
SDTW↑ 24.9 27.2 33.5 34.4 
35.7 36.9 </p>
<p>R 4 R → R 4 R </p>
<p>PL </p>
<p>28.5 26.1 12.3 26.4 
23.8 19.0 </p>
<p>NE↓ </p>
<p>8.5 8.3 7.9 
8.4 
7.9 8.2 </p>
<p>SR↑ </p>
<p>25.7 24.9 28.7 24.7 
29.6 27.3 </p>
<p>SPL↑ </p>
<p>14.1 16.0 22.1 11.6 
14.0 14.7 </p>
<p>CLS↑ </p>
<p>20.7 23.6 36.3 39.2 
47.8 49.4 
NDTW↑ 20.6 22.7 31.3 31.3 
38.1 39.6 
SDTW↑ 9.0 9.2 13.2 13.7 
18.1 17.3 </p>
<p>R 6 R → R 6 R </p>
<p>PL </p>
<p>34.1 43.4 11.8 28.0 
28.4 27.2 </p>
<p>NE↓ </p>
<p>9.5 9.6 9.2 
9.4 
9.4 9.3 </p>
<p>SR↑ </p>
<p>18.1 17.8 18.2 20.5 
21.7 22.0 </p>
<p>SPL↑ </p>
<p>9.6 7.9 14.8 
7.4 
7.8 8.1 </p>
<p>CLS↑ </p>
<p>23.4 20.3 31.6 39.0 
47.1 47.4 
NDTW↑ 19.3 17.8 25.9 25.8 
32.6 33.4 
SDTW↑ 6.5 5.9 7.6 
9.5 
11.5 11.8 </p>
<p>R 8 R → R 8 R </p>
<p>PL </p>
<p>40.0 53.0 12.4 42.3 
35.6 39.1 </p>
<p>NE↓ </p>
<p>9.9 10.1 10.2 10.7 
9.6 9.9 </p>
<p>SR↑ </p>
<p>20.2 18.6 19.7 18.2 
22.3 22.0 </p>
<p>SPL↑ </p>
<p>12.4 9.8 15.4 
5.3 
7.3 7.0 </p>
<p>CLS↑ </p>
<p>19.8 16.3 25.7 37.2 
46.4 46.4 
NDTW↑ 15.8 13.5 19.4 21.6 
29.6 28.3 
SDTW↑ 5.1 4.4 5.8 
7.6 
10.4 10.1 </p>
<p>Table 9 :
9Indomain results. Each model is trained on the training set of R2R, R4R, R6R and R8R datasets, and evaluated on the corresponding unseen validation set ( + : pre-trained with data augmentation).
Anecdotally, we do not have to learn from long navigation experiences. Instead, we extrapolate from our experiences of learning to navigate in shorter distances or smaller spaces (perhaps a skill we learn when we were babies or kids).
Available at https://github.com/Sha-Lab/ babywalk
(a)R2RFigure 8: Additional trajectories by human experts and VLN agents on two navigation tasks.
Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, CVPR. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision- and-language navigation: Interpreting visually- grounded navigation instructions in real environ- ments. In CVPR.</p>            </div>
        </div>

    </div>
</body>
</html>
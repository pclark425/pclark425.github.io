<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2151 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2151</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2151</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-276725462</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.21151v1.pdf" target="_blank">A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images</a></p>
                <p><strong>Paper Abstract:</strong> This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2151.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2151.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoising Diffusion Probabilistic Models (DDPM) and related diffusion frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of generative neural networks that produce images by learning a reverse denoising process from noise to data through many small steps; noted in the paper as state-of-the-art for high-fidelity, diverse, and controllable image synthesis and the dominant approach for text-to-image and image-to-image tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Diffusion Models (DDPM / score-based / SDE frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural network / probabilistic generative model (score-based / SDE)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>image synthesis; scientific imaging (microscopy, MRI, materials, medical)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates high-quality synthetic images from noise, conditioned on text or images (text-to-image, image-to-image); can be guided toward classes or conditions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to benchmark datasets when available; quantitative image-quality metrics (SSIM, PSNR) and diversity/fidelity metrics; expert (domain scientist) qualitative review; cross-validation on held-out data; comparison to physical/scientific models where possible</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicitly measured by distance from training data / out-of-distribution status; novelty assessed qualitatively by absence of matching ground-truth, expert judgement, and by lack of coverage in curated benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>qualitatively described as very high image quality, high diversity and controllability for in-distribution tasks; however, when generating images of phenomena not present in training data, models are prone to hallucination and can produce physically/biologically impossible outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation is effective for familiar, well-represented domains using SSIM/PSNR and benchmarks, and via expert review; validation becomes unreliable or impossible for truly novel scenarios due to lack of ground truth and sparse curated scientific datasets</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation reliability decreases as novelty increases — for out-of-distribution generations there may be no experimental ground truth, causing higher risk of undetected hallucinations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — generation can produce plausible-looking but scientifically incorrect outputs more easily than validation can definitively detect them, especially for novel or rare phenomena</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>degrades relative to in-distribution; models may hallucinate physically impossible features when extrapolating beyond training data</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified in paper; authors note 'hallucination' risk and imply confidence/uncertainty estimates and calibration degrade on novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>qualitatively higher for validation when expert review, experimental comparison, or domain-specific simulation is required compared to the relatively fast image generation process; exact costs not provided</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>incorporation of domain knowledge, curated scientific datasets, expert-in-the-loop validation, benchmarks tailored to scientific imagery, transfer learning/fine-tuning on domain data, uncertainty quantification and ensemble checks, classifier guidance or classifier-free guidance with careful conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diffusion models achieve the highest image quality and controllability among modern generators for in-distribution tasks, but they are prone to hallucination when extrapolating; validation methods exist for familiar tasks (SSIM/PSNR, benchmarks, expert review) but fail or become infeasible for novel scientific scenarios, producing a generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2151.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent / Stable Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Diffusion Models (Stable Diffusion variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient diffusion approach that performs diffusion in a lower-dimensional latent space (often using a pretrained VAE encoder/decoder) enabling high-resolution image generation with conditioning (text/image) and classifier-free guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Latent Diffusion / Stable Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural network / latent-space diffusion model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>text-to-image and image-to-image; applicable to scientific image generation via conditioning/fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates high-resolution images efficiently in latent space; supports conditioning on text embeddings or image features to steer outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>same as diffusion models generally: image-quality metrics (SSIM, PSNR), benchmark comparisons, domain expert evaluation, and checking against domain-specific datasets or physical constraints if available</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>measured by presence/absence of similar latents in training set and by domain expert assessment; novelty often inferred from lack of matching ground-truth examples</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>reported as high for in-domain, high-resolution synthetic images and efficient compared to pixel-space diffusion; controllability through classifier-free guidance enhances adherence to conditioning; performance on novel scientific phenomena is limited and subject to hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>effective for familiar image types but limited on novel scientific images due to sparse benchmarks and potential mismatch between latent representations and domain-specific features</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>latent-space methods inherit the same novelty-dependent validation degradation; domain-specific fine-tuning can mitigate but not eliminate risks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — strong generative fidelity in-domain but validation remains weak for out-of-distribution scientific cases</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>worse than in-distribution; liable to produce plausible-looking but scientifically incorrect images</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified; paper implies calibration/uncertainty worsens for novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation is computationally efficient in latent space; validation (expert review, experimental checks) remains comparatively expensive especially for novel cases</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>domain-specific fine-tuning, curated scientific latent datasets, classifier-free guidance tuned with domain embeddings, human-in-the-loop verification</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Latent diffusion (Stable Diffusion) provides computationally efficient, high-quality image generation and improved controllability, but validation for novel scientific outputs is limited by lack of ground truth and dataset mismatch, maintaining a generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2151.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Classifier-free guidance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classifier-free Guidance (guided diffusion without separate classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A guidance technique for conditional diffusion models that trains a single model to produce both conditional and unconditional outputs and interpolates between them at inference time to control adherence to conditions without a separate classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Classifier-free guidance</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>inference-guidance technique for diffusion neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>conditional image generation (text-to-image, image-to-image); applicable to scientific conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>improves control over generated images to better match conditioning inputs (e.g., text embeddings), reducing some mismatch between intent and output</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>improves empirical alignment between conditioning and outputs, validated via qualitative inspection, conditioning-consistency metrics, and downstream task performance; does not by itself provide scientific correctness checks</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not a novelty metric itself; helps enforce conditioning but cannot ensure physical validity for novel scenarios absent proper training data</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>improves fidelity to conditioning and often yields better subjective alignment; still subject to hallucination when conditioning refers to phenomena not represented in training data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>helps produce outputs that are easier to validate against the conditioning prompt, but does not replace scientific validation; validation remains limited for novel tasks</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>classifier-free guidance can reduce conditional mismatch in familiar tasks but has limited effect on validation reliability for novel/out-of-distribution content</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>partial — guidance reduces generation errors relative to prompts but does not close the gap between plausible-looking generation and scientific validity when ground truth is absent</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>guidance helps adherence to prompt but cannot prevent physically incorrect extrapolations for out-of-distribution prompts</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified; guidance may overconfidently force adherence to conditioning even when training support is weak</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>no major extra cost at training (single model) and moderate cost at inference for different guidance weights; validation cost unchanged</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>combining guidance with domain embeddings, fine-tuning on domain data, and human-in-the-loop checking</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Classifier-free guidance improves conditional controllability and alignment to prompts, which aids validation for familiar tasks, but it does not solve validation failures for novel scientific content and can give a false sense of correctness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2151.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Language-Image Pretraining (CLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal vision-and-language model that embeds images and text into a shared space, used to align generated images with textual descriptions and to guide generation toward prompt-consistent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CLIP</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal transformer / contrastive embedding model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>text-image alignment; text-to-image conditioning and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>not a generator itself; used to guide or evaluate image generation systems to increase fidelity to textual prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>serves as an automated alignment validator by scoring image-text similarity; used as a proxy metric for prompt adherence and for ranking candidate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>similarity in embedding space can indicate novelty (low similarity to known pairs), but CLIP does not directly quantify scientific novelty or physical correctness</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>when integrated (e.g., DALL-E 2) improves semantic alignment between prompt and generated image for familiar concepts; does not ensure scientific validity for novel phenomena</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>useful for checking prompt-image alignment on familiar concepts; unreliable as sole validator for scientific correctness or out-of-distribution content</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>CLIP-based alignment may produce high similarity scores for visually plausible but scientifically incorrect images, especially for novel cases where training data lacks domain specifics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — CLIP helps validate alignment to prompts but cannot validate adherence to underlying scientific principles, creating an asymmetry between perceived and actual correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>degrades for domain-specific or specialized scientific imagery that differs from CLIP's training distribution</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified in paper; authors imply limitations for domain-specific calibration</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>low to moderate (embedding and similarity computation) compared to experimental validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>augment CLIP with domain-specific encoders/embeddings or combine with expert review and domain constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLIP is effective for measuring prompt-image semantic alignment in familiar domains but cannot substitute for scientific validation; reliance on CLIP risks accepting visually plausible yet scientifically invalid outputs, especially for novel phenomena.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2151.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DALL-E family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DALL-E, DALL-E 2, DALL-E 3 (OpenAI text-to-image models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based and diffusion-based multimodal models for generating images from textual descriptions; noted in the review as influential milestones in text-to-image synthesis and as employing mechanisms (e.g., CLIP, LLM components) to improve alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DALL-E (1/2/3)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multimodal transformer / diffusion pipeline (varies across versions)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>text-to-image generation; general creative and illustrative image synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates images from textual prompts, combining disparate concepts into coherent visual outputs; used as a baseline for text-to-image research</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>evaluated by image quality metrics, human evaluations for prompt adherence, and CLIP-based alignment scores; scientific validation not intrinsic to model</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty inferred from creative combinations distinct from training data and by human judgement; not explicitly quantified in the review</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>reported improved text comprehension and image quality across versions; strong for familiar, in-distribution prompts but liable to hallucination or physical impossibilities when extrapolating</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>human evaluation and CLIP-based metrics effective for assessing prompt adherence in familiar domains; these do not guarantee scientific correctness for novel cases</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>human and CLIP-based validation less reliable for novel scientific prompts lacking ground truth, increasing risk of undetected errors</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — visually coherent generations can outpace the ability of standard validation methods (CLIP, human subjective checks) to detect scientific inaccuracies</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>degrades and may produce implausible or impossible scientific images for out-of-distribution prompts</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified; authors highlight hallucination concerns indicating calibration issues for novel prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation cost is moderate to high; validation (expert review, experimental tests) is significantly more expensive and time-consuming</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>integrating domain-specific models/constraints, curated scientific datasets, expert-in-the-loop checks, and formalized V&V protocols</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DALL-E models advanced text-to-image quality and alignment but the review warns they can hallucinate and misrepresent scientific principles when extrapolating; common validation (CLIP, human ratings) is insufficient for novel scientific outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2151.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GANs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Networks (GANs) and variants (CGAN, DCGAN, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adversarially trained generator-discriminator models that synthesize images by learning to fool a discriminator; historically significant for sharp images but subject to instability and mode collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GANs (Vanilla GAN, CGAN, DCGAN, StackGAN, AttnGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural network / adversarial generative model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>image synthesis; text-to-image (via conditional GAN variants); some niche scientific image tasks (e.g., upscaling, style transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates sharp, diverse images; conditional variants can accept text or images to guide generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>evaluated via image quality and diversity metrics, human assessment, and task-specific benchmarks; less stable training requires careful validation across runs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty assessed by diversity metrics and distance from training modes; mode collapse can reduce novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>high image sharpness for many tasks but training instability (mode collapse) can limit diversity and robustness; performance on specialized scientific imagery is limited without large domain datasets</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation via benchmarks and metrics works for familiar tasks but GAN instability and mode collapse complicate consistent validation; for novel scientific outputs validation remains insufficient</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>mode collapse and instability make validation of novelty difficult; out-of-distribution or rare phenomena can be missed or misrepresented</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>yes — GANs can produce visually convincing results while validation may not detect missing modes or incorrect scientific detail, particularly for novel cases</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>poor relative to diffusion models; tends to overfit or collapse rather than faithfully extrapolate</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified; GAN outputs do not provide intrinsic uncertainty estimates and thus are poorly calibrated for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>training and repeated runs to ensure stability are costly; experimental validation for scientific claims remains more expensive</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>conditional architectures, attention mechanisms (AttnGAN), hierarchical refinements (StackGAN), and incorporation of domain constraints; still require expert validation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GANs can produce sharp images but suffer from training instability and mode collapse; these characteristics make generating and validating novel scientific images challenging, reinforcing the generation-validation gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2151.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAEs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoders (VAEs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latent variable generative models that encode data into a continuous latent space and decode samples, trading some quality for training stability and explicit probabilistic modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Variational Autoencoders (VAEs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic latent-variable neural network</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>image synthesis, latent representations for diffusion models, potential use in scientific image generation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>can generate images by sampling latent space; often used as encoders/decoders in latent diffusion frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>likelihood-based objectives (ELBO), reconstruction metrics, SSIM/PSNR, and expert evaluation for scientific imagery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty reflected by distance in latent space from known examples; VAEs can struggle to generate high-variability novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>moderate image quality (often blurrier than GANs or diffusion) but stable training; limited diversity and controllability compared to diffusion/GANs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation via likelihood and reconstruction metrics is straightforward for familiar data but does not ensure scientific correctness for novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>VAEs' poorer fidelity and lower diversity make generating and validating novel, high-detail scientific images more difficult</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>present — VAEs' conservative outputs can be easier to validate visually but may lack the detail needed for scientific utility; validation still requires domain checks</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>limited; tends to produce blurred, less detailed extrapolations</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not quantified; VAEs provide probabilistic outputs but calibration for novel scientific cases is not addressed</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>training stable and efficient; validation (domain checks) remains externally costly</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>use as latent encoders in LDMs, combine with downstream models or fine-tuning on domain data, incorporate domain constraints</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VAEs offer stable training and probabilistic modeling but lower fidelity and diversity, limiting their usefulness for generating detailed novel scientific images and leaving validation challenges largely unresolved.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2151.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2151.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification & Validation (V&V)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Verification and Validation practices for generative AI outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of methods and procedures described in the review for assessing correctness and reliability of generative-model outputs, especially for scientific imagery, including unit testing, benchmark evaluation, cross-validation, expert review, and domain-specific comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Verification & Validation (V&V) protocols</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>methodological framework (testing and evaluation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>applies across generative AI for scientific imaging and other scientific-data-generating systems</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>not a generator — V&V is the validation framework applied to outputs from generators</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>unit testing, benchmark dataset evaluation, cross-validation, quantitative image metrics (SSIM, PSNR), expert domain-scientist qualitative evaluation, comparison with existing scientific models/datasets, use of curated benchmarks where available</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty complicates V&V — absence of ground truth or benchmark coverage indicates novelty; V&V recommends expert scrutiny and domain-specific tests to measure novelty impact</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (V&V does not generate); the review notes that without sufficient domain data, V&V cannot reliably certify novel outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>effective when validated against ground-truth or comprehensive benchmarks; degrades or is impossible for completely novel scenarios due to lack of reference data and sparse curated scientific imaging benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>strong negative effect — novelty reduces the ability of standard V&V to detect incorrect or hallucinated outputs, increasing false acceptance risk</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>V&V is weaker than generation for novel, out-of-distribution outputs; generation may appear plausible while V&V cannot confirm correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>V&V performance drops for out-of-distribution outputs; benchmarks and metrics become less informative</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>V&V reliability and calibration depend on benchmark coverage and domain-specific tests; calibration is poor when confronted with novel data lacking ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>often significantly higher than generation — experimental or expert validation and data curation are time- and resource-intensive</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>develop curated scientific benchmarks, assemble broader and more representative training datasets, enforce domain constraints, implement human-in-the-loop workflows, uncertainty quantification, and formalized V&V pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper emphasizes V&V as essential but currently limited: standard metrics and benchmarks work for familiar domains but fail for novel scientific scenarios, creating a substantial validation bottleneck that requires curated datasets, domain expertise, and new protocols to close.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>High-Resolution Image Synthesis with Latent Diffusion Models <em>(Rating: 2)</em></li>
                <li>Classifier-free diffusion guidance <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Score-based generative modeling through stochastic differential equations <em>(Rating: 2)</em></li>
                <li>Conditional diffusion models for semantic 3d brain mri synthesis <em>(Rating: 2)</em></li>
                <li>Diffusion models beat gans on image synthesis <em>(Rating: 1)</em></li>
                <li>Zero-shot text-to-image generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2151",
    "paper_id": "paper-276725462",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "Diffusion Models",
            "name_full": "Denoising Diffusion Probabilistic Models (DDPM) and related diffusion frameworks",
            "brief_description": "A family of generative neural networks that produce images by learning a reverse denoising process from noise to data through many small steps; noted in the paper as state-of-the-art for high-fidelity, diverse, and controllable image synthesis and the dominant approach for text-to-image and image-to-image tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Diffusion Models (DDPM / score-based / SDE frameworks)",
            "system_type": "neural network / probabilistic generative model (score-based / SDE)",
            "domain": "image synthesis; scientific imaging (microscopy, MRI, materials, medical)",
            "generation_capability": "generates high-quality synthetic images from noise, conditioned on text or images (text-to-image, image-to-image); can be guided toward classes or conditions",
            "validation_method": "comparison to benchmark datasets when available; quantitative image-quality metrics (SSIM, PSNR) and diversity/fidelity metrics; expert (domain scientist) qualitative review; cross-validation on held-out data; comparison to physical/scientific models where possible",
            "novelty_measure": "implicitly measured by distance from training data / out-of-distribution status; novelty assessed qualitatively by absence of matching ground-truth, expert judgement, and by lack of coverage in curated benchmarks",
            "generation_performance": "qualitatively described as very high image quality, high diversity and controllability for in-distribution tasks; however, when generating images of phenomena not present in training data, models are prone to hallucination and can produce physically/biologically impossible outputs",
            "validation_performance": "validation is effective for familiar, well-represented domains using SSIM/PSNR and benchmarks, and via expert review; validation becomes unreliable or impossible for truly novel scenarios due to lack of ground truth and sparse curated scientific datasets",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "validation reliability decreases as novelty increases — for out-of-distribution generations there may be no experimental ground truth, causing higher risk of undetected hallucinations",
            "generation_validation_asymmetry": "yes — generation can produce plausible-looking but scientifically incorrect outputs more easily than validation can definitively detect them, especially for novel or rare phenomena",
            "out_of_distribution_performance": "degrades relative to in-distribution; models may hallucinate physically impossible features when extrapolating beyond training data",
            "calibration_quality": "not quantified in paper; authors note 'hallucination' risk and imply confidence/uncertainty estimates and calibration degrade on novel outputs",
            "validation_computational_cost": "qualitatively higher for validation when expert review, experimental comparison, or domain-specific simulation is required compared to the relatively fast image generation process; exact costs not provided",
            "human_validation_required": true,
            "gap_closing_mechanisms": "incorporation of domain knowledge, curated scientific datasets, expert-in-the-loop validation, benchmarks tailored to scientific imagery, transfer learning/fine-tuning on domain data, uncertainty quantification and ensemble checks, classifier guidance or classifier-free guidance with careful conditioning",
            "evidence_type": "supports",
            "key_findings": "Diffusion models achieve the highest image quality and controllability among modern generators for in-distribution tasks, but they are prone to hallucination when extrapolating; validation methods exist for familiar tasks (SSIM/PSNR, benchmarks, expert review) but fail or become infeasible for novel scientific scenarios, producing a generation-validation gap.",
            "uuid": "e2151.0"
        },
        {
            "name_short": "Latent / Stable Diffusion",
            "name_full": "Latent Diffusion Models (Stable Diffusion variant)",
            "brief_description": "An efficient diffusion approach that performs diffusion in a lower-dimensional latent space (often using a pretrained VAE encoder/decoder) enabling high-resolution image generation with conditioning (text/image) and classifier-free guidance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Latent Diffusion / Stable Diffusion",
            "system_type": "neural network / latent-space diffusion model",
            "domain": "text-to-image and image-to-image; applicable to scientific image generation via conditioning/fine-tuning",
            "generation_capability": "generates high-resolution images efficiently in latent space; supports conditioning on text embeddings or image features to steer outputs",
            "validation_method": "same as diffusion models generally: image-quality metrics (SSIM, PSNR), benchmark comparisons, domain expert evaluation, and checking against domain-specific datasets or physical constraints if available",
            "novelty_measure": "measured by presence/absence of similar latents in training set and by domain expert assessment; novelty often inferred from lack of matching ground-truth examples",
            "generation_performance": "reported as high for in-domain, high-resolution synthetic images and efficient compared to pixel-space diffusion; controllability through classifier-free guidance enhances adherence to conditioning; performance on novel scientific phenomena is limited and subject to hallucination",
            "validation_performance": "effective for familiar image types but limited on novel scientific images due to sparse benchmarks and potential mismatch between latent representations and domain-specific features",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "latent-space methods inherit the same novelty-dependent validation degradation; domain-specific fine-tuning can mitigate but not eliminate risks",
            "generation_validation_asymmetry": "yes — strong generative fidelity in-domain but validation remains weak for out-of-distribution scientific cases",
            "out_of_distribution_performance": "worse than in-distribution; liable to produce plausible-looking but scientifically incorrect images",
            "calibration_quality": "not quantified; paper implies calibration/uncertainty worsens for novel outputs",
            "validation_computational_cost": "generation is computationally efficient in latent space; validation (expert review, experimental checks) remains comparatively expensive especially for novel cases",
            "human_validation_required": true,
            "gap_closing_mechanisms": "domain-specific fine-tuning, curated scientific latent datasets, classifier-free guidance tuned with domain embeddings, human-in-the-loop verification",
            "evidence_type": "supports",
            "key_findings": "Latent diffusion (Stable Diffusion) provides computationally efficient, high-quality image generation and improved controllability, but validation for novel scientific outputs is limited by lack of ground truth and dataset mismatch, maintaining a generation-validation gap.",
            "uuid": "e2151.1"
        },
        {
            "name_short": "Classifier-free guidance",
            "name_full": "Classifier-free Guidance (guided diffusion without separate classifier)",
            "brief_description": "A guidance technique for conditional diffusion models that trains a single model to produce both conditional and unconditional outputs and interpolates between them at inference time to control adherence to conditions without a separate classifier.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Classifier-free guidance",
            "system_type": "inference-guidance technique for diffusion neural networks",
            "domain": "conditional image generation (text-to-image, image-to-image); applicable to scientific conditioning",
            "generation_capability": "improves control over generated images to better match conditioning inputs (e.g., text embeddings), reducing some mismatch between intent and output",
            "validation_method": "improves empirical alignment between conditioning and outputs, validated via qualitative inspection, conditioning-consistency metrics, and downstream task performance; does not by itself provide scientific correctness checks",
            "novelty_measure": "not a novelty metric itself; helps enforce conditioning but cannot ensure physical validity for novel scenarios absent proper training data",
            "generation_performance": "improves fidelity to conditioning and often yields better subjective alignment; still subject to hallucination when conditioning refers to phenomena not represented in training data",
            "validation_performance": "helps produce outputs that are easier to validate against the conditioning prompt, but does not replace scientific validation; validation remains limited for novel tasks",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "classifier-free guidance can reduce conditional mismatch in familiar tasks but has limited effect on validation reliability for novel/out-of-distribution content",
            "generation_validation_asymmetry": "partial — guidance reduces generation errors relative to prompts but does not close the gap between plausible-looking generation and scientific validity when ground truth is absent",
            "out_of_distribution_performance": "guidance helps adherence to prompt but cannot prevent physically incorrect extrapolations for out-of-distribution prompts",
            "calibration_quality": "not quantified; guidance may overconfidently force adherence to conditioning even when training support is weak",
            "validation_computational_cost": "no major extra cost at training (single model) and moderate cost at inference for different guidance weights; validation cost unchanged",
            "human_validation_required": true,
            "gap_closing_mechanisms": "combining guidance with domain embeddings, fine-tuning on domain data, and human-in-the-loop checking",
            "evidence_type": "mixed",
            "key_findings": "Classifier-free guidance improves conditional controllability and alignment to prompts, which aids validation for familiar tasks, but it does not solve validation failures for novel scientific content and can give a false sense of correctness.",
            "uuid": "e2151.2"
        },
        {
            "name_short": "CLIP",
            "name_full": "Contrastive Language-Image Pretraining (CLIP)",
            "brief_description": "A multimodal vision-and-language model that embeds images and text into a shared space, used to align generated images with textual descriptions and to guide generation toward prompt-consistent outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CLIP",
            "system_type": "multimodal transformer / contrastive embedding model",
            "domain": "text-image alignment; text-to-image conditioning and evaluation",
            "generation_capability": "not a generator itself; used to guide or evaluate image generation systems to increase fidelity to textual prompts",
            "validation_method": "serves as an automated alignment validator by scoring image-text similarity; used as a proxy metric for prompt adherence and for ranking candidate outputs",
            "novelty_measure": "similarity in embedding space can indicate novelty (low similarity to known pairs), but CLIP does not directly quantify scientific novelty or physical correctness",
            "generation_performance": "when integrated (e.g., DALL-E 2) improves semantic alignment between prompt and generated image for familiar concepts; does not ensure scientific validity for novel phenomena",
            "validation_performance": "useful for checking prompt-image alignment on familiar concepts; unreliable as sole validator for scientific correctness or out-of-distribution content",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "CLIP-based alignment may produce high similarity scores for visually plausible but scientifically incorrect images, especially for novel cases where training data lacks domain specifics",
            "generation_validation_asymmetry": "yes — CLIP helps validate alignment to prompts but cannot validate adherence to underlying scientific principles, creating an asymmetry between perceived and actual correctness",
            "out_of_distribution_performance": "degrades for domain-specific or specialized scientific imagery that differs from CLIP's training distribution",
            "calibration_quality": "not quantified in paper; authors imply limitations for domain-specific calibration",
            "validation_computational_cost": "low to moderate (embedding and similarity computation) compared to experimental validation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "augment CLIP with domain-specific encoders/embeddings or combine with expert review and domain constraints",
            "evidence_type": "supports",
            "key_findings": "CLIP is effective for measuring prompt-image semantic alignment in familiar domains but cannot substitute for scientific validation; reliance on CLIP risks accepting visually plausible yet scientifically invalid outputs, especially for novel phenomena.",
            "uuid": "e2151.3"
        },
        {
            "name_short": "DALL-E family",
            "name_full": "DALL-E, DALL-E 2, DALL-E 3 (OpenAI text-to-image models)",
            "brief_description": "Transformer-based and diffusion-based multimodal models for generating images from textual descriptions; noted in the review as influential milestones in text-to-image synthesis and as employing mechanisms (e.g., CLIP, LLM components) to improve alignment.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DALL-E (1/2/3)",
            "system_type": "multimodal transformer / diffusion pipeline (varies across versions)",
            "domain": "text-to-image generation; general creative and illustrative image synthesis",
            "generation_capability": "generates images from textual prompts, combining disparate concepts into coherent visual outputs; used as a baseline for text-to-image research",
            "validation_method": "evaluated by image quality metrics, human evaluations for prompt adherence, and CLIP-based alignment scores; scientific validation not intrinsic to model",
            "novelty_measure": "novelty inferred from creative combinations distinct from training data and by human judgement; not explicitly quantified in the review",
            "generation_performance": "reported improved text comprehension and image quality across versions; strong for familiar, in-distribution prompts but liable to hallucination or physical impossibilities when extrapolating",
            "validation_performance": "human evaluation and CLIP-based metrics effective for assessing prompt adherence in familiar domains; these do not guarantee scientific correctness for novel cases",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "human and CLIP-based validation less reliable for novel scientific prompts lacking ground truth, increasing risk of undetected errors",
            "generation_validation_asymmetry": "yes — visually coherent generations can outpace the ability of standard validation methods (CLIP, human subjective checks) to detect scientific inaccuracies",
            "out_of_distribution_performance": "degrades and may produce implausible or impossible scientific images for out-of-distribution prompts",
            "calibration_quality": "not quantified; authors highlight hallucination concerns indicating calibration issues for novel prompts",
            "validation_computational_cost": "generation cost is moderate to high; validation (expert review, experimental tests) is significantly more expensive and time-consuming",
            "human_validation_required": true,
            "gap_closing_mechanisms": "integrating domain-specific models/constraints, curated scientific datasets, expert-in-the-loop checks, and formalized V&V protocols",
            "evidence_type": "supports",
            "key_findings": "DALL-E models advanced text-to-image quality and alignment but the review warns they can hallucinate and misrepresent scientific principles when extrapolating; common validation (CLIP, human ratings) is insufficient for novel scientific outputs.",
            "uuid": "e2151.4"
        },
        {
            "name_short": "GANs",
            "name_full": "Generative Adversarial Networks (GANs) and variants (CGAN, DCGAN, etc.)",
            "brief_description": "Adversarially trained generator-discriminator models that synthesize images by learning to fool a discriminator; historically significant for sharp images but subject to instability and mode collapse.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GANs (Vanilla GAN, CGAN, DCGAN, StackGAN, AttnGAN)",
            "system_type": "neural network / adversarial generative model",
            "domain": "image synthesis; text-to-image (via conditional GAN variants); some niche scientific image tasks (e.g., upscaling, style transfer)",
            "generation_capability": "generates sharp, diverse images; conditional variants can accept text or images to guide generation",
            "validation_method": "evaluated via image quality and diversity metrics, human assessment, and task-specific benchmarks; less stable training requires careful validation across runs",
            "novelty_measure": "novelty assessed by diversity metrics and distance from training modes; mode collapse can reduce novelty",
            "generation_performance": "high image sharpness for many tasks but training instability (mode collapse) can limit diversity and robustness; performance on specialized scientific imagery is limited without large domain datasets",
            "validation_performance": "validation via benchmarks and metrics works for familiar tasks but GAN instability and mode collapse complicate consistent validation; for novel scientific outputs validation remains insufficient",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "mode collapse and instability make validation of novelty difficult; out-of-distribution or rare phenomena can be missed or misrepresented",
            "generation_validation_asymmetry": "yes — GANs can produce visually convincing results while validation may not detect missing modes or incorrect scientific detail, particularly for novel cases",
            "out_of_distribution_performance": "poor relative to diffusion models; tends to overfit or collapse rather than faithfully extrapolate",
            "calibration_quality": "not quantified; GAN outputs do not provide intrinsic uncertainty estimates and thus are poorly calibrated for novelty",
            "validation_computational_cost": "training and repeated runs to ensure stability are costly; experimental validation for scientific claims remains more expensive",
            "human_validation_required": true,
            "gap_closing_mechanisms": "conditional architectures, attention mechanisms (AttnGAN), hierarchical refinements (StackGAN), and incorporation of domain constraints; still require expert validation",
            "evidence_type": "supports",
            "key_findings": "GANs can produce sharp images but suffer from training instability and mode collapse; these characteristics make generating and validating novel scientific images challenging, reinforcing the generation-validation gap.",
            "uuid": "e2151.5"
        },
        {
            "name_short": "VAEs",
            "name_full": "Variational Autoencoders (VAEs)",
            "brief_description": "Latent variable generative models that encode data into a continuous latent space and decode samples, trading some quality for training stability and explicit probabilistic modeling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Variational Autoencoders (VAEs)",
            "system_type": "probabilistic latent-variable neural network",
            "domain": "image synthesis, latent representations for diffusion models, potential use in scientific image generation pipelines",
            "generation_capability": "can generate images by sampling latent space; often used as encoders/decoders in latent diffusion frameworks",
            "validation_method": "likelihood-based objectives (ELBO), reconstruction metrics, SSIM/PSNR, and expert evaluation for scientific imagery",
            "novelty_measure": "novelty reflected by distance in latent space from known examples; VAEs can struggle to generate high-variability novel outputs",
            "generation_performance": "moderate image quality (often blurrier than GANs or diffusion) but stable training; limited diversity and controllability compared to diffusion/GANs",
            "validation_performance": "validation via likelihood and reconstruction metrics is straightforward for familiar data but does not ensure scientific correctness for novel outputs",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "VAEs' poorer fidelity and lower diversity make generating and validating novel, high-detail scientific images more difficult",
            "generation_validation_asymmetry": "present — VAEs' conservative outputs can be easier to validate visually but may lack the detail needed for scientific utility; validation still requires domain checks",
            "out_of_distribution_performance": "limited; tends to produce blurred, less detailed extrapolations",
            "calibration_quality": "not quantified; VAEs provide probabilistic outputs but calibration for novel scientific cases is not addressed",
            "validation_computational_cost": "training stable and efficient; validation (domain checks) remains externally costly",
            "human_validation_required": true,
            "gap_closing_mechanisms": "use as latent encoders in LDMs, combine with downstream models or fine-tuning on domain data, incorporate domain constraints",
            "evidence_type": "supports",
            "key_findings": "VAEs offer stable training and probabilistic modeling but lower fidelity and diversity, limiting their usefulness for generating detailed novel scientific images and leaving validation challenges largely unresolved.",
            "uuid": "e2151.6"
        },
        {
            "name_short": "Verification & Validation (V&V)",
            "name_full": "Verification and Validation practices for generative AI outputs",
            "brief_description": "Collection of methods and procedures described in the review for assessing correctness and reliability of generative-model outputs, especially for scientific imagery, including unit testing, benchmark evaluation, cross-validation, expert review, and domain-specific comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Verification & Validation (V&V) protocols",
            "system_type": "methodological framework (testing and evaluation pipeline)",
            "domain": "applies across generative AI for scientific imaging and other scientific-data-generating systems",
            "generation_capability": "not a generator — V&V is the validation framework applied to outputs from generators",
            "validation_method": "unit testing, benchmark dataset evaluation, cross-validation, quantitative image metrics (SSIM, PSNR), expert domain-scientist qualitative evaluation, comparison with existing scientific models/datasets, use of curated benchmarks where available",
            "novelty_measure": "novelty complicates V&V — absence of ground truth or benchmark coverage indicates novelty; V&V recommends expert scrutiny and domain-specific tests to measure novelty impact",
            "generation_performance": "N/A (V&V does not generate); the review notes that without sufficient domain data, V&V cannot reliably certify novel outputs",
            "validation_performance": "effective when validated against ground-truth or comprehensive benchmarks; degrades or is impossible for completely novel scenarios due to lack of reference data and sparse curated scientific imaging benchmarks",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "strong negative effect — novelty reduces the ability of standard V&V to detect incorrect or hallucinated outputs, increasing false acceptance risk",
            "generation_validation_asymmetry": "V&V is weaker than generation for novel, out-of-distribution outputs; generation may appear plausible while V&V cannot confirm correctness",
            "out_of_distribution_performance": "V&V performance drops for out-of-distribution outputs; benchmarks and metrics become less informative",
            "calibration_quality": "V&V reliability and calibration depend on benchmark coverage and domain-specific tests; calibration is poor when confronted with novel data lacking ground truth",
            "validation_computational_cost": "often significantly higher than generation — experimental or expert validation and data curation are time- and resource-intensive",
            "human_validation_required": true,
            "gap_closing_mechanisms": "develop curated scientific benchmarks, assemble broader and more representative training datasets, enforce domain constraints, implement human-in-the-loop workflows, uncertainty quantification, and formalized V&V pipelines",
            "evidence_type": "supports",
            "key_findings": "The paper emphasizes V&V as essential but currently limited: standard metrics and benchmarks work for familiar domains but fail for novel scientific scenarios, creating a substantial validation bottleneck that requires curated datasets, domain expertise, and new protocols to close.",
            "uuid": "e2151.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "High-Resolution Image Synthesis with Latent Diffusion Models",
            "rating": 2
        },
        {
            "paper_title": "Classifier-free diffusion guidance",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Score-based generative modeling through stochastic differential equations",
            "rating": 2
        },
        {
            "paper_title": "Conditional diffusion models for semantic 3d brain mri synthesis",
            "rating": 2
        },
        {
            "paper_title": "Diffusion models beat gans on image synthesis",
            "rating": 1
        },
        {
            "paper_title": "Zero-shot text-to-image generation",
            "rating": 1
        }
    ],
    "cost": 0.0173895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A REVIEW ON GENERATIVE AI FOR TEXT-TO-IMAGE AND IMAGE-TO-IMAGE GENERATION AND IMPLICATIONS TO SCIENTIFIC IMAGES
March 14, 2025</p>
<p>Zineb Sordo zsordo@lbl.gov 
Eric Chagnon echagnon@lbl.gov 
Daniela Ushizima dushizima@lbl.gov </p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>Applied Math and Computational Research Lawrence Berkeley National Laboratory Berkeley
94720CA</p>
<p>A REVIEW ON GENERATIVE AI FOR TEXT-TO-IMAGE AND IMAGE-TO-IMAGE GENERATION AND IMPLICATIONS TO SCIENTIFIC IMAGES
March 14, 202593279B86D04D1E25EB8DE2DDC02FA5E2arXiv:2502.21151v2[cs.CV]
This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI.We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models.For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding.Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.</p>
<p>Introduction</p>
<p>Generative AI (genAI) has emerged as a powerful tool with the ability to create novel digital content, including images, text, and music [5].However, using generative AI to create scientific images of phenomena unseen by the model continues to be challenging, and prone to hallucination [43] and misrepresentation of scientific principles.If the model extrapolates beyond its training data, it can generate images that, while visually plausible, are physically or biologically impossible [37].This can lead to the propagation of inaccurate scientific concepts and hinder genuine discovery [19,20].This paper overviews the major milestones in the last few years, then describes how Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs) and Diffusion Models have revolutionized these areas.Finally, we delineate potential avenues for verification and validation.</p>
<p>Overall, this paper focuses on two key subdomains: text-to-image and image-to-image generation.</p>
<p>This review aims to:</p>
<p>• Analyze their applications in text-to-image and image-to-image generation.</p>
<p>• Compare and contrast the strengths and weaknesses of these approaches for scientific data generation.</p>
<p>• Discuss the challenges and future directions of research in this field.</p>
<p>Background</p>
<p>This section provides an overview of the key advancements in text-to-image and image-to-image generation technologies, particularly considering the technologies from major tech companies, such as Google, Meta, Microsoft and OpenAI.We highlight significant software releases and the underlying algorithms that have shaped the landscape of generative AI from 2021 to 2024.</p>
<p>In 2021, OpenAI introduced DALL-E, a groundbreaking model that utilized a variant of the GPT-3 architecture, which is a large language model (LLM), to generate images from textual descriptions.DALL-E employed a transformer-based architecture, leveraging the principles of attention mechanisms to understand and synthesize complex relationships between text and visual elements.The model was trained on a diverse dataset of text-image pairs, enabling it to create novel images that often combined disparate concepts in coherent and imaginative ways.This marked a significant advancement in generative models, setting the stage for future developments in text-to-image synthesis [30,25].</p>
<p>Figure 1: Major highlights of language and multimodal models, with less focus on text-to-image generation models [38].</p>
<p>The year 2022 saw significant advancements in text-to-image generation technologies.Google introduced Imagen, a diffusion-based model that generates high-quality images from textual prompts.Imagen utilized a two-step process involving a denoising diffusion probabilistic model (DDPM), which iteratively refined a random noise image into a coherent visual representation based on the input text.While Imagen did not directly employ Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs) [8], its diffusion approach presented an alternative to these traditional generative frameworks.Meanwhile, OpenAI released DALL-E 2, which improved upon its predecessor by incorporating CLIP (Contrastive Language-Image Pretraining) [25] to better align the generated images with textual descriptions [28].CLIP itself is a multi-modal vision and language model that understands and relates textual and visual information.CLIP has since become the benchmark for text-to-image and multi modal text+image-to-image generation.Meta's Make-A-Scene also emerged, allowing users to have more control over the composition of generated images through scene graphs, enhancing the interactivity of the image creation process [6].Additionally, Microsoft integrated OpenAI's models into its Azure platform, making these advanced capabilities more accessible to developers and businesses.</p>
<p>In 2023, the landscape of generative AI continued to evolve with notable releases and innovations.Google unveiled Imagen Video, extending the capabilities of its diffusion models to generate videos from text prompts, thus introducing temporal coherence and movement to the generative process [10].OpenAI launched DALL-E 3, which featured enhanced text comprehension and image generation capabilities, further refining the alignment between textual input and visual output through improved training techniques and larger datasets [1].This model continued to leverage the principles of LLMs to enhance its generative abilities.Anthropic's Claude AI also began to incorporate multimodal functionalities, allowing for a richer interaction between text and images, while not strictly focused on image generation, it illustrated the growing trend of integrating LLMs with visual understanding.Meta's Segment Anything Model (SAM), a zero-shot algorithm, has facilitated advanced image segmentation, providing tools for image-to-image manipulation by enabling users to specify regions of interest within images [16].Finally, Microsoft's Copilot integrated these generative AI technologies into design workflows, allowing users to seamlessly leverage AI-driven image generation and manipulation tools, thus democratizing access to sophisticated design capabilities [36].</p>
<p>In 2024, major tech players release improved technologies for text-to-image and image-to-image generation.Google, with its ongoing development of Imagen and related models, focused on enhanced photorealism and semantic understanding, leveraging diffusion models and potentially incorporating LLMs for improved prompt interpretation.Meta expanded its offerings, building upon its Emu architecture, emphasizing both speed and quality, exploring variations of diffusion models potentially incorporating VAEs for efficient latent space manipulation [17][7] [33].OpenAI continued refining DALL-E, focusing on higher resolution and more consistent image generation, further optimizing its diffusion-based pipeline and potentially integrating LLM-driven refinement steps.Anthropic, while primarily known for LLMs, began exploring visual generation in conjunction with its Claude model, potentially integrating diffusion models with their sophisticated contextual understanding capabilities.Microsoft further solidified its position with updates to its Designer and Image Creator powered by DALL-E 3, focusing on user accessibility and integration within its ecosystem.Underpinning these advancements are diffusion models, which iteratively denoise images from Gaussian noise, often operating within a latent space defined by VAEs for computational efficiency.Some models, though less prevalent in 2024 for high-fidelity image generation, may still utilize GANs for specific tasks like upscaling or style transfer, though diffusion models have become the dominant approach for general text-to-image and image-to-image tasks.The integration of LLMs, especially in prompt understanding and refinement, became a key trend, ensuring generated images more accurately reflect the intent of the user.</p>
<p>3 Key Generative Architectures</p>
<p>Variational Auto-Encoder (VAE)</p>
<p>First introduced in 2013 [15], the Variational Auto-Encoder (VAE) is a type of generative neural network capable of learning a probability distribution over a set of data points without labels.A VAE learns to encode input data into a lower-dimensional latent space and then decode it back to the original space by sampling latents, while ensuring the latent representations follow a known probability distribution.</p>
<p>VAE is categorized as a model with an explicit intractable density function (tractable models allow for explicit likelihood computation) that learns a probability distribution using variational inference and latent variables.Intuitively, latent variables (LV) "explain" the data in a "simpler" way, and more rigorously, LV result from a transformation of the data points into a continuous lower-dimensional space.Mathematically, we define x as a data point that follows a probability distribution p(x) and z, to be a latent variable that follows a probability distribution p(z), then:</p>
<p>• p(x) is the marginal distribution (and goal of the model) • p(z) is the prior distribution • p(x|z) is the likelihood mapping latents z to data points x • p(x, z) = p(x|z) * p(z) is the joint distribution of data points and latent variables • p(z|x) is the posterior distribution that describes z that can be produced by x x ∼ p(z|x) (see Figure 2).To find the parameters of the marginal distribution p(x), we can apply gradient descent which translates to compute the following:
∇logp θ (x) = p θ (z|x)∇ θ logp θ (x, z)dz(1)
The goal of variational inference is to approximate the posterior distribution p(z|x) with an explicit tractable probability distribution and allow its computation as an optimization problem.We can call this distribution the variational posterior q(z|x).During training, the goal is to minimize the Kullback-Leibler (KL) divergence, which expresses the difference between the true posterior and the variational posterior and is given by (with θ being the model parameters):
KL(P ||Q) = ∞ −∞ p(x)log( p(x) q(x) )dx = E x∼p(x) <a href="2">log( p(x) q(x, θ) </a>
To enable backpropagation in the decoder part of the VAE, this method considers a reparametrization trick where instead of just sampling from the latent distribution, they add a random noise to the mean and standard deviation to make gradient computation possible.The final loss function is given by the following equation:
L θ,ϕ (x) = E q ϕ(z|x) [logp ϕ (x|z)] − KL(q ϕ (z|x)||p θ (z))(3)
The first part of the previous equation (also called negative reconstruction error associated to the Decoder part of the VAE) controls how well the model reconstructs x given z of the variational posterior whereas the second part (corresponding the Encoder part of the VAE) controls how close the variational posterior q(z|x) is to the prior p(x) i.e. how well the dimensionality reduction of the Encoder captures the data features within the latent space.</p>
<p>Generative Adversarial Networks (GANs)</p>
<p>The first Generative Adversarial Network (GAN) was introduced in 2014 [9,8] and represents a major advancement in generative learning.GANs are a class of machine learning models that consists of two key components: a generator and a discriminator, where the generator aims to produce synthetic data, and the discriminator attempts to distinguish between the real data and synthetic data (see Figure 4. • The generator G(z) maps random noise z ∼ p(z) (also called latents) to the data distribution and outputs the synthetic image in the shape of a 1D-vector.The stochasticity given by this random sampling will provide a non-deterministic output, which is how the model creates diversity in the generation process.The goal here is to fool the discriminator and minimize log(1 − D(G(z))), which amounts to maximizing the discriminator's mistakes.• The discriminator D(x) takes as input a real and synthetic image (generated by the generator) and outputs the probability that the image corresponds to the real data distribution or not.The goal here is to maximize the loss function or the probability that it correctly classifies real and fake images.</p>
<p>This adversarial process drives both the generator and the discriminator to improve, resulting in high-quality synthetic data.In addition, the fact that the generator is only trained to fool the discriminator makes this Vanilla GAN model unsupervised.</p>
<p>The goal of the GAN is to solve the min-max game or adversarial game between the generator and the discriminator with the following objective function and optimization problem:
min G max D V (G, D) = E x∼p data (x) [logD(x)] + E z∼p f ake (z) <a href="4">log(1 − D(G(z)))</a>
where D(x) is the probability that x is real, G(z) is the generated sample, and thus D(G(z)) is the probability that the generated image given latent z is real.One of the most common limitations of GANs is the so-called mode collapse problem where the generator fails to represent accurately the pixel space of all possible outputs.This issue is common in high-resolution images, where too many fine-scale features must be captured.In that case, the generator gets stuck in a parameter setting with a similar level of noise that can consistently fool the discriminator and only captures a subset of the real data distribution.It then fails to produce diversity in its outputs and collapses to producing only a few types of synthetic samples.</p>
<p>Conditional GAN (CGAN)</p>
<p>As an extension of the Vanilla GAN, the Conditional GAN was introduced in 2014 [21] and uses conditional information (image or text) to guide the generation process.The CGAN performs conditioning generation by feeding information to both the generator and the discriminator.The generator G(z, y) takes as input random noise z, and the conditional embedding y and learns to generate data given this condition, whereas, the discriminator D(x, y) learns to classify real and fake images by checking that the condition y is met.The min-max optimization function becomes:
min G max D V (G, D) = E x∼p data (x) [logD(x|y)] + E z∼p f ake (z) <a href="5">log(1 − D(G(z|y), y))</a>
StackGAN [41] and Attentional GAN (AttnGAN) [40] are influential CGAN architectures that advanced text-to-image generation.StackGAN introduced a hierarchical approach, generating low-resolution images and iteratively refining them to high-resolution outputs.AttnGAN innovated with attention mechanisms, allowing the model to selectively attend to specific words or phrases in the text description when generating corresponding image regions.</p>
<p>Deep Convolutional GAN (DCGAN)</p>
<p>Following the initial development of GANs, various architectures emerged, notably Deep Convolutional Generative Adversarial Networks (DCGANs) introduced by Radford et al. in 2015 [29], which extended the foundational GAN framework.While the Vanilla GAN's architecture contains simple downsampling and upsampling layers with ReLU activations and a Sigmoid activation for the discriminator, this variant of the GAN is made of strided convolution layers, batch norm layers, and LeakyReLU activation functions.This architecture is adapted to small size images such as RGB inputs of shape (3,64,64).</p>
<p>Diffusion Models</p>
<p>Diffusion models, now producing state-of-the-art high-fidelity and diverse images, have evolved from the initial work of Sohl-Dickstein et al. in 2015 [34] to the significantly impactful Denoising Diffusion Probabilistic Models (DDPM) by Ho et al. in 2020 [11].These models differ from the previous generative models as they decompose the image generation process through small denoising steps.In fact, the idea behind diffusion models is that they take an input image x 0 and gradually add Gaussian noise in what is called the forward process.The second part of the network is the reverse process, or sampling process, which consists of removing the noise to obtain new data (see Figure 5).</p>
<p>The forward process in the diffusion network consists of a Markov chain of T steps.Given an input image x 0 sampled from the true data distribution x 0 ∼ q(x 0 ), at each step t &lt; T , Gaussian noise is added to x t−1 , according to a variance schedule β 1 , ..., β t to obtain x t ∼ q(x t |x t−1 ) where the x 1 , ..., x T are latents of the same dimensionality as x 0 :
q(x t |x t−1 ) = N (x t ; µ t = 1 − β t x t−1 , Σ t = β t I) (6)
Where I is the identity matrix and the variances β t can be learned or kept constant as hyperparameters.In the case of the DDPM paper, the authors used a linear schedule increasing from β 1 = 10 −4 to β T = 0.02.The scheduler, however, can be linear, quadratic, cosine [24] etc.The posterior probability can be defined as:
q(x 1:T |x 0 ) = T t=1 q(x t |x t−1 )(7)
And using the reparametrization trick to obtain a tractable closed-form sampling at any timestep, we define α t = 1 − β t , ᾱ = t s=0 α s where ϵ 0 , ..., ϵ t−1 ∼ N (0, I) and finally have:
x t ∼ q(x t |x 0 ) = N (x t ; ᾱt x 0 , (1 − ᾱ)I)(8)
Given that β t is a hyperparameter, it is possible to compute α t and ᾱt for all timesteps.We can therefore sample the noise at any timestep t and get the latent variables x t .</p>
<p>The second part of diffusion models is the reverse process which is also a Markov chain with learned Gaussian transitions starting at p(x T ) = N (x T ; 0, I).The goal of the reverse process is to learn the reverse distribution q(x t−1 |x t ) by approximating it with a parametrized model p θ (where p θ is Gaussian and the mean and variance will be parametrized and learned by a neural network):
p θ (x t−1 |x t ) = N (x t−1 ; µ θ (x t , t), Σ θ (x t , t))(9)p θ (x 0:T ) = p θ (x T ) T t=1 p θ (x t−1 |x t )(10)
To train diffusion models, we optimize the negative log-likelihood of the training data.This optimization, similar to the approach used in Variational Autoencoders (VAEs), is achieved by maximizing the Evidence Lower Bound (ELBO), a tractable approximation.The following expression represents the ELBO after a series of computational steps:
log p(x) ≥ E q(x1|x0) [log p θ (x 0 |x 1 )] − D KL (q(x T |x 0 )∥p(x T )) − T t=2 E q(xt|x0) <a href="11">D KL (q(x t−1 |x t , x 0 )∥p θ (x t−1 |x t ))</a>log p(x) ≥ L 0 − L T − T t=2 L t−1(12)
Where:</p>
<p>• E q(x1|x0) [log p θ (x 0 |x 1 )] is the reconstruction term • D KL (q(x T |x 0 )∥p(x T )) basically shows how close x T is to the Standard Gaussian distribution.</p>
<p>• T t=2 L t−1 = L t represents the difference between the denoising steps p θ (x t−1 |x t ) and the approximated ones q θ (x t−1 |x t , x 0 ).The KL divergence compares p θ (x t−1 |x t ) against forward process posteriors, which are tractable when conditioned on x 0 .</p>
<p>Therefore maximizing the likelihood amounts to learning the denoising steps L t .Based on further calculations, the DDPM paper shows that instead of predicting the mean of the distribution during the training of the reverse process, the model will predict the noise ϵ at each timestep t using the following simplified formula of the denoising term in the ELBO (also the loss function of the reverse process network):
L simple (θ) := E t,x0,ϵ ∥ϵ − ϵ θ ( √ ᾱt x 0 + √ 1 − ᾱt ϵ, t)∥ 2(13)
The model associated to the loss function of the reverse process is a U-Net architecture with residual blocks, group normalization as well as self-attention blocks.The timestep t is concatenated to the input image using a cosine positional embedding into each residual block.This denoising U-Net is trained to predict the noise at each timestep of the process.</p>
<p>Conditional Image Generation with Guided Diffusion Classifier Guidance</p>
<p>Similarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes conditional image generation in the network.In that scenario, the model adds conditioning information y at each diffusion step:
p θ (x 0:T |y) = p θ (x T ) T t=1 p θ (x t−1 |x t , y)(14)
Using Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show that guided diffusion models aim to learn ∇logp θ (x t |y) such that:
∇logp θ (x t |y) = ∇logp θ (x t ) + s.∇logp θ (y|x t )(15)
It was also shown in [34] and [3] that a classifier guidance model defined by f Φ (y|x t , t) can guide the diffusion towards the target class y by training f Φ (y|x t , t) on a noisy image x t to predict class y.To do so, we build a classconditional diffusion model with mean µ(x t |y) and variance Σ θ (x t |y) and perturb the mean by the gradients of logf Φ (y|x t , t) of class y, resulting in:
μ(x t |y) = µ θ (x t |y) + s • Σ θ (x t |y)∇logf Φ (y|x t , t)(16)
Figure 6: Algorithm of classifier guided diffusion.Source: [3] Classifier Free-Guidance</p>
<p>Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers.Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process.In other words, the authors train a conditional diffusion model ϵ θ (x t |y) and an unconditional model ϵ θ (x t |y = 0) as a single neural network as follows:
εθ (x t |y) = ϵ θ (x t |0) + s • (ϵ θ (x t |y) − ϵ θ (x t |0))(17)
This approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings.We will see that many models rely on classifier free-guidance especially when training on multi-modal data.</p>
<p>Score-based generative models</p>
<p>Score-Based Diffusion Models (SBDMs) are a class of diffusion models proposed by [35] that use score functions (gradient of the log probability density function) and Lagevin dynamics (iterative process where we draw samples from a distribution based only on its score function).Like GANs, SBDMs use adversarial training and try to generate images that are indistinguishable from real images.</p>
<p>Instead of learning a probability density p(x), the neural network s θ estimates the score function ∇ x logp(x) directly, and the training objective can be as follows:
E px ∥∇ x log p(x) − s θ (x) − ∥ 2 2 = p(x∥∇ x log p(x) − s θ (x)∥ 2 2 dx(18)
While Langevin dynamics can sample p(x) using the approximated score function, directly estimating ∇ x logp(x is difficult and imprecise.To address this, diffusion models learn score functions at various noise levels, achieved by perturbing the data with multiple scales of Gaussian noise.</p>
<p>So given the data distribution p(x), we perturb it with Gaussian noise N (0, σ 2 i I) where i = 1, 2, . . ., L to obtain a noise-perturbed distribution:
p σi (x) = p(y)N (x; y, σ 2 i I) dy(19)
Then we train a network s θ (x, i), known as the Noise Conditional Score-Based Network (NCSN), to estimate the score function ∇ x log p σi (x) .The training objective is a weighted sum of Fisher divergences for all noise scales:
L i=1 λ(i)E pσ i (x) ∥∇ x log p σi (x) − s θ (x, i)∥ 2 2 (20)
The authors of [35] combine components of NSCNs and DDPMs into one generative model, based on Stochastic Differential Equations (SDE) that does not depend on the data and no trainable parameters.Rather than perturbing data with a finite set of noise distributions, we utilize a continuous range of distributions that evolve over time through a diffusion process {x t } t∈[0,T ] .This process is governed by a predefined SDE.Then, it is possible to generate new samples by reversing this process.The forward process going from an input image x 0 to random noise x T is defined such that:
dx = f (x, t)dt + g(t)dw(21)
Where:</p>
<p>• dw is a Wiener process (random noise),</p>
<p>• f (x, t) is the drift term and a vector valued function</p>
<p>• g(t) is the diffusion term and a scalar function After adding noise to the original data distribution for enough time steps, the perturbed distribution becomes close to a tractable noise distribution.Then it is possible to generate new samples by reversing the diffusion process and computing the reverse SDE given that the SDE was chosen to have a corresponding reverse SDE in closed form (see Figure 7):
dx = f (x, t) − g 2 (t)∇ x log p t (x) dt + g(t)dw(22)
Figure 7: Score-based generative modeling through SDE</p>
<p>Stable Diffusion and Latent diffusion models</p>
<p>Latent diffusion models (LDMs) are yet another innovative extension of diffusion models [32].Instead of applying the diffusion on a high-dimensional input (pixel space), we project the input image into a smaller latent space and apply diffusion with latents as inputs.The authors of [32] propose to use an encoder network g to downsample the input into a latent representation z t = g(x t ) and apply the forward process to z t .Then the reverse process is the same as a standard diffusion process with a U-Net to generate new data that are then upsampled by a decoder network (see Figure 8).Therefore, given an encoder ε (Stable diffusion uses a pre-trained VAE encoder network), then the loss can be formulated as:
L LDM = E ε(x),t,ϵ <a href="23">∥ϵ − ϵ θ (z t , t) 2 ∥</a>
Figure 8: Latent diffusion architecture.Source: [18] Stable diffusion can also be conditioned, in particular, using classifier-free guidance by adding conditional embeddings such as image features or text descriptions using a text encoder (e.g.CLIP's text encoder) to steer the generation process.</p>
<p>Diffusion Transformers (DiT)</p>
<p>One of the most recent diffusion-based models is the Diffusion Transformer (DiT) proposed in [26] which is an architecture that combines the principles of diffusion models and transformer models and that generates high-quality synthetic images.It leverages the iterative denoising process inherent in diffusion models while utilizing the powerful representation learning capabilities of transformers for improved sample generation.The authors in [26] replace the U-Net backbone, in the LDM model, by a neural network called a Transformer [39].Transformers are a class of models based on self-attention mechanisms, and they have been proven to excel in tasks involving sequential data (like language processing).They work by attending to all input tokens at once and using multi-head self-attention to process the input efficiently.Mathematically, the attention mechanism can be formulated as follows:
Attention(Q, K, V ) = softmax QK T √ d k V(24)
Where:</p>
<p>• Q (query), K (key), and V (value) are input representations.</p>
<p>• d k is the dimensionality of the key vectors, and the softmax function normalizes the attention scores.</p>
<p>In the context of a Diffusion Transformer (see Figure 9), the input to the transformer is typically a set of tokens or features (e.g., image patches, sequence tokens), and self-attention helps the model attend to dependencies across all tokens to capture long-range relationships.In the reverse process of the diffusion model, the transformer network is responsible for predicting the noise at each step, conditioned on the noisy data.For example, given the noisy image at time step t, the transformer can model long-range spatial dependencies across the image patches (or sequence tokens) and generate a clean image at the next step:
x t−1 = Transformer(µ θ (x t , t), context)(25)
Where:</p>
<p>• µ θ (x t , t) is the predicted noise (as described in the reverse diffusion equation),</p>
<p>• context could be a conditioning input, such as a text prompt (in the case of text-to-image generation).</p>
<p>Comparative Analysis</p>
<p>We can define a set of metrics to evaluate a model family's general performance in image generation.Image Quality refers to the level of detail in the generated image.A model with high Image Quality strictly adheres to the imposed restrictions placed on it while maintaining a high level of detail, and absence of artifacts.A model with low Image Quality consistently generates images with large amounts of noise and/or artifacts and incoherent features [14].A model's Diversity refers to its range of potential outputs.A model with high diversity can produce a wide spectrum of images while maintaining a constant image quality.A model with low diversity can only generate images in a narrow range with a constant image quality [23].Leaving this narrow range can lead to significant and rapid decreases in image quality.Controllability refers to how easy it is to guide the image generation process with some additional input.For example, if you wanted to generate variations of an image you could condition the model with an input image to help shape the generated image.A highly controllable model can take into account additional user input, understand the underlying features, and apply those features to the generated image.Training stability refers to the model's ability to reliably and smoothly converge over the training process.</p>
<p>Within the scope of generative models for image synthesis, Diffusion Models stand out for their ability to produce the highest quality images, often surpassing GANs, which also generate sharp visuals but may not achieve the same level of detail as diffusion-based approaches.VAEs, on the other hand, tend to yield blurrier images, indicating a trade-off in image fidelity.When it comes to diversity, both GANs and Diffusion Models excel at generating a wide variety of outputs, while VAEs can struggle with high variability, limiting their effectiveness in certain applications.In terms of controllability, Diffusion Models offer the most significant level of control over the generation process, allowing for precise adjustments, whereas GANs provide moderate to high control that can vary based on specific architectural choices.VAEs, however, exhibit limited controllability, making them less suitable for applications requiring fine-tuned image generation.Lastly, in terms of training stability, VAEs and Diffusion Models are generally more stable during the training process, reducing the likelihood of issues, while GANs often face challenges related to instability and mode collapse, which can hinder their performance and diversity [44].Table 1 summarizes aspects about image quality, diversity, controllability and training stability.opportunity to create a more diverse datasets from a few "approved" images, which could be used by researchers to train models [4][27] [22].The challenge with scientific image generation lies in the Controllability or controlling their generation since pre-existent models are typically trained on data dissimilar to specialized imagery like microscopy data.So if you tried to just condition on a single cross-section on a standard GAN or Diffusion Model the results are likely lackluster.Alternatively, training a model from scratch would require a large dataset, which is actually the motivation for using image generation in the first place.Gathering sufficient amounts of data coming from experimental settings is often difficulty, and sometimes impossible, but without the sufficient quantity to reach convergence during training, the models can be useless.Considering the aforementioned strenghts, Diffusion Models are expected to exhibit optima performance in the synthesis of scientific imagery, as they address each of these criteria.</p>
<p>Verification &amp; Validation</p>
<p>Hallucinations and unexpected outcomes are some of the issues associated with GenAI.Other problems include inherent biases within training datasets can skew the generated images, reinforcing existing misconceptions [19] or overlooking important, yet underrepresented, scientific phenomena.Validation becomes exceptionally difficult when dealing with completely novel scenarios, as there may be no existing experimental or observational data for comparison.This lack of ground truth poses risks of generating misleading visualizations that could inadvertently guide research down unproductive paths, therefore only rigorous scrutiny and expert validation could potentially mitigate these risks [20] Verification and validation (V&amp;V) are essential for establishing the reliability and accuracy of AI generative models, and several efforts have focused on creating standardized benchmarks [13], however curated datasets using scientific imaging are either extremely narrow [31] or sparse [42].Verification assesses a model's adherence to specified requirements and its performance under defined conditions.This includes unit testing for component correctness and performance evaluation against benchmark datasets.Cross-validation further examines the predictive performance across data subsets, indicating robustness.Validation determines whether the model accurately reflects real-world phenomena.In scientific imaging, validation involves qualitative expert (domain scientist) evaluations of generated image realism and quantitative metrics such as structural similarity index (SSIM) or peak signal-to-noise ratio (PSNR) for image quality.Incorporating domain-specific knowledge strengthens reliability.For example, in biological or material sciences imaging, comparisons against existing scientific models and datasets ensure generated outputs are both visually and scientifically sound.Through rigorous V&amp;V, researchers can avoid major pitfalls of generative AI models, and potentially model utilization in critical scientific applications.</p>
<p>Conclusion &amp; Future Directions</p>
<p>The future of text-to-image and image-to-image technologies promises significant advancements, with profound implications across diverse fields, notably scientific data analysis.We can anticipate continuous refinements in diffusion models, leading to hyper-realistic image generation coupled with increasingly granular control over specific attributes and detail.AI models will develop a deeper comprehension of contextual relationships, enabling the production of more nuanced and precise visual representations.Additionally, ongoing optimization of algorithms and hardware will yield faster generation times and reduced computational costs, while cloud-based platforms and mobile applications could democratize access to these technologies.A significant trend is the rapid progression of light-weight multimodal models [13,2], with expectations of substantial improvements in quality and coherence, particularly taking advantage of high-performance computer systems.Finally, AI will increasingly personalize image generation, learning individual user preferences to produce highly tailored visual outputs.</p>
<p>The impact of these technologies on scientific data analysis, particularly with scarce image sets from specialized instruments, will be transformative.AI-driven data augmentation promises to enable the generation of synthetic data to supplement limited datasets, enhancing the training of machine learning models for critical tasks like image segmentation and object detection.Moreover, AI will translate abstract scientific data into intuitive visual representations, facilitating the identification of patterns and trends in fields like genomics and materials science.By generating visual representations of potential scenarios, AI will assist scientists in formulating hypotheses and designing experiments, such as simulating molecular interactions or astronomical phenomena.AI can also be employed to identify and rectify errors in scientific images, improving the accuracy and reliability of data analysis.Furthermore, AI will foster increased collaboration by creating easily understandable visual representations of data for diverse scientific audiences.</p>
<p>Despite the immense potential, challenges remain.AI models can inherit biases from training data, leading to inaccurate results, which requires careful attention to dataset representativeness.The "black box" nature of some AI models poses challenges to interpretability, requiring efforts to develop more transparent models for scientific applications.Crucially, validation of AI-generated results against experimental data and established scientific principles is essential, especially when dealing with scarce datasets, to ensure the responsible and effective application of these powerful tools.</p>
<p>Disclosure</p>
<p>This article incorporates text and table generation facilitated by generative artificial intelligence.Although these tools aided in the organization and presentation of information, the authors retain full responsibility for the accuracy and scientific validity of the content.This article is a working in progress and further version will be shared shortly.</p>
<p>Figure 2 :
2
Figure 2: Variational Inference</p>
<p>Figure 3 :
3
Figure 3: VAE Encode -Decoder architecture</p>
<p>Figure 4 :
4
Figure 4: Architecture of the Vanilla GAN</p>
<p>Figure 5 :
5
Figure5: Denoising diffusion probabilistic models (DDPMs).Source:[18]</p>
<p>Figure 9 :
9
Figure 9: Architecture of the Diffusion Transformer</p>
<p>Table 1 :
1
Comparison of VAEs, GANs, and Diffusion Models for Text-to-Image Generation
Model TypeImage QualityDiversityControllabilityTraining StabilityVariationalModerate to High:Moderate: Capable ofModerate: Can con-High: More stableAutoencodersGenerally producesgenerating diverse im-dition on text embed-during training com-(VAEs)images with goodages but may struggledings but lacks fine-pared to GANs, butquality but can bewith high variabilitygrained control overcan suffer from is-blurry due to the lossin complex datasets.image features.sues like posteriorfunction used.collapse.GenerativeHigh: Known forHigh: Capable of pro-Moderate to High:Moderate: TrainingAdversarialgenerating sharp andducing a wide varietyCanimplementcan be unstable andNetworksdetailed images, es-of images, especiallyvariouscondition-sensitive to hyperpa-(GANs)pecially with tech-with diverse traininging methods (e.g.,rameters; mode col-niques like Progres-data.text-to-image) butlapse can occur, lead-sive Growing GANs.may require complexing to reduced diver-architecturesforsity.precise control.DiffusionVery High: AchievesHigh: Generates di-High: Allows forHigh:GenerallyModelsstate-of-the-art imageverse images effec-more explicit controlmore stable thanquality, often surpass-tively, with the poten-over the generationGANs during train-ing GANs and VAEstial for high variabil-process through iter-ing, with well-definedin realism and detail.ity.ative denoising stepstraining objectivesand conditioning.that reduce issueslike mode collapse.
Scientific images can be detailed and high-resolution as many of them are acquired using advanced instruments, e.g., microscopes. In order to generate valuable synthetic images to augment scientific datasets, image quality is expected to be higher than in other domains, such as art. For example, MRI scans of human brains must both be detailed and expressly go through the HIPAA guidelines. Being able to generate synthetic MRI brains scans represent an invaluable
AcknowledgmentsThis work was supported by the US Department of Energy (DOE) Office of Science Advanced Scientific Computing Research (ASCR) and Basic Energy Sciences (BES) under Contract No. DE-AC02-05CH11231 to the Center for Advanced Mathematics for Energy Research Applications (CAMERA) program.It also included partial support from the DOE ASCR-funded project Autonomous Solutions for Computational Research with Immersive Browsing &amp; Exploration (ASCRIBE) and Laboratory Directed Research &amp; Development (LDRD) Program and the project Analytics through Diffusion Transformer Models (ADTM) for Scientific Image and Text.Competing interestsThe authors declare that they have no competing interests.
Improving image generation with better captions. James Betker, Gabriel Goh, Li Jing, Jianfeng Timbrooks, Linjie Wang, Li, Longouyang, Juntangzhuang, Joycelee, Yufeiguo, Wesammanassra, Prafulladhariwal, Caseychu, Aditya Yunxinjiao, Ramesh, 2023</p>
<p>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, arXiv:2401.03382Nvlm: Open frontier-class multimodal llms. 2024arXiv preprint</p>
<p>Diffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Conditional diffusion models for semantic 3d brain mri synthesis. Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao, IEEE Journal of Biomedical and Health Informatics. 2872024</p>
<p>David Foster, Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play. O'Reilly Media. 20232nd edition</p>
<p>Greater creative control for ai image generation. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman, Jul 2022</p>
<p>Emu video: Factorizing text-to-video generation by explicit image conditioning. Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra, 2024</p>
<p>Deep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016MIT Press</p>
<p>Generative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Z Ghahramani, M Welling, C Cortes, N Lawrence, K Q Weinberger, Curran Associates, Inc201427</p>
<p>Imagen video: High definition video generation with diffusion models. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, P Diederik, Ben Kingma, Mohammad Poole, David J Norouzi, Tim Fleet, Salimans, 2022</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>. Jonathan Ho, Tim Salimans, arXiv:2207.125982022Classifier-free diffusion guidance. arXiv preprint</p>
<p>Granite vision: a lightweight, open-source multimodal model for enterprise intelligence. Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Shila Ofekkoifman, Sriram Raghavan, Tanveer Syeda-Mahmood. Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira De Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Peter Staar, Tal Drory, and Rogerio FerisPreprint, 2024. Available at</p>
<p>Analyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2020</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, 2022</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick, arXiv:2304.026432023Segment anything</p>
<p>Emu: Enhancing image generation models using photogenic needles in a haystack. Narine Kokhlikyan, Bargav Jayaraman, Florian Bordes, Chuan Guo, Kamalika Chaudhuri, Sep 2023</p>
<p>Akshay Kulkarni, Adarsha Shivananda, Anoosh Kulkarni, Dilip Gudivada, Diffusion Model and Generative AI for Images. Apress2023</p>
<p>The longtail impact of generative ai on disinformation: Harmonizing dichotomous perspectives. Jason S Lucas, Maung Barani, Maryam Maung, Keegan Tabar, Dongwon Mcbride, Lee, IEEE Intelligent Systems. 3952024</p>
<p>Ai hallucinations: A misnomer worth clarifying. Negar Maleki, Balaji Padmanabhan, Kaushik Dutta, 2024 IEEE Conference on Artificial Intelligence (CAI). 2024</p>
<p>Conditional generative adversarial nets. Mehdi Mirza, Simon Osindero, 2014</p>
<p>A morphology focused diffusion probabilistic model for synthesis of histopathology images. Puria Azadi Moghadam, Sanne Van Dalen, Karina C Martin, Jochen Lennerz, Stephen Yip, Hossein Farahani, Ali Bashashati, 2022</p>
<p>Reliable fidelity and diversity metrics for generative models. Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, Jaejun Yoo, Proceedings of the 37th International Conference on Machine Learning. Hal Daumé, Iii , Aarti Singh, the 37th International Conference on Machine LearningPMLRJul 2020119of Proceedings of Machine Learning Research</p>
<p>Improved denoising diffusion probabilistic models. Alex Nichol, Prafulla Dhariwal, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)PMLR2021139of Proceedings of Machine Learning Research</p>
<p>Openai, Clip, 02/27/2025Connecting text and images. </p>
<p>Scalable diffusion models with transformers. William Peebles, Saining Xie, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)October 2023</p>
<p>Brain imaging generation with latent diffusion models. H L Walter, Petru-Daniel Pinaya, Jessica Tudosiu, Pedro F Da Dafflon, Virginia Costa, Parashkev Fernandez, Sebastien Nachev, M Jorge Ourselin, Cardoso, Anirban Mukhopadhyay, Ilkay Oksuz, Sandy Engelhardt, Dajiang Zhu, and Yixuan Yuan. ChamSpringer Nature Switzerland2022Deep Generative Models</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 2021</p>
<p>Unsupervised representation learning with deep convolutional generative adversarial networks. Alec Radford, Luke Metz, Soumith Chintala, 2016</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, 2021</p>
<p>Cric searchable image database as a public platform for conventional pap smear cytology data. Mariana T Rezende, Raniere Silva, Fagner De, O Bernardo, Alessandra H G Tobias, Paulo H C Oliveira, M Tales, Caio S Machado, Fatima N S Costa, Daniela M Medeiros, Claudia M Ushizima, Andrea G C Carneiro, Bianchi, Nature Scientific Data. 811512021</p>
<p>High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjorn Ommer, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer SocietyJune 2022</p>
<p>Emu edit: Precise image editing via recognition and generation tasks. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, Proceedings of the 32nd International Conference on Machine Learning. Francis Bach, David Blei, the 32nd International Conference on Machine LearningLille, FrancePMLRJul 201537</p>
<p>Score-based generative modeling through stochastic differential equations. Yang Song, Stefano Ermon, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Introducing microsoft 365 copilot -your copilot for work. Jared Spataro, Mar 2023</p>
<p>Ai hallucination: towards a comprehensive classification of distorted information in artificial intelligence-generated content. Yujie Sun, Dongfang Sheng, Zihan Zhou, Yifei Wu, Humanities and Social Sciences Communications. 11112782024</p>
<p>. Alan D Thompson, Life.architect. 2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730</p>
<p>Attngan: Fine-grained text to image generation with attentional generative adversarial networks. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2018</p>
<p>Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas, 2017 IEEE International Conference on Computer Vision (ICCV). 2017</p>
<p>Sdrbench: Scientific data reduction benchmark for lossy compressors. Kai Zhao, Sheng Di, Xin Lian, Sihuan Li, Dingwen Tao, Julie Bessac, Zizhong Chen, Franck Cappello, 2020 IEEE International Conference on Big Data (Big Data). 2020</p>
<p>Larger and more instructable language models become less reliable. Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, José Hernández-Orallo, Nature. 63480322024</p>
<p>High-quality and diverse few-shot image generation via masked discrimination. Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan, IEEE Transactions on Image Processing. 332024</p>            </div>
        </div>

    </div>
</body>
</html>
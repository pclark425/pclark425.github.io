<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6549 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6549</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6549</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276960938</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.10167v1.pdf" target="_blank">"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While effective, these methods require labor-intensive prompt engineering, raising the question of whether reasoning can be induced without reliance on explicit prompts. In this work, we unlock the reasoning capabilities of LLMs without explicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a novel decoding strategy that systematically nudges LLMs to continue reasoning, thereby preventing immature reasoning processes. Specifically, we monitor the model's generation and inject a designated phrase whenever it is likely to conclude its response prematurely, before completing the reasoning process. Our experimental evaluations on diverse reasoning benchmarks demonstrate that our proposed strategy substantially improves LLM reasoning capabilities, highlighting the potential of decoding-based interventions as an alternative to traditional prompting techniques.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6549.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6549.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A test-time decoding intervention that monitors next-token predictions and, when <eos> is among the top-k candidates, injects a short designated phrase (most often "Well") to nudge the model to continue reasoning and avoid premature termination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B, Falcon3-10B-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 7B / 7B / 10B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention / sequential nudge</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (primary: consistent short nudge 'Well'), with optional mixed/pool-based variations</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (MultiArith, GSM8K, StrategyQA, BBH-Disambiguation QA, BBH-Logical Deduction, MATH, AQUA, CSQA, BBH-Date Understanding, BBH-Web of Lies)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Arithmetic (grade-school and more advanced), commonsense reasoning, and logical deduction tasks requiring multi-step chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy decoding) and Zero-shot-CoT (prompting: 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>AID is motivated by observed immature reasoning failure modes (silence, no reasoning, incomplete reasoning). The method mainly uses a single neutral nudge ('Well') which consistently improved accuracy across tasks and models; the paper also explores pooled/randomized conjunction injections (addition/contrast/mix pools) to introduce controlled variability. Authors report AID recovers from ineffective prompts and combines synergistically with prompt-based CoT (often producing larger relative gains). Ablations were run over injection phrase choices and top-k parameter k.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6549.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-MultiArith-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1-8B on MultiArith with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of AID applied to LLaMA-3.1-8B on the MultiArith arithmetic benchmark, showing a large accuracy increase from the greedy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID) applied to zero-shot decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / test-time decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elementary/grade-school arithmetic word problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>50.56</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy decoding) baseline = 15.56%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>35.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Applying AID (injecting 'Well' when <eos> is likely) increased accuracy from 15.56% to 50.56% on MultiArith; further combining with zero-shot-CoT prompting produced an additional boost to 78.33%, indicating that AID complements prompting and that prompting+decoding intervention can be synergistic.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6549.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-GSM8K-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1-8B on GSM8K with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of AID to LLaMA-3.1-8B on the GSM8K grade-school math benchmark, showing large improvement over greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / test-time decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school-level multi-step math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>34.57</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy decoding) baseline = 6.97%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>27.6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>AID raised GSM8K accuracy from a low greedy baseline to substantially higher performance, supporting the claim that immature decoding termination is a major failure mode preventing models from demonstrating latent reasoning abilities. The authors also noted that 32.22% of responses that engaged in proper CoT reasoning yielded correct answers (analysis of latent capability).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6549.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-MultiArith-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-7B on MultiArith with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of AID on Gemma-7B for MultiArith showing modest improvement when used alone and very large gains when combined with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well'), with optional pooled variations</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elementary arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>15.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy decoding) baseline = 10.0%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Standalone AID produced a modest absolute gain for Gemma on MultiArith (10.00 → 15.00). However, combining AID with zero-shot-CoT prompting produced a very large jump (reported to 73.33%), indicating model scale/architecture interacts strongly with benefit from prompting+decoding interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6549.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-MultiArith-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-v0.3 on MultiArith with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AID applied to Mistral-7B-v0.3 increases MultiArith accuracy substantially over greedy decoding and further benefits from prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential test-time decoding</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well') with model-specific k tuning</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Basic arithmetic word problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>27.78</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy decoding) baseline = 16.11%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>11.67</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>AID improved Mistral's MultiArith performance (16.11 → 27.78); combining with zero-shot-CoT brought it to 37.22%. The paper notes model-specific preferences for top-k threshold (k), e.g., Mistral preferred k=5 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6549.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-StrategyQA-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-v0.3 on StrategyQA with Adaptive Injection Decoding (recovery from detrimental prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AID recovers and surpasses performance lost due to zero-shot-CoT prompting on StrategyQA for Mistral-7B-v0.3.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID) combined with zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention + prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Yes/no commonsense questions requiring multi-step strategy-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>36.83</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CoT (prompting) degraded baseline = 16.30% (zero-shot prior was 23.58%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>20.53</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors highlight that zero-shot-CoT prompting sometimes reduced Mistral's accuracy (23.58 → 16.30). AID not only recovered that loss but surpassed previous bests (to 36.83%), demonstrating AID's robustness to ineffective prompt phrasing and its ability to encourage continued, correct reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6549.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Disambig-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-v0.3 on BBH-DisambiguationQA with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AID applied to Mistral on Disambiguation QA yields large absolute improvements and recovers from prompt-induced degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID) + zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BBH-Disambiguation QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pronoun resolution and ambiguity detection requiring commonsense/contextual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>32.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CoT baseline = 13.20%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>18.8</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper reports AID can mitigate detrimental effects of naive CoT prompting; here AID produced a large absolute gain relative to the prompted baseline (13.20 → 32.00). Authors interpret this as preventing premature termination and encouraging coherent reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6549.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-Logical-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3.1-8B on BBH-Logical Deduction with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AID applied together with zero-shot-CoT improves logical deduction accuracy for LLaMA-3.1-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID) + zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention + prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BBH-Logical Deduction (Five Objects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Logical deduction puzzles requiring multi-step inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CoT baseline = 16.80%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported improvement (16.80 → 24.80) indicates AID helps sustain and complete chains of thought on logical deduction tasks. Authors attribute gains to preventing premature <eos>-driven termination and encouraging continuation of internal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6549.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-Logical-AID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-7B on BBH-Logical Deduction with Adaptive Injection Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemma-7B's logical deduction accuracy increases when AID is applied in conjunction with prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Injection Decoding (AID) + zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding intervention</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (injection 'Well')</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BBH-Logical Deduction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Logical deduction problems (Five Objects variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CoT baseline = 18.80%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report modest but consistent gains (18.80 → 24.00) for Gemma on logical reasoning when AID is combined with prompting, supporting generality across architectures but also showing variable absolute gains depending on model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6549.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot (Greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline decoding strategy where the model generates token-by-token without explicit reasoning prompts or decoding-time interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to multiple models in the paper (LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–10B in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot (greedy decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / greedy decoding baseline</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous (no enforced reasoning style)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various reasoning tasks used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>AID, Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Serves as the main baseline; exhibits immature reasoning failure modes (silence, no reasoning, incomplete reasoning) frequently. Paper reports many absolute baseline values (e.g., LLaMA MultiArith 15.56%, GSM8K 6.97%, Gemma MultiArith 10.00%, Mistral MultiArith 16.11%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6549.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (e.g., 'Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that appends an instruction (commonly 'Let's think step by step') to induce chain-of-thought style reasoning without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to multiple models (LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–10B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot Chain-of-Thought (Zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (explicit step-by-step instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to induce stepwise reasoning across arithmetic, commonsense, and logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot (greedy) and AID</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper uses Zero-shot-CoT as a primary comparative method. Authors observe it is effective but highly sensitive to phrasing and sometimes degrades performance (example: Mistral StrategyQA 23.58% → 16.30% when prompt applied), and that AID can mitigate or reverse such degradations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6549.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conjunction-Pools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conjunction-based injection pools (ADDITION_POOL, CONTRAST_POOL, MIX_POOL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sets of semantically similar short injection phrases (e.g., 'and','so','therefore' for addition) from which AID can randomly sample to introduce controlled variability in injections.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with models in ablations (e.g., LLaMA-3.1-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (ablation reported)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Conjunction pool injection (randomized intra-pool sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>test-time decoding variation / diversity injection</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (controlled diversity via pool sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith (ablation) and others in phrase study</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to examine effects of slightly varying injection phrasing on arithmetic reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Single fixed injection ('Well')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors constructed ADDITION_POOL, CONTRAST_POOL, and MIX_POOL and randomly sampled injection phrases during inference to test the effect of controlled diversity. Overall, the single neutral phrase 'Well' was the most consistent performer, but pools were useful to study direction-shifting vs continuity effects.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6549.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6549.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Immature-Reasoning-Failures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Immature Reasoning Failure Categories (Silence, No Reasoning, Incomplete Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A taxonomy used in the paper to diagnose LLM failures: Silence (outputs only <eos>), No Reasoning (uninformative/repetitive output), and Incomplete Reasoning (starts chain-of-thought but halts prematurely).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across evaluated models (LLaMA-3.1-8B et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–10B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Diagnostic taxonomy (not a reasoning method)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>analysis / diagnostic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>N/A (describes failure modes rather than styles)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and other datasets (analysis primarily reported on GSM8K LLaMA outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diagnostic classification of incorrect responses to reason about causes of failure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>error share (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report that immature reasoning accounted for a major portion of errors (e.g., Silence accounted for 58.05% of incorrect responses on LLaMA-3.1-8B GSM8K outputs); incomplete reasoning accounted for 5.45% of incorrect GSM8K responses. This motivated AID.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain-of-thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Tree-of-thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought reasoning without prompting <em>(Rating: 2)</em></li>
                <li>Universal self-consistency for large language model generation. <em>(Rating: 1)</em></li>
                <li>Simple test-time scaling. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6549",
    "paper_id": "paper-276960938",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "AID",
            "name_full": "Adaptive Injection Decoding",
            "brief_description": "A test-time decoding intervention that monitors next-token predictions and, when &lt;eos&gt; is among the top-k candidates, injects a short designated phrase (most often \"Well\") to nudge the model to continue reasoning and avoid premature termination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B, Falcon3-10B-Base",
            "model_size": "8B / 7B / 7B / 10B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID)",
            "reasoning_method_type": "test-time decoding intervention / sequential nudge",
            "reasoning_style_diversity": "single style (primary: consistent short nudge 'Well'), with optional mixed/pool-based variations",
            "benchmark_name": "Multiple (MultiArith, GSM8K, StrategyQA, BBH-Disambiguation QA, BBH-Logical Deduction, MATH, AQUA, CSQA, BBH-Date Understanding, BBH-Web of Lies)",
            "task_description": "Arithmetic (grade-school and more advanced), commonsense reasoning, and logical deduction tasks requiring multi-step chain-of-thought.",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot (greedy decoding) and Zero-shot-CoT (prompting: 'Let's think step by step')",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "AID is motivated by observed immature reasoning failure modes (silence, no reasoning, incomplete reasoning). The method mainly uses a single neutral nudge ('Well') which consistently improved accuracy across tasks and models; the paper also explores pooled/randomized conjunction injections (addition/contrast/mix pools) to introduce controlled variability. Authors report AID recovers from ineffective prompts and combines synergistically with prompt-based CoT (often producing larger relative gains). Ablations were run over injection phrase choices and top-k parameter k.",
            "ablation_study_present": true,
            "uuid": "e6549.0",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA-MultiArith-AID",
            "name_full": "LLaMA-3.1-8B on MultiArith with Adaptive Injection Decoding",
            "brief_description": "Evaluation of AID applied to LLaMA-3.1-8B on the MultiArith arithmetic benchmark, showing a large accuracy increase from the greedy baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID) applied to zero-shot decoding",
            "reasoning_method_type": "sequential / test-time decoding",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "MultiArith",
            "task_description": "Elementary/grade-school arithmetic word problems requiring multi-step reasoning.",
            "performance_metric": "accuracy (%)",
            "performance_value": 50.56,
            "comparison_target_method": "Zero-shot (greedy decoding) baseline = 15.56%",
            "performance_difference": 35.0,
            "statistical_significance": null,
            "analysis_notes": "Applying AID (injecting 'Well' when &lt;eos&gt; is likely) increased accuracy from 15.56% to 50.56% on MultiArith; further combining with zero-shot-CoT prompting produced an additional boost to 78.33%, indicating that AID complements prompting and that prompting+decoding intervention can be synergistic.",
            "ablation_study_present": true,
            "uuid": "e6549.1",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA-GSM8K-AID",
            "name_full": "LLaMA-3.1-8B on GSM8K with Adaptive Injection Decoding",
            "brief_description": "Application of AID to LLaMA-3.1-8B on the GSM8K grade-school math benchmark, showing large improvement over greedy decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID)",
            "reasoning_method_type": "sequential / test-time decoding",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "GSM8K",
            "task_description": "Grade-school-level multi-step math word problems.",
            "performance_metric": "accuracy (%)",
            "performance_value": 34.57,
            "comparison_target_method": "Zero-shot (greedy decoding) baseline = 6.97%",
            "performance_difference": 27.6,
            "statistical_significance": null,
            "analysis_notes": "AID raised GSM8K accuracy from a low greedy baseline to substantially higher performance, supporting the claim that immature decoding termination is a major failure mode preventing models from demonstrating latent reasoning abilities. The authors also noted that 32.22% of responses that engaged in proper CoT reasoning yielded correct answers (analysis of latent capability).",
            "ablation_study_present": true,
            "uuid": "e6549.2",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemma-MultiArith-AID",
            "name_full": "Gemma-7B on MultiArith with Adaptive Injection Decoding",
            "brief_description": "Evaluation of AID on Gemma-7B for MultiArith showing modest improvement when used alone and very large gains when combined with CoT prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-7B",
            "model_size": "7B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID)",
            "reasoning_method_type": "test-time decoding intervention",
            "reasoning_style_diversity": "single style (injection 'Well'), with optional pooled variations",
            "benchmark_name": "MultiArith",
            "task_description": "Elementary arithmetic word problems.",
            "performance_metric": "accuracy (%)",
            "performance_value": 15.0,
            "comparison_target_method": "Zero-shot (greedy decoding) baseline = 10.0%",
            "performance_difference": 5.0,
            "statistical_significance": null,
            "analysis_notes": "Standalone AID produced a modest absolute gain for Gemma on MultiArith (10.00 → 15.00). However, combining AID with zero-shot-CoT prompting produced a very large jump (reported to 73.33%), indicating model scale/architecture interacts strongly with benefit from prompting+decoding interventions.",
            "ablation_study_present": true,
            "uuid": "e6549.3",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mistral-MultiArith-AID",
            "name_full": "Mistral-7B-v0.3 on MultiArith with Adaptive Injection Decoding",
            "brief_description": "AID applied to Mistral-7B-v0.3 increases MultiArith accuracy substantially over greedy decoding and further benefits from prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.3",
            "model_size": "7B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID)",
            "reasoning_method_type": "sequential test-time decoding",
            "reasoning_style_diversity": "single style (injection 'Well') with model-specific k tuning",
            "benchmark_name": "MultiArith",
            "task_description": "Basic arithmetic word problems requiring multi-step reasoning.",
            "performance_metric": "accuracy (%)",
            "performance_value": 27.78,
            "comparison_target_method": "Zero-shot (greedy decoding) baseline = 16.11%",
            "performance_difference": 11.67,
            "statistical_significance": null,
            "analysis_notes": "AID improved Mistral's MultiArith performance (16.11 → 27.78); combining with zero-shot-CoT brought it to 37.22%. The paper notes model-specific preferences for top-k threshold (k), e.g., Mistral preferred k=5 in experiments.",
            "ablation_study_present": true,
            "uuid": "e6549.4",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mistral-StrategyQA-AID",
            "name_full": "Mistral-7B-v0.3 on StrategyQA with Adaptive Injection Decoding (recovery from detrimental prompting)",
            "brief_description": "AID recovers and surpasses performance lost due to zero-shot-CoT prompting on StrategyQA for Mistral-7B-v0.3.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.3",
            "model_size": "7B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID) combined with zero-shot-CoT",
            "reasoning_method_type": "test-time decoding intervention + prompting",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "StrategyQA",
            "task_description": "Yes/no commonsense questions requiring multi-step strategy-based reasoning.",
            "performance_metric": "accuracy (%)",
            "performance_value": 36.83,
            "comparison_target_method": "Zero-shot-CoT (prompting) degraded baseline = 16.30% (zero-shot prior was 23.58%)",
            "performance_difference": 20.53,
            "statistical_significance": null,
            "analysis_notes": "Authors highlight that zero-shot-CoT prompting sometimes reduced Mistral's accuracy (23.58 → 16.30). AID not only recovered that loss but surpassed previous bests (to 36.83%), demonstrating AID's robustness to ineffective prompt phrasing and its ability to encourage continued, correct reasoning.",
            "ablation_study_present": true,
            "uuid": "e6549.5",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mistral-Disambig-AID",
            "name_full": "Mistral-7B-v0.3 on BBH-DisambiguationQA with Adaptive Injection Decoding",
            "brief_description": "AID applied to Mistral on Disambiguation QA yields large absolute improvements and recovers from prompt-induced degradation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.3",
            "model_size": "7B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID) + zero-shot-CoT",
            "reasoning_method_type": "test-time decoding intervention",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "BBH-Disambiguation QA",
            "task_description": "Pronoun resolution and ambiguity detection requiring commonsense/contextual reasoning.",
            "performance_metric": "accuracy (%)",
            "performance_value": 32.0,
            "comparison_target_method": "Zero-shot-CoT baseline = 13.20%",
            "performance_difference": 18.8,
            "statistical_significance": null,
            "analysis_notes": "The paper reports AID can mitigate detrimental effects of naive CoT prompting; here AID produced a large absolute gain relative to the prompted baseline (13.20 → 32.00). Authors interpret this as preventing premature termination and encouraging coherent reasoning chains.",
            "ablation_study_present": true,
            "uuid": "e6549.6",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLaMA-Logical-AID",
            "name_full": "LLaMA-3.1-8B on BBH-Logical Deduction with Adaptive Injection Decoding",
            "brief_description": "AID applied together with zero-shot-CoT improves logical deduction accuracy for LLaMA-3.1-8B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B",
            "model_size": "8B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID) + zero-shot-CoT",
            "reasoning_method_type": "test-time decoding intervention + prompting",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "BBH-Logical Deduction (Five Objects)",
            "task_description": "Logical deduction puzzles requiring multi-step inference.",
            "performance_metric": "accuracy (%)",
            "performance_value": 24.8,
            "comparison_target_method": "Zero-shot-CoT baseline = 16.80%",
            "performance_difference": 8.0,
            "statistical_significance": null,
            "analysis_notes": "Reported improvement (16.80 → 24.80) indicates AID helps sustain and complete chains of thought on logical deduction tasks. Authors attribute gains to preventing premature &lt;eos&gt;-driven termination and encouraging continuation of internal reasoning.",
            "ablation_study_present": true,
            "uuid": "e6549.7",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Gemma-Logical-AID",
            "name_full": "Gemma-7B on BBH-Logical Deduction with Adaptive Injection Decoding",
            "brief_description": "Gemma-7B's logical deduction accuracy increases when AID is applied in conjunction with prompting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-7B",
            "model_size": "7B",
            "reasoning_method_name": "Adaptive Injection Decoding (AID) + zero-shot-CoT",
            "reasoning_method_type": "test-time decoding intervention",
            "reasoning_style_diversity": "single style (injection 'Well')",
            "benchmark_name": "BBH-Logical Deduction",
            "task_description": "Logical deduction problems (Five Objects variant).",
            "performance_metric": "accuracy (%)",
            "performance_value": 24.0,
            "comparison_target_method": "Zero-shot-CoT baseline = 18.80%",
            "performance_difference": 5.2,
            "statistical_significance": null,
            "analysis_notes": "Authors report modest but consistent gains (18.80 → 24.00) for Gemma on logical reasoning when AID is combined with prompting, supporting generality across architectures but also showing variable absolute gains depending on model capacity.",
            "ablation_study_present": true,
            "uuid": "e6549.8",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Zero-shot (Greedy)",
            "name_full": "Zero-shot greedy decoding",
            "brief_description": "Baseline decoding strategy where the model generates token-by-token without explicit reasoning prompts or decoding-time interventions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Applied to multiple models in the paper (LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B, etc.)",
            "model_size": "various (7B–10B in experiments)",
            "reasoning_method_name": "Zero-shot (greedy decoding)",
            "reasoning_method_type": "sequential / greedy decoding baseline",
            "reasoning_style_diversity": "homogeneous (no enforced reasoning style)",
            "benchmark_name": "Multiple (as above)",
            "task_description": "Various reasoning tasks used as baselines.",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "AID, Zero-shot-CoT",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Serves as the main baseline; exhibits immature reasoning failure modes (silence, no reasoning, incomplete reasoning) frequently. Paper reports many absolute baseline values (e.g., LLaMA MultiArith 15.56%, GSM8K 6.97%, Gemma MultiArith 10.00%, Mistral MultiArith 16.11%).",
            "ablation_study_present": false,
            "uuid": "e6549.9",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting (e.g., 'Let's think step by step')",
            "brief_description": "A prompting technique that appends an instruction (commonly 'Let's think step by step') to induce chain-of-thought style reasoning without few-shot exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to multiple models (LLaMA-3.1-8B, Mistral-7B-v0.3, Gemma-7B)",
            "model_size": "various (7B–10B)",
            "reasoning_method_name": "Zero-shot Chain-of-Thought (Zero-shot-CoT)",
            "reasoning_method_type": "sequential / prompting",
            "reasoning_style_diversity": "single style (explicit step-by-step instruction)",
            "benchmark_name": "Multiple",
            "task_description": "Used to induce stepwise reasoning across arithmetic, commonsense, and logical tasks.",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot (greedy) and AID",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper uses Zero-shot-CoT as a primary comparative method. Authors observe it is effective but highly sensitive to phrasing and sometimes degrades performance (example: Mistral StrategyQA 23.58% → 16.30% when prompt applied), and that AID can mitigate or reverse such degradations.",
            "ablation_study_present": true,
            "uuid": "e6549.10",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Conjunction-Pools",
            "name_full": "Conjunction-based injection pools (ADDITION_POOL, CONTRAST_POOL, MIX_POOL)",
            "brief_description": "Sets of semantically similar short injection phrases (e.g., 'and','so','therefore' for addition) from which AID can randomly sample to introduce controlled variability in injections.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used with models in ablations (e.g., LLaMA-3.1-8B)",
            "model_size": "8B (ablation reported)",
            "reasoning_method_name": "Conjunction pool injection (randomized intra-pool sampling)",
            "reasoning_method_type": "test-time decoding variation / diversity injection",
            "reasoning_style_diversity": "mixed (controlled diversity via pool sampling)",
            "benchmark_name": "MultiArith (ablation) and others in phrase study",
            "task_description": "Used to examine effects of slightly varying injection phrasing on arithmetic reasoning performance.",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Single fixed injection ('Well')",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors constructed ADDITION_POOL, CONTRAST_POOL, and MIX_POOL and randomly sampled injection phrases during inference to test the effect of controlled diversity. Overall, the single neutral phrase 'Well' was the most consistent performer, but pools were useful to study direction-shifting vs continuity effects.",
            "ablation_study_present": true,
            "uuid": "e6549.11",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Immature-Reasoning-Failures",
            "name_full": "Immature Reasoning Failure Categories (Silence, No Reasoning, Incomplete Reasoning)",
            "brief_description": "A taxonomy used in the paper to diagnose LLM failures: Silence (outputs only &lt;eos&gt;), No Reasoning (uninformative/repetitive output), and Incomplete Reasoning (starts chain-of-thought but halts prematurely).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across evaluated models (LLaMA-3.1-8B et al.)",
            "model_size": "various (7B–10B)",
            "reasoning_method_name": "Diagnostic taxonomy (not a reasoning method)",
            "reasoning_method_type": "analysis / diagnostic",
            "reasoning_style_diversity": "N/A (describes failure modes rather than styles)",
            "benchmark_name": "GSM8K and other datasets (analysis primarily reported on GSM8K LLaMA outputs)",
            "task_description": "Diagnostic classification of incorrect responses to reason about causes of failure.",
            "performance_metric": "error share (%)",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors report that immature reasoning accounted for a major portion of errors (e.g., Silence accounted for 58.05% of incorrect responses on LLaMA-3.1-8B GSM8K outputs); incomplete reasoning accounted for 5.45% of incorrect GSM8K responses. This motivated AID.",
            "ablation_study_present": false,
            "uuid": "e6549.12",
            "source_info": {
                "paper_title": "\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection Decoding",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain-of-thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chainofthought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree-of-thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "treeofthoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Chain-of-thought reasoning without prompting",
            "rating": 2,
            "sanitized_title": "chainofthought_reasoning_without_prompting"
        },
        {
            "paper_title": "Universal self-consistency for large language model generation.",
            "rating": 1,
            "sanitized_title": "universal_selfconsistency_for_large_language_model_generation"
        },
        {
            "paper_title": "Simple test-time scaling.",
            "rating": 1,
            "sanitized_title": "simple_testtime_scaling"
        }
    ],
    "cost": 0.020650250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding
18 Mar 2025</p>
<p>Hyunbin Jin hyunbin.jin@snu.ac.kr 
Graduate School of Data Science
Seoul National University</p>
<p>Je Won 
Graduate School of Data Science
Seoul National University</p>
<p>Seunghyun Bae sh.bae@snu.ac.kr 
Graduate School of Data Science
Seoul National University</p>
<p>Taesup Kim taesup.kim@snu.ac.kr 
Graduate School of Data Science
Seoul National University</p>
<p>"Well, Keep Thinking": Enhancing LLM Reasoning with Adaptive Injection Decoding
18 Mar 2025D36C676C0EDBEB2FB32B54E80F965F32arXiv:2503.10167v2[cs.CL]
Large language models (LLMs) exhibit strong reasoning abilities, often attributed to few-shot or zero-shot chain-of-thought (CoT) prompting.While effective, these methods require labor-intensive prompt engineering, raising the question of whether reasoning can be induced without reliance on explicit prompts.In this work, we unlock the reasoning capabilities of LLMs without explicit prompting.Inspired by zero-shot CoT and CoT-decoding, we propose a novel decoding strategy that systematically nudges LLMs to continue reasoning, thereby preventing immature reasoning processes.Specifically, we monitor the model's generation and inject a designated phrase whenever it is likely to conclude its response prematurely, before completing the reasoning process.Our experimental evaluations on diverse reasoning benchmarks demonstrate that our proposed strategy substantially improves LLM reasoning capabilities, highlighting the potential of decoding-based interventions as an alternative to traditional prompting techniques.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable performance across a wide range of tasks, including question-answering, code generation, and machine translation.Scaling up model size has led to improvements in fluency, generalization, and sample efficiency (Wei et al., 2022b).However, despite these advancements, LLMs continue to struggle with complex reasoning tasks such as arithmetic, commonsense reasoning, and multi-step logical inference.To address these challenges, researchers have primarily relied on prompt engineering techniques, such as fewshot Chain-of-Thought (CoT) prompting and zeroshot CoT (Brown et al., 2020;Besta et al., 2023; Figure 1: Overview of our proposed method, Adaptive Injection Decoding, with injecting a designated phrase "Well" whenever the model is likely to conclude its response immaturely with a <eos> token.Madaan et al., 2023;Wei et al., 2022a;Yao et al., 2023).These approaches guide LLMs by providing structured reasoning examples or explicit step-bystep instructions, significantly improving performance on various reasoning benchmarks.However, they exhibit key limitations in that few-shot prompting requires human-crafted exemplars, while zeroshot prompting is highly sensitive to prompt phrasing.More recently, some studies have explored CoT reasoning without prompts by modifying the decoding strategy (Wang and Zhou, 2023;Wang et al., 2023a).Additionally, a test-time scaling method has been introduced, which intervenes during inference to control test-time compute (Muennighoff et al., 2025).</p>
<p>Despite various efforts to enhance LLM reasoning, existing approaches have not directly addressed the fundamental causes of their reasoning failures.Through our analysis, we categorize these failures into three primary types, highlighting the need for a more targeted intervention.First, models terminate generation without producing a response to given questions, returning only the <eos> token, a phenomenon we refer to as silence.Second, models often produce responses that lack actual logical reasoning, responding with uninformative outputs or irrelevant patterns, such as numeric sequences or question repetition, a failure type we call no reasoning.Finally, in some cases, models initiate reasoning but halt prematurely, leading to incomplete or logically inconsistent answers, which we refer to as incomplete reasoning.These failure types suggest that LLMs do not inherently lack reasoning ability, rather their reasoning processes are disrupted or prematurely terminated during decoding.</p>
<p>To address these reasoning failures, we posed a fundamental question: "How do humans think effectively when solving complex problems?What strategies help people avoid reasoning failures, and how do they enrich their thought process to reach accurate conclusions?".When solving complex problems, humans rarely produce an immediate answer.Instead, they pause, reflect, and often restart their thought process before reaching a final conclusion.When encountering difficulties, people frequently engage in self-prompting mechanisms, such as asking themselves guiding questions, articulating partial thoughts, or seeking alternative perspectives to expand their reasoning.These cognitive strategies ensure that reasoning continues rather than stopping prematurely, allowing for deeper and more thorough problem-solving.</p>
<p>Inspired by this cognitive process, we propose a novel test-time intervention technique based on an adaptive decoding strategy.Rather than fully relying on a greedy decoding strategy, our method dynamically intervenes with a designated injection phrase during inference when the model's reasoning process is on the verge of prematurely terminating or drifting away from logical reasoning.</p>
<p>Similar to how humans engage in self-reflection to sustain their reasoning process, our adaptive injection decoding strategy serves as an implicit signal that prevents premature termination and encourages adequate reasoning.Our contributions are as follows:</p>
<p>• We identify the immature reasoning as a major limitation in LLMs and systematically classify common reasoning failures.</p>
<p>• We propose a novel test-time intervention that extends reasoning dynamically with an adaptive injection decoding strategy, ensuring logical continuity.</p>
<p>• Our approach can significantly improve reasoning performance across reasoning benchmarks and is computationally efficient, making it suitable for practical real-world applications.</p>
<p>Observations: Immature Reasoning</p>
<p>We conducted an experiment using LLaMA 3.1-8B on the GSM8K dataset to assess the reasoning performance of LLMs and identify the underlying factors contributing to their suboptimal performance.Our analysis highlights several challenges affecting the model's reasoning abilities while also revealing insights into its latent potential.The following sections summarize key observations, with specific examples provided in Appendix D. We identify Immature Reasoning as a major factor behind the poor reasoning performance of LLMs.Immature reasoning refers to instances where the model prematurely halts or drifting away (Observation 1) Silence The model fails to generate any response despite receiving an input question.This typically occurs when the model gives up on reasoning and refrains from attempting an answer.In our evaluation of LLaMA x ← x+ next_token 20:</p>
<p>r ← r+ next_token 21: end for 22: return r on GSM8k, 32.22% of responses that engaged in proper CoT reasoning yielded correct answers.This suggests that LLMs are not inherently deficient in problem-solving capabilities.Instead, immature reasoning disrupts the reasoning process, preventing models from fully realizing their potential.</p>
<p>Adaptive Injection Decoding</p>
<p>To mitigate immature reasoning, we introduce a novel decoding strategy.Our approach, Adaptive Injection Decoding (AID), dynamically intervenes in such cases by injecting a predefined injection phrase (e.g., "Well") as the next token whenever the probability of generating the <eos> token meets a specified criterion.This method is designed to prevent the model from prematurely concluding its reasoning process or deviating logical reasoning.</p>
<p>Given an input query x as an input prompt, the model f generates a sequence of response tokens r 1 , r 2 , . . ., r t through greedy decoding.At each decoding step t, if the <eos> token ranks among the top-k next token predictions, it is replaced with injection phrase p.The updated response r 1 , r 2 , . . ., r t , p is then fed back into the model, ensuring continuity of reasoning.The algorithmic details of our method are provided in Algorithm 1.</p>
<p>To properly prevent premature termination and encourage reasoning, we rigorously evaluated various injection phrases, including single words, con-junctions, full sentences, and machine language expressions.Among these, "Well" consistently outperformed other alternatives, as shown in Table 2, and was selected for all subsequent experiments.Further details on the experimental analysis are discussed later in Section 4.3.</p>
<p>Additionally, our experiments revealed distinct model-specific tendencies in handling the <eos> token, particularly in how early and strongly it is considered during generation.LLaMA-3.1-8Btends to terminate responses earlier, whereas Mistral-7B-v0.3 and Gemma-7B generate longer outputs.However, despite their delayed termination, Mistral-7B-v0.3 and Gemma-7B consistently ranked <eos> among the top candidates from the early stages of generation.This suggests that while they had a higher potential for early termination, alternative tokens were ultimately selected.Building on this observation, we explored replacing injection phrases when <eos> appeared in the top-k candidates and further investigated optimal top-k values for each model in subsequent experiments (see Section 4.3 for details).</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Models We mainly investigate LLMs, including LLaMA-3.1-8B,Mistral-7B-v0.3, and Gemma-7B (Touvron et al., 2023;Siino, 2024;Mesnard et al., 2024).</p>
<p>Tasks and Datasets We evaluate our proposed method across a variety of reasoning benchmarks, including tasks such as arithmetic reasoning, commonsense reasoning and logical reasoning.(a) Arithmetic Reasoning: MultiArith (Roy and Roth, 2015) includes elementary school arithmetic questions.GSM8K (Cobbe et al., 2021) consists of grade-school-level math word problems requiring multi-step reasoning.(b) Commonsense Reasoning: StrategyQA (Geva et al., 2021) includes yes/no questions that require multi-step, strategy-based reasoning.BBH-Disambiguation QA (Suzgun et al., 2023) consists of questions on pronoun resolution and ambiguity detection.(c) Logical Reasoning: BBH-Logical Deduction (Five Objects) (Suzgun et al., 2023) includes questions involving logical deduction.</p>
<p>Baselines We compare our method with the following baselines.(a) Zero-shot (i.e., Greedy Decoding): The model generates responses without any intervention, guidance, or modification.(b) Zeroshot-CoT (Prompting): An initial prompt is added to the input to facilitate CoT reasoning.Specifically, we use the prompt "Let's think step by step" to guide progressive problem-solving.</p>
<p>LLM-based Evaluation</p>
<p>To assess the accuracy of the generated responses, we utilize o1-mini via OpenAI's chat completion API (version 2024-09-12) (OpenAI, 2024).The evaluation prompt template, including grading criteria and methodology, is provided in Appendix C. We also compare o1-mini and gpt-4o-mini with human evaluations across multiple datasets using LLaMA-3.1-8B(Table 22), where o1-mini shows markedly closer alignment to human judgment.Further details are in Appendix C.</p>
<p>Results</p>
<p>Table 1 presents the performance of our method applied to both zero-shot and zero-shot-CoT baselines.Our approach consistently improves performance across all three models, enhancing results on a wide range of tasks, including arithmetic, commonsense, and logical reasoning.These improvements are reflected in the Avg column of Table 1.</p>
<p>Arithmetic Reasoning</p>
<p>In MultiArith, each model achieves at least a 150% increase in accuracy simply by injecting our proposed phrase "Well" during inference, even without using zeroshot-CoT prompting.For instance, in LLaMA-3.1-8B,our decoding strategy increases accuracy from 15.56% to 50.56%, with an additional prompt (e.g., "Let's think step by step") further improving performance to 78.33%.A similar trend is observed in Gemma-7B, where accuracy rises from 10.00% to 15.00%, and then to 73.33% with the inclusion of the additional prompt.Similarly, Mistral-7B-v0.3 demonstrates an improvement from 16.11% to 27.78%, ultimately achieving an accuracy of 37.22%.</p>
<p>Commonsense and Logical Reasoning Along with arithmetic reasoning, we also observe significant improvements in commonsense and logical reasoning tasks.For instance, Mistral-7B-v0.3 achieves a +20.53% performance gain (16.30% → 36.83%) in StrategyQA and a +18.80% increase (13.20% → 32.00%) in DisambiguationQA under zero-shot-CoT with our method.Similarly, LLaMA-3.1-8Bimproves from 16.80% to 24.80% in Logical Deduction under zero-shot-CoT, while Gemma-7B increases from 18.80% to 24.00% under the same conditions.These results highlight that even simple intervention during decoding can yield substantial performance gains across a wide range of reasoning tasks.</p>
<p>Enhancing Prompt-based Reasoning Our method integrates seamlessly with prompting techniques, consistently improving performance compared to using prompting alone.As shown in Table 1, the average performance of each model across datasets demonstrates that combining our decoding strategy with an initial prompt results in a significant enhancement over prompting alone.Notably, this improvement is nearly twice as large as that achieved in the zero-shot setting.For example, in MultiArith, where zero-shot prompting already provides some improvement, the application of our method results in additional performance gains.</p>
<p>Recovery from Ineffective Prompting In certain cases, utilizing zero-shot-CoT prompts can reduce accuracy compared to not using any prompts, as demonstrated in Table 1 (e.g., StrategyQA, Dis-ambiguityQA, Logical Deduction).This detrimental effect of zero-shot-CoT prompting has been previously reported.Chakraborty et al. (2023) highlight that language model performance is highly sensitive to prompt phrasing, and manually designed prompts often lead to suboptimal results in zero-shot settings.However, our method effectively mitigates these adverse effects, either restoring or even surpassing performance observed prior to the application of prompts.For instance, in StrategyQA, applying a zero-shot-CoT prompt re-  duces Mistral-7B-v0.3's accuracy from 23.58% to 16.30%.In contrast, injecting our method boosts accuracy to accuracy to 36.83%, effectively counteracting the negative effects of prompting and surpassing the model's previous best performance on this dataset.A similar trend is observed in Logical Deduction, further validating the effectiveness of our approach.</p>
<p>Ablation Study and Analysis</p>
<p>Injection Phrase Analysis Building on our findings in Section 2, we now examine how different candidate phrases can help prevent abrupt termination and encourage more complete reasoning.The goal is to nudge the model toward continuing its chain of thought, rather than concluding too soon.Here, k = 0 represents the case where our method is not applied.</p>
<p>To compare how different injection words or phrases influence performance, we conducted a comprehensive ablation study, as shown in Table 2.Among all tested options, "Well" proved to be the most consistently effective and was selected for our experiments.</p>
<p>We interpret "Well" stands out because it provides a neutral yet effective nudge, encouraging the model to continue reasoning without introducing restrictive biases.Unlike phrases like "I might be wrong."or "Keep reasoning," which explicitly direct the model's confidence, "Well" does not steer the reasoning in a particular direction.It also avoids forcing a specific reasoning trajectory, unlike conjunction words, and does not impose a rigid structured reasoning path, unlike step-indicating words.By sidestepping these constraints, "Well" serves as a subtle yet impactful cue, fostering deeper reasoning while preserving flexibility across various problem types.</p>
<p>Our finding aligns with prior work showing that large language models are highly sensitive to short prompts or cues, including those that subtly convey the need for additional reasoning steps (Kim et al., 2024).Further details and examples of our injection-phrase experiments are provided in Appendix A.</p>
<p>The Effect of k in AID During our experiments, we found that the prevalence of the <eos> token among top-ranked outputs varied substantially across models.To explore this behavior, we conducted an ablation study on the MultiArith dataset, adjusting the value of k for each model and measuring accuracy under zero-shot and zero-shot-CoT conditions.Here, k = 0 indicates that our method is not applied.As shown in Figure 4, LLaMA-3.1-8Bachieved its best performance at k = 2.Meanwhile, Gemma-7B and Mistral-7B-v0.3 exhibited varying preferences for either k = 2 or k = 5, depending on whether zero-shot-CoT was enabled.After examining these trends, we settled on k = 2 for Gemma-7B and k = 5 for Mistral-7B-v0.3, keeping these settings consistent in all subsequent experiments.More Diverse Datasets Beyond the tasks featured in our main results, we expanded our evaluation to more varied datasets for a finer-grained ablation study with LLaMA-3.1-8B.3, our method provides substantial performance boosts over both zero-shot and zero-shot-CoT baselines across arithmetic, commonsense, and logical reasoning tasks.For instance, in the zero-shot setting, our approach improves MultiArith accuracy from 15.56% to 50.56% and GSM8K from 6.97% to 34.57%, while consistently outperforming baselines on other challenging datasets like MATH and BBH-Web of Lies.These gains underscore the versatility and effectiveness of our method in handling a wide range of reasoning tasks.</p>
<p>Model Scales On the AQUA dataset, we found that at smaller scales (LLaMA-3.1-8B),the performance gains were rather minimal.However, once the model size increased to around 10B, the improvements became more evident.This indicates that no matter how much we refine the inherent capabilities, if the model itself does not have sufficient internal knowledge to begin with, the impact remains limited.In contrast, larger models tend to benefit more on challenging tasks, suggesting that once a model's internal capacity reaches a certain threshold, our method can further unlock its   For example, examining AQUA on smaller Llama models shows that the difference between Zero-shot (29.13%) and Zero-shot CoT (28.74%) is negligible.Yet, when moving to a 10B model, we see a clear boost in performance.This implies there is a point below which smaller models cannot substantially improve, even if we attempt to unlock their inherent reasoning capacity.Meanwhile, Falcon3-10B-Base (Team, 2024) exhibits noticeable gains on AQUA, aligning with this trend.</p>
<p>Related Works</p>
<p>A variety of methods have been explored to enhance the reasoning capabilities of large language models (LLMs).These techniques can be broadly categorized into tuning-based, promptbased, sampling-based, and test-time approaches.Below, we discuss how our decoding-based strategy relates to these prior efforts, especially in its ability to improve reasoning without extensive additional resources.</p>
<p>Tuning-Based Approaches Early research on boosting LLM reasoning focused on fine-tuning or distillation.For instance, instruction tuning (Wei et al., 2021) trains models on a broad set of tasks and instructions, improving zero-shot performance across multiple NLP benchmarks.System 2 distillation (Yu et al., 2023) further refines model reasoning by transferring multi-step solutions from a larger teacher model.Although these methods can substantially enhance performance, they typically require large datasets and significant computational resources.Additionally, recent exploration into symbolic or chain-of-thought distillation (Li et al., 2024) still relies heavily on the quality of the larger model's outputs.Our work diverges from these in that we tackle reasoning failures purely at inference time, without additional model training or data curation.</p>
<p>Prompt Engineering Prompt-based methods have garnered significant attention as a more resource-efficient alternative to fine-tuning.Chainof-thought (CoT) (Wei et al., 2022a) structures responses into step-by-step explanations, while extensions such as Tree-of-Thoughts (Yao et al., 2023) and Graph-of-Thoughts (Besta et al., 2023) reorganize the reasoning flow into more complex hierarchies.Other lines of work employ iterative refinement, as in Self-Refine (Madaan et al., 2023), where the model iteratively critiques and updates its solution.Although these approaches can boost LLM reasoning, they often involve substantial prompt engineering or multiple rounds of inference, which can be costly in practice.By contrast, our method obviates heavy prompt tuning and avoids repeated sampling, opting instead for a lightweight, decoding-centric solution.</p>
<p>Sampling-Based Methods Approaches based on sampling seek to improve reliability of reasoning in LLMs by generating multiple candidate solutions.For example, self-consistency sampling (Wang et al., 2023a) aggregates multiple outputs to select a consensus or majority-vote answer, while universal self-consistency (Chen et al., 2024) generalizes this concept to tasks where multiple lines of reasoning may converge on the correct outcome.These methods benefit from exploring diverse reasoning paths but come at the cost of multiple forward passes through the model.In many settings, this extra computational burden can be prohibitive.</p>
<p>Our approach, in contrast, requires only one pass; we adaptively intervene during decoding to extend the chain of reasoning rather than generating multiple completions.</p>
<p>Test-Time Strategies More recent investigations have shifted attention to inference-time or postprocessing methods that address reasoning shortcomings without altering the base model or training regimen.Wang et al. (2023b) introduces a penalty mechanism for abrupt shifts in reasoning flow, while Snell et al. (2023) proposes adaptive inference that strategically allocates resources for verification.Budget forcing (Muennighoff et al., 2025) constrains or terminates chain-of-thought decoding under a specified computational budget.Although these methods highlight the potential of inference-level interventions, many still rely on specialized heuristics or additional compute to manage extended reasoning steps.</p>
<p>By contrast, our work unlocks LLMs' latent reasoning abilities without explicit prompting or additional overhead.Specifically, we reduce immature reasoning by inserting a brief injection phrase whenever the model risks early termination with the <eos> token.This simple yet powerful approach integrates seamlessly into a standard decoding pipeline and has proven effective in mitigating abrupt endings across diverse reasoning tasks.</p>
<p>Conclusion</p>
<p>In this work, we addressed the challenge of immature reasoning in large language models (LLMs), wherein the model's reasoning process terminates prematurely, leading to silence, no reasoning, or incomplete reasoning.Our observations show that despite the substantial capacity of modern LLMs, this issue remains a major source of error in multistep problem-solving.</p>
<p>To mitigate this, we proposed a novel decoding strategy.Rather than relying heavily on prompt engineering, our approach injects a designated "nudging" word whenever the model is likely to produce the <eos> token prematurely.This lightweight, test-time intervention encourages continued reasoning, acting as an implicit signal that mirrors human thinking process.</p>
<p>Extensive experiments across diverse reasoning tasks-including arithmetic, commonsense, and logical reasoning-demonstrate that our method substantially improves performance in both zero-shot and zero-shot-CoT settings.Moreover, it enhances reasoning stability, compensating for ineffective prompts and adapting to varying task complexities.</p>
<p>Overall, our proposed method, Adaptive Injection Decoding, represents a promising direction for unlocking the latent reasoning potential of LLMs while maintaining computational efficiency and ease of deployment.Future work could explore the dynamic selection of injected phrases based on task characteristics, the extension of our method to more complex generation tasks, or the integration with advanced sampling strategies (e.g., selfconsistency, tree-of-thought decoding) to further enhance robustness and reliability.</p>
<p>Limitations</p>
<p>While our method significantly enhances LLM performance, several limitations warrant attention.First, we tested models up to only 10 billion parameters, so the generalizability to extremely small models or much larger systems remains to be explored.Second, our approach relies on a simple injection phrase ("Well") identified to perform robustly in experiments.Although it was proven effective on multiple tasks, variations in domain or language may require additional tuning of this trigger.Third, our experiments focus on reasoning benchmarks in English; extending to multilingual scenarios might reveal different patterns of immature reasoning and alternative effective triggers.</p>
<p>A Detailed Analysis of Injection Phrases</p>
<p>In this section, we present a detailed study on the effectiveness of various phrase injections at test time, as summarized in Table 5.Our goal is to explore how small nudge phrases can discourage premature termination and guide the model toward more extensive and coherent chains of thought.Consistent with our findings in Section 4.3, we verify here that "Well" is the most consistently effective phrase among all options tested.This line of inquiry dovetails with prior research demonstrating that LLM decision-making can be strongly influenced by external prompts, including psychological or social pressures (Kim et al., 2024).</p>
<p>Category</p>
<p>Phrase  Conclusion-Driven Phrases.We observed that phrases prompting the model to continue toward a conclusion-for example, So, Well, and Because-consistently yielded higher accuracies (e.g., 49.44%, 50.56%, and 49.44%, respectively).By comparison, Therefore performed relatively poorly (33.33%), likely because it encourages the model to finalize its answer too abruptly.This pattern underscores why "Well" stood out in our main study: it nudges the model to continue reasoning without prematurely settling on a final answer.</p>
<p>Phrases Initiating Reasoning.Certain phrases that denote a fresh start in thought, such as Let and Step, also contributed to improved outcomes (38.33% and 44.44%).Their effectiveness appears to stem from prompting the model to begin a new sequence of reasoning.However, these gains were still somewhat lower than those achieved by "Well," suggesting that "Well" provides a more neutral yet effective cue.</p>
<p>Continuity vs. Disruption.Phrases like And, which force the model to continue enumerating ideas, did not perform substantially better than the greedy baseline.Similarly, phrases that intentionally change the direction of thinking, such as But or Or, yielded low accuracy (17.78% and 12.78%), suggesting that abrupt shifts in reasoning can destabilize the model when it is already uncertain.Moreover, within the Conjunction Pool, Addition Pool achieves a higher accuracy (26.11%) than both Mix Pool (23.33%) and Contrast Pool (20.56%).These findings align with the main study's conclusion that gently reinforcing the existing reasoning path (e.g., via "Well") is often more beneficial than forcing a sudden logical pivot.</p>
<p>Self-Doubt or Delay.Although phrases like Wait improve slightly on the baseline (21.11% vs. 15.56%),they do not show strong gains overall.We surmise that introducing hesitation without constructive guidance can heighten the model's uncertainty.Full-sentence prompts expressing self-doubt (e.g., Am I doing alright?) or self-assurance (I can do it.)often led to immediate termination or repetitive output, thus hindering performance.Interestingly, the phrase I might be wrong.reached a markedly higher accuracy (33.33%), echoing the discussion in Section 4.3 that acknowledging potential error can sometimes encourage more careful reasoning.</p>
<p>Machine Language and Formatting.Interestingly, the use of machine-like phrases (e.g., # or <start of text>) reached accuracies near or above 25%, indicating that inducing a formatting or structural shift can be beneficial.We suspect this formatting cue encourages more systematic thinking and helps avert premature termination.Similarly, inserting \t improved upon the greedy baseline (19.44% vs. 15.56%),possibly by delaying the immediate production of <eos>.However, more verbose sentence-level injections (e.g., Keep reasoning., Think deeper.)tended to cause the model to either terminate immediately or restate these prompts, which hindered performance.</p>
<p>Discussion.Overall, these results corroborate our main finding that "Well" is the most consistently effective injection phrase.Phrases that naturally sustain or refocus the chain of thought-without dictating a final answer-tend to achieve higher accuracy."Well" exemplifies a subtle yet impactful nudge, fostering continued reasoning while preserving flexibility across various problem types.Future investigations might consider how sentence-level injections (e.g., explicit self-doubt such as I might be wrong.)would scale in larger LLMs, where nuanced disclaimers could further modulate reasoning under stress or social pressures (Kim et al., 2024).</p>
<p>A.1 Illustrative Examples of Phrase Injections</p>
<p>Below, we provide concrete examples demonstrating how specific injection words or phrases can influence the model's responses.These examples illuminate common behaviors (e.g., silence, no reasoning, incomplete reasoning) that certain phrases can mitigate-or in some cases exacerbate.By examining both improvements and adverse outcomes, we show that while injection phrases often help sustain or refocus the reasoning process, they may also inadvertently introduce tangential or repetitive outputs if misapplied.</p>
<p>Silence.When the model (greedy) immediately outputs <eos>, injecting a phrase like Because can encourage it to elaborate: Q: For Halloween Debby and her sister combined the candy they received.Debby had 32 pieces of candy while her sister had 42.If they ate 35 pieces the first night, how many pieces do they have left?A: <eos> (Silence) Q: For Halloween Debby and her sister combined the candy they received.Debby had 32 pieces of candy while her sister had 42.If they ate 35 pieces the first night, how many pieces do they have left?A: Because the problem is asking for the total number of pieces left, we need to find the total number of pieces they had.To do this, we need to add the number of pieces each had.32 + 42 = 74.Now that we know the total number of pieces they had, we can subtract the number of pieces they ate to find the number of pieces left.74 -35 = 39.Therefore, they have 39 pieces of candy left.</p>
<p>No Reasoning.Similarly, So can redirect the model from meaningless enumeration to a more coherent answer: Q: Paige had 11 songs on her mp3 player.If she deleted 9 old songs from it and then added 8 new songs, how many songs does she have on her mp3 player?A: 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.Q: Paige had 11 songs on her mp3 player.If she deleted 9 old songs from it and then added 8 new songs, how many songs does she have on her mp3 player?A: 1. 2.So, Paige has 10 songs on her mp3 player.</p>
<p>I might be wrong.Conveying self-doubt can allow the model to proceed more cautiously and produce a correct answer, rather than halting:</p>
<p>Table 18: A case where injecting I might be wrong.helps the model proceed and produce the correct answer.</p>
<p>Q: For Halloween Emily received 5 pieces of candy from neighbors and 13 pieces from her older sister.If she only ate 9 pieces a day, how long would the candy last her?A: I might be wrong.I think it would last 2 days.</p>
<p>I can do it.,I am doing quite well.Self-affirming phrases can lead to overconfidence or repetitive affirmations without solving the problem: Q: A store had 40 oranges in a bin.If they threw away 25 of the old ones and put 21 new ones in the bin how many would be in the bin?A: I can do it.I just don't know how to explain it.</p>
<p>B Conjunction Pools</p>
<p>As noted in Section A (and shown in Table 5), conjunction-based phrases exhibit notable differences depending on whether they reinforce existing reasoning or introduce a sharp directional shift.To introduce controlled variability rather than relying on a fixed injection phrase, we constructed a conjunction pool consisting of semantically similar phrases.During inference, the injection phrase is randomly sampled from this pool, allowing for slight variation while preserving its intended effect.Table 21 presents the complete lists used in our experiments.Pool Conjunctions ADDITION_POOL {"and", "so", "therefore", "then", "thus", "or", "in addition", "furthermore"} CONTRAST_POOL {"however", "but", "on the other hand", "yet", "in contrast", "nevertheless", "unlike", "instead", "conversely"} MIX_POOL ADDITION_POOL + CONTRAST_POOL Specifically, we define separate pools for addition conjunctions, contrast conjunctions, and a mixed set that combines both.For each instance, an injection phrase is randomly drawn from the relevant pool: the ADDITION_POOL for reinforcing prior reasoning, the CONTRAST_POOL for shifting direction, and the MIX_POOL for a broader range of transitions.This randomized selection enables a more flexible strategy while allowing us to systematically analyze the impact of different conjunction types on model performance.</p>
<p>C Evaluation Prompt and Human Alignment</p>
<p>We use o1-mini via OpenAI's chat completion API (version 2024-09-12) (OpenAI, 2024) to assess correctness in our experiments.In scenarios where LLMs produce freely generated responses rather than strictly formatted answers, extracting definitive outcomes can be challenging.Consequently, LLM-based evaluation becomes a practical solution.Among the models we tested, o1-mini showed the most robust performance and closely matched human judgments, even under these open-ended conditions.</p>
<p>As o1-mini demonstrates advanced language understanding, we specifically designed a detailed set of grading criteria to minimize misclassification and ensure consistent evaluation outcomes.Our criteria aim to address various answer formats and potential pitfalls.In particular, the rules instruct o1-mini to:</p>
<ol>
<li>
<p>Compare only the final stated answer with the ground truth, ignoring any intermediate reasoning provided by the large language model (LLM).</p>
</li>
<li>
<p>Verify that the final answer choice aligns with the LLM's self-generated options (if any), marking mismatched selections as incorrect.</p>
</li>
<li>
<p>Disregard auxiliary examples or sub-problems introduced by the LLM as part of its reasoning, focusing strictly on the main query and final response.</p>
</li>
<li>
<p>Consider minor variations in phrasing valid, as long as the intended meaning remains correct.</p>
</li>
<li>
<p>For True/False questions, infer correctness from the overall meaning of the response (e.g., a statement affirming or denying a proposition) when an explicit "true" or "false" label is not provided.Q: Ned had to wash 9 short sleeve shirts and 21 long sleeve shirts before school.If he had only washed 29 of them by the time school started, how many did he not wash?A: 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 12</p>
</li>
</ol>
<p>Q:</p>
<p>The following paragraphs each describe a set of five objects arranged in a fixed order.The statements are logically consistent within each paragraph.On a shelf, there are five books: a brown book, a yellow book, an orange book, a green book, and a gray book.The green book is to the left of the gray book.The brown book is the third from the left.The gray book is the second from the right.The yellow book is to the left of the green book.</p>
<p>Options: (A) The brown book is the third from the left (B) The yellow book is the third from the left (C) The orange book is the third from the left (D) The green book is the third from the left (E) The gray book is the third from the left A:</p>
<p>(C) The orange book is the third from the left Solution:</p>
<p>The green book is to the left of the gray book.The brown book is the third from the left.The gray book is the second from the right.The yellow book is to the left of the green book.</p>
<p>The green book is to the left of the gray book.The brown book is the third from the left.The gray book is the second from the right.The yellow book is to the left of the green book.</p>
<p>The green book is to the left of the gray book.The brown book is the third from the left.The gray book is the second from the right.The yellow book is to the left of the green book.</p>
<p>The green book is to the left of the gray book.The brown book is the third from the left.The gray book is the second from the right.The yellow book is to the left of the green book.Q: The school cafeteria had 23 apples.If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have?A: 26 ### Guidance The distributive property is a property of multiplication that allows us to distribute a factor over a sum or difference.
3(a + b) = 3a + 3b 5(a − b) = 5a − 5b
The distributive property is used to simplify expressions.</p>
<h4>Example A Simplify: $5(a+b)$ Solution: Use the distributive property to distribute the 5 over the sum.Incomplete Reasoning encompasses scenarios where the model initiates a chain of thought but stops prematurely or changes topic before reaching the correct conclusion.This phenomenon accounted for 5.45% of incorrect GSM8K responses in LLaMA 3.1-8B.Q: Zoe was unboxing some of her old winter clothes.She found 8 boxes of clothing and inside each box there were 4 scarves and 6 mittens.How many pieces of winter clothing did Zoe have total?</h4>
<p>A: 1. 8 scarves and 6 mittens 2. 8 scarves and 24 mittens 3. 32 scarves and 24 mittens 4. 32 scarves and 48 mittens Zoe was unboxing some of her old winter clothes.She found 8 boxes of clothing and inside each box there were 4 scarves and 6 mittens.How many pieces of winter clothing did Zoe have total?1. 8 scarves and 6 mittens 2. 8 scarves and 24 mittens 3. 32 scarves and 24 mittens 4. 32 scarves and 48 mittens  We can solve this equation by subtracting 0.002 from both sides.0.005x + 0.002 − 0.002 = 0.007 − 0.002 0.005x = 0.005 Now we can divide both sides by 0.005</p>
<p>E Artifact Licenses</p>
<p>All artifacts employed in this study (i.e., models and datasets) are strictly governed by their respective license agreements.Specifically, models such as LLaMA-3.1-8B,Mistral-7B-v0.3, Gemma-7B, and Falcon3-10B-Base are subject to the terms and conditions set forth by their creators, which include usage restrictions, redistribution rights, and potential attribution requirements.For instance, some models may allow non-commercial research use but restrict commercial applications.Likewise, the datasets utilized (e.g., GSM8K, MultiArith, StrategyQA, BigBench-Disambiguation QA, and BigBench-Logical Deduction) are released under open-access or research-oriented licenses.Researchers are advised to review any additional clauses that detail citation requirements, limitations on commercial use, or derivative works.</p>
<p>We confirm that all experiments in this work were conducted in strict compliance with these licenses.Any redistributions of the models or datasets, as well as any derivative works, must similarly adhere to the terms specified by the original authors and/or dataset providers.Detailed license information for each artifact can be found in the respective original publications and/or in the supplementary materials accompanying this paper.</p>
<p>Figure 2 :
2
Figure 2: Example question and generated answers of Llama-3.1-8B with (a) Standard Zero-shot, (b) Zero-shot + Ours, (c) Zero-shot-CoT, and (d) Zero-shot-CoT + Ours settings.</p>
<p>Figure 3 :
3
Figure 3: Distribution of immature reasoning categories for incorrect answers in GSM8K using LLaMA 3.1-8B</p>
<p>Figure 4 :
4
Figure 4: The Effect of k in AID.We report zero-shot and zero-shot-CoT accuracy on the MultiArith dataset while adjusting k (top-k) in AID .Here, k = 0 represents the case where our method is not applied.</p>
<p>(a) AQUA(Ling et al., 2017): a multiple-choice set of arithmetic word problems with varying difficulty, (b) Com-monsenseQA(Talmor et al., 2019): a benchmark for commonsense reasoning, (c) BigBench-Date Understanding(Suzgun et al., 2023): which focuses on temporal and date-related reasoning, (d) MATH(Hendrycks et al., 2021): high-school level math problems that extend beyond basic arithmetic, (e) BBH-Web of Lies(Suzgun et al., 2023): tests related to logical reasoning in the presence of deceptive or incorrect statements.Notably, GSM8K, MATH, and AQUA form an approximate difficulty hierarchy (GSM8K &lt; MATH &lt; AQUA) but exhibit similar improvement patterns under our approach.As shown in Table</p>
<p>Q:</p>
<p>Faye had 34 coloring books.If she gave away 3 of them, but then bought 48 more, how many would she have total?A: A. 79 B. 81 C. 82 D. 83 E. 84I am doing quite well.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am in the 90th percentile.I am</p>
<p>the distributive property to distribute the 3 over the difference.3(a−b) = 3a − 3b = 3a − 3b #### Example CSimplify: $2(a+b+c)$ Solution: Use the distributive property to distribute the 2 over the sum.2(a + b + c) = 2a + 2b + 2c = 2a + 2b + 2c #### Concept Problem RevisitedThe school cafeteria had 23 apples.If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have?The cafeteria had 23 apples.They used 20 to make lunch for the students.They bought 6 more.23 apples.They used 20 to make lunch for the students.They bought 6 more.23 − 20 = 3 20 + 3 = 23 The cafeteria had 23 D.3 Incomplete Reasoning: Resulting in an Incorrect or Partial Answer</p>
<p>this equation by subtracting 0.02 from both sides.0.05x + 0.02 − 0.02 = 0.07 − 0.02 0.05x = 0.05 Now we can divide both sides by 0.05.</p>
<p>Table 1 :
1
Accuracy (%) of LLaMA-3.1-8B,Mistral-7B-v0.3, and Gemma-7B models across a wide range of reasoning tasks.We compared four settings: Zero-shot, Zero-shot + Ours, Zero-shot-CoT, and Zero-shot-CoT + Ours.
ModelMethodArithmeticCommonsenseLogicalAvgMultiArith GSM8K StrategyQA DisambiguationQA Logical DeductionZero-shot15.56%6.97%26.35%36.00%28.40%22.66%Llama-3.1-8BZero-shot + Ours Zero-shot-CoT50.56% 77.22%34.57% 48.90%30.13% 24.31%37.20% 32.00%32.00% 16.80%36.90% 39.85%Zero-shot-CoT + Ours78.33%34.34%45.27%34.00%24.80%43.35%Zero-shot10.00%48.98%31.88%10.80%24.80%25.29%Gemma-7bZero-shot + Ours Zero-shot-CoT15.00% 46.11%45.26% 38.59%34.79% 19.36%16.80% 34.00%27.20% 18.80%27.81% 31.37%Zero-shot-CoT + Ours73.33%44.81%23.44%36.00%24.00%40.32%Zero-shot16.11%7.73%23.58%34.40%14.80%19.32%Mistral-7B-v0.3Zero-shot + Ours Zero-shot-CoT27.78% 29.44%15.31% 17.21%25.62% 16.30%37.20% 13.20%18.00% 28.80%24.78% 20.99%Zero-shot-CoT + Ours37.22%15.39%36.83%32.00%20.40%28.37%</p>
<p>Table 2 :
2
Injection Phrase Analysis.Accuracy of different injection phrases in the MultiArith dataset, evaluated on the LLaMA-3.1-8Bmodel.</p>
<p>Table 3 :
3
Accuracy (%) of the LLaMA-3.1-8Bmodel across a wide range of reasoning tasks: four Mathematical Reasoning tasks (MultiArith, GSM8K, MATH, AQUA), four Commonsense Reasoning tasks (CSQA, StrategyQA, BBH-Date Understanding, BBH-DisambiguationQA), and two Logical Reasoning tasks (BBH-Logical Deduction, BBH-Web of Lies).
LLaMA-3.1-8BFalcon3-10B-BaseZero-shot Zero-shot-CoT Zero-shot Zero-shot-CoTBaseline 15.35%29.13%43.70%40.16%Ours24.41%28.74%57.09%51.57%</p>
<p>Table 4 :
4
Performance (accuracy %) on AQUA."Zeroshot" and "Zero-shot-CoT" refer to the respective prompting strategies, with different model scales.</p>
<p>reasoning abilities.</p>
<p>Table 5 :
5
Injection Phrase Ablation</p>
<p>Table 6 :
6
Zero-Shot Case.The model outputs <eos> token on the first generation, showing Silence.</p>
<p>Table 7 :
7
Injecting Because helps the model overcome silence, ultimately producing the answer.</p>
<p>Table 8 :
8
Zero-Shot Case.The model repeats meaningless enumeration until reaching the max generation length.</p>
<p>Table 9 :
9
Injecting So helps the model avoid repetitive output and focus on problem solving.</p>
<p>Table 19 :
19
A case where injecting I can do it.causes the model to be overconfident, offering no real solution.</p>
<p>Table 20 :
20
A case where injecting I am doing quite well.causes the model to loop repetitive affirmations.</p>
<p>Table 21 :
21
Conjunction pools used in our experiments.</p>
<p>Table 25 :
25
Mistral-7B (on CommonsenseQA) produces unrelated grammar questions in 42.49% of its responses, as exemplified below.What do people aim to do at work? (A) complete job (B) learn from each other (C) kill animals (D) wear hats (E) talk to each other A: Random Topics: Adjectives and AdverbsAdjectives and AdverbsAdjectives and Adverbs Next quiz: Grammar The <strong><em>_</em></strong>_ of the sentence is the subject.A. verb B. noun C. adjective D. adverb</p>
<p>Q:</p>
<p>Table 26 :
26
Mistral 7B (on MultiArith) outputs irrelevant numeric sequences and terminates generation.</p>
<p>Table 27 :
27
Llama3 8B (on CommonsenseQA) producing additional, unnecessary choices in multiple-choice scenarios.
Q: When drinking booze what can you do to stay busy? (A) reach tentative agreement (B) stay inbed (C) stop bicycle (D) examine thing (E) suicide
A: (F) stay busy (G) stay sober (H) stay in shape Answer: F Explanation: When drinking booze what can you do to stay busy?When drinking booze what can you do to stay busy?</p>
<p>Table 28 :
28
Gemma 7B (on MultiArith) arbitrarily generating answer choices even for non-multiple-choice questions.Luke was helping the cafeteria workers pick up lunch trays, but he could only carry 4 trays at a time.If he had to pick up 20 trays from one table and 16 trays from another, how many trips will he make?
A: A. 4B. 5C. 6D. 7E. 8Show moreStep 11 of 2Determine the number of trips Luke will make.20 ÷ 4 = 5 tripsResult2 of 2B. 5Answer: FExplanation: When drinking booze what can you do to stay busy? When drinking booze whatcan you do to stay busy?
Q:</p>
<p>Table 29 :
29
Gemma-7B (on Logical Deduction) generating an answer first before reasoning through it, but the subsequent solution lacks meaningful reasoning.</p>
<p>Table 30 :
30
Mistral-7B (on MultiArith) frequently generates "# Guidance" and "# Example", mimicking a few-shot style, which reflects the model's learned pattern for mathematical problem-solving.</p>
<p>Table 31 :
31
failing to derive the final answer by prematurely stopping the Chain-of-Thought (CoT) process.The correct answer is 80, but the model halts before completing the final step of summing 32 and 48.</p>
<p>Table 32 :
32
Mistral 7B (on MultiArith) creating and solving its own few-shot examples instead of solving the given problem.Paige had 43 math problems and 12 science problems for homework.If she finished 44 of the problems at school, how many problems did she have to do for homework?A: "1.43 2. 44 3. 45 4. 46 ### Watch This Watch this video for help with the concepts in this section.### Guidance In the last section, we learned how to solve equations with fractions.In this section, we will learn how to solve equations with decimals.#### Example A Solve the equation for x .0.2x + 0.3 = 0.5 Solution: We can solve this equation by subtracting 0.3 from both sides.0.2x + 0.3 − 0.3 = 0.5 − 0.3 0.2x = 0.2 Now we can divide both sides by 0.2.</p>
<p>Q:</p>
<p>Table24: Gemma 7B (on MultiArith) repeating the given question instead of solving it.Q:The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? A: The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples would they have? The school cafeteria had 23 apples. If they used 20 to make lunch for the students and then bought 6 more, how many apples
Incomplete Reasoning.Injecting Well prompts the model to provide step-by-step reasoning instead of a rushed or partial conclusion:Table10: The model provides the correct choice but without showing any reasoning process.Q: Bianca and her friends were recycling paper for their class.For every 3 pounds they recycled they earned one point.If Bianca recycled 24 pounds and her friends recycled 3 pounds, how many points did they earn?A: 8 9 10 11 A. 7 B. 8 C. 9 D. 10 Answer: C Q: Bianca and her friends were recycling paper for their class.For every 3 pounds they recycled they earned one point.If Bianca recycled 24 pounds and her friends recycled 3 pounds, how many points did they earn?A: 8 9 10 11 A. 7 B. 8 C. 9 D. 10 Answer: C Well, Bianca and her friends recycled 24 pounds and 3 pounds, respectively.So, the total pounds they recycled is 24 + 3 = 27 pounds.For every 3 pounds they recycled they earned one point.So, the total points they earned is 27 / 3 = 9 points.Therefore.While Therefore might sometimes help finalize a correct answer, we also observed instances where it led to premature or abrupt conclusions: Q: Gwen was organizing her book case making sure each of the shelves had exactly 4 books on it.If she had 5 shelves of mystery books and 3 shelves of picture books, how many books did she have total?A: Gwen was organizing her book case making sure each of the shelves had exactly 4 books on it.If she had 5 shelves of mystery books and 3 shelves of picture books, how many books did she have total?(The model initially repeats the question.)Therefore, the total number of books is 5<em>4 + 3</em>4 = 32 books.Q: For a birthday party Jerry bought 41 regular sodas and 22 diet sodas.If his fridge would only hold 9 on each shelf, how many shelves would he fill up?A: Let x be the number of shelves.9x = 41 + 22 9x = 63 x = 7 Jerry would need 7 shelves to hold all the sodas.Q: A company invited 47 people to a luncheon, but 7 of them didn't show up.If the tables they had held 5 people each, how many tables do they need?A: And if the tables they had held 6 people each, how many tables do they need?<code>L LM's Response: Well, the answer is 24 hours.But how did we get there?I know that the total amount of money he earned is 7 times the amount of money he earned from each customer.So, 7(3) = 21.</code>T he final answer here is <strong>21</strong>, so compare it with the ground truth.2. If the LLM selects an incorrect choice from predefined options generated by itself, mark it incorrect-even if it derives the correct answer in its reasoning.-Example: <code>L LM's Response: (A) 11 (B) 13 (C) 15 Well, I know that 50 -40 = 10.So she had 10 eggs.The final answer is (A).</code>T he correct answer is <strong>10</strong>, but the LLM's selected choice <strong>(A) 11</strong> is incorrect.3. Ignore example problems the LLM generates during reasoning.-If the LLM solves auxiliary example problems generated by itself before answering the given question, disregard those and judge based only on its answer to the main question.-Example:<code>Q uestion: James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.How many total meters does he run a week?Answer: 540 LLM's Response:The distributive property states that...Concept Problem Revisited James decides to run 3 sprints 3 times a week.He runs 60 meters each sprint.3 × 3 = 9 9 × 60 = 540James runs 540 meters a week.</code>T he final answer is <strong>540</strong>, which should be compared to the ground truth.4. Handle answer format variations properly.-4.1.If the LLM's answer matches a choice's meaning but not its letter label, mark it correct.-Example:D Examples of Immature ReasoningImmature reasoning refers to instances where LLMs fail to complete the Chain of Thought (CoT) reasoning process.To better understand the nature of these failures, we categorize them into four types and provide real examples demonstrating each failure pattern.Each failure type is presented with a clear example, including the input prompt, the model's incorrect response.D.1 Silence: Failure to Generate Any ResponseSilence occurs when the model fails to produce any output in response to a given question.Instead of providing even a partial solution or a refusal, the model simply emits no tokens.As shown in our main analysis, this pattern accounted for 58.05% of incorrect responses in LLaMA 3.1-8B's GSM8K outputs.Q: Janet's ducks lay 16 eggs per day.She eats three for breakfast every morning and bakes muffins for her friends every day with four.She sells the remainder at the farmers' market daily for 2 per fresh duck egg.How much in dollars does she make every day at the farmers' market?A: <eos>D.2 No Reasoning: Generating a Response Without ReasoningNo Reasoning refers to answers that do not exhibit any chain-of-thought or logical progression.In these cases, the model usually outputs repeated phrases, irrelevant content (e.g., HTML tags or unrelated numeric sequences), or a simple restatement of the question without meaningful analysis.
Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, T Hoefler, 2023</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 34th Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; Vancouver, Canada2020. NeurIPS 2020Ilya Sutskever, and Dario Amodei</p>
<p>Zero-shot approach to overcome perturbation sensitivity of prompts. Mohna Chakraborty, Adithya Kulkarni, Qi Li, arXiv:2305.156892023arXiv preprint</p>
<p>Universal self-consistency for large language model generation. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, Denny Zhou, ICML 2024 Workshop on In-Context Learning. 2024ICL Poster</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168[cs.LG]Training verifiers to solve math word problems. 2021</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, arXiv:2101.022352021. 2021Accepted for publication in TACL</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS 2021 Datasets and Benchmarks Track. 2021. 19 Oct 2021. Nov 2024</p>
<p>Will LLMs sink or swim? exploring decision-making under pressure. Kyusik Kim, Hyeonseok Jeon, Jeongwoo Ryu, Bongwon Suh, 10.18653/v1/2024.findings-emnlp.668Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Symbolic chain-of-thought distillation: Small models can also. Liunian Harold, Li , Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin Choi, 2024think" step-by-step</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:2306.14050arXiv:1705.04146 [cs.AI]2017Preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, 2023</p>
<p>Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri. Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Giuseppe Pier, Aakanksha Sessa, Adam Chowdhery, Aditya Roberts, Alex Barua, Alex Botev, Ambrose Castro-Ros, Amélie Slone, Andrea Héliou, Anna Tacchetti, Antonia Bulanova, Beth Paterson, Bobak Tsai, Charline Le Shahriari, Christopher A Lan, Clément Choquette-Choo, Daniel Crepy, Daphne Cer, David Ippolito, Elena Reid, Eric Buchatskaya, Eric Ni, Geng Noland, George Yan, George-Christian Tucker, Grigory Muraru, Henryk Rozhdestvenskiy, Ian Michalewski, Ivan Tenney, Jacob Grishchenko, James Austin, Jane Keeling, Jean-Baptiste Labanowski, Jeff Lespiau, Jenny Stanway, Jeremy Brennan, Johan Chen, Justin Ferret, Justin Chiu, Katherine Mao-Jones, Kathy Lee, Katie Yu, Lars Lowe Millican, Lisa Sjoesund, Lucas Lee, Machel Dixon, Maciej Reid, Mateo Mikuła, Michael Wirth, Nikolai Sharman, Nithum Chinaev, Olivier Thain, Oscar Bachem, Oscar Chang, Paige Wahltinez, Paul Bailey, Petko Michel, Rahma Yotov, Ramona Chaabouni, Reena Comanescu, Rohan Jana, Ross Anil, Ruibo Mcilroy, Ryan Liu, Samuel L Mullins, Sebastian Smith, Ted Borgeaud, Tom Klimenko, Vlad Hennigan, Wojciech Feinberg, Yu-Hui Stokowiec, Zafarali Chen, Zhitao Ahmed, Tris Gong, Ludovic Warkentin, Minh Peran, Giang, arXiv:2403.08295[cs.CLZoubin Ghahramani. Douglas EckClément Farabet, Oriol Vinyalset al. 2024. Gemma: Open models based on gemini research and technology</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393[cs.CL].(v2s1: Simple test-time scaling. 2025. Feb 2025</p>
<p>OpenAI O1 Mini: Advancing Cost-Efficient Reasoning. 2024OpenAI</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/D15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational Linguistics2015</p>
<p>Marco Siino, 10.18653/v1/2024.semeval-1.24Mistral at SemEval-2024 task 5: Mistral 7B for argument reasoning in civil procedure. 2024</p>
<p>Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024). the 18th International Workshop on Semantic Evaluation (SemEval-2024)Mexico City, MexicoAssociation for Computational Linguistics</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. C Snell, J Lee, K Xu, A Kumar, 2023</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, Findings of the Association for Computational Linguistics: ACL 2023. 2023</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937[cs.CLCommonsenseqa: A question answering challenge targeting commonsense knowledge. 2019. 2019</p>
<p>The falcon 3 family of open models. -Llm Falcon, Team, arXiv:2302.13971[cs.CL]Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 2024. 2023</p>
<p>Selfconsistency improves chain-of-thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E H Chi, S Narang, A Chowdhery, D Zhou, 2023a</p>
<p>Chain-of-thought reasoning without prompting. X Wang, D Zhou, 2023Google DeepMindTechnical report</p>
<p>. Y Wang, Q Liu, J Xu, T Liang, X Chen, Z He, L Song, D Yu, J Li, Z Zhang, R Wang, Z Tu, H Mi, D Yu, 2023bThoughts are all over the place: On the underthinking of o1-like llms</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, 2021Google ResearchTechnical report</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, 2022a</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, ; S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, Transactions on Machine Learning Research. Survey Certification. 2022b. 2023Tree of thoughts: Deliberate problem solving with large language models</p>
<p>Meta FAIR. <code>Q uestion: What is likely to satisfy someone's curiosity? (A) hear news (B) read book (C) see favorite show (D) comedy show (E) go somewhere Answer: A LLM's Response: The answer is hearing news. P Yu, J Xu, J Weston, I Kulikov, 2023Technical reportFor multiple-choice questions where the LLM outputs only the letter choice, ensure it matches the meaning of the correct answer. À nswer: (B) read book LLM's Response: B</code>T his is correct since (B) corresponds to "read book". -4.</p>
<p>Example: <code>Q uestion: Does the moon have its own light? Answer: False LLM's Response: The moon does not generate its own light; it only reflects sunlight.</code>T his should be marked correct, as the meaning aligns with "false. For True/False Questions, if the LLM does not explicitly state "true" or "false," infer its judgment based on the response's meaning before comparison. Question: {question} Answer: {answer} LLM's Response: {llm_answer} Is the predicted answer correct? Dataset Multiarith GSM8K StrategyQA DisambiguationQA Logical Deduction gpt-4o-mini</p>
<p>Comparison of LLM-based evaluations (using o1-mini and gpt-4o-mini) with human evaluation. All results were obtained via zero-shot generation using LLaMA-3. Table. 22</p>            </div>
        </div>

    </div>
</body>
</html>
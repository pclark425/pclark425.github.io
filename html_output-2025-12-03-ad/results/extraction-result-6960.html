<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6960 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6960</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6960</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-276741572</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.00902v1.pdf" target="_blank">Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction</a></p>
                <p><strong>Paper Abstract:</strong> Self-reflection for Large Language Models (LLMs) has gained significant attention. Existing approaches involve models iterating and improving their previous responses based on LLMs' internal reflection ability or external feedback. However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance. Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn issues. To mitigate this, we introduce Instruct-of-Reflection (IoRT), a novel and general reflection framework that leverages dynamic-meta instruction to enhance the iterative reflection capability of LLMs. Specifically, we propose the instructor driven by the meta-thoughts and self-consistency classifier, generates various instructions, including refresh, stop, and select, to guide the next reflection iteration. Our experiments demonstrate that IoRT achieves an average improvement of 10.1% over established baselines in mathematical and commonsense reasoning tasks, highlighting its efficacy and applicability.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6960.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6960.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IoRT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-of-Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic iterative reflection framework that augments reflection with meta-thoughts and an instructor that issues refresh/stop/select instructions driven by a self-consistency classifier to mitigate redundant, drift, and stubborn iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B/13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box large language models (API or chat models) used as generators and reflectors; GPT-3.5/GPT-4 used as meta-thinker and instructor in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT-3.5, GPT-4, Llama2 7B/13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Instruct-of-Reflection (IoRT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate meta-thoughts (abstract guidance), produce an initial response (refresh), produce a reflective response (reflector evaluates plausibility/correctness and produces feedback), then an instructor (augmented by meta-thoughts and a self-consistency classifier) chooses one of three actions: stop (halt iteration), select (choose better of two responses), or refresh (force regeneration) to direct the next iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterative with dynamic instructor (select/stop/refresh)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (mathematical reasoning); StrategyQA (commonsense reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems (GSM8K, SVAMP) and multi-hop commonsense yes/no questions (StrategyQA).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-3.5 — GSM8K: 78.3% acc; SVAMP: 82.9% acc; StrategyQA: 66.8% acc. GPT-4 — GSM8K: 94.0%; SVAMP: 93.4%; StrategyQA: 75.4%. Llama2-7B — GSM8K: 19.5%; SVAMP: 43.5%; StrategyQA: 47.3%. Llama2-13B — GSM8K: 30.5%; SVAMP: 53.0%; StrategyQA: 54.2%. Llama2-70B — GSM8K: 58.6%; SVAMP: 75.2%; StrategyQA: 62.0%. (These are the initial-response accuracies reported in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>GPT-3.5 (IoRT) — GSM8K: 84.6% acc; SVAMP: 88.1% acc; StrategyQA: 71.9% acc. GPT-4 (IoRT) — GSM8K: 95.4%; SVAMP: 95.3%; StrategyQA: 78.8%. Llama2-7B (IoRT) — GSM8K: 24.0%; SVAMP: 45.7%; StrategyQA: 53.1%. Llama2-13B (IoRT) — GSM8K: 40.8%; SVAMP: 57.5%; StrategyQA: 60.1%. Llama2-70B (IoRT) — GSM8K: 66.0%; SVAMP: 80.1%; StrategyQA: 67.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>IoRT still makes occasional misjudgments (paper reports that with oracle evaluation the final-iteration performance can exceed IoRT by 1.6% on GPT-3.5 math), so instructor decisions are not perfect. Meta-thinker and instructor currently rely on commercial models (GPT-3.5 used) because open-source models showed limitations in abstract reasoning; average improvements vary by model size (larger models are already more stable). Maximum iterations were capped at 4; average iterations for IoRT ≈ 2.2, so some settings still require multiple API calls. IoRT reduces but does not eliminate redundant, drift, and stubborn failures (rare mis-selection or incorrect refresh decisions reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6960.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6960.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-correct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsic self-correction (self-correct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reflection procedure where the model uses only its internal knowledge/parameters to critique and revise its own previous answer without external tools or explicit instructor guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated in analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-Turbo used as a black-box model performing generate-then-reflect cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-correct (intrinsic self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial response, then the model internally evaluates and attempts to revise its answer using only its own outputs as feedback (no external tools or instructor).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (static iterative pipeline: response→reflect→revise)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems (GSM8K, SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Initial response accuracies vary by setup; paper reports that when evaluated without oracle, intrinsic self-correction can degrade performance relative to initial responses (see paper for per-experiment initials).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Without oracle labels, performance is unstable and can decrease — paper reports drops up to −2.4% on GSM8K and SVAMP (GPT-3.5) after iterative intrinsic self-correction; with oracle-only evaluation (i.e., only accepting ✗⇒✓ corrections) accuracy appears to improve steadily but this relies on oracle information.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Static iterative self-correction suffers from three failure modes identified in the paper: redundant iterations (✓⇒✓), drift (✓⇒✗), and stubbornness/invalid consistent iterations (✗⇒✗). The proportion of cases where correct answers become incorrect (✓⇒✗) is often comparable to or exceeds ✗⇒✓, explaining overall degradation. Improvements that depend on oracle filtering of changes vanish when oracle signals are absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6960.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6960.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRITIC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRITIC (Self-Correcting with Tool-Interactive Critiquing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-interactive reflection method that uses external tool interactions (e.g., code execution) as feedback to evaluate and revise model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated in analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Black-box LLM interacting with external tools (code executors) used to produce and evaluate program-of-thought solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5 (API model)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>CRITIC</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial output (often program-of-thought), use tool execution results as external feedback to critique the output, then revise the solution accordingly (static iterate: initial→evaluate via tools→revise).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect with external tool feedback (static iterative pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP (mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems where program-of-thought code can be executed and evaluated by a code executor.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Initial program-of-thought accuracies vary by model and dataset; paper compares CRITIC's iterative behavior to self-correct using GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>When evaluated without oracle labels, CRITIC's performance can also degrade: the paper reports drops up to −3.0% on GSM8K and SVAMP (GPT-3.5) after iterative reflection without oracle. Compared to intrinsic self-correct, CRITIC tends to have a lower ratio of ✓⇒✗ (i.e., more stability) but still suffers from instability when oracle guidance is removed.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CRITIC follows a static pipeline and can still cause degradation when iterations are unconstrained and not steered by an instructor; reported improvements in prior work sometimes relied on oracle filtering (only accepting corrections that change incorrect→correct), which is unavailable in realistic evaluation. CRITIC reduces some drift but does not fully eliminate redundancy or stubborn incorrect persistence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6960.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6960.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflection (Shinn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (self-reflection baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior autonomous-agent style self-reflection method where an agent/memory loop is used to reflect and revise outputs; used as a baseline in this paper and shown in experiments to sometimes decrease performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated as baseline (various LLMs in referenced work); used as baseline in this paper's comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent-style architectures that incorporate a dynamic memory and iterative self-reflection loop (as described in the referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (depending on referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflection (agent-memory loop)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model maintains and updates an external memory and reflects on prior outputs to plan and produce revised answers; used as an iterative self-improvement baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect iterations (agent-memory feedback loop)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>StrategyQA (commonsense) and mathematical reasoning tasks as baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense yes/no multi-step questions and mathematical problems used in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Baseline initial methods (e.g., CoT) achieve certain accuracies; paper reports comparisons to self-reflection baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Paper reports that self-reflection baseline decreased performance on StrategyQA (example: self-reflection caused a −6.0% decrease relative to CoT in StrategyQA in the reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-reflection methods can be high-overhead and may decrease performance when run without careful control or oracle supervision; in this paper self-reflection (as implemented in baseline) led to degraded performance on StrategyQA and high token/API call overheads in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6960.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6960.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Contrast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Contrast (inconsistent-perspectives reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reflection method that generates diverse solving perspectives and contrasts them to improve final selection; used as a strong baseline and occasionally outperforms IoRT on some model/dataset combinations at the cost of higher call overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Contrast: Better reflection through inconsistent solving perspectives</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated as a baseline across GPT and Llama2 models (per paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling-based method that generates multiple diverse solution attempts (inconsistent perspectives) and contrasts/votes among them to select the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (GPT-3.5, GPT-4, Llama2 variants in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Contrast</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple diverse solutions (different perspectives) and compare/contrast them (often via voting or aggregating) to improve final answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>multiple-samples + contrast/vote (not iterative instruction-driven reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, SVAMP, StrategyQA (baselines/comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning and commonsense reasoning benchmarks used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Initial response accuracies depend on base prompting; paper notes Self-Contrast outperformed IoRT on GSM8K with Llama2-7B and on SVAMP with GPT-3.5 in specific cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Self-Contrast can achieve higher accuracy in some model/dataset combos but at cost of higher call overhead (paper states Self-Contrast incurred ~7.0% higher overhead than IoRT in one comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gains come with increased API/LLM call overhead; not necessarily robust across all models/datasets. It addresses diversity by sampling but does not implement an instructor to stop or refresh iterations, so it can be less efficient than IoRT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Self-Contrast: Better reflection through inconsistent solving perspectives <em>(Rating: 2)</em></li>
                <li>Self-Correcting with Tool-Interactive Critiquing (CRITIC) <em>(Rating: 2)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 1)</em></li>
                <li>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6960",
    "paper_id": "paper-276741572",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "IoRT",
            "name_full": "Instruct-of-Reflection",
            "brief_description": "A dynamic iterative reflection framework that augments reflection with meta-thoughts and an instructor that issues refresh/stop/select instructions driven by a self-consistency classifier to mitigate redundant, drift, and stubborn iterations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-0613, GPT-4-0613, Llama2-chat (7B/13B/70B)",
            "model_description": "Black-box large language models (API or chat models) used as generators and reflectors; GPT-3.5/GPT-4 used as meta-thinker and instructor in experiments.",
            "model_size": "various (GPT-3.5, GPT-4, Llama2 7B/13B/70B)",
            "reflection_method_name": "Instruct-of-Reflection (IoRT)",
            "reflection_method_description": "Generate meta-thoughts (abstract guidance), produce an initial response (refresh), produce a reflective response (reflector evaluates plausibility/correctness and produces feedback), then an instructor (augmented by meta-thoughts and a self-consistency classifier) chooses one of three actions: stop (halt iteration), select (choose better of two responses), or refresh (force regeneration) to direct the next iteration.",
            "iteration_type": "generate-then-reflect iterative with dynamic instructor (select/stop/refresh)",
            "num_iterations": 4,
            "task_name": "GSM8K, SVAMP (mathematical reasoning); StrategyQA (commonsense reasoning)",
            "task_description": "Grade-school math word problems (GSM8K, SVAMP) and multi-hop commonsense yes/no questions (StrategyQA).",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "GPT-3.5 — GSM8K: 78.3% acc; SVAMP: 82.9% acc; StrategyQA: 66.8% acc. GPT-4 — GSM8K: 94.0%; SVAMP: 93.4%; StrategyQA: 75.4%. Llama2-7B — GSM8K: 19.5%; SVAMP: 43.5%; StrategyQA: 47.3%. Llama2-13B — GSM8K: 30.5%; SVAMP: 53.0%; StrategyQA: 54.2%. Llama2-70B — GSM8K: 58.6%; SVAMP: 75.2%; StrategyQA: 62.0%. (These are the initial-response accuracies reported in the paper.)",
            "performance_after_reflection": "GPT-3.5 (IoRT) — GSM8K: 84.6% acc; SVAMP: 88.1% acc; StrategyQA: 71.9% acc. GPT-4 (IoRT) — GSM8K: 95.4%; SVAMP: 95.3%; StrategyQA: 78.8%. Llama2-7B (IoRT) — GSM8K: 24.0%; SVAMP: 45.7%; StrategyQA: 53.1%. Llama2-13B (IoRT) — GSM8K: 40.8%; SVAMP: 57.5%; StrategyQA: 60.1%. Llama2-70B (IoRT) — GSM8K: 66.0%; SVAMP: 80.1%; StrategyQA: 67.9%.",
            "improvement_observed": true,
            "limitations_or_failure_cases": "IoRT still makes occasional misjudgments (paper reports that with oracle evaluation the final-iteration performance can exceed IoRT by 1.6% on GPT-3.5 math), so instructor decisions are not perfect. Meta-thinker and instructor currently rely on commercial models (GPT-3.5 used) because open-source models showed limitations in abstract reasoning; average improvements vary by model size (larger models are already more stable). Maximum iterations were capped at 4; average iterations for IoRT ≈ 2.2, so some settings still require multiple API calls. IoRT reduces but does not eliminate redundant, drift, and stubborn failures (rare mis-selection or incorrect refresh decisions reported).",
            "uuid": "e6960.0",
            "source_info": {
                "paper_title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-correct",
            "name_full": "Intrinsic self-correction (self-correct)",
            "brief_description": "Reflection procedure where the model uses only its internal knowledge/parameters to critique and revise its own previous answer without external tools or explicit instructor guidance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (evaluated in analysis)",
            "model_description": "GPT-3.5-Turbo used as a black-box model performing generate-then-reflect cycles.",
            "model_size": "GPT-3.5 (API model)",
            "reflection_method_name": "Self-correct (intrinsic self-correction)",
            "reflection_method_description": "Generate an initial response, then the model internally evaluates and attempts to revise its answer using only its own outputs as feedback (no external tools or instructor).",
            "iteration_type": "generate-then-reflect (static iterative pipeline: response→reflect→revise)",
            "num_iterations": 4,
            "task_name": "GSM8K, SVAMP (mathematical reasoning)",
            "task_description": "Grade-school math word problems (GSM8K, SVAMP).",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Initial response accuracies vary by setup; paper reports that when evaluated without oracle, intrinsic self-correction can degrade performance relative to initial responses (see paper for per-experiment initials).",
            "performance_after_reflection": "Without oracle labels, performance is unstable and can decrease — paper reports drops up to −2.4% on GSM8K and SVAMP (GPT-3.5) after iterative intrinsic self-correction; with oracle-only evaluation (i.e., only accepting ✗⇒✓ corrections) accuracy appears to improve steadily but this relies on oracle information.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Static iterative self-correction suffers from three failure modes identified in the paper: redundant iterations (✓⇒✓), drift (✓⇒✗), and stubbornness/invalid consistent iterations (✗⇒✗). The proportion of cases where correct answers become incorrect (✓⇒✗) is often comparable to or exceeds ✗⇒✓, explaining overall degradation. Improvements that depend on oracle filtering of changes vanish when oracle signals are absent.",
            "uuid": "e6960.1",
            "source_info": {
                "paper_title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CRITIC",
            "name_full": "CRITIC (Self-Correcting with Tool-Interactive Critiquing)",
            "brief_description": "A tool-interactive reflection method that uses external tool interactions (e.g., code execution) as feedback to evaluate and revise model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (evaluated in analysis)",
            "model_description": "Black-box LLM interacting with external tools (code executors) used to produce and evaluate program-of-thought solutions.",
            "model_size": "GPT-3.5 (API model)",
            "reflection_method_name": "CRITIC",
            "reflection_method_description": "Generate initial output (often program-of-thought), use tool execution results as external feedback to critique the output, then revise the solution accordingly (static iterate: initial→evaluate via tools→revise).",
            "iteration_type": "generate-then-reflect with external tool feedback (static iterative pipeline)",
            "num_iterations": 4,
            "task_name": "GSM8K, SVAMP (mathematical reasoning)",
            "task_description": "Grade-school math word problems where program-of-thought code can be executed and evaluated by a code executor.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Initial program-of-thought accuracies vary by model and dataset; paper compares CRITIC's iterative behavior to self-correct using GPT-3.5.",
            "performance_after_reflection": "When evaluated without oracle labels, CRITIC's performance can also degrade: the paper reports drops up to −3.0% on GSM8K and SVAMP (GPT-3.5) after iterative reflection without oracle. Compared to intrinsic self-correct, CRITIC tends to have a lower ratio of ✓⇒✗ (i.e., more stability) but still suffers from instability when oracle guidance is removed.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "CRITIC follows a static pipeline and can still cause degradation when iterations are unconstrained and not steered by an instructor; reported improvements in prior work sometimes relied on oracle filtering (only accepting corrections that change incorrect→correct), which is unavailable in realistic evaluation. CRITIC reduces some drift but does not fully eliminate redundancy or stubborn incorrect persistence.",
            "uuid": "e6960.2",
            "source_info": {
                "paper_title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Reflection (Shinn)",
            "name_full": "Reflexion (self-reflection baseline)",
            "brief_description": "Prior autonomous-agent style self-reflection method where an agent/memory loop is used to reflect and revise outputs; used as a baseline in this paper and shown in experiments to sometimes decrease performance.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "use",
            "model_name": "evaluated as baseline (various LLMs in referenced work); used as baseline in this paper's comparisons",
            "model_description": "Agent-style architectures that incorporate a dynamic memory and iterative self-reflection loop (as described in the referenced paper).",
            "model_size": "various (depending on referenced work)",
            "reflection_method_name": "Self-Reflection (agent-memory loop)",
            "reflection_method_description": "The model maintains and updates an external memory and reflects on prior outputs to plan and produce revised answers; used as an iterative self-improvement baseline.",
            "iteration_type": "generate-then-reflect iterations (agent-memory feedback loop)",
            "num_iterations": 4,
            "task_name": "StrategyQA (commonsense) and mathematical reasoning tasks as baseline comparisons",
            "task_description": "Commonsense yes/no multi-step questions and mathematical problems used in paper comparisons.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Baseline initial methods (e.g., CoT) achieve certain accuracies; paper reports comparisons to self-reflection baseline.",
            "performance_after_reflection": "Paper reports that self-reflection baseline decreased performance on StrategyQA (example: self-reflection caused a −6.0% decrease relative to CoT in StrategyQA in the reported comparisons).",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Self-reflection methods can be high-overhead and may decrease performance when run without careful control or oracle supervision; in this paper self-reflection (as implemented in baseline) led to degraded performance on StrategyQA and high token/API call overheads in some settings.",
            "uuid": "e6960.3",
            "source_info": {
                "paper_title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Self-Contrast",
            "name_full": "Self-Contrast (inconsistent-perspectives reflection)",
            "brief_description": "A reflection method that generates diverse solving perspectives and contrasts them to improve final selection; used as a strong baseline and occasionally outperforms IoRT on some model/dataset combinations at the cost of higher call overhead.",
            "citation_title": "Self-Contrast: Better reflection through inconsistent solving perspectives",
            "mention_or_use": "use",
            "model_name": "Evaluated as a baseline across GPT and Llama2 models (per paper comparisons)",
            "model_description": "Sampling-based method that generates multiple diverse solution attempts (inconsistent perspectives) and contrasts/votes among them to select the final answer.",
            "model_size": "various (GPT-3.5, GPT-4, Llama2 variants in comparisons)",
            "reflection_method_name": "Self-Contrast",
            "reflection_method_description": "Generate multiple diverse solutions (different perspectives) and compare/contrast them (often via voting or aggregating) to improve final answer quality.",
            "iteration_type": "multiple-samples + contrast/vote (not iterative instruction-driven reflection)",
            "num_iterations": null,
            "task_name": "GSM8K, SVAMP, StrategyQA (baselines/comparisons)",
            "task_description": "Mathematical reasoning and commonsense reasoning benchmarks used in comparisons.",
            "evaluation_metric": "accuracy",
            "performance_before_reflection": "Initial response accuracies depend on base prompting; paper notes Self-Contrast outperformed IoRT on GSM8K with Llama2-7B and on SVAMP with GPT-3.5 in specific cases.",
            "performance_after_reflection": "Self-Contrast can achieve higher accuracy in some model/dataset combos but at cost of higher call overhead (paper states Self-Contrast incurred ~7.0% higher overhead than IoRT in one comparison).",
            "improvement_observed": "mixed",
            "limitations_or_failure_cases": "Gains come with increased API/LLM call overhead; not necessarily robust across all models/datasets. It addresses diversity by sampling but does not implement an instructor to stop or refresh iterations, so it can be less efficient than IoRT.",
            "uuid": "e6960.4",
            "source_info": {
                "paper_title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Self-Contrast: Better reflection through inconsistent solving perspectives",
            "rating": 2,
            "sanitized_title": "selfcontrast_better_reflection_through_inconsistent_solving_perspectives"
        },
        {
            "paper_title": "Self-Correcting with Tool-Interactive Critiquing (CRITIC)",
            "rating": 2,
            "sanitized_title": "selfcorrecting_with_toolinteractive_critiquing_critic"
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_really_improve_by_selfcritiquing_their_own_plans"
        },
        {
            "paper_title": "Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems",
            "rating": 1,
            "sanitized_title": "gpt4_doesnt_know_its_wrong_an_analysis_of_iterative_prompting_for_reasoning_problems"
        }
    ],
    "cost": 0.01799925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction
2 Mar 2025</p>
<p>Liping Liu liuliping@bupt.edu.cn 
Beijing University of Posts and Telecommunications</p>
<p>Chunhong Zhang zhangch@bupt.edu.cn 
Beijing University of Posts and Telecommunications</p>
<p>Likang Wu 
Tianjin University</p>
<p>Chuang Zhao 
The Hong Kong University of Science and Technology</p>
<p>Zheng Hu huzheng@bupt.edu.cn 
Beijing University of Posts and Telecommunications</p>
<p>Ming He heming01@foxmail.com 
AI Lab of Lenovo Research</p>
<p>Jianping Fan 
AI Lab of Lenovo Research</p>
<p>Yuntao Bai 
Andy Jones 
Kamal Ndousse 
Amanda Askell 
Anna Chen 
Nova Dassarma 
Dawn Drain 
Saurav Kadavath 
Sandipan Kundu 
Jackson Kernion 
Anna Chen 
Anna Goldie 
Azalia Mirhoseini 
Cameron Mckinnon 
Carol Chen 
Catherine Olsson 
Wenhu Chen 
Xueguang Ma 
Xinyi Wang 
William W Cohen 
Xinyun Chen 
Maxwell Lin 
Nathanael Schärli 
Denny 2024 Zhou 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Mark Chen 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Deep Ganguli 
Nicholas Schiefer 
Thomas I Liao 
Kamilė Lukošiūtė 
Anna Chen 
Luyu Gao 
Zhuyun Dai 
Panupong Pasupat 
Anthony Chen 
Arun Tejasvi Chaganty 
Yicheng Fan 
Vincent Zhao 
Ni Lao 
Jie Huang 
HuaixiuSwaroop Mishra 
Steven Zheng 
XinyAdams Wei Yu 
Geunwoo Kim 
Pierre Baldi 
Stephen Mcaleer 
Aman Madaan 
Niket Tandon 
Prakhar Gupta 
Skyler Hallinan 
Sarah Wiegreffe 
Uri Alon 
Nouha Dziri 
Shrimai Prabhumoye 
Yiming Yang 
Shashank Gupta 
Bodhisattwa Prasad Majumder 
Katherine Hermann 
Sean Welleck 
Yazdan- Bakhsh Amir </p>
<p>AI Lab of Lenovo Research</p>
<p>Aakanksha Chowdhery
Sharan Narang, Jacob Devlin, Gaurav Mishra, Hyung Won Chung, SebasMaarten Bosma, Adam Roberts, Paul Barham, Charles Sutton</p>
<p>Advances in Neural Information Processing Systems
36</p>
<p>Instruct-of-Reflection: Enhancing Large Language Models Iterative Reflection Capabilities via Dynamic-Meta Instruction
2 Mar 20256F0808647821980A7D7E3C14630786C1arXiv:2503.00902v1[cs.CL]Preprint, arXiv:2212.08073
Self-reflection for Large Language Models (LLMs) has gained significant attention.Existing approaches involve models iterating and improving their previous responses based on LLMs' internal reflection ability or external feedback.However, recent research has raised doubts about whether intrinsic self-correction without external feedback may even degrade performance.Based on our empirical evidence, we find that current static reflection methods may lead to redundant, drift, and stubborn issues.To mitigate this, we introduce Instruct-of-Reflection (IoRT), a novel and general reflection framework that leverages dynamic-meta instruction to enhance the iterative reflection capability of LLMs.Specifically, we propose the instructor driven by the meta-thoughts and self-consistency classifier, generates various instructions, including refresh, stop, and select, to guide the next reflection iteration.Our experiments demonstrate that IoRT achieves an average improvement of 10.1% over established baselines in mathematical and commonsense reasoning tasks, highlighting its efficacy and applicability.Our code is available at https://github.com/llp635/IoRT.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Touvron et al., 2023;Achiam et al., 2023;Anil et al., 2023;Claude, 2024) have demonstrated remarkable capabilities across various natural language processing tasks (Bai et al., 2022a;Wei et al., 2022a;Chu et al., 2023), particularly when leveraging a range of prompting strategies such as Chain-of-Thought (Wei et al., 2022b) which improve the reasoning primarily investigate performance changes in intrinsic reflection, we explore whether the performance of LLMs in reflection deteriorates when external feedback is available.Therefore, in our exploration experiments, we evaluate the performance of CRITIC (Gou et al., 2024), which leverages the results of tool interactions as external feedback, enabling a systematic investigation of how this feedback impacts the performance of reflection.Our findings ( § 3.1) indicate that neither intrinsic self-correction nor CRITIC performs satisfactorily.To investigate the reasons behind that, we comprehensively analyze the performance across multiple iterations of reflection, which is different from prior research focusing on a single reflection.Figure 1 shows that the i th reflection is derived from the (i − 1) th response, this static iterative reflection leads to three issues: 1) Redundant (✓⇒✓): Although it does not alter accuracy in iterations, it does increase overhead.2) Drift (✓⇒✗): Selfcorrection is highly random, undermining both accuracy and reliability.3) Stubborn (✗⇒✗): LLMs consistently persist in errors, failing to properly identify and correct potential mistakes.Additionally, we found that the prevalence of these three issues varies across different LLMs and tasks.</p>
<p>As a remedy to these challenges, we propose Instruct of Reflection (IoRT), an efficient and dynamic iterative reflection framework designed to continuously optimize the reflection process.The framework including three stages: (i) meta-thinker generates meta-thoughts, (ii) refreshing to generate an initial response, and reflector to self-reflect based on feedback.(iii) instruct the iteration.Notably, aiming to mitigate redundant, drift, and stubborn issues, in the third stage, the instructor, augmented by meta-thought and driven by the selfconsistency classifier, generates refresh, stop, or select instructions to direct the next reflection iteration.Specifically, when the Instructor determines that both responses have correctly addressed the problem, it generates a stop instruction to promptly halt the reflection process, effectively reducing redundancy.If the instructor assesses that both answers are incorrect, it generates a refresh instruction, helping the LLMs prevent stubbornness.In cases where answers are inconsistent, the instructor selects the more optimal response as the output of the current iteration and the foundation for the next reflection, thereby reducing drift and improving performance accuracy.</p>
<p>We evaluate the performance of IoRT across var-ious LLMs and two distinct tasks: mathematical reasoning and commonsense reasoning.The result demonstrates that IoRT outperforms baselines on established strong baselines, underscoring the critical role of the instructor in augmenting the iterative reflection capabilities of LLMs.Our main contributions can be summarized as follows:</p>
<p>• Our exploration experiments reveal that LLMs struggle to correct previous errors.The static iterative reflection suffers from redundant, drift, and stubborn, which may increase overhead or even deteriorate performance.</p>
<p>• We propose Instruct-of-Reflection (IoRT), a dynamic iterative reflection framework that integrates abstract reasoning into the reflection process, adaptively generating different instruction to regulate the reflection process.</p>
<p>• Experimental results on three datasets and five different LLMs underscore the efficacy, overhead efficiency, and generality of IoRT.Additionally, without relying on any oracle labels, IoRT demonstrates its practicality.</p>
<p>2 Related Works</p>
<p>Prompt-based Reasoning</p>
<p>To exploit the reasoning ability in LLMs, (Wei et al., 2022b) proposes Chain-of-Thought (CoT) prompting which guides the model to generate a series of text describing reasoning steps before reaching the final answer.Subsequently, (Chen et al., 2023) introduces Program-of-Thoughts (PoT) prompting which uses llms to generate programming language statements.In recent years, the self-correction capabilities of LLMs have garnered significant attention, with LLMs expected to iteratively improve their previous responses based on feedback (Bai et al., 2022b).Existing studies on self-correction can be categorized into two types based on the feedback source: external feedback and internal feedback (Pan et al., 2024).Internal feedback relies on the model's inherent knowledge and parameters, while external feedback involves inputs from humans, other models (Wang et al., 2023b;Paul et al., 2024b), or external tools and knowledge sources (Chen et al., 2024;Olausson et al., 2023;Gao et al., 2023).We comprehensively evaluate the performance of CRITIC (Gou et al., 2024), which leverages specialized tools, such as code executors, to evaluate and reflect on the external feedback obtained through its interactions with these tools.However, recent studies (Huang et al., 2024;Stechly et al., 2023;Valmeekam et al., 2023;Zhang et al., 2024) cast doubt on the ability of LLMs to correct previous responses based solely on their inherent reflection capacity.Additionally, research on reflection with external feedback (Kim et al., 2024b;Shinn et al., 2023) have been doubted on using oracle labels which are are often unavailable in practice to determine when to stop the selfcorrection loop during the reflection.The improvements vanish when the reflection no longer relies on oracle labels (Huang et al., 2024).Apart from these studies questioning the use of oracle in selfreflection during the iterative reflection process, we also observe that improvements in (Gou et al., 2024) stem from using oracle labels to correct only the incorrect samples during evaluation.Unlike previous findings, we observe that both intrinsic self-correction and external feedback can lead to a deterioration in performance after reflection.Additionally, our proposed Instruct of Reflection Tool (IoRT) diverges from existing reflection methods by emphasizing continuous performance optimization through diverse instructional guidance.We design the instructor to guide the next reflection iteration without relying on oracle labels.</p>
<p>Abstract Reasoning</p>
<p>Instead of addressing the problems directly, abstract reasoning relies on higher-level thoughts to guide thinking (Zheng et al., 2024), making it essential for sophisticated cognitive processing in artificial intelligence systems (Lake et al., 2017;Chollet, 2019;Qiu et al., 2024).For example, consider the question "What are the roots of a quadratic equation?"Abstract reasoning involves extracting the fundamental principle of "the quadratic equation root formula" and applying this principle to this specific question.(Zheng et al., 2024) uses the concepts and principles to guide reasoning by taking a step back.(Fu et al., 2024) guides the model to generate hints such as specific knowledge or key ideas.(Xiong et al., 2024) designs a preliminary study to quantify and delve into the abstract reasoning abilities of LLMs.(Yang et al., 2024) adapts by refining or retrieving high-level thought templates to instantiate specific reasoning structures.In our research, we build on (Fu et al., 2024) to generate meta-thoughts, thereby enhancing the abstract reasoning abilities of the reflective instructor.In contrast to the above works, which obtain an initial response through abstract reasoning, we incorpo- rate abstract reasoning into the reflection process by designing the instructor within our framework.</p>
<p>Empirical Evidence for Iterative Reflection</p>
<p>Prior research primarily focuses on performance changes in a single reflection, which fails to fully capture the potential of reflection for long-term improvements.To gain deeper insights into how reflection can gradually refine answers, We comprehensively evaluate the performance changes of self-correct and CRITIC (Gou et al., 2024) across multiple reflection iterations (Huang et al., 2024).</p>
<p>Performance of Iterative Reflection</p>
<p>We systematically evaluate the performance of selfcorrect (w/o external feedback) and CRITIC (w/ tool interactions as an available external feedback) across multiple iterative reflections on the GSM8K and SVAMP datasets with GPT-3.5.Additionally, we simulate an evaluation method (w/ oracle) that relies on oracle labels, focusing only on corrections from incorrect to correct answers in each iteration, while disregarding instances where correct answers are modified into incorrect ones.</p>
<p>In Figure 2, evaluating reflection using oracle labels, the accuracy of both self-correct and CRITIC improves steadily with each iteration.In contrast, without oracle labels, the performance is unstable and even deteriorate, regardless of the availability of external feedback.For instance, on GSM8K and SVAMP, the performances of self-correct and CRITIC drop by up to −2.4% and −3.0%, respectively.Notably, compared to self-correct, the performance is more reliable with CRITIC.</p>
<p>Further Performance Analysis</p>
<p>Why does performance degrade after reflection?To investigate this question, we further conduct the following analysis:</p>
<p>Step 1: We classify all samples in GSM8K into four categories based on the correctness before and after reflection.Then analyze the performance changes in each iteration using self-correct and CRITIC on GPT-3.5.</p>
<p>Step 2: We classify all samples into four categories based on the consistency and correctness of the answers throughout all iterations: 1) Redundant Iteration means the answers consistently remain correct.2) Invalid Consistent Iteration insists on a consistent wrong answer.3) Drift Iteration generates both incorrect and correct during the iterative reflection.4) Invalid Inconsistent Iteration continuous changes occur in iterations without ever converging to the correct solution.The detailed results are as follows: Reflection can not only be beneficial but also detrimental.As shown in Table 5, for both self-correct and CRITIC, the proportion of ✓⇒✗ is often comparable to or even exceeds that of ✗⇒✓, which explains the performance degradation after reflection.Notably, CRITIC demonstrates a lower ratio of ✓⇒✗ compared to self-correct, suggesting that external feedback promotes the stability of the reflection.</p>
<p>Models of different sizes yield varying reflection performance.In Figure 3, significant differences in reflection performance are observed across various LLMs.For instance, GPT-4 shows a 94.3% share of stable iterations, while 89% of LLaMA-2 7B's iterations are unstable.It implies that larger models like GPT-4, benefiting from vast training data and substantial computational resources (Ouyang et al., 2022;Chowdhery et al., 2023;Chung et al., 2024), exhibit more stable performance compared to smaller models.</p>
<p>Various reflective iterations correspond to different limitations.As shown in Figure 1, LLMs often perform the i th reflection based on the (i − 1) th response, and this static iterative reflection highlights the following three limitations: I. Redundant Iteration⇒ Redundant: Although re-dundant iterations do not affect accuracy, they contribute to unnecessary costs and delays.II.Drift Iteration⇒Drift: Especially, ✓⇒✗ indicates that LLMs often exhibit a high degree of uncertainty, leading to reflection drift and a decline in accuracy.III.Invalid iteration⇒Stubborn: LLMs are too stubborn to recognize and correct mistakes.Our method effectively addresses these three issues, achieving optimization of accuracy and efficiency in reflection during iterations.</p>
<p>Instruct-of-Reflection (IoRT)</p>
<p>Prior sections illustrate the challenges LLMs encounter in static iterative reflection such as redundancy, drift and stubborn.How to precisely identify effective reflections while mitigating the impact of harmful or redundant reflections?As a remedy, we innovatively propose Instruct-of-Reflection (IoRT), a novel framework that implements dynamic-meta instruction for the iterative reflection process.We can get an overview of the IoRT framework in Figure 4. Specifically, IoRT involves three main steps: generate meta-thoughts, refresh and selfreflect, and instruct the iteration.As the core module, the instructor is augmented by meta-thoughts and driven by self-consistency classifier, generating instructions including refresh, stop, and select, directing the next reflection iteration.</p>
<p>Generate Meta Thoughts</p>
<p>Human often summarize and abstract higher-level ideas when solving problems (Yang et al., 2024).Inspired by Hint-before-Solving Prompting (HSP) (Fu et al., 2024) and Meta-Reasoning (Zeng et al., 2024), we propose meta-thinker which enables LLMs to explicitly generate meta-thoughts through a few-shot learning for problem solving.Metathoughts are high-level knowledge based on abstract reasoning, encompassing analytical methods and fundamental overall strategies.They enhance the instructor's role as a teacher by providing a comprehensive overview of the reflection process.For instance, for the question "Can a honey bee sting a human more than once?", since the bee's stinger is closely related to its ability to sting, the meta-thought for this question primarily focuses on the basic principle of the "changes in the stinger".The meta memory module stores meta-thoughts, distilled from various tasks, recorded in the form of E = {(q i , m i )}, expressing them as a pair relationship e i = (q i , m i ), where q i represents the question statement and m i represents the meta-thought associated with solving question q i .Specifically, meta-thoughts construction includes 4 steps.Initialization: for each dataset, we randomly select k questions and manually define their meta-thoughts as shown in Table 6 to initialize the meta-thought module.Retrieval: For the input question x, meta-thinker retrieves the k most relevant prompt examples by applying the cosine similarity function S(q i , x) to calculate the embedding similarity between the questions q i and x, the top k questions and their corresponding metathoughts set are identified:
{e 1 sim , e 2 sim , . . . , e k sim } = argmax q i ∈M S(q i , x) (1) S(q i , x) = q i • x ∥q i ∥∥x∥(2)
Generation: Based on these k similar questions and their corresponding meta-thoughts, metathinker employs a model f (•) to generate metathought m x for the question x through few-shot learning, which can be expressed as:
m x = f e 1 sim , e 2 sim , . . . , e k sim , x(3)
Updating: The new meta-thought m x not only enhances the instructor's abstract reasoning capa-bilities but also facilitates the updating of the metamemory module, which continuously evolves by accumulating meta-thoughts for problem solving, thereby enabling more efficient and intelligent reasoning.In this context, the update of E can be formulated as:
E ← E ∪ {(x, m x )} (4)</p>
<p>Refresh and Self-Reflect</p>
<p>In the refresh process, given a black-box LLM g(•) and a question x, the LLM utilizes its few-shot contextual learning ability to generate an initial output response R 0 o .In the i th iteration, we extracted the basic answer
A i b from the basic response R i b .
The specifics of answer extraction from the response are elaborated in Appendix A. The reflector assesses the quality of R i b using evaluation metrics such as plausibility and correctness, and provides feedback accordingly.Based on the input question x, the basic response R i b , the basic answer A i b and the evaluation feedback f i , a reflective response R i r is generated as follows:
R i r = g(x, R i b , A i b , f i ) (5)</p>
<p>Instruct the Iteration</p>
<p>As analyzed in ( § 3), the answers after reflection may improve, degrade, or remain unchanged.</p>
<p>Therefore, we design the instructor to manage the reflection process, ensuring stable performance improvement throughout the iterative reflection.Meta-Thought Augment Instructor We incorporate the meta-thought m x into the instructor's prompt.During decision-making, the instructor treats the meta-thought as a critical evaluation criterion.By emphasizing abstract reasoning, the metathought elevates the LLMs from simply solving problems to instructing the process.This approach enables a more holistic evaluation of responses during each iteration, ensuring that the instructor's decisions not only satisfy problem-specific requirements but also undergo rigorous analysis.</p>
<p>Self-Consistency Classifier Does not use any LLM, self-consistency classifier determines consistency by comparing whether the basic answer A i b and the reflective answer A i r are equal.We implement a self-consistency classifier to evaluate the quality of responses, providing feedback to the instructor to guide decision-making.Based on the following three scenarios, the instructor generates a select, stop or refresh instruction:
• Select Instruction If A i b ̸ = A i r , itR i o = Instructor(R i b , A i b , R i r , A i r , m x , x) (6) If A i b = A i r
, it indicates that the reflection did not change the output, we designate R i b as the output for the i th iteration R i o .The Instructor will evaluates both R i b and R i r to determine whether to issue a stop or refresh instruction:</p>
<p>• Stop Instruction If the Instructor deems both R i b and R i r reasonable for solving the problem, a stop instruction is issued, and the iteration concludes at the i th round.In our experiments, we set the maximum number of iterations to N .To ensure comprehensive evaluation, all subsequent iterations are set equal to the response from the i th iteration, represented as:
R N o , R (N −1) o , . . . , R (i+1) o = R i o (7) • Refresh Instruction If R i b and R i r fail to resolve the problem, in the (i + 1) th iteration, a black- box LLM g(•) will generate a new response to update R (i+1) r . R (i+1) b is set equal to R i b . IoRT will then compare R (i+1) b
and R (i+1) r once again.Such invalid iterations can trap the model in a resource-intensive loop.By refreshing, we break this deadlock and introduce new ideas, enabling the model to generate more effective solutions.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Benchmark We evaluated our method in two reasoning scenarios: mathematical reasoning and commonsense reasoning.Specifically, for commonsense reasoning, we use the StrategyQA (Geva et al., 2021), while for mathematical reasoning, we use GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021).These three datasets are widely adopted in existing research, serving as standard benchmarks for evaluating reasoning capabilities.</p>
<p>Baselines We compare IoRT with the following strong baselines: 1. Chain-of-Thought prompting (CoT) (Wei et al., 2022b); 2. Self-Consistency (SC) (Wang et al., 2023c); 3. Plan-and-Solve Prompting (PS) (Wang et al., 2023a); 4. Multi-Agent Debate (Du et al., 2023;Liang et al., 2023); 5. Self-Contrast (Zhang et al., 2024); 6. Programof-thought (PoT) (Chen et al., 2023); 7. Hintbefore-Solving Prompting (Fu et al., 2024); 8. Self-Reflection (Shinn et al., 2023); 9. Self-Correcting with Tool-Interactive Critiquing (CRITIC) (Gou et al., 2024).For simplicity in notation, we use "CoT-SC(8)" and "PoT-SC(8)" to denote the approach that retrieves eight CoT or PoT reasoning chains to make majority vote.</p>
<p>Implementation Details In Figure 4, we use the GPT-3.5-Turbo-0613,GPT-4-0613, and Llama2-Chat models at three parameter scales (7B, 13B, and 70B) for black-box LLM and the reflector.Throughout the experiment, GPT-3.5-Turbo-0613serves as both the meta-thinker and the instructor.We uniformly set the temperature to 0.3 and the maximum number of iterations to 4. To ensure the reliability of the results, we conduct five rounds of experiments for each dataset and report the average scores as the final evaluation results.For evaluation metrics, we report accuracy.To evaluate the computational cost, we also report the average number of API/LLM calls (#Calls Avgs.) (Zhang et al., 2024) across three datasets, as well as the average number of tokens consumed per question (#Tokens Num.) for commonsense reasoning tasks.</p>
<p>Methods</p>
<p>GSM8K</p>
<p>SVAMP #Calls Avg.GPT-3.5 GPT-4 L-7B L-13B L-70B GPT-3.5 GPT-4 L-7B L-13B L-70B Table 2: The performance in the final iteration on commonsense reasoning.We utilize CoT to produce the initial responses.For the Multi-Agent, we configure three agents to engage in a three-round debate.</p>
<p>Main Results</p>
<p>Mathematical Reasoning.According to the results presented in Table 1, IoRT demonstrates superior performance improvement compared to the current strong baselines.Our method achieves an average improvement of f approximately +4.4% compared to PoT.In contrast, CRITIC, which also generate initial responses based on PoT, results in performance decreases of −2.6%.Besides, compared to the text describing reasoning baselines such as CoT, our method also achieve significant improvements of +8.9%.It is worth noting that IoRT achieves an average performance improvement of +2.4% compared to PoT+HSP directly incorporating abstract reasoning to generate an initial response.This indicates that integrating metathought into the instructor optimizes reflecting con-tinuously.Moreover, IoRT reduces average call overhead by approximately 27.6% compared to iterative reflection, demonstrating that our approach effectively balances accuracy and call overhead.</p>
<p>Although self-contrast outperforms our method on GSM8K with Llama2 7B and on SVAMP with GPT-3.5, this improvement comes at the cost of increased call overheads.Specifically, self-contrast incurs 7.0% higher overhead compared to IoRT.Commonsense Reasoning.Table 2 compares the performance of IoRT on commonsense reasoning tasks.We observe that IoRT outperforms the baseline across all models on StrategyQA.Specifically, our method gains +5.2% performance increase over CoT, whereas self-reflection results in a decrease of −6.0%.IoRT also achieves an average performance improvement of 2.1% compared to CoT+HSP on StrategyQA, demonstrating the efficacy of incorporating meta-thought into the reflection process.Additionally, with a token consumption of 3877, IoRT significantly reduces overhead compared to high-cost methods like self-reflection (5944) and CoT-SC(8) (4145) while maintaining competitive reasoning performance.</p>
<p>Ablation Study</p>
<p>We perform an ablation study to assess the impact of each steps on model performance: iteration R i o is modified as R i r .</p>
<p>• IoRT(w/o SC): It indicates that regardless of whether the responses R i b and R i r are consistent, the instructor only selects until reaches the maximum number of iterations N .</p>
<p>• IoRT (w/o MT): It implies that meta-thoughts are not provided and instructor relies solely on its own capacity to generate instructions.</p>
<p>As shown in Table 3, we find that modules and settings of IoRT positively impact accuracy and efficiency improvements.</p>
<p>Select instruction effectively mitigates drift in reflections.IoRT* exhibits a performance decrease of -4.4% compared to IoRT, emphasizing that LLMs frequently struggle with generation stability during reflection.Thus, selecting instruction is essential for successful iterative reflection.</p>
<p>Self-consistency has minimal impact on accuracy but significantly improves efficiency.IoRT(w/o SC) results in no clear change(-0.51%) in accuracy compared to IoRT, indicating that the instructor enables accurately identifying the better response during the iterations.However, in the experiment, IoRT (w/o SC) requires four iterations to stop, while the average number of iterations for IoRT is just 2.2, indicating that self-consistency can reduce overhead.</p>
<p>Meta-thought enhances the reasoning capabilities of the Instructor.IoRT (w/o MT) shows a −2.1% decline compared to IoRT but still achieves a +2.9% improvement over the initial response.This highlights the positive impact of our dynamic iterative framework on reflection, with metathoughts further enhancing the instructor's ability to accurately identify correct reflections.</p>
<p>Discussions</p>
<p>Performance Analysis.In § 3, our experiments indicate that the performance of CRITIC even deteriorates.In Figure 5 (right), IoRT achieves stable improvements across different LLMs.This is because CRITIC follows a static reflection pipeline: initial response → evaluation → revision.In contrast, IoRT adopts a dynamic iterative pipeline: initial response → evaluation → revision → instruction.By comparing Table 5 and Table 4, we observe that with each iteration of IoRT, the occurrences of ✓⇒✗ and ✗⇒✗ cases significantly reduced.The results indicate that IoRT can identify ✓ in drifting iterations through select instructions, or generate a refresh instruction to promptly terminate stubborn iterations, effectively ensuring stable improvement and optimization.</p>
<p>Overheads and Manual Efforts Analysis.In Figure 5 (left), we investigate the average number of API/LLM calls across various datasets on IoRT.</p>
<p>In our experiments, we set the maximum number of iterations to 4. Self-reflection and IoRT (w/o SC) were configured with a fixed number of iterations, leading to 9 and 14 call overheads, respectively.Compared to self-reflection and IoRT (w/o SC), IoRT achieves substantial improvements, reducing call overheads by more than 18.8% and 47.9%, respectively.Rather than simply halting reflection after the maximum number of iterations, IoRT integrates self-consistency checks and meta-thoughts, enabling the identification of correct responses and the generation of stop instructions to timely halt iterations, significantly reducing redundancy and overhead.Moreover, our method defines only three roles: meta-thinker, reflector, and instructor, which is considerably fewer than multi-agent debate.This simplification eases the manual effort required for pre-configuring agent roles.</p>
<p>Model Generality and Differences.As shown in Table 1, compared to CoT-SC(8), IoRT achieves significant performance improvements of 2.5% on GPT-3.5, 1.9% on GPT-4, 2.3% on Llama2 7B , 3.3% on Llama2 13B , and 5.8% on Llama2 70B , with an average improvement across the three datasets, indicating that IoRT demonstrates robust generality across both commercial LLMs (GPT) and open-source models (Llama-2).Moreover, these results indicate that IoRT often achieves more significant improvements with smaller-scale models, indicating its effectiveness in optimizing the performance of models with fewer parameters.This could be because larger-scale models produce more stable outputs, while smaller-scale models are more likely to generate varied responses through reflection, resulting in greater diversity in their outputs.In prior studies relying on static reflection methods, performance becomes unstable due to the influence of such diversity, and the ratio of ✓⇒✗ can lead to performance degradation.However, IoRT can quickly capture correct answers from diverse outputs, leading to notable performance improvements.The key difference enabling this is the select instruction in IoRT, which is unaffected by ✓⇒✗ and ensures stable performance improvement due to ✗⇒✓.Notably, Llama2 70B demonstrates a greater ability to generate accurate responses through reflection compared to Llama2 7B and Llama2 13B .With the smaller parameter sizes, Llama2 7B and Llama2 13B struggle to produce correct answers, leading to more frequent invalid and inconsistent iterations.</p>
<p>Analysis of Figure 5 (left) shows differences in call overheads for IoRT across various models.GPT averages 1.9 fewer LLM calls than Llama2, possibly because larger-scale models tend to produce more redundant iterations, so they often stop early with self-consistency checks.In contrast, smaller-scale models have less stable outputs, resulting in more iterations.</p>
<p>Conclusion</p>
<p>We conducted a comprehensive analysis of the iterative reflection performance of LLMs.The empirical evidence suggests that the performances of these reflection methods are unsatisfactory, primar-ily due to the limitations of static iterative reflection, which leads to redundant, drift, and stubborn issues.To mitigate this, we propose Instruct-of-Reflection (IoRT), a dynamic iterative reflection framework that integrates abstract reasoning into the reflection, generating adaptive instruction to regulate the iterative reflection.Experimental results demonstrate that IoRT surpasses a series of strong baselines and reduces overhead across a variety of scenarios and with different LLMs.</p>
<p>Limitations</p>
<p>Despite the significant improvements of our method over a series of strong baselines, it is not always able to provide perfect guidance during the reflection process.For instance, as shown in Figure 2, in experiments on mathematical reasoning with GPT-3.5, the final iteration performance, when evaluated using oracle labels, exceeds our method by 1.6%.This indicates that occasional misjudgments still occur in IoRT, though their frequency remains relatively low.Additionally, due to the current limitations of open-source models (Touvron et al., 2023) in abstract reasoning and guiding the reflection process, we did not use them as our metathinker and instructor in this experiment.In future work, we try to address these limitations by exploring fine-tuning, distillation, or other methods to enhance open-source models (Achiam et al., 2023), equipping them with robust abstract reasoning and instructional capabilities, potentially enabling them to match or even exceed the performance of commercial models.</p>
<p>A Datasets and Evaluation Metrics</p>
<p>Datasets Table 6 presents detailed information about the datasets used in our experiments, including the data source, total number of test samples, task type, answer type, and the number of samples used to generate meta-thoughts, initial responses and reflection prompts.</p>
<p>Evaluation Metrics Accuracy is used as the evaluation metric in our experiments.we use various methods to generate responses and exact answers in different tasks.For StrategyQA , we utilize Chain of Thought (CoT) to produce the response, then use regular expressions to extract the option following the phrase "so the answer is" and check if the extracted option matches the correct answer.For GSM8K and SVAMP, we employ the Program of Thought (PoT) to generate code for problem solving, which is then executed using a code executor to obtain the results.If the code contains errors, the program will raise an error and we define the results as None.Finally, we utilize the accuracy based on whether the results match the standard answers.The outputs from the two codes are different.Evaluate the following two Python code snippets based on the given question, output of the code and meta-thought, and recommend which code is better to solve this question and explain why.</p>
<p>B Complementary Experiments Results
Question: [Question x] Meta Thought: [m i ] Basic Response: [R i b ] Basic Answer: [A i b ] Reflect Response: [R i r ] Reflect Answer: [A i r ]
Better Response:</p>
<p>Select instruction (Commonsense Reasoning)</p>
<p>Evaluate the following two reasoning chain based on the given question and meta though.Recommend which reasoning chain is better and explain why.Better Response:</p>
<p>D Case Study</p>
<p>To better understand the IoRT working mechanism, we analyze cases in different scenarios.</p>
<p>Iterative Reflection Process Output</p>
<p>Question: Would a veteran of the Phillippine-American War come home craving SPAM?</p>
<p>Iteration 0 False (✓)</p>
<p>Generate Meta-thoughts:</p>
<p>Research when SPAM was introduced to the market and compare that to the timeline of the Philippine-American War to assess the likelihood of a veteran having access to or knowledge of SPAM upon returning home.</p>
<p>Initial Response:</p>
<p>Let's think step by step.The Philippine-American War occurred between 1899 and 1902, and SPAM was introduced in 1937.There was no direct relationship between these two events, so it's unlikely that a veteran of the Philippine-American War would come home craving SPAM.So the answer is: false.</p>
<p>Extract the Answer: False (✓) CoT 0</p>
<p>Self-Reflect:
Iteration 1 False (✓)
What's the problem with the above answer?</p>
<ol>
<li>
<p>Plausibility: The answer logically explains the lack of a direct relationship between the Philippine-American War and the introduction of SPAM in 1937, which makes it unlikely for a veteran of the war to come home craving SPAM.The reasoning is sound and aligns with historical timelines.</p>
</li>
<li>
<p>Truthfulness: The information provided is accurate.The timeline of the events mentioned is correct, and there is indeed no direct connection between the Philippine-American War and the introduction of SPAM in 1937.</p>
</li>
</ol>
<p>Here's the better answer: Let's think step by step.The Philippine-American War took place between 1899 and 1902, while SPAM was introduced in 1937, which means there is no direct correlation between the two events.Therefore, it is unlikely that a veteran of the Philippine-American War would come home craving SPAM.So the answer is: false.</p>
<p>Extract the Answer: False (✓) CoT 1</p>
<p>Instruct the Iteration (Stop):</p>
<p>Both COTs provided accurate and logical reasoning based on the question and meta thought provided.They correctly assessed the timeline of the events and concluded that there was no direct correlation between the Philippine-American War and the introduction of SPAM, making it unlikely for a veteran of that war to come home craving SPAM.Therefore, there is no need to regenerate a new answer as the current responses are appropriate.Decision: Stop iteration.</p>
<p>Figure 2 :
2
Figure 2: Compare the performance changes of selfcorrect and CRITIC during iterative reflection across two datasets, evaluating with and without Oracle.</p>
<p>Figure 3 :
3
Figure 3: Analyze the different types of iteration of CRITIC on GSM8K.</p>
<p>Figure 4 :
4
Figure 4: IoRT comprises three steps: (1) generate meta-thoughts, (2) refresh and self-reflect, and (3) instruct the iteration.Repeat steps 8 ⃝ to 14 ⃝ after each iteration until the instructor signals to stop or the maximum iterations N is reached.</p>
<p>suggests that at least one of the responses is incorrect.The instructor carefully evaluates both R i b and R i r based on the meta-thought m x , and then selects the better response from them as R i o represented as the output for the i th iteration.Concurrently, R i+1 b is updated to R i o , represented as:</p>
<p>Figure 5 :
5
Figure 5: Left: Comparison of the average number of API/LLM calls across various datasets on IoRT.Right: The performance of GPT and Llama-2 models on GSM8K with IoRT.</p>
<p>The outputs from the two codes are the same.Please evaluate the following two Python code snippets based on the question, output of the code and meta thought.Decide whether to refresh a new solution or to stop.The answers from the reasoning chains are the same.Please evaluate these two reasoning chains based on the following question and meta thought.Decide whether to refresh a new solution or to stop.</p>
<p>Table 1 :
1
(Zhang et al., 2024)oRT on mathematical reasoning.IoRT employs PoT to generate the initial responses.The best results are highlighted in bold.The text describing reasoning experimental results for CoT, CoT-SC(8), Multi-Agent, Self-Contrast, and Self-Reflection were sourced from(Zhang et al., 2024).L-denotes Llama2-chat.
Text Describing ReasoningCoT76.693.919.828.352.679.893.037.5 40.266.01PS75.794.120.330.457.883.693.345.2 53.770.91CoT+HSP80.194.020.732.358.983.792.940.4 50.372.52Self-Contrast84.495.420.542.364.289.094.044.554.675.37.8CoT-SC(8)83.594.221.437.661.184.692.545.2 53.772.08Multi-Agent83.893.523.834.959.684.193.242.5 49.270.19Self-Reflection75.895.117.031.849.380.591.536.1 42.563.09Programming Language ReasoningPoT78.394.019.530.558.682.993.443.5 53.075.21PoT+HSP84.094.917.035.164.986.394.743.6 55.178.42PoT-SC(8)83.794.422.537.764.387.093.845.3 56.077.88Self-Reflection75.894.515.725.655.582.590.133.8 47.970.79CRITIC77.393.714.326.856.784.293.938.2 45.672.09IoRT84.695.424.0 40.866.088.195.345.757.580.17.3MethodsGPT-3.5 GPT-4 L-7B L-13B L-70B #Token Num.CoT66.875.447.354.262.0514CoT-SC(8)69.177.049.457.163.64145CoT+HSP71.577.950.556.365.21018PS68.276.148.357.564.91090Multi-Agent68.576.650.856.865.73922Self-Reflection60.776.936.847.054.35944IoRT71.978.853.1 60.167.93877</p>
<p>•</p>
<p>IoRT<em>: This setting indicates the result of the i th
ModelDatasetsInitial IoRT</em>IoRT (w/o SC) (w/o MT) IoRTIoRTGMS8K78.380.684.483.084.6GPT-3.5SVAMP82.983.889.086.488.1StrategyQA66.865.971.670.571.9GMS8K94.094.194.694.395.4GPT-4SVAMP93.493.893.993.795.3StrategyQA75.477.077.776.678.8GMS8K19.516.323.521.524.0L-7BSVAMP43.543.445.144.245.7StrategyQA47.347.751.349.753.1GMS8K30.533.239.536.640.8L-13BSVAMP53.053.756.955.457.5StrategyQA54.252.260.558.760.1GMS8K58.661.866.364.166.0L-70BSVAMP75.276.178.877.980.1StrategyQA62.063.268.565.667.9</p>
<p>Table 3 :
3
Performance comparisons upon different components and settings under instructing reflection strategy.Evaluation on the final iteration.Initial indicates the performance of generating the initial response.</p>
<p>Table 4 :
4
Performance of IoRT with GPT-3.5 during iterations on GSM8K.
GSM8KIteration ✓⇒✓ ✓⇒✗ ✗⇒✓ ✗⇒✗0⇒171.2% 7.1% 5.1% 16.6%Self-Correct1⇒2 2⇒371.8% 4.5% 4.9% 18.8% 72.3% 4.4% 3.0% 20.3%3⇒473.0% 2.3% 2.8% 21.9%0⇒171.0% 7.3% 4.9% 16.8%CRITIC1⇒2 2⇒373.3% 2.6% 4.5% 19.6% 74.3% 3.5% 2.5% 19.7%3⇒475.4% 1.4% 1.9% 21.3%</p>
<p>Table 5 :
5
Performance of self-correct and CRITIC with GPT-3.5 during iterations on GSM8K.
C Prompting TemplateC.1 Prompt for Generate Meta ThoughtsGenerate Meta ThoughtsYou are a meta-thinker, skilled in abstract rea-soning. Given a question, you should gen-erate a meta-thought including the necessaryknowledge, analytical methods, and fundamen-tal strategies for solving the provided question.Meta-thought Example: [{e i sim }]Question: [Question x]Meta-thought:</p>
<p>Table 6 :
6
Detailed statistics of the datasets used in our experiment.Num.indicates the number of samples in each dataset.Ans Type describes the form of the answer.#Meta represents the number of few-shot examples used for generating meta-thoughts.# Refresh refers to the number of chain-of-thought exemplars used as few-shot prompts for commonsense reasoning tasks, or program-of-thought exemplars for mathematical reasoning tasks.# Reflect represents the number of reflection exemplars used as few-shot prompts.
C.2 Prompt for Refresh and Self-ReflectRefresh (Commonsense Reasoning)You are a knowledgeable expert in generalRefresh (Mathematical Reasoning)knowledge and common sense. Given you aYou are a mathematics and programming expert. Given a question, you should write python codequestion, you should think step by step and then generate the answer.to solve the following questions. Store yourQuestion: [Question x]result as a variable named 'answer'.Initial Response:Question: [Question x]Initial Response:Self-Reflect (Commonsense Reasoning)Self-Reflect (Mathematical Reasoning)You are a general knowledge and common sense expert. Given previous responses, whichYou are a mathematics and programming expert.include a question, a reasoning chain and theGiven previous responses, including a question,answer, you should carefully evaluate these re-a python code solution and the output of thissponses and provide detailed feedback.code, you should carefully evaluate these re-sponses and provide detailed feedback.Question: [Question x]Question: [Question x]Basic Response: [R i b ]Basic Response: [R i b ]Basic Answer: [A i b ]Basic Answer: [A i b ]Evaluation Feedback:Evaluation Feedback:You are a general knowledge and commonYou are a mathematics and programming ex-pert. Based on evaluation feedback including a question, a python code solution, the output of this code, and the code evaluation, you should generate a better code based on the feedback.sense expert. Based on evaluation feedback including a question, a reasoning chain, a an-swer and the evaluation feedback, you should generate a better reasoning chain based on the feedback.Question: [Question x]Question: [Question x]Basic Response: [R i b ]Basic Response: [R i b ]Basic Answer: [A i b ]Basic Answer: [A i b ]Evaluation Feedback: [f i ]Evaluation Feedback: [f i ]Reflect Response:Reflect Response:</p>
<p>Table 7 :
7
An illustrative example from StrategyQA demonstrating how IoRT evaluate the quality of responses and generates a stop instruction when the results before and after reflection are consistent.</p>
<p>AcknowledgementsWe appreciate the reviewers for their insightful comments and suggestions.This work was supported by the Beijing Association of Higher Education project MS2023151.Iterative Reflection ProcessOutput Question: Can you buy a fair trade laptop?Iteration 0 True (✗) Generate Meta-thoughts:Research the availability of fair trade electronics and the criteria for a product to be certified as fair trade.Consider the supply chains involved in laptop production and whether any companies offer laptops that meet fair trade standards.Initial Response:Let's think step by step.Fair trade principles are typically associated with agricultural or manufactured goods where workers are paid fair wages and operate in safe conditions.While there aren't fair trade-certified laptops per se, some companies may adhere to fair trade practices in the production of their electronics to ensure fair pay and safe working conditions for workers involved in their supply chain.However, it's less common in the electronics industry compared to agricultural products.So the answer is: true.Extract the Answer: True (✗) CoT 0 Self-Reflect:What's the problem with the above answer?1. Plausibility:The answer provides a reasonable explanation of fair trade principles and their application in the electronics industry.However, it does not definitively state whether fair trade laptops are available for purchase, leaving room for interpretation.Truthfulness:The information provided is accurate in explaining fair trade practices and their common association with agricultural products.While fair trade-certified laptops may not be prevalent, some companies do indeed prioritize fair trade principles in their electronics production to ensure ethical practices.Here's the better answer: Fair trade-certified laptops are not commonly available in the market due to the focus of fair trade principles on agricultural and manufactured goods.While some electronics companies may adhere to fair trade practices in their supply chain, specifically for components like minerals or labor conditions, the concept of fair trade laptops as a distinct certification is not widespread.So the answer is: false.Extract the Answer: False (✓) CoT 1Instruct the Iteration (Select):COT 1 provides a more accurate and precise response by directly addressing the question of fair trade-certified laptops and their availability in the market.It aligns closely with the meta thought provided and offers a clear 'false' conclusion based on the information presented.Better COT after comparison: COT 1 Table 8: An illustrative example of the better response selection of IoRT in StrategyQA.When the results before and after reflection are inconsistent, a select instruction is triggered automatically and the instructor then chooses the better response.Iterative Reflection ProcessOutput Question: Can a honey bee sting a human more than once?Iteration 0 False (✓) Generate Meta-thoughts:Research the physical structure of a honey bee's stinger and how it behaves when the bee stings a mammalian target.Consider what happens to the bee and its stinger after the stinging event.Initial Response:Let's think step by step.When a honey bee stings a human, its stinger gets embedded in the skin and is ripped off from the bee's body, causing the bee to die shortly after.So, honey bees can sting a human only once as they cannot survive after stinging.So the answer is: false.Extract the Answer: False (✓) CoT 0 Self-Reflect:What's the problem with the above answer?1. Plausibility:The answer explains the mechanism of a honey bee's stinger getting embedded and the bee dying after stinging.While this information is correct, it does not fully address the question of whether a honey bee can sting a human more than once.Truthfulness:The information provided about a honey bee's stinger getting ripped off and the bee dying after stinging is accurate.However, the answer fails to consider that not all bees behave the same way.Honey bees are unique in this aspect, but other types of bees, such as bumblebees, can sting multiple times without dying.Here's the better answer:Honey bees can sting a human only once as their stinger gets embedded in the skin and is ripped off, causing the bee to die.However, it's important to note that not all bees behave this way.For example, bumblebees can sting multiple times without dying.So the answer is: true.Extract the Answer: True (✗) CoT 1Instruct the Iteration (Select):Based on the question and meta-thought provided, CoT 0 is recommended as the better choice because it provides a logical, and directly relevant answer to the question without introducing unnecessary information about other bee species.It adheres closely to the meta thought provided and offers a concise response based on the specific behavior of honey bees.Better COT after comparison: CoT 0 Table 9: An illustrative example of the better response selection of IoRT in StrategyQA.When the results before and after reflection are inconsistent, a select instruction is triggered automatically and the instructor then chooses the better response.Iterative Reflection ProcessOutput Question: Is a doctorate required to teach at a SUNY School?Iteration 0 True (✗)Generate Meta-thoughts:Research the general requirements for faculty positions at State University of New York (SUNY) schools and the typical educational qualifications expected for teaching at the college level, particularly in the subject area of interest.Initial Response: Let's think step by step.SUNY schools are part of the State University of New York system.Each SUNY school may have different requirements for their faculty positions, but generally, a doctorate is a common requirement for teaching at the university level.So the answer is: true.Extract the Answer: True (✗) CoT 0 Self-Reflect:What's the problem with the above answer?1. Plausibility:The answer logically addresses the question by explaining that while specific SUNY schools may have varying requirements, having a doctorate is generally a common requirement for teaching at the university level.This explanation aligns with the common standards in academia.Truthfulness:The information provided is accurate.While there may be exceptions based on specific disciplines or positions within SUNY schools, having a doctorate is indeed a common requirement for teaching at the university level, including SUNY schools.Here's the better answer: Generally, having a doctorate is a common requirement for teaching at the university level, including SUNY schools.So the answer is: true.Extract the Answer: True (✗) CoT 1 Instruct the Iteration (Refresh): Both COTs provide similar answers stating that a doctorate is generally required to teach at a SUNY school.However, the answers lack depth in addressing the specific requirements for faculty positions at SUNY schools and the educational qualifications expected for teaching at the college level.To improve the response, a more detailed analysis based on the meta thought provided is necessary.Decision: Refreshing the response.RefreshIteration 2 False (✓)A doctorate is not always required to teach at a SUNY (State University of New York) school.While many professors at SUNY schools hold doctorate degrees, some instructors may have a master's degree or other qualifications depending on the specific requirements of the position and the subject being taught.So the answer is: false.Extract the Answer: False (✓) CoT 2Instruct the Iteration (Select):Based on the question and meta-thought provided, COT 2 provides a more nuanced and accurate answer by considering the variability in educational qualifications for teaching at SUNY schools, which aligns better with the research and understanding of the requirements for faculty positions at SUNY schools.Better COT after comparison: COT 2 Table 10: An illustrative example from StrategyQA demonstrating how IoRT generates a refresh instruction when the results before and after reflection are consistent.Iterative Reflection ProcessOutput Question: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM? Iteration 0 8.0 (✓)Generate Meta-thoughts:To determine how many centimeters shorter the candle will be after burning for a certain number of hours, you need to calculate the total time the candle burns and then multiply that by the rate at which the candle melts per hour.Initial Response: burning_hours = 5 -1 melting_rate_per_hour = 2 answer = burning_hours * melting_rate_per_hour Extract the Answer: 8.0 (✓) Code 0 Self-Reflect:What's the problem with the above code?1.The output of 8.0 seems reasonable, as the candle would be 8 centimeters shorter after burning from 1:00 PM to 5:00 PM.2. Let's check the code: burning_hours = 5 -1 melting_rate_per_hour = 2 answer = burning_hours * melting_rate_per_hour The code calculates the total burning hours and the melting rate per hour correctly, and then calculates the total length the candle would be shorter by multiplying the burning hours with the melting rate per hour.Overall, the code correctly calculates the length the candle would be shorter after burning from 1:00 PM to 5:00 PM.Here's a better code: burning_hours = 4 melting_rate_per_hour = 2 answer = burning_hours * melting_rate_per_hour Extract the Answer: 8.0 (✓) Code 1 Instruct the Iteration (Stop):Both code snippets correctly calculate the number of centimeters the candle will be shorter after burning for the specified hours.The calculations are consistent with the hint provided, and the outputs match the expected result.There is no need to regenerate a new answer as the current solution is clear, logical, and directly addresses the question.Decision: stop iteration.Iterative Reflection ProcessOutput Question: Gloria is shoe shopping when she comes across a pair of boots that fit her shoe budget.However, she has to choose between the boots and two pairs of high heels that together cost five dollars less than the boots.If one pair of heels costs $33 and the other costs twice as much, how many dollars are the boots?Iteration 0 94.0 (✗)Generate Meta-thoughts:To determine the cost of the boots, you need to calculate the total cost of the two pairs of high heels and then add five dollars to that amount.Initial Response: cost_of_high_heels_1 = 33 cost_of_high_heels_2 = 2 * cost_of_high_heels_1 total_cost_of_high_heels = cost_of_high_heels_1 + cost_of_high_heels_2 difference = 5 cost_of_boots = total_cost_of_high_heels -difference answer = cost_of_boots Extract the Answer: 94.0 (✗) Code 0 Self-Reflect:What's the problem with the above answer?1.The cost of the boots should be a positive number, 94 &gt; 0, it's reasonable.2. Let's check the code: cost_of_high_heels_1 = 33 cost_of_high_heels_2 = 2 * cost_of_high_heels_1 total_cost_of_high_heels = cost_of_high_heels_1 + cost_of_high_heels_2 It calculates the total cost of the two pairs of high heels, that's correct.difference = 5 cost_of_boots = total_cost_of_high_heels -difference This calculates the cost of the boots, it's correct.In summary, the code correctly calculates the cost of the boots.The code is correct and provides the expected output.Here's a better code: cost_of_high_heels_1 = 33 cost_of_high_heels_2 = 2 * cost_of_high_heels_1 total_cost_of_high_heels = cost_of_high_heels_1 + cost_of_high_heels_2 cost_of_boots = total_cost_of_high_heels + 5 answer = cost_of_boots Extract the Answer: 104.0 (✓) Code 1Instruct the Iteration (Select):Code 1 is more aligned with the hint provided as it correctly adds 5 dollars to the total cost of the two pairs of high heels to calculate the cost of the boots.This approach is consistent with the problem description and the meta thought given.Better Code after comparison: Code 1Table 12: An illustrative example of the better response selection of IoRT in GSM8K.When the results before and after reflection are inconsistent, a select instruction is triggered automatically and the instructor then chooses the better response.Iterative Reflection ProcessOutput Question: Richard lives in an apartment building with 15 floors.Each floor contains 8 units, and 3/4 of the building is occupied.What's the total number of unoccupied units In the building?Iteration 0 30.0 (✓)Generate Meta-thoughts:To calculate the total number of unoccupied units in the building, first determine the total number of units in the building.Then, find the number of occupied units.Finally, calculate the number of unoccupied units.Initial Response: total_floors = 15 units_per_floor = 8 occupancy_rate = 3/4 total_units = total_floors * units_per_floor occupied_units = total_units * occupancy_rate unoccupied_units = total_units -occupied_units answer = unoccupied_units Extract the Answer: 30.0 (✓) Code 0 Self-Reflect:What's the problem with the above answer?1.The total number of unoccupied units should be a whole number, 30.0 is not a whole number, so the answer is not reasonable.2. Let's check the code: total_floors = 15 units_per_floor = 8 occupancy_rate = 3/4 It calculates the total number of units, the number of occupied units, and then the number of unoccupied units, which is correct.answer = unoccupied_units The code assigns the number of unoccupied units to the variable answer, which is correct.The code seems to be correct, but the output is not a whole number.This could be due to the way the division is handled in Python.When dividing integers, Python returns a float if the result is not a whole number.To ensure we get a whole number as the output, we can use integer division.Here's a better code: total_floors = 15 units_per_floor = 8 occupancy_rate = 3/4 total_units = total_floors * units_per_floor occupied_units = total_units * occupancy_rate unoccupied_units = total_units -occupied_units answer = unoccupied_units Extract the Answer: 120.0 (✗) Code 1Instruct the Iteration (Select): Code 0 is the recommended choice because it follows the hint accurately by using floatingpoint division to calculate the occupancy rate.The output of Code 0 aligns with the problem statement and the correct calculation methodology provided in the hint.On the other hand, Code 1 deviates from the correct calculation by using integer division, leading to an incorrect result for unoccupied units.Better Code after comparison: Code 0Table 13: An illustrative example of the better response selection of IoRT in GSM8K.When the results before and after reflection are inconsistent, a select instruction is triggered automatically and the instructor then chooses the better response.Iterative Reflection ProcessOutput Question: Vincent can buy flowers in packages of 3 for $2.50 or in packages of 2 for $1. How much money does he save by buying 18 flowers at the better price?Iteration 0 3.0 (✗)Generate Meta-thoughts:To determine the savings Vincent can achieve by buying flowers at the better price, you need to calculate the cost per flower for each package option and then multiply the cost of the cheaper option by the total number of flowers to find the total cost.Initial Response: num_flowers = 18 price_3_flowers = 2.5 price_2_flowers = 1 total_price_3_flowers = (num_flowers // 3)<em>price_3_flowers remaining_flowers = num_flowers % 3 total_price_2_flowers = (remaining_flowers // 2) * price_2_flowers answer=(num_flowers</em>price_2_flowers)-(total_price_3_flowers+total_price_2_flowers) Extract the Answer: 3.0 (✗) Code 0 Self-Reflect:What's the problem with the above answer?1.The amount of money saved should be a positive number, 3 &gt; 0, it's reasonable.2. Let's check the code: &gt; num_flowers = 18 &gt; price_3_flowers = 2.5 &gt; price_2_flowers = 1 These lines define the number of flowers and the prices for different packages, that's correct.&gt; total_price_3_flowers = (num_flowers // 3) * price_3_flowers This line calculates the total cost of buying flowers in packages of 3, it's correct.&gt; remaining_flowers = num_flowers % 3 &gt; total_price_2_flowers = (remaining_flowers // 2) * price_2_flowers These lines calculate the total cost of buying flowers in packages of 2 for the remaining flowers, it's correct.&gt; answer = (num_flowers<em>price_2_flowers)-(total_price_3_flowers+total_price_2_flowers) This line calculates the amount of money saved by buying at the better price, it's correct.Overall, the code correctly calculates the amount of money saved by buying 18 flowers at the better price.Here's the better code: num_flowers = 18 price_3_flowers = 2.5 price_2_flowers = 1 total_price_3_flowers = (num_flowers // 3)</em>price_3_flowers remaining_flowers = num_flowers % 3 total_price_2_flowers = (remaining_flowers//2)* price_2_flowers money_saved = (num_flowers // 3) * price_3_flowers + (num_flowers % 3) * price_2_flowers -num_flowers * price_2_flowers answer = abs(money_saved) Extract the Answer: 3.0 (✗) Code 1Instruct the Iteration (Refresh):The error lies in both code segments not correctly implementing the meta-thought's guidance to compare the total costs of each purchasing option, ultimately resulting in incorrect calculations of Vincent's savings.Decision: Refreshing the response.Refresh Iteration 2 6.0 (✓) : num_flowers = 18 price_3_flowers = 2.5 price_2_flowers = 1 cost_using_3_flower_packages = (num_flowers / 3) * price_3_flowers + (num_flowers mod 3) * price_2_flowers cost_using_2_flower_packages = (num_flowers / 2) * price_2_flowers + (num_flowers mod 2) * (price_2_flowers / 2) money_saved = cost_using_2_flower_packages -cost_using_3_flower_packages answer = money_saved Extract the Answer: 6.0 (✓) Code 2 Instruct the Iteration (Select): COT 2 aligns better with the meta-thought of accurately calculating total costs for each buying strategy.Code 2 indicates a greater benefit from selecting the optimal option, ensuring Vincent maximizes his savings on flowers.Better Code after comparison: Code 2
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Processing Systems, NIPS '23. essing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc</p>
<p>Demystifying gpt self-repair for code generation. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, arXiv:2306.098962023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , Transactions of the Association for Computational Linguistics. 122024</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Refiner: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European Chapter2024a1Long Papers</p>
<p>REFINER: Reasoning feedback on intermediate representations. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, Proceedings of the 18th Conference of the European Chapter. the Association for Computational Linguistics. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics2024b1Long Papers</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling language models: Methods, analysis &amp; insights from training gopher. 2021arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.11366202329arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. 2023</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, 10.18653/v1/2023.acl-long.147Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a1</p>
<p>Tianlu Wang, Ping Yu, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Zettlemoyer, arXiv:2308.04592Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023b. Shepherd: A critic for language model generation. arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023c</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Transactions on Machine Learning Research. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022b35</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui, Qi Zhang, Xuanjing Huang, arXiv:2305.14497Self-polish: Enhance reasoning in large language models via problem refinement. 2023arXiv preprint</p>
<p>Meaningful learning: Advancing abstract reasoning in large language models via generic fact guidance. Kai Xiong, Xiao Ding, Ting Liu, Bing Qin, Dongliang Xu, Qing Yang, Hongtao Liu, Yixin Cao, arXiv:2403.090852024arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thoughtaugmented reasoning with large language models. 2024arXiv preprint</p>
<p>Exchange-of-thought: Enhancing large language model capabilities through cross-model communication. Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, Xipeng Qiu, 10.18653/v1/2023.emnlp-main.936Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Mr-gsm8k: A metareasoning benchmark for large language model evaluation. Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia, arXiv:2312.170802024Preprint</p>
<p>Self-contrast: Better reflection through inconsistent solving perspectives. Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu, 10.18653/v1/2024.acl-long.197Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Quoc V Chi, Denny Le, Zhou, The Twelfth International Conference on Learning Representations. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3813 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3813</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3813</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-411b16add23976ffcdf6422f932453f6ebcca119</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/411b16add23976ffcdf6422f932453f6ebcca119" target="_blank">EvoPrompting: Language Models for Code-Level Neural Architecture Search</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.</p>
                <p><strong>Paper Abstract:</strong> Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3813.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3813.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meyerson2023_symbolic_regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language model crossover: Variation through few-shot prompting (Meyerson et al., 2023) - symbolic regression mention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention of an approach that uses a large language model as a crossover operator (via few-shot prompting) to produce variations of text-based genotypes, including application to a toy symbolic regression domain where performance was reported comparable to state-of-the-art symbolic regression methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language model crossover: Variation through few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic regression / program-typed symbolic discovery (toy)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>mathematics / symbolic regression (toy domain)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>text-based genotypes / program strings produced by LM (few-shot prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LM used as a crossover operator via few-shot prompting to generate variations of text-based genotypes; diversity managed via MAP-Elites in the cited work (according to this paper's related-work discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic expressions / program representations (candidate equations/programs)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>toy symbolic regression tasks (as described by Meyerson et al.); no standard large-scale scholarly-paper corpus described here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper beyond the statement 'performance comparable to state-of-the-art' for the toy symbolic regression task (no numeric metrics provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The cited work reportedly demonstrated performance comparable to state-of-the-art approaches on a toy symbolic regression task; this paper only references that result and does not provide quantitative numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>According to this paper, the cited study was applied to toy symbolic regression (limited scope); it also used MAP-Elites to trade off quality and diversity, which requires a hand-designed descriptor space—this is mentioned as a difference from the present EvoPrompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>The cited work was compared to state-of-the-art on a toy symbolic regression task and reportedly achieved comparable performance; it differs from EvoPrompting in using MAP-Elites and in domain/application scope.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>No explicit failure cases or counterexamples for the symbolic regression application are reported in this paper's discussion of Meyerson et al.; the reference is brief.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompting: Language Models for Code-Level Neural Architecture Search', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3813.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3813.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lehman2022_evolution_through_large_models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolution through large models (Lehman et al., 2022) - LM as mutation operator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention of a study that fine-tunes a language model to produce Python code diffs conditioned on fixed messages and uses that fine-tuned LM as a mutation operator within an evolutionary algorithm; validated in the Sodarace domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolution through large models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>evolutionary program variation / domain-specific program search (Sodarace)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evolutionary design / simulated agents (Sodarace domain)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>code diffs / Python program changes (text/code)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tune an LM to produce Python diffs given simple fixed messages describing changes, then use the LM as a mutation operator inside an evolutionary algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>program modifications (Python diffs) that produce new candidate programs/individuals</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Sodarace domain (simulated agents) used for validation in the cited work; no large corpus-of-papers law-discovery dataset is mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in detail in this paper; evaluation is domain-specific (Sodarace performance) in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The cited study validated the LM-as-mutation-operator approach on the Sodarace domain; this paper reports that validation but does not provide numeric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Approach is demonstrated in a domain-specific setting (Sodarace); the present paper contrasts the cited work's use of fixed message prompts and mutation with EvoPrompting's use of crossover and prompt-tuning. No broader law-discovery claims are made.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>The cited work is distinguished from EvoPrompting in using an LM as a mutation operator with fixed messages and in its domain; direct comparisons beyond domain/approach differences are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>No explicit failure cases or negative examples from the cited study are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompting: Language Models for Code-Level Neural Architecture Search', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3813.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3813.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noorbakhsh2021_symbolic_math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained language models are symbolic mathematics solvers too! (Noorbakhsh et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention that large pretrained language models can perform symbolic mathematics tasks, indicating LMs' capability on technical mathematical problem-solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pretrained language models are symbolic mathematics solvers too!</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic mathematics solving</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>mathematics / symbolic computation</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>symbolic math problem statements / formatted math text (as used in symbolic math benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use of pretrained language models to solve symbolic mathematics problems (paper referenced for the claim that LMs can 'do math'); details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic solutions / mathematical derivations (answers to math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Not specified in this paper; referenced generally as part of related work on LMs doing math.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified here; the referenced work evaluates math-solving ability but numbers are not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This paper cites Noorbakhsh et al. as evidence that LMs can perform symbolic mathematics; no quantitative results are given in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper also cites work noting LLM limitations in arithmetic and symbolic induction (e.g., Qian et al., 2022), indicating that while LMs can solve some symbolic math, there are known failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>No detailed comparison to traditional symbolic solvers or symbolic-regression systems is provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>The paper references broader literature (Qian et al., 2022) documenting limitations in arithmetic and symbolic induction for LMs, but does not present specific counterexamples here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompting: Language Models for Code-Level Neural Architecture Search', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language model crossover: Variation through few-shot prompting <em>(Rating: 2)</em></li>
                <li>Evolution through large models <em>(Rating: 1)</em></li>
                <li>Pretrained language models are symbolic mathematics solvers too! <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3813",
    "paper_id": "paper-411b16add23976ffcdf6422f932453f6ebcca119",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [
        {
            "name_short": "Meyerson2023_symbolic_regression",
            "name_full": "Language model crossover: Variation through few-shot prompting (Meyerson et al., 2023) - symbolic regression mention",
            "brief_description": "Mention of an approach that uses a large language model as a crossover operator (via few-shot prompting) to produce variations of text-based genotypes, including application to a toy symbolic regression domain where performance was reported comparable to state-of-the-art symbolic regression methods.",
            "citation_title": "Language model crossover: Variation through few-shot prompting",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "task_type": "symbolic regression / program-typed symbolic discovery (toy)",
            "domain": "mathematics / symbolic regression (toy domain)",
            "input_data_type": "text-based genotypes / program strings produced by LM (few-shot prompts)",
            "method_description": "LM used as a crossover operator via few-shot prompting to generate variations of text-based genotypes; diversity managed via MAP-Elites in the cited work (according to this paper's related-work discussion).",
            "output_type": "symbolic expressions / program representations (candidate equations/programs)",
            "benchmark_or_dataset": "toy symbolic regression tasks (as described by Meyerson et al.); no standard large-scale scholarly-paper corpus described here.",
            "evaluation_metrics": "Not specified in this paper beyond the statement 'performance comparable to state-of-the-art' for the toy symbolic regression task (no numeric metrics provided here).",
            "results_summary": "The cited work reportedly demonstrated performance comparable to state-of-the-art approaches on a toy symbolic regression task; this paper only references that result and does not provide quantitative numbers.",
            "limitations_or_challenges": "According to this paper, the cited study was applied to toy symbolic regression (limited scope); it also used MAP-Elites to trade off quality and diversity, which requires a hand-designed descriptor space—this is mentioned as a difference from the present EvoPrompting approach.",
            "comparison_to_other_methods": "The cited work was compared to state-of-the-art on a toy symbolic regression task and reportedly achieved comparable performance; it differs from EvoPrompting in using MAP-Elites and in domain/application scope.",
            "notable_counterexamples": "No explicit failure cases or counterexamples for the symbolic regression application are reported in this paper's discussion of Meyerson et al.; the reference is brief.",
            "uuid": "e3813.0",
            "source_info": {
                "paper_title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Lehman2022_evolution_through_large_models",
            "name_full": "Evolution through large models (Lehman et al., 2022) - LM as mutation operator",
            "brief_description": "Mention of a study that fine-tunes a language model to produce Python code diffs conditioned on fixed messages and uses that fine-tuned LM as a mutation operator within an evolutionary algorithm; validated in the Sodarace domain.",
            "citation_title": "Evolution through large models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "task_type": "evolutionary program variation / domain-specific program search (Sodarace)",
            "domain": "evolutionary design / simulated agents (Sodarace domain)",
            "input_data_type": "code diffs / Python program changes (text/code)",
            "method_description": "Fine-tune an LM to produce Python diffs given simple fixed messages describing changes, then use the LM as a mutation operator inside an evolutionary algorithm.",
            "output_type": "program modifications (Python diffs) that produce new candidate programs/individuals",
            "benchmark_or_dataset": "Sodarace domain (simulated agents) used for validation in the cited work; no large corpus-of-papers law-discovery dataset is mentioned.",
            "evaluation_metrics": "Not specified in detail in this paper; evaluation is domain-specific (Sodarace performance) in the cited work.",
            "results_summary": "The cited study validated the LM-as-mutation-operator approach on the Sodarace domain; this paper reports that validation but does not provide numeric outcomes.",
            "limitations_or_challenges": "Approach is demonstrated in a domain-specific setting (Sodarace); the present paper contrasts the cited work's use of fixed message prompts and mutation with EvoPrompting's use of crossover and prompt-tuning. No broader law-discovery claims are made.",
            "comparison_to_other_methods": "The cited work is distinguished from EvoPrompting in using an LM as a mutation operator with fixed messages and in its domain; direct comparisons beyond domain/approach differences are not provided here.",
            "notable_counterexamples": "No explicit failure cases or negative examples from the cited study are reported in this paper.",
            "uuid": "e3813.1",
            "source_info": {
                "paper_title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Noorbakhsh2021_symbolic_math",
            "name_full": "Pretrained language models are symbolic mathematics solvers too! (Noorbakhsh et al., 2021)",
            "brief_description": "Mention that large pretrained language models can perform symbolic mathematics tasks, indicating LMs' capability on technical mathematical problem-solving.",
            "citation_title": "Pretrained language models are symbolic mathematics solvers too!",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "task_type": "symbolic mathematics solving",
            "domain": "mathematics / symbolic computation",
            "input_data_type": "symbolic math problem statements / formatted math text (as used in symbolic math benchmarks)",
            "method_description": "Use of pretrained language models to solve symbolic mathematics problems (paper referenced for the claim that LMs can 'do math'); details not provided in this paper.",
            "output_type": "symbolic solutions / mathematical derivations (answers to math problems)",
            "benchmark_or_dataset": "Not specified in this paper; referenced generally as part of related work on LMs doing math.",
            "evaluation_metrics": "Not specified here; the referenced work evaluates math-solving ability but numbers are not reported in this paper.",
            "results_summary": "This paper cites Noorbakhsh et al. as evidence that LMs can perform symbolic mathematics; no quantitative results are given in the present paper.",
            "limitations_or_challenges": "This paper also cites work noting LLM limitations in arithmetic and symbolic induction (e.g., Qian et al., 2022), indicating that while LMs can solve some symbolic math, there are known failure modes.",
            "comparison_to_other_methods": "No detailed comparison to traditional symbolic solvers or symbolic-regression systems is provided in this paper's discussion.",
            "notable_counterexamples": "The paper references broader literature (Qian et al., 2022) documenting limitations in arithmetic and symbolic induction for LMs, but does not present specific counterexamples here.",
            "uuid": "e3813.2",
            "source_info": {
                "paper_title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting",
            "rating": 2
        },
        {
            "paper_title": "Evolution through large models",
            "rating": 1
        },
        {
            "paper_title": "Pretrained language models are symbolic mathematics solvers too!",
            "rating": 1
        }
    ],
    "cost": 0.012317,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>EvoPrompting: Language Models for Code-Level Neural Architecture Search</h1>
<p>Angelica Chen*<br>New York University angelica.chen@nyu.edu</p>
<p>David M. Dohan ${ }^{\dagger}$ OpenAI<br>david@ddohan.com</p>
<p>David R. So ${ }^{\dagger}$<br>Jane Street<br>david.r.so.ai@gmail.com</p>
<h4>Abstract</h4>
<p>Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as general adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompttuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.</p>
<h2>1 Introduction</h2>
<p>Scaling of Transformers (Vaswani et al., 2017) has produced language models (LM) with impressive performance. Beyond achieving state-of-the-art results on conventional natural language processing tasks, these LMs demonstrate breakthrough technical capabilities, such as learning how to code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving reasoning problems (Wei et al., 2022). Yet, despite these strides, several works have noted LMs' current limitations in solving complex problems and creating novel solutions (Qian et al., 2022; Dakhel et al., 2022). In this work, we improve upon a base LM's ability to propose novel and diverse solutions to complex reasoning problems by iteratively evolving in-context prompts and prompt-tuning the LM. We call this technique EvoPrompting and demonstrate its success on the difficult task of deep learning architecture design. Our key finding is that, while LMs perform poorly at designing novel and effective neural architectures via naive few-shot prompting, EvoPrompting enables LMs to create novel and effective deep neural architectures, particularly when combined with prompt-tuning methods.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of EvoPrompting. After <em>initializing</em> the search with a handful of manually designed program seeds, the meta-learning loop begins. First, our code-pretrained LM uses the seeds as in-context prompt examples to <em>generate</em> candidate architectures. Those candidate architectures are then <em>trained</em> on the task training data and <em>evaluated</em> on the task validation set. Next, the most fit members of the population are <em>selected</em> as in-context examples for the next meta-learning loop and all evaluated individuals are used as training data for prompt-tuning the LM. From there, the meta-learning loop begins again.</p>
<p>EvoPrompting is based on the recently popularized practice of in-context prompting. Prompting is the technique of conditioning a LM’s decoded output on a custom prefix known as a <em>prompt</em>, which can include natural language task instructions or a few input-output examples. The prompt is used only at inference time and requires no gradient updates (Brown et al., 2020). In past work, prompting has been demonstrated to elicit impressive performance on a wide variety of tasks without requiring task-specific fine-tuning (Sanh et al., 2021; Wei et al., 2022; Kojima et al., 2022). Here, we leverage LM prompting for the task of designing improved deep learning architectures.</p>
<p>To engineer adequately powerful prompts, we draw inspiration from existing ideas in the field of neural architecture search. There, evolution has long been used to search over discrete spaces to efficiently discover improved deep learning architectures (Yao, 1999; Real et al., 2017). However, evolutionary approaches typically require careful manual design of a discrete search space (<em>e.g.</em> a small set of known convolutional neural network components, as in Real et al. (2017) or TensorFlow primitives, as in So et al. (2021)). As a result, the performance of the evolutionary algorithm is then sensitive to and possibly limited by the design of the search space. In EvoPrompting the LM’s vocabulary replaces the search space, which both increases the flexibility of the search and reduces reliance on manual design. The LM is also an <em>adaptive</em> mutation/crossover operator, in the sense that it can be improved round over round via prompt-tuning. Furthermore, EvoPrompting also improves on naive few-shot prompting by using an evolutionary search approach to iteratively improve the in-context examples for few-shot prompting.</p>
<p>To demonstrate the effectiveness of this method, we first do extensive testing and analyses on the relatively low-compute problem of MNIST-1D (Greydanus, 2020). The key finding of these experiments is that EvoPrompting is capable of producing conventional convolutional architectures superior to published manually designed models (Section 4.1). In Section 4.2 we then apply our method to the more challenging task of designing graph neural networks using problems from the CLRS Algorithmic Reasoning Benchmark (Veličković et al., 2022), where EvoPrompting generates novel architectures that outperform state-of-the-art models on 21 out of 30 algorithmic reasoning tasks (Appendix 3).</p>
<p>The contributions of this work are summarized as follows:</p>
<ol>
<li>We propose EvoPrompting, a method that utilizes evolutionary search to create and curate data to improve LM in-context prompting examples. Although this work focuses on the specific task of neural architecture design to develop this method, EvoPrompting is generally applicable to LM tasks that rely on in-context learning (ICL) or prompt-tuning.</li>
<li>A study applying LMs to code-level neural architecture design. Our experiments demonstrate that applying few-shot prompting alone to neural architecture design is unsuccessful, but few-</li>
</ol>
<p>shot prompting with EvoPrompting enables LMs to create architectures that outperform those designed by human experts.
3. Novel graph neural network architectures that were discovered using EvoPrompting. These architectures outperform the current state-of-the-art architecture, TripletGMPNN (Ibarz et al., 2022), on 21 out of 30 CLRS Algorithmic Reasoning Benchmark tasks (Appx. 3).</p>
<h1>2 Related Work</h1>
<p>LMs for code generation Scaling Transformers (Vaswani et al., 2017) is currently a popular route for reliably creating state-of-the-art natural language systems (Brown et al., 2020; Du et al., 2021; BigScience Workshop et al., 2022; Zhang et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). Many works have observed that large LMs are capable of performing technical tasks such as writing code (Chen et al., 2021), doing math (Noorbakhsh et al., 2021), and solving complex reasoning problems (Wei et al., 2022). Our work is most closely related to efforts that have applied LMs to coding tasks (Chen et al., 2021; Odena et al., 2021; Xu et al., 2022; Wang et al., 2021; Ahmad et al., 2021; Feng et al., 2020), since our technique proposes architectures in code.</p>
<p>Prompting Brown et al. (2020) demonstrated that LMs can be prompted with in-context examples to steer LM decoding towards solving problems in-context without gradient updates. Numerous works have utilized this prompting to further boost LM abilities (Sanh et al., 2021; Wei et al., 2022; Kojima et al., 2022). Others have focused on optimizing these prompts (Min et al., 2022; Liu et al., 2021) as via approaches such as augmentation with retrieval systems (Rubin et al., 2021), permutations of few-shot examples (Lu et al., 2021; Zhao et al., 2021), generating prompts via LMs (Zhou et al., 2022), and instruction-tuning (Wei et al., 2021; Ouyang et al., 2022; Sanh et al., 2021). From the perspective of Dohan et al. (2022), prompts are parameters that can be tuned using probabilistic inference techniques. Brooks et al. (2022) proposes using few-shot prompts to implement both the rollout policy and world model of a policy iteration algorithm. Our EvoPrompting method extends these efforts by proposing evolutionary search as a means to both better design prompts for ICL and tune the base LM to use the prompt more effectively.</p>
<p>Evolutionary Algorithms Our method is closely related to evolutionary neural architecture search (NAS) (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019; Liu et al., 2020), in which architectures are represented as discrete DNAs, and evolved and filtered based on fitness metrics that assess architecture performance. However, our method can search over arbitrary strings of code, whereas conventional evolutionary NAS algorithms rely on hand-crafted search spaces that can strongly bias and contrain the search (Li \&amp; Talwalkar, 2019; Sciuto et al., 2019; Bender et al., 2020; Real et al., 2020; So et al., 2021). A work close to ours is Lehman et al. (2022), in which an LM is fine-tuned to produce Python code diffs given one of three fixed messages that describe what should be changed, and then used as the mutation operator in an evolutionary algorithm. Their work is validated on the Sodarace domain. Our work differs in that we use an LM as a crossover operator, without specifying the class of changes to make, which may offer greater flexibility. Furthermore, we evaluate our approach on the real-world task of NAS, rely on mixed temperature sampling of the LM for diversity instead of using a QD algorithm, and also use prompt-tuning in our algorithm. We choose not to use a QD algorithm such as MAP-Elites since this approach requires the design and discretization of a descriptor space, which is complex and difficult to hand-design for the space of all possible neural networks.
Another concurrent work is Meyerson et al. (2023), which uses an LM as a crossover operator to produce variations of text-based genotypes in the domains of symbolic regression, text sentiment, images, and Sodaracer programs. Like Lehman et al. (2022), they use MAP-Elites to trade off quality with diversity in two of the domains and demonstrate that their overall algorithm reliably produces a diverse range of outputs. They additionally demonstrated performance comparable to state-of-the-art approaches on the toy task of symbolic regression. Their study varies from ours in a number of ways - we apply our algorithm to the real-world task of NAS, we optimize for a tradeoff between state-of-the-art task performance and model size, we condition on target performance in our prompts, we do not use MAP-Elites, and we use prompt-tuning to iteratively improve the LM's crossover abilities instead.</p>
<h1>3 EvoPrompting Method</h1>
<h3>3.1 Architecture search problem formulation</h3>
<p>Let our target task be denoted by $\mathcal{T}$ and $\mathcal{D}$ be a dataset consisting of input-output pairs $(x, y) \in \mathcal{D}$ for task $\mathcal{T}$. Define the probability distribution $\pi_{\theta}: \mathcal{V} \rightarrow{0,1}$ over vocabulary $\mathcal{V}$ as a language/code model parameterized by $\theta$, from which we can sample code segments $c \in \mathcal{V}^{<em>}$ (for $\mathcal{V}^{</em>}$ the Kleene closure of $\mathcal{V}$, i.e. the set of all concatenations of symbols in $\mathcal{V}$ ). We also have an evaluation function $\operatorname{Eval}<em _mathcal_T="\mathcal{T">{\mathcal{T}}(c, \mathcal{D}): \mathcal{V}^{<em>} \times \mathcal{D} \rightarrow \mathbb{R}$ that trains the model architecture given by code $c$ on $\mathcal{D}$ and outputs some real-valued fitness score $s \in \mathbb{R}$, which can be a function of model accuracy and other model characteristics. Our ultimate goal is to identify some set of code samples $c \sim \mathcal{V}^{</em>}$ that define neural network architectures that, when trained on $\mathcal{D}$, maximize the reward $\operatorname{EVAL}</em>)$.}}(c, \mathcal{D</p>
<h3>3.2 LMs for evolutionary crossover and mutation</h3>
<p>The goal of our algorithm is to generate a set $C$ consisting of $k$ neural network architectures that maximize the reward $\operatorname{EVAL}_{\mathcal{T}}(c, \mathcal{D})$ for arbitrary pairs of $(\mathcal{D}, \mathcal{T})$ :</p>
<p>$$
\arg \max <em _theta="\theta">{C=\left{\begin{array}{l}
c \mid c \sim \pi</em>
\end{array}\right}} \mathbb{E}<em _in="\in" _mathcal_D="\mathcal{D" _x_="(x," y_="y)">{c \in C} \mathbb{E}</em>)\right]
$$}}\left[\operatorname{EVAL}_{\mathcal{T}}(c, \mathcal{D</p>
<p>Since this optimization problem is generally intractable, we turn to a black-box evolutionary approach for iteratively generating, scoring, and selecting the best neural network architectures. Indeed, evolution has been demonstrated to perform particularly well in this domain because of how sparse high quality solutions tend to be (Real et al., 2017, 2018). Although evolution has been used for architecture search many times before (Real et al., 2017, 2018; Elsken et al., 2018; So et al., 2019), we improve upon this approach by using an LM for crossover and mutation operations.
Using an LM in this manner has multiple appealing properties. While past evolutionary approaches for neural architecture search have required careful design and specification of a discrete search space (e.g. the space of high level modules (Real et al., 2018; So et al., 2019), TensorFlow statements (So et al., 2021), or basic mathematical operations (Real et al., 2020)), our algorithm's search space includes any neural network architecture that can be represented in Python. This allows for greater flexibility and diversity of the output architectures, and reduces the amount of manual design and human bias involved in the algorithm. Furthermore, modern pre-trained LMs are typically trained on massive datasets containing a significant number of source code files. This pre-training process encodes useful knowledge about code structure and functionality that is not otherwise available in evolutionary algorithms. Lastly, LMs can also be used as self-adaptive crossover operators, in which the crossover operator is incrementally trained round after round to generate higher reward crossovers.</p>
<h3>3.3 EvoPrompting meta-learning algorithm</h3>
<p>Our complete algorithm is described in Algorithm 1. At the core of our algorithm is a scoring function, which describes the general "fitness" of a model on the task at hand. Since higher accuracy can often be achieved simply by increasing the number of parameters in a model, we use the negative product of the validation error and the model size as the fitness (see step 6 in Algorithm 3). More complicated objective functions have previously been used for dual objective neural architecture search (Bender et al., 2020), but we find this simple product works best in our case and requires minimal tuning. Generally the higher the fitness, the better (with some caveats, noted in our description of fitness-based selection below).
The end-to-end meta-learning algorithm has several stages, which we describe below:</p>
<p>Initialization We start by setting our global historical population $G$ to the empty list and initializing our current population $P$ with a few seed architectures that are known to be well-designed (step 3 in Algorithm 1), which warm-starts the search (So et al., 2019). These seed models are evaluated using the same $\operatorname{EVAL}_{\mathcal{T}}(c, \mathcal{D})$ function that is used to evaluate new candidate models (see below).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Complete meta-learning evolutionary algorithm using \(p_{\theta}\) as a crossover and mutation
operator.
    Input: \(\mathrm{LM} \pi_{\theta_{0}}\), dataset \(\mathcal{D}\), task \(\mathcal{T}, T\) number of rounds, \(m\) number of few-shot prompts per
        round, \(n\) number of samples to generate per prompt, \(k\) number of in-context examples per prompt,
        \(p\) number of survivors to select per generation, \(\alpha\) the upper threshold for the test error
        \(G \leftarrow[]\)
        \(P \leftarrow \operatorname{INITIALIZEPOPULATION}(p)\)
        \(t \leftarrow 0\)
        while \(t&lt;T\) do
            \(C \leftarrow \operatorname{CrossMut}\left(\pi_{\theta_{t}}, P, m, k, n\right)\)
            \(C_{\text {EVALED }} \leftarrow \operatorname{FILTERANDEVAL}(C, \mathcal{T}, \mathcal{D}, \alpha)\)
            \(G \leftarrow G+C_{\text {EVALED }}\)
            if \(t&lt;T-1\) then
                \(P \leftarrow \operatorname{GetTop}(G, p)\)
                \(\theta_{t+1} \leftarrow \operatorname{TRain}\left(\theta_{t}, C_{\text {EVALED }} \backslash P\right)\)
            end if
            \(t \leftarrow t+1\)
        end while
        Return \(\operatorname{GetTop}(G, p)\)
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Algorithm 2 The crossover and mutation algorithm, \(\operatorname{CrossMut}\left(\pi_{\theta_{t}}, P, m, k, n\right)\), where
Uniform \((P)\) denotes the uniform distribution over the set \(P\). The set of potential parents \(P\) consists
of the top examples from the previous round.
    Input: \(\mathrm{LM} \pi_{\theta}\), population of code samples and fitnesses \(P=\left\{(c, s) \mid c \in \mathcal{V}^{*}, \operatorname{EVAL}_{\mathcal{T}}(c, \mathcal{D})=\right.\)
    \(s\}, \(m\) number of few-shot prompts to create, \(k\) number of in-context examples in each prompt,
    and \(n\) number of samples to sample per prompt.
    \(C \leftarrow[]\)
    \(i \leftarrow 0\)
    while \(i&lt;m\) do
        \(E \leftarrow\left\{x_{j}\right\}_{j=1}^{k}\), where \(x_{j} \stackrel{i . i . d .}{\sim} \operatorname{Uniform}(P)\)
        \(p \leftarrow \operatorname{MAKEFEWSHOTPROMPT}(E)\)
        \(C_{i} \leftarrow\left\{c_{j}\right\}_{j=1}^{n}\), where \(c_{j} \stackrel{i . i . d .}{\sim} \pi_{\theta}(\cdot \mid p)\)
        \(C \leftarrow C+C_{i}\)
        \(i \leftarrow i+1\)
    end while
    Output: \(C\)
</code></pre></div>

<div class="codehilite"><pre><span></span><code>Algorithm 3 The algorithm for filtering and scoring child models, FilterANDEVAL \((C, \mathcal{T}, \mathcal{D}, \alpha)\).
    Input: set of code samples \(C\), task \(\mathcal{T}\), dataset \(\mathcal{D}\), evaluation function \(\operatorname{EVAL}_{\mathcal{T}}(c, \mathcal{D})\), upper
        threshold for error \(\alpha\)
    \(C_{\text {EVALED }} \leftarrow[]\)
    for c in \(C\) do
        c.error \(\leftarrow \operatorname{EVAL}_{\mathcal{T}}(\mathrm{c}, \mathcal{D})\)
        if c.error \(&lt;\alpha\) then
            \(s \leftarrow-\mathrm{c} . \operatorname{model} \_\operatorname{size} \times \mathrm{c} . \operatorname{error}\)
            \(C_{\text {EVALED }} \leftarrow C_{\text {EVALED }}+[(c, s)]\)
        end if
    end for
    Output: \(C_{\text {EVALED }}\)
</code></pre></div>

<p>Crossing over and mutating the parent models To mutate and apply crossover to the parents $P$ selected in the last step, we use both the source code and the evaluation metrics of each model in $P$ to create few-shot prompts.
In the last line of the prompt, we create a target set of metrics to condition $\pi_{\theta}$ 's generations on that indicate the desired validation accuracy and model size of the proposed architecture. We set the target</p>
<p>model size as $90 \%$ of the minimum model size of the parent models, rounded to the nearest 100 parameters, and the target validation accuracy as $102 \%$ of the maximum validation accuracy of the parent models, rounded to the nearest tenth of a percent. We create $m$ such prompts per round, each with $k$ in-context examples selected uniformly at random from $P$. An example of a prompt is shown in Listing 1.</p>
<div class="codehilite"><pre><span></span><code><span class="ss">&quot;&quot;&quot;Metrics:</span>
<span class="ss">{&#39;num_params&#39;: &#39;4800&#39;, &#39;val_accuracy&#39;: &#39;0.865&#39;}</span>
<span class="ss">&quot;&quot;&quot;</span>
<span class="k">class</span><span class="w"> </span><span class="n">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="k">Module</span><span class="p">)</span><span class="err">:</span>
<span class="nv">@nn</span><span class="p">.</span><span class="n">compact</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="ss">&quot;&quot;&quot;Metrics:</span>
<span class="ss">{&#39;num_params&#39;: &#39;4300&#39;, &#39;val_accuracy&#39;: &#39;0.880&#39;}</span>
<span class="ss">&quot;&quot;&quot;</span>
<span class="k">class</span><span class="w"> </span><span class="n">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="k">Module</span><span class="p">)</span><span class="err">:</span>
</code></pre></div>

<p>Listing 1: The format of our few-shot prompts. In practice we use 2-shot prompts but we omit the second in-context example here for brevity.</p>
<p>Finally, we use $\pi_{\theta}$ to generate $n$ samples per prompt, yielding a total of $n \times m$ child samples per round of evolution. We denote this portion of the algorithm as $\operatorname{CrossMut}\left(\pi_{\theta_{t}}, P, m, k, n\right)$ (Algorithm 2 and step 6 of Algorithm 1).</p>
<p>Filtering and scoring child samples To score and filter child samples $c$ generated by $\pi_{\theta}$, we use the evaluation function $\operatorname{Eval}_{\mathcal{T}}(c, \mathcal{D})$, which trains the model encoded by $c$ on the dataset $\mathcal{D}$ and returns the lowest validation error encountered during training. All child models are trained for the same number of steps, with the same optimizer hyperparameters. Since our fitness function can potentially be gamed by generating arbitrarily small models, we also add a validation error threshold $\alpha$, which is the upper limit of the validation error that a model can incur without being removed from $G$, the global population. We refer to this function as FilterAndEval $(C, \mathcal{T}, \mathcal{D}, \alpha)$ (Algorithm 3 and step 7 of Algorithm 1). Lastly, we add the remaining trainable models and their associated fitness scores into $G$ (step 8 of Algorithm 1).</p>
<p>Fitness-based selection After evaluating all child models in the current round, we apply fitnessbased selection to identify top candidate models for crossover (step 10 of Algorithm 1). We denote this as $\operatorname{GETTOP}(G, p)$, which refers simply to selecting the $p$ models with the highest fitness scores from $G$. Once these models have been selected, they are permanently removed from the population and cannot be used again as parents for crossover.</p>
<p>Training $\pi_{\theta_{t}}$ Lastly, all child models generated in the current round that were not previously selected for crossover (i.e. $C_{\text {EVALED }} \backslash P$ ) are used to prompt-tune $\pi_{\theta}$ for the next round (step 11 of Algorithm 1).</p>
<h1>4 Experiments and Results</h1>
<p>We evaluate our meta-learning algorithm on two datasets - MNIST-1D (Greydanus, 2020) and the CLRS algorithmic reasoning benchmark (Veličković et al., 2022). While the former benchmark is lightweight and permits us to do a more thorough analysis of our algorithm, the latter is a newer benchmark that covers 30 different algorithms with more headroom for discovering novel architectures with better performance.
In all of our experiments, our $\pi_{\theta_{0}}$ (i.e. the crossover operator) is a 62B parameter PALM model (Chowdhery et al., 2022) pre-trained on 1.3 T tokens of conversational, web, and code documents. It was additionally fine-tuned on a corpus of 64B tokens containing near-deduplicated, permissively-licensed Python source code files from Github. We always sample from $\pi_{\theta_{0}}$ with</p>
<p>mixed temperature sampling, in which the sampling temperature is selected uniformly from $[0.2,0.6,0.8,1.0]$. Between each round, the model is prompt-tuned (Lester et al., 2021) for 5 epochs with a soft prompt length of 16, batch size of 16, and learning rate of 0.1 (as described in Section 3.3 and Step 11 of Algorithm 1). Unless stated otherwise, we run 10 rounds of evolution with 10 prompts per round and 16 samples generated per prompt, yielding a total of 160 models generated per round and 1600 models generated during the entire search. Duplicate models and un-trainable models are not scored, but do count into the 1600. All other EvoPrompting hyperparameters are listed in Appendix A.1.</p>
<h1>4.1 MNIST-1D</h1>
<p>Dataset We apply our method first to MNIST-1D (Greydanus, 2020), a one-dimensional, scaleddown version of the MNIST-1D dataset containing examples that are 20 times smaller than the original MNIST dataset. Each example is only 40-dimensional, with 4000 examples in the training dataset and 1000 in test. Since there is no validation dataset, we randomly set aside 500 examples from the training dataset to use as the validation dataset. Despite being more lightweight, MNIST-1D distinguishes more between different architecture types (Greydanus, 2020) than its larger counterpart MNIST (LeCun et al., 1998).</p>
<p>Meta-learning set-up Throughout the model search we use the AdamW optimizer (Loshchilov \&amp; Hutter, 2019) to train each child model on a single NVIDIA Tesla P100 GPU for 8000 steps, with learning rate 0.01 and batch size 128. We score child models according to the best validation accuracy achieved during training. We also seed the search with 4 seed models - the 3 hand-designed neural baselines from the original MNIST-1D paper (Greydanus, 2020) (GRU, CNN, and MLP) and a fourth, larger CNN model of our own design. All four are implemented with Flax (Heek et al., 2020). We refer the reader to Appendix A. 2 for the source code of these seed models.</p>
<p>Baselines We compare EvoPrompting with the following baselines:</p>
<ul>
<li>Naive few-shot prompting: This baseline simply generates code samples $c \sim \pi_{\theta_{0}}(\cdot \mid p)$, where $p$ is a 2 -shot prompt constructed using in-context examples randomly selected from the seed models (Listing 1). This is essentially an ablation of steps 7-12 in Algorithm 1 with $T=1$. We increase the number of samples generated per prompt for the naive prompting baseline such that the total number of samples generated by $\pi_{\theta}$ matches that of the other baselines.</li>
<li>EvoPrompting ( - prompt-tuning): We run the entire algorithm as is, but without prompttuning between each round. This is an ablation of step 11 from Algorithm 1</li>
<li>EvoPrompting (random parents): Instead of selecting the most fit models from the last round as parents for the next round, we select parents randomly. This is an ablation of Step 10 in Algorithm 1, which is the $\operatorname{GetTOP}(G, p)$ step.</li>
</ul>
<p>EvoPrompting finds smaller and more accurate models Figure 2a shows a comparison of the test error and model size of the top 20 models discovered by EvoPrompting compared with those of our seed models and three baselines. The points approximate a Pareto frontier, below which each algorithm cannot improve on one dimension without hurting the other. EvoPrompting possesses the Pareto frontier closest to the origin, indicating that it finds more optimal models in terms of accuracy and size. In fact, many models in EvoPrompting's top 20 discovered models are orders of magnitude smaller than those of the other baselines, while still having lower test error.
We also note that - on this task in particular - EvoPrompting excels especially at optimizing convolutional architectures. Many of the top 20 models are narrower and deeper convolutional architectures, with smaller strides, less padding, and no dense layers. These models consistently perform better than the shallower, denser, and wider convolutional architectures seen in earlier rounds of the model search.
Another important aspect of a meta-learning algorithm is the relationship between the number of individuals evaluated and the maximum fitness observed so far, i.e. the sample efficiency. Neural architecture search can be an expensive process, with the most open-ended searches requiring the evaluation of trillions of individuals (Real et al., 2020). Thus, it is crucial to identify fit candidates</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Pareto frontiers of the model size versus test error of the top 20 experiments for each variation of the MNIST1D model search. Frontiers closer to the origin are considered more desirable.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Number of child models generated versus maximum fitness in sample, as estimated using 100 bootstrap samples of size 20 for each point along the x -axis.</p>
<p>Figure 2: EvOPrompting discovers smaller and better performing architectures on MNIST-1D than alternative search methods.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Number of child models generated versus maximum fitness of top model seen so far (as estimated using 100 bootstrap samples of size 20 for each point along the x -axis) when searching over neural network models for three CLRS tasks. As mentioned in Section 4.2, these algorithms were selected because our preliminary analyses indicated that they had the most headroom for architectural improvements.
using as few samples as possible. Figure 2b compares how the fitness of the best-performing child model improves as a function of the number of child samples generated thus far. The random parents baseline plateaus the quickest, reaching a maximum fitness by the time approximately 200 individuals have been generated. Furthermore, the maximum fitness it reaches is significantly worse than that of the other experiments. On the other hand, EvOPrompting without prompt-tuning and normal EvOPhOMPTING do not plateau until much later on. EvOPrompting's plateau is the highest and therefore fitter on average than the individuals discovered by any of the other experiments.
It is also evident from both Figure 2a and 2b that performance suffers when any individual component is removed. Interestingly, Figure 2a indicates that prompting with randomly selected parents combined with prompt-tuning is no more effective than naive prompting alone. This highlights the importance of selecting helpful in-context examples, particularly in a task for which we assume that less training signal exists in the pre-training data. However, selecting more fit models as in-context examples without prompt-tuning also does not perform nearly as well as our full method.</p>
<p>Trajectory over meta-learning rounds We also explored the trajectory of our meta-learning algorithm round over round, as shown in Appendix A.3. In general, we observe that EvOPrompting starts out further away from the origin (in round 0 ) and ends up closest to the origin in round 10, which signifies that it discovers - on average - the smallest and most accurate models in the last round. However, the search does not always yield improvements on both axes between consecutive rounds. In rounds $0-2$ and $6-10$, EvOPrompting improves test error while trading off model size. On the other hand, both dimensions are simultaneously improved upon in rounds 3-5.</p>
<h1>4.2 CLRS</h1>
<p>Although the MNIST-1D task offers an efficient and practical setting for evaluating a meta-learning algorithm, CNN architectures already perform fairly well on this task and neural image classification architectures have been extensively studied as a whole. There also exists the possibility that our LM has seen many convolutional architectures in its pre-training data. Instead, we turn to a different learning task and class of neural network architectures in order to assess whether our meta-learning framework generalizes to other tasks, datasets, and neural architectures.</p>
<p>Dataset The CLRS algorithmic reasoning benchmark (Veličković et al., 2022) evaluates the ability of neural networks to learn algorithmic reasoning across a set of 30 classical algorithms covered in the Introduction to Algorithms textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009). This benchmark is useful not only as a difficult logical reasoning task for neural networks, but also as a measure of a neural network's algorithmic alignment (Xu et al., 2020). In brief, algorithmic alignment refers to a model's ability to reason like an algorithm (i.e. using the computation graph for a task), rather than relying upon memorization or other less sample efficient learning strategies. Although a model can approximate an algorithm by pattern-matching against similar inputs or relying on other shortcuts, it cannot generalize to arbitrarily long inputs or edge cases without learning the computation graph underlying the algorithm.
Accordingly, the CLRS benchmark represents the algorithms' inputs and outputs as graphs, and the steps of the algorithm as a trajectory of operations over the input graph. This problem setup can be straightforwardly processed by graph neural networks, which is explored in Ibarz et al. (2022). They find that a Triplet-GMPNN model (a message-passing neural network (Gilmer et al., 2017) with gating and triplet edge processing) exhibits the best performance when trained and evaluated across all 30 algorithms at once.</p>
<p>Table 1: A comparison of OOD accuracy and model size (in number of parameters) of models newly discovered by EvOPROMPTING on select CLRS tasks where EvOPROMPTING has discovered more accurate architectures without large increases in model size, compared with the baseline model (the Triplet-GMPNN from Ibarz et al. (2022)). OOD accuracy numbers for the baseline model are from Ibarz et al. (2022). For the full table of results on all CLRS tasks, including accuracies of our own implementation of the Triplet-GMPNN, see Appendix 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">CLRS Task</th>
<th style="text-align: center;">Best Performing Model</th>
<th style="text-align: center;">Model Size $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OOD Accuracy $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Baseline</td>
</tr>
<tr>
<td style="text-align: left;">Articulation Points</td>
<td style="text-align: center;">QuADNOdEMINMAX</td>
<td style="text-align: center;">497969</td>
<td style="text-align: center;">531913</td>
<td style="text-align: center;">$\mathbf{9 3 . 5} \pm \mathbf{1 . 8 \%}$</td>
<td style="text-align: center;">$88.3 \pm 2.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BFS</td>
<td style="text-align: center;">MAXMEAN</td>
<td style="text-align: center;">522931</td>
<td style="text-align: center;">523963</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0 \%}$</td>
<td style="text-align: center;">$99.7 \pm 0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Bubble Sort</td>
<td style="text-align: center;">CONCATREP</td>
<td style="text-align: center;">568533</td>
<td style="text-align: center;">524477</td>
<td style="text-align: center;">$\mathbf{8 8 . 9} \pm \mathbf{2 . 8 \%}$</td>
<td style="text-align: center;">$67.7 \pm 5.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">DFS</td>
<td style="text-align: center;">DIV2MEAN</td>
<td style="text-align: center;">660158</td>
<td style="text-align: center;">661190</td>
<td style="text-align: center;">$\mathbf{6 8 . 1} \pm \mathbf{1 . 4 \%}$</td>
<td style="text-align: center;">$47.8 \pm 4.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Floyd Warshall</td>
<td style="text-align: center;">CONCATREP</td>
<td style="text-align: center;">669145</td>
<td style="text-align: center;">625089</td>
<td style="text-align: center;">$\mathbf{6 1 . 4} \pm \mathbf{0 . 8 \%}$</td>
<td style="text-align: center;">$48.5 \pm 1.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Heapsort</td>
<td style="text-align: center;">CONCATREP</td>
<td style="text-align: center;">703710</td>
<td style="text-align: center;">659654</td>
<td style="text-align: center;">$\mathbf{6 9 . 9} \pm \mathbf{4 . 2 \%}$</td>
<td style="text-align: center;">$31.0 \pm 5.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Insertion Sort</td>
<td style="text-align: center;">DIV2MEAN</td>
<td style="text-align: center;">523445</td>
<td style="text-align: center;">524477</td>
<td style="text-align: center;">$\mathbf{8 9 . 5} \pm \mathbf{2 . 6 \%}$</td>
<td style="text-align: center;">$78.1 \pm 4.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Quicksort</td>
<td style="text-align: center;">DIV2MEAN</td>
<td style="text-align: center;">524727</td>
<td style="text-align: center;">525759</td>
<td style="text-align: center;">$\mathbf{8 5 . 2} \pm \mathbf{4 . 3 \%}$</td>
<td style="text-align: center;">$64.6 \pm 5.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Task Scheduling</td>
<td style="text-align: center;">TANHEXPANDTRIPLETS</td>
<td style="text-align: center;">262333</td>
<td style="text-align: center;">262333</td>
<td style="text-align: center;">$\mathbf{8 8 . 2} \pm \mathbf{0 . 4 \%}$</td>
<td style="text-align: center;">$87.3 \pm 0.4 \%$</td>
</tr>
</tbody>
</table>
<p>Meta-learning set-up Similar to our MNIST-1D set-up, we use the AdamW optimizer to train each child model on a single NVIDIA Tesla P100 GPU. However, since most of the explored child models were much larger than the MNIST-1D models, we only trained each child model for 2000 steps. Anecdotally, we observed that the performance of different models often diverged by 2000 steps, which provided sufficient signal for the model search process. We otherwise followed the hyperparameters for single-task training in Ibarz et al. (2022) and evaluated models using validation accuracy.
Unlike our MNIST-1D set-up, we only search over the triplet representations of a Triplet-GMPNN model (see Ibarz et al. (2022) for more details), rather than the entire graph processor. We also seed the search with nine different seed models - each a variant of a Triplet-GMPNN model with a different triplet representation. Each seed triplet representation incorporates a minor tweak of a single component of the original triplet representation designed by Ibarz et al. (2022). These include a fully-connected output layer, a sum aggregation, fully-connected node/edge/graph representations,</p>
<p>a simple linear triplet representation, and a bilinear representation (Mnih \&amp; Hinton, 2007). All nine are implemented with Haiku (Hennigan et al., 2020), an object-oriented neural network library for Jax (see Appendix A. 5 for the source code of the seed models.)</p>
<p>Generalizing beyond image classification models We search using EvoPrompting on 3 individual algorithms in the CLRS benchmark - the articulation points, Graham scan, and Kruskal's minimum spanning tree algorithms. We select these algorithms because our preliminary analyses with hand-designed architectures showed that they had the most headroom for improvement, although we found that the discovered architectures transfer well to other CLRS benchmark tasks as well (Appx. 3). Our search results are shown in Figure 3. EvoPrompting continues to find models that are more "fit" than our other two baselines, though we observed that the results also show more variation than our results for MNIST-1D did.</p>
<p>Analyzing newly discovered models Our search across triplet representations yielded several new designs that we sought to evaluate across all algorithms in the CLRS benchmark. Although these new models were discovered in model searches over single algorithms, they oftentimes generalized to other algorithms that were unseen during the model search. Figure 5 shows the trajectory of validation accuracy during training and Table 1 provides OOD accuracies for these models on a few select algorithms. (We defer the reader to Appendix A. 4 for the full source code of each newly discovered model and Table A. 6 for the full list of OOD accuracies for every algorithm in the CLRS benchmark.)
We note that the model search suggested several simple but effective changes. For example, instead of taking the maximum of the triplet representation, the QuadNodeMinMax model uses quadruplet node representations instead of triplets, and it subtracts the minimum of the quad representation from the max instead. CONCATREP represents the node, edge, and graph representations as a concatenation of a projection feedforward layer, and MAXMEAN takes the maximum of the triplet representations prior to taking the mean and passing it through the output dense layer. Div2MEAN scales each of the node representations by $1 / 2$ and uses a mean aggregation of the triplet representations instead of the max aggregation. TANIEXPANDTRIPLETS applies additional dimension expansion to the triplet representations and applies a hyperbolic tangent function after the max aggregation. See Appx. A. 4 for the full code of each discovered model.
Of the 5 newly discovered models that we chose to analyze, CONCATREP is the only one that increases model size. However, as shown in Table 1, CONCATREP frequently yielded improvements in OOD accuracy that far exceeded the percent increase in model size. For instance, on the heapsort algorithm CONCATREP increased OOD accuracy by $125.19 \%$ while only increasing model size by $6.68 \%$ over the baseline. The other four newly discovered models shown in Table 1 simultaneously improved OOD accuracy while decreasing model size on the articulation points, BFS, DFS, insertion sort, quicksort, and task scheduling algorithms. On the rest of the CLRS algorithms (Table A.6), our newly discovered models typically achieved OOD accuracy comparable to or better than the baseline, while maintaining similar model size.</p>
<h1>5 Conclusion</h1>
<p>We have shown that embedding a pre-trained LM in an evolutionary algorithm significantly improves the LM's performance on the task of neural architecture design. Our approach has demonstrated success at not only optimizing convolutional architectures for the MNIST-1D task, but also at developing new kinds of GNNs for the CLRS algorithmic benchmark. This demonstrates: 1) using evolutionary techniques can vastly improve the in-context capabilities of pre-trained LMs, and 2) EvoPrompting can discover novel and state-of-the-art architectures that optimize for both accuracy and model size. Furthermore, EvoPrompting is general enough to be easily adapted to search for solutions to other kinds of reasoning tasks beyond NAS. We leave the adaptation of EvoPrompting for other tasks to future work.
However, our study is limited by the lack of an extensive comparison against other standard NAS techniques because EvoPrompting was designed for open-ended search, whereas other techniques were not, which would introduce a potential confounder. We include one such comparison on NATS-Bench in Appendix A.7, as well as a discussion of the confounders thereof.</p>
<h1>6 Acknowledgements</h1>
<p>We thank Maarten Bosma, Kefan Xiao, Yifeng Lu, Quoc Le, Ed Chi, Borja Ibarz, Petar Veličković, Chen Liang, Charles Sutton, and the Google Brain AutoML team for providing valuable discussions and feedback that influenced the direction of this project. We also thank the Google Student Researcher program for providing the resources and opportunities necessary for this project to take place.</p>
<h2>References</h2>
<p>Ahmad, W. U., Chakraborty, S., Ray, B., and Chang, K.-W. Unified pre-training for program understanding and generation. ArXiv, abs/2103.06333, 2021.</p>
<p>Bender, G., Liu, H., Chen, B., Chu, G., Cheng, S., Kindermans, P.-J., and Le, Q. V. Can weight sharing outperform random architecture search? an investigation with tunas. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14311-14320, 2020.</p>
<p>BigScience Workshop, :, Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurençon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D. I., Radev, D., Ponferrada, E. G., Levkovizh, E., Kim, E., Natan, E. B., De Toni, F., Dupont, G., Kruszewski, G., Pistilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I., Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J., Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Von Werra, L., Weber, L., Phan, L., allal, L. B., Tanguy, L., Dey, M., Muñoz, M. R., Masoud, M., Grandury, M., Šaško, M., Huang, M., Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M. C., Jauhar, M. A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel, O., de Gibert, O., Villegas, P., Henderson, P., Colombo, P., Amuok, P., Lhoest, Q., Harliman, R., Bommasani, R., López, R. L., Ribeiro, R., Osei, S., Pyysalo, S., Nagel, S., Bose, S., Muhammad, S. H., Sharma, S., Longpre, S., Nikpoor, S., Silberberg, S., Pai, S., Zink, S., Torrent, T. T., Schick, T., Thrush, T., Danchev, V., Nikoulina, V., Laippala, V., Lepercq, V., Prabhu, V., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si, C., Taşar, D. E., Salesky, E., Mielke, S. J., Lee, W. Y., Sharma, A., Santilli, A., Chaffin, A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Strobelt, H., Fries, J. A., Rozen, J., Gao, L., Sutawika, L., Bari, M. S., Al-shaibani, M. S., Manica, M., Nayak, N., Teehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S. H., Kim, T., Bers, T., Fevry, T., Neeraj, T., Thakker, U., Raunak, V., Tang, X., Yong, Z.-X., Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H. W., Tae, J., Phang, J., Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M., Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N., Tazi, N., Sanseviero, O., von Platen, P., Cornette, P., Lavallée, P. F., Lacroix, R., Rajbhandari, S., Gandhi, S., Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A., Ligozat, A.-L., Subramonian, A., Névéol, A., Lovering, C., Garrette, D., Tunuguntla, D., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G. I., Schoelkopf, H., Kalo, J.-C., Novikova, J., Forde, J. Z., Clive, J., Kasai, J., Kawamura, K., Hazan, L., Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal, O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S., Shavrina, T., Scialom, T., Yun, T., Limisiewicz, T., Rieser, V., Protasov, V., Mikhailov, V., Pruksachatkun, Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A., Khan, A., Faranak, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi, A., Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B., Saxena, B., Ferrandis, C. M., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen, D. A., Tan, E., Baylor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I., Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J. B., Dutra, L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinlolu, M., McKenna, M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy, N., Samuel, O., An, R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S., Roy, S., Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap, A. R., Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A., Beilharz, B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C.,</p>
<p>Fourrier, C., Periñán, D. L., Molano, D., Yu, D., Manjavacas, E., Barth, F., Fuhrimann, F., Altay, G., Bayrak, G., Burns, G., Vrabec, H. U., Bello, I., Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J. D., Sivaraman, K. R., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovetz, M. H., Takeuchi, M., Pàmies, M., Castillo, M. A., Nezhurina, M., Sänger, M., Samwald, M., Cullan, M., Weinberg, M., De Wolf, M., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam, N., Dahlberg, N., Broad, N. M., Muellner, N., Fung, P., Haller, P., Chandrasekhar, R., Eisenberg, R., Martin, R., Canalli, R., Su, R., Su, R., Cahyawijaya, S., Garda, S., Deshmukh, S. S., Mishra, S., Kiblawi, S., Ott, S., Sang-aroonsiri, S., Kumar, S., Schweter, S., Bharati, S., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y., Bajaj, Y. S., Venkatraman, Y., Xu, Y., Xu, Y., Xu, Y., Tan, Z., Xie, Z., Ye, Z., Bras, M., Belkada, Y., and Wolf, T. Bloom: A 176b-parameter open-access multilingual language model, 2022. URL https://arxiv.org/abs/2211.05100.</p>
<p>Brooks, E., Walls, L., Lewis, R. L., and Singh, S. In-context policy iteration, 2022. URL https: //arxiv.org/abs/2210.03821.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S. A., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to Algorithms, Third Edition. The MIT Press, 3rd edition, 2009. ISBN 0262033844.</p>
<p>Dakhel, A. M., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., and Jiang, Z. M. Github copilot ai pair programmer: Asset or liability? ArXiv, abs/2206.15331, 2022.</p>
<p>Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., and Sutton, C. Language model cascades, 2022. URL https://arxiv.org/abs/2207.10342.</p>
<p>Dong, X., Liu, L., Musial, K., and Gabrys, B. NATS-Bench: Benchmarking nas algorithms for architecture topology and size. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021. doi: 10.1109/TPAMI.2021.3054824. doi:10.1109/TPAMI.2021.3054824.</p>
<p>Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts, 2021. URL https://arxiv.org/abs/2112.06905.</p>
<p>Elsken, T., Metzen, J. H., and Hutter, F. Efficient multi-objective neural architecture search via lamarckian evolution. arXiv: Machine Learning, 2018.</p>
<p>Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. Codebert: A pre-trained model for programming and natural languages. ArXiv, abs/2002.08155, 2020.</p>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning Volume 70, ICML'17, pp. 1263-1272. JMLR.org, 2017.</p>
<p>Greydanus, S. Scaling <em>down</em> deep learning. CoRR, abs/2011.14439, 2020. URL https://arxiv. org/abs/2011.14439.</p>
<p>Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., and van Zee, M. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax.</p>
<p>Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku.</p>
<p>Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Bennani, M. A., Csordás, R., Dudzik, A., Bovsnjak, M., Vitvitskyi, A., Rubanova, Y., Deac, A., Bevilacqua, B., Ganin, Y., Blundell, C., and Veličković, P. A generalist neural algorithmic learner. ArXiv, abs/2209.11142, 2022.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022.</p>
<p>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE, 86:2278-2324, 1998.</p>
<p>Lehman, J., Gordon, J., Jain, S., Ndousse, K., Yeh, C., and Stanley, K. O. Evolution through large models. ArXiv, abs/2206.08896, 2022.</p>
<p>Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning, 2021. URL https://arxiv.org/abs/2104.08691.</p>
<p>Li, L. and Talwalkar, A. S. Random search and reproducibility for neural architecture search. ArXiv, abs/1902.07638, 2019.</p>
<p>Liu, H., Brock, A., Simonyan, K., and Le, Q. V. Evolving normalization-activation layers. ArXiv, abs/2004.02967, 2020.</p>
<p>Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for gpt-3? In Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out, 2021.</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Annual Meeting of the Association for Computational Linguistics, 2021.</p>
<p>Meyerson, E., Nelson, M. J., Bradley, H., Moradi, A., Hoover, A. K., and Lehman, J. Language model crossover: Variation through few-shot prompting, 2023. URL https://arxiv.org/abs/ 2302.12170.</p>
<p>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? ArXiv, abs/2202.12837, 2022.</p>
<p>Mnih, A. and Hinton, G. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine Learning, ICML '07, pp. 641-648, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595937933. doi: 10.1145/1273496.1273577. URL https://doi.org/10.1145/1273496.1273577.</p>
<p>Noorbakhsh, K., Sulaiman, M., Sharifi, M., Roy, K., and Jamshidi, P. Pretrained language models are symbolic mathematics solvers too! ArXiv, abs/2110.03501, 2021.</p>
<p>Odena, A., Sutton, C., Dohan, D. M., Jiang, E., Michalewski, H., Austin, J., Bosma, M. P., Nye, M., Terry, M., and Le, Q. V. Program synthesis with large language models. In n/a, pp. n/a, n/a, 2021. $\mathrm{n} / \mathrm{a}$.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.</p>
<p>Qian, J., Wang, H., Li, Z., LI, S., and Yan, X. Limitations of language models in arithmetic and symbolic induction. ArXiv, abs/2208.05051, 2022.</p>
<p>Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Tan, J., Le, Q. V., and Kurakin, A. Large-scale evolution of image classifiers. ArXiv, abs/1703.01041, 2017.</p>
<p>Real, E., Aggarwal, A., Huang, Y., and Le, Q. V. Regularized evolution for image classifier architecture search. In AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Real, E., Liang, C., So, D. R., and Le, Q. V. Automl-zero: Evolving machine learning algorithms from scratch. In International Conference on Machine Learning, 2020.</p>
<p>Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning. ArXiv, abs/2112.08633, 2021.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Févry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot task generalization. ArXiv, abs/2110.08207, 2021.</p>
<p>Sciuto, C., Yu, K., Jaggi, M., Musat, C. C., and Salzmann, M. Evaluating the search phase of neural architecture search. ArXiv, abs/1902.08142, 2019.</p>
<p>So, D. R., Liang, C., and Le, Q. V. The evolved transformer. ArXiv, abs/1901.11117, 2019.
So, D. R., Mańke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q. V. Primer: Searching for efficient transformers for language modeling, 2021. URL https://arxiv.org/abs/2109.08668.</p>
<p>Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y., Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern, K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak, M., Chi, E., and Le, Q. Lamda: Language models for dialog applications, 2022. URL https://arxiv.org/abs/2201.08239.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017. URL https://arxiv.org/abs/1706.03762.</p>
<p>Veličković, P., Badia, A. P., Budden, D., Pascanu, R., Banino, A., Dashevskiy, M., Hadsell, R., and Blundell, C. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning, 2022.</p>
<p>Wang, Y., Wang, W., Joty, S. R., and Hoi, S. C. H. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. ArXiv, abs/2109.00859, 2021.</p>
<p>Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., hsin Chi, E. H., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.</p>
<p>Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. A systematic evaluation of large language models of code. Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, 2022.</p>
<p>Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. What can neural networks reason about? In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rJxbJeHFPS.</p>
<p>Yao, X. Evolving artificial neural networks. Proc. IEEE, 87:1423-1447, 1999.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/2205.01068.</p>
<p>Zhao, T., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. ArXiv, abs/2102.09690, 2021.</p>
<p>Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are human-level prompt engineers. ArXiv, abs/2211.01910, 2022.</p>
<h1>A Appendix</h1>
<h2>A. 1 EvOPrompting Hyperparameters</h2>
<p>Table 2: Values of hyperparameters used in EvoPROMPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">HYPERPARAMETER</th>
<th style="text-align: left;">DESCRIPTION</th>
<th style="text-align: center;">VALUE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$p$</td>
<td style="text-align: left;">Num. parents to select in every generation</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">$k$</td>
<td style="text-align: left;">Num. in-context examples in prompt</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">$T$</td>
<td style="text-align: left;">Num. rounds of evolution</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">$m$</td>
<td style="text-align: left;">Num. prompts per round</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">$n$</td>
<td style="text-align: left;">Num. samples to generate per prompt</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: left;">Lower threshold for test error</td>
<td style="text-align: center;">0.5</td>
</tr>
</tbody>
</table>
<h2>A. 2 MNIST-1D Seed Models</h2>
<p>Below we provide the source code for the four seed models used in the MNIST-1D model search.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="k">Module</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nl">features</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span>
<span class="w">    </span><span class="nl">nlayer</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="nv">@nn</span><span class="p">.</span><span class="w"> </span><span class="n">compact</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">..., None</span><span class="o">]</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Conv</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="w"> </span><span class="n">features</span><span class="p">,</span><span class="w"> </span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span><span class="w"> </span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nlayer</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="err">:</span>
<span class="w">            </span><span class="n">xp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Conv</span><span class="p">(</span>
<span class="w">                </span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="w"> </span><span class="n">features</span><span class="p">,</span>
<span class="w">                </span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
<span class="w">            </span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">            </span><span class="n">xp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">xp</span><span class="p">)</span>
<span class="w">            </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">xp</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">window_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span><span class="w"> </span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">flatten</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">256</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">x</span>
</code></pre></div>

<p>Listing 2: A hand-designed convolutional model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="k">Module</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nl">features</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">25</span>
<span class="w">    </span><span class="nv">@nn</span><span class="p">.</span><span class="w"> </span><span class="n">compact</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">[</span><span class="n">..., None</span><span class="o">]</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Conv</span><span class="p">(</span>
<span class="w">            </span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">features</span><span class="p">,</span><span class="w"> </span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span><span class="w"> </span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span>
<span class="w">        </span><span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
<span class="w">        </span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="err">:</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>        x = nn.Conv(
            features=self.features, kernel_size=(3,), strides=(2,)
    , padding=(1,)
        )(x)
        x = nn.relu(x)
    x = x.reshape((x.shape[0], -1))
    x = nn.Dense(features=10)(x)
    return x
</code></pre></div>

<p>Listing 3: A Flax implementation of the convolutional baseline from Greydanus (2020).</p>
<div class="codehilite"><pre><span></span><code><span class="kd">class</span><span class="w"> </span><span class="nx">Model</span><span class="p">(</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">):</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;A simple GRU model.&quot;&quot;&quot;</span>
<span class="w">    </span><span class="nx">hidden_size</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">6</span>
<span class="w">    </span><span class="nx">seed</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">42</span>
<span class="w">    </span><span class="err">@</span><span class="nx">nn</span><span class="p">.</span><span class="w"> </span><span class="nx">compact</span>
<span class="w">    </span><span class="nx">def</span><span class="w"> </span><span class="nx">__call__</span><span class="p">(</span><span class="kp">self</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">):</span>
<span class="w">        </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">jnp</span><span class="p">.</span><span class="nx">expand_dims</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="nx">rng</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">jax_random</span><span class="p">.</span><span class="nx">PRNGKey</span><span class="p">(</span><span class="kp">self</span><span class="p">.</span><span class="nx">seed</span><span class="p">)</span>
<span class="w">        </span><span class="nx">gru</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">recurrent</span><span class="p">.</span><span class="nx">GRU</span><span class="p">(</span>
<span class="w">            </span><span class="nx">hidden_size</span><span class="p">=</span><span class="kp">self</span><span class="p">.</span><span class="nx">hidden_size</span><span class="p">,</span>
<span class="w">            </span><span class="nx">num_layers</span><span class="p">=</span><span class="mi">1</span><span class="p">,</span>
<span class="w">            </span><span class="nx">dropout_rate</span><span class="p">=</span><span class="m m-Double">0.0</span><span class="p">,</span>
<span class="w">            </span><span class="nx">bidirectional</span><span class="p">=</span><span class="nx">True</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="nx">lengths</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">full</span><span class="p">([</span><span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span><span class="w"> </span><span class="nx">x</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="w">    </span><span class="nx">initialized_params</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">gru</span><span class="p">.</span><span class="nx">init</span><span class="p">(</span><span class="nx">rng</span><span class="p">,</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lengths</span><span class="p">)</span>
<span class="w">    </span><span class="nx">params</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">initialized_params</span><span class="p">[</span><span class="err">&#39;</span><span class="nx">params</span><span class="err">&#39;</span><span class="p">]</span>
<span class="w">    </span><span class="nx">outputs</span><span class="p">,</span><span class="w"> </span><span class="nx">_</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">gru</span><span class="p">.</span><span class="nx">apply</span><span class="p">({</span><span class="err">&#39;</span><span class="nx">params</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="nx">params</span><span class="p">},</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">lengths</span><span class="p">)</span>
<span class="w">    </span><span class="nx">outputs</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">outputs</span><span class="p">.</span><span class="nx">reshape</span><span class="p">((</span><span class="nx">outputs</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="w">    </span><span class="nx">x</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">nn</span><span class="p">.</span><span class="nx">Dense</span><span class="p">(</span><span class="nx">features</span><span class="p">=</span><span class="mi">10</span><span class="p">)(</span><span class="nx">outputs</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">x</span>
</code></pre></div>

<p>Listing 4: A Flax implementation of the GRU baseline from Greydanus (2020).</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="n">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="k">Module</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="nl">hidden_size</span><span class="p">:</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span>
<span class="w">    </span><span class="nv">@nn</span><span class="p">.</span><span class="w"> </span><span class="n">compact</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)(</span><span class="n">x</span><span class="p">))</span>
<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Model</span>
</code></pre></div>

<p>Listing 5: A Flax implementation of the fully connected baseline from Greydanus (2020).</p>
<h1>A. 3 Trajectory of search for MNIST-1D models</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: The average model size and test error of the child models produced in each round of the model search. Data points closer to the origin represent rounds that yielded more "fit" models.</p>
<h1>A. 4 Newly Discovered CLRS GNNs</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Maximum fitness scores of five of the newly discovered models, compared against the baseline, on eight of the CLRS tasks.</p>
<p>Below we list the Python source code of five of the newly discovered GNNs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_triplet_msgs_quad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">edge_fts</span><span class="p">,</span><span class="w"> </span><span class="n">graph_fts</span><span class="p">,</span><span class="w"> </span><span class="n">nb_triplet_fts</span>
<span class="w">    </span><span class="p">,</span><span class="w"> </span><span class="n">out_size</span><span class="p">):</span>
<span class="n">node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nb_triplet_fts</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
<span class="n">triplet_node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">node_rep</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">node_rep</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">node_reps</span><span class="p">]</span>
<span class="n">node_pair_inversions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)]</span>
<span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">    </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">    </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">perm</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    triplet_node_reps, node_pair_inversions
    ],
    ],
    )
    return jnp.max(triplets, axis=1) - jnp.min(triplets, axis=1)
</code></pre></div>

<p>Listing 6: The triplet representation that we refer to as QUADNODEMINMAX.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_triplet_msgs_concatrep</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">edge_fts</span><span class="p">,</span><span class="w"> </span><span class="n">graph_fts</span><span class="p">,</span>
<span class="w">    </span><span class="n">nb_triplet_fts</span><span class="p">,</span><span class="w"> </span><span class="n">out_size</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">rep_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="k">size</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="n">proj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="p">.</span><span class="n">nets</span><span class="p">.</span><span class="n">MLP</span><span class="p">(</span><span class="o">[</span><span class="n">size</span><span class="o">]</span><span class="p">)</span>
<span class="w">        </span><span class="n">ff</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="p">.</span><span class="n">nets</span><span class="p">.</span><span class="n">MLP</span><span class="p">(</span><span class="o">[</span><span class="n">size * 4, size</span><span class="o">]</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">jnp</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="o">[</span>
<span class="n">            proj(x),</span>
<span class="n">            ff(x),</span>
<span class="n">        </span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">triplet_node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">rep_fn(z, nb_triplet_fts) for _ in range</span>
<span class="n">        (3)</span><span class="o">]</span>
<span class="w">    </span><span class="n">triplet_edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">rep_fn(edge_fts, nb_triplet_fts) for _ in</span>
<span class="n">        range(3)</span><span class="o">]</span>
<span class="w">    </span><span class="n">triplet_graph_rep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rep_fn</span><span class="p">(</span><span class="n">graph_fts</span><span class="p">,</span><span class="w"> </span><span class="n">nb_triplet_fts</span><span class="p">)</span>
<span class="w">    </span><span class="n">node_pair_permutations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">(2, 3), (1, 3), (1, 2)</span><span class="o">]</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="p">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="nl">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="o">[</span>
<span class="n">            jnp.expand_dims(tri_node_rep, axis=perm)</span>
<span class="n">            for tri_node_rep, perm in zip(</span>
<span class="n">                triplet_node_reps, node_pair_permutations</span>
<span class="n">            )</span>
<span class="n">        </span><span class="o">]</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">functools</span><span class="p">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="nl">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="o">[</span>
<span class="n">            jnp.expand_dims(tri_edge_rep, axis=i)</span>
<span class="n">            for tri_edge_rep, i in zip(triplet_edge_reps, range(3,</span>
<span class="n">        0, -1))</span>
<span class="n">        </span><span class="o">]</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">jnp</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">triplet_graph_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">))</span>
<span class="w">    </span><span class="n">output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">output_layer</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<p>Listing 7: The triplet representation that we refer to as CONCATREP.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_triplet_msgs_tanhexplandtriplets</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">edge_fts</span><span class="p">,</span><span class="w"> </span><span class="n">graph_fts</span><span class="p">,</span>
<span class="w">    </span><span class="n">nb_triplet_fts</span><span class="p">,</span><span class="w"> </span><span class="n">out_size</span><span class="p">)</span><span class="err">:</span>
<span class="n">node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hk.Linear(nb_triplet_fts) for _ in range(3)</span><span class="o">]</span>
<span class="n">edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hk.Linear(nb_triplet_fts) for _ in range(3)</span><span class="o">]</span>
<span class="n">graph_rep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="p">.</span><span class="n">nets</span><span class="p">.</span><span class="n">MLP</span><span class="p">(</span><span class="o">[</span><span class="n">nb_triplet_fts</span><span class="o">]</span><span class="p">)</span>
<span class="n">triplet_node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">node_rep(z) for node_rep in node_reps</span><span class="o">]</span>
<span class="n">triplet_edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">edge_rep(edge_fts) for edge_rep in</span>
<span class="n">    edge_reps</span><span class="o">]</span>
<span class="n">node_pair_permutations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">(2, 3), (1, 3), (1, 2)</span><span class="o">]</span>
<span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="p">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">    </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="nl">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">    </span><span class="err">[</span>
<span class="w">        </span><span class="n">jnp</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">perm</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="p">[</span>
<span class="w">            </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span><span class="n">triplet_edge_reps</span><span class="p">,</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">graph_rep</span><span class="p">(</span><span class="n">graph_fts</span><span class="p">),</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="mi">3</span><span class="p">))</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">graph_rep</span><span class="p">(</span><span class="n">graph_fts</span><span class="p">),</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="mi">1</span><span class="p">))</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">graph_rep</span><span class="p">(</span><span class="n">graph_fts</span><span class="p">),</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="mi">2</span><span class="p">))</span>
<span class="w">    </span><span class="n">output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">output_layer</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
</code></pre></div>

<p>Listing 8: The triplet representation that we refer to as TANHEXPANDTRIPLETS.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_triplet_msgs_div2mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">edge_fts</span><span class="p">,</span><span class="w"> </span><span class="n">graph_fts</span><span class="p">,</span>
<span class="w">    </span><span class="n">nb_triplet_fts</span><span class="p">,</span><span class="w"> </span><span class="n">out_size</span><span class="p">):</span>
<span class="w">    </span><span class="n">node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nb_triplet_fts</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="w">    </span><span class="n">edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nb_triplet_fts</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="w">    </span><span class="n">triplet_node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">node_rep</span><span class="p">(</span><span class="n">z</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">node_rep</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">node_reps</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">    </span><span class="n">triplet_edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">edge_rep</span><span class="p">(</span><span class="n">edge_fts</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">edge_rep</span><span class="w"> </span><span class="ow">in</span>
<span class="w">        </span><span class="n">edge_reps</span><span class="p">]</span>
<span class="w">    </span><span class="n">node_pair_permutations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)]</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="p">[</span>
<span class="w">            </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">perm</span><span class="w"> </span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">perm</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span>
<span class="w">                </span><span class="n">triplet_node_reps</span><span class="p">,</span><span class="w"> </span><span class="n">node_pair_permutations</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="p">[</span>
<span class="w">            </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">perm</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span><span class="n">triplet_edge_reps</span><span class="p">,</span><span class="w"> </span><span class="nb">range</span>
<span class="w">        </span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="n">output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">output_layer</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<p>Listing 9: The triplet representation that we refer to as Div2MeAn.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_triplet_msgs_maxmean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">edge_fts</span><span class="p">,</span><span class="w"> </span><span class="n">graph_fts</span><span class="p">,</span>
<span class="w">    </span><span class="n">nb_triplet_fts</span><span class="p">,</span><span class="w"> </span><span class="n">out_size</span><span class="p">)</span><span class="err">:</span>
<span class="n">node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hk.Linear(nb_triplet_fts) for _ in range(3)</span><span class="o">]</span>
<span class="n">edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hk.Linear(nb_triplet_fts) for _ in range(3)</span><span class="o">]</span>
<span class="n">graph_rep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="p">.</span><span class="n">nets</span><span class="p">.</span><span class="n">MLP</span><span class="p">(</span><span class="o">[</span><span class="n">nb_triplet_fts</span><span class="o">]</span><span class="p">)</span>
<span class="n">triplet_node_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">node_rep(z) for node_rep in node_reps</span><span class="o">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">triplet_edge_reps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">edge_rep</span><span class="p">(</span><span class="n">edge_fts</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">edge_rep</span><span class="w"> </span><span class="ow">in</span>
<span class="w">    </span><span class="n">edge_reps</span><span class="p">]</span>
<span class="n">node_pair_permutations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)]</span>
<span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">            </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">        </span><span class="p">[</span>
<span class="w">            </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">perm</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">tri_node_rep</span><span class="p">,</span><span class="w"> </span><span class="n">perm</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span>
<span class="w">                </span><span class="n">triplet_node_reps</span><span class="p">,</span><span class="w"> </span><span class="n">node_pair_permutations</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="p">],</span>
<span class="p">)</span>
<span class="n">triplets</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
<span class="w">    </span><span class="n">lambda</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">y</span><span class="p">,</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">            </span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="n">tri_edge_rep</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">zip</span><span class="p">(</span><span class="n">triplet_edge_reps</span><span class="p">,</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span>
<span class="w">        </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="w">        </span><span class="p">],</span>
<span class="p">)</span>
<span class="n">triplets</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span><span class="w"> </span><span class="o">-</span><span class="mf">100.0</span><span class="p">)</span>
<span class="n">output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hk</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
<span class="k">return</span><span class="w"> </span><span class="n">output_layer</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">triplets</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>

<p>Listing 10: The triplet representation that we refer to as MAXMEAN.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done while a Student Researcher at Google DeepMind.
${ }^{\dagger}$ Work done while at Google DeepMind.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
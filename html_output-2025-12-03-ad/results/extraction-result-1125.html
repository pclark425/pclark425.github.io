<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1125 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1125</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1125</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-268856868</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01867v1.pdf" target="_blank">Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL). Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment. Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model. Therefore, the quality of the model is fundamental for the performance of the posterior tasks. In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering. We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration. With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward. In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup. Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps. Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1125.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1125.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LapMCEnt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Laplace approximation + entropy metric active exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian model-based RL exploration agent that uses Laplace approximation to obtain a Gaussian posterior over network weights and an entropy-based information-gain utility (−log |J^T H^{-1} J|) to drive active exploration of robot dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LapMCEnt (Laplace + entropy metric)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based RL agent that trains a neural network dynamics model, computes a Laplace Gaussian approximation to the weight posterior around the MAP estimate (subnetwork Laplace on top-n weights), translates that to a predictive covariance Σ_LA = J^T H^{-1} J + diag(σ_θ), and uses this analytic epistemic uncertainty to compute an exploration utility; policy learning (both exploration and evaluation) uses SAC on the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / information-gain maximization (experimental design) via Laplace-based predictive entropy</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each planning/learning iteration the agent re-trains (or updates) the Bayesian dynamics model from the replay buffer and recomputes an exploration policy π_ex that maximizes the expected information gain u(s,a). For Laplace it approximates p(θ|D) ≈ N(θ_MAP, H^{-1}), linearizes to obtain predictive covariance and uses an entropy proxy u(s,a) ∝ −log |J^T H^{-1} J| (assuming homoscedastic noise). The policy is recomputed periodically (every n_pol steps) and actions are selected to visit transitions expected to maximize epistemic uncertainty reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HalfCheetah (MuJoCo) and Coppelia (RLBench with Panda robot: PushButton, MoveBlock-ToTarget)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous-state and continuous-action MDPs; stochastic physics-based dynamics (MuJoCo, Coppelia/V-REP); high-dimensional robot manipulation (partially observable only insofar as real sensors, but treated as full-state MDP in experiments); dynamics include contact interactions (box) that can be sparse in data; reward redefined to continuous 0–100 for RLBench tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>HalfCheetah: |S|=17, |A|=6; Coppelia (Panda): |S|=43, |A|=8; exploration budget n_ex = 20,000 real steps (warmup 256); exploration policy computed over n_pol_eps episodes (50) of n_pol_steps (50); evaluation episodes use SAC with model-imagined rollouts (examples: n_pol_eps up to 250, n_pol_steps up to 100 when training eval policies).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>LapMCEnt outperforms reactive baselines and deep-ensemble based MAX in HalfCheetah and achieves the best or near-best performance among tested Bayesian approaches in Coppelia; in HalfCheetah LapMCEnt surpasses the model-free SAC baseline despite using far fewer real steps (LapMCEnt used 20k real steps vs SAC trained with ~200k real steps in the reported comparison). (No absolute cumulative-reward numbers given in paper; comparisons are relative.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline (random exploration / PERX reactive) and model-free SAC achieve lower evaluation rewards when trained with the same small real-step budget; SAC with much larger real-step budget (~200k) attains comparable or sometimes lower performance than LapMCEnt trained with 20k steps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High: exploration pipeline uses 20k real interactions (n_ex = 20k) to produce a dynamics model sufficient to train task policies in simulation; authors report LapMCEnt overtakes SAC while using approximately an order-of-magnitude fewer real steps (20k vs ~200k).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Separation of concerns: a dedicated exploration MDP uses an information-gain utility to maximize epistemic uncertainty reduction (pure exploration policy π_ex); after exploration, the learned model is used to train evaluation/task policies (π_ev) via SAC on the model (exploitation). Policy re-computation frequency and episodic collection control exploration dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: model-free SAC (large-sample baseline), reactive exploration methods (PERX and random exploration), and model-based active exploration MAX (deep ensembles + Jensen-Rényi divergence).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Laplace approximation combined with an entropy-based experimental-design utility yields superior calibration (lowest AUSE = 0.048) and strong task performance in HalfCheetah and Coppelia, with better sample efficiency than SAC and lower storage/training cost than deep ensembles; LapMCEnt is most robust on harder manipulation (MoveBlock) among tested Bayesian methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Laplace introduces additional computational overhead to compute/fit the posterior covariance; subnetwork Laplace approximation was required (only top-n weights treated as Bayesian). On complex contact-rich tasks (MoveBlock) exploration may still fail to collect sufficient transitions to learn object dynamics (agent predicted the box never moves), indicating limits when long state-action sequences are required to reach informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1125.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1125.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-dropout Ent/Rényi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo dropout based Bayesian active exploration (entropy or Rényi/Jensen metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active exploration agent that approximates model posterior via MC-dropout (N forward stochastic passes) and computes exploration utility as sample-based predictive entropy or Jensen-Rényi divergence to prioritize informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MC-dropout based information-driven exploration (MCdrop Ent / MCdrop Rényi)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Dynamics model is a single neural network with dropout layers kept active at inference; the posterior is approximated via N=32 stochastic forward passes, producing a mixture-of-Gaussians predictive distribution. Utility is computed via sample-based approximations—either an entropy of the approximate Gaussian mixture (via sample-moment fit) or Jensen-Rényi divergence over samples—and used to construct an exploration policy; policies trained using SAC on the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / information-gain maximization using MC-dropout posterior samples (entropy or Jensen-Rényi divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent periodically retrains the dropout network on the growing replay buffer and recomputes the exploration policy π_ex; at inference it uses N stochastic dropout passes to estimate predictive uncertainty and computes a utility u(s,a) from sample statistics (approximated mixture covariance or Rényi entropy) to prefer transitions with high epistemic uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HalfCheetah (MuJoCo) and Coppelia (RLBench Panda robot tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous, stochastic dynamics; high-dimensional robot state for manipulation (contacts); rewards can be sparse for object interactions (e.g., MoveBlock requires interaction to observe box dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>HalfCheetah: |S|=17, |A|=6; Coppelia: |S|=43, |A|=8; experiments used N=32 forward passes/samples, dropout probability p=0.25, exploration budget n_ex = 20k real steps.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MC-dropout based exploration performs well and outperforms naive/reactive baselines; in HalfCheetah MC-dropout methods achieve strong performance though in reported calibration Laplace slightly outperforms (MCdrop AUSE ≈ 0.085). In some tasks MC-dropout with Rényi ties with or trails Laplace-based entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Reactive exploration methods (PERX, random) and model-free SAC with equivalent small budgets perform worse; SAC with much larger budgets (~200k steps) is a stronger but less sample-efficient baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>MC-dropout variants use the same exploration budget (20k real interactions) and achieve marked sample-efficiency improvements over model-free SAC trained on raw interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same pipeline separation: exploration via π_ex maximizing uncertainty-based utility estimated from MC-dropout samples; exploitation via training task policies on the learned model using SAC.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Laplace+entropy (LapMCEnt), deep ensembles (MAX), PERX, random exploration, and model-free SAC.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MC-dropout is an effective, computationally efficient Bayesian approximation for active exploration, requiring only a single trained network and N forward passes; it yields good performance and faster training than deep ensembles, but is less well-calibrated than Laplace in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Calibration (AUSE) worse than Laplace in reported experiments; approximation via MC-dropout can still misrepresent posterior in some regions. For highly contact-dependent tasks (MoveBlock), sample collection limitations still hamper learning object dynamics regardless of uncertainty estimator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1125.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1125.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAX (Jensen-Rényi)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based active exploration (deep ensembles + Jensen-Rényi divergence) — MAX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline active exploration method that constructs a deep-ensemble approximation to the posterior (N networks) and computes exploration utility via Jensen–Rényi (or Rényi) divergence among ensemble predictive distributions to select informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modelbased active exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAX (deep ensembles + Jensen-Rényi divergence)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an ensemble of N independently trained neural network dynamics models to approximate p(θ|D) via multiple MAP estimates. The exploration utility is approximated as the JS / Jensen-Rényi divergence across ensemble predictive distributions (mixture of Gaussians) to estimate epistemic disagreement; the agent follows an exploration policy maximizing that utility.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / information-gain maximization via disagreement (Jensen–Rényi divergence) among deep-ensemble predictive samples</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Train N separate models (deep ensemble) on the replay buffer, use their predictive mixture to compute a divergence-based utility (Jensen-Rényi) per candidate action, and follow a periodically recomputed exploration policy that seeks transitions maximizing ensemble disagreement (epistemic uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Used as a baseline in HalfCheetah and Coppelia experiments (same environments as main study)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Continuous, stochastic, high-dimensional dynamics; same specifics as experimental environments (HalfCheetah and RLBench Panda tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same: HalfCheetah |S|=17, |A|=6; Coppelia |S|=43, |A|=8; ensemble size n_ens = 32 in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>MAX performs well relative to reactive baselines but is outperformed by Laplace- and MC-dropout-based methods in these experiments (LapMCEnt and MC-dropout variants achieved higher evaluation rewards). Calibration (AUSE) for MAX reported as 0.075 (worse than Laplace's 0.048), and training time was substantially higher (122.75 s) due to training multiple networks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to reactive exploration (PERX, random) and model-free SAC with limited real steps, MAX outperforms reactive baselines but is less sample-efficient or less performant than LapMCEnt under the 20k-step budget in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient in practical resource terms because of computational and storage overhead: training 32 networks increases training time and memory (authors report deep ensembles storage ~32× single model), but in terms of reward-per-real-step it is better than random/reactive exploration though worse than LapMCEnt in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same pipeline approach: a dedicated exploration policy maximizing ensemble disagreement; evaluation/exploitation uses models to train task policies on imagined rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Laplace+entropy (LapMCEnt), MC-dropout, PERX, random exploration, and model-free SAC.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Deep-ensemble disagreement (MAX) is a strong baseline for epistemic-driven exploration but carries large computational and storage costs; Jensen–Rényi divergence pairs well with ensembles, yet in these robot-manipulation tasks Laplace+entropy and MC-dropout methods achieved equal or superior performance with lower resource requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High computational cost (train-time and storage); cannot be pretrained/finetuned easily because ensembles are trained from random initializations; calibration (AUSE) and wall-clock training are worse than Laplace and MC-dropout in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modelbased active exploration <em>(Rating: 2)</em></li>
                <li>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning <em>(Rating: 2)</em></li>
                <li>Simple and scalable predictive uncertainty estimation using deep ensembles <em>(Rating: 2)</em></li>
                <li>Laplace redux: effortless Bayesian deep learning <em>(Rating: 1)</em></li>
                <li>An experimental design perspective on model-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1125",
    "paper_id": "paper-268856868",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "LapMCEnt",
            "name_full": "Laplace approximation + entropy metric active exploration",
            "brief_description": "A Bayesian model-based RL exploration agent that uses Laplace approximation to obtain a Gaussian posterior over network weights and an entropy-based information-gain utility (−log |J^T H^{-1} J|) to drive active exploration of robot dynamics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LapMCEnt (Laplace + entropy metric)",
            "agent_description": "Model-based RL agent that trains a neural network dynamics model, computes a Laplace Gaussian approximation to the weight posterior around the MAP estimate (subnetwork Laplace on top-n weights), translates that to a predictive covariance Σ_LA = J^T H^{-1} J + diag(σ_θ), and uses this analytic epistemic uncertainty to compute an exploration utility; policy learning (both exploration and evaluation) uses SAC on the learned model.",
            "adaptive_design_method": "Active learning / information-gain maximization (experimental design) via Laplace-based predictive entropy",
            "adaptation_strategy_description": "At each planning/learning iteration the agent re-trains (or updates) the Bayesian dynamics model from the replay buffer and recomputes an exploration policy π_ex that maximizes the expected information gain u(s,a). For Laplace it approximates p(θ|D) ≈ N(θ_MAP, H^{-1}), linearizes to obtain predictive covariance and uses an entropy proxy u(s,a) ∝ −log |J^T H^{-1} J| (assuming homoscedastic noise). The policy is recomputed periodically (every n_pol steps) and actions are selected to visit transitions expected to maximize epistemic uncertainty reduction.",
            "environment_name": "HalfCheetah (MuJoCo) and Coppelia (RLBench with Panda robot: PushButton, MoveBlock-ToTarget)",
            "environment_characteristics": "Continuous-state and continuous-action MDPs; stochastic physics-based dynamics (MuJoCo, Coppelia/V-REP); high-dimensional robot manipulation (partially observable only insofar as real sensors, but treated as full-state MDP in experiments); dynamics include contact interactions (box) that can be sparse in data; reward redefined to continuous 0–100 for RLBench tasks.",
            "environment_complexity": "HalfCheetah: |S|=17, |A|=6; Coppelia (Panda): |S|=43, |A|=8; exploration budget n_ex = 20,000 real steps (warmup 256); exploration policy computed over n_pol_eps episodes (50) of n_pol_steps (50); evaluation episodes use SAC with model-imagined rollouts (examples: n_pol_eps up to 250, n_pol_steps up to 100 when training eval policies).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "LapMCEnt outperforms reactive baselines and deep-ensemble based MAX in HalfCheetah and achieves the best or near-best performance among tested Bayesian approaches in Coppelia; in HalfCheetah LapMCEnt surpasses the model-free SAC baseline despite using far fewer real steps (LapMCEnt used 20k real steps vs SAC trained with ~200k real steps in the reported comparison). (No absolute cumulative-reward numbers given in paper; comparisons are relative.)",
            "performance_without_adaptation": "Baseline (random exploration / PERX reactive) and model-free SAC achieve lower evaluation rewards when trained with the same small real-step budget; SAC with much larger real-step budget (~200k) attains comparable or sometimes lower performance than LapMCEnt trained with 20k steps.",
            "sample_efficiency": "High: exploration pipeline uses 20k real interactions (n_ex = 20k) to produce a dynamics model sufficient to train task policies in simulation; authors report LapMCEnt overtakes SAC while using approximately an order-of-magnitude fewer real steps (20k vs ~200k).",
            "exploration_exploitation_tradeoff": "Separation of concerns: a dedicated exploration MDP uses an information-gain utility to maximize epistemic uncertainty reduction (pure exploration policy π_ex); after exploration, the learned model is used to train evaluation/task policies (π_ev) via SAC on the model (exploitation). Policy re-computation frequency and episodic collection control exploration dynamics.",
            "comparison_methods": "Compared against: model-free SAC (large-sample baseline), reactive exploration methods (PERX and random exploration), and model-based active exploration MAX (deep ensembles + Jensen-Rényi divergence).",
            "key_results": "Laplace approximation combined with an entropy-based experimental-design utility yields superior calibration (lowest AUSE = 0.048) and strong task performance in HalfCheetah and Coppelia, with better sample efficiency than SAC and lower storage/training cost than deep ensembles; LapMCEnt is most robust on harder manipulation (MoveBlock) among tested Bayesian methods.",
            "limitations_or_failures": "Laplace introduces additional computational overhead to compute/fit the posterior covariance; subnetwork Laplace approximation was required (only top-n weights treated as Bayesian). On complex contact-rich tasks (MoveBlock) exploration may still fail to collect sufficient transitions to learn object dynamics (agent predicted the box never moves), indicating limits when long state-action sequences are required to reach informative transitions.",
            "uuid": "e1125.0",
            "source_info": {
                "paper_title": "Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MC-dropout Ent/Rényi",
            "name_full": "Monte Carlo dropout based Bayesian active exploration (entropy or Rényi/Jensen metrics)",
            "brief_description": "An active exploration agent that approximates model posterior via MC-dropout (N forward stochastic passes) and computes exploration utility as sample-based predictive entropy or Jensen-Rényi divergence to prioritize informative transitions.",
            "citation_title": "Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation",
            "mention_or_use": "use",
            "agent_name": "MC-dropout based information-driven exploration (MCdrop Ent / MCdrop Rényi)",
            "agent_description": "Dynamics model is a single neural network with dropout layers kept active at inference; the posterior is approximated via N=32 stochastic forward passes, producing a mixture-of-Gaussians predictive distribution. Utility is computed via sample-based approximations—either an entropy of the approximate Gaussian mixture (via sample-moment fit) or Jensen-Rényi divergence over samples—and used to construct an exploration policy; policies trained using SAC on the learned model.",
            "adaptive_design_method": "Active learning / information-gain maximization using MC-dropout posterior samples (entropy or Jensen-Rényi divergence)",
            "adaptation_strategy_description": "The agent periodically retrains the dropout network on the growing replay buffer and recomputes the exploration policy π_ex; at inference it uses N stochastic dropout passes to estimate predictive uncertainty and computes a utility u(s,a) from sample statistics (approximated mixture covariance or Rényi entropy) to prefer transitions with high epistemic uncertainty.",
            "environment_name": "HalfCheetah (MuJoCo) and Coppelia (RLBench Panda robot tasks)",
            "environment_characteristics": "Continuous, stochastic dynamics; high-dimensional robot state for manipulation (contacts); rewards can be sparse for object interactions (e.g., MoveBlock requires interaction to observe box dynamics).",
            "environment_complexity": "HalfCheetah: |S|=17, |A|=6; Coppelia: |S|=43, |A|=8; experiments used N=32 forward passes/samples, dropout probability p=0.25, exploration budget n_ex = 20k real steps.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MC-dropout based exploration performs well and outperforms naive/reactive baselines; in HalfCheetah MC-dropout methods achieve strong performance though in reported calibration Laplace slightly outperforms (MCdrop AUSE ≈ 0.085). In some tasks MC-dropout with Rényi ties with or trails Laplace-based entropy.",
            "performance_without_adaptation": "Reactive exploration methods (PERX, random) and model-free SAC with equivalent small budgets perform worse; SAC with much larger budgets (~200k steps) is a stronger but less sample-efficient baseline.",
            "sample_efficiency": "MC-dropout variants use the same exploration budget (20k real interactions) and achieve marked sample-efficiency improvements over model-free SAC trained on raw interactions.",
            "exploration_exploitation_tradeoff": "Same pipeline separation: exploration via π_ex maximizing uncertainty-based utility estimated from MC-dropout samples; exploitation via training task policies on the learned model using SAC.",
            "comparison_methods": "Compared against Laplace+entropy (LapMCEnt), deep ensembles (MAX), PERX, random exploration, and model-free SAC.",
            "key_results": "MC-dropout is an effective, computationally efficient Bayesian approximation for active exploration, requiring only a single trained network and N forward passes; it yields good performance and faster training than deep ensembles, but is less well-calibrated than Laplace in these experiments.",
            "limitations_or_failures": "Calibration (AUSE) worse than Laplace in reported experiments; approximation via MC-dropout can still misrepresent posterior in some regions. For highly contact-dependent tasks (MoveBlock), sample collection limitations still hamper learning object dynamics regardless of uncertainty estimator.",
            "uuid": "e1125.1",
            "source_info": {
                "paper_title": "Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MAX (Jensen-Rényi)",
            "name_full": "Model-based active exploration (deep ensembles + Jensen-Rényi divergence) — MAX",
            "brief_description": "A baseline active exploration method that constructs a deep-ensemble approximation to the posterior (N networks) and computes exploration utility via Jensen–Rényi (or Rényi) divergence among ensemble predictive distributions to select informative transitions.",
            "citation_title": "Modelbased active exploration",
            "mention_or_use": "mention",
            "agent_name": "MAX (deep ensembles + Jensen-Rényi divergence)",
            "agent_description": "Uses an ensemble of N independently trained neural network dynamics models to approximate p(θ|D) via multiple MAP estimates. The exploration utility is approximated as the JS / Jensen-Rényi divergence across ensemble predictive distributions (mixture of Gaussians) to estimate epistemic disagreement; the agent follows an exploration policy maximizing that utility.",
            "adaptive_design_method": "Active learning / information-gain maximization via disagreement (Jensen–Rényi divergence) among deep-ensemble predictive samples",
            "adaptation_strategy_description": "Train N separate models (deep ensemble) on the replay buffer, use their predictive mixture to compute a divergence-based utility (Jensen-Rényi) per candidate action, and follow a periodically recomputed exploration policy that seeks transitions maximizing ensemble disagreement (epistemic uncertainty).",
            "environment_name": "Used as a baseline in HalfCheetah and Coppelia experiments (same environments as main study)",
            "environment_characteristics": "Continuous, stochastic, high-dimensional dynamics; same specifics as experimental environments (HalfCheetah and RLBench Panda tasks).",
            "environment_complexity": "Same: HalfCheetah |S|=17, |A|=6; Coppelia |S|=43, |A|=8; ensemble size n_ens = 32 in reported experiments.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "MAX performs well relative to reactive baselines but is outperformed by Laplace- and MC-dropout-based methods in these experiments (LapMCEnt and MC-dropout variants achieved higher evaluation rewards). Calibration (AUSE) for MAX reported as 0.075 (worse than Laplace's 0.048), and training time was substantially higher (122.75 s) due to training multiple networks.",
            "performance_without_adaptation": "Compared to reactive exploration (PERX, random) and model-free SAC with limited real steps, MAX outperforms reactive baselines but is less sample-efficient or less performant than LapMCEnt under the 20k-step budget in these tasks.",
            "sample_efficiency": "Less sample-efficient in practical resource terms because of computational and storage overhead: training 32 networks increases training time and memory (authors report deep ensembles storage ~32× single model), but in terms of reward-per-real-step it is better than random/reactive exploration though worse than LapMCEnt in experiments.",
            "exploration_exploitation_tradeoff": "Same pipeline approach: a dedicated exploration policy maximizing ensemble disagreement; evaluation/exploitation uses models to train task policies on imagined rollouts.",
            "comparison_methods": "Compared against Laplace+entropy (LapMCEnt), MC-dropout, PERX, random exploration, and model-free SAC.",
            "key_results": "Deep-ensemble disagreement (MAX) is a strong baseline for epistemic-driven exploration but carries large computational and storage costs; Jensen–Rényi divergence pairs well with ensembles, yet in these robot-manipulation tasks Laplace+entropy and MC-dropout methods achieved equal or superior performance with lower resource requirements.",
            "limitations_or_failures": "High computational cost (train-time and storage); cannot be pretrained/finetuned easily because ensembles are trained from random initializations; calibration (AUSE) and wall-clock training are worse than Laplace and MC-dropout in reported experiments.",
            "uuid": "e1125.2",
            "source_info": {
                "paper_title": "Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modelbased active exploration",
            "rating": 2,
            "sanitized_title": "modelbased_active_exploration"
        },
        {
            "paper_title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
            "rating": 2,
            "sanitized_title": "dropout_as_a_bayesian_approximation_representing_model_uncertainty_in_deep_learning"
        },
        {
            "paper_title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "rating": 2,
            "sanitized_title": "simple_and_scalable_predictive_uncertainty_estimation_using_deep_ensembles"
        },
        {
            "paper_title": "Laplace redux: effortless Bayesian deep learning",
            "rating": 1,
            "sanitized_title": "laplace_redux_effortless_bayesian_deep_learning"
        },
        {
            "paper_title": "An experimental design perspective on model-based reinforcement learning",
            "rating": 1,
            "sanitized_title": "an_experimental_design_perspective_on_modelbased_reinforcement_learning"
        }
    ],
    "cost": 0.012837,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation
2 Apr 2024</p>
<p>Carlos Plou 
Ana C Murillo 
Ruben Martinez-Cantin 
Active Exploration in Bayesian Model-based Reinforcement Learning for Robot Manipulation
2 Apr 2024650D4930039065AF19EC6E67EEBC185EarXiv:2404.01867v1[cs.RO]
Efficiently tackling multiple tasks within complex environment, such as those found in robot manipulation, remains an ongoing challenge in robotics and an opportunity for data-driven solutions, such as reinforcement learning (RL).Model-based RL, by building a dynamic model of the robot, enables data reuse and transfer learning between tasks with the same robot and similar environment.Furthermore, data gathering in robotics is expensive and we must rely on data efficient approaches such as model-based RL, where policy learning is mostly conducted on cheaper simulations based on the learned model.Therefore, the quality of the model is fundamental for the performance of the posterior tasks.In this work, we focus on improving the quality of the model and maintaining the data efficiency by performing active learning of the dynamic model during a preliminary exploration phase based on maximize information gathering.We employ Bayesian neural network models to represent, in a probabilistic way, both the belief and information encoded in the dynamic model during exploration.With our presented strategies we manage to actively estimate the novelty of each transition, using this as the exploration reward.In this work, we compare several Bayesian inference methods for neural networks, some of which have never been used in a robotics context, and evaluate them in a realistic robot manipulation setup.Our experiments show the advantages of our Bayesian model-based RL approach, with similar quality in the results than relevant alternatives with much lower requirements regarding robot execution steps.Unlike related previous studies that focused the validation solely on toy problems, our research takes a step towards more realistic setups, tackling robotic arm end-tasks.</p>
<p>I. INTRODUCTION</p>
<p>Robotic manipulation stands out as one of the most crucial application domains for robotics, offering potential benefits to various industries, including manufacturing, supply chain, and healthcare [1], [2].In this paper, we tackle robotic manipulation problem through the lens of reinforcement learning which has shown a great potential to acquire complex behaviors from low-level data [3], [4], [5].In reinforcement learning (RL), the robot learns a policy or controller that maximizes a reward signal while solving a specific task.Despite the widespread potential benefits of RL, the field of robotic manipulation introduces various ongoing challenges in RL such as coping with the high complexity of manipulation tasks and the ability to efficiently tackle several tasks and transfer or reuse knowledge [6], [7].</p>
<p>RL strategies are grouped in two main branches: (1) model-based approaches in which the policy is built from This work was supported by DGA T45 23R and MCIN/AEI/ERDF/NextGenerationEU/PRTR project PID2021-125514NB-I00 1 All authors are with the Instituto de Investigación en Ingeniería de Aragón (i3A) and DIIS, University of Zaragoza * c.plou@unizar.esFig. 1.Overview of the active exploration problem through Bayesian model-based RL.The Bayesian model is responsible for predicting both the next state distribution and its degree of novelty.Lastly, we exploit the knowledge acquired during exploration to solve different tasks.simulated episodes, using a model that has been previously learnt to emulate the dynamics of the robot and the environment -i.e., to predict the resulting state after taking an action from the current state-and (2) model-free strategies in which the policy is directly built from episode execution in the robotic platform.</p>
<p>Model-based RL (MBRL) approaches are more sample efficient than model-free strategies since they utilize a model of the environment, akin to human imagination, to predict the outcomes of various actions.By leveraging this predictive capability, MBRL can select appropriate actions without the need for extensive trial and error in the real robot, thereby reducing the associated costs in terms of sample requirements [8].Furthermore, if we get a model that emulates the robot dynamics, we can leverage it to solve as many tasks as wanted, efficiently building a specific policy for each task.</p>
<p>To develop a model capable of simulating dynamics, it is essential to gather comprehensive data that accurately reflects these dynamics.This data collection is crucial for the model's training and, consequently, for building useful policies.Active learning appears as a highly suitable tool for this exploration.Active learning states that if a learning algorithm is allowed to choose the data from which it learns, it can yield enhanced performance levels with reduced training requirements.In this work, we leverage active learning basis to guide the exploration towards the most unknown regions of the state space, so as to learn more about them.The main challenge in this setting is to define a reward or utility which measures the information gain of visiting a region, which intrinsically requires a way to capture the information or uncertainty that is captured in a model and the novel observations.One approach for incorporating uncertainty in deep learning models is Bayesian deep learning (BDL), a branch of deep learning which leverages Bayesian statistics theory and enables to compute the information gain for observing new data.In this paper, we take advantage of BDL techniques to make our model Bayesian and, hence, exploit the uncertainty measure and compute the information utility of the exploration problem.Furthermore, quantifying how certain a deep learning model is about its prediction is essential in a vast amount of deep learning applications as it provides more robust predictions and is fundamental for safety related applications, such as medical applications or human-robot interaction.</p>
<p>To summarise, we propose how to combine some basis of BDL and active learning to efficiently explore the action and state spaces of a RL agent.Unlike other RL strategies that directly construct policies aimed at solving specific tasks, our approach is centered on thoroughly understanding the dynamics of the robot and the environment to, construct tailored policies for solving each of the diverse end-tasks with minimal interaction with the real robot.Our paper undertakes a comprehensive examination of diverse BDL techniques and information-based measures, showcasing superior performance, calibration, and efficiency compared to state-ofthe-art methods across multiple environments.Remarkably, our work extends the demonstration to robotic arms, opening new lines of research in this domain.Figure 1 shows an overview of our work, with a specific focus on the robotic arm environment in which it will operate.</p>
<p>II. RELATED WORK</p>
<p>This work explores novel RL strategies for robotic manipulation using BDL.We next discuss key related works in model-based RL, centering on the exploration, as well as relevant prior work on BDL.</p>
<p>A. Model-based Reinforcement Learning</p>
<p>Among the various RL research directions, MBRL stands out as a promising avenue to enhance the sample efficiency, benefiting a wide range of applications such as trajectory sampling [9] or robot navigation [10].In MBRL, a model is employed to learn the dynamics of state transitions.Current methodologies for constructing such models encompass both non-parametric such as Gaussian processes [11] or parametric, such as neural networks [12], [13] making either deterministic [14] or probabilistic predictions [15].An alternative involves world models which leverage generative models to hallucinate its own environment [16], [17].However, learning an accurate model in diverse DRL tasks with relatively complex environments as robotic manipulation is not a straightforward endeavor.Hence, our emphasis will be on pursuing a thorough exploration of the dynamics to obtain the most accurate model.In addition, MBRL based on Bayesian neural networks use deep ensembles which states to train N models with different architectures, hyperparameters, or initial weights [18].This is a very effective method [19], [20] but computationally expensive due to the need for training multiple networks.In contrast, MC-dropout approximates the posterior distribution through sampling M forward passes, while maintaining dropout layers active during inference [21], thus requiring training the network only once.In contrast, the Laplace approximation tries to find a Gaussian approximation to the posterior distribution of the weights [22], which has great advantages for robotics applications and provides better calibrations.</p>
<p>B. Exploration and Active Learning in DRL</p>
<p>Most of the exploration algorithms in reinforcement learning are reactive exploration methods.These methods rely on chance discoveries to drive exploration [23], [24].The agent explores the environment in a more random manner, hoping to accidentally find new and interesting areas.In contrast, a new trend is active exploration which exploits the active learning theory [25].</p>
<p>In the realm of machine learning, active learning (similarly to experimental design in other fields [26]) stands as a prominent strategy that optimizes the learning process by iteratively selecting the most informative instances from an unlabeled dataset [27].Previous work have addressed exploration for Transition Query Reinforcement Learning where all transitions are available for querying directly [28].However, in the general case in RL, most transitions are only available once the agent has reached a certain state, forcing to develop full active exploration policies instead of querying methods [26].Active exploration is more efficient and effective in high-dimensional environments.This approach requires a novelty or information-based measure which existing formulations include visitation count [24], [29], prediction error [30], [31], learning progress [32] and diversity in the visited states [33], [34].Other approaches leverage deep ensembles technique to compute the utility as either the variance [35] or the Jensen-Rényi Divergence [25] among samples.In contrast, we measure utility leveraging the entropy of the predictive distribution computed by Laplace Approximation.</p>
<p>III. BAYESIAN MODEL-BASED REINFORCEMENT LEARNING</p>
<p>Let the problem be represented as a Markov decision process (MDP) tuple (S, A, t * , R, ρ 0 ) where 1 S is the state space, 2 A is the action space, 3 t * = p * (s ′ |s, a) : S×A× S → [0, +∞) is the transition function, possibly unknown, representing the probability of reaching next state s ′ given the current state s and taking action a, 4 R : S × A → R is the reward function and 5 ρ 0 is the probability density function of the initial state.Without loss of generality, we are going to assume that both state and action spaces (S, A) are continuous.</p>
<p>In MBRL we build a probabilistic model to predict the distribution of the transition model, that is, p * (s ′ |s, a).We will regard this model as a neural network with weights θ.This neural network can output the parameters of a Gaussian distribution (µ θ , σ θ ) so as to predict the probability (
)1
This model is usually trained from a replay buffer of collected transitions D -i.e., sequences of states and actions as inputs {(s i , a i )} n i=1 and next states as outcomes or labels {s ′ i } n i=1 -to find the maximum a posteriori (MAP) of their weights,
θ M AP = arg max θ log p(θ|D) = arg max θ log p(D|θ)p(θ),(2)
where, first, p(D|θ) is the likelihood and, second, the prior distribution of the weights is p(θ) ∼ N (0, 1/γ 2 ), where γ 2 is the prior precision.As result, we get N θ M AP .</p>
<p>However, this strategy lacks proper uncertainty calibration because it captures only the uncertainty associated with the data, called aleatoric uncertainty, but it misses any model uncertainty, called epistemic uncertainty.A more reliable prediction is to compute a distribution over model parameters p(θ|D) and compute the transition function by marginalization of those parameters, that is:
p(s ′ |s, a) = Θ p(s ′ |s, a, θ)p(θ|D)dθ.(3)
By capturing the epistemic uncertainty, we are able to determine how much knowledge a model has and, more importantly for exploration, how much knowledge can be incorporated by adding new data (Figure 2).However, this equation is intractable and we must rely on approximate solutions to estimate it as described next (Figure 3).</p>
<p>A. Approximate Bayesian inference</p>
<p>Previous works that rely on Bayesian deep learning for MBRL use deep ensembles for approximate inference [25], [35].This method has become popular due to its predictive performance and simplicity to implement, as the Bayesian model is built using several networks trained from random initial parameters.The method is highly expensive both in terms on computing power, as the training process needs to be replicated multiple times, and in memory footprint, as the approach needs to manage and maintain several models at once.Furthermore, as the models need to be trained from random initializations, they cannot be pretrained or finetuned, limiting the transfer learning capabilities.</p>
<p>This work studies alternatives to deep ensembles in RL and robotics (Laplace approximation and Monte Carlo dropout) with the following characteristics: computationally efficient, excellent performance and the possibility of using pretrained networks.This makes these alternatives easy to integrate in an existing learning pipeline and aims to overcome the computational constraints associated with the deep ensembles technique.</p>
<p>Deep ensembles-.This method uses Bayesian model average through computing the predictive posterior by marginalization of the model weights θ trained on dataset D [18].Although strictly speaking, the deep ensemble samples are not generated from the full posterior distribution p(θ|D).The fact that they start from random locations may capture the multimodality of the predictive posterior distribution [19].Hence, following [35] we randomly initialize N neural networks and train them to find their MAP estimates {θ 1 , . . ., θ N }. Figure 4 shows an scheme of this method and the alternatives studied, described next.</p>
<p>Monte Carlo dropout-.Monte Carlo dropout (MCdroput) approximates the posterior as the sample distribution of N forward passes during inference time, maintaining active the random dropout layers.In other words, the network includes some dropout layers which are kept active at inference time.</p>
<p>Laplace approximation-.Trying to cover the predictive distribution (3) in a more reliable way, we may approximate the posterior distribution of the weights through Laplace approximation [22] as a Gaussian distribution,
p(θ|D) ≈ N (θ|θ M AP , H −1 ),(4)
where H = ∇ 2 θ log p(D|θ)| θ M AP and, subsequently, translate this uncertainty into the predictive variable.A first-order Taylor approximation (linearization) allows us to get a Gaussian predictive distribution (5) where [µ θ M AP , σ θ M AP ] = f θ M AP (s, a) is the network prediction and J = ∇ θ µ θ (s, a)| θ M AP .Nevertheless, when dealing with larger models, we are obliged to apply subnetwork inference.This technique considers as Bayesian weights only the n sub weights with greatest absolute values; i.e, the n sub weights that are theoretically more relevant [36].Besides, to translate this uncertainty into the predictive distribution, we will leverage Monte Carlo.Specifically, we take N samples of the subnetwork weights from (4) and perform a forward pass per sample, setting the remaining weights with their MAP estimate value.
p(s ′ |s, a) ≈ N (s ′ |µ θ M AP , J T H −1 J + diag(σ θ M AP )),</p>
<p>IV. EXPLORATION FOR MBRL</p>
<p>In the context of this work, we define the exploration problem as gathering valuable information in the replay buffer D to learn the most accurate dynamic model p(s ′ |s, a).Therefore, we can pose the exploration problem as an active learning problem.However, contrary to pure query strategy [28], data cannot be queried independently since a given state can only be reached by querying a previous sequence of states and actions that reaches the target state.Thus, our exploration problem can be defined as a MDP (S, A, t * , u, ρ 0 ), similar to the one presented in Section III, replacing the reward task by a utility function u that measures the novelty of the transition.</p>
<p>A. Exploration as experimental design</p>
<p>In our framework, we set an exploration MDP where the reward term is an experimental desing based utility.This utility is based on the expected information gain (IG) obtained by reaching and observing future state-action pair (s, a).The information gain may be defined as the amount of information gained about a random variable or signal from observing another random variable.In our scenario, the information gain by a transition z = {s, a, s ′ } can be defined as the KL-divergence of the model posterior distribution after and before adding the transition data, that is:
IG(s, a, s ′ ) = D KL (p(θ|D ∪ z) ∥ p(θ|D)) ,(6)
From this expression, the utility of an action is:
u(s, a) = S IG(s, a, s ′ )p(s ′ |s, a)ds ′ ,(7)
which can be used to compute the utility of a policy
U (π) = E p(θ|D) E p(s,a|π,θ) <a href="8">u(s, a</a>
where p(s, a|π, θ) is the probability of being in state s and selecting action a by following the policy π and the transition model defined by θ.Then, we can also define the action utility as the Jensen-Shanon (JS) divergence following the derivation of Shyam et al. [25]:
u(s, a) = H s ′ (E θ [p(s ′ |s, a, θ)]) − E θ <a href="9">H s ′ (p(s ′ |s, a, θ))</a> note that the first term corresponds to the entropy of E θ [p(s ′ |s, a, θ)], which is exactly the predictive posterior as defined in equation ( 3).Conceptually, it captures the total uncertainty (epistemic and aleatoric) of our model.The second term represents the expected entropy of the predictive distribution, that is, the aleatoric uncertainty.Therefore, we can see how the JS divergence is maximized where the epistemic uncertainty is maximized.This is consistent with the classical interpretation of epistemic uncertainty which captures the information or knowledge encoded in the model, contrary to the aleatoric uncertainty which encodes the data noise.Intuitively, the exploration is driven by data where we have high uncertainty because our model lacks information, not because the data themselves are noisy.</p>
<p>If we have a sample representation of our model {θ i } N i=1 , like in the case of deep ensembles of MC-dropout, we can compute the utility as the utility of a mixture of Gaussians:
u(s, a) = H 1 N N i=1 N θi − 1 N N i=1 H (N θi ) .(10)
However, the first term is the Shanon entropy of a mixture of Gaussians which does not have analytical solution.Instead, we can use the generalized Rényi entropy H α with α = 2 which has a closed form [25].Note that the Shanon entropy is H = lim α→1 H α .The problem of Rényi entropy with α &gt; 1 is that it is biased towards events of higher probability and ignores events of low probability.One advantage of the Laplace approximation is that the posterior model is approximated as a Gaussian distribution with a predictive covariance Σ LA = J T H −1 J + σ θ M AP (s, a)).Thus,
H s ′ (E θ [p(s ′ |s, a, θ)]) c = − log |Σ LA | E θ [H s ′ (p(s ′ |s, a, θ))] c = −E θ [log |σ θ (s, a))|]
where c = represents equal up to a constant term.In general, the second term does not have analytical form.However, we can assume homoscedastic noise, that is, σ θ is a constant value, independent of (s, a).In practice, we found that even if we try to learn heteroscedastic noise dependent of the current state and action, the variability is negligible compared to other terms.Therefore, if we assume homoscedastic noise, we can simplify our utility function:
u(s, a) c = H s ′ (E θ [p(s ′ |s, a, θ)]) c = − log |J T H −1 J|. (11)
This metric, which we call directly entropy metric, can be seen as a particular case of the idea commented before that epistemic uncertainty has to be the driving factor in exploration.This metric is also related to other metrics based on disagreement [35].In fact, we can also define a metric based purely on the entropy of the predictive model using the sample approximation of MC-dropout or deep ensembles.</p>
<p>Starting from (10), instead of approximating by Rényi entropy, we can approximate the mixture of Gaussians {N θ1 , . . ., N θ N } by a new Gaussian N M with mean µ M = 1 N N i=1 µ θi and covariance Σ M defined as,
Σ M = 1 N N i=1 diag(σ θi ) + µ θi µ T θi − µ M µ T M(12)
Then, if we also assume homoscedastic noise, we get a the entropy metric based on samples:
u(s, a) c = − log 1 N N i=1 µ θi µ T θi − µ M µ T M .(13)
V. PIPELINE</p>
<p>The pipeline followed to explore and evaluate is inspired by Shyam et al. [25].This section describes our pipeline main stages, which are summarized in Figure 5.</p>
<p>A. Exploration Pipeline</p>
<p>First of all, the backbone and focus of this problem is the exploration algorithm.This may be summarized as follows:</p>
<p>1) The agent starts acting randomly along n ex warm steps.As our agent interacts with the environment, it collects trajectories of the form {s i , a i , s ′ i } in the buffer D, where i denotes the number of exploration steps.</p>
<p>2) We train a model f ex from D that learns to simulate the dynamics of the robot.We leverage this model to build an exploration policy π ex which uses the information gain measure as utility.</p>
<p>3) The agent acts along n pol steps following policy π ex .</p>
<p>We continue saving these steps {s i , a i , s ′ i } in D. We repeat stages 2-3 until the agent reaches n ex steps exploration steps.Meanwhile, each n eval exploration steps, we evaluate the exploration accomplished up to that moment, entering in the evaluation pipeline with all the collected trajectories D up to that moment.</p>
<p>B. Evaluation Pipeline</p>
<p>We will enter in this pipeline to evalute the exploration conducted.To achieve this goal, we will try to solve a task from the knowledge attained in the exploration pipeline.Theoretically, the more exhaustive the exploration, the better the performance trying to solve the task.Hence, the evaluation pipeline may be explained in the next steps:</p>
<p>1) For each task of the environment, we repeat n k times the following stages in order to get a more accurate estimate of the agent's performance.2) We train a model f ev with all the collected trajectories D up to that moment to simulate the dynamics of the robot.We use this model to rapidly build an evaluation policy π ev following the reward of the desired task.</p>
<p>3) The agent acts along n ev steps steps following policy π ev .The evaluation metric is either the sum or the maximum of the task reward along these steps.For learning both pure exploration and task-specific policies from the model, we employed Soft-Actor Critic (SAC) which is the gold standard for reinforcement learning in robotics [37].Specifically, we run n pol eps episodes along n pol steps steps, during which the trained model f forecasts the consequences of the taken actions.This algorithm offers a significant efficiency advantage over model-free approaches, particularly in the total number of steps performed by the robot.For instance, employing SAC directly without a model would require n t • n k • n SAC steps real world interactions, where n t denotes the number of tasks.In contrast, with this pipeline, the robot only needs to execute n ex steps , which is substantially fewer than n SAC steps , as we will observe in SectionVI.This difference arises because the model is responsible for simulating most of the steps, demonstrating an improvement in sampling efficiency.</p>
<p>VI. EVALUATION DETAILS A. Environments</p>
<p>In order to evaluate our approach to model-based active exploration, we conducted experiments on several continuous environments of two widely used benchmarks such as OpenAI Gym [38] and RLBench [39].First, the tasks of OpenAI Gym are built using the MuJoCo physics engine [40].These are some commonly used environments in the RL literature, and consequently, we will utilize them for comparison with other studies.Second, RLBench is built around a CoppeliaSim/V-REP [41].We also test our Bayesian approaches and information metrics in a realistic robot manipulation problem, increasing the task's difficulty and consequently amplifying the challenge for the exploration algorithm.Specifically, the environments and tasks are the following ones:</p>
<p>• HalfCheetah-.It is a MuJoCo based planar kinematic string whose state space (|S| = 17) consists of the positional and velocity values of different joints, while the action space contains 6 continuous torques applied to the robot's joints.In this problem we considered two different tasks: (1) Running: move forward as faster as possible and (2) Flipping: perform flips.• Coppelia-.We utilize the Panda robot, with a state space (|S| = 43) encompassing the position, velocity, and forces of both its joints and gripper.Moreover, its action space comprises 8 velocity joints.We employed two different RlBench tasks: (1) PushButton: approximating its grip towards a button and (2) MoveBlock-ToTarget: push a red block positioned stochastically on a table towards the goal.Both rewards have been redefined to be continuous, ranging from 0 to 100.</p>
<p>B. Experimental details</p>
<p>The base neural network consists of a multi-layer perceptron (MLP) with 5 layers and 512 neurons per hidden layer.The input and the output sizes of the the MLP are |S| + |A| and 2|S|, respectively.We build the Bayesian approaches taking N = 32 models/forward passes/samples.Specifically, dropout is introduced in the middle hidden layer with a probability of p = 0.25 and Laplace approximation is applied only in the subnetwork comprised of the n subnet = 1k weights with highest absolute values.Regarding the utility measure, we will compare our entropy measure with respect to the Jensen-Rényi divergence [25].</p>
<p>Concerning the pipeline, we run n ex steps = 20k exploration steps.From which the first n ex warm = 256 steps correspond to warm-up.Besides, we recompute the exploration policy each n pol = 25 exploration steps and evaluate each n eval = 2000 exploration steps.First, to compute the exploration policy, we run n pol eps = 50 episodes along n pol steps = 50 steps where the consequence of n act = 128 actions are computed simultaneously.Second, to evaluate the specific task we run n k = 3 evaluation episodes in which we start computing the task policy (n pol eps = 250, n pol steps = 100, n act = 128) and, afterwards, we run n ev steps = 100 steps.The experiments were conducted with a Intel Core™ i7-12700K processor with 20 cores, and NVIDIA GeForce RTX 3090 GPU.</p>
<p>VII. RESULTS</p>
<p>In this section, we conduct a comparative analysis of the diverse Bayesian models and utility measures under examination.In addition to reward comparisons, we delve into computational time and storage analyses for the various Bayesian models.The state-of-the-art works selected as baselines can be categorized into three distinct approaches to tackling the challenge:</p>
<p>1) Model-free RL: Utilizing SAC [37] which is the gold standard algorithm in model-free RL.We construct a policy for each task by executing the same steps on the robotic platform as our exploration process.Besides, we set an upper bound on performance by running SAC for a significantly greater number of steps.</p>
<p>2) Model-based Exploration: while maintaining the existing pipeline, we replace our exploration approach by a reactive exploration algorithm as PERX [30] and simple naive random exploration.3) Model-based active Exploration (MAX) [25]: this work calculates the Jensen-Rényi divergence with a 32-deep ensemble based on equation (10).</p>
<p>A. Rewards</p>
<p>Our focus is on evaluating the performance of our approaches and baselines concerning exploration.We quantify performance by examining the rewards achieved during the evaluation phase, understanding that a higher task reward corresponds to a more comprehensive understanding of the environment -i.e., indicative of effective exploration-.To ensure a fair comparison, we employ the same Bayesian model in the evaluation stage, uniquely varying the buffer (collected along the exploration) used for model training.</p>
<p>HalfCheetah-. Figure 6 shows how Bayesian modelbased active exploration approaches clearly outperform the remaining methods.Specifically, those approaches where the Bayesian model is built from Laplace approximation and MC-dropout (ours) achieve a higher performance than those built from deep ensembles.Furthermore, these results indicate that our utility measure is more suitable for Laplacebased model, whereas deep ensembles fit better with Rényi entropy.Lastly, note that LapMCEnt approach overtakes the performance achieved by SAC despite of running x20 times more steps in the robot (20k vs 2×200k).</p>
<p>Coppelia-.Leveraging the methods that have shown the best performance in HalfCheetah, we focus on our problem of interest: Coppelia.In this high-dimensional environment, we observe again how Bayesian model-based active exploration approaches outperform the other approaches.These Bayesian approaches achieve a really similar performance.Besides, it is observed the difference in the task complexity.While these methods solve the PushButton task in nearly all the evaluation episodes, they have some difficulties with MoveBlock task.The reason is that, to solve this task, it is needed to know the dynamics of the box and to get this, the model requires many data about it.Otherwise, it will predict that the box is never moved.However, this data must be collected by querying a large sequence of states and actions that, finally, ends with an interaction robot-box.Therefore, the data collected along the exploration about the box dynamics does not seem to be enough for learning its kinetics.Particularly, the peak in the average of both tasks is reached by LapMCEnt.</p>
<p>B. Calibration</p>
<p>From here on, our experiments focus on the HalfCheetah environment to study other interesting metrics.We have observed the significant role of uncertainty estimation in Bayesian models in guiding exploration towards less-explored regions.It is crucial that this uncertainty aligns with the model's lack of knowledge (error), i.e., it is well calibrated.To assess this alignment, we utilize the Area Under Sparsification Error curve (AUSE) metric [19].Specifically, we will leverage the buffer stored at the end of the exploration (20k steps) to train each Bayesian model with the first 18k steps and test with the last 2k steps.Thus, we will compare its model prediction errors with respect to their utility values.These results can be found in Table I.Notably, Laplace-based model exhibit the highest calibration performance.MC-dropout and deep ensembles together with Rényi entropy (MAX) trail further in terms of calibration effectiveness.</p>
<p>C. Computational time and load</p>
<p>Table I also includes a comparative assessment of the Bayesian models in terms of computational time and memory efficiency.First, we trained all models with identical training Fig. 7. Results in Coppelia for the best approaches.As can be seen, the Laplace approximation with our entropy metric is more robust in the more difficult tasks (move block) due to its better support while sampling methods overfit to straightforward tasks (push button).sets (18k buffer size) and measure the time spent in a forward pass (inference).Remarkably, implementing MAX leads to an increase of training time since it requires to fit n ens neural networks.Additionally, it is observed that Laplace Approximation introduces extra computational overhead compared to MCdropout, since it must fit the covariance matrix of the posterior distribution (4).Regarding inference times, there are no substantial distinctions among Bayesian models.</p>
<p>Finally, we evaluate the Bayesian models from the perspective of storage requirements.The storage demands of these models depend on several factors, including the number of weights (n weights ) in the model, the amount of neural networks (n ens ) stacked to construct deep ensembles, and the size of the covariance matrix for the Laplace Approximation distribution (n 2 subnet ).We can express the storage cost as,
Cost = O n ens • n weights + n 2 subnet .(14)
In our scenario, wherein our MLP encompasses approximately n weights ≈ 1M of weights, opting for deep emsembles (MAX) leads to a memory overhead 32 times greater than that of MC-dropout and 16 times greater than that of Laplace Approximation, respectively.</p>
<p>VIII. CONCLUSIONS</p>
<p>This work demonstrates the benefits of model based RL for robotics systems, where sample efficiency is of paramount importance, by replacing expensive real world interactions by cheap model evaluations during policy training.Furthermore, we can perform active learning on the model reducing even further the need for real world interactions by carefully selecting the most informative ones.In addition, once the model is trained, it can be used for generalization of multiple tasks.The proposed Bayesian models not only pave the way for novel utility formulations but also offer enhancements in performance, calibration, and efficiency.Our research opens avenues for further exploration and application of Bayesian methodologies in addressing complex challenges in RL, such as robotic systems.We also highlight the potential of alternative Bayesian inference methods such as Laplace approximation or MC-dropout which have been barely investigated for robotics applications.In particular, the Laplace approximation presents interesting properties for robotic applications, such as open the posibility of using pretrained or foundational models or having a Gaussian posterior distribution with full support which can be used in posterior decision making.</p>
<p>Fig. 2 .
2
Fig. 2. Scheme of our Bayesian model in a toy example.The plot shows the difference between the aleatoric and the epistemic uncertainty.Besides, it showcases the predictive distribution for a given pair (s, a).</p>
<p>Fig. 3 .
3
Fig. 3. Differences among BDL methods for approximating the posterior distribution p(θ|D).While deep ensembles and MC-dropout both yield sampling approaches around the different local maxima of the posterior, Laplace approach estimates a Gaussian distribution around its peak θ M AP .</p>
<p>Fig. 4 .
4
Fig. 4.Scheme of BDL methods for approximating the predictive distribution in a neural network.While deep ensembles and MC-dropout get samples from multiple forward passes, Laplace method estimates a Gaussian through linearization technique (in the picture, the Laplace method is applied in a subnetwork).</p>
<p>Fig. 5 .
5
Fig. 5. Summary of both exploration and evaluation pipelines with their key stages.The starting points are marked with Init.Particularly, the exploration begins collecting data from a random policy and the evaluation starts from the buffer of trajectories collected along the exploration.</p>
<p>Fig. 6 .
6
Fig. 6.Results in HalfCheetah.First column model-based active exploration algorithms (ours + MAX).Second column, other approaches as presented in[25].MAX and SAC trained for 200k steps are represented in both sides for reference.As can be seen both Laplace and MCdropout outperform deep ensembles (MAX and EnsEnt) and the entropy metric is more consistent than the Jensen-Rényi divergence with Laplace approximation.</p>
<p>TABLE I CALIBRATION
I
AND COMPUTATIONAL TIME METRICS FOR THE DIFFERENT MODELS TESTED.BEST VALUES IN BOLD.
ModelAUSE ↓Training (s) Inference (s) ↓MAX [25]0.075122.756.25e-3LapMCEnt0.04839.986.00e-3MCdropRényi0.08534.356.91e-3</p>
<p>A survey on deep reinforcement learning algorithms for robotic manipulation. Dong Han, Beni Mulyana, Vladimir Stankovic, Samuel Cheng, Sensors. 23737622023</p>
<p>Robot learning towards smart robotic manufacturing: A review. Zhihao Liu, Quan Liu, Wenjun Xu, Lihui Wang, Zude Zhou, 202277102360Robotics and Computer-Integrated Manufacturing</p>
<p>Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine, ICRA. IEEE2017</p>
<p>Deep dynamics models for learning dexterous manipulation. Anusha Nagabandi, Kurt Konolige, Sergey Levine, Vikash Kumar, CoRL. PMLR2020</p>
<p>How to train your robot with deep reinforcement learning: lessons we have learned. Julian Ibarz, Jie Tan, Chelsea Finn, Mrinal Kalakrishnan, Peter Pastor, Sergey Levine, IJRR. 404-52021</p>
<p>Transfer learning in deep reinforcement learning: A survey. Zhuangdi Zhu, Kaixiang Lin, Anil K Jain, Jiayu Zhou, 2023TPAMI</p>
<p>Carlo D' Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters, arXiv:2401.09561Sharing knowledge in multi-task deep reinforcement learning. 2024arXiv preprint</p>
<p>A survey on model-based reinforcement learning. Fan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, Yang Yu, Science China Information Sciences. 6721211012024</p>
<p>Safe trajectory sampling in model-based reinforcement learning. Sicelukwanda Zwane, Denis Hadjivelichkov, Yicheng Luo, Yasemin Bekiroglu, Dimitrios Kanoulas, Marc P Deisenroth, CASE2023</p>
<p>Robust policy search for robot navigation. Javier Garcia, -Barcos , Ruben Martinez-Cantin, RA-L. 622021</p>
<p>PILCO: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, ICML. 2011</p>
<p>When to trust your model: Model-based policy optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, NeurIPS. 322019</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, NeurIPS. 312018</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, Sergey Levine, IEEE ICRA. 2018</p>
<p>Morel: Model-based offline reinforcement learning. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims, NeurIPS. 332020</p>
<p>World models. David Ha, Jürgen Schmidhuber, NeurIPS. 2018</p>
<p>Model-based reinforcement learning via imagination with derived memory. Yao Mu, Yuzheng Zhuang, Bin Wang, Guangxiang Zhu, Wulong Liu, Jianyu Chen, Ping Luo, Shengbo Li, Chongjie Zhang, Jianye Hao, NeurIPS. 342021</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. NeurIPS, 30. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, 2017</p>
<p>Evaluating scalable Bayesian deep learning methods for robust computer vision. Martin Fredrik K Gustafsson, Thomas B Danelljan, Schon, CVPR Workshops. 2020</p>
<p>Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, Jasper Snoek, NeurIPS. 322019</p>
<p>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, ICML. PMLR2016</p>
<p>Bayesian interpolation. J C David, Mackay, Neural computation. 431992</p>
<p>Deep exploration via bootstrapped dqn. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, NeurIPS. 292016</p>
<p>Vime: Variational information maximizing exploration. Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel, NeurIPS. 292016</p>
<p>Modelbased active exploration. Pranav Shyam, Wojciech Jaśkowski, Faustino Gomez, ICML. PMLR2019</p>
<p>Active learning for autonomous intelligent agents: Exploration, curiosity, and interaction. Manuel Lopes, Luis Montesano, arXiv:1403.14972014arXiv preprint</p>
<p>Active learning in robotics: A review of control principles. Annalisa T Taylor, Thomas A Berrueta, Todd D Murphey, Mechatronics. 771025762021</p>
<p>An experimental design perspective on model-based reinforcement learning. Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger, ICLR2021</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, NeurIPS. 292016</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, ICML. PMLR2017</p>
<p>Curious model-building control systems. Jürgen Schmidhuber, IJCNN. 1991</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. Manuel Lopes, Tobias Lang, Marc Toussaint, Pierre-Yves Oudeyer, NeurIPS. 252012</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, arXiv:1802.060702018arXiv preprint</p>
<p>Abandoning objectives: Evolution through the search for novelty alone. Joel Lehman, Kenneth O Stanley, Evolutionary computation. 1922011</p>
<p>Self-supervised exploration via disagreement. Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta, ICML. 2019</p>
<p>Laplace reduxeffortless Bayesian deep learning. Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, Philipp Hennig, NeurIPS. 3420089-20103, 2021</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, ICML. 2018</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>David Rovick Arrojo, and Andrew J Davison. RlBench: The robot learning benchmark &amp; learning environment. Stephen James, Zicong Ma, RA-L. 522020</p>
<p>MuJoCo: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IEEE/RSJ IROS. 2012</p>
<p>V-REP: A versatile and scalable robot simulation framework. Eric Rohmer, P N Surya, Marc Singh, Freese, IEEE/RSJ IROS. 2013</p>            </div>
        </div>

    </div>
</body>
</html>
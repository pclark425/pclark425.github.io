<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8011 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8011</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8011</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-df7d26339adf4eb0c07160947b9d2973c24911ba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df7d26339adf4eb0c07160947b9d2973c24911ba" target="_blank">Extracting Training Data from Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> USENIX Security Symposium</p>
                <p><strong>Paper TL;DR:</strong> This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model, and finds that larger models are more vulnerable than smaller models.</p>
                <p><strong>Paper Abstract:</strong> It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. 
We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. 
We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8011.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8011.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-token Perplexity (Language Model Likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard likelihood-based metric for language models measuring how well a model predicts a sequence; used as exp(-average log probability) over tokens (lower is better / more likely).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL), 345M (Medium), 117M (Small)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity (LM likelihood)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute per-token perplexity for a generated sequence using the target language model: P = exp(-1/n * sum_i log f_theta(x_i | x_{<i})). Low perplexity indicates high model-assigned likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perplexity value (scalar, lower=more likely)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Perplexity P = exp(- (1/n) * sum_{i=1..n} log f_theta(x_i | x_{1..i-1})). Unitless positive real number; smaller indicates sequence is more probable under the model.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to samples generated from GPT-2 and compared against web pages / training data; not tied to an external benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used in automated ranking; final confirmation performed by human reviewers (see separate human evaluation entry).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Perplexity alone yielded low-precision membership inference: e.g., for Top-n and Temperature sampling, only 9% and 3% of inspected samples respectively were memorized when ranked by perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>High likelihood sequences are not always memorized (false positives), e.g., repetitive sequences or trivial memorization; perplexity alone has poor precision for finding rare memorized strings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8011.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PerplexityRatio_SmMed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity Ratio vs Smaller Models (Small/Medium comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A membership-inference metric computed as the ratio (or difference in log-perplexities) between the large model's perplexity and a smaller model's perplexity; outliers indicate unexpected high likelihood under the larger model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (compared across sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL) compared to 345M (Medium) and 117M (Small)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / security / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric / membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity ratio to smaller model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute ratio (or log-difference) of perplexities assigned by the target (large) model and by a reference smaller model; sequences with anomalously low perplexity under the large model relative to the smaller one are flagged as potential memorized examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Perplexity ratio or log-perplexity difference (unitless); higher ratio indicates candidate memorization</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>ratio = logP_large / logP_small or equivalent; identifies samples where large model assigns much higher likelihood than small model.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to 200k generated samples per generation strategy from GPT-2; validated against GPT-2 training data</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Top-ranked candidates (100 per configuration) manually inspected and checked against web/training data.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Comparison-based metrics (Small/Medium ratios) found many rare memorized examples: e.g., Small and Medium strategies yielded multiple high-entropy memorized examples (13 and 10 respectively) and higher true positive rates than raw perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Requires access to a reference model (trained separately or smaller variant); effectiveness depends on differences in memorization behavior between models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8011.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>zlibEntropyRatio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>zlib Compression Entropy Ratio</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-neural baseline membership-inference metric that compares the model perplexity to the sequence's compressed length (zlib) to detect sequences that are non-compressible yet highly likely under the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP / security</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric / membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity to zlib-entropy ratio</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute compressed-size (bits) via zlib for a generated sequence and compare (ratio or log-difference) to the model's log-perplexity; sequences where the model assigns high likelihood but zlib compression shows high entropy are flagged as likely memorized (outliers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Ratio of log-perplexity to zlib-compressed entropy (unitless); higher indicates candidate memorization</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>metric = logP_model / (zlib_compressed_bits) or equivalent; extreme outliers in the top-left of perplexity vs zlib plot indicate memorization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to 200k generated samples; visualized in scatterplots (Figure 3/4)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>100 candidates per configuration manually inspected; zlib strategy produced high-precision lists (e.g., Internet+zlib had 67% true positive rate).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>zlib comparison was highly effective: e.g., 67% of Internet-conditioned samples selected by zlib were confirmed memorized; overall contributed substantially to the 604 confirmed memorized examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>zlib tends to capture repeated/trivial low-complexity sequences and may favor higher-k memorization (common text); less effective at finding some high-entropy rare strings compared to model-comparison metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8011.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LowercaseRatio</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity Ratio on Lowercased Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonicalization-based membership-inference metric comparing model perplexity on original vs lowercased versions of a sample to detect memorized sequences that rely on capitalization patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric / membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Perplexity ratio (original vs lowercased)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute perplexity on the original sample and on the lowercased version; high ratio indicates the model is sensitive to original capitalization (a potential sign of memorization of case-sensitive strings).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Ratio of perplexities (original / lowercased) (unitless); larger values indicate candidate memorization</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>metric = P_original / P_lowercased (or equivalently logP difference); unitless scalar.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied across generated sample sets from GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Candidates selected by this metric were manually inspected; metric often found headlines and logs with irregular capitalization.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Lowercasing strategy identified many headline/log-style memorized content; contributed to the 604 confirmed memorized examples (specific counts per table: e.g., Lowercase produced 53 Top-n, 22 Temperature, 60 Internet memorized samples in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Captures capitalization-dependent memorization but may miss memorized content not relying on case; canonicalization may also change semantics and affect metric reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8011.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WindowPerplexity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sliding-window Minimum Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A localized perplexity metric that takes the lowest average perplexity across sliding windows of tokens to detect short memorized substrings inside otherwise high-perplexity contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric / membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Sliding-window minimum perplexity (window length 50 tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute average perplexity over sliding windows (size 50 tokens) of the generated sequence and take the minimum; this highlights concentrated memorized substrings that would be diluted by averaging across the whole sample.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Minimum window perplexity (scalar, unitless); lower indicates a high-likelihood substring within the sample.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>min_{w sliding windows} exp(- (1/|w|) * sum log f_theta(token | context)); reported as perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to generated samples from GPT-2; window size chosen after cursory hyper-parameter sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used to rank candidates; selected candidates manually verified.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Window-based ranking helped recover examples with local memorized substrings; Table 2 shows Window strategy contributed substantially to identified memorized samples (e.g., Window found 58 Internet samples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Choice of window size is heuristic; may miss very short or very long memorized spans if window poorly matched.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8011.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MembershipInference_Likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Likelihood-based Membership Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Attack-style evaluation approach that predicts whether a sequence was in training data by using how confidently the model assigns likelihood (or related statistics) to the sequence; used here to detect memorized training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer security / ML privacy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>attack evaluation / membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Likelihood-based membership inference</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use model-assigned likelihood (perplexity) and auxiliary comparisons (to other models or compressors) to predict whether a sequence appears in training data; rank samples and verify top candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>True positive rate / precision among top-ranked candidates; counts of confirmed memorized examples</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = (# confirmed memorized among inspected candidates) / (# inspected candidates). Reported as percentage; absolute counts also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to 3x200k generated samples, selected 100 candidates per 18 configurations (total 1,800), validated against GPT-2 training data</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>1,800 selected samples manually inspected by 4 authors; confirmation via Google search and collaboration with GPT-2 authors checking training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Found 604 unique memorized examples out of 1,800 candidates (33.5% aggregate true positive); best configuration achieved 67% precision (Internet + zlib).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Method is heuristic and depends on quality/diversity of generated candidates; membership inference can suffer from false positives (repetitive outputs) and false negatives; requires human validation for ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8011.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-nSampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-n Sampling (a.k.a. top-k sampling with n)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/sampling strategy that zeros out all but the top-n token probabilities before renormalizing; used to generate candidate sequences from the LM for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>sampling / generation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Top-n sampling (n=40 used)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>At each decoding step keep only the top-n highest-probability tokens and sample from the renormalized distribution; used to generate 256-token samples for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Generated sample diversity and downstream membership-inference precision (empirical)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Empirically measured by number of unique memorized examples recovered and precision among inspected candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Generates samples from GPT-2; 200k samples per generation strategy</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Generated samples ranked and manually inspected as described in membership inference entry.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Top-n baseline yielded low diversity and lower precision; Top-n sampling produced 191 unique memorized samples (out of the 1,800 candidate set) compared to 273 when conditioning on Internet text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Tends to generate less diverse outputs and repeated memorized sequences; not ideal alone for broad discovery of memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8011.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecayingTemperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decaying Temperature Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling strategy that starts with a high softmax temperature to explore diverse prefixes and decays to temperature 1 to allow the model to follow high-confidence continuations, improving chance of hitting memorized strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>sampling / generation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Temperature-decay sampling (start t=10 -> t=1 over first ~20 tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Flatten probabilities early to diversify prefix sampling then reduce randomness to allow completion of high-likelihood (possibly memorized) continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Empirical recovery of memorized examples and sample diversity</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Measured by count of unique memorized sequences recovered and precision among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to GPT-2 sample generation (200k samples)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Candidates selected using this sampling strategy were manually inspected as part of the 1,800-sample evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Temperature sampling alone had low precision when ranked by perplexity (3% of inspected were memorized), but contributed to discovery when combined with comparison metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Choice of temperature schedule is heuristic; high temperature early may still prevent following memorized continuations if decay is too slow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8011.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternetConditioning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditioning on Internet Text (context seeding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling strategy that seeds the model with real-world prefixes from a Common Crawl subset (Internet text) to elicit completions similar to model training distribution and increase likelihood of exposing memorized content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>sampling / generation method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Internet-conditioned sampling</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample prefixes (5-10 tokens) from scraped Common Crawl data and use them as context for model generation (then top-n sampling) to diversify prefixes toward data similar to training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Empirical recovery counts and precision of memorized outputs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Measured by number of unique memorized examples recovered and percent precision among candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Common Crawl subset (50 MB scraped), used as conditioning prefixes; GPT-2 training data (Reddit-sourced) used for validation</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Internet-conditioned generation was the most effective: generated 273 unique memorized samples and, when paired with zlib metric, yielded up to 67% precision.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Conditioning on Internet text recovered the largest number of unique memorized examples (273) and produced the highest-precision combinations (e.g., Internet+zlib 67%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Potential overlap (small) between the conditioning corpus and the model's training data may 'cheat' slightly; relies on availability of realistic prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8011.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeamSearchLikeDecoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam-search-like Decoding for Long Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-recall decoding technique (beam-search-like) used to extend extracted memorized snippets into much longer verbatim sequences compared to greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>decoding method</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Beam-search-like extension decoding</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a seed snippet, apply a beam-search-like procedure (from prior work) to find high-likelihood continuations conservatively, enabling extraction of long verbatim sequences (e.g., full files).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Ability to extend memorized snippets (length in tokens/lines) and success rate of producing verbatim training content</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported as lengths recovered (e.g., 1450 lines of source code) and counts of extended memorized sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to identified memorized 256-token samples to extend their length; validated against GPT-2 training corpus</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Extended outputs were manually checked against training data; examples include recovering entire license texts and thousands of lines of code.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Successfully extended many memorized samples; e.g., recovered a 1450-line source file and entire license texts, indicating much memorized content can be expanded.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Beam-search-like methods can be computationally heavier and may require additional context; greedy decoding may miss long verbatim continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8011.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-EideticMemorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Eidetic Memorization (Formal Criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal definition introduced in this paper defining extractable memorization: a string s is k-eidetic memorized if it is extractable from the LM and appears in at most k distinct training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML privacy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>formal criterion / evaluative definition</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>k-eidetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Two-part definition: (1) extractability  there exists a prefix c such that s is the most likely continuation (or extractable via sampling/decoding); (2) frequency constraint  s appears in at most k distinct training examples. Smaller k indicates stronger (more harmful) memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>k (integer) indicating maximum number of distinct training examples containing the string; often reported as k=1 (most severe)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>k  Z_{>=1}; s is k-eidetic memorized if extractable and |{x  X : s  x}|  k. Extractability operationalized via generation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied specifically to GPT-2 training corpus (Reddit-sourced OpenWebText-style dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human verification used to count occurrences of s in training data (OpenAI authors provided grep/fuzzy 3-gram checks); Table 3 provides k=1 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Identified many k=1 high-entropy strings (e.g., UUIDs, random IDs) and reported counts of occurrences in training data; overall 604 unique memorized examples identified, including many with small k.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Definition admits pathological cases (e.g., prompting models to repeat arbitrary strings); authors mitigate by restricting prompts to short prefixes; k depends on how training examples are defined (document-level) which affects interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8011.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DifferentialPrivacy_DP-SGD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential Privacy (DP) and DP-SGD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Framework and algorithm family for training models with formal privacy guarantees by adding noise and clipping gradients (e.g., DP-SGD), discussed as a principled mitigation against memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / privacy / ML</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>privacy framework / defense</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Differential privacy (DP) via DP-SGD</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Train models with mechanisms that bound how much a single training record can influence parameters (DP guarantees); DP-SGD clips per-example gradients and adds calibrated noise during training to provide (,)-DP at a chosen granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>DP privacy parameters (, ) and downstream utility (accuracy / perplexity)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Privacy budget  (lower = stronger privacy) with failure probability ; utility measured in standard ML metrics (e.g., perplexity, accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Discussed generally; referenced works and frameworks (Opacus, TensorFlow Privacy) but not applied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable here; DP only discussed as mitigation tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>No DP training performed in this paper; authors note DP can prevent memorization but typically degrades utility and increases training time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Applying DP at web-scale is challenging (granularity, utility loss); DP may prevent capturing long-tail distributions and is computationally costly for large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8011.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeDupTrigramMultiset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fuzzy De-duplication via Trigram Multiset Overlap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A de-duplication criterion used when selecting candidate samples to avoid double-counting memorized content by marking samples as duplicates if 50% of trigrams overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / NLP / data processing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>data preprocessing criterion</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Trigram-multiset overlap de-duplication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute multiset of word-level trigrams for samples; mark s1 duplicate of s2 if |tri(s1)  tri(s2)|  |tri(s1)| / 2, thereby avoiding duplicated memorized content in candidate lists.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Duplicate detection boolean / count; reduces redundant manual inspection</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Overlap fraction threshold = 0.5 on trigram-multisets; deterministic rule.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to 200k generated samples to choose top-100 candidates per configuration</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used prior to human inspection to ensure candidates are diverse; manual verification followed.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Fuzzy de-duplication used to select diverse sets of 100 candidates per configuration (reducing redundant examples like repeated license files).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Heuristic choice of trigram and 50% threshold may merge distinct but related examples or fail to merge paraphrases; tuned for practical efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8011.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8011.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanManualInspection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual Human Verification via Web Search and Training-data Matching</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluation procedure where authors manually inspect candidate sequences using Internet search and collaborating queries to the original training dataset authors to confirm memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computer science / ML evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human evaluation / ground-truth verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual inspection + training-data confirmation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each candidate, authors search the Internet for exact matches; suspected memorized sequences were then sent to GPT-2 authors who ran fuzzy 3-gram matches and greps on the training corpus to confirm occurrences and counts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Binary memorized / not-memorized labels; counts of occurrences in training data; aggregated precision (e.g., 33.5% true positive across 1,800 candidates).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = # confirmed memorized / # inspected; occurrence counts reported as integer frequencies in training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>1,800 candidate sequences (100 from each of 18 configurations); GPT-2 training dataset used for validation; Google search and web cache used as auxiliary evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>1,800 samples manually inspected by four authors; OpenAI performed corpus checks (fuzzy 3-gram and grep) to confirm membership; selection bias applied to favor low-ranked samples via sqrt sampling scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>604 unique memorized examples confirmed out of 1,800 candidates (33.5%); best variant achieved 67% precision; manual inspection and training-data checks formed ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Manual inspection is labor-intensive and may miss memorized examples (false negatives); fuzzy 3-gram matching can produce false positives; possible partial overlap between conditioning corpus and training data may bias results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Extracting Training Data from Large Language Models', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The secret sharer: Evaluating and testing unintended memorization in neural networks <em>(Rating: 2)</em></li>
                <li>Deep learning with differential privacy <em>(Rating: 2)</em></li>
                <li>Membership inference attacks against machine learning models <em>(Rating: 2)</em></li>
                <li>AutoPrompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 1)</em></li>
                <li>Learning differentially private recurrent language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8011",
    "paper_id": "paper-df7d26339adf4eb0c07160947b9d2973c24911ba",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Perplexity",
            "name_full": "Per-token Perplexity (Language Model Likelihood)",
            "brief_description": "Standard likelihood-based metric for language models measuring how well a model predicts a sequence; used as exp(-average log probability) over tokens (lower is better / more likely).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL), 345M (Medium), 117M (Small)",
            "scientific_domain": "computer science / natural language processing",
            "theory_type": "evaluation metric",
            "evaluation_method_name": "Perplexity (LM likelihood)",
            "evaluation_method_description": "Compute per-token perplexity for a generated sequence using the target language model: P = exp(-1/n * sum_i log f_theta(x_i | x_{&lt;i})). Low perplexity indicates high model-assigned likelihood.",
            "evaluation_metric": "Perplexity value (scalar, lower=more likely)",
            "metric_definition": "Perplexity P = exp(- (1/n) * sum_{i=1..n} log f_theta(x_i | x_{1..i-1})). Unitless positive real number; smaller indicates sequence is more probable under the model.",
            "dataset_or_benchmark": "Applied to samples generated from GPT-2 and compared against web pages / training data; not tied to an external benchmark",
            "human_evaluation_details": "Used in automated ranking; final confirmation performed by human reviewers (see separate human evaluation entry).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Perplexity alone yielded low-precision membership inference: e.g., for Top-n and Temperature sampling, only 9% and 3% of inspected samples respectively were memorized when ranked by perplexity.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "High likelihood sequences are not always memorized (false positives), e.g., repetitive sequences or trivial memorization; perplexity alone has poor precision for finding rare memorized strings.",
            "uuid": "e8011.0",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "PerplexityRatio_SmMed",
            "name_full": "Perplexity Ratio vs Smaller Models (Small/Medium comparison)",
            "brief_description": "A membership-inference metric computed as the ratio (or difference in log-perplexities) between the large model's perplexity and a smaller model's perplexity; outliers indicate unexpected high likelihood under the larger model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (compared across sizes)",
            "model_size": "1.5B (XL) compared to 345M (Medium) and 117M (Small)",
            "scientific_domain": "computer science / security / NLP",
            "theory_type": "evaluation metric / membership inference",
            "evaluation_method_name": "Perplexity ratio to smaller model",
            "evaluation_method_description": "Compute ratio (or log-difference) of perplexities assigned by the target (large) model and by a reference smaller model; sequences with anomalously low perplexity under the large model relative to the smaller one are flagged as potential memorized examples.",
            "evaluation_metric": "Perplexity ratio or log-perplexity difference (unitless); higher ratio indicates candidate memorization",
            "metric_definition": "ratio = logP_large / logP_small or equivalent; identifies samples where large model assigns much higher likelihood than small model.",
            "dataset_or_benchmark": "Applied to 200k generated samples per generation strategy from GPT-2; validated against GPT-2 training data",
            "human_evaluation_details": "Top-ranked candidates (100 per configuration) manually inspected and checked against web/training data.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Comparison-based metrics (Small/Medium ratios) found many rare memorized examples: e.g., Small and Medium strategies yielded multiple high-entropy memorized examples (13 and 10 respectively) and higher true positive rates than raw perplexity.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Requires access to a reference model (trained separately or smaller variant); effectiveness depends on differences in memorization behavior between models.",
            "uuid": "e8011.1",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "zlibEntropyRatio",
            "name_full": "zlib Compression Entropy Ratio",
            "brief_description": "A non-neural baseline membership-inference metric that compares the model perplexity to the sequence's compressed length (zlib) to detect sequences that are non-compressible yet highly likely under the LM.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP / security",
            "theory_type": "evaluation metric / membership inference",
            "evaluation_method_name": "Perplexity to zlib-entropy ratio",
            "evaluation_method_description": "Compute compressed-size (bits) via zlib for a generated sequence and compare (ratio or log-difference) to the model's log-perplexity; sequences where the model assigns high likelihood but zlib compression shows high entropy are flagged as likely memorized (outliers).",
            "evaluation_metric": "Ratio of log-perplexity to zlib-compressed entropy (unitless); higher indicates candidate memorization",
            "metric_definition": "metric = logP_model / (zlib_compressed_bits) or equivalent; extreme outliers in the top-left of perplexity vs zlib plot indicate memorization",
            "dataset_or_benchmark": "Applied to 200k generated samples; visualized in scatterplots (Figure 3/4)",
            "human_evaluation_details": "100 candidates per configuration manually inspected; zlib strategy produced high-precision lists (e.g., Internet+zlib had 67% true positive rate).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "zlib comparison was highly effective: e.g., 67% of Internet-conditioned samples selected by zlib were confirmed memorized; overall contributed substantially to the 604 confirmed memorized examples.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "zlib tends to capture repeated/trivial low-complexity sequences and may favor higher-k memorization (common text); less effective at finding some high-entropy rare strings compared to model-comparison metrics.",
            "uuid": "e8011.2",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "LowercaseRatio",
            "name_full": "Perplexity Ratio on Lowercased Text",
            "brief_description": "A canonicalization-based membership-inference metric comparing model perplexity on original vs lowercased versions of a sample to detect memorized sequences that rely on capitalization patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "evaluation metric / membership inference",
            "evaluation_method_name": "Perplexity ratio (original vs lowercased)",
            "evaluation_method_description": "Compute perplexity on the original sample and on the lowercased version; high ratio indicates the model is sensitive to original capitalization (a potential sign of memorization of case-sensitive strings).",
            "evaluation_metric": "Ratio of perplexities (original / lowercased) (unitless); larger values indicate candidate memorization",
            "metric_definition": "metric = P_original / P_lowercased (or equivalently logP difference); unitless scalar.",
            "dataset_or_benchmark": "Applied across generated sample sets from GPT-2",
            "human_evaluation_details": "Candidates selected by this metric were manually inspected; metric often found headlines and logs with irregular capitalization.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Lowercasing strategy identified many headline/log-style memorized content; contributed to the 604 confirmed memorized examples (specific counts per table: e.g., Lowercase produced 53 Top-n, 22 Temperature, 60 Internet memorized samples in Table 2).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Captures capitalization-dependent memorization but may miss memorized content not relying on case; canonicalization may also change semantics and affect metric reliability.",
            "uuid": "e8011.3",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "WindowPerplexity",
            "name_full": "Sliding-window Minimum Perplexity",
            "brief_description": "A localized perplexity metric that takes the lowest average perplexity across sliding windows of tokens to detect short memorized substrings inside otherwise high-perplexity contexts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "evaluation metric / membership inference",
            "evaluation_method_name": "Sliding-window minimum perplexity (window length 50 tokens)",
            "evaluation_method_description": "Compute average perplexity over sliding windows (size 50 tokens) of the generated sequence and take the minimum; this highlights concentrated memorized substrings that would be diluted by averaging across the whole sample.",
            "evaluation_metric": "Minimum window perplexity (scalar, unitless); lower indicates a high-likelihood substring within the sample.",
            "metric_definition": "min_{w sliding windows} exp(- (1/|w|) * sum log f_theta(token | context)); reported as perplexity.",
            "dataset_or_benchmark": "Applied to generated samples from GPT-2; window size chosen after cursory hyper-parameter sweep.",
            "human_evaluation_details": "Used to rank candidates; selected candidates manually verified.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Window-based ranking helped recover examples with local memorized substrings; Table 2 shows Window strategy contributed substantially to identified memorized samples (e.g., Window found 58 Internet samples).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Choice of window size is heuristic; may miss very short or very long memorized spans if window poorly matched.",
            "uuid": "e8011.4",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "MembershipInference_Likelihood",
            "name_full": "Likelihood-based Membership Inference",
            "brief_description": "Attack-style evaluation approach that predicts whether a sequence was in training data by using how confidently the model assigns likelihood (or related statistics) to the sequence; used here to detect memorized training examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer security / ML privacy",
            "theory_type": "attack evaluation / membership inference",
            "evaluation_method_name": "Likelihood-based membership inference",
            "evaluation_method_description": "Use model-assigned likelihood (perplexity) and auxiliary comparisons (to other models or compressors) to predict whether a sequence appears in training data; rank samples and verify top candidates.",
            "evaluation_metric": "True positive rate / precision among top-ranked candidates; counts of confirmed memorized examples",
            "metric_definition": "Precision = (# confirmed memorized among inspected candidates) / (# inspected candidates). Reported as percentage; absolute counts also reported.",
            "dataset_or_benchmark": "Applied to 3x200k generated samples, selected 100 candidates per 18 configurations (total 1,800), validated against GPT-2 training data",
            "human_evaluation_details": "1,800 selected samples manually inspected by 4 authors; confirmation via Google search and collaboration with GPT-2 authors checking training corpus.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Found 604 unique memorized examples out of 1,800 candidates (33.5% aggregate true positive); best configuration achieved 67% precision (Internet + zlib).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Method is heuristic and depends on quality/diversity of generated candidates; membership inference can suffer from false positives (repetitive outputs) and false negatives; requires human validation for ground truth.",
            "uuid": "e8011.5",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "Top-nSampling",
            "name_full": "Top-n Sampling (a.k.a. top-k sampling with n)",
            "brief_description": "A decoding/sampling strategy that zeros out all but the top-n token probabilities before renormalizing; used to generate candidate sequences from the LM for analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "sampling / generation method",
            "evaluation_method_name": "Top-n sampling (n=40 used)",
            "evaluation_method_description": "At each decoding step keep only the top-n highest-probability tokens and sample from the renormalized distribution; used to generate 256-token samples for ranking.",
            "evaluation_metric": "Generated sample diversity and downstream membership-inference precision (empirical)",
            "metric_definition": "Empirically measured by number of unique memorized examples recovered and precision among inspected candidates.",
            "dataset_or_benchmark": "Generates samples from GPT-2; 200k samples per generation strategy",
            "human_evaluation_details": "Generated samples ranked and manually inspected as described in membership inference entry.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Top-n baseline yielded low diversity and lower precision; Top-n sampling produced 191 unique memorized samples (out of the 1,800 candidate set) compared to 273 when conditioning on Internet text.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Tends to generate less diverse outputs and repeated memorized sequences; not ideal alone for broad discovery of memorization.",
            "uuid": "e8011.6",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "DecayingTemperature",
            "name_full": "Decaying Temperature Sampling",
            "brief_description": "A sampling strategy that starts with a high softmax temperature to explore diverse prefixes and decays to temperature 1 to allow the model to follow high-confidence continuations, improving chance of hitting memorized strings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "sampling / generation method",
            "evaluation_method_name": "Temperature-decay sampling (start t=10 -&gt; t=1 over first ~20 tokens)",
            "evaluation_method_description": "Flatten probabilities early to diversify prefix sampling then reduce randomness to allow completion of high-likelihood (possibly memorized) continuations.",
            "evaluation_metric": "Empirical recovery of memorized examples and sample diversity",
            "metric_definition": "Measured by count of unique memorized sequences recovered and precision among candidates.",
            "dataset_or_benchmark": "Applied to GPT-2 sample generation (200k samples)",
            "human_evaluation_details": "Candidates selected using this sampling strategy were manually inspected as part of the 1,800-sample evaluation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Temperature sampling alone had low precision when ranked by perplexity (3% of inspected were memorized), but contributed to discovery when combined with comparison metrics.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Choice of temperature schedule is heuristic; high temperature early may still prevent following memorized continuations if decay is too slow.",
            "uuid": "e8011.7",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "InternetConditioning",
            "name_full": "Conditioning on Internet Text (context seeding)",
            "brief_description": "A sampling strategy that seeds the model with real-world prefixes from a Common Crawl subset (Internet text) to elicit completions similar to model training distribution and increase likelihood of exposing memorized content.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "sampling / generation method",
            "evaluation_method_name": "Internet-conditioned sampling",
            "evaluation_method_description": "Sample prefixes (5-10 tokens) from scraped Common Crawl data and use them as context for model generation (then top-n sampling) to diversify prefixes toward data similar to training distribution.",
            "evaluation_metric": "Empirical recovery counts and precision of memorized outputs",
            "metric_definition": "Measured by number of unique memorized examples recovered and percent precision among candidates.",
            "dataset_or_benchmark": "Common Crawl subset (50 MB scraped), used as conditioning prefixes; GPT-2 training data (Reddit-sourced) used for validation",
            "human_evaluation_details": "Internet-conditioned generation was the most effective: generated 273 unique memorized samples and, when paired with zlib metric, yielded up to 67% precision.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Conditioning on Internet text recovered the largest number of unique memorized examples (273) and produced the highest-precision combinations (e.g., Internet+zlib 67%).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Potential overlap (small) between the conditioning corpus and the model's training data may 'cheat' slightly; relies on availability of realistic prefixes.",
            "uuid": "e8011.8",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "BeamSearchLikeDecoding",
            "name_full": "Beam-search-like Decoding for Long Extraction",
            "brief_description": "A high-recall decoding technique (beam-search-like) used to extend extracted memorized snippets into much longer verbatim sequences compared to greedy decoding.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP",
            "theory_type": "decoding method",
            "evaluation_method_name": "Beam-search-like extension decoding",
            "evaluation_method_description": "Given a seed snippet, apply a beam-search-like procedure (from prior work) to find high-likelihood continuations conservatively, enabling extraction of long verbatim sequences (e.g., full files).",
            "evaluation_metric": "Ability to extend memorized snippets (length in tokens/lines) and success rate of producing verbatim training content",
            "metric_definition": "Reported as lengths recovered (e.g., 1450 lines of source code) and counts of extended memorized sequences.",
            "dataset_or_benchmark": "Applied to identified memorized 256-token samples to extend their length; validated against GPT-2 training corpus",
            "human_evaluation_details": "Extended outputs were manually checked against training data; examples include recovering entire license texts and thousands of lines of code.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Successfully extended many memorized samples; e.g., recovered a 1450-line source file and entire license texts, indicating much memorized content can be expanded.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Beam-search-like methods can be computationally heavier and may require additional context; greedy decoding may miss long verbatim continuations.",
            "uuid": "e8011.9",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "k-EideticMemorization",
            "name_full": "k-Eidetic Memorization (Formal Criterion)",
            "brief_description": "A formal definition introduced in this paper defining extractable memorization: a string s is k-eidetic memorized if it is extractable from the LM and appears in at most k distinct training examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / ML privacy",
            "theory_type": "formal criterion / evaluative definition",
            "evaluation_method_name": "k-eidetic memorization",
            "evaluation_method_description": "Two-part definition: (1) extractability  there exists a prefix c such that s is the most likely continuation (or extractable via sampling/decoding); (2) frequency constraint  s appears in at most k distinct training examples. Smaller k indicates stronger (more harmful) memorization.",
            "evaluation_metric": "k (integer) indicating maximum number of distinct training examples containing the string; often reported as k=1 (most severe)",
            "metric_definition": "k  Z_{&gt;=1}; s is k-eidetic memorized if extractable and |{x  X : s  x}|  k. Extractability operationalized via generation procedures.",
            "dataset_or_benchmark": "Applied specifically to GPT-2 training corpus (Reddit-sourced OpenWebText-style dataset)",
            "human_evaluation_details": "Human verification used to count occurrences of s in training data (OpenAI authors provided grep/fuzzy 3-gram checks); Table 3 provides k=1 examples.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Identified many k=1 high-entropy strings (e.g., UUIDs, random IDs) and reported counts of occurrences in training data; overall 604 unique memorized examples identified, including many with small k.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Definition admits pathological cases (e.g., prompting models to repeat arbitrary strings); authors mitigate by restricting prompts to short prefixes; k depends on how training examples are defined (document-level) which affects interpretation.",
            "uuid": "e8011.10",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "DifferentialPrivacy_DP-SGD",
            "name_full": "Differential Privacy (DP) and DP-SGD",
            "brief_description": "Framework and algorithm family for training models with formal privacy guarantees by adding noise and clipping gradients (e.g., DP-SGD), discussed as a principled mitigation against memorization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computer science / privacy / ML",
            "theory_type": "privacy framework / defense",
            "evaluation_method_name": "Differential privacy (DP) via DP-SGD",
            "evaluation_method_description": "Train models with mechanisms that bound how much a single training record can influence parameters (DP guarantees); DP-SGD clips per-example gradients and adds calibrated noise during training to provide (,)-DP at a chosen granularity.",
            "evaluation_metric": "DP privacy parameters (, ) and downstream utility (accuracy / perplexity)",
            "metric_definition": "Privacy budget  (lower = stronger privacy) with failure probability ; utility measured in standard ML metrics (e.g., perplexity, accuracy).",
            "dataset_or_benchmark": "Discussed generally; referenced works and frameworks (Opacus, TensorFlow Privacy) but not applied in experiments.",
            "human_evaluation_details": "Not applicable here; DP only discussed as mitigation tradeoff.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "No DP training performed in this paper; authors note DP can prevent memorization but typically degrades utility and increases training time.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Applying DP at web-scale is challenging (granularity, utility loss); DP may prevent capturing long-tail distributions and is computationally costly for large LMs.",
            "uuid": "e8011.11",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "DeDupTrigramMultiset",
            "name_full": "Fuzzy De-duplication via Trigram Multiset Overlap",
            "brief_description": "A de-duplication criterion used when selecting candidate samples to avoid double-counting memorized content by marking samples as duplicates if 50% of trigrams overlap.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / NLP / data processing",
            "theory_type": "data preprocessing criterion",
            "evaluation_method_name": "Trigram-multiset overlap de-duplication",
            "evaluation_method_description": "Compute multiset of word-level trigrams for samples; mark s1 duplicate of s2 if |tri(s1)  tri(s2)|  |tri(s1)| / 2, thereby avoiding duplicated memorized content in candidate lists.",
            "evaluation_metric": "Duplicate detection boolean / count; reduces redundant manual inspection",
            "metric_definition": "Overlap fraction threshold = 0.5 on trigram-multisets; deterministic rule.",
            "dataset_or_benchmark": "Applied to 200k generated samples to choose top-100 candidates per configuration",
            "human_evaluation_details": "Used prior to human inspection to ensure candidates are diverse; manual verification followed.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Fuzzy de-duplication used to select diverse sets of 100 candidates per configuration (reducing redundant examples like repeated license files).",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Heuristic choice of trigram and 50% threshold may merge distinct but related examples or fail to merge paraphrases; tuned for practical efficiency.",
            "uuid": "e8011.12",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "HumanManualInspection",
            "name_full": "Manual Human Verification via Web Search and Training-data Matching",
            "brief_description": "Human evaluation procedure where authors manually inspect candidate sequences using Internet search and collaborating queries to the original training dataset authors to confirm memorization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "1.5B (XL)",
            "scientific_domain": "computer science / ML evaluation",
            "theory_type": "human evaluation / ground-truth verification",
            "evaluation_method_name": "Manual inspection + training-data confirmation",
            "evaluation_method_description": "For each candidate, authors search the Internet for exact matches; suspected memorized sequences were then sent to GPT-2 authors who ran fuzzy 3-gram matches and greps on the training corpus to confirm occurrences and counts.",
            "evaluation_metric": "Binary memorized / not-memorized labels; counts of occurrences in training data; aggregated precision (e.g., 33.5% true positive across 1,800 candidates).",
            "metric_definition": "Precision = # confirmed memorized / # inspected; occurrence counts reported as integer frequencies in training corpus.",
            "dataset_or_benchmark": "1,800 candidate sequences (100 from each of 18 configurations); GPT-2 training dataset used for validation; Google search and web cache used as auxiliary evidence.",
            "human_evaluation_details": "1,800 samples manually inspected by four authors; OpenAI performed corpus checks (fuzzy 3-gram and grep) to confirm membership; selection bias applied to favor low-ranked samples via sqrt sampling scheme.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "604 unique memorized examples confirmed out of 1,800 candidates (33.5%); best variant achieved 67% precision; manual inspection and training-data checks formed ground truth.",
            "comparison_to_human_generated": false,
            "comparison_results": null,
            "limitations_noted": "Manual inspection is labor-intensive and may miss memorized examples (false negatives); fuzzy 3-gram matching can produce false positives; possible partial overlap between conditioning corpus and training data may bias results.",
            "uuid": "e8011.13",
            "source_info": {
                "paper_title": "Extracting Training Data from Large Language Models",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The secret sharer: Evaluating and testing unintended memorization in neural networks",
            "rating": 2
        },
        {
            "paper_title": "Deep learning with differential privacy",
            "rating": 2
        },
        {
            "paper_title": "Membership inference attacks against machine learning models",
            "rating": 2
        },
        {
            "paper_title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 1
        },
        {
            "paper_title": "Learning differentially private recurrent language models",
            "rating": 1
        }
    ],
    "cost": 0.022213,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Extracting Training Data from Large Language Models</h1>
<p>Nicholas Carlini ${ }^{1}$ Florian Tramr ${ }^{2}$ Eric Wallace ${ }^{3}$ Matthew Jagielski ${ }^{4}$<br>Ariel Herbert-Voss ${ }^{5,6}$ Katherine Lee ${ }^{1}$ Adam Roberts ${ }^{1}$ Tom Brown ${ }^{5}$<br>Dawn Song ${ }^{3}$ lfar Erlingsson ${ }^{7}$ Alina Oprea ${ }^{4}$ Colin Raffel ${ }^{1}$<br>${ }^{1}$ Google ${ }^{2}$ Stanford ${ }^{3}$ UC Berkeley ${ }^{4}$ Northeastern University ${ }^{5}$ OpenAI ${ }^{6}$ Harvard ${ }^{7}$ Apple</p>
<h4>Abstract</h4>
<p>It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.</p>
<p>We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data.</p>
<p>We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.</p>
<h2>1 Introduction</h2>
<p>Language models (LMs)statistical models which assign a probability to a sequence of words-are fundamental to many natural language processing tasks. Modern neural-networkbased LMs use very large model architectures (e.g., 175 billion parameters [7]) and train on massive datasets (e.g., nearly a terabyte of English text [55]). This scaling increases the ability of LMs to generate fluent natural language [53,74,76], and also allows them to be applied to a plethora of other tasks [29,39,55], even without updating their parameters [7].</p>
<p>At the same time, machine learning models are notorious for exposing information about their (potentially private) training data-both in general $[47,65]$ and in the specific case of language models [8,45]. For instance, for certain models it is known that adversaries can apply membership inference attacks [65] to predict whether or not any particular example was in the training data.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our extraction attack. Given query access to a neural network language model, we extract an individual person's name, email address, phone number, fax number, and physical address. The example in this figure shows information that is all accurate so we redact it to protect privacy.</p>
<p>Such privacy leakage is typically associated with overfitting [75]-when a model's training error is significantly lower than its test error-because overfitting often indicates that a model has memorized examples from its training set. Indeed, overfitting is a sufficient condition for privacy leakage [72] and many attacks work by exploiting overfitting [65].</p>
<p>The association between overfitting and memorization haserroneously-led many to assume that state-of-the-art LMs will not leak information about their training data. Because these models are often trained on massive de-duplicated datasets only for a single epoch [7, 55], they exhibit little to no overfitting [53]. Accordingly, the prevailing wisdom has been that "the degree of copying with respect to any given work is likely to be, at most, de minimis" [71] and that models do not significantly memorize any particular training example.</p>
<p>Contributions. In this work, we demonstrate that large language models memorize and leak individual training examples. In particular, we propose a simple and efficient method for extracting verbatim sequences from a language model's training set using only black-box query access. Our key insight is that, although training examples do not have noticeably lower losses than test examples on average, certain worstcase training examples are indeed memorized.</p>
<p>In our attack, we first generate a large, diverse set of highlikelihood samples from the model, using one of three generalpurpose sampling strategies. We then sort each sample using one of six different metrics that estimate the likelihood of each sample using a separate reference model (e.g., another LM), and rank highest the samples with an abnormally high likelihood ratio between the two models.</p>
<p>Our attacks directly apply to any language model, including those trained on sensitive and non-public data [10,16]. We use the GPT-2 model [54] released by OpenAI as a representative language model in our experiments. We choose to attack GPT-2 to minimize real-world harm-the GPT-2 model and original training data source are already public.</p>
<p>To make our results quantitative, we define a testable definition of memorization. We then generate 1,800 candidate memorized samples, 100 under each of the $3 \times 6$ attack configurations, and find that over 600 of them are verbatim samples from the GPT-2 training data (confirmed in collaboration with the creators of GPT-2). In the best attack configuration, $67 \%$ of candidate samples are verbatim training examples. Our most obviously-sensitive attack extracts the full name, physical address, email address, phone number, and fax number of an individual (see Figure 1). We comprehensively analyze our attack, including studying how model size and string frequency affects memorization, as well as how different attack configurations change the types of extracted data.</p>
<p>We conclude by discussing numerous practical strategies to mitigate privacy leakage. For example, differentially-private training [1] is theoretically well-founded and guaranteed to produce private models if applied at an appropriate record level, but it can result in longer training times and typically degrades utility. We also make recommendations, such as carefully de-duplicating documents, that empirically will help to mitigate memorization but cannot prevent all attacks.</p>
<h2>2 Background \&amp; Related Work</h2>
<p>To begin, we introduce the relevant background on large (billion-parameter) neural network-based language models (LMs) as well as data privacy attacks.</p>
<h3>2.1 Language Modeling</h3>
<p>Language models are a fundamental building block of current state-of-the-art natural language processing pipelines $[12,31,50,52,55]$. While the unsupervised objectives used
to train these models vary, one popular choice is a "next-step prediction" objective [5, 31, 44, 52]. This approach constructs a generative model of the distribution</p>
<p>$$
\operatorname{Pr}\left(x_{1}, x_{2}, \ldots, x_{n}\right)
$$</p>
<p>where $x_{1}, x_{2}, \ldots, x_{n}$ is a sequence of tokens from a vocabulary $\mathcal{V}$ by applying the chain rule of probability</p>
<p>$$
\operatorname{Pr}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\Pi_{i=1}^{n} \operatorname{Pr}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)
$$</p>
<p>State-of-the-art LMs use neural networks to estimate this probability distribution. We let $f_{\theta}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)$ denote the likelihood of token $x_{i}$ when evaluating the neural network $f$ with parameters $\theta$. While recurrent neural networks (RNNs) [26, 44] used to be a common choice for the neural network architecture of LMs, attention-based models [4] have recently replaced RNNs in state-of-the-art models. In particular, Transformer LMs [70] consist of a sequence of attention layers and are the current model architecture of choice. Because we believe our results are independent of the exact architecture used, we will not describe the Transformer architecture in detail here and instead refer to existing work [3].</p>
<p>Training Objective. A language model is trained to maximize the probability of the data in a training set $\mathcal{X}$. In this paper, each training example is a text document-for example, a specific news article or webpage from the internet. Formally, training involves minimizing the loss function</p>
<p>$$
\mathcal{L}(\theta)=-\log \Pi_{i=1}^{n} f_{\theta}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)
$$</p>
<p>over each training example in the training dataset $\mathcal{X}$. Because of this training setup, the "optimal" solution to the task of language modeling is to memorize the answer to the question "what token follows the sequence $x_{1}, \ldots, x_{i-1}$ ?" for every prefix in the training set. However, state-of-the-art LMs are trained with massive datasets, which causes them to not exhibit significant forms of memorization: empirically, the training loss and the test loss are nearly identical [7, 53, 55].</p>
<p>Generating Text. A language model can generate new text (potentially conditioned on some prefix $x_{1}, \ldots, x_{i}$ ) by iteratively sampling $\hat{x}<em _theta="\theta">{i+1} \sim f</em>}\left(x_{i+1} \mid x_{1}, \ldots, x_{i}\right)$ and then feeding $\hat{x<em i_2="i+2">{i+1}$ back into the model to sample $\hat{x}</em>$ [18]).} \sim$ $f_{\theta}\left(x_{i+2} \mid x_{1}, \ldots, \hat{x}_{i+1}\right)$. This process is repeated until a desired stopping criterion is reached. Variations of this text generation method include deterministically choosing the most-probable token rather than sampling (i.e., "greedy" sampling) or setting all but the top- $n$ probabilities to zero and renormalizing the probabilities before sampling (i.e., top- $n$ sampling ${ }^{1</p>
<p>GPT-2. Our paper focuses on the GPT variant of Transformer LMs [7,52,54]. Specifically, we demonstrate our training data extraction attacks on GPT-2, a family of LMs that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>were all trained using the same dataset and training algorithm, but with varying model sizes. GPT-2 uses a word-pieces [61] vocabulary with a byte pair encoder [22].</p>
<p>GPT-2 XL is the largest model with 1.5 billion parameters. For the remainder of this paper, the "GPT-2" model refers to this 1.5 billion parameter model or, when we specifically indicate this, its Small and Medium variants with 124 million and 334 million parameters, respectively.</p>
<p>The GPT-2 model family was trained on data scraped from the public Internet. The authors collected a dataset by following outbound links from the social media website Reddit. The webpages were cleaned of HTML, with only the document text retained, and then de-duplicated at the document level. This resulted in a final dataset of 40 GB of text data, over which the model was trained for approximately 12 epochs. ${ }^{2}$ As a result, GPT-2 does not overfit: the training loss is only roughly $10 \%$ smaller than the test loss across all model sizes.</p>
<h3>2.2 Training Data Privacy</h3>
<p>It is undesirable for models to remember any details that are specific to their (potentially private) training data. The field of training data privacy develops attacks (to leak training data details) and defenses (to prevent leaks).</p>
<p>Privacy Attacks. When models are not trained with privacy-preserving algorithms, they are vulnerable to numerous privacy attacks. The least revealing form of attack is the membership inference attack $[28,47,65,67]$ : given a trained model, an adversary can predict whether or not a particular example was used to train the model. Separately, model inversion attacks [21] reconstruct representative views of a subset of examples (e.g., a model inversion attack on a face recognition classifier might recover a fuzzy image of a particular person that the classifier can recognize).</p>
<p>Training data extraction attacks, like model inversion attacks, reconstruct training datapoints. However, training data extraction attacks aim to reconstruct verbatim training examples and not just representative "fuzzy" examples. This makes them more dangerous, e.g., they can extract secrets such as verbatim social security numbers or passwords. Training data extraction attacks have until now been limited to small LMs trained on academic datasets under artificial training setups (e.g., for more epochs than typical) $[8,66,68,73]$, or settings where the adversary has a priori knowledge of the secret they want to extract (e.g., a social security number) $[8,27]$.</p>
<p>Protecting Privacy. An approach to minimizing memorization of training data is to apply differentially-private training techniques $[1,9,43,60,64]$. Unfortunately, training models with differentially-private mechanisms often reduces accuracy [34] because it causes models to fail to capture the long</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tails of the data distribution [19, 20,67]. Moreover, it increases training time, which can further reduce accuracy because current LMs are limited by the cost of training [35, 38, 55]. As a result, state-of-the-art LMs such as GPT-2 [53], GPT-3 [7], and T5 [55] do not apply these privacy-preserving techniques.</p>
<h2>3 Threat Model \&amp; Ethics</h2>
<p>Training data extraction attacks are often seen as theoretical or academic and are thus unlikely to be exploitable in practice [71]. This is justified by the prevailing intuition that privacy leakage is correlated with overfitting [72], and because state-of-the-art LMs are trained on large (near terabyte-sized [7]) datasets for a few epochs, they tend to not overfit [53].</p>
<p>Our paper demonstrates that training data extraction attacks are practical. To accomplish this, we first precisely define what we mean by "memorization". We then state our threat model and our attack objectives. Finally, we discuss the ethical considerations behind these attacks and explain why they are likely to be a serious threat in the future.</p>
<h3>3.1 Defining Language Model Memorization</h3>
<p>There are many ways to define memorization in language modeling. As mentioned earlier, memorization is in many ways an essential component of language models because the training objective is to assign high overall likelihood to the training dataset. LMs must, for example, "memorize" the correct spelling of individual words.</p>
<p>Indeed, there is a research direction that analyzes neural networks as repositories of (memorized) knowledge [51, 59]. For example, when GPT-2 is prompted to complete the sentence "My address is 1 Main Street, San Francisco CA", it generates "94107": a correct zip code for San Francisco, CA. While this is clearly memorization in some abstract form, we aim to formalize our definition of memorization in order to restrict it to cases that we might consider "unintended" [8].</p>
<h3>3.1.1 Eidetic Memorization of Text</h3>
<p>We define eidetic memorization as a particular type of memorization. ${ }^{3}$ Informally, eidetic memorization is data that has been memorized by a model despite only appearing in a small set of training instances. The fewer training samples that contain the data, the stronger the eidetic memorization is.</p>
<p>To formalize this notion, we first define what it means for a model to have knowledge of a string $s$. Our definition is loosely inspired by knowledge definitions in interactive proof systems [24]: a model $f_{\emptyset}$ knows a string $s$ if $s$ can be extracted by interacting with the model. More precisely, we focus on black-box interactions where the model generates $s$ as the most likely continuation when prompted with some prefix $c$ :</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Definition 1 (Model Knowledge Extraction) A string $s$ is extractable ${ }^{4}$ from an $L M f_{\theta}$ if there exists a prefix $c$ such that:</p>
<p>$$
s \leftarrow \underset{s^{\prime}:\left|s^{\prime}\right|=N}{\arg \max } f_{\theta}\left(s^{\prime} \mid c\right)
$$</p>
<p>We abuse notation slightly here to denote by $f_{\theta}\left(s^{\prime} \mid c\right)$ the likelihood of an entire sequence $s^{\prime}$. Since computing the most likely sequence $s$ is intractable for large $N$, the argmax in Definition 1 can be replaced by an appropriate sampling strategy (e.g., greedy sampling) that reflects the way in which the model $f_{\theta}$ generates text in practical applications. We then define eidetic memorization as follows:</p>
<p>Definition 2 ( $k$-Eidetic Memorization) A string $s$ is $k$ eidetic memorized (for $k \geq 1$ ) by an $L M f_{\theta}$ if $s$ is extractable from $f_{\theta}$ and $s$ appears in at most $k$ examples in the training data $X:\left|{x \in X: s \subseteq x}\right| \leq k$.</p>
<p>Key to this definition is what "examples" means. For GPT2, each webpage is used (in its entirety) as one training example. Since this definition counts the number of distinct training examples containing a given string, and not the total number of times the string occurs, a string may appear multiple times on one page while still counting as $k=1$ memorization.</p>
<p>This definition allows us to define memorization as a spectrum. While there is no definitive value of $k$ at which we might say that memorization is unintentional and potentially harmful, smaller values are more likely to be so. For any given $k$, memorizing longer strings is also "worse" than shorter strings, although our definition omits this distinction for simplicity.</p>
<p>For example, under this definition, memorizing the correct spellings of one particular word is not severe if the word occurs in many training examples (i.e., $k$ is large). Memorizing the zip code of a particular city might be eidetic memorization, depending on whether the city was mentioned in many training examples (e.g., webpages) or just a few. Referring back to Figure 1, memorizing an individual person's name and phone number clearly (informally) violates privacy expectations, and also satisfies our formal definition: it is contained in just a few documents on the Internet-and hence the training data.</p>
<h3>3.2 Threat Model</h3>
<p>Adversary's Capabilities. We consider an adversary who has black-box input-output access to a language model. This allows the adversary to compute the probability of arbitrary sequences $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)$, and as a result allows the adversary to obtain next-word predictions, but it does not allow the adversary to inspect individual weights or hidden states (e.g., attention vectors) of the language model.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>This threat model is highly realistic as many LMs are available through black-box APIs. For example, the GPT3 model [7] created by OpenAI is available through black-box API access. Auto-complete models trained on actual user data have also been made public, although they reportedly use privacy-protection measures during training [10].</p>
<p>Adversary's Objective. The adversary's objective is to extract memorized training data from the model. The strength of an attack is measured by how private (formalized as being $k$-eidetic memorized) a particular example is. Stronger attacks extract more examples in total (both more total sequences, and longer sequences) and examples with lower values of $k$.</p>
<p>We do not aim to extract targeted pieces of training data, but rather indiscriminately extract training data. While targeted attacks have the potential to be more adversarially harmful, our goal is to study the ability of LMs to memorize data generally, not to create an attack that can be operationalized by real adversaries to target specific users.</p>
<p>Attack Target. We select GPT-2 [54] as a representative LM to study for our attacks. GPT-2 is nearly a perfect target. First, from an ethical standpoint, the model and data are public, and so any memorized data that we extract is already public. ${ }^{5}$ Second, from a research standpoint, the dataset (despite being collected from public sources) was never actually released by OpenAI. Thus, it is not possible for us to unintentionally "cheat" and develop attacks that make use of knowledge of the GPT-2 training dataset.</p>
<h3>3.3 Risks of Training Data Extraction</h3>
<p>Training data extraction attacks present numerous privacy risks. From an ethical standpoint, most of these risks are mitigated in our paper because we attack GPT-2, whose training data is public. However, since our attacks would apply to any LM, we also discuss potential consequences of future attacks on models that may be trained on private data.</p>
<p>Data Secrecy. The most direct form of privacy leakage occurs when data is extracted from a model that was trained on confidential or private data. For example, GMail's autocomplete model [10] is trained on private text communications between users, so the extraction of unique snippets of training data would break data secrecy.</p>
<p>Contextual Integrity of Data. The above privacy threat corresponds to a narrow view of data privacy as data secrecy.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>A broader view of the privacy risks posed by data extraction stems from the framework of data privacy as contextual integrity [48]. That is, data memorization is a privacy infringement if it causes data to be used outside of its intended context. An example violation of contextual integrity is shown in Figure 1. This individual's name, address, email, and phone number are not secret-they were shared online in a specific context of intended use (as contact information for a software project) - but are reproduced by the LM in a separate context. Due to failures such as these, user-facing applications that use LMs may inadvertently emit data in inappropriate contexts, e.g., a dialogue system may emit a user's phone number in response to another user's query.</p>
<p>Small- $k$ Eidetic Risks. We nevertheless focus on $k$-eidetic memorization with a small $k$ value because it makes extraction attacks more impactful. While there are cases where large- $k$ memorization may still matter (for example, a company may refer to the name of an upcoming product multiple times in private-and even though it is discussed often the name itself may still be sensitive) we study the small- $k$ case.</p>
<p>Moreover, note that although we frame our paper as an "attack", LMs will output memorized data even in the absence of an explicit adversary. We treat LMs as black-box generative functions, and the memorized content that we extract can be generated through honest interaction with the LM. Indeed, we have even discovered at least one memorized training example among the 1,000 GPT-3 samples that OpenAI originally released in its official repository [49].</p>
<h3>3.4 Ethical Considerations</h3>
<p>In this paper, we will discuss and carefully examine specific memorized content that we find in our extraction attacks. This raises ethical considerations as some of the data that we extract contains information about individual users.</p>
<p>As previously mentioned, we minimize ethical concerns by using data that is already public. We attack the GPT-2 model, which is available online. Moreover, the GPT-2 training data was collected from the public Internet [54], and is in principle available to anyone who performs the same (documented) collection process as OpenAI, e.g., see [23].</p>
<p>However, there are still ethical concerns even though the model and data are public. It is possible-and indeed we find it is the case-that we might extract personal information for individuals from the training data. For example, as shown in Figure 1, we recovered a person's full name, address, and phone number. In this paper, whenever we succeed in extracting personally-identifying information (usernames, phone numbers, etc.) we partially mask out this content with the token We are aware of the fact that this does not provide complete mediation: disclosing that the vulnerability exists allows a malicious actor to perform these attacks on their own to recover this personal information.</p>
<p>Just as responsible disclosure still causes some (limited) harm, we believe that the benefits of publicizing these attacks outweigh the potential harms. Further, to make our attacks public, we must necessarily reveal some sensitive information. We contacted the individual whose information is partially shown in Figure 1 to disclose this fact to them in advance and received permission to use this example. Our research findings have also been disclosed to OpenAI.</p>
<p>Unfortunately, we cannot hope to contact all researchers who train large LMs in advance of our publication. We thus hope that this publication will spark further discussions on the ethics of memorization and extraction among other companies and research teams that train large LMs [2,36,55,63].</p>
<h2>4 Initial Training Data Extraction Attack</h2>
<p>We begin with a simple strawman baseline for extracting training data from a language model in a two-step procedure.</p>
<ul>
<li>Generate text. We generate a large quantity of data by unconditionally sampling from the model (Section 4.1).</li>
<li>Predict which outputs contain memorized text. We next remove the generated samples that are unlikely to contain memorized text using a membership inference attack (Section 4.2).
These two steps correspond directly to extracting model knowledge (Definition 1), and then predicting which strings might be $k$-eidetic memorization (Definition 2).</li>
</ul>
<h3>4.1 Initial Text Generation Scheme</h3>
<p>To generate text, we initialize the language model with a onetoken prompt containing a special start-of-sentence token and then repeatedly sample tokens in an autoregressive fashion from the model (see Section 2.1 for background). We hope that by sampling according to the model's assigned likelihood, we will sample sequences that the model considers "highly likely", and that likely sequences correspond to memorized text. Concretely, we sample exactly 256 tokens for each trial using the top- $n$ strategy from Section 2.1 with $n=40$.</p>
<h3>4.2 Initial Membership Inference</h3>
<p>Given a set of samples from the model, the problem of training data extraction reduces to one of membership inference: predict whether each sample was present in the training data [65]. In their most basic form, past membership inference attacks rely on the observation that models tend to assign higher confidence to examples that are present in the training data [46]. Therefore, a potentially high-precision membership inference classifier is to simply choose examples that are assigned the highest likelihood by the model.</p>
<p>Since LMs are probabilistic generative models, we follow prior work [8] and use a natural likelihood measure: the per-</p>
<p>plexity of a sequence measures how well the LM "predicts" the tokens in that sequence. Concretely, given a sequence of tokens $x_{1}, \ldots, x_{n}$, the perplexity is defined as</p>
<p>$$
\mathscr{P}=\exp \left(-\frac{1}{n} \sum_{i=1}^{n} \log f_{\theta}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)\right)
$$</p>
<p>That is, if the perplexity is low, then the model is not very "surprised" by the sequence and has assigned on average a high probability to each subsequent token in the sequence.</p>
<h3>4.3 Initial Extraction Results</h3>
<p>We generate 200,000 samples using the largest version of the GPT-2 model (XL, 1558M parameters) following the text generation scheme described in Section 4.1. We then sort these samples according to the model's perplexity measure and investigate those with the lowest perplexity.</p>
<p>This simple baseline extraction attack can find a wide variety of memorized content. For example, GPT-2 memorizes the entire text of the MIT public license, as well as the user guidelines of Vaughn Live, an online streaming site. While this is "memorization", it is only $k$-eidetic memorization for a large value of $k$-these licenses occur thousands of times.</p>
<p>The most interesting (but still not eidetic memorization for low values of $k$ ) examples include the memorization of popular individuals' Twitter handles or email addresses (omitted to preserve user privacy). In fact, all memorized content we identify in this baseline setting is likely to have appeared in the training dataset many times.</p>
<p>This initial approach has two key weaknesses that we can identify. First, our sampling scheme tends to produce a low diversity of outputs. For example, out of the 200,000 samples we generated, several hundred are duplicates of the memorized user guidelines of Vaughn Live.</p>
<p>Second, our baseline membership inference strategy suffers from a large number of false positives, i.e., content that is assigned high likelihood but is not memorized. The majority of these false positive samples contain "repeated" strings (e.g., the same phrase repeated multiple times). Despite such text being highly unlikely, large LMs often incorrectly assign high likelihood to such repetitive sequences [30].</p>
<h2>5 Improved Training Data Extraction Attack</h2>
<p>The proof-of-concept attack presented in the previous section has low precision (high-likelihood samples are not always in the training data) and low recall (it identifies no $k$-memorized content for low $k$ ). Here, we improve the attack by incorporating better methods for sampling from the model (Section 5.1) and membership inference (Section 5.2).</p>
<h3>5.1 Improved Text Generation Schemes</h3>
<p>The first step in our attack is to randomly sample from the language model. Above, we used top- $n$ sampling and conditioned the LM on the start-of-sequence token as input. This strategy has clear limitations [32]: it will only generate sequences that are likely from beginning to end. As a result, top- $n$ sampling from the model will cause it to generate the same (or similar) examples several times. Below we describe two alternative techniques for generating more diverse samples from the LM.</p>
<h3>5.1.1 Sampling With A Decaying Temperature</h3>
<p>As described in Section 2.1, an LM outputs the probability of the next token given the prior tokens $\operatorname{Pr}\left(x_{i} \mid x_{1}, \ldots, x_{i-1}\right)$. In practice, this is achieved by evaluating the neural network $z=$ $f_{\theta}\left(x_{1}, \ldots, x_{i-1}\right)$ to obtain the "logit" vector $z$, and then computing the output probability distribution as $y=\operatorname{softmax}(z)$ defined by $\operatorname{softmax}(z)<em i="i">{i}=\exp \left(z</em>\right)$.}\right) / \sum_{j=1}^{n} \exp \left(z_{j</p>
<p>One can artificially "flatten" this probability distribution to make the model less confident by replacing the output $\operatorname{softmax}(z)$ with $\operatorname{softmax}(z / t)$, for $t&gt;1$. Here, $t$ is called the temperature. A higher temperature causes the model to be less confident and more diverse in its output.</p>
<p>However, maintaining a high temperature throughout the generation process would mean that even if the sampling process began to emit a memorized example, it would likely randomly step off the path of the memorized output. Thus, we use a softmax temperature that decays over time, starting at $t=10$ and decaying down to $t=1$ over a period of the first 20 tokens ( $\approx 10 \%$ of the length of the sequence). This gives a sufficient amount of time for the model to "explore" a diverse set of prefixes while also allowing it to follow a high-confidence paths that it finds.</p>
<h3>5.1.2 Conditioning on Internet Text</h3>
<p>Even when applying temperature sampling, there are still some prefixes that are unlikely to be sampled but nevertheless occur in actual data. As a final strategy, our third sampling strategy seeds the model with prefixes from our own scrapes of the Internet. This sampling strategy ensures that we will generate samples with a diverse set of prefixes that are similar in nature to the type of data GPT-2 was trained on.</p>
<p>We follow a different data collection process as used in GPT-2 (which follows Reddit links) in order to reduce the likelihood that our dataset has any intersection with the model's training data. In particular, we select samples from a subset of Common Crawl ${ }^{6}$ to feed as context to the model. ${ }^{7}$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Workflow of our extraction attack and evaluation. 1) Attack. We begin by generating many samples from GPT-2 when the model is conditioned on (potentially empty) prefixes. We then sort each generation according to one of six metrics and remove the duplicates. This gives us a set of potentially memorized training examples. 2) Evaluation. We manually inspect 100 of the top-1000 generations for each metric. We mark each generation as either memorized or not-memorized by manually searching online, and we confirm these findings by working with OpenAI to query the original training data. An open-source implementation of our attack process is available at https://github.com/ftramer/LM_Memorization.</p>
<p>As in prior work [55], we perform basic data-sanitization by removing HTML and JavaScript from webpages, and we de-duplicate data on a line-by-line basis. This gives us a dataset of 50 MB of text. We randomly sample between 5 and 10 tokens of context from this scraped data and then continue LM generation with top- $n$ sampling as in Section 4.1.</p>
<h3>5.2 Improved Membership Inference</h3>
<p>Performing membership inference by filtering out samples with low likelihood has poor precision due to failures in the underlying language model: there are many samples that are assigned spuriously high likelihood. There are predominantly two categories of such samples:</p>
<ul>
<li>Trivial memorization. We identify many cases where GPT-2 outputs content that is uninteresting because of how common the text is. For example, it repeats the numbers from 1 to 100 with high probability.</li>
<li>Repeated substrings. One common failure mode of LMs is their propensity to repeatedly emit the same string over and over [30,37]. We found many of the high-likelihood samples that are not memorized are indeed repeated texts (e.g., "I love you. I love you...").</li>
</ul>
<p>Our insight is that we can filter out these uninteresting (yet still high-likelihood samples) by comparing to a second LM. Given a second model that accurately captures text likelihood, we should expect it will also assign high likelihood to these forms of memorized content. Therefore, a natural strategy for finding more diverse and rare forms of memorization is to filter samples where the original model's likelihood is "unexpectedly high" compared to a second model. Below we discuss four methods for achieving this.</p>
<p>Comparing to Other Neural Language Models. Assume that we have access to a second LM that memorizes a different set of examples than GPT-2. One way to achieve this would be to train a model on a disjoint set of training data, in which case it is unlikely that the two models will memorize the same data for small $k$. An alternate strategy is to take a much smaller model trained on the same underlying dataset: because smaller models have less capacity for memorization, we conjecture that there are samples that are $k$-eidetic memorized (for small $k$ ) by the largest GPT-2 model, but which are not memorized by smaller GPT-2 models. Specifically, we use the Small (117M parameters) and Medium (345M parameters) models.</p>
<p>Comparing to zlib Compression. It is not necessary that we compare to another neural LM; any technique that quantifies some notion of "surprise" for a given sequence can be useful. As a simple baseline method, we compute the zlib [41] entropy of the text: the number of bits of entropy when the sequence is compressed with zlib compression. We then use the ratio of the GPT-2 perplexity and the zlib entropy as our membership inference metric. Although text compressors are simple, they can identify many of the examples of trivial memorization and repeated patterns described above (e.g., they are excellent at modeling repeated substrings).</p>
<p>Comparing to Lowercased Text. Instead of detecting memorization by comparing one model to another model, another option detects memorization by comparing the perplexity of the model to the perplexity of the same model on a "canonicalized" version of that sequence. Specifically, we measure the ratio of the perplexity on the sample before and after lowercasing it, which can dramatically alter the perplexity of memorized content that expects a particular casing.</p>
<p>Perplexity on a Sliding Window. Sometimes a model is not confident when the sample contains one memorized substring surrounded by a block of non-memorized (and high perplexity) text. To handle this, we use the minimum perplexity when averaged over a sliding window of 50 tokens. ${ }^{8}$</p>
<h2>6 Evaluating Memorization</h2>
<p>We now evaluate the various data extraction methods and study common themes in the resulting memorized content.</p>
<h3>6.1 Methodology</h3>
<p>An overview of our experimental setup is shown in Figure 2. We first build three datasets of 200,000 generated samples (each of which is 256 tokens long) using one of our strategies:</p>
<ul>
<li>Top-n (4.1) samples naively from the empty sequence.</li>
<li>Temperature (5.1.1) increases diversity during sampling.</li>
<li>Internet (5.1.2) conditions the LM on Internet text.</li>
</ul>
<p>We order each of these three datasets according to each of our six membership inference metrics:</p>
<ul>
<li>Perplexity: the perplexity of the largest GPT-2 model.</li>
<li>Small: the ratio of log-perplexities of the largest GPT-2 model and the Small GPT-2 model.</li>
<li>Medium: the ratio as above, but for the Medium GPT-2.</li>
<li>zlib: the ratio of the (log) of the GPT-2 perplexity and the zlib entropy (as computed by compressing the text).</li>
<li>Lowercase: the ratio of perplexities of the GPT-2 model on the original sample and on the lowercased sample.</li>
<li>Window: the minimum perplexity of the largest GPT-2 model across any sliding window of 50 tokens.</li>
</ul>
<p>For each of these $3 \times 6=18$ configurations, we select 100 samples from among the top-1000 samples according to the chosen metric. ${ }^{9}$ This gives us 1,800 total samples of potentially memorized content. In real-world attacks, adversaries will look to uncover large amounts of memorized content and thus may generate many more samples. We focus on a smaller set as a proof-of-concept attack.</p>
<p>Data De-Duplication. To avoid "double-counting" memorized content, we apply an automated fuzzy de-duplication step when we select the 100 samples for each configuration.</p>
<p>Given a sample $s$, we define the trigram-multiset of $s$, denoted $\operatorname{tri}(s)$ as a multiset of all word-level trigrams in $s$ (with words split on whitespace and punctuation characters). For example, the sentence "my name my name my name" has two trigrams ("my name my" and "name my name") each of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>multiplicity 2 . We mark a sample $s_{1}$ as a duplicate of another sample $s_{2}$, if their trigram multisets are similar, specifically if $\left|\operatorname{tri}\left(s_{1}\right) \cap \operatorname{tri}\left(s_{2}\right)\right| \geq\left|\operatorname{tri}\left(s_{1}\right)\right| / 2$.</p>
<p>Evaluating Memorization Using Manual Inspection. For each of the 1,800 selected samples, one of four authors manually determined whether the sample contains memorized text. Since the training data for GPT-2 was sourced from the public Web, our main tool is Internet searches. We mark a sample as memorized if we can identify a non-trivial substring that returns an exact match on a page found by a Google search.</p>
<p>Validating Results on the Original Training Data. Finally, given the samples that we believe to be memorized, we work with the original authors of GPT-2 to obtain limited query access to their training dataset. To do this we sent them all 1,800 sequences we selected for analysis. For efficiency, they then performed a fuzzy 3-gram match to account for memorization with different possible tokenizations. We marked samples as memorized if all 3-grams in the memorized sequence occurred in close proximity in the training dataset. This approach eliminates false negatives, but has false positives. It can confirm that our samples are memorized but cannot detect cases where we missed memorized samples. In some experiments below, we report exact counts for how often a particular sequence occurs in the training data. We obtained these counts by asking the GPT-2 authors to perform a separate grep over the entire dataset to get an exact count.</p>
<h3>6.2 Results</h3>
<p>In total across all strategies, we identify $\mathbf{6 0 4}$ unique memorized training examples from among the 1,800 possible candidates, for an aggregate true positive rate of $33.5 \%$ (our best variant has a true positive rate of $67 \%$ ). Below, we categorize what types of content is memorized by the model, and also study which attack methods are most effective.</p>
<p>Categories of Memorized Content. We manually grouped the memorized samples into different categories (a description of these categories is in Appendix A). The results are shown in Table 1. Most memorized content is fairly canonical text from news headlines, log files, entries from forums or wikis, or religious text. However, we also identify a significant amount of unique data, containing 128-bit UUIDs, (correctlyresolving) URLs containing random substrings, and contact information of individual people and corporations. In Section 6.3, we study these cases in more detail.</p>
<p>Efficacy of Different Attack Strategies. Table 2 shows the number of memorized samples broken down by the different text generation and membership inference strategies.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">109</td>
</tr>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">79</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notices</td>
<td style="text-align: right;">54</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items (games, countries, etc.)</td>
<td style="text-align: right;">54</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">53</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">50</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals (non-news samples only)</td>
<td style="text-align: right;">46</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content (products, subscriptions, etc.)</td>
<td style="text-align: right;">45</td>
</tr>
<tr>
<td style="text-align: left;">High entropy (UUIDs, base64 data)</td>
<td style="text-align: right;">35</td>
</tr>
<tr>
<td style="text-align: left;">Contact info (address, email, phone, twitter, etc.)</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">31</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">30</td>
</tr>
<tr>
<td style="text-align: left;">Religious texts</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td style="text-align: left;">Pseudonyms</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Donald Trump tweets and quotes</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Web forms (menu items, instructions, etc.)</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Tech news</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Lists of numbers (dates, sequences, etc.)</td>
<td style="text-align: right;">10</td>
</tr>
</tbody>
</table>
<p>Table 1: Manual categorization of the 604 memorized training examples that we extract from GPT-2, along with a description of each category. Some samples correspond to multiple categories (e.g., a URL may contain base-64 data). Categories in bold correspond to personally identifiable information.</p>
<p>Sampling conditioned on Internet text is the most effective way to identify memorized content, however, all generation schemes reveal a significant amount of memorized content. For example, the baseline strategy of generating with top- $n$ sampling yields 191 unique memorized samples, whereas conditioning on Internet text increases this to 273.</p>
<p>As discussed earlier, looking directly at the LM perplexity is a poor membership inference metric when classifying data generated with top- $n$ or temperature sampling: just $9 \%$ and $3 \%$ of inspected samples are memorized, respectively. The comparison-based metrics are significantly more effective at predicting if content was memorized. For example, $\mathbf{6 7 \%}$ of Internet samples marked by zlib are memorized.</p>
<p>Figure 3 compares the zlib entropy and the GPT-2 XL perplexity for each sample, with memorized examples highlighted. Plots for the other strategies are shown in Figure 4 in Appendix B. Observe that most samples fall along a diagonal, i.e., samples with higher likelihood under one model also have higher likelihood under another model. However, there are numerous outliers in the top left: these samples correspond to those that GPT-2 assigns a low perplexity (a high likelihood) but zlib is surprised by. These points, especially those which are extreme outliers, are more likely to be memorized than those close to the diagonal.</p>
<p>The different extraction methods differ in the type of memorized content they find. A complete breakdown of the data is given in Appendix A; however, to briefly summarize:
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The zlib entropy and the perplexity of GPT-2 XL for 200,000 samples generated with top- $n$ sampling. In red, we show the 100 samples that were selected for manual inspection. In blue, we show the 59 samples that were confirmed as memorized text. Additional plots for other text generation and detection strategies are in Figure 4.</p>
<ol>
<li>The zlib strategy often finds non-rare text (i.e., has a high $k$-memorization). It often finds news headlines, license files, or repeated strings from forums or wikis, and there is only one "high entropy" sequence this strategy finds.</li>
<li>Lower-casing finds content that is likely to have irregular capitalization, such as news headlines (where words are capitalized) or error logs (with many uppercase words).</li>
<li>The Small and Medium strategies often find rare content. There are 13 and 10 high entropy examples found by using the Small and Medium GPT-2 variants, respectively (compared to just one with zlib).</li>
</ol>
<h3>6.3 Examples of Memorized Content</h3>
<p>We next manually analyze categories of memorized content that we find particularly compelling. (Additional examples are presented in Appendix C.) Recall that since GPT-2 is trained on public data, our attacks are not particularly severe. Nevertheless, we find it useful to analyze what we are able to extract to understand the categories of memorized contentwith the understanding that attacking a model trained on a sensitive dataset would give stronger results.</p>
<p>Personally Identifiable Information. We identify numerous examples of individual peoples' names, phone numbers, addresses, and social media accounts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Inference <br> Strategy</th>
<th style="text-align: center;">Text Generation Strategy</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Top- $n$</td>
<td style="text-align: right;">Temperature</td>
<td style="text-align: right;">Internet</td>
</tr>
<tr>
<td style="text-align: left;">Perplexity</td>
<td style="text-align: center;">9</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">39</td>
</tr>
<tr>
<td style="text-align: left;">Small</td>
<td style="text-align: center;">41</td>
<td style="text-align: right;">42</td>
<td style="text-align: right;">58</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">38</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">45</td>
</tr>
<tr>
<td style="text-align: left;">zlib</td>
<td style="text-align: center;">59</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">67</td>
</tr>
<tr>
<td style="text-align: left;">Window</td>
<td style="text-align: center;">33</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">58</td>
</tr>
<tr>
<td style="text-align: left;">Lowercase</td>
<td style="text-align: center;">53</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">60</td>
</tr>
<tr>
<td style="text-align: left;">Total Unique</td>
<td style="text-align: center;">191</td>
<td style="text-align: right;">140</td>
<td style="text-align: right;">273</td>
</tr>
</tbody>
</table>
<p>Table 2: The number of memorized examples (out of 100 candidates) that we identify using each of the three text generation strategies and six membership inference techniques. Some samples are found by multiple strategies; we identify 604 unique memorized examples in total.</p>
<p>We find 46 examples that contain individual peoples' names. When counting occurrences of named individuals, we omit memorized samples that relate to national and international news (e.g., if GPT-2 emits the name of a famous politician, we do not count this as a named individual here). We further find 32 examples that contain some form of contact information (e.g., a phone number or social media handle). Of these, 16 contain contact information for businesses, and 16 contain private individuals' contact details.</p>
<p>Some of this memorized content is exclusive to just a few documents. For example, we extract the usernames of six users participating in an IRC conversation that appeared in exactly one training document.</p>
<p>URLs. We identify 50 examples of memorized URLs that correctly resolve to live webpages. Many of these URLs contain uncommon pieces of text, such as random numbers or base-64 encoded strings. We also identify several URLs that resolve correctly but we cannot identify their source (and we thus do not count them as "memorized" in our evaluation).</p>
<p>Code. We identify 31 generated samples that contain snippets of memorized source code. Despite our ability to recover the source code verbatim, we are almost always unable to recover the original authorship notices or terms of use. Often, this information is given either before the code itself or in a LICENSE file that appears separately. For many of these samples, we can also extend their length and recover thousands of lines of (near verbatim) source code (see Section 6.4).</p>
<p>Unnatural Text. Memorization is not limited to naturallooking text. We find 21 instances of random number sequences with at least 50 bits of entropy. ${ }^{10}$ For example, we</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Examples of $k=1$ eidetic memorized, highentropy content that we extract from the training data. Each is contained in just one document. In the best case, we extract a 87 -characters-long sequence that is contained in the training dataset just 10 times in total, all in the same document.
extract the following UUID:
1e4bd2a8-e8c8-4a62-adcd-40a936480059
from the model; a Google search for this string identifies just 3 documents containing this UUID, and it is contained in just one GPT-2 training document (i.e., it is 1-eidetic memorized). Other memorized random number sequences include UUIDs contained in only a few documents (not listed to preserve privacy), git commit hashes, random IDs used for ad tracking, and product model numbers.</p>
<p>Table 3 gives nine examples of $k=1$ eidetic memorized content, each of which is a random sequences between 10 and 87 characters long. In each of these cases, the memorized example is contained in exactly one training document, and the total number of occurrences within that single document varies between just 10 and 311.</p>
<p>Data From Two Sources. We find samples that contain two or more snippets of memorized text that are unrelated to one another. In one example, GPT-2 generates a news article about the (real) murder of a woman in 2013, but then attributes the murder to one of the victims of a nightclub shooting in Orlando in 2016. Another sample starts with the memorized Instagram biography of a pornography producer, but then goes on to incorrectly describe an American fashion model as a pornography actress. This type of generation is not $k$-eidetic memorization (these independent pieces of information never appear in the same training documents), but it is an example of a contextual integrity violation.</p>
<p>Removed Content. Finally, GPT-2 memorizes content that has since been removed from the Internet, and is thus now primarily accessible through GPT-2. We are aware of this content as it is still cached by Google search, but is no longer</p>
<p>present on the linked webpage. Some of this data is not particularly interesting in its own right, e.g., error logs due to a misconfigured webserver that has since been fixed. However, the fact that this type of memorization occurs highlights that LMs that are trained entirely on (at-the-time) public data may end up serving as an unintentional archive for removed data.</p>
<h3>6.4 Extracting Longer Verbatim Sequences</h3>
<p>In our previous experiments, we extract strings of 256 tokens in length. Here, we briefly investigate if we can extract longer sequences. In particular, we extend the length of some of the memorized sequences by seeding the model with each sample and continuing to generate. To do this, we apply a beam-search-like decoding method introduced in prior work [8] instead of greedy decoding which often fails to generate long verbatim sequences.</p>
<p>We can extend many of the memorized samples. For example, we identify a piece of source code taken from a repository on GitHub. We can extend this snippet to extract an entire file, namely 1450 lines of verbatim source code. We can also extract the entirety of the MIT, Creative Commons, and Project Gutenberg licenses. This indicates that while we have extracted 604 memorized examples, we could likely extend many of these to much longer snippets of memorized content.</p>
<h3>6.5 Memorization is Context-Dependent</h3>
<p>Consistent with recent work on constructing effective "prompts" for generative LMs [7, 62], we find that the memorized content is highly dependent on the model's context.</p>
<p>For example, GPT-2 will complete the prompt " 3.14159 " with the first 25 digits of $\pi$ correctly using greedy sampling. However, we find that GPT-2 "knows" (under Definition 2) more digits of $\pi$ because using the beam-search-like strategy introduced above extracts 500 digits correctly.</p>
<p>Interestingly, by providing the more descriptive prompt "pi is 3.14159 ", straight greedy decoding gives the first 799 digits of $\pi$-more than with the sophisticated beam search. Further providing the context "e begins 2.7182818 , pi begins 3.14159 ", GPT-2 greedily completes the first 824 digits of $\pi$.</p>
<p>This example demonstrates the importance of the context: in the right setting, orders of magnitude more extraction is feasible than when the context is just slightly suboptimal. We find that this holds true for our memorized examples as well. None of the 273 extracted samples found using Internet conditioning can be reliably reproduced when using the same prefix initially provided to GPT-2 that produced this sample. However, nearly all can be reproduced with high probability if we provided the entire sequence of data up to (but not including) the beginning of the memorized content.</p>
<p>The important lesson here is that our work vastly underestimates the true amount of content that GPT-2 memorized.</p>
<p>There are likely prompts that would identify much more memorized content, but because we stick to simple prompts we do not find this memorized content.</p>
<h2>7 Correlating Memorization with Model Size \&amp; Insertion Frequency</h2>
<p>Thus far, we have shown that language models can memorize verbatim training strings, even when they are trained for few epochs and achieve small train-test accuracy gaps. A natural question is how many times a string must appear for it to be memorized (i.e., $k$ in Definition 2). Prior work has investigated LM memorization by varying the number of times particular "canary" tokens were inserted into a training dataset [8]. The main limitation of this approach is that it is synthetic: canaries are inserted artificially after the dataset has been collected and may not be representative of natural data.</p>
<p>Here, we study how well GPT-2 memorizes naturally occurring canaries in the training data. In particular, we consider a piece of memorized content with the following prefix:</p>
<div class="codehilite"><pre><span></span><code>{&quot;color&quot;:&quot;fuchsia&quot;,&quot;link&quot;:&quot;https://www.
reddit.com/r/The_Donald/comments/
</code></pre></div>

<p>The reddit.com URL above is completed by a specific 6 -character article ID and a title. We located URLs in this specific format in a single document on pastebin.com. Each URL appears a varying number of times in this document, and hence in the GPT-2 training dataset. ${ }^{11}$ Table 4 shows a subset of the URLs that appear more than once, and their respective counts in the document. ${ }^{12}$ This allows us to ask the question: how many times must an example appear in the training dataset for us to extract it?</p>
<p>Methods. We attempt two approaches to extract URLs of this format, and run three variants of GPT-2 (XL, Medium, and Small). The two approaches vary the "difficulty" of the attack, so even if the more difficult fails the easier may succeed.</p>
<p>First, we directly prompt each variant of GPT-2 with the prefix above, and use top- $n$ sampling to generate 10,000 possible extensions. Then, we test whether any of the URLs in the training document were among those that were emitted by GPT-2. We count a URL as emitted if it matches verbatim with one of the 10,000 generations.</p>
<p>Some URLs are not extractable with this technique, and so we make the problem easier for GPT-2 by additionally providing GPT-2 the 6-character random token that begins each URL. Given this additional prefix, we then sample from</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Occurrences</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Memorized?</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">URL (trimmed)</td>
<td style="text-align: center;">Docs</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">XL</td>
<td style="text-align: center;">M</td>
</tr>
<tr>
<td style="text-align: center;">////51y/milo_evacua...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">////zin/hi_my_name...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">////7ne/for_all_yo...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">////5mj/fake_news.....</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">////5wn/reddit_admi...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">////lp8/26_evening...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">////lia/so_pizzagat...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">////sbf/late_night...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">////eta/make_christ...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">////dev/its_officia...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">////3c7/scott_adams...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">////k2o/because_his...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">////hu3/armynavy_ga...</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: We show snippets of Reddit URLs that appear a varying number of times in a single training document. We condition GPT-2 XL, Medium, or Small on a prompt that contains the beginning of a Reddit URL and report a $\checkmark$ if the corresponding URL was generated verbatim in the first 10,000 generations. We report a $1 / 2$ if the URL is generated by providing GPT-2 with the first 6 characters of the URL and then running beam search.
the model using the beam search procedure. This task is easier in two ways: we have first provided more context and additionally use a higher recall sampling strategy.</p>
<p>Results. Table 4 summarizes the key results. Under the more difficult of the two approaches, the full-sized 1.5 billion parameter GPT-2 model emits all examples that are inserted 33 times or more, the medium-sized 345 million parameter memorizes half of the URLs, and the smallest 117 million parameter model memorizes none of these URLs.</p>
<p>When given the additional context and using beam search, the medium model can emit four more URLs, and the small model only emits the one URL that was inserted 359 times.</p>
<p>These results illustrate two fundamental lessons in LM memorization. First, larger models memorize significantly more training data: even hundreds of millions of parameters are not enough to memorize some of the training points. The ability of LMs to improve with model size has been extensively studied $[35,38]$; we show a negative trend where these improvements come at the cost of decreased privacy. Second, for the largest LM, complete memorization occurs after just 33 insertions. This implies that any potentially sensitive information that is repeated a non-trivial amount of times is at risk for memorization, even if it was only repeated multiple times in a single training document.</p>
<h2>8 Mitigating Privacy Leakage in LMs</h2>
<p>Now that we have shown that memorized training data can be extracted from LMs, a natural question is how to mitigate these threats. Here we describe several possible strategies.</p>
<p>Training With Differential Privacy. Differential privacy (DP) $[13,14]$ is a well-established notion of privacy that offers strong guarantees on the privacy of individual records in the training dataset. Private machine learning models can be trained with variants of the differentially private stochastic gradient descent (DP-SGD) algorithm [1] which is widely implemented [17, 25]. Large companies have even used DP in production machine learning models to protect users' sensitive information $[15,69]$. The tradeoffs between privacy and utility of models have been studied extensively: differentially-private training typically prevents models from capturing the long tails of the data distribution and thus hurts utility [19, 20, 67].</p>
<p>In the content of language modeling, recent work demonstrates the privacy benefits of user-level DP models [56]. Unfortunately, this work requires labels for which users contributed each document; such labels are unavailable for data scraped from the open Web. It may instead seem natural to aim for DP guarantees at the granularity of individual webpages, but rare snippets of text (e.g., an individual's name and contact information as in Figure 1) might appear in more than one webpage. It is thus unclear how to apply DP in a principled and effective way on Web data.</p>
<p>Curating the Training Data. One cannot manually vet the extremely large training datasets used for training LMs. However, there are methods to limit the amount of sensitive content that is present, e.g., by identifying and filtering personal information or content with restrictive terms of use [11, 58].</p>
<p>Aside from attempting to remove sensitive content, it is also important to carefully de-duplicate the data. Many language modeling datasets are de-duplicated at the documentor paragraph-level, which means that a single document can still contain many repeated occurrences of a sensitive piece of content. We envision more sophisticated strategies to deduplicate the training data, or limit the contribution of any single source of training data.</p>
<p>It is also vital to carefully source the training data. Many of the potentially-sensitive training examples that we extracted (e.g., individuals' personal information) came from websites that are known to host sensitive content, e.g., pastebin is the 12th most popular domain in GPT-2's training set.</p>
<p>Overall, sanitizing data is imperfect-some private data will always slip through-and thus it serves as a first line of defense and not an outright prevention against privacy leaks.</p>
<p>Limiting Impact of Memorization on Downstream Applications. In many downstream applications, e.g., dialogue</p>
<p>systems [76] and summarization models [29], LMs are finetuned on task-specific data. On the positive side, this finetuning process may cause the LM to "forget" [42, 57] some of the data that is memorized during the pre-training stage. On the negative side, fine-tuning may introduce its own privacy leakages if the task-specific data also contains private information. An interesting direction for future work is to explore how memorization is inherited by fine-tuned models.</p>
<p>Downstream applications built on top of language models could also attempt to filter out generated text that contains memorized content, if such content can be reliably detected (e.g., using various membership inference strategies).</p>
<p>Auditing ML Models for Memorization. Finally, after mitigating privacy leaks, it is vital to audit models to empirically determine the privacy level they offer in practice [33]. Auditing is important even when using differential privacy, as it can complement theoretical upper bounds on privacy leakage [1]. We envision using our proposed methods, as well as existing attacks $[8,33,65,72]$, to audit LMs.</p>
<h2>9 Lessons and Future Work</h2>
<p>Extraction Attacks Are a Practical Threat. Prior work shows that ( $100 \times$ to $1000 \times$ smaller) language models potentially memorize training data in semi-realistic settings [8, 73]. Our results show that state-of-the-art LMs do memorize their training data in practice, and that adversaries can extract this data with simple techniques. Our attacks are practical even when the data contains a given sequence only a few times.</p>
<p>As our attacks interact with a language model as a blackbox, our results approximate the worst-case behavior of language models when interacting with benign users. In particular, among 600,000 (honestly) generated samples, our attacks find that at least 604 (or $0.1 \%$ ) contain memorized text.</p>
<p>Note that this is likely an extremely loose lower bound. We only manually inspected 1,800 potential candidate memorized samples; if we had started with more candidates we would likely have identified significantly more memorized content. Developing improved techniques for extracting memorized data, including attacks that are targeted towards specific content, is an interesting area for future work.</p>
<p>Memorization Does Not Require Overfitting. It is often believed that preventing overfitting (i.e., reducing the traintest generalization gap) will prevent models from memorizing training data. However, large LMs have no significant traintest gap, and yet we still extract numerous examples verbatim from the training set. The key reason is that even though on average the training loss is only slightly lower than the validation loss, there are still some training examples that have anomalously low losses. Understanding why this happens is an important problem for future work [6, 40].</p>
<p>Larger Models Memorize More Data. Throughout our experiments, larger language models consistently memorized more training data than smaller LMs. For example, in one setting the 1.5 billion parameter GPT-2 model memorizes over $18 \times$ as much content as the 124 million parameter model (Section 7). Worryingly, it is likely that as LMs become bigger (in fact they already are $100 \times$ larger than the GPT-2 model we study [7]), privacy leakage will become even more prevalent.</p>
<p>Memorization Can Be Hard to Discover. Much of the training data that we extract is only discovered when prompting the LM with a particular prefix. Currently, we simply attempt to use high-quality prefixes and hope that they might elicit memorization. Better prefix selection strategies [62] might identify more memorized data.</p>
<p>Adopt and Develop Mitigation Strategies. We discuss several directions for mitigating memorization in LMs, including training with differential privacy, vetting the training data for sensitive content, limiting the impact on downstream applications, and auditing LMs to test for memorization. All of these are interesting and promising avenues of future work, but each has weaknesses and are incomplete solutions to the full problem. Memorization in modern LMs must be addressed as new generations of LMs are emerging and becoming building blocks for a range of real-world applications.</p>
<h2>10 Conclusion</h2>
<p>For large language models to be widely adopted, they must address the training data memorization problems that we have identified. Our extraction attacks are practical and efficient, and can recover hundreds of training examples from a model, even when they are contained in just one training document.</p>
<p>Our analysis is best viewed as a cautionary tale of what could happen when training large LMs on sensitive data. Even though our attacks target GPT-2 (which allows us to ensure that our work is not harmful), the same techniques apply to any LM. Moreover, because memorization gets worse as LMs become larger, we expect that these vulnerabilities will become significantly more important in the future.</p>
<p>There will therefore need to be techniques developed to specifically address our attacks. Training with differentiallyprivate techniques is one method for mitigating privacy leakage, however, we believe that it will be necessary to develop new methods that can train models at this extreme scale (e.g., billions of parameters) without sacrificing model accuracy or training time. More generally, there are many open questions that we hope will be investigated further, including why models memorize, the dangers of memorization, and how to prevent memorization.</p>
<h2>Acknowledgements</h2>
<p>We are grateful for comments on early versions of this paper by Dan Boneh, Andreas Terzis, Carey Radebaugh, Daphne Ippolito, Christine Robson, Kelly Cooke, Janel Thamkul, Austin Tarango, Jack Clark, Ilya Mironov, and Om Thakkar. Florian Tramr is supported by NSF award CNS-1804222.</p>
<h2>Summary of Contributions</h2>
<ul>
<li>Nicholas, Dawn, Ariel, Tom, Colin and lfar proposed the research question of extracting training data from GPT-2 and framed the threat model.</li>
<li>Colin, Florian, Matthew, and Nicholas stated the memorization definitions.</li>
<li>Florian, Ariel, and Nicholas wrote code to generate candidate memorized samples from GPT-2 and verify the ground truth memorization.</li>
<li>Florian, Nicholas, Matthew, and Eric manually reviewed and categorized the candidate memorized content.</li>
<li>Katherine, Florian, Eric, and Colin generated the figures.</li>
<li>Adam, Matthew, and Eric ran preliminary investigations in language model memorization.</li>
<li>Nicholas, Florian, Eric, Colin, Katherine, Matthew, Ariel, Alina, lfar, Dawn, and Adam wrote and edited the paper.</li>
<li>Tom, Adam, and Colin gave advice on language models and machine learning background.</li>
<li>Alina, lfar, and Dawn gave advice on the security goals.</li>
</ul>
<h2>References</h2>
<p>[1] Martn Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM CCS, 2016.
[2] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.
[3] Jay Alammar. The illustrated transformer. Visualizing Machine Learning One Concept at a Time, 2018.
[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In $I C L R, 2015$.
[5] Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. $J M L R, 2003$.
[6] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? arXiv preprint arXiv:2012.06421, 2020.
[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
[8] Nicholas Carlini, Chang Liu, lfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium, 2019.
[9] Kamalika Chaudhuri and Claire Monteleoni. Privacypreserving logistic regression. In NIPS, 2009.
[10] Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M Dai, Zhifeng Chen, Timothy Sohn, and Yonghui Wu. Gmail smart compose: Real-Time assisted writing. In $K D D, 2019$.
[11] Andrea Continella, Yanick Fratantonio, Martina Lindorfer, Alessandro Puccetti, Ali Zand, Christopher Kruegel, and Giovanni Vigna. Obfuscation-Resilient Privacy Leak Detection for Mobile Apps Through Differential Analysis. In NDSS, 2017.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.
[13] C Dwork, F McSherry, K Nissim, and A Smith. Calibrating noise to sensitivity in private data analysis. In TCC, 2006.
[14] Cynthia Dwork. Differential privacy: A survey of results. In TAMC, 2008.
[15] lfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. RAPPOR: Randomized aggregatable privacy-preserving ordinal response. In ACM CCS, 2014.
[16] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. Nature, 2017.
[17] Facebook. Opacus. https://github.com/pytorch/ opacus.
[18] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In ACL, 2018.</p>
<p>[19] Vitaly Feldman. Does learning require memorization? A short tale about a long tail. In STOC, 2020.
[20] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In NeurIPS, 2020.
[21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In ACM CCS, 2015.
[22] Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23-38, 1994.
[23] Aaron Gokaslan and Vanya Cohen. OpenWebText corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.
[24] Shafi Goldwasser, Silvio Micali, and Charles Rackoff. The knowledge complexity of interactive proof systems. SICOMP, 1989.
[25] Google. Tensorflow Privacy. https://github.com/ tensorflow/privacy.
[26] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[27] Peter Henderson, Koustuv Sinha, Nicolas AngelardGontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. Ethical challenges in datadriven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123-129, 2018.
[28] Sorami Hisamoto, Matt Post, and Kevin Duh. Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system? In TACL, 2020.
[29] Andrew Hoang, Antoine Bosselut, Asli Celikyilmaz, and Yejin Choi. Efficient adaptation of pretrained transformers for abstractive summarization. arXiv preprint arXiv:1906.00138, 2019.
[30] Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In $I C L R, 2020$.
[31] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In ACL, 2018.
[32] Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In $A C L$.
[33] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? In NeurIPS, 2020.
[34] Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice. In USENIX Security Symposium, 2019.
[35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[36] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.
[37] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In NAACL, 2016.
[38] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference of transformers. In ICML, 2020.
[39] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[40] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. Understanding membership inferences on well-generalized learning models. arXiv preprint arXiv:1802.04889, 2018.
[41] Jean loup Gailly and Mark Adler. zlib compression library.
[42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation. 1989.
[43] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In $I C L R, 2018$.
[44] Tomas Mikolov, Martin Karafit, Lukas Burget, Jan Cernock, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, 2010.
[45] Randall Munroe. Predictive models. https://xkcd. com/2169/, 2019.</p>
<p>[46] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning with membership privacy using adversarial regularization. In ACM SIGSAC, 2018.
[47] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In IEEE S\&amp;P, 2019.
[48] Helen Nissenbaum. Privacy as contextual integrity. Washington Law Review, 2004.
[49] OpenAI. Language models are few-shot learners. https://github.com/openai/gpt-3, 2020.
[50] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.
[51] Fabio Petroni, Tim Rocktschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In EMNLP, 2019.
[52] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.
[53] Alec Radford, Jeffrey Wu, Dario Amodei, Daniela Amodei, Jack Clark, Miles Brundage, and Ilya Sutskever. Better language models and their implications. OpenAI Blog, 2019.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.
[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. In $J M L R$, 2020.
[56] Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and Franoise Beaufays. Training production language models without memorizing user data. arXiv preprint arXiv:2009.10031, 2020.
[57] Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Psychological review, 1990.
[58] Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud Legout, and David Choffnes. ReCon: Revealing and controlling PII leaks in mobile network traffic. In MobiSys, 2016.
[59] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In EMNLP, 2020.
[60] Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang, and Nina Taft. Learning in a large function space: Privacy-preserving mechanisms for SVM learning. Privacy and Confidentiality, 2012.
[61] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL, 2016.
[62] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, 2020.
[63] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.
[64] Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In ACM CCS, 2015.
[65] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In IEEE S\&amp;P, 2017.
[66] Congzheng Song and Ananth Raghunathan. Information leakage in embedding models. In ACM CCS, 2020.
[67] Congzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In $K D D, 2018$.
[68] Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Franoise Beaufays. Understanding unintended memorization in federated learning. arXiv preprint arXiv:2006.07490, 2020.
[69] Abhradeep Guha Thakurta, Andrew H. Vyrros, Umesh S. Vaishampayan, Gaurav Kapoor, Julien Freudiger, Vivek Rangarajan Sridhar, and Doug Davidson. Learning new words, 2017. US Patent 9,594,741.
[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
[71] Kit Walsh. USPTO request for comments on intellectual property protection for artificial intelligence innovation - public comment by the electronic frontier foundation. https://www.uspto.gov/sites/default/ files/documents/Electronic\ Frontier\ Foundation_RFC-84-FR-58141.PDF, 2020.</p>
<p>[72] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In IEEE CSF, 2018.
[73] Santiago Zanella-Bguelin, Lukas Wutschitz, Shruti Tople, Victor Rhle, Andrew Paverd, Olga Ohrimenko, Boris Kpf, and Marc Brockschmidt. Analyzing information leakage of updates to natural language models. In ACM CCS, 2020.
[74] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In NeurIPS, 2019.
[75] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. ICLR, 2017.
[76] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. DialoGPT: Large-scale generative pretraining for conversational response generation. In ACL Demo Track, 2020.</p>
<h2>A Categorization of Memorized Data</h2>
<p>Table 5 describes the high-level categories that we assigned to the 604 memorized samples extracted from GPT-2. Note that a single sample can belong to multiple categories. Tables 6 and 7 (omitted for space) show the categorization broken down by attack strategy.</p>
<h2>B Distribution of Model Perplexities</h2>
<p>Figure 4 shows the distribution of the perplexities of samples generated with each of our three text generation strategies and ordered based on our six membership inference strategies.</p>
<h2>C Additional Case Studies of Memorization</h2>
<p>Here we present additional results from our manual analysis of the memorized content.</p>
<h2>Memorized Leaked Podesta Emails from WikiLeaks.</h2>
<p>We identify several memorized URLs that originated from the leaked Podesta Emails available on WikiLeaks ${ }^{13}$. There is only one training document that contains these memorized URLs. Due to the nature of email, the text of one message is often included in subsequent replies to this email. As a result, a URL that is used (intentionally) only once can be included in the dataset tens of times due to the replies.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Memorized Donald Trump Quotes and Tweets. The GPT-2 training dataset was collected when the 2016 US Presidential election was often in the news. As a result, we find several instances of memorized quotes from Donald Trump, both in the form of official remarks made as President (found in the official government records), as well as statements made on Twitter.</p>
<p>Memorized Promotional Content. We extract memorized samples of promotional content, such as advertisements for books, beauty products, software products. One of these samples includes a link to an author's valid Patreon account, along with a list of named and pseudonymous prior donors.</p>
<p>Memorized Number Sequences. We identify many examples where GPT-2 emits common number sequences. Nearly ten examples contain the integers counting up from some specific value. We also find examples of GPT-2 counting the squares $1,2,4,8,16$, 25, 36, Fibonacci numbers $1,1,2,3,5,8,13,21$, $34,55,89,144,233,377,610,987$, or digits of $\pi$, 3.14159265358979323846264. None of these examples should be unexpected, but the quantity of memorized number sequences was surprising to us.</p>
<p>Memorized News Headlines. Numerous memorized text snippets are verbatim copies of news articles and headlines. A large number of these memorized samples are attributed to a single source: thehill.com, an American news website. Interestingly, most of these samples follow the exact same template: (1) they contain a list of different news headlines separated by a "pipe" symbol (!), (2) the sample begins with two merged words, e.g., "TrumpJesuit", (3) the headline list ends with the all-caps word "MORE", and (4) the sample contains the all-caps word "ADVERTISEMENT".</p>
<p>We indeed find pages on the Web that contain copies of headlines from thehill.com under this exact template. The peculiarities of these snippets likely contributed to their memorization. For example, the token TrumpJesuit does not appear in any other context on the entire Web.</p>
<p>Memorized Base-64 Content. One particularly interesting form of memorization that we identify is the ability of GPT-2 to emit base-64 encoded content. For example, we extract out of the model the following sequence:</p>
<div class="codehilite"><pre><span></span><code>bWFzdGVyfGltYWdlc3w3OTkxOXxpbWFnZS9wbmd
8aWlhZ2VzL2hkZS9oMDQvODg0NTY3MjYxMTg3MC
5wbmd8ZmFkMTMlNmFiYWJhZjFiMjJlYTAyNzU0Z
</code></pre></div>

<p>which decodes to the sequence "masterlimages1799191image /pnglimages/hde/h04/8845672611870.png1...". Despite our attempts, we are unable to identify where this content originates.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: For each of our three text generation strategies (Top- $n$, Internet and Temperature), we generate 200,000 samples using GPT-2 and apply a de-duplication procedure. The two left-most plots show the distribution of perplexities for the full sample, and the most likely window of 50 tokens. The remaining plots compare the distribution of perplexities of GPT-2 to other measure of sample likelihood: zlib entropy, perplexity under GPT-2 Small and GPT-2 Medium, and perplexity of lower-cased samples. Each plot highlights the 100 samples we selected for manual inspection (red) and the subset that was confirmed as memorized (blue).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">US and international <br> news</td>
<td style="text-align: right;">109</td>
<td style="text-align: left;">General news articles or headlines, mostly <br> about US politics</td>
</tr>
<tr>
<td style="text-align: left;">Log files and error <br> reports</td>
<td style="text-align: right;">79</td>
<td style="text-align: left;">Logs produced by software or hardware</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of <br> use, copyright <br> notices</td>
<td style="text-align: right;">54</td>
<td style="text-align: left;">Software licenses or website terms of use, <br> copyright for code, books, etc.</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">54</td>
<td style="text-align: left;">Ordered lists, typically alphabetically, of <br> games, books, countries, etc.</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">53</td>
<td style="text-align: left;">User posts on online forums or entries in <br> specific wikis</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">50</td>
<td style="text-align: left;">A URL that resolves to a live page</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">46</td>
<td style="text-align: left;">Samples that contain names of real individu- <br> als. We limit this category to non-news sam- <br> ples. E.g., we do not count names of politi- <br> cians or journalists within news articles</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">45</td>
<td style="text-align: left;">Descriptions of products, subscriptions, <br> newsletters, etc.</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">35</td>
<td style="text-align: left;">Random content with high entropy, e.g., <br> UUIDs Base64 data, etc.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">32</td>
<td style="text-align: left;">Physical addresses, email addresses, phone <br> numbers, twitter handles, etc.</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">31</td>
<td style="text-align: left;">Snippets of source code, including <br> JavaScript</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">30</td>
<td style="text-align: left;">Structured configuration data, mainly for <br> software products</td>
</tr>
<tr>
<td style="text-align: left;">Religious texts</td>
<td style="text-align: right;">25</td>
<td style="text-align: left;">Extracts from the Bible, the Quran, etc.</td>
</tr>
<tr>
<td style="text-align: left;">Pseudonyms</td>
<td style="text-align: right;">15</td>
<td style="text-align: left;">Valid usernames that do not appear to be tied <br> to a physical name</td>
</tr>
<tr>
<td style="text-align: left;">Donald Trump <br> tweets and quotes</td>
<td style="text-align: right;">12</td>
<td style="text-align: left;">Quotes and tweets from Donald Trump, of- <br> ten from news articles</td>
</tr>
<tr>
<td style="text-align: left;">Web forms</td>
<td style="text-align: right;">11</td>
<td style="text-align: left;">Lists of user menu items, Website instruc- <br> tions, navigation prompts (e.g., "please enter <br> your email to continue")</td>
</tr>
<tr>
<td style="text-align: left;">Tech news</td>
<td style="text-align: right;">11</td>
<td style="text-align: left;">News related to technology</td>
</tr>
<tr>
<td style="text-align: left;">Lists of numbers</td>
<td style="text-align: right;">10</td>
<td style="text-align: left;">Lists of dates, number sequences, $\pi$, etc.</td>
</tr>
<tr>
<td style="text-align: left;">Sports news</td>
<td style="text-align: right;">9</td>
<td style="text-align: left;">News related to sports</td>
</tr>
<tr>
<td style="text-align: left;">Movie synopsis, cast</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;">List of actors, writers, producers. Plot syn- <br> opsis.</td>
</tr>
<tr>
<td style="text-align: left;">Pornography</td>
<td style="text-align: right;">5</td>
<td style="text-align: left;">Content of pornographic nature, often lists <br> of adult film actors.</td>
</tr>
</tbody>
</table>
<p>Table 5: Descriptions for the categories of memorized text. Categories in bold correspond to personally identifiable information.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">88</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">34</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">25</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: left;">Donald Trump tweets and quotes</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Pseudonyms</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Sports news</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Movie synopsis or cast</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>(a) Top-n (191 samples)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">86</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">53</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">40</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">36</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">33</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">29</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Pseudonyms</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Tech news</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Pornography</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">Web forms</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">Lists of numbers</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<p>(b) Internet (273 samples)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">31</td>
</tr>
<tr>
<td style="text-align: left;">Religious texts</td>
<td style="text-align: right;">28</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">24</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">17</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Tech news</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Lists of numbers</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>(c) Temperature (140 samples)
Table 6: Memorized content found in samples produced by each of the our three text generation strategies. We show categories with at least 5 samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Lists of numbers</td>
<td style="text-align: right;">5</td>
</tr>
</tbody>
</table>
<p>(a) Perplexity (51 samples)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">21</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>(b) Window (119 samples)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">17</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Religious texts</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">13</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">13</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">other</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Donald Trump tweets and quotes</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>(c) zlib (172 samples)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Log files and error reports</td>
<td style="text-align: right;">17</td>
</tr>
<tr>
<td style="text-align: left;">Forum or Wiki entry</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Religious texts</td>
<td style="text-align: right;">14</td>
</tr>
<tr>
<td style="text-align: left;">Valid URLs</td>
<td style="text-align: right;">13</td>
</tr>
<tr>
<td style="text-align: left;">High entropy</td>
<td style="text-align: right;">13</td>
</tr>
<tr>
<td style="text-align: left;">Lists of named items</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">License, terms of use, copyright notice</td>
<td style="text-align: right;">12</td>
</tr>
<tr>
<td style="text-align: left;">Promotional content</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Configuration files</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Named individuals</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">other</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">US and international news</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">Contact info</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">Donald Trump tweets and quotes</td>
<td style="text-align: right;">7</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">6</td>
</tr>
</tbody>
</table>
<p>(e) Small (141 samples)</p>
<p>Table 7: Memorized content found using our six membership inference strategies. We show categories with at least 5 samples.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://en.wikipedia.org/wiki/Podesta_emails&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ Since the training data is sourced from the public Web, all the outputs of our extraction attacks can also be found via Internet searches. Indeed, to evaluate whether we have found memorized content, we search for the content on the Internet and are able to find these examples relatively easily.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268536992</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.13244v4.pdf" target="_blank">Instruction multi-constraint molecular generation using a teacher-student large language model</a></p>
                <p><strong>Paper Abstract:</strong> Background While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Results We introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the “teachers.” To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these “teachers,” enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules that meet complex property requirements described in natural language across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 82.58%, 68.03%, and 67.48%, respectively. The model also exhibits adaptability through zero-shot testing, creating molecules that satisfy combinations of properties that have not been encountered. It can comprehend text inputs with various language styles, extending beyond the confines of outlined prompts. Conclusions TSMMG presents an effective model for multi-constraint molecular generation using natural language. This framework is not only applicable to drug discovery but also serves as a reference for other related fields. Supplementary information The online version contains supplementary material available at 10.1186/s12915-025-02200-3.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSMMG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teacher-Student-based Multi-constraint Molecular Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based decoder LLM (initialized from GPT2-small) fine-tuned on text–molecule pairs distilled from multiple 'teacher' tools to generate SMILES directly from natural-language property prompts, supporting multi-constraint and zero-shot multi-property generation for drug-discovery style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TSMMG</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (transformer decoder) fine-tuned; teacher-student knowledge-distillation framework</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~117M parameters (GPT2-small initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large natural-language corpus via GPT-2 weights, then fine-tuned on ≈2 million PubChem molecules for which multiple properties/structural annotations were extracted by external 'teacher' tools and rendered into natural-language templates (text–SMILES paired data with One-to-Many and Many-to-One sampling schemes).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct conditional generation of SMILES from natural-language prompts (fine-tuned autoregressive decoding); knowledge distillation from multiple property predictors and parsers; sampling temperature controls randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (output); natural-language textual prompts (input); IUPAC strings used internally for functional-group extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo molecular generation for drug discovery: generating molecules satisfying physicochemical properties (LogP, QED, SA), target affinities (e.g., DRD2, GSK3, EP2, EP4, BTK, FGFR4, KPCD3, 3CL), and ADMET properties (BBB, HIA).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Functional group presence (FG/IUPAC), LogP (with ±1 margin), QED (>0.6), Synthetic Accessibility (SAs < 4), predicted affinity thresholds (>0.5), BBB (>0.5), HIA (>0.5); multi-constraint combinations up to 5 constraints in zero-shot testing.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Training-data construction and evaluation used RDKit (physicochemical properties, validity), admetSAR (BBB/HIA ADMET predictions), SVM affinity predictors (Olivecrona et al., Jin et al.), an IUPAC parser / SMILES2IUPAC model, and downstream computational docking with UCSF Chimera (preparation), DOCK6 (docking), PLIP and PyMOL (visualization).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>≈2 million PubChem molecules were used to construct text–SMILES pairs; teacher predictors provided labels; evaluation used held-out datasets and generated samples (5,000 generated molecules per multi-constraint task).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (SMILES parse via RDKit), Uniqueness, Novelty (not in training set), Diversity (1 - average Tanimoto similarity on Morgan fingerprints), Success Rate (SR): fraction satisfying all task constraints; SR(nFG) computed ignoring FG matching for FG-constrained tasks; task-specific cutoffs (e.g., QED>0.6, SAs<4, affinity>0.5, BBB/HIA>0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Validity: ≈99.87% across 2-, 3-, 4-constraint tasks; Uniqueness (avg): 90.27% (2-constraint), 81.2% (3-constraint), 81.89% (4-constraint); Novelty (avg): 92.79%, 87.6%, 87.87%; Diversity (scaled): 90.47, 89.3, 89.37; Success Rates (SR): 82.58% (2-constraint), 68.03% (3-constraint), 67.48% (4-constraint). Zero-shot 5-constraint case produced molecules which were docked to EP2 (PDB 7CX2) and EP4 (PDB 5YWY) showing plausible interactions (qualitative docking/visualization results). Temperature sweep: increasing sampling temperature increased uniqueness but decreased SR (e.g., Unique rose ~17% from T=0.5 to 1.5; SR dropped ~21%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Relies on teacher predictors whose errors propagate; functional-group (FG) frequency imbalance in training data reduced uniqueness and SR for some FG+activity tasks (e.g., DRD2/GSK3); exponential combination space of properties makes exhaustive training impractical; no wet-lab synthesis/biological testing reported (only computational docking); sensitivity to prompt wording (template shifts reduced SR and novelty for some tasks); computational cost for fine-tuning (8×A100, ~6 days).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6865.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES2IUPAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES-to-IUPAC GPT2-based translator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT2-small-based conditional generation model trained to translate SMILES strings into IUPAC names for high-throughput functional-group extraction during dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SMILES2IUPAC</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM fine-tuned for sequence-to-sequence translation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>~117M parameters (GPT2-small)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained on 2 million SMILES–IUPAC pairs collected from PubChem.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional autoregressive generation: input SMILES → generate IUPAC text.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Input: SMILES; Output: IUPAC name (text).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>High-throughput IUPAC name generation to extract structural features/functional groups for constructing natural-language training templates.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Accuracy of IUPAC generation sufficient to recover functional groups reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrated into the data-generation pipeline to produce IUPAC names used by the IUPAC parser and template generator.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>2M SMILES–IUPAC pairs from PubChem.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Functional-group extraction accuracy comparing predicted IUPAC-derived FGs to ground truth IUPAC-derived FGs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On 1000 unseen molecules, predicted IUPAC names yielded correct functional-group recovery at 94% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Prediction errors (≈6%) can lead to incorrect FG labels in training data; prior tools for SMILES→IUPAC had low throughput, motivating this custom model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6865.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 small (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard GPT-2 small language model (12-layer transformer) used as initialization for both TSMMG and SMILES2IUPAC; provides natural-language pretraining backbone for mapping text to molecular sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (pretrained language model), used as fine-tuning base</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>117M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on broad natural-language corpora (OpenAI GPT-2 pretraining datasets), then weights downloaded from HuggingFace and fine-tuned on constructed text–SMILES pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive decoding; fine-tuned to map natural-language prompts to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Natural language tokens (input) and SMILES tokens (output) after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Base architecture enabling natural-language-conditioned molecule generation (TSMMG) and SMILES→IUPAC translation (SMILES2IUPAC).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Weights initialized from HuggingFace; fine-tuning used AdamW optimizer on GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Public language corpora for pretraining (not detailed in paper) and the authors' constructed PubChem-derived text–SMILES dataset for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as initialization; TSMMG fine-tuned for ≈6 days on 8 A100 GPUs with batch size 32, lr=5e-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Pretraining on natural language requires careful fine-tuning to learn SMILES syntax and chemical property mappings; size may limit capacity relative to much larger LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6865.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDKit (chemoinformatics toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source cheminformatics library used as a 'teacher' to compute physicochemical properties (LogP, QED, SAs, MW, PSA, HBA/HBD, aromatic rings) and to validate SMILES syntax during evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>cheminformatics toolkit / rule-based property calculator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (deterministic/tool).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used for calculating properties and parsing SMILES (validity checks).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Operates on SMILES and molecular graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Property extraction for dataset labeling and validity checks during generation/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Used to compute LogP, QED thresholds, SAs, and to validate generated SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Served as a core 'teacher' in constructing text–molecule pairs and for evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to PubChem molecules and generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity computed by RDKit parsing; LogP, QED, SAs values used in SR criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Deterministic calculations but not perfect predictors of experimental properties (inherent limitations of computed descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6865.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>admetSAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>admetSAR (SVM-based ADMET predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An SVM-based predictor used as a teacher to label molecules for ADMET properties such as blood-brain barrier permeability (BBB) and human intestinal absorption (HIA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>admetSAR</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>supervised SVM predictor for ADMET properties</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained ADMET prediction models (from Cheng et al.); used here as black-box teachers to label PubChem molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used to produce binary/score labels (BBB, HIA) for textual templates.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Operates on SMILES / molecular descriptors computed from SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Labeling dataset for ADMET-constrained molecular generation (BBB/HIA).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Threshold: BBB>0.5 and HIA>0.5 used as success cutoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrated as a teacher during dataset construction; labels used in prompts and evaluation (SR).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to PubChem-derived molecules and generated samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Predictor accuracy limits the fidelity of labels; models are approximations and may produce false positives/negatives that affect downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6865.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Affinity SVMs (Olivecrona/Jin)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SVM-based affinity predictors (Olivecrona et al. and Jin et al. models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SVM and other supervised affinity predictors used as teacher models to label molecules for target activity (e.g., DRD2, GSK3, JNK3) during training and to evaluate generated molecules (threshold affinity > 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Olivecrona/Jin SVM affinity predictors</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>supervised classifiers/SVMs for target-binding probability prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained models (from referenced works) applied as black-box teachers to create affinity labels for PubChem molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used to assign binary/score labels for target binding (used in text templates and SR calculation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES / molecular descriptors / fingerprints as input to predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Provide activity labels for drug-target affinity constraints (DRD2, GSK3, JNK3 etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Affinity score > 0.5 considered active; used in multi-constraint prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Teacher predictions incorporated into text–SMILES training pairs and used to compute SR on generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Applied to PubChem molecules; used in evaluation of generated molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Predictor quality affects label correctness; limited generalization can bias the student model and generated molecules toward teacher model biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6865.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Docking pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UCSF Chimera + DOCK6 + PLIP + PyMOL pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational docking and visualization pipeline used for qualitative validation of generated molecules (zero-shot 5-constraint EP2/EP4 binders): Chimera for prep, DOCK6 for docking, PLIP and PyMOL for interaction analysis and visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Docking pipeline (Chimera/DOCK6/PLIP/PyMOL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>computational docking and structure-visualization tools</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not applicable (tools used for downstream in silico validation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Used to assess spatial fit and interactions of TSMMG-generated molecules with EP2 (PDB 7CX2) and EP4 (PDB 5YWY).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>3D ligand conformers derived from SMILES; protein structures from PDB.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>In silico structural validation of predicted multi-constraint molecules (binding to EP2 and EP4).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Docking score and visualization of hydrogen bonds / interactions used qualitatively; no reported numeric docking thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Sequence: Chimera (prep) → DOCK6 (docking) → PLIP/PyMOL (interaction analysis & figures).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Polished PDB receptor structures (EP2 PDB 7CX2, EP4 PDB 5YWY) and TSMMG-generated ligands.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative docking poses, identification of hydrogen bonds to receptor residues (visualized), no experimental binding assays reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Four TSMMG-generated molecules were shown to fit ligand sites and form hydrogen bonds in docking models (qualitative figure evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Docking is computational and not proof of experimental binding; no reported docking scores or comparative baselines; docking quality depends on receptor preparation and scoring limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6865.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (text–molecule translation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that addressed bidirectional translation between natural language and molecular sequences by pretraining on unpaired corpora and fine-tuning on small paired sets (ChEBI-20); cited as related work and contrasted with TSMMG's larger teacher-driven paired dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder / sequence-to-sequence LLM (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining on language and molecules and fine-tuning on ChEBI-20 (small paired set) as reported in the literature (cited by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Bidirectional translation (text↔molecule) via fine-tuning; mentioned as limited by small paired datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES and textual descriptions (ChEBI comments).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Text–molecule translation and molecular captioning (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Noted limitation: ChEBI-20 has small size and strong one-to-one mappings, limiting one-to-many learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 (33,010 pairs) as discussed in the paper's related work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>ChEBI-20 dataset small and biased toward one-to-one mappings, limiting generalization for generative tasks (as discussed by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6865.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolXPT (wrapping molecules with text for generative pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior approach that integrates molecules into LLM inputs as multimodal tokens for generative pretraining; cited as related work and contrasted with TSMMG's teacher-distillation dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolXPT: Wrapping Molecules with Text for Generative Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolXPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>multimodal LLM / molecule-in-text approach (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Molecule tokens wrapped in text for pretraining (cited prior art).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES or embedded molecular tokens inside text context (as per cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generative pretraining for molecule-related tasks (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as prior art; paper argues TSMMG addresses data scarcity and quality issues that limit such prior methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6865.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT / MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT / MolGPT (transformer-decoder molecular generators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-decoder based models previously used for SMILES generation and related tasks; cited as related methods that treat constraints as conditional codes or use decoder architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolReGPT / MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-decoder style models (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Varies by referenced work (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional SMILES generation via transformer decoders; some treat properties as conditional codes.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Conditional de novo molecular generation (related work).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property-conditioning (LogP, QED, SA, TPSA etc.) described in related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI/ZINC/other datasets in related work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Cited to motivate a need for larger, higher-quality text–molecule paired datasets and simpler unified models like TSMMG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6865.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6865.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubChem (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubChem (public chemical database)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Public molecular library used as the source of ≈2 million molecules for teacher labeling and construction of the text–SMILES training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PubChem</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>chemical database / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Millions of molecules (≈2 million used by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Raw SMILES and associated metadata drawn from PubChem; molecules used as base for teacher annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not applicable (data source).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES, InChI, molecular records from PubChem.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Training data source for TSMMG and SMILES2IUPAC; used to sample diverse molecules for teacher labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>PubChem molecules processed by RDKit, admetSAR, affinity predictors and IUPAC parser to produce text–SMILES pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>PubChem contains diverse chemotypes but teacher labeling quality and representativeness of specific FGs/targets depends on teacher coverage; class imbalance in FG frequencies observed to impact generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Instruction multi-constraint molecular generation using a teacher-student large language model', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>MolXPT: Wrapping Molecules with Text for Generative Pretraining <em>(Rating: 2)</em></li>
                <li>MolGPT: molecular generation using a transformer-decoder model <em>(Rating: 2)</em></li>
                <li>Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning <em>(Rating: 2)</em></li>
                <li>Composing molecules with multiple property constraints <em>(Rating: 2)</em></li>
                <li>Molecular de-novo design through deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>admetSAR: a comprehensive source and free tool for assessment of chemical ADMET properties <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6865",
    "paper_id": "paper-268536992",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "TSMMG",
            "name_full": "Teacher-Student-based Multi-constraint Molecular Generation",
            "brief_description": "A transformer-based decoder LLM (initialized from GPT2-small) fine-tuned on text–molecule pairs distilled from multiple 'teacher' tools to generate SMILES directly from natural-language property prompts, supporting multi-constraint and zero-shot multi-property generation for drug-discovery style tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TSMMG",
            "model_type": "decoder-only LLM (transformer decoder) fine-tuned; teacher-student knowledge-distillation framework",
            "model_size": "~117M parameters (GPT2-small initialization)",
            "training_data_description": "Pretrained on large natural-language corpus via GPT-2 weights, then fine-tuned on ≈2 million PubChem molecules for which multiple properties/structural annotations were extracted by external 'teacher' tools and rendered into natural-language templates (text–SMILES paired data with One-to-Many and Many-to-One sampling schemes).",
            "generation_method": "Direct conditional generation of SMILES from natural-language prompts (fine-tuned autoregressive decoding); knowledge distillation from multiple property predictors and parsers; sampling temperature controls randomness.",
            "chemical_representation": "SMILES (output); natural-language textual prompts (input); IUPAC strings used internally for functional-group extraction.",
            "target_application": "De novo molecular generation for drug discovery: generating molecules satisfying physicochemical properties (LogP, QED, SA), target affinities (e.g., DRD2, GSK3, EP2, EP4, BTK, FGFR4, KPCD3, 3CL), and ADMET properties (BBB, HIA).",
            "constraints_used": "Functional group presence (FG/IUPAC), LogP (with ±1 margin), QED (&gt;0.6), Synthetic Accessibility (SAs &lt; 4), predicted affinity thresholds (&gt;0.5), BBB (&gt;0.5), HIA (&gt;0.5); multi-constraint combinations up to 5 constraints in zero-shot testing.",
            "integration_with_external_tools": "Training-data construction and evaluation used RDKit (physicochemical properties, validity), admetSAR (BBB/HIA ADMET predictions), SVM affinity predictors (Olivecrona et al., Jin et al.), an IUPAC parser / SMILES2IUPAC model, and downstream computational docking with UCSF Chimera (preparation), DOCK6 (docking), PLIP and PyMOL (visualization).",
            "dataset_used": "≈2 million PubChem molecules were used to construct text–SMILES pairs; teacher predictors provided labels; evaluation used held-out datasets and generated samples (5,000 generated molecules per multi-constraint task).",
            "evaluation_metrics": "Validity (SMILES parse via RDKit), Uniqueness, Novelty (not in training set), Diversity (1 - average Tanimoto similarity on Morgan fingerprints), Success Rate (SR): fraction satisfying all task constraints; SR(nFG) computed ignoring FG matching for FG-constrained tasks; task-specific cutoffs (e.g., QED&gt;0.6, SAs&lt;4, affinity&gt;0.5, BBB/HIA&gt;0.5).",
            "reported_results": "Validity: ≈99.87% across 2-, 3-, 4-constraint tasks; Uniqueness (avg): 90.27% (2-constraint), 81.2% (3-constraint), 81.89% (4-constraint); Novelty (avg): 92.79%, 87.6%, 87.87%; Diversity (scaled): 90.47, 89.3, 89.37; Success Rates (SR): 82.58% (2-constraint), 68.03% (3-constraint), 67.48% (4-constraint). Zero-shot 5-constraint case produced molecules which were docked to EP2 (PDB 7CX2) and EP4 (PDB 5YWY) showing plausible interactions (qualitative docking/visualization results). Temperature sweep: increasing sampling temperature increased uniqueness but decreased SR (e.g., Unique rose ~17% from T=0.5 to 1.5; SR dropped ~21%).",
            "experimental_validation": false,
            "challenges_or_limitations": "Relies on teacher predictors whose errors propagate; functional-group (FG) frequency imbalance in training data reduced uniqueness and SR for some FG+activity tasks (e.g., DRD2/GSK3); exponential combination space of properties makes exhaustive training impractical; no wet-lab synthesis/biological testing reported (only computational docking); sensitivity to prompt wording (template shifts reduced SR and novelty for some tasks); computational cost for fine-tuning (8×A100, ~6 days).",
            "uuid": "e6865.0",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "SMILES2IUPAC",
            "name_full": "SMILES-to-IUPAC GPT2-based translator",
            "brief_description": "A GPT2-small-based conditional generation model trained to translate SMILES strings into IUPAC names for high-throughput functional-group extraction during dataset construction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SMILES2IUPAC",
            "model_type": "decoder-only LLM fine-tuned for sequence-to-sequence translation",
            "model_size": "~117M parameters (GPT2-small)",
            "training_data_description": "Trained on 2 million SMILES–IUPAC pairs collected from PubChem.",
            "generation_method": "Conditional autoregressive generation: input SMILES → generate IUPAC text.",
            "chemical_representation": "Input: SMILES; Output: IUPAC name (text).",
            "target_application": "High-throughput IUPAC name generation to extract structural features/functional groups for constructing natural-language training templates.",
            "constraints_used": "Accuracy of IUPAC generation sufficient to recover functional groups reliably.",
            "integration_with_external_tools": "Integrated into the data-generation pipeline to produce IUPAC names used by the IUPAC parser and template generator.",
            "dataset_used": "2M SMILES–IUPAC pairs from PubChem.",
            "evaluation_metrics": "Functional-group extraction accuracy comparing predicted IUPAC-derived FGs to ground truth IUPAC-derived FGs.",
            "reported_results": "On 1000 unseen molecules, predicted IUPAC names yielded correct functional-group recovery at 94% accuracy.",
            "experimental_validation": false,
            "challenges_or_limitations": "Prediction errors (≈6%) can lead to incorrect FG labels in training data; prior tools for SMILES→IUPAC had low throughput, motivating this custom model.",
            "uuid": "e6865.1",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "GPT2-small",
            "name_full": "GPT-2 small (pretrained)",
            "brief_description": "The standard GPT-2 small language model (12-layer transformer) used as initialization for both TSMMG and SMILES2IUPAC; provides natural-language pretraining backbone for mapping text to molecular sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_type": "decoder-only LLM (pretrained language model), used as fine-tuning base",
            "model_size": "117M parameters",
            "training_data_description": "Pretrained on broad natural-language corpora (OpenAI GPT-2 pretraining datasets), then weights downloaded from HuggingFace and fine-tuned on constructed text–SMILES pairs.",
            "generation_method": "Autoregressive decoding; fine-tuned to map natural-language prompts to SMILES.",
            "chemical_representation": "Natural language tokens (input) and SMILES tokens (output) after fine-tuning.",
            "target_application": "Base architecture enabling natural-language-conditioned molecule generation (TSMMG) and SMILES→IUPAC translation (SMILES2IUPAC).",
            "constraints_used": null,
            "integration_with_external_tools": "Weights initialized from HuggingFace; fine-tuning used AdamW optimizer on GPUs.",
            "dataset_used": "Public language corpora for pretraining (not detailed in paper) and the authors' constructed PubChem-derived text–SMILES dataset for fine-tuning.",
            "evaluation_metrics": null,
            "reported_results": "Used as initialization; TSMMG fine-tuned for ≈6 days on 8 A100 GPUs with batch size 32, lr=5e-4.",
            "experimental_validation": null,
            "challenges_or_limitations": "Pretraining on natural language requires careful fine-tuning to learn SMILES syntax and chemical property mappings; size may limit capacity relative to much larger LLMs.",
            "uuid": "e6865.2",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RDKit",
            "name_full": "RDKit (chemoinformatics toolkit)",
            "brief_description": "Open-source cheminformatics library used as a 'teacher' to compute physicochemical properties (LogP, QED, SAs, MW, PSA, HBA/HBD, aromatic rings) and to validate SMILES syntax during evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RDKit",
            "model_type": "cheminformatics toolkit / rule-based property calculator",
            "model_size": "",
            "training_data_description": "Not applicable (deterministic/tool).",
            "generation_method": "Used for calculating properties and parsing SMILES (validity checks).",
            "chemical_representation": "Operates on SMILES and molecular graphs.",
            "target_application": "Property extraction for dataset labeling and validity checks during generation/evaluation.",
            "constraints_used": "Used to compute LogP, QED thresholds, SAs, and to validate generated SMILES.",
            "integration_with_external_tools": "Served as a core 'teacher' in constructing text–molecule pairs and for evaluation metrics.",
            "dataset_used": "Applied to PubChem molecules and generated molecules.",
            "evaluation_metrics": "Validity computed by RDKit parsing; LogP, QED, SAs values used in SR criteria.",
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Deterministic calculations but not perfect predictors of experimental properties (inherent limitations of computed descriptors).",
            "uuid": "e6865.3",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "admetSAR",
            "name_full": "admetSAR (SVM-based ADMET predictor)",
            "brief_description": "An SVM-based predictor used as a teacher to label molecules for ADMET properties such as blood-brain barrier permeability (BBB) and human intestinal absorption (HIA).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "admetSAR",
            "model_type": "supervised SVM predictor for ADMET properties",
            "model_size": "",
            "training_data_description": "Pretrained ADMET prediction models (from Cheng et al.); used here as black-box teachers to label PubChem molecules.",
            "generation_method": "Used to produce binary/score labels (BBB, HIA) for textual templates.",
            "chemical_representation": "Operates on SMILES / molecular descriptors computed from SMILES.",
            "target_application": "Labeling dataset for ADMET-constrained molecular generation (BBB/HIA).",
            "constraints_used": "Threshold: BBB&gt;0.5 and HIA&gt;0.5 used as success cutoffs.",
            "integration_with_external_tools": "Integrated as a teacher during dataset construction; labels used in prompts and evaluation (SR).",
            "dataset_used": "Applied to PubChem-derived molecules and generated samples.",
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": false,
            "challenges_or_limitations": "Predictor accuracy limits the fidelity of labels; models are approximations and may produce false positives/negatives that affect downstream generation.",
            "uuid": "e6865.4",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Affinity SVMs (Olivecrona/Jin)",
            "name_full": "SVM-based affinity predictors (Olivecrona et al. and Jin et al. models)",
            "brief_description": "SVM and other supervised affinity predictors used as teacher models to label molecules for target activity (e.g., DRD2, GSK3, JNK3) during training and to evaluate generated molecules (threshold affinity &gt; 0.5).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Olivecrona/Jin SVM affinity predictors",
            "model_type": "supervised classifiers/SVMs for target-binding probability prediction",
            "model_size": "",
            "training_data_description": "Pretrained models (from referenced works) applied as black-box teachers to create affinity labels for PubChem molecules.",
            "generation_method": "Used to assign binary/score labels for target binding (used in text templates and SR calculation).",
            "chemical_representation": "SMILES / molecular descriptors / fingerprints as input to predictors.",
            "target_application": "Provide activity labels for drug-target affinity constraints (DRD2, GSK3, JNK3 etc.).",
            "constraints_used": "Affinity score &gt; 0.5 considered active; used in multi-constraint prompts.",
            "integration_with_external_tools": "Teacher predictions incorporated into text–SMILES training pairs and used to compute SR on generated molecules.",
            "dataset_used": "Applied to PubChem molecules; used in evaluation of generated molecules.",
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": false,
            "challenges_or_limitations": "Predictor quality affects label correctness; limited generalization can bias the student model and generated molecules toward teacher model biases.",
            "uuid": "e6865.5",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Docking pipeline",
            "name_full": "UCSF Chimera + DOCK6 + PLIP + PyMOL pipeline",
            "brief_description": "A computational docking and visualization pipeline used for qualitative validation of generated molecules (zero-shot 5-constraint EP2/EP4 binders): Chimera for prep, DOCK6 for docking, PLIP and PyMOL for interaction analysis and visualization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Docking pipeline (Chimera/DOCK6/PLIP/PyMOL)",
            "model_type": "computational docking and structure-visualization tools",
            "model_size": "",
            "training_data_description": "Not applicable (tools used for downstream in silico validation).",
            "generation_method": "Used to assess spatial fit and interactions of TSMMG-generated molecules with EP2 (PDB 7CX2) and EP4 (PDB 5YWY).",
            "chemical_representation": "3D ligand conformers derived from SMILES; protein structures from PDB.",
            "target_application": "In silico structural validation of predicted multi-constraint molecules (binding to EP2 and EP4).",
            "constraints_used": "Docking score and visualization of hydrogen bonds / interactions used qualitatively; no reported numeric docking thresholds.",
            "integration_with_external_tools": "Sequence: Chimera (prep) → DOCK6 (docking) → PLIP/PyMOL (interaction analysis & figures).",
            "dataset_used": "Polished PDB receptor structures (EP2 PDB 7CX2, EP4 PDB 5YWY) and TSMMG-generated ligands.",
            "evaluation_metrics": "Qualitative docking poses, identification of hydrogen bonds to receptor residues (visualized), no experimental binding assays reported.",
            "reported_results": "Four TSMMG-generated molecules were shown to fit ligand sites and form hydrogen bonds in docking models (qualitative figure evidence).",
            "experimental_validation": false,
            "challenges_or_limitations": "Docking is computational and not proof of experimental binding; no reported docking scores or comparative baselines; docking quality depends on receptor preparation and scoring limitations.",
            "uuid": "e6865.6",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (text–molecule translation model)",
            "brief_description": "Prior work that addressed bidirectional translation between natural language and molecular sequences by pretraining on unpaired corpora and fine-tuning on small paired sets (ChEBI-20); cited as related work and contrasted with TSMMG's larger teacher-driven paired dataset.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "mention",
            "model_name": "MolT5",
            "model_type": "encoder-decoder / sequence-to-sequence LLM (mentioned in related work)",
            "model_size": "",
            "training_data_description": "Pretraining on language and molecules and fine-tuning on ChEBI-20 (small paired set) as reported in the literature (cited by the paper).",
            "generation_method": "Bidirectional translation (text↔molecule) via fine-tuning; mentioned as limited by small paired datasets.",
            "chemical_representation": "SMILES and textual descriptions (ChEBI comments).",
            "target_application": "Text–molecule translation and molecular captioning (related work).",
            "constraints_used": "Noted limitation: ChEBI-20 has small size and strong one-to-one mappings, limiting one-to-many learning.",
            "integration_with_external_tools": null,
            "dataset_used": "ChEBI-20 (33,010 pairs) as discussed in the paper's related work.",
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "ChEBI-20 dataset small and biased toward one-to-one mappings, limiting generalization for generative tasks (as discussed by the authors).",
            "uuid": "e6865.7",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MolXPT",
            "name_full": "MolXPT (wrapping molecules with text for generative pretraining)",
            "brief_description": "Referenced prior approach that integrates molecules into LLM inputs as multimodal tokens for generative pretraining; cited as related work and contrasted with TSMMG's teacher-distillation dataset construction.",
            "citation_title": "MolXPT: Wrapping Molecules with Text for Generative Pretraining",
            "mention_or_use": "mention",
            "model_name": "MolXPT",
            "model_type": "multimodal LLM / molecule-in-text approach (mentioned in related work)",
            "model_size": "",
            "training_data_description": "Not detailed in this paper (cited).",
            "generation_method": "Molecule tokens wrapped in text for pretraining (cited prior art).",
            "chemical_representation": "SMILES or embedded molecular tokens inside text context (as per cited work).",
            "target_application": "Generative pretraining for molecule-related tasks (related work).",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Mentioned as prior art; paper argues TSMMG addresses data scarcity and quality issues that limit such prior methods.",
            "uuid": "e6865.8",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MolReGPT / MolGPT",
            "name_full": "MolReGPT / MolGPT (transformer-decoder molecular generators)",
            "brief_description": "Transformer-decoder based models previously used for SMILES generation and related tasks; cited as related methods that treat constraints as conditional codes or use decoder architectures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolReGPT / MolGPT",
            "model_type": "transformer-decoder style models (related work)",
            "model_size": "",
            "training_data_description": "Varies by referenced work (cited in related work).",
            "generation_method": "Conditional SMILES generation via transformer decoders; some treat properties as conditional codes.",
            "chemical_representation": "SMILES",
            "target_application": "Conditional de novo molecular generation (related work).",
            "constraints_used": "Property-conditioning (LogP, QED, SA, TPSA etc.) described in related literature.",
            "integration_with_external_tools": null,
            "dataset_used": "ChEBI/ZINC/other datasets in related work (not detailed here).",
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "Cited to motivate a need for larger, higher-quality text–molecule paired datasets and simpler unified models like TSMMG.",
            "uuid": "e6865.9",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "PubChem (dataset)",
            "name_full": "PubChem (public chemical database)",
            "brief_description": "Public molecular library used as the source of ≈2 million molecules for teacher labeling and construction of the text–SMILES training dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PubChem",
            "model_type": "chemical database / dataset",
            "model_size": "Millions of molecules (≈2 million used by authors)",
            "training_data_description": "Raw SMILES and associated metadata drawn from PubChem; molecules used as base for teacher annotation.",
            "generation_method": "Not applicable (data source).",
            "chemical_representation": "SMILES, InChI, molecular records from PubChem.",
            "target_application": "Training data source for TSMMG and SMILES2IUPAC; used to sample diverse molecules for teacher labeling.",
            "constraints_used": null,
            "integration_with_external_tools": "PubChem molecules processed by RDKit, admetSAR, affinity predictors and IUPAC parser to produce text–SMILES pairs.",
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": null,
            "experimental_validation": null,
            "challenges_or_limitations": "PubChem contains diverse chemotypes but teacher labeling quality and representativeness of specific FGs/targets depends on teacher coverage; class imbalance in FG frequencies observed to impact generation quality.",
            "uuid": "e6865.10",
            "source_info": {
                "paper_title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "MolXPT: Wrapping Molecules with Text for Generative Pretraining",
            "rating": 2,
            "sanitized_title": "molxpt_wrapping_molecules_with_text_for_generative_pretraining"
        },
        {
            "paper_title": "MolGPT: molecular generation using a transformer-decoder model",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning",
            "rating": 2,
            "sanitized_title": "multiconstraint_molecular_generation_based_on_conditional_transformer_knowledge_distillation_and_reinforcement_learning"
        },
        {
            "paper_title": "Composing molecules with multiple property constraints",
            "rating": 2,
            "sanitized_title": "composing_molecules_with_multiple_property_constraints"
        },
        {
            "paper_title": "Molecular de-novo design through deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "molecular_denovo_design_through_deep_reinforcement_learning"
        },
        {
            "paper_title": "admetSAR: a comprehensive source and free tool for assessment of chemical ADMET properties",
            "rating": 1,
            "sanitized_title": "admetsar_a_comprehensive_source_and_free_tool_for_assessment_of_chemical_admet_properties"
        }
    ],
    "cost": 0.018709749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Instruction Multi-Constraint Molecular -Generation Using a Teacher-Student Large Language Model</p>
<p>Peng Zhou 
Jianmin Wang 
Chunyan Li 
Zixu Wang 
Yiping Liu 
Siqi Sun 
Jianxin Lin 
Leyi Wei 
Xibao Cai 
Houtim Lai 
Wei Liu 
Longyue Wang 
Yuansheng Liu yuanshengliu@hnu.edu.cn 
Xiangxiang Zeng xzeng@hnu.edu.cn </p>
<p>College of Information Science and Engineering
Hunan University
ChangshaChina</p>
<p>The Interdisciplinary Graduate Program in Integrative Biotechnology
Yonsei University
21983IncheonKorea</p>
<p>School of Informatics
Yunnan Normal University
650500KunmingChina</p>
<p>Department of Computer Science
University of Tsukuba
3058577TsukubaJapan</p>
<p>Research Institute of Intelligent Complex Systems
Fudan University
200433ShanghaiChina</p>
<p>Shanghai AI Laboratory
200232ShanghaiChina</p>
<p>Centre for Artificial Intelligence driven Drug Discovery
Faculty of Applied Science
SAR
Macao Polytechnic University
MacaoChina</p>
<p>School of Informatics
Xiamen University
XiamenChina</p>
<p>Tencent AI Lab</p>
<p>Instruction Multi-Constraint Molecular -Generation Using a Teacher-Student Large Language Model
60A2326BA483E1F7E4CF298E546CAE38Molecular GenerationLarge Language ModelMulti-constraint
While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge.Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'.To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts.We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 82.58%, 68.03%, and 67.48%, respectively.The model also exhibits adaptability through zero-shot testing, creating molecules that satisfy combinations of properties that have not been encountered.It can comprehend text inputs with various language styles, extending beyond the confines of outlined prompts, as confirmed through empirical validation.Additionally, the knowledge distillation feature of TSMMG contributes to the continuous enhancement of small models, while the innovative approach to dataset construction effectively addresses the issues of data scarcity and quality, which positions TSMMG as a promising tool in the domains of drug discovery and materials science.</p>
<p>Background</p>
<p>The development and application of molecular generation models play an essential role in the field of Artificial Intelligence for Drug Discovery (AIDD).Molecular generation models are instrumental in addressing the challenges and complexities associated with the identification and design of novel therapeutic compounds.In contrast to traditional virtual screening approaches, involving the sift of desired molecules from existing libraries, these innovative models are engineered to directly generate novel molecules.Their ability to navigate vast chemical spaces, optimize lead compounds, and facilitate de novo design positions them as indispensable tools in the pursuit of novel and effective therapeutic interventions [1][2][3][4] [5].These models not only exhibit the ability to generate chemically valid molecules that precisely adhere to the requirements of molecular analysis tools [6] [7], but they also excel in the generation of molecules that meet specific constraints, like quantitative estimate of drug-likeness (QED) and molecular hydrophobicity (LogP) [8] [9].However, a primary challenge in the realm of drug discovery lies in identifying molecules that conform to a multitude of constraints, including binding affinity, LogP, QED, synthetic accessibility (SA), and toxicity, among others, rather than merely generating compounds that are chemically valid or solely meeting specific criteria [10] [11].Several works have been introduced to address this challenge, presenting methodologies capable of generating molecules that adhere to a spectrum of concurrent condition constraints.For instance, Li et al. introduced a conditional generative model proficient in generating molecules that meet both SA and QED criteria, even yielding dual-target inhibitors for JNK3 and GSK3 [12].Jin et al. achieved this feat by extracting diverse substructures with varying properties and reassembling them to produce molecules satisfying QED, SA, and the inhibition of both JNK3 and GSK3 [13].Bagal et al. employed a transformer decoder architecture, treating constraint conditions as conditional codes, to explore the generation of molecules under various combinations of multiple constraints, including LogP, TPSA (Total Polar Surface Area), and SA [14].Wang et al. utilized a combination of a conditional transformer, knowledge distillation, and reinforcement learning to generate molecules with activity against DRD2, while also ensuring adherence to QED and SA criteria [10].</p>
<p>Although significant progress has been made in prior endeavors, it is important to acknowledge that multi-constraint molecular generation methods still suffer from several noteworthy limitations, which hinder their practical applicability.These limitations undermine the overall effectiveness and efficiency of these methods in generating molecules that simultaneously meet diverse sets of constraints in drug discovery.These limitations fall into the following points: (1) Current multiconstraint molecular generation methods heavily rely on a narrow set of constraints.These methods predominantly focus on specific molecular properties, such as LogP, QED, SA, DRD2, JNK3, and GSK3.As a result, they may overlook other crucial aspects like substructures, bioavailability, and toxicity.The restricted range of constraints limits the comprehensive exploration of diverse chemical properties, potentially hindering the discovery and optimization of molecules with broader applicability in drug discovery and related domains.(2) These methods often require extensive fine-tuning when applied to different tasks.They tend to generate molecules that closely adhere to the feature distribution of the training dataset.As a consequence, adapting these models to changes in the target space or applying them to diverse tasks necessitates significant retraining.This inflexibility makes the models less adaptable, introducing a substantial burden in terms of computational resources and time when confronted with variations in the application context.(3) They often involve intricate designs.The complexity of the models and algorithms used can be a significant obstacle in their practical application.Users may find it challenging to understand and navigate the complexities of the methods, impacting their usability.Improving the simplicity of these models is essential to make them more accessible and applicable in real-world scenarios, especially in drug discovery and related domains.</p>
<p>To address the challenges, we introduce the Teacher-Student-based Multi-constraint Molecular Generation (TSMMG) model, a natural language-based multi-constraint molecular generation approach.TSMMG offers several pivotal advantages: (1) Flexible Data Generation Framework: Our approach presents a versatile data generation framework that leverages a range of molecular tools and advanced models to selectively extract molecules with diverse properties from publicly available molecular libraries.The framework is based on the concept of knowledge distillation, where various tools and models used to extract molecular knowledge are referred to as 'teachers', while our model, TSMMG, is referred to as the 'student'.Any molecular-related tool or model can serve as a teacher, providing diverse molecular knowledge to TSMMG.The utilization of the teacher-student paradigm in our model provides a highly scalable approach, facilitating the seamless absorption of molecular knowledge beyond the scope of this paper.Moreover, this approach can be easily extended to other domains, such as materials science.(2) Multi-task capability: Harnessing the capabilities of large language models, we can concurrently train across multiple tasks.By formulating distinct prompts, we delineate unique molecular spaces without the need for repetitive fine-tuning.This strategy capitalizes on the adaptability of large language models.(3) Simple Architecture: The proposed model adopts a transformer-based decoder architecture.This design, characterized by its simplicity, eliminates the need for intricate preprocessing of molecular data.</p>
<p>To showcase the expressive capabilities of our proposed model, we meticulously designed 16 sets of experiments for multi-constraint molecular generation.These experiments covered a spectrum of tasks, including molecular substructures, physicochemical properties, affinity with targets, and ADMET properties.Our findings from these experiments are compelling: TSMMG not only yields over 99% of legally valid molecules based on natural language instructions but also, notably, a substantial proportion of these molecules impeccably aligns with the specified properties in the textual descriptions.Furthermore, we conducted a noteworthy case study involving a zero-shot 5constraint task.In this scenario, TSMMG successfully produced molecules capable of simultaneous binding to EP2 and EP4, showcasing favorable drug-likeness and synthetic accessibility, along with the ability to penetrate the blood-brain barrier.This case study serves as an additional testament to the vast potential embedded in TSMMG.Additionally, our model demonstrated its prowess in understanding natural language beyond the prompts outlined in this paper, as empirically validated.This expanded capability further solidifies the model's practical applicability.Moreover, we observed that integrating novel molecules generated by our model significantly enhances the teacher model's performance.This collaborative synergy fosters continuous improvement between the teacher and student models, underscoring the model's adaptability and potential for iterative refinement.</p>
<p>Result</p>
<p>TSMMG approach</p>
<p>As shown in Figure 1, TSMMG process involves the following steps: (1) To begin, a substantial dataset of molecules is collected from publicly available molecular libraries.This dataset undergoes analysis by advanced molecular parsing tools and models, which referred to as "teachers".These teachers extract extensive information, encompassing structural details, physicochemical properties, binding affinities to various targets, and other pertinent attributes for each molecule.The resulting knowledge is then organized into natural language descriptions, which are paired with the corresponding molecules.(2) Next, the "student" model, TSMMG, is introduced and trained using the knowledge obtained in the previous step.TSMMG is designed to create a direct mapping from natural language descriptions to molecular language.By absorbing a diverse range of knowledge expressed in natural language, the model acquires the capability to generate molecules that possess the specified properties outlined in the text.It's worth noting that TSMMG undergoes pre-training on a vast corpus of pure text, enabling it to effectively understand and interpret natural language.(3) When presented with a text description that includes multiple constraints, TSMMG can generate entirely novel molecules that fulfill these textual descriptions.In doing so, it effectively bridges the gap between natural language and molecular language for the purpose of multi-constraint molecular generation.</p>
<p>Figure 1 The process of TSMMG is illustrated as follows: (a) In the initial step, a comprehensive dataset of molecules is gathered from publicly available molecular libraries.This dataset is then subjected to advanced molecular parsing tools and models, which referred to as "teachers."</p>
<p>These teachers extract a wealth of information, including structural details, physicochemical properties, binding affinities to various targets, and other relevant attributes for each molecule.</p>
<p>The acquired knowledge is then structured into natural language descriptions, resulting in a substantial dataset containing paired natural language descriptions and their corresponding molecules.(b) The "Student" model, TSMMG, is introduced and trained using the knowledge acquired in the previous step.TSMMG learns to directly map from natural language descriptions to molecular language.By absorbing diverse knowledge expressed in natural language, the model gains the ability to generate molecules that possess the specified properties detailed in the text.Note that TSMMG undergoes pre-training on a vast corpus of pure text, which enables it to comprehend and interpret natural language effectively.(c) When presented with a text description containing multiple constraints, TSMMG can generate entirely new molecules that satisfy these textual descriptions, effectively bridging the gap between natural language and molecular language for multi-constraint molecular generation.</p>
<p>Multi-constraint tasks</p>
<p>Task setting</p>
<p>To comprehensively demonstrate the efficacy of the TSMMG model, we categorized multiconstraint tasks into three types, each based on different levels of complexity: two-constraint molecular generation, three-constraint molecular generation, and four-constraint molecular generation.Each of these three task categories comprises eight one-constraint tasks.These oneconstraint tasks encompass: Structure Task: Task 1. Specifying a functional group (FG).Physicochemical Property Tasks: Task 2. Specifying the level of hydrophilicity and hydrophobicity (LogP = 1).Task 3. Specifying the level of quantitative estimate of drug-likeness (QED &gt; 0.6).Task 4. Specifying the level of synthetic accessibility score (SAs &lt; 4).Activity Tasks: Task 5. Generating molecules with high affinity for the dopamine type 2 receptor (DRD2 &gt; 0.5).Task 6. Generating molecules with high affinity for the glycogen synthase kinase-3 beta (GSK3 &gt; 0.5).ADMET Property Tasks: Task 7. Generating molecules capable of crossing the blood-brain barrier (BBB &gt; 0.5).Task 8. Generating molecules that can be absorbed by the human small intestine (HIA &gt; 0.5).</p>
<p>We employ the '+' symbol to concatenate multiple one-constraint tasks, thereby representing multiconstraint tasks.Within the two-constraint tasks, we considered eight subtasks, combining structure tasks with individual physicochemical property tasks, activity tasks, and ADMET property tasks.These include (1) FG+FG, 2FG for short; (2) FG+LogP; (3) FG+QED; (4) FG+SAs; (5) FG+DRD2; (6) FG+GSK3; (7) FG+BBB; and (8) FG+HIA.In the three-constraint tasks, we explored subtasks such as (1) FG+DRD2+QED; (2) FG+GSK3+QED; (3) FG+BBB+QED; and (4) FG+HIA+QED.The four-constraint tasks include (1) FG+DRD2+QED+SAs; (2) FG+GSK3+QED+SAs; (3) FG+BBB+QED+SAs; and (4) FG+HIA+QED+SAs.</p>
<p>It is essential to emphasize that all these tasks were completed within a single model, employing different natural language prompts.The model underwent comprehensive training in a unified process, eliminating the need for repetitive fine-tuning.The specific prompts used in these experiments are detailed in Table 1.</p>
<p>BTK</p>
<p>The molecule can bind to BTK.</p>
<p>FGFR4</p>
<p>The molecule can bind to FGFR4</p>
<p>KPCD3</p>
<p>The molecule can bind to KPCD3.</p>
<p>3CL</p>
<p>The molecule can bind to 3CL.</p>
<p>Performance Analysis</p>
<p>The experimental results, depicted in Figure 2-a and Figure 2-b, unveil several noteworthy findings: Validity: The model demonstrates a remarkable ability to generate molecules that adhere to the syntax rules of SMILES (Simplified Molecular Input Line Entry System) [15], with an impressive average validity rate of 99.87%, 99.89%, and 99.87% for two constraint tasks, three constraint tasks, and four constraint tasks, respectively.This underscores the model's proficiency in consistently producing grammatically correct molecules.Uniqueness: Most generated molecules are unique, with an outstanding average uniqueness rate of 90.27%, 81.2%, and 81.89% for two constraint tasks, three constraint tasks, and four constraint tasks, respectively.From a specific task perspective, the uniqueness of tasks related to DRD2 and GSK3 is relatively low, averaging less than 70%, while other tasks score above 90%.In the next section, we will analyze the reasons behind this situation.Overall, the model consistently generates largely distinct molecules across various tasks.Novelty:</p>
<p>The average novelty of the generated molecules stands at 92.79%, 87.6%, and 87.87%.Similar to uniqueness, tasks related to DRD2 and GSK3, such as FG+DRD2+QED (82.76%),FG+GSK3+QED (82.44%),FG+DRD2+QED+SA (83.9%), and FG+GSK3+QED+SA (83.14%), have relatively lower novelty scores, while other tasks have novelty scores exceeding 90%.In general, the model demonstrates a capacity to generate innovative molecules for most of the tasks at hand.Diversity: Most generated molecules exhibit notable structural differences, as reflected in the outstanding average diversity score of 90.47, 89.3, and 89.37.Similarly, tasks related to DRD2 and GSK3 exhibit lower diversity compared to other tasks.Success Rates: The average success rate stands at 82.58%, 68.03%, and 67.48% for two constraint tasks, three constraint tasks, and four constraint tasks, respectively, which demonstrates the model's efficacy in generating novel molecules that effectively meet all requirements specified by natural language.</p>
<p>Impact of FGs on Performance</p>
<p>An important distinction from previous methods is that we consider functional groups (FG) as an additional constraint, allowing for more precise control over the direction of generation.Given the relatively low uniqueness in tasks related to DRD2 and GSK3, we use the FG+DRD2 task as an example to further discuss the impact of FG on generation results.</p>
<p>Firstly, we analyze the reasons for the low uniqueness (68.48) of the FG+DRD2 task.In this task, our prompt template is "The molecule contains [FG].It is active to DRD2."We randomly selected 1000 FGs to form 1000 prompts, each generating 5 molecules, totaling 5000 molecules.The only difference between each prompt is the FG, so we group according to the number of unique molecules generated by each prompt and then extract the FG from these prompts for analysis.As shown in Figure 2-c-(1), we see that the number of FGs that led to the generation of 1, 2, 3, 4, and 5 unique molecules were 195, 287, 252, 184, and 82, respectively.Over 80% of the FG-associated prompts can generate two or more unique molecules, with 82 FG-associated prompts each generating 5 completely different molecules.A significant portion of FG-associated prompts tend to generate identical molecules.We speculate that the main reason for this is the inconsistent frequency of these FGs in the training set, causing the model to be unable to effectively learn the larger molecular space corresponding to the FG.In light of this speculation, we grouped these 1000 FGs according to the number of unique molecules generated and calculated the average frequency of the FGs in the training set within each group.As shown in Figure 2-c-(2), this is basically consistent with our speculation.Except for the group generating one unique number of molecules as group 1, as the number of unique molecules generated increases, the frequency of the corresponding group's FG in the training set also increases, indicating that these FGs can be better trained.The average frequency of the FGs in the group with one unique number is slightly higher than that of the group with a 2 unique number, and lower than the other groups, which we assume is an acceptable bias.We then checked the frequency of the FGs in the DRD2 related training set.As shown in Figure 2-c Given the significant impact of FG on the success rate, we calculated the success ratio without considering FG matching, abbreviated as SR (nFG).For example, for the FG+DRD2+QED+SA task, SR (nFG) only considers whether DRD2, QED, and SA meet the constraints.The results are shown in Figure 2-d.It can be seen that in the two-constraint task, three-constraint task, and fourconstraint task, the success rate without considering FG is 13.06%, 16.51%, and 16.76% higher than the success rate considering FG, respectively.</p>
<p>The above observations suggest that as more molecules and FGs are added to the training set, our model should be able to achieve more significant performance.</p>
<p>Effect of Temperature on Performance</p>
<p>During the inference process of large language models, temperature is a parameter of great interest.</p>
<p>A lower temperature implies lower randomness, while a higher temperature means the model has greater freedom.We conducted tests on all tasks by setting different temperatures.Figure 2-e shows the average performance of all tasks when the temperature is set to 0.5, 0.75, 1.0, 1.25, and 1.5, respectively.It can be observed that as the temperature increases, the ability to generate valid molecules remains virtually unchanged, still maintaining above 99%.Novelty and diversity also remain almost unchanged.However, unique and SR show a noticeable increase or decrease.Unique increases from 73.42% to 90.04%, an improvement of approximately 17%, indicating that as the temperature increases, the model can generate more unique molecules.At the same time, SR decreases from 83.03% to 61.94%, a reduction of about 21%.The decrease in SR is roughly consistent with the increase in unique, which means that although increasing the temperature from 0.5 to 1.5 generates 17% more unique molecules, most of them do not satisfy all constraint conditions.</p>
<p>Case Study of a Five-constraint molecular generation</p>
<p>Given the availability of corresponding predictors and a sufficiently large molecular library, it is theoretically feasible to construct training sets for any combination of desired molecular properties.This would enable the model to generate molecules that encompass a wide range of attributes.However, the challenge arises as the number of properties and their combinations increases, resulting in an exponential growth in the total number of possible property combinations.The exhaustive coverage of all these combinations becomes impractical.To address this challenge, we embarked on an investigation to determine if a model could effectively generate molecules when trained using individual properties but tested on arbitrary combinations.This research aimed to assess the model's adaptability to novel scenarios.To this end, we designed a task that entailed the generation of molecules exhibiting high drug-likeness, good synthetic accessibility, blood-brain barrier permeability, and the ability to bind to both the prostaglandin E2 receptor EP2 subtype [16] and prostaglandin E receptor EP4 [17].The input prompt constructed for this task was: 'The molecule exhibits a high QED score, good synthetic accessibility.It can pass through the bloodbrain barrier and binds to both Prostanoid EP2 and EP4 receptors.'During the training phase, each molecule was associated with only one property, meaning the model was exposed to molecules corresponding to each of the five properties within this prompt.However, the model had not encountered molecules that simultaneously met all five of these properties, and indeed, it had not seen molecules that met even two properties explicitly simultaneously.This task presents a formidable challenge from multiple perspectives.From the model's input perspective, the model encounters significantly longer input text, a departure from its prior training data.In terms of molecular properties, the model must not only comprehend the mapping of individual properties to molecular spaces but also grasp the complex mapping of multiple properties from a single property mapping.Surprisingly, the model proves to be up to the task, successfully generating molecules that simultaneously satisfy all the condition constraints.As illustrated in Figure 3, we showcase four molecules that meet all the property requirements specified in the textual description.To validate their compatibility with the receptors of EP2 (PDB ID: 7CX2) and EP4 (PDB ID: 5YWY), we employed UCSF Chimera [18] for molecular docking preparation and UCSF Dock6 [19] to conduct molecular docking.Finally, we used PLIP [20] and PyMOL [21] for visualizing the docking results.The docking results reveal that these molecules effectively fit into the ligands and establish hydrogen bonds with different residues, demonstrating their potential for fulfilling the specified molecular properties.This experimental outcome holds profound significance, as it demonstrates the model's robust capability to generate molecules that satisfy complex multi-constraints during zero-shot testing, even when initially trained with one-constraint data.This versatility underscores the model's adaptability and its potential to address intricate challenges in molecular generation.</p>
<p>Diversity of Input Text</p>
<p>Given that TSMMG is trained based on GPT-2 [40], which has undergone extensive pre-training on natural language datasets, we have a reasonable basis to hypothesize that TSMMG can comprehend the similarities in natural language.Specifically, when provided with prompts that share the same semantics but exhibit subtle differences in their expressions, TSMMG is likely to generate accurate molecules.This hypothesis stems from the fact that GPT-2 has acquired the ability to capture various linguistic patterns and semantic relationships during its training process.Consequently, it may possess the capability to generalize and transfer its knowledge to related but slightly different prompts.In essence, TSMMG's potential to generate correct molecules may persist even with prompt variations due to its underlying understanding of linguistic similarities.</p>
<p>To test this hypothesis, we explored the use of diverse templates that encompass different language habits and variations.By making slight modifications to the original training templates, we aimed to assess TSMMG's ability to generate correct molecules when input prompts were slightly altered.For example, during the training phase, we utilized the template 'The molecule contains [FG], it can be absorbed by the human intestinal tract.' for the FG+HIA task.We introduced minor adjustments to create two new prompts: 'I want a molecule that contains [FG] and can be absorbed by the human intestinal.' and 'Give me a molecule which contains [FG] and can be absorbed by the human intestinal.'We conducted experiments using these diverse prompts across four different tasks, as presented in Table 2, and the results are summarized in Table 3.The experiments demonstrated that TSMMG consistently generated molecules that met the specified requirements to a large extent, even with modified prompts.As shown in the Table 2, the validity of the generated molecules can still reach over 99% after using prompts of different styles.For the FG+BBB and FG+HIA tasks, using the T1 and T2 templates both resulted in approximately a 9% decrease in SR compared to using the T0 template, while uniqueness, novelty, and diversity showed almost no significant changes.For the FG+DRD2 task, when using the T1 template, SR decreased by 33.36%, novelty decreased by 12.12%, while uniqueness increased by 3.36%; when using the T2 template, SR decreased by 30%, novelty decreased by 11.22%, while uniqueness increased by 1.58%.The FG+GSK3 task and the FG+DRD2 task show the same trend, that is, when using the T1 and T2 templates, SR and novelty shows a significant decrease and uniqueness shows a certain degree of increase, while other indicators show relatively small differences.These results suggest that TSMMG exhibits a certain degree of tolerance to diverse prompts and can continue to generate molecules that meet the specified requirements, even when the prompts are modified.It is important to note that while TSMMG may demonstrate tolerance to prompt variations, the extent of its ability to generalize and generate accurate molecules may vary depending on the specific prompt and task.</p>
<p>Discussion</p>
<p>TSMMG as producer</p>
<p>The development of TSMMG can be viewed as a form of knowledge distillation [46], as depicted in Figure 4-a.Initially, diverse molecular properties are obtained using teacher models.These properties are then encapsulated into natural language descriptions and combined with molecular sequences to create text-molecule pairs.TSMMG is trained using these textmolecule pairs as training data, enabling it to acquire the knowledge inherent in the properties through natural language.By leveraging this process, TSMMG becomes proficient in generating molecules that exhibit the desired properties.Notably, TSMMG has the ability to generate novel molecules possessing specific properties based on the acquired knowledge.This offers a feedback loop to the teacher models, allowing them to refine and update their knowledge.To illustrate this, we conducted further experiments involving Serine/threonine-protein kinase D3 (KPCD3), Bruton's tyrosine kinase (BTK), Fibroblast growth factor receptor 4 (FGFR4), and Papain-like protease 3CL.Initially, each target dataset is randomly divided into training and test sets.Subsequently, a random subset is partitioned from the training dataset to serve as the validation set, and an SVM predictor is trained using the training set and validated using the validation set.This process is repeated 100 times to select the best predictor.Finally, the chosen predictor is applied to the test set to obtain the F1 score.For comparison, different numbers of molecules generated by TSMMG that can bind to the corresponding target are randomly added to the training set to train new SVM predictors.This process is also repeated 100 times to yield consistent statistical data.These added molecules are referred to as pseudo samples.The experimental results presented in Figure 4-b demonstrate that the addition of pseudo samples significantly enhances the performance of the predictors.Notable improvements are observed in KPCD3, BTK, FGFR4, and 3CL by approximately 13%, 4%, 17%, and 7%, respectively.Furthermore, as the number of pseudo samples increases, the performance of each predictor tends to converge.These results indicate that TSMMG can discern the commonalities shared by molecules with specific properties and generate novel molecules that embody these commonalities.Moreover, TSMMG's unique ability to learn from teacher models and provide feedback for updating the knowledge of these models initiates a symbiotic relationship that promotes continuous improvement in their respective capabilities.</p>
<p>Conclusions</p>
<p>In conclusion, the TSMMG model represents a significant leap forward in the field of multiconstraint molecular generation for drug discovery and related applications.TSMMG's innovative approach addresses several critical limitations in existing methodologies, unlocking the potential for more efficient and versatile drug discovery processes.Through a comprehensive set of experiments, TSMMG's efficacy is demonstrated across a range of tasks, from two-constraint to four-constraint molecular generation.The model consistently excels in generating molecules that adhere to predefined conditions, boasting impressive success rates across various property combinations.Moreover, TSMMG exhibits remarkable abilities when subjected to zero-shot testing, generating molecules that fulfill complex multi-constraint requirements, even when such combinations were not present in the training data.This feature demonstrates TSMMG's adaptability and its potential to revolutionize molecular design.</p>
<p>TSMMG offers a fresh perspective and promising capabilities, paving the way for more streamlined, efficient, and adaptable approaches.Its potential to expand into other domains, such as materials science, signifies the wide-reaching impact of this innovative model.TSMMG stands as a testament to the synergy between natural language understanding and molecular generation, opening new doors for researchers in the quest for novel compounds and therapies.</p>
<p>Method Problem Setting</p>
<p>Natural language serves as a user-friendly means for human-machine interaction, making it an ideal solution for generating molecules from natural language descriptions.Recent successes in the development of large language models (LLMs) [22][23] inspire the vision that we may achieve the generation of molecules from diverse molecular spaces by simply modifying the input prompt.This approach offers a promising solution to address the challenge of generality in molecular generation.Despite both natural language and SMILES being sequence data formats, SMILES can be viewed as a specialized molecular language that can be challenging for humans to interpret.From this perspective, generating molecular sequences from natural language descriptions can be regarded as a translation task, an area where LLMs excel.</p>
<p>Given a natural language sequence,  = { 1 ,  2 , … ,   } , the objective is to generate a corresponding molecule represented by a SMILES sequence,  = { 1 ,  2 , … ,   } , which can be formulated as conditional probability:</p>
<p>𝑃(𝑆|𝑊)</p>
<p>In order to ensure the quality of the generated molecules, it is imperative to adhere to the following prerequisites: (1) Validity.The generated SMILES representation, , should strictly adhere to the syntax rules of the SMILES format, guaranteeing that it forms a valid and well-structured molecule.</p>
<p>(2) Relevance: The molecule represented by  should accurately reflect the physical and chemical properties described by the natural language sequence  .This entails that if there exists a subsequence  , in  that specifies a particular property, there should be a corresponding subsequence  , in  that satisfies the desired property.(3) Diversity: While satisfying the validity and relevance criteria, the generated  should exhibit diversity.In other words, the generated molecules should not be identical or overly similar, providing a range of molecular structures that fulfill the given properties.( 4) Novelty: The model should possess the ability to generate  that are not present in the training set.This capability ensures that the generated molecules introduce new and previously unseen chemical structures, thereby expanding the exploration space beyond the confines of the training data.</p>
<p>The quandary of translating natural language into molecular language, albeit bearing resemblances to conventional machine translation, poses distinctive challenges.In this context, three fundamental patterns of correspondence between natural language and molecular sequences can be discerned: (1) One-to-One Mapping: In this pattern, a specific text description corresponds to a single, specific molecular sequence.Models like MolT5 [24], MolXPT [25], and MoleculeSTM [26] have tackled this problem as a query task, aiming to establish a direct mapping relationship between text and molecular sequences.However, this approach may not be ideal for generating novel molecules with diverse properties, as it relies on a fixed ground truth and does not explore beyond the known data.</p>
<p>(2) One-to-Many Mapping: Here, a text description can correspond to multiple different molecular sequences.This pattern allows the model to learn the feature distribution of the target space, enabling sampling from the distribution to generate new molecules.Models like those proposed by Kotsias et al. [27] and Wang et al. [10] leverage this pattern effectively by training on specific datasets containing molecules with shared properties which implicitly embracing the One-to-Many mapping pattern.(3) Many-to-One Mapping: In this pattern, a specific molecular sequence can be described in various ways.By understanding the inherent relationship between different attributes, it is possible to discover new properties of a molecule.This pattern offers opportunities for exploring diverse attributes of molecules beyond their known properties.</p>
<p>In order to develop a universal molecular generative model capable of generating molecules with various desired properties without the need for retraining, it is essential to accumulate a substantial amount of data that explicitly adheres to the One-to-Many and Many-to-One mapping patterns.The primary challenge lies in acquiring a sufficient number of text-molecule pairs in a rapid, convenient, and cost-effective manner.</p>
<p>Data Generation Framework</p>
<p>Several studies have explored the integration of natural language and molecular language.MolT5 [24], aimed to achieve bidirectional translation between natural language and molecular language.</p>
<p>The model underwent initial pre-training on an extensive collection of unpaired natural language corpora and molecular sequences, followed by fine-tuning on the text-molecule paired dataset ChEBI-20.However, ChEBI-20 presents two notable limitations.Firstly, it contains a relatively small set of 33,010 text-molecule pairs, making it challenging to establish the correspondence between natural language and molecular language.Secondly, the text descriptions in this dataset, sourced from the comment field in ChEBI [28], often contain information unrelated to molecular properties.Additionally, these descriptions exhibit a strong one-to-one relationship with the molecules, posing challenges for the model to explore the specific molecular space associated with desired properties.MolXPT [25] proposed a method that involves incorporating molecular sequences within the input text for Large Language Models (LLMs).CLAMP [29] introduced a fusion approach, combining a molecule encoder and a text encoder for property prediction tasks.Christofidellis et al. [30] presented a unified model capable of handling various text-to-text, text-tomolecule, molecule-to-text, and molecule-to-molecule tasks.MolReGPT [31] implemented tasks such as molecule captioning and text-based molecule generation by assigning ChatGPT a role as a biochemist, facilitating in-context learning.</p>
<p>However, a common limitation in all of the above-mentioned studies is their reliance on the ChEBI dataset, which constrains their performance due to data scarcity and quality issues.As of now, limited research efforts have been directed at addressing these issues in natural language-based molecular generation.Therefore, we propose a knowledge distillation-based approach to construct an extensive and high-quality dataset of natural language-molecule pairs.Figure 1-a provides an overview of the framework employed for the creation of our dataset.The underlying concept revolves around the utilization of advanced molecular parsing tools and models to extract knowledge related to molecules.Subsequently, this acquired knowledge is transformed into natural language text, resulting in paired data comprising molecules and their corresponding textual descriptions.Within this framework, the tools and models responsible for extracting molecular knowledge are collectively referred to as 'teachers', while TSMMG assumes the role of the 'student'.TSMMG undertakes the task of learning various properties associated with molecules from these 'teachers'.It also comprehends the mapping relationship between these properties and the molecular structures themselves.This knowledge empowers TSMMG to generate new molecules based on specified properties using natural language descriptions.Within this framework, multiple 'teachers' are employed, each with distinct capabilities related to molecular properties and structures.These teachers encompass a range of tools and models, including: Physicochemical Property Teacher: RDKit, a tool capable of parsing molecules to extract physicochemical properties such as molecular weight (MW), the number of aromatic rings (AROM), LogP, SA, QED, the number of hydrogen bond acceptors (HBA), the number of hydrogen bond donors (HBD), and topological molecular polar surface area (PSA).</p>
<p>ADMET Property Prediction Models: admetSAR [32], based on Support Vector Machine (SVM), predicts ADMET properties, such as blood-brain barrier permeability and absorptivity.</p>
<p>Affinity Prediction Models: Olivecrona et al.'s SVM-based models [33] and Jin et al.'s models [34] can predict the binding probabilities of molecules to specific targets, including DRD2, GSK3, and JNK3.Newer models such as MolTrans [35], DrugBAN [36], and TransformerCPI [37] are designed to predict the affinity of small molecules to receptor proteins and more.</p>
<p>Structural Information Extraction:</p>
<p>In addition to these property-related teachers, the IUPAC name of a molecule, which bears structural information, is considered.The IUPAC name exhibits a grammar resembling natural language and provides standardized descriptions of molecules.By breaking down IUPAC names, it is possible to extract structural components of a molecule.For instance, deconstructing the molecule '(2-methyl-5-methylsulfonylphenyl)methanamine' yields the functional groups 'methyl,' 'methylsulfonylphenyl,' and 'methanamine.'Therefore, an IUPAC parser is proposed, along with a set of rules for dissecting IUPAC names, serving as an additional 'teacher' for extracting the internal structure of molecules.</p>
<p>Through these 'teachers,' we acquire extensive knowledge about molecules, including their structural information, physicochemical properties, and binding affinities to specific receptors.This information is then transcribed into natural language descriptions and combined with the corresponding molecules to create text-molecule pairs.For an example as shown in Figure 1-b, let's consider a molecule represented as 'CCN1CCCC1CNC(=O)c1c(OC)ccc(Cl)c1O.'We can break down its IUPAC name to extract the functional group 'methoxybenzamide.'By utilizing RDKit, we determine its LogP, QED, and SAs.We further predict its affinity with DRD2 through a classifier proposed by Olivecrona et al. [33], and evaluate its likelihood of passing through the blood-brain barrier using admetSAR.These various properties are then associated with the molecule using natural language templates.</p>
<p>The data generation method offers several notable advantages: (1) With numerous publicly accessible molecular databases like PubChem [38] and ZINC [39], our approach allows for the rapid acquisition of a large number of text-molecule pairs.This effectively overcomes the data limitations often encountered in natural language-based molecular generation models; (2) The text molecule pairs generated through this method exhibit a high degree of relevance.Each segment of text contains certain properties of the molecules, enabling the model to learn the mapping relationship between text descriptions and molecular properties more effectively; (3) There is a wealth of advanced tools and models available for molecular structure analysis and property prediction.Our framework simplifies the process of transferring knowledge from these advanced tools and models into a student model in natural language form.This empowers the student model to generate molecules that incorporate this knowledge.(4) The method is highly scalable, allowing for the seamless transfer of knowledge for various molecular properties.It can be applied to an array of properties, making it versatile for different research needs.(5) Our method supports continuous knowledge updates.This means that the student model can benefit from the latest and more robust models, ensuring that it remains up-to-date and well-informed.</p>
<p>Training Model</p>
<p>We began by collecting 2 million molecules from PubChem.Subsequently, we harnessed the tools and models mentioned earlier to extract comprehensive knowledge regarding these molecules.This knowledge was then translated into natural language text using predefined templates and combined with the corresponding SMILES representations.To maximize the model's capabilities, we thoughtfully organized the data to encompass both One-to-Many and Many-to-One patterns.This approach ensures that the model learns the underlying distribution of specific inputs, promoting adaptability and preventing the mere memorization of fixed responses.</p>
<p>For instance, let's take molecule M, which possesses ten pieces of extracted knowledge.If we were to compile all ten pieces into a single text, denoted as T, the resulting molecule space associated with T would likely be highly restricted, potentially corresponding to just one specific molecule, let's say, molecule A. This would essentially create a One-to-One data pattern.To overcome this limitation, we adopt a strategy where, for each molecule, we selectively choose a subset of its knowledge to compose the text.The goal here is to craft this text in a way that it corresponds to as many molecules as possible.This method empowers the model to gain insights into the broader distribution of molecules linked to the provided text, rather than locking it into a specific, isolated instance.</p>
<p>Then the training of TSMMG involves two key steps: pre-training on a large natural language corpus and fine-tuning on text-molecule paired data that we have constructed.In the first stage, TSMMG undergoes pre-training on a large natural language corpus.This enables TSMMG to learn and understand natural language by capturing the statistical patterns and linguistic structures present in the data.The pre-training stage helps TSMMG acquire a general understanding of language and forms the initial foundation for subsequent training stages.The second stage involves a fine-tuning on the text-molecule paired data that contains descriptions of various properties as shown in Tabel 3.This fine-tuning stage focuses on teaching TSMMG the mapping between text descriptions and molecular sequences as well as the syntax of SMILES.By fine-tuning TSMMG on this specific dataset, it becomes proficient in generating molecules based on specific text-described-property such as functional groups, LogP, physicochemical properties, drug-like properties, and affinity scores to certain targets.</p>
<p>The architecture of TSMMG is the same as GPT [40].TSMMG follows the settings of GPT2small, which consists of 12 layers and has a total of 117 million parameters.We downloaded the weights of GPT2small from Huggingface model repository [47] to initialize TSMMG.This helps with cost and computational considerations by leveraging pre-trained weights for an efficient starting point.And since the weights are trained by a large number of language corpus, we can directly fine-tune the model using the text-molecule paired data we construct.We fine-tune TSMMG on 8 A100 40G GPUs for around 6 days.We use the subsequent hyperparameters: a batch size of 32, a learning rate set to 5e-4, a warmup steps of 100.We use AdamW [42] as the optimizer.</p>
<p>Metrics</p>
<p>To evaluate the performance of the TSMMG model, we employed four common metrics in molecular generation: Validity, Uniqueness, Novelty and Diversity.Each of these metrics was essential for a comprehensive evaluation: Validity assesses whether the generated molecules conform to the syntax rules of SMILES.We utilized RDKit [43] to parse the generated molecules, considering them valid if the parsing process was successful.Uniqueness measures the proportion of non-repetitive molecules among the generated set.It ensures that the model produces diverse molecules.Novelty signifies whether the generated molecules are previously unseen in the training dataset, preventing the model from regenerating known molecules.Diversity describes the structural differences between generated molecules, it is calculated as:
𝐷𝑖𝑣𝑒𝑟𝑠𝑖𝑡𝑦 = 1 − 2 𝑛(𝑛−1)
∑ (, ) ,</p>
<p>.</p>
<p>Where (, ) is calculated based on the Tanimoto distance with respect to the Morgan fingerprints of generated molecules X and Y.</p>
<p>In addition to these standard metrics, we introduced the concept of success rate (SR) to measure whether the generated molecules meet predefined conditions.We establish different criteria to define the success of generated molecules based on the specific task.These criteria are outlined as follows: FG: We leveraged IUPAC nomenclature to identify functional groups within the molecules.By parsing IUPAC names and matching them to generated SMILES-encoded molecules, we checked if the generated molecules contained the specified functional groups.</p>
<p>LogP: Using RDKit, we calculated the LogP values of the generated molecules and compared them to predefined values.The generation was considered successful if the LogP value fell within a margin of 1 from the specified value.</p>
<p>QED and SAs: For these tasks, we adopted criteria similar to prior work [10], considering QED as high if its value exceeded 0.6 and SAs as good if the score was less than 4.</p>
<p>DRD2 and GSK3:</p>
<p>We employed the models proposed by Jin et al. [34] to predict the affinity scores of the generated molecules.A molecule was considered successful if its corresponding affinity score exceeded 0.5 for either target.</p>
<p>BBB and HIA:</p>
<p>We used models developed by Cheng et al. [32] to predict scores, determining if a molecule could pass through the blood-brain barrier (BBB) if its BBB score exceeded 0.5 or if it could be absorbed by the human small intestine (HIA) if its HIA score was above 0.5.</p>
<p>Moreover, for each multi-constraint task, we only considered molecules successful if they met all constraints contained in this task simultaneously.We generated 5000 molecules to evaluate the model's performance for each multi-constraint task.Note that we uniformly express all metric results in percentage format.While converting Diversity to a percentage may lack intrinsic meaning, for the sake of ease of comparison with other metrics, we multiply it by 100.However, we refrain from appending the '%' symbol to distinguish it from other metrics.</p>
<p>Translating SMILES to IUPAC</p>
<p>There are several open works that provided their solutions for translating SMILES to IUPAC name, such as STOUT [44] and IUPAC2Struct [45], but the interfaces they released are not for high throughput experiments.Considering experimental efficiency, we trained our own SMILES2IUPAC model based on GPT2small.We formulate this problem also as conditional probability (|) where generating a corresponding IUPAC name  = { 1 ,  2 , … ,   } by a given SMILES sequence  = { 1 ,  2 , … ,   }.We collect 2 million SMILES-IUPAC paired data from Pubchem to train this model.</p>
<p>The model size and settings of SMILES2IUPAC are the same as TSMMG.For evaluating the trained SMILES2IUPAC model, we pass 1000 unseen molecules to it and generate 1000 corresponding predicted IUPAC names.We then break down the predicted IUPAC names to identify the functional groups, and check if all these functional groups exist in the corresponding ground truth IUPAC names.The experimental results show an accuracy rate of 94%.</p>
<p>Figure 2 2
2
Figure 2 (a) Experimental results for TSMMG across various tasks, encompassing 8 two constraint tasks, 4 three constraint tasks, and 4 four-constraint tasks.(b) Average experimental results on two-constraint, three-constraint, and four constraint tasks.(c) FG analysis for task FG+DRD2.(d) Shows the comparison of the success rate SR (nFG) without considering whether FG matches and the success rate considering whether FG matches under different constraint tasks.(e) Shows the impact of different temperatures on the model.</p>
<p>Figure</p>
<p>Figure (a) Docking reference for E 2 and E 4. (b) Molecules generated by TSMMG that can simultaneously bind to both E 2 and E 4 receptors.The input prompt is: "The molecule exhibits a high QED score, good synthetic accessibility.It can pass through the blood-brain barrier and binds to both rostanoid E 2 and E 4 receptors."During training, TSMMG has seen molecules that can individually bind to both E 2 and E 4 receptors, but it has not explicitly received molecules that simultaneously satisfy all the constraints in this prompt.Nevertheless, it still successfully generates the desired molecules.</p>
<p>contains [FG].It is active to DRD2.[D2D2/GSK3].T1 I want a molecule that contains [FG] and can bind to [D2D2/GSK3].T2 Give me a molecule which contains [FG] and can bind to [D2D2/GSK3].BBB T0 The molecule contains [FG].It can pass through the blood-brain barrier.T1 I want a molecule that contains [FG] and can pass through the blood-brain barrier.T2 Give me a molecule which contains [FG] and can pass through the blood-brain barrier.HIA T0 The molecule contains [FG].It can be absorbed by human intestinal.T1 I want a molecule that contains [FG] and can be absorbed by human intestinal.T2 Give me a molecule which contains [FG] and can be absorbed by human intestinal.</p>
<p>Figure</p>
<p>Figure TSMMG as a producer.Molecules generated by TSMMG can be used to improve the accuracy of the predictor.(a) We leverage a large number of property predictors, which can be regarded as teacher models, to obtain molecular properties, and then use these properties to construct textual descriptions to train TSMMG.The molecules generated by TSMMG can also be used to update the corresponding property predictors.This has two benefits: firstly, it allows us to verify whether TSMMG has effectively extracted the latent representation of the property, and secondly, it can improve the accuracy of the property predictors.(b) The experimental results on four property predictors are shown in the figure.The horizontal axis represents the number of generated molecules added to the training data, which we refer to as pseudosamples.As can be observed, the accuracy of the property predictors increases and tends to converge as the number of pseudo-samples increases.</p>
<p>Table 1
1
The prompts we use in this work.[FG], [FG1] and [FG2] refer to any functional group, and [VALUE] refers to a real number.
taskprompt2FGThe molecule contains [FG1],[FG2].FG+LogPThe molecule contains [FG]. Its LogP is [VALUE].FG+QEDThe molecule contains [FG]. It has a high qed score.FG+SAThe molecule contains [FG]. It has good synthetic accessibility.FG+DRD2The molecule contains [FG]. It is active to DRD2.FG+GSK3The molecule contains [FG]. It is active to GSK3.FG+BBBThe molecule contains [FG]. It can pass through the blood-brain barrier.FG+HIAThe molecule contains [FG]. It can be absorbed by human intestinal.FG+DRD2+QEDThe molecule contains [FG]. It is active to DRD2. It has a high qed score.FG+GSK3+QEDThe molecule contains [FG]. It is active to GSK3. It has a high qed score.FG+BBB+QEDThe molecule contains [FG]. It can pass through the blood-brain barrier. It has a high qed score.
FG+HIA+QEDThe molecule contains[FG].It can be absorbed by human intestinal.It has a high qed score.FG+DRD2+QED+SAs The molecule contains [FG].It is active to DRD2.It has a high qed score.It has good synthetic accessibility.FG+GSK3+QED+SAs The molecule contains [FG].It is active to GSK3.It has a high qed score.It has good synthetic accessibility.FG+BBB+QED+SAs The molecule contains [FG].It can pass through the blood-brain barrier.It has a high qed score.It has good synthetic accessibility.FG+HIA+QED+SAs The molecule contains [FG].It can be absorbed by human intestinal.It has a high qed score.It has good synthetic accessibility.</p>
<p>group's FG in all training set is 1339, and the ratio of VUNS molecules generated by these FG-associated prompts is as high as 91%.
-(3), aconsiderable portion of functional groups (FGs) did not appear in the training set related to DRD2.Despite this, our model still demonstrates the capability to generate correct molecules.
Further, we consider the ratio of molecules that simultaneously satisfy Valid, Unique, Novelty, and Success (VUNS) criteria generated by different groups.As shown in Figure2-c-(4), combined with Figure 2-c-(2) and Figure 2-c-(3), as the frequency of FGs in the training set increases, the model is more capable of generating more novel molecules that meet the constraints.In group 5, the average frequency of this</p>
<p>Table 2
2
rompts we used in order to test the tolerance of TSMMG to diverse inputs,[FG]refers to any functional group.</p>
<p>Table</p>
<p>Experimental results with different template styles.
templatetaskvaliduniquenoveldiversitySRSR (nFG)FG+DRD299.80%68.48%92.54%85.77%78.04%93.18%T0FG+GSK3 FG+BBB99.92% 99.82%69.79% 95.53%92.88% 94.30%89.23% 92.05%79.44% 79.24%94.40% 96.14%FG+HIA99.98%96.16%92.64%91.54%79.94%95.80%FG+DRD299.70%71.84%80.42%86.26%44.68%82.12%T1FG+GSK3 FG+BBB99.68% 99.68%73.49% 94.30%83.90% 95.48%89.56% 92.28%47.36% 70.64%84.88% 96.84%FG+HIA99.78%94.11%94.82%91.99%69.34%93.86%FG+DRD299.80%70.06%81.32%86.18%48.04%83.52%T2FG+GSK3 FG+BBB99.68% 99.74%71.89% 95.67%84.68% 95.40%89.52% 92.17%50.22% 70.72%85.60% 96.52%FG+HIA99.90%95.24%95.48%91.86%71.10%94.12%
AcknowledgmentsWe would like to thank the National Natural Science Foundation of China for their support of this work (No. 62425204, 62122025, U22A2037, 62450002, 62432011).We also appreciate the contributions of all authors involved in this study, whose collaboration made this research possible.Availability of data and materialsSource data and codes are provided with this paper.The datasets used for training and evaluating TSMMG are available at: https://github.com/HHW-zhou/TSMMG.FundingThis work was partly supported by the National Natural Science Foundation of China (62425204, 62122025, U22A2037, 62450002, 62432011).DeclarationsEthics approval and consent to participateThis study involves computational experiments that are non-invasive and do not directly intervene with any human or animal subjects.Therefore, ethical approval from an institutional review board is not required.The consent of participants and the protection of personal information are not applicable, as the experimental data are sourced from publicly available datasets.Consent for publicationAll authors have provided their consent for publication of this study.There are no identifiable individuals or personal data included in this manuscript, ensuring compliance with publication ethics.Competing interestsThe other authors have declared no competing interests.Author contributionsL.W., YS.L., and X.Z.conceived the study of TSMMG and the experimental assays.P.Z.developed TSMMG.P.Z., J.W., C.L., and Z.W. performed all experiments.X.Z., YS.L., L.W., and P.Z.drafted the manuscript and charts.Y.L., C.L., S.S, J.L., L.W., X.C., H.L., and W.L. critically revised the manuscript.All authors critically revised and gave final approval of the manuscript.
Generative models for automatic chemical design. Daniel Schwalbe-Koda, Rafael Gómez-Bombarelli, Machine Learning Meets Quantum Physics. 2020</p>
<p>Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Pablo Gainza, Nature Methods. 172020</p>
<p>Development of a protein-ligand extended connectivity (PLEC) fingerprint and its application for binding affinity predictions. Maciej Wójcikowski, Bioinformatics. 352019</p>
<p>Elucidating the multiple roles of hydration for accurate proteinligand binding prediction via deep learning. Amr H Mahmoud, Communications Chemistry. 3192020</p>
<p>Improved protein-ligand binding affinity prediction with structure-based deep fusion inference. Derek Jones, Journal of chemical information and modeling. 612021</p>
<p>MoFlow: an invertible flow model for generating molecular graphs. Chengxi Zang, Fei Wang, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>MolGrow: A graph normalizing flow for hierarchical molecular generation. Maksim Kuznetsov, Daniil Polykovskiy, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Alex Zhavoronkov, Nature biotechnology. 372019</p>
<p>Learning to navigate the synthetically accessible chemical space using reinforcement learning. Sai Gottipati, Krishna, International Conference on Machine Learning. PMLR2020</p>
<p>Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Jike Wang, Nature Machine Intelligence. 32021</p>
<p>Mars: Markov molecular sampling for multi-objective drug discovery. Yutong Xie, arXiv:2103.104322021arXiv preprint</p>
<p>Multi-Objective De Novo Drug Design with Conditional Graph Generative Model. Yibo Li, abs/1801.07299.1Journal of Cheminformatics. 2018</p>
<p>Multi-objective molecule generation using interpretable substructures. Wengong Jin, Regina Barzilay, Tommi Jaakkola, International conference on machine learning. PMLR2020</p>
<p>MolGPT: molecular generation using a transformer-decoder model. Bagal, Viraj, Journal of Chemical Information and Modeling. 622021</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Ligand recognition, unconventional activation, and G protein coupling of the prostaglandin E2 receptor EP2 subtype. Changxiu Qu, Science Advances. 712682021</p>
<p>Ligand binding to human prostaglandin E receptor EP4 at the lipidbilayer interface. Yosuke Toyoda, Nature chemical biology. 152019</p>
<p>UCSF Chimera -A visualization system for exploratory research and analysis. Eric F Pettersen, 1605.0-1612.0Journal of computational chemistry. 2502004</p>
<p>Dock 6: Impact Of New Features And Current Docking Performance. William J Allen, Journal of computational chemistry. 362015</p>
<p>Plip 2021: Expanding The Scope Of The Protein-Ligand Interaction Profiler To Dna And Rna. Melissa F Adasme, Nucleic Acids Research. 492021</p>
<p>. L Schrödinger, November, 2015</p>
<p>. OpenAI: Introducing chatgpt. 2023</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>Translation between molecules and natural language. Carl Edwards, arXiv:2204.118172022arXiv preprint</p>
<p>MolXPT: Wrapping Molecules with Text for Generative Pretraining. Zequn Liu, arXiv:2305.106882023arXiv preprint</p>
<p>Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. Shengchao Liu, arXiv:2212.107892022arXiv preprint</p>
<p>Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Panagiotis-Christos Kotsias, Nature Machine Intelligence. 22020</p>
<p>ChEBI: a database and ontology for chemical entities of biological interest. Kirill Degtyarenko, Nucleic acids research. 36suppl_12007</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. P Seidl, A Vall, S Hochreiter, arXiv:2303.033632023arXiv preprint</p>
<p>Unifying molecular and textual representations via multi-task language modelling. D Christofidellis, G Giannone, J Born, arXiv:2301.125862023arXiv preprint</p>
<p>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective. Jiatong Li, CoRR abs/2306.066152023</p>
<p>admetSAR: a comprehensive source and free tool for assessment of chemical ADMET properties. Feixiong Cheng, 2012</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrations2020</p>
<p>Molecular de-novo design through deep reinforcement learning. Marcus Olivecrona, Journal of cheminformatics. 92017</p>
<p>Composing molecules with multiple property constraints. Wengong Jin, Regina Barzilay, Tommi Jaakkola, arXiv:2002.032442020arXiv preprint</p>
<p>Moltrans: Molecular Interaction Transformer For Drug-Target Interaction Prediction. Huang Kexin, Intelligent Systems in Molecular Biology. 372021</p>
<p>Interpretable bilinear attention network with domain adaptation improves drug-target prediction. Peizhen Bai, machine intelligence abs/2208.02194.2Nature. 2023</p>
<p>Transformercpi: Improving Compound-Protein Interaction Prediction By Sequence-Based Deep Learning With Self-Attention Mechanism And Label Reversal Experiments. Lifan Chen, Bioinformatics. 362020</p>
<p>PubChem 2023 update. Sunghwan Kim, Nucleic Acids Research. 512023</p>
<p>Zinc20-A Free Ultralarge-Scale Chemical Database For Ligand Discovery. John J Irwin, Journal of Chemical Information and Modeling. 602020</p>
<p>Language models are unsupervised multitask learners. Alec Radford, OpenAI blog. 192019</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, arXiv:1711.051012017arXiv preprint</p>
<p>Rdkit documentation. Greg Landrum, 201314</p>
<p>STOUT: SMILES to IUPAC names using neural machine translation. Kohulan Rajan, Achim Zielesny, Christoph Steinbeck, Journal of Cheminformatics. 132021</p>
<p>Transformer-based artificial neural networks for the conversion between chemical notations. Lev Krasnov, Scientific Reports. 112021</p>
<p>Knowledge distillation: A survey. Jianping Gou, International Journal of Computer Vision. 1292021</p>            </div>
        </div>

    </div>
</body>
</html>
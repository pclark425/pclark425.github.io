<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1129 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1129</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1129</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-253265220</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/1910.09143v5.pdf" target="_blank">Dynamic Subgoal-based Exploration via Bayesian Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the number of replications per trial -- in order to overcome the challenges of sparse rewards, expensive interactions, and noise. An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains. We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1129.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1129.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BESD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Exploratory Subgoal Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost-aware Bayesian optimization framework that meta-optimizes dynamic subgoal-based exploration strategies (subgoal locations, episode length, and number of replications) for a distribution of sparse-reward navigation MDPs; uses a GP surrogate over (θ,τ) and a knowledge-gradient-per-cost acquisition to allocate expensive environment interactions efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BESD (Bayesian Exploratory Subgoal Design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A meta-level optimizer that recommends a subgoal-augmented exploration strategy θ which is then used by an underlying RL algorithm (QL in experiments) to learn policies in sampled training environments. Core components: Gaussian process surrogate f(θ,τ) with product kernel (Matérn 5/2 over θ × polynomial/time kernel over τ), noise model separating environment variance and replication variance, and a knowledge-gradient-inspired acquisition function that maximizes expected improvement in the final recommendation per unit training cost (ν_n/(q·τ)). Uses potential-based reward shaping to implement subgoals as intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (knowledge-gradient per cost; GP surrogate over design × training time; adaptive selection of episode length and replications)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each training iteration BESD selects (θ, τ, q) that maximizes the expected gain in final-test performance (one-step lookahead knowledge gradient ν_n) normalized by the evaluation cost q·τ. It uses past noisy observations y_n(θ,τ,q) of u(θ,τ) across different sampled environments, updates the GP posterior µ_n(θ,τ),k_n, and trades off running longer episodes vs. more replications based on predicted information gain per interaction. It models learning curves by including τ as an input dimension, allowing early/short evaluations to inform about final τ_max performance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple benchmark distributions: GW10, GW20, TR (Treasure-in-Room), MC (Mountain Car), KEY2, KEY3</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown environments drawn i.i.d. from a distribution Ξ; domains are episodic MDPs with sparse/delayed extrinsic rewards, stochastic transitions (small wind noise in gridworlds), partially unknown task-specific object locations (treasure/key), and (in MC) continuous state dynamics. Not formulated as adversarial or explicitly partially observable beyond the agent not knowing the particular MDP realization prior to test.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by domain: GW10: 10×10 discrete grid state space, 4 actions, walls with randomized door positions, wind noise up to 0.02; GW20: 20×20 grid, 4 actions, two walls and doors of size 8, stochastic wind; TR: 10×10 with a small treasure room, reward 10 for treasure/goal, γ=0.98; MC: continuous 2D state (position, velocity), action discrete or small continuous (classic mountain car), randomized start in [-0.6,-0.4]; KEY2/KEY3: 10×10 grid with one wall and highly varying key locations (two or three subgoals parameterized). Episode-length/τ choices and q sets differ per domain (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Across all evaluated domains BESD achieved the best empirical results (lowest regret or highest reward) versus baselines; experimental plots averaged over 50 replications show consistently lower log-regret or higher discounted reward as a function of cumulative training cost. Reported test-time speedups (performance ratio = performance with BESD subgoals / performance without subgoals) range from ~3× (worst cases: MC, KEY2, KEY3) up to nearly 20× (best cases: GW10, GW20, TR) depending on domain and test-time budget; BESD also achieves lower regret for the same total number of environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines without adaptive subgoal tuning (plain Q-learning and transfer Q-learning) perform poorly across domains: high regret / low reward; random subgoal selection (RND) also performs poorly. Exact baseline numeric curves are provided in paper figures; relative improvements summarized above (~3× to ~20×) compare BESD-enabled policies vs learning-from-scratch/no-subgoal baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>BESD is designed for sample efficiency: it jointly optimizes selection of θ, τ and q to reduce total environment interactions. In experiments, BESD uses small discrete sets T and Q (domain-dependent: e.g., GW10 T={200,600,1000}, Q={5,20}; GW20 T={4000,7000,10000}, q=20 fixed; TR T={400,1200,2000}, Q={5,20}; MC T={4000,7000,10000}, Q={10,50}; KEY2/3 T={400,700,1000}, Q={5,20}) and starts with 10 initial observations per τ. Empirically BESD achieves superior policies with fewer total interactions than EI/LCB baselines which always used τ_max and q_max. Precise counts: paper reports cumulative-cost vs performance plots (x-axis total interactions); BESD reaches lower regret earlier than baselines in all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff handled via the acquisition ν_n(θ,τ,q) which is the expected increase in the posterior-max (µ_n*) at τ_max after sampling (θ,τ,q); BESD selects the (θ,τ,q) maximizing ν_n/(q·τ). This yields adaptive exploration (trying uncertain θ or short τ runs to cheaply gather information) vs exploitation (evaluating promising θ more fully) based on predicted information gain per unit cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to: Hyperband (HB), Expected Improvement (EI) BO, Lower Confidence Bound (LCB) BO, Random subgoals (RND), Q-learning without subgoals (QL), Transfer Q-learning (TQL). Also ablations/variants discussed in related work (e.g., freeze-thaw BO, multi-fidelity BO ideas).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BESD consistently outperforms baselines in cost-constrained training for sparse-reward navigation tasks by: (1) including τ in the GP surrogate to model learning curves and using τ and q as control levers, (2) employing a knowledge-gradient-per-cost acquisition ν/(q·τ) to trade off cost vs information, and (3) using potential-based subgoal reward shaping as a plug-and-play exploration mechanism. Theoretical guarantees: asymptotic optimality on discretized domain and bounded additive suboptimality relative to the continuous domain (probabilistic Lipschitz bound). Empirically BESD yields large test-time speedups (3x–20x) over learning from scratch and is more cost-efficient than EI and LCB which always evaluate at full cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported/acknowledged limitations include: requirement that θ be low-to-moderate dimensional for vanilla GP surrogate scaling (authors suggest scalable GP extensions for higher dimensions); initial sampling cost can consume significant budget (recommendation 2^d−1 initial points rule-of-thumb); if number of subgoals is free, BO may greedily pick the largest number (authors did not pursue BO over number of subgoals); early iterations can recommend suboptimal strategies before convergence (e.g., TR behavior 'B') though BESD often corrects this over time; assumptions in model include Gaussian observation noise and independence between environment noise and replication noise; experiments are on simulated benchmarks (not yet deployed on real robots) and computational cost of acquisition optimization grows with θ-dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1129.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperband</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperband (adaptive resource allocation / early-stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bandit-based adaptive configuration evaluation algorithm that allocates resources across many configurations using brackets of successive halving and early-stopping to quickly eliminate poor designs; used here as a baseline for adaptive resource allocation over subgoal designs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyperband: A novel bandit-based approach to hyperparameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hyperband (HB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An adaptive evaluation scheduler: samples many θ from a Latin hypercube, evaluates each for a small budget τ_min, keeps top fraction (1/η), and increases budget for survivors according to successive-halving; treats subgoal design tuning as a pure-exploration bandit problem with early-stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive resource allocation / successive halving (Hyperband)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Allocates short evaluations to many candidates, eliminates low-performing candidates early, and progressively allocates more budget to promising designs; does not model learning curves with a GP posterior but uses rank-based early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark distributions as BESD (GW10, GW20, TR, MC, KEY2, KEY3) used as baseline comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same sparse-reward navigation domains with stochastic transitions and randomized environment instances drawn from Ξ</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As per the corresponding domain (e.g., GW10 10×10 grid, etc.); Hyperband settings used R and η defaults (η=3,R=81 in paper's description)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Hyperband is competitive early in some easier domains (GW10, TR) due to fast early elimination, but over larger budgets BESD typically outperforms HB across most domains; HB performs well in early phases but often plateaus below BESD asymptotic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Good early sample efficiency due to evaluating many short runs and early stopping; however, lacks a model to trade off information vs cost and so is less efficient than BESD over longer budgets in most domains.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration via many cheap candidates; exploitation via successive halving keeping top fraction and allocating more resources only to survivors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to BESD, EI, LCB, RND, QL, TQL in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>HB shows strong early performance in easier domains due to aggressive early-stopping but is outperformed by BESD with larger budgets; demonstrates the value of adaptive resource allocation but also the benefit of a surrogate model that reasons about learning curves.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not explicitly model the learning curve or environment noise; can be suboptimal when cheap partial evaluations are not predictive of full-budget outcomes, and it lacks the one-step lookahead decision-theoretic perspective for cost-efficiency that BESD provides.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1129.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement (Bayesian optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard Bayesian optimization acquisition function that selects the next point maximizing expected improvement over the best observed value; used here as a baseline BO method but always evaluated at full cost (τ_max, q_max).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Expected Improvement (EI) BO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GP-based Bayesian optimization using the expected improvement acquisition; in experiments EI always evaluates candidates using full episode length τ_max (and typically q_max replications), lacking BESD's cost-aware short-evaluation option.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (Expected Improvement acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects θ maximizing expected improvement under current GP posterior but without adapting τ or q for cost; thus selects full-cost evaluations each round.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks (GW10, GW20, TR, MC, KEY2, KEY3) as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward navigation MDPs drawn from Ξ with stochastic transitions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As described per domain; EI runs used τ_max evaluations each iteration</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>EI achieves reasonable performance but is less cost-efficient than BESD because it always uses full-cost evaluations; in plots EI lags BESD in cumulative-cost vs performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient under training-cost accounting because it cannot reduce τ or q and thus spends more interactions per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Tradeoff via expected improvement (posterior mean and variance); no explicit cost-normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BESD, LCB, HB, RND, QL, TQL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates that BO with a GP surrogate but without cost-aware evaluation control is inferior in cost-constrained settings to BESD which can adapt τ and q.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Suboptimal in the cost-aware training regime due to always using τ_max and q_max; does not exploit informative short runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1129.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lower Confidence Bound (GP-UCB style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BO acquisition that trades exploration and exploitation via µ_n − κ·σ_n (LCB); used as an experimental baseline and configured to always evaluate at full cost in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LCB (Lower Confidence Bound) BO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GP-based BO that selects θ minimizing an LCB score (µ_n − κ·σ_n) at τ_max; in experiments κ=2 and evaluations occur at full budget (τ_max, q_max).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (LCB / GP-UCB style acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Balances exploration (via posterior variance) and exploitation (via posterior mean) through a fixed bonus term κ·σ_n; no adaptation of τ or q for cost-efficiency in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark distributions used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward, stochastic grid and continuous domains drawn from Ξ</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Domain-dependent (see BESD entry)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>LCB performs similarly to EI in many domains but typically worse than BESD under training-cost accounting because it uses full-cost evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Less sample-efficient in terms of environment interactions compared to BESD because of full-cost evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Controlled by κ (fixed to 2 in experiments); trades exploration via σ_n against exploitation via µ_n.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to BESD, EI, HB, RND, QL, TQL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LCB baseline highlights that GP-based acquisition alone is not sufficient for cost-aware training; ability to adapt evaluation cost (τ,q) is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not cost-aware in experiments; thus underperforms when interactions are expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1129.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random subgoal sampling (Latin hypercube)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Random baseline that samples subgoal designs via Latin hypercube sampling each iteration (no learning of which subgoals are good); used to show necessity of careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random subgoal selection (RND)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each iteration chooses subgoal parameters θ randomly (Latin hypercube) and evaluates them (with some τ and q scheme); no model or adaptive allocation of resources beyond random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>None (random search)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation beyond randomized sampling; does not use past observations to inform future choices.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark distributions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward navigation MDPs drawn from Ξ</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As domain-specific (GW10, GW20, TR, MC, KEY2, KEY3)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Per-paper: RND performs poorly across all domains, demonstrating that arbitrary subgoal choices do not yield cost-efficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very poor: requires many interactions to stumble on effective subgoal strategies, as shown by high regret in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>None; purely exploratory random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as a baseline against BESD, EI, LCB, HB, QL, TQL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Random subgoal selection is insufficient; careful, model-based optimization of subgoals and evaluation cost is necessary for cost-efficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Systematically outperformed by model-based methods in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1129.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q-learning (no subgoals)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline RL algorithm (Watkins & Dayan) with ϵ-greedy exploration (ϵ=0.2) applied directly in sparse-reward environments without subgoals; used to show learning-from-scratch performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q-learning (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard tabular Q-learning with ϵ-greedy behavioral policy (ϵ=0.2) used as the underlying RL algorithm in all experiments; here used both as a baseline (no subgoal shaping) and as the inner learner that BESD augments with subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>No (standard exploration ϵ-greedy)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Standard RL learning dynamics; exploration via fixed ϵ-greedy randomness; does not adapt experimental design across environments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks (GW10, GW20, TR, MC, KEY2, KEY3)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward MDPs where naive random exploration is often insufficient to find goals within budget</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As per each domain; QL is used with episode budgets τ and multiple replications when used in meta-optimization evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Per-paper: QL without subgoals exhibits high regret and low sample efficiency across domains and is outperformed by BESD-augmented exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Poor in sparse-reward settings; often requires prohibitively many interactions to discover goal states under random exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Fixed via ϵ-greedy parameter (ϵ=0.2 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared as baseline to BESD-enabled subgoal shaping; also used as inner RL-ALGO for BESD.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Demonstrates the utility of subgoal-based shaped intrinsic rewards: Q-learning augmented with BESD-recommended subgoals learns much faster than QL from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Ineffective alone in sparse reward domains without additional structured exploration guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1129.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1129.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Q-learning (heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic baseline that initializes Q-values at test time using stored results from a randomly chosen training environment (policy reuse idea), intended as a cheap transfer baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Transfer Q-learning (TQL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Heuristic that transfers Q-values learned in previous training environments to initialize the test-instance Q-table, then runs Q-learning; designed to capture a simple transfer baseline without structured subgoal design.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>No (policy reuse heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No active adaptive experimental design; reuses learned Q-values from a random prior training environment as initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmark distributions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Sparse-reward MDPs with inter-environment variability</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Domain-specific; depends on chosen stored Q-values and similarity between training and test environments</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Per-paper: TQL performs poorly compared to BESD and is similar to plain QL; simple transfer-by-randomly-initialized-Q is not sufficient for robust cross-environment sample-efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally poor in experiments; not a reliable substitute for cost-aware meta-optimization of exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Standard Q-learning dynamics after initialization; no meta-level tradeoff mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as a baseline against BESD, EI, LCB, HB, RND, QL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Simple transfer heuristics without cost-aware subgoal design do not provide the sample efficiency benefits of BESD.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance highly sensitive to similarity between the stored training environment and the test environment; often not beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dynamic Subgoal-based Exploration via Bayesian Optimization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Optimizing waypoints for monitoring spatiotemporal phenomena <em>(Rating: 2)</em></li>
                <li>Freeze-thaw Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Multi-information source optimization <em>(Rating: 2)</em></li>
                <li>Hyperband: A novel bandit-based approach to hyperparameter optimization <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1129",
    "paper_id": "paper-253265220",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "BESD",
            "name_full": "Bayesian Exploratory Subgoal Design",
            "brief_description": "A cost-aware Bayesian optimization framework that meta-optimizes dynamic subgoal-based exploration strategies (subgoal locations, episode length, and number of replications) for a distribution of sparse-reward navigation MDPs; uses a GP surrogate over (θ,τ) and a knowledge-gradient-per-cost acquisition to allocate expensive environment interactions efficiently.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BESD (Bayesian Exploratory Subgoal Design)",
            "agent_description": "A meta-level optimizer that recommends a subgoal-augmented exploration strategy θ which is then used by an underlying RL algorithm (QL in experiments) to learn policies in sampled training environments. Core components: Gaussian process surrogate f(θ,τ) with product kernel (Matérn 5/2 over θ × polynomial/time kernel over τ), noise model separating environment variance and replication variance, and a knowledge-gradient-inspired acquisition function that maximizes expected improvement in the final recommendation per unit training cost (ν_n/(q·τ)). Uses potential-based reward shaping to implement subgoals as intrinsic rewards.",
            "adaptive_design_method": "Bayesian optimization (knowledge-gradient per cost; GP surrogate over design × training time; adaptive selection of episode length and replications)",
            "adaptation_strategy_description": "At each training iteration BESD selects (θ, τ, q) that maximizes the expected gain in final-test performance (one-step lookahead knowledge gradient ν_n) normalized by the evaluation cost q·τ. It uses past noisy observations y_n(θ,τ,q) of u(θ,τ) across different sampled environments, updates the GP posterior µ_n(θ,τ),k_n, and trades off running longer episodes vs. more replications based on predicted information gain per interaction. It models learning curves by including τ as an input dimension, allowing early/short evaluations to inform about final τ_max performance.",
            "environment_name": "Multiple benchmark distributions: GW10, GW20, TR (Treasure-in-Room), MC (Mountain Car), KEY2, KEY3",
            "environment_characteristics": "Unknown environments drawn i.i.d. from a distribution Ξ; domains are episodic MDPs with sparse/delayed extrinsic rewards, stochastic transitions (small wind noise in gridworlds), partially unknown task-specific object locations (treasure/key), and (in MC) continuous state dynamics. Not formulated as adversarial or explicitly partially observable beyond the agent not knowing the particular MDP realization prior to test.",
            "environment_complexity": "Varies by domain: GW10: 10×10 discrete grid state space, 4 actions, walls with randomized door positions, wind noise up to 0.02; GW20: 20×20 grid, 4 actions, two walls and doors of size 8, stochastic wind; TR: 10×10 with a small treasure room, reward 10 for treasure/goal, γ=0.98; MC: continuous 2D state (position, velocity), action discrete or small continuous (classic mountain car), randomized start in [-0.6,-0.4]; KEY2/KEY3: 10×10 grid with one wall and highly varying key locations (two or three subgoals parameterized). Episode-length/τ choices and q sets differ per domain (reported in paper).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Across all evaluated domains BESD achieved the best empirical results (lowest regret or highest reward) versus baselines; experimental plots averaged over 50 replications show consistently lower log-regret or higher discounted reward as a function of cumulative training cost. Reported test-time speedups (performance ratio = performance with BESD subgoals / performance without subgoals) range from ~3× (worst cases: MC, KEY2, KEY3) up to nearly 20× (best cases: GW10, GW20, TR) depending on domain and test-time budget; BESD also achieves lower regret for the same total number of environment interactions.",
            "performance_without_adaptation": "Baselines without adaptive subgoal tuning (plain Q-learning and transfer Q-learning) perform poorly across domains: high regret / low reward; random subgoal selection (RND) also performs poorly. Exact baseline numeric curves are provided in paper figures; relative improvements summarized above (~3× to ~20×) compare BESD-enabled policies vs learning-from-scratch/no-subgoal baselines.",
            "sample_efficiency": "BESD is designed for sample efficiency: it jointly optimizes selection of θ, τ and q to reduce total environment interactions. In experiments, BESD uses small discrete sets T and Q (domain-dependent: e.g., GW10 T={200,600,1000}, Q={5,20}; GW20 T={4000,7000,10000}, q=20 fixed; TR T={400,1200,2000}, Q={5,20}; MC T={4000,7000,10000}, Q={10,50}; KEY2/3 T={400,700,1000}, Q={5,20}) and starts with 10 initial observations per τ. Empirically BESD achieves superior policies with fewer total interactions than EI/LCB baselines which always used τ_max and q_max. Precise counts: paper reports cumulative-cost vs performance plots (x-axis total interactions); BESD reaches lower regret earlier than baselines in all domains.",
            "exploration_exploitation_tradeoff": "Tradeoff handled via the acquisition ν_n(θ,τ,q) which is the expected increase in the posterior-max (µ_n*) at τ_max after sampling (θ,τ,q); BESD selects the (θ,τ,q) maximizing ν_n/(q·τ). This yields adaptive exploration (trying uncertain θ or short τ runs to cheaply gather information) vs exploitation (evaluating promising θ more fully) based on predicted information gain per unit cost.",
            "comparison_methods": "Compared experimentally to: Hyperband (HB), Expected Improvement (EI) BO, Lower Confidence Bound (LCB) BO, Random subgoals (RND), Q-learning without subgoals (QL), Transfer Q-learning (TQL). Also ablations/variants discussed in related work (e.g., freeze-thaw BO, multi-fidelity BO ideas).",
            "key_results": "BESD consistently outperforms baselines in cost-constrained training for sparse-reward navigation tasks by: (1) including τ in the GP surrogate to model learning curves and using τ and q as control levers, (2) employing a knowledge-gradient-per-cost acquisition ν/(q·τ) to trade off cost vs information, and (3) using potential-based subgoal reward shaping as a plug-and-play exploration mechanism. Theoretical guarantees: asymptotic optimality on discretized domain and bounded additive suboptimality relative to the continuous domain (probabilistic Lipschitz bound). Empirically BESD yields large test-time speedups (3x–20x) over learning from scratch and is more cost-efficient than EI and LCB which always evaluate at full cost.",
            "limitations_or_failures": "Reported/acknowledged limitations include: requirement that θ be low-to-moderate dimensional for vanilla GP surrogate scaling (authors suggest scalable GP extensions for higher dimensions); initial sampling cost can consume significant budget (recommendation 2^d−1 initial points rule-of-thumb); if number of subgoals is free, BO may greedily pick the largest number (authors did not pursue BO over number of subgoals); early iterations can recommend suboptimal strategies before convergence (e.g., TR behavior 'B') though BESD often corrects this over time; assumptions in model include Gaussian observation noise and independence between environment noise and replication noise; experiments are on simulated benchmarks (not yet deployed on real robots) and computational cost of acquisition optimization grows with θ-dimension.",
            "uuid": "e1129.0",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Hyperband",
            "name_full": "Hyperband (adaptive resource allocation / early-stopping)",
            "brief_description": "A bandit-based adaptive configuration evaluation algorithm that allocates resources across many configurations using brackets of successive halving and early-stopping to quickly eliminate poor designs; used here as a baseline for adaptive resource allocation over subgoal designs.",
            "citation_title": "Hyperband: A novel bandit-based approach to hyperparameter optimization",
            "mention_or_use": "use",
            "agent_name": "Hyperband (HB)",
            "agent_description": "An adaptive evaluation scheduler: samples many θ from a Latin hypercube, evaluates each for a small budget τ_min, keeps top fraction (1/η), and increases budget for survivors according to successive-halving; treats subgoal design tuning as a pure-exploration bandit problem with early-stopping.",
            "adaptive_design_method": "Adaptive resource allocation / successive halving (Hyperband)",
            "adaptation_strategy_description": "Allocates short evaluations to many candidates, eliminates low-performing candidates early, and progressively allocates more budget to promising designs; does not model learning curves with a GP posterior but uses rank-based early stopping.",
            "environment_name": "Same benchmark distributions as BESD (GW10, GW20, TR, MC, KEY2, KEY3) used as baseline comparisons",
            "environment_characteristics": "Same sparse-reward navigation domains with stochastic transitions and randomized environment instances drawn from Ξ",
            "environment_complexity": "As per the corresponding domain (e.g., GW10 10×10 grid, etc.); Hyperband settings used R and η defaults (η=3,R=81 in paper's description)",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Hyperband is competitive early in some easier domains (GW10, TR) due to fast early elimination, but over larger budgets BESD typically outperforms HB across most domains; HB performs well in early phases but often plateaus below BESD asymptotic performance.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Good early sample efficiency due to evaluating many short runs and early stopping; however, lacks a model to trade off information vs cost and so is less efficient than BESD over longer budgets in most domains.",
            "exploration_exploitation_tradeoff": "Exploration via many cheap candidates; exploitation via successive halving keeping top fraction and allocating more resources only to survivors.",
            "comparison_methods": "Compared directly to BESD, EI, LCB, RND, QL, TQL in experiments.",
            "key_results": "HB shows strong early performance in easier domains due to aggressive early-stopping but is outperformed by BESD with larger budgets; demonstrates the value of adaptive resource allocation but also the benefit of a surrogate model that reasons about learning curves.",
            "limitations_or_failures": "Does not explicitly model the learning curve or environment noise; can be suboptimal when cheap partial evaluations are not predictive of full-budget outcomes, and it lacks the one-step lookahead decision-theoretic perspective for cost-efficiency that BESD provides.",
            "uuid": "e1129.1",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "EI",
            "name_full": "Expected Improvement (Bayesian optimization)",
            "brief_description": "A standard Bayesian optimization acquisition function that selects the next point maximizing expected improvement over the best observed value; used here as a baseline BO method but always evaluated at full cost (τ_max, q_max).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Expected Improvement (EI) BO baseline",
            "agent_description": "GP-based Bayesian optimization using the expected improvement acquisition; in experiments EI always evaluates candidates using full episode length τ_max (and typically q_max replications), lacking BESD's cost-aware short-evaluation option.",
            "adaptive_design_method": "Bayesian optimization (Expected Improvement acquisition)",
            "adaptation_strategy_description": "Selects θ maximizing expected improvement under current GP posterior but without adapting τ or q for cost; thus selects full-cost evaluations each round.",
            "environment_name": "Same benchmarks (GW10, GW20, TR, MC, KEY2, KEY3) as baseline",
            "environment_characteristics": "Sparse-reward navigation MDPs drawn from Ξ with stochastic transitions",
            "environment_complexity": "As described per domain; EI runs used τ_max evaluations each iteration",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "EI achieves reasonable performance but is less cost-efficient than BESD because it always uses full-cost evaluations; in plots EI lags BESD in cumulative-cost vs performance.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient under training-cost accounting because it cannot reduce τ or q and thus spends more interactions per evaluation.",
            "exploration_exploitation_tradeoff": "Tradeoff via expected improvement (posterior mean and variance); no explicit cost-normalization.",
            "comparison_methods": "Compared to BESD, LCB, HB, RND, QL, TQL.",
            "key_results": "Demonstrates that BO with a GP surrogate but without cost-aware evaluation control is inferior in cost-constrained settings to BESD which can adapt τ and q.",
            "limitations_or_failures": "Suboptimal in the cost-aware training regime due to always using τ_max and q_max; does not exploit informative short runs.",
            "uuid": "e1129.2",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "LCB",
            "name_full": "Lower Confidence Bound (GP-UCB style)",
            "brief_description": "A BO acquisition that trades exploration and exploitation via µ_n − κ·σ_n (LCB); used as an experimental baseline and configured to always evaluate at full cost in this paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LCB (Lower Confidence Bound) BO baseline",
            "agent_description": "GP-based BO that selects θ minimizing an LCB score (µ_n − κ·σ_n) at τ_max; in experiments κ=2 and evaluations occur at full budget (τ_max, q_max).",
            "adaptive_design_method": "Bayesian optimization (LCB / GP-UCB style acquisition)",
            "adaptation_strategy_description": "Balances exploration (via posterior variance) and exploitation (via posterior mean) through a fixed bonus term κ·σ_n; no adaptation of τ or q for cost-efficiency in experiments.",
            "environment_name": "Same benchmark distributions used in experiments",
            "environment_characteristics": "Sparse-reward, stochastic grid and continuous domains drawn from Ξ",
            "environment_complexity": "Domain-dependent (see BESD entry)",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "LCB performs similarly to EI in many domains but typically worse than BESD under training-cost accounting because it uses full-cost evaluations.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Less sample-efficient in terms of environment interactions compared to BESD because of full-cost evaluations.",
            "exploration_exploitation_tradeoff": "Controlled by κ (fixed to 2 in experiments); trades exploration via σ_n against exploitation via µ_n.",
            "comparison_methods": "Compared to BESD, EI, HB, RND, QL, TQL.",
            "key_results": "LCB baseline highlights that GP-based acquisition alone is not sufficient for cost-aware training; ability to adapt evaluation cost (τ,q) is critical.",
            "limitations_or_failures": "Not cost-aware in experiments; thus underperforms when interactions are expensive.",
            "uuid": "e1129.3",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "RND",
            "name_full": "Random subgoal sampling (Latin hypercube)",
            "brief_description": "Random baseline that samples subgoal designs via Latin hypercube sampling each iteration (no learning of which subgoals are good); used to show necessity of careful tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Random subgoal selection (RND)",
            "agent_description": "At each iteration chooses subgoal parameters θ randomly (Latin hypercube) and evaluates them (with some τ and q scheme); no model or adaptive allocation of resources beyond random sampling.",
            "adaptive_design_method": "None (random search)",
            "adaptation_strategy_description": "No adaptation beyond randomized sampling; does not use past observations to inform future choices.",
            "environment_name": "Same benchmark distributions",
            "environment_characteristics": "Sparse-reward navigation MDPs drawn from Ξ",
            "environment_complexity": "As domain-specific (GW10, GW20, TR, MC, KEY2, KEY3)",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Per-paper: RND performs poorly across all domains, demonstrating that arbitrary subgoal choices do not yield cost-efficient exploration.",
            "sample_efficiency": "Very poor: requires many interactions to stumble on effective subgoal strategies, as shown by high regret in experiments.",
            "exploration_exploitation_tradeoff": "None; purely exploratory random sampling.",
            "comparison_methods": "Used as a baseline against BESD, EI, LCB, HB, QL, TQL.",
            "key_results": "Random subgoal selection is insufficient; careful, model-based optimization of subgoals and evaluation cost is necessary for cost-efficient exploration.",
            "limitations_or_failures": "Systematically outperformed by model-based methods in experiments.",
            "uuid": "e1129.4",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "QL",
            "name_full": "Q-learning (no subgoals)",
            "brief_description": "Baseline RL algorithm (Watkins & Dayan) with ϵ-greedy exploration (ϵ=0.2) applied directly in sparse-reward environments without subgoals; used to show learning-from-scratch performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q-learning (baseline)",
            "agent_description": "Standard tabular Q-learning with ϵ-greedy behavioral policy (ϵ=0.2) used as the underlying RL algorithm in all experiments; here used both as a baseline (no subgoal shaping) and as the inner learner that BESD augments with subgoals.",
            "adaptive_design_method": "No (standard exploration ϵ-greedy)",
            "adaptation_strategy_description": "Standard RL learning dynamics; exploration via fixed ϵ-greedy randomness; does not adapt experimental design across environments.",
            "environment_name": "Same benchmarks (GW10, GW20, TR, MC, KEY2, KEY3)",
            "environment_characteristics": "Sparse-reward MDPs where naive random exploration is often insufficient to find goals within budget",
            "environment_complexity": "As per each domain; QL is used with episode budgets τ and multiple replications when used in meta-optimization evaluations",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Per-paper: QL without subgoals exhibits high regret and low sample efficiency across domains and is outperformed by BESD-augmented exploration.",
            "sample_efficiency": "Poor in sparse-reward settings; often requires prohibitively many interactions to discover goal states under random exploration.",
            "exploration_exploitation_tradeoff": "Fixed via ϵ-greedy parameter (ϵ=0.2 in experiments).",
            "comparison_methods": "Compared as baseline to BESD-enabled subgoal shaping; also used as inner RL-ALGO for BESD.",
            "key_results": "Demonstrates the utility of subgoal-based shaped intrinsic rewards: Q-learning augmented with BESD-recommended subgoals learns much faster than QL from scratch.",
            "limitations_or_failures": "Ineffective alone in sparse reward domains without additional structured exploration guidance.",
            "uuid": "e1129.5",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "TQL",
            "name_full": "Transfer Q-learning (heuristic)",
            "brief_description": "A heuristic baseline that initializes Q-values at test time using stored results from a randomly chosen training environment (policy reuse idea), intended as a cheap transfer baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Transfer Q-learning (TQL)",
            "agent_description": "Heuristic that transfers Q-values learned in previous training environments to initialize the test-instance Q-table, then runs Q-learning; designed to capture a simple transfer baseline without structured subgoal design.",
            "adaptive_design_method": "No (policy reuse heuristic)",
            "adaptation_strategy_description": "No active adaptive experimental design; reuses learned Q-values from a random prior training environment as initialization.",
            "environment_name": "Same benchmark distributions",
            "environment_characteristics": "Sparse-reward MDPs with inter-environment variability",
            "environment_complexity": "Domain-specific; depends on chosen stored Q-values and similarity between training and test environments",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Per-paper: TQL performs poorly compared to BESD and is similar to plain QL; simple transfer-by-randomly-initialized-Q is not sufficient for robust cross-environment sample-efficiency.",
            "sample_efficiency": "Generally poor in experiments; not a reliable substitute for cost-aware meta-optimization of exploration strategies.",
            "exploration_exploitation_tradeoff": "Standard Q-learning dynamics after initialization; no meta-level tradeoff mechanism.",
            "comparison_methods": "Used as a baseline against BESD, EI, LCB, HB, RND, QL.",
            "key_results": "Simple transfer heuristics without cost-aware subgoal design do not provide the sample efficiency benefits of BESD.",
            "limitations_or_failures": "Performance highly sensitive to similarity between the stored training environment and the test environment; often not beneficial.",
            "uuid": "e1129.6",
            "source_info": {
                "paper_title": "Dynamic Subgoal-based Exploration via Bayesian Optimization",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Optimizing waypoints for monitoring spatiotemporal phenomena",
            "rating": 2,
            "sanitized_title": "optimizing_waypoints_for_monitoring_spatiotemporal_phenomena"
        },
        {
            "paper_title": "Freeze-thaw Bayesian optimization",
            "rating": 2,
            "sanitized_title": "freezethaw_bayesian_optimization"
        },
        {
            "paper_title": "Multi-information source optimization",
            "rating": 2,
            "sanitized_title": "multiinformation_source_optimization"
        },
        {
            "paper_title": "Hyperband: A novel bandit-based approach to hyperparameter optimization",
            "rating": 2,
            "sanitized_title": "hyperband_a_novel_banditbased_approach_to_hyperparameter_optimization"
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 1,
            "sanitized_title": "a_tutorial_on_bayesian_optimization_of_expensive_cost_functions_with_application_to_active_user_modeling_and_hierarchical_reinforcement_learning"
        }
    ],
    "cost": 0.0194545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dynamic Subgoal-based Exploration via Bayesian Optimization
12 Oct 2023</p>
<p>Yijia Wang 
Matthias Poloczek matthias.poloczek@gmx.de 
Daniel R Jiang drjiang@meta.com </p>
<p>University of Pittsburgh</p>
<p>Meta AI
University of Pittsburgh</p>
<p>Dynamic Subgoal-based Exploration via Bayesian Optimization
12 Oct 2023C212D0D71010270A304952D4B7265A8DarXiv:1910.09143v5[math.OC]https:openreview. netforum? id= ThJl4d5JRg
Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration.Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy.It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution.Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration.We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies.The algorithm adjusts a variety of levers -the locations of the subgoals, the length of each episode, and the number of replications per trial -in order to overcome the challenges of sparse rewards, expensive interactions, and noise.An experimental evaluation demonstrates that the new approach outperforms existing baselines across a number of problem domains.We also provide a theoretical foundation and prove that the method asymptotically identifies a near-optimal subgoal design.* The work was done before Matthias joined Amazon.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) is becoming the standard for approaching control problems -usually modeled by a Markov decision process (MDP) -in environments whose dynamics are unknown and learned from data.In many applications involving navigation tasks, rewards are sparse and delayed.Since most RL algorithms rely, at least initially, on random exploration, this can cause an agent to require a large, often impractical number of interactions with the environment before obtaining any rewards.Simultaneously, in real-world settings, it is often the case that fast and cheap interactions with the environment are not available, making it nearly impossible to apply RL algorithms.To address the two issues of sparse rewards and expensive interactions in navigation tasks, our objective in this paper is to design methods for learning better exploration policies in a cost-efficient manner: specifically, we propose a Bayesian optimization approach to optimize an exploration strategy based on subgoals, where each subgoal is defined as a set of states that the agent must reach, serving as an intermediate target for the agent to "complete" before navigating to the primary goal.</p>
<p>An illustrative example comes from the field of robotics: autonomous systems have long been used to explore unknown or dangerous terrains (Matthies et al., 1995;Apostolopoulos et al., 2001;Ferguson et al., 2004;Thrun et al., 2004).Policies learned offline (e.g., via a simulator) are common in these situations, but it may be beneficial to introduce agents that execute an offline-learned exploration policy to guide the learning of an online policy that can better tailor to the details of the test environment.An example of this general idea can be found in Matthies et al. (1995), which describes the design of a rover for the Mars Pathfinder mission.One of the main tasks is navigating the rover in a rocky terrain and reaching a goal (the test environment).To train for the eventual mission, the engineers utilized an "indoor arena" that mimics the test environment.The need for cost-efficient training also arises in the setting of safe robot navigation (Oliveira et al., 2020).Existing approaches to exploration have largely ignored the need to be cost-efficient during the training process and therefore are challenging to apply to real-world scenarios (see Section 2 for a detailed discussion of related work).</p>
<p>In our setup, an agent is given a fixed (and small) number of opportunities to train in environments randomly drawn from a distribution Ξ (henceforth, we refer to these as "training environments"), with the caveat that each interaction in the training environment incurs a cost.After these opportunities are exhausted, the agent enters a random test environment ξ ∼ Ξ and executes an underlying RL algorithm to adapt to the particulars of ξ, while aided by the higher-level exploration strategy learned for Ξ.One can view this formulation as a meta-optimization problem with two levels: an upper-level problem to select an exploration strategy, represented by parameters θ, and a lower-level RL task that explores with the help of the exploration strategy θ on an environment instance ξ ∼ Ξ.The blue square is the starting location of the agent, the grey region is a wall, the yellow region is the location of the key, and the red region is the door (goal).</p>
<p>Note that although the second subgoal is not exactly in the location of the key, it brings the agent to the correct vicinity, allowing the underlying RL algorithm (executed online) to further adapt to the environment's particular details.</p>
<p>We propose optimizing over a class of dynamic subgoal exploration strategies in the upper-level optimization problem.To illustrate this concept, consider the sparse-reward environment shown in Figure 1a, where an agent is tasked with picking up a "key" in the yellow region, in order to exit the "door" in the red region.</p>
<p>The grey region is a wall.An RL algorithm paired with a naive exploration strategy making use of random actions (such as ϵ-greedy) requires a prohibitively large number of random actions before finding a suitable path to the door through the key, while avoiding the wall.A dynamic subgoal strategy is an ordered set of subgoals (along with associated rewards leading to each subgoal, omitted here for illustrative clarity) that leads the agent on a trajectory where the underlying RL algorithm is more likely to discover the optimal behavior.Figures 1b-1d together show an example of a dynamic subgoal exploration with three subgoals, which first leads the agent to the vicinity of the key and later towards the door.Note that the situation here in Figure 1 is simplified in that we are actually interested in finding dynamic subgoal strategies that work on average across a distribution of environments, rather than a single environment.</p>
<p>Our Contributions</p>
<p>Our main contributions are as follows.We first propose a framework for cost-efficient learning of a dynamic subgoal exploration strategy for a distribution of environments; in other words, interactions with the environment are expensive during training, making most gradient-based approaches infeasible.We instead leverage the Bayesian optimization (BO) paradigm, a well-known class of sample-efficient optimization techniques (Brochu et al., 2010;Snoek et al., 2012;Herbol et al., 2018;Frazier, 2018), and propose a new acquisition function as a core ingredient of our approach.The Gaussian process (GP) surrogate model used by the BO formulation has the ability to reason about the learning curve of the underlying RL algorithm, enabling us to introduce two additional levers in the BO learning process to improve cost-efficiency: (1) how long to run each episode of training, (2) the number of replications to run in each training environment.These levers allow us to intelligently trade-off running a longer trial versus more replications of shorter trials; the motivation is that, given τ 1 &lt; τ 2 , an accurate evaluation of a particular exploration strategy θ after τ 1 steps may be more informative than a noisy evaluation of θ after τ 2 steps, even though the same number of environment interactions are used in both cases.The proposed algorithm, Bayesian exploratory subgoal design (BESD), is outlined in Figure 2. We also prove an asymptotic guarantee on the quality of the solution found by our approach, compared to the best possible subgoal-based exploration strategy within a given parameterized class.</p>
<p>Related Work</p>
<p>Our framework of cost-efficient learning of exploration strategies through BO appears to be distinct from existing formulations in its strong focus on expensive environmental interactions during training, made possible through the additional control levers of episode length and number of replications.Nevertheless, our work is related to a number of distinct areas of study: Bayesian optimization, exploration for RL, intrinsic reward and reward design in RL, multi-task RL, and transfer learning.Here, we attempt to give a tour through the various strands of relevance in each field.</p>
<p>Bayesian Optimization</p>
<p>BO is a technique for optimizing black-box functions in a sample-efficient manner, in particular for tuning ML models and design of experiments (Snoek et al., 2012;Brochu et al., 2010;Frazier, 2018;Herbol et al., 2018).BO methods for problems with multiple information sources or fidelities (Swersky et al., 2013;2014;Feurer et al., 2015;Domhan et al., 2015;Li et al., 2017) is especially relevant to our proposed method's ability to select the length of an RL training episode, which builds upon ideas from Picheny &amp; Ginsbourger (2013), Poloczek et al. (2017), andKlein et al. (2017).The first paper proposes fitting GP to partially converged simulations, and the latter two propose acquisition functions that consider the ratio of "information gain" to cost of evaluation.Our approach also reasons about multiple replications in an environment, similar to the problem studied in Binois et al. (2019) in the context of computer experiments.Our work fills a gap in the BO literature where the length of training and number of replications are selected jointly in a cost-aware setting, a natural and powerful idea that has not been considered in the literature.Our theoretical analysis builds upon techniques developed in Frazier et al. (2008) and Poloczek et al. (2017) but extend them in new directions, accounting for the ability to select the number of replications, and providing a characterization of the asymptotic suboptimality due to using a discretized domain.1</p>
<p>BO has previously been applied in the setting of navigation planning.Martinez-Cantin et al. (2007), Martinez-Cantin et al. (2009), andBinney et al. (2013) use BO to optimize a sequence of waypoints for a robot to follow.While our method similarly optimizes a sequence of subgoals, we use the subgoals as an exploration strategy (over a distribution of environments) on top of an existing RL algorithm, rather than as a direct specification of the control policy.In order to allow subgoals to provide exploration in a "plug-and-play" manner for existing RL algorithms, our approach also features a novel integration of subgoals with potential-based intrinsic rewards.</p>
<p>In two other works, Tesch et al. (2011) and Garcia-Barcos &amp; Martinez-Cantin (2021), BO is directly applied to optimize a parameterized policy, but this is limited to low-dimensional parameterizations of the policy: Tesch et al. (2011) tune a two-dimensional gait parameter, while Garcia-Barcos &amp; Martinez-Cantin (2021) tune four policy parameters.In our work, we augment an underlying RL algorithm that can learn arbitrary policies with a BO-optimized low-dimensional exploration strategy, striking a balance between flexibility and cost-efficiency.</p>
<p>Exploration in Reinforcement Learning</p>
<p>Naive exploration strategies such as ϵ-greedy can lead to unreasonably large data requirements, making exploration a commonly studied topic in RL.Most existing work focus on proposing a fixed exploration strategy that is executed for a single underlying environment.For example, some previous related work employ approaches based on optimism (Kearns &amp; Singh, 2002;Stadie et al., 2015;Bellemare et al., 2016;Tang et al., 2017) and posterior sampling (Osband et al., 2016;Russo &amp; Van Roy, 2014;Osband &amp; Van Roy, 2017;Morere &amp; Ramos, 2018) to guide exploration.Others insert an active learning (Shyam et al., 2019) or experimental design (Mehta et al., 2021) perspective into the model-based RL framework.</p>
<p>Our work departs from these existing studies in that we formulate the problem of exploration as a metaoptimization over a parameterized class of exploration strategies and aim to find a suitable strategy for a distribution of environments.A more closely related paper is Gupta et al. (2018), which extends the model-agnostic meta-learning (MAML) approach of (Finn et al., 2017a) to the problem of exploration for a set of tasks in a way that is similar in spirit to our formulation.However, their gradient-based approach is not sample-efficient and they do not consider costly environment interactions during training.In addition, Gupta et al. (2018) make use of task-specific parameters during training, limiting their approach to a small set of environments.For a more comprehensive list of methods for exploration in RL, we refer the reader to the excellent survey of Amin et al. (2021).</p>
<p>Hierarchical Reinforcement Learning and Options</p>
<p>Our proposed approach is related to the hierarchical reinforcement learning (HRL) framework, which refers to methods that decompose a complex, long-horizon problem into smaller subtasks; see Barto &amp; Mahadevan (2003) and Pateria et al. (2021) for extensive reviews of the topic.A well-known type of HRL is feudal reinforcement learning, introduced in Dayan &amp; Hinton (1992), where a high-level manager delegates low-level workers to complete subtasks.Examples of more recent work that follow this feudal hierarchy paradigm include Kulkarni et al. (2016), Levy et al. (2018), andNachum et al. (2018).Our work exhibits a similar flavor in that a high-level BO method sets a subgoal-based exploration strategy, which is then executed by the underlying RL algorithm.</p>
<p>The concept of options, which are temporally extended actions represented as a policy and a termination condition, also fall under the HRL framework.Options can improve the efficiency of RL through the use of previously acquired "skills" (Sutton et al., 1999;Precup et al., 1998).These skills might be acquired with the help of a human, either fully user-specified (e.g., Jothimurugan et al. (2021)) or obtained from expert demon-strations (e.g., Pan et al. (2018), Paul et al. (2019)).In this paper, a subgoal is a particular type of option and therefore, our dynamic subgoal exploration strategy can be thought of, at a high level, as a sequence of options.</p>
<p>Of particular relevance to our work is when options are automatically discovered, a problem that is well-known to be challenging.One stream of work views option discovery to be (at least somewhat) detached from the RL reward maximization objective, using state visitation frequencies (Stolle &amp; Precup, 2002;McGovern &amp; Barto, 2001;Goel &amp; Huber, 2003), clustering (Mannor et al., 2004), novelty (Şimşek &amp; Barto, 2004), local graph partitioning (Şimşek et al., 2005), or diversity objectives (Eysenbach et al., 2018;Zhang et al., 2020), to name a few examples.Approaches that considers a joint objective for option learning RL reward maximization objective like ours (Kulkarni et al., 2016;Vezhnevets et al., 2016;Bacon et al., 2017;Frans et al., 2018;Veeriah et al., 2021) typically use large, neural network-based representations along with gradient-based (meta-)optimization and do not focus on cost-aware training.The method that we propose in this paper is unique from previous works in that (1) it is designed specifically for the case where cost-aware training is warranted and uses BO for option-learning, (2) it offers an integrated objective for subgoal-design and RL reward maximization, and (3) it uses a novel combination of subgoals and reward shaping, which has a simpler representation than a generic option.</p>
<p>Intrinsic Reward and Reward Design</p>
<p>When a particular subgoal of our proposed dynamic subgoal exploration strategy is active, we "turn on" a set of artificial rewards that incentivize the agent to move toward that subgoal (these rewards are then removed after the agent moves on to the next subgoal).Hence, the literature on intrinsic reward and reward design in RL are also relevant.Intrinsic reward (also called intrinsic motivation) helps an agent learn increasingly complex behavior in a self-motivated way (Randløv &amp; Alstrøm, 1998;Ng et al., 1999;Huang &amp; Weng, 2002;Kaplan &amp; Oudeyer, 2004;Şimşek &amp; Barto, 2006;Tenorio-Gonzalez et al., 2010;Pathak et al., 2017;Achiam &amp; Sastry, 2017;Lample &amp; Chaplot, 2017).Several works from the reward design literature are most closely related to our paper.Sorg et al. (2010) and Guo et al. (2016) directly optimize the intrinsic reward parameters, via gradient ascent, to maximize the outcome of the learning process.Similarly, Zheng et al. (2018) use intrinsic rewards in policy gradient, and treat the parameters of policy as a function of the parameters of intrinsic rewards.Again, these methods differ from ours in that they do not consider the costliness of training and focus on finding intrinsic rewards for a single MDP.</p>
<p>Multi-task RL and Transfer Learning</p>
<p>Also related to our setting are methods that aim to train agents with the capability of solving (or adapting to) multiple sequential decision making tasks (Pickett &amp; Barto, 2002;Konidaris &amp; Barto, 2006;Wilson et al., 2007;Fernández et al., 2010;Deisenroth et al., 2014;Doshi-Velez &amp; Konidaris, 2016;Finn et al., 2017a;b;Pinto &amp; Gupta, 2017;Espeholt et al., 2018;Hessel et al., 2019;Vithayathil Varghese &amp; Mahmoud, 2020); such methods generally fall under the umbrella of multi-task RL or transfer learning.As before, many of these methods require the training of large neural networks and are not designed for a cost-aware setting.Despite their stated purpose of being sample-efficient in adapting to new tasks, most multi-task RL or transfer learning approaches do not place a strong emphasis on cost-efficiency of training on existing tasks.This is an important distinction to our work.The two papers that are closest in spirit to our work are Pickett &amp; Barto (2002), where macro-actions are extracted from previous tasks, and Konidaris &amp; Barto (2006), where shaped rewards are learned for a set of tasks.One drawback of Pickett &amp; Barto (2002) is that it assumes access to optimal policies for an initial set of MDPs.Konidaris &amp; Barto (2006) directly uses previous value functions as shaped rewards (thereby requiring the agent to solve some tasks from scratch) and does not provide an avenue for cost-effective exploration.</p>
<p>Problem Formulation</p>
<p>This section formulates the problem mathematically, by defining the original (sparse-reward) MDPs and how a dynamic subgoal exploration strategy induces an auxiliary, "subgoal-augmented" MDPs.We then describe the iterative training process.</p>
<p>Original MDPs M ξ with Sparse Rewards</p>
<p>Consider a family of MDPs {M ξ = ⟨S, A, T ξ , R ξ , γ⟩} ξ parameterized by a random variable ξ ∼ Ξ, where S and A are the state and action spaces, T ξ is the transition matrix, R ξ : S × A × S → R is the extrinsic2 reward function, γ ∈ [0, 1] is the discount factor3 , and Ξ is the environment distribution (not assumed to be known, nor does it need to be finite or discrete).4A sparse-reward environment is an environment where R ξ is non-zero only for a small number of "goal" states.To ensure that all quantities are well-defined, we assume that R ξ is bounded, as is common in the reinforcement learning literature.We assume common state and action spaces across the distribution of MDPs (i.e., they are independent of ξ), while the reward and transition functions vary with ξ.</p>
<p>Given S and A, a policy π is a mapping such that π(• | s) is a distribution over A for any state s ∈ S. For any ξ ∼ Ξ, define the value function of policy π at any state s as
V π ξ (s) = E ∞ t=0 γ t R ξ (s t , a t , s t+1 ) π, s ,(1)
where the notation of "conditioning" on π and s indicates that s 0 = s is the initial state and
a t ∼ π(• | s t ).
For the MDP M ξ , its optimal value function and associated optimal policy are
V * ξ (s) = sup π V π ξ (s) and π * ξ (s) ∈ arg max a∈A E R ξ (s, a, s ′ ) + γV * ξ (s ′ ) | s, a .
Now that we have defined the value function, let us comment on the environment distribution Ξ.In Section 3.4, we will formulate the meta-optimization problem, which requires that the expected performance of any policy π over the environment distribution, i.e., E ξ [V π ξ (s)], is well-defined.However, since we assumed bounded rewards, implying bounded performance V π ξ (s), it will always be the case that this expectation exists and we do not require further assumptions on Ξ.</p>
<p>When the extrinsic reward function R ξ is sparse, it produces little to no learning signal for the agent.Under most RL algorithms, the agent essentially performs random exploration and does not start learning until the first time it wanders to the goal.The time it takes to find the goal under a random exploration strategy is often prohibitively long.The ϵ-greedy exploration strategy, which takes a random action with probability ϵ and the best action under the current value function approximation, is an example of a random exploration strategy.</p>
<p>Dynamic Subgoal Exploration Strategies</p>
<p>An intrinsic reward is an artificial reward signal experienced by the agent that does not come directly from the environment.A subgoal is defined by a (usually small) set of states, such that when the agent lands in any of them, the subgoal is considered "completed."A dynamic subgoal exploration strategy is a sequence of subgoals, along with an associated reward shaping function for each subgoal, that provides an intrinsic reward signal for the agent.If the locations of the subgoals are chosen well, this strategy can help the agent explore the environment efficiently.We call this a dynamic strategy because the subgoals are turned on one-by-one and consequently introduces a new state into the MDP (described in detail below).</p>
<p>Suppose there are K subgoals and let θ ∈ Θ be a parameter that fully describes a subgoal exploration strategy, including the subgoal locations, associated rewards, and sequencing.Let G θ,k ⊆ S be a set of "target" states associated with the kth subgoal, for k ∈ {1, 2, . . ., K}, in the sense that if the agent lands in some state in G θ,k , then the kth subgoal is considered "completed."In addition, we define an artificial reward function g θ,k (s, s ′ ) that, when activated, provides a sequence of rewards that leads the agent toward subgoal k.Concretely, we use potential-based reward shaping from Ng et al. (1999) to achieve this.Let Φ θ,k be a potential function, a function that assigns a value for each state in S, with higher potential indicating a more "valuable" state.Φ θ,k should have the property that target states in G θ,k have the highest potential.Then, let
g θ,k (s, s ′ ) = γΦ θ,k (s ′ ) − Φ θ,k (s),(2)
for all s, s ′ ∈ S. The definition of g θ,k (s, s ′ ) in ( 2) can be interpreted as the difference in potential between states s ′ and s (with discount γ).This potential difference motivates the agent to move towards the target states (high potential) of kth subgoal.Thus, a parameterization of a set of K subgoals, which forms our exploration strategy, is fully described by
{G θ,k } K k=1 , {g θ,j } K k=1
, the locations and associated reward shaping functions.</p>
<p>Example 1 (Key and Door Environment) Let us consider a distribution of maze MDPs with states {(i, j)} 1≤i,j≤10 and a sparse reward in the upper left corner at (0, 10).In addition, suppose that the agent needs to pick up a key in order to receive the reward at (0, 10), where the location of the key is uncertain but likely to be in the right half of the room.The environment illustrated in Figure 1 can be considered to be one possible realization from this distribution of mazes.Now, let us consider a subgoal design with K = 3 subgoals.The simple parameterization θ = (i 1 , j 1 , i 2 , j 2 , i 3 , j 3 ), with
G θ,k = {(i k , j k )} and Φ θ,k (s) = e −∥s−(i k ,j k )∥ 2
specifies that for k ∈ {1, 2, 3}, the kth subgoal is located at a single state (i k , j k ) and the artificial reward potential is a Gaussian centered at (i k , j k ).Using Figure 1 as a visual reference, one can imagine that the subgoal design θ = (1, 2, 8, 4, 2, 8) would be useful in guiding the agent toward the vicinity of the key on the right side of the room and then toward the vicinity of the goal.Once the agent is in the correct vicinity, the underlying RL algorithm can discover the precise locations of the key and goal in the particular environment realization more quickly.</p>
<p>Subgoal Parameterization vs State Dimensionality</p>
<p>For the types of navigation tasks that we are concerned with in this paper, the dimension of the subgoal parameterization θ need not scale with the dimension of the state s, which would pose a potential scalability issue.Instead, one general rule-of-thumb to keep in mind is that for a dynamic subgoal exploration strategy to be effective in navigation tasks, the dimension of θ only needs to scale with the number components of s that pertain to the spatial positioning of the agent.The next example provides an illustration.</p>
<p>Example 2 (Mountain Car Environment, with dim(θ) &lt; dim(s)) Consider the well-known Mountain</p>
<p>Car problem, a continuous control task where an underpowered car, operating in a one-dimensional space, must make its way up a steep mountain (Sutton &amp; Barto, 2018, Example 10.1).The state is two-dimensional, s = (x, ẋ), where x ∈ [−1.2, 0.5] is the position of the agent while ẋ ∈ [−0.07, 0.07] is its velocity.A possible subgoal design with
K = 2 is θ = (i 1 , i 2 ), with G θ,k = {(i k , ẋ) | ẋ ∈ [−0.07, 0.07]} and Φ θ,k (s) = e −(x−i k ) 2
for each k.In other words, the agent reaches a subgoal target state if its position is i k , for any value of its velocity.Also, the artificial reward only depends on the spatial position x rather than the full state (x, ẋ).In Section 5, we give numerical results for exactly this example.</p>
<p>One could imagine that the concept illustrated in Example 2 also applies to more complex robotics environments with a high-dimensional state, where the number of components related to the spatial positioning is relatively small.This suggests that the subgoal parameterization (and the resulting BO problem) is often of much lower dimension than that of the state itself.</p>
<p>Subgoal-Augmented MDPs M ξ,θ</p>
<p>Now that we have described how a particular subgoal design is parameterized, the remaining question is how these are integrated in a useful way into the original, sparse-reward MDP described in Section 3.1.We propose the notion of a subgoal-augmented, auxiliary MDP, where the K subgoals are sequentially "activated."This way, we encode subgoal ordering5 into the exploration strategy, meaning that the agent only moves on to the next subgoal after finishing the current one.</p>
<p>Let M ξ,θ denote an auxiliary, subgoal-augmented MDP based on an original MDP M ξ , except that it is includes rewards and transitions associated with the dynamic subgoal exploration strategy θ.We introduce an auxiliary state i ∈ I := {0, 1, . . ., K}, where i represents the number of subgoals reached by the agent so far.Initially, we have i 0 = 0.The state of the M ξ,θ is (s, i) ∈ S × I and the transition for the auxiliary state is i ′ = i + 1 {s ′ ∈ G θ,i+1 } , where we take G θ,K+1 = ∅.This means the auxiliary state i is updated to i + 1 whenever s ′ reaches the next subgoal.Let the intrinsic reward of the agent be:
G θ (s t , i t , s t+1 ) = K k=1 1 {k=it} • g θ,k+1 (s t , s t+1 ),
where the indicator function encodes the logic that if i t subgoals have been completed so far, then the current target is subgoal i t + 1 and only the rewards leading to subgoal j + 1 are active.The new reward function consists of both extrinsic (R θ ) and intrinsic (G θ ) rewards:
Rξ,θ (s, i, a, s ′ ) = R θ (s, a, s ′ ) + G θ (s, i, s ′ ).
The value function for the new MDP M ξ,θ is written
V π ξ,θ (s, i) = E ∞ t=0 γ t Rξ,θ (s t , i t , a t , s t+1 ) | π, s, i ,(3)
where π(•|s, i) is now a policy defined on the new state space S × I. Figure 3 gives an example of the overall setup: Figure 3a shows the original MDP environment M ξ , where the dark gray cells are walls, the light gray represent uncertainty in the size of the "doors", and the red cells represent goal states (the sparse, extrinsic reward).Figure 3b shows the possible rewards the agent can encounter in the augmented MDP M ξ,θ , for a random selection of subgoals θ.The original sparse reward is represented by the red bar in the corner and the first subgoal is the one that is farther from the goal.Both subgoals are singletons and the potential functions are radial basis functions centered at the subgoal locations, similar to the parameterization described in Example 1.Note that this randomly selected set of subgoals θ is not a good exploration strategy for the environment in Figure 3a (as it leads the agent toward a wall), motivating the need for optimizing their locations, as we discuss in the next section.</p>
<p>Optimizing the Exploration Strategy</p>
<p>The selection of the subgoal design θ depends on the agent's underlying learning algorithm, which could in principle be any RL algorithm where intermediate rewards influence the learning process: this includes any temporal difference-based algorithm that makes use of learned value functions.In the numerical results of Section 5, our agent learns via Q-learning Watkins &amp; Dayan (1992).We refer to the underlying RL algorithm as RL-ALGO.Let us use the notation RL-ALGO[τ, M] to refer to the policy learned by RL-ALGO on MDP M after τ training interactions.We remind the reader that the subgoal-based exploration strategy is fixed before the test environment is revealed, so that the sequence of events in the test phase is as follows:</p>
<p>1.A subgoal design θ for exploration is selected.</p>
<ol>
<li>
<p>The agent is placed in a new environment ξ.</p>
</li>
<li>
<p>The agent uses the subgoal-augmented MDP M ξ,θ and an RL algorithm with a budget of τ max interactions to learn a policy RL-ALGO τ max , M ξ,θ .</p>
</li>
<li>
<p>The agent's policy is evaluated using only the extrinsic reward function R ξ of the original MDP.Our goal is to find an exploration strategy θ ∈ Θ such that a policy trained using θ behaves well in the original MDP:
max θ∈Θ E ∞ t=0 γ t R ξ (s t , a t , s t+1 ) πτmax ξ,θ , (s 0 , i 0 ) with πτmax ξ,θ = RL-ALGO τ max , M ξ,θ ,(4)
where (s 0 , i 0 ) is the initial augmented state.The interpretation of the objective in ( 4) is as follows: Evaluate πτmax ξ,θ (a policy for the subgoal-augmented MDP) using the same dynamics as the subgoal-augmented MDP, but without the rewards associated with the subgoals.In other words, the policy takes actions based on the augmented state (s t , i t ), but only receives rewards associated with the original MDP.This explains why we need to consider a starting state (s 0 , i 0 ), rather than just s 0 (note that the reward does not depend on the auxiliary state i t ).</p>
</li>
</ol>
<p>The expectation in (4) is taken over the random choice of a test environment ξ, the stochastic dynamics within M ξ , and the stochasticity of the learning algorithm itself.It is convenient to explicitly define the following:
u(θ, τ ) = E ∞ t=0 γ t R ξ (s t , a t , s t+1 ) πτ ξ,θ , s 0 , i 0 with πτ ξ,θ = RL-ALGO τ, M ξ,θ .
Although the objective function in (4) is u(θ, τ max ), the notation u(θ, τ ) will be useful in Section 4, where we discuss using fewer than τ max interactions to learn about u(θ, τ max ) as a way of reducing cost.</p>
<p>Iterative Training and Additional Cost-Reduction Levers</p>
<p>In our setting, we observe the performance of exploration strategies and the resulting policies in a sequence of training environment realizations ξ 1 , ξ 2 , . . ., ξ N drawn from the MDP distribution Ξ.By default, each complete evaluation of the objective function in (4) u(θ, τ max ) for a fixed θ requires running RL-ALGO for τ max interactions.Since each interaction in the training environments is expensive (e.g., in robotics applications, this could involve time, labor, and equipment), we want to consider ways to reduce the number of training interactions.To do so, we propose two additional levers:</p>
<ol>
<li>
<p>Maximum number of interactions.For each training environment ξ n , we allow the specification of a maximum number of interactions τ n , chosen from a finite set T ⊆ N. In episodic tasks, if an episode finishes before τ n interactions are used, we start a new episode and continue in this manner until exactly τ n environment interactions are exhausted.In the next section, we describe our probabilistic model of the RL training curve, which allows observations of short episodes to be informative about the final performance.This also can reduce the risk of spending too many interactions with an unpromising exploration strategy.</p>
</li>
<li>
<p>Multiple replications.We can reduce the variance of performance observations by averaging over the observed cumulative reward over q n i.i.d.replications, for a total of τ n q n interactions in training environment ξ n+1 .Each replication is an independent invocation of an agent.We suppose that q n is chosen from a finite set Q.The idea here is that even with the same number of total interactions, a lower variance observation of a "preliminary" result could be more informative than a higher variance observation of the "full" result.</p>
</li>
</ol>
<p>To summarize, three decisions are made at the beginning of each training opportunity n: (1) a choice of subgoal design θ n , (2) the maximum number of interactions τ n , and (3) the number q n of independent replications to use for this particular θ n .For each of the q n replications, we obtain a policy
πτ n ξ n+1 ,θ n = RL-ALGO τ n , M ξ n+1 ,θ n ,
before observing a estimate of its performance.After the q n training replications are complete, we compute the average performance over the q n replications.Written more succinctly, our observation in episode n takes the form
y n+1 (θ n , τ n , q n ) = u(θ n , τ n ) + ε n+1 env + ε n+1 rep (q n ), where ε n+1
env represents the deviation from the u(θ n , τ n ) due to the random environment ξ n+1 , while the observation noise ε n+1 rep (q n ) represents the noise that can be reduced via multiple replications, i.e., the noise in πτ n ξ n+1 ,θ n due to a sample run of RL-ALGO.Thus, ε n+1 rep (q n ) depends on the number of replications q n .Naturally, a larger number of replications implies a smaller observation noise.Note that the observations {y n } are i.i.d., since a new MDP is sampled in each iteration.The total training cost incurred is cumulative number of interactions:
N −1 n=0 τ n q n .
After training opportunities 0, 1, . . ., N − 1, we reach the test phase and commit to a final subgoal design θ N rec .This design is evaluated on the test MDP ξ N +1 ∼ Ξ with an agent that has a full budget of τ max interactions.</p>
<p>Bayesian Optimization for Cost-Efficient Exploration</p>
<p>The setup for BO typically consists of two components: (1) a probabilistic surrogate model (usually a Gaussian process) for modeling the objective function, and (2) an acquisition function, which given a dataset of past observations, assigns a score to each potential observation location (Garnett, 2023).Selecting the optimizer of the acquisition function as the next point to sample gives rise to strategy for balancing exploration with exploitation.The BO "loop" repeats the following: sample a point (using a combination of the current surrogate model and acquisition function), observe new data, and update the surrogate model.For a detailed tutorial, we refer readers to Frazier (2018) and Garnett (2023).</p>
<p>For our setting of learning a dynamic subgoal exploration strategy, we propose a tailored probabilistic model for the RL learning curve and an acquisition function for selecting the next subgoal design, the maximum episode length, and the number of replications to run.Although shorter episodes and smaller number of replications are more cost-efficient, they also decrease the chance of reaching the goal and produce higher observation noise.Thus, the acquisition function must carefully trade off these downsides with the cost of interactions.We call this the Bayesian Exploratory Subgoal Design (BESD) acquisition function.</p>
<p>Surrogate Model</p>
<dl>
<dt>In order to enable the ability to dynamically select the maximum episode length of training, as described in Section 3.5, our approach uses a GP surrogate model over u(θ, τ ), rather than u(θ, τ max ).In other words, our model is a function of both θ and τ rather than just θ, enabling it to capture the performance of a policy trained with subgoals θ, for a variety of episode lengths.Assume that Θ ⊆ R m .We place a GP prior f on the latent function u with mean function µ : Θ × T → R and covariance function k</dt>
<dd>(Θ × T ) × (Θ × T ) → R + .
More precisely, to capture the structure of the RL learning curve, we set µ to the mean of an initial set Algorithm 1 Bayesian Exploratory Subgoal Design 1. Set n = 0. Estimate hyperparameters of the GP prior f using initial samples.</dd>
</dl>
<ol>
<li>
<p>Compute next decision (θ n , τ n , q n ) according to the acquisition function (7).</p>
</li>
<li>
<p>Train in environment ξ n+1 augmented with θ n (M ξ n+1 ,θ n ) using levers (τ n , q n ).</p>
</li>
<li>
<p>Observe y n+1 (θ n , τ n ) and update posterior on f . 5. If n &lt; N , increment n and return to Step 2. 6.Return a subgoal recommendation θ N rec that maximizes µ N (θ, τ max ).</p>
</li>
</ol>
<p>of samples and use a multidimensional product kernel, based on the kernel used in Klein et al. (2017) for modeling ML performance as a function of parameters and time:
k (θ, τ ), (θ ′ , τ ′ ) = k θ (θ, θ ′ ) k τ (τ, τ ′ ),(5)
where the first kernel k θ is the (5/2)-Matérn kernel and
k τ is a polynomial kernel k τ (τ, τ ′ ) = ϕ(τ ) ⊺ Σ ϕ ϕ(τ ′ )
with ϕ(τ ) = (1, τ ) ⊺ and hyperparameters Σ ϕ .Note that the covariance under k is large only if the covariance is large under both k θ and k τ .We make the modeling assumption that ε n+1 env and ε n+1 rep (q n ) are independent, zero mean, and normally distributed6 with variances σ 2 env and σ 2 rep /q n , respectively.This allows us to take advantage of standard GP machinery to analytically compute the posterior on f conditioned on the history after n steps.This posterior is another GP, whose mean and kernel functions are denoted µ n (θ, τ ) and k n ((θ, τ ), (θ ′ , τ ′ )); the exact expressions can be found in, e.g., Rasmussen &amp; Williams (2006).</p>
<p>We remind the reader that the dimensionality of the GP surrogate model is dim(Θ) + 1, i.e., the dimension of the subgoal parameterization, along with an additional dimension for τ .As illustrated in Example 2, it will often be the case for navigation domains that the dimension of the subgoal parameterization is smaller than that of the state space of the underlying RL problem (due to the relatively small number of spatial components of the state).Therefore, dynamic subgoal exploration strategies can be tractably modeled and optimized for broad classes of navigation problems, even with vanilla GPs.7</p>
<p>Acquisition Function</p>
<p>As described above, the proposed algorithm proceeds in iterations, selecting one set of subgoals θ n along with τ n and q n , to be evaluated in each training environment.We now propose the acquisition function for making these evaluation decisions.An overview of the BO setup is given in Algorithm 1.</p>
<p>Suppose the training budget is used up after training iterations 0, 1, . . ., N − 1.Then, the optimal risk-neutral decision is to use subgoals on the test MDP ξ N +1 that have maximum expected performance under the posterior.The expected score of this choice is µ n * where
µ n * := max θ µ n (θ, τ max ),(6)
where
µ n (θ, τ max ) = E n [f (θ, τ max )]
. Here E n is the conditional expectation with respect to the history after the first n observations: (θ 0 , τ 0 , q 0 , y 1 , . . ., θ n−1 , τ n−1 , q n−1 , y n ).Note that although we are allowed to use fewer than τ max interactions in training environments to reduce cost, the agent uses its full budget for the test MDP ξ N +1 .</p>
<p>The proposed acquisition function is based upon the knowledge gradient, which is one-step lookahead approach (Frazier et al., 2008;2009).That means we imagine for each training MDP that it is the last opportunity before the test MDP and act optimally.Full lookahead approaches require solving an intractable dynamic programming problem; however, we show that nonetheless, the one-step approach is asymptotically optimal in Theorem 1 and Theorem 2. If we evaluate (θ, τ, q), i.e., the subgoals θ for τ steps and q replications, then the expected gain in performance in the test MDP of the recommended exploration strategy after the evaluation, based on (6), with respect to the current best is
ν n (θ, τ, q) = E n µ n+1 * | θ n = θ, τ n = τ, q n = q − µ n * .
Therefore, the one-step optimal strategy is to choose the next subgoals θ n , maximum episode length τ n , and number of replications q n so that ν n is maximized.However, this strategy would generally allocate a maximum number of steps τ max and replications q max for the evaluation of the next subgoal design, as observing τ max during training is most informative, and repeating for q max replications reduces the noise maximally.In other words, this strategy does not consider the cost of training.</p>
<p>Hence, we propose an acquisition strategy that maximizes the gain in performance per effort:
(θ n , τ n , q n ) ∈ arg max θ,τ,q ν n (θ, τ, q) qτ . (7)
The optimization problem ( 7) is challenging when the domain Θ is continuous, so we take the approach of replacing it with a discrete domain Θ ⊆ Θ (for example, this could be selected by a Latin hypercube design (Stein, 1987)).This approach has been applied successfully in other knowledge gradient style acquisition functions (Scott et al., 2011;Wu &amp; Frazier, 2016;Poloczek et al., 2017).We provide a novel theoretical guarantee on the asymptotic suboptimality of a discretized optimization domain.It characterizes the Lipschitz constant explicitly, thereby improving on the analysis of Poloczek et al. (2017); see Theorem 2 in the next section.</p>
<p>Theoretical Analysis</p>
<p>We now provide our main theoretical results on the asymptotic optimality of BESD.Detailed proofs can be found in Appendix A. For convenience, we suppose µ(θ, τ ) = 0 for all (θ, τ ), and further assume that the kernel k(•, •) has continuous partial derivatives up to the fourth order.Recall that θ N rec ∈ Θ is the final recommendation made in iteration N :
θ N rec ∈ arg max θ∈ Θ µ N (θ, τ max ).
Our first theorem is concerned with the finite, discretized optimization domain Θ.</p>
<p>Theorem 1 The acquisition function described in ( 7) has the property of asymptotic optimality with respect to Θ, i.e., lim
N →∞ f (θ N rec , τ max ) = max θ∈ Θ f (θ, τ max ), almost surely. That is, the recommended design θ N rec becomes optimal as N → ∞.
If the optimization domain Θ = Θ, then Theorem 1 suffices.Unfortunately, for many applications, the subgoal parameterizations will naturally be continuous.Next, we provide an additive bound on the difference between the solution of BESD in Θ and the unknown optimum in Θ, as the number of iterations N tends to infinity.</p>
<p>We use a probabilistic Lipschitz constant of a GP from Lederer et al. (2019) to quantify the performance with respect to the full, continuous subgoal parameter space.We make use of the fact that the derivative df (θ, τ max )/dθ i is another GP with covariance
k ∂i (θ, θ ′ ) = ∂ 2 ∂θ i ∂θ ′ i k (θ, τ max ), (θ ′ , τ max ) ,
for all i = 1, 2, . . ., m (Rasmussen &amp; Williams, 2006, Section 9.4).See also Ghosal et al. (2006) and Wu et al. (2017) for other uses of this property.For each i = 1, 2, . . ., m, define the constant  where L ∂i k be a Lipschitz constant of the kernel k ∂i and k ∂ max = max θ∈Θ k ∂i (θ, θ).
L i δ = k ∂ max 2 log 2m δ + 12 √ 6m max k ∂ max , L ∂i k max θ,θ ′ ∈Θ dist(θ, θ ′ ) ,(8)</p>
<p>Theorem 2</p>
<p>The acquisition function of ( 7) has bounded asymptotic suboptimality with respect to the original domain Θ in the sense that with probability at least 1 − δ, it holds that
lim N →∞ f (θ N rec , τ max ) ≥ max θ∈Θ f (θ, τ max ) − d • ∥L δ ∥ where d = max θ∈Θ min θ ′ ∈ Θ dist(θ, θ ′
) is a measure on the "coarseness" of the discretization and L δ is the vector (L 1 δ , L 2 δ , . . ., L m δ ), with each L i δ defined as in ( 8).</p>
<p>Numerical Experiments</p>
<p>We now show numerical experiments to demonstrate the cost-effectiveness of the BESD framework.BESD is implemented using the MOE package (Clark et al., 2014) and the full source code be found at the following URL: https://github.com/yjwang0618/subgoal-based-exploration.In the experiments that follow, we use the BESD approach to optimize dynamic subgoal exploration strategies consisting of two or three subgoals.</p>
<p>BESD is given a few choices for the interaction length τ and number of replications q (values reported for each benchmark below).Each replication of the BESD is given an initial set of 10 observations for each value of τ (these initial observations incur interaction costs just like future observations).The potential function at state s with the jth subgoal activated is Φ j (s) = w 1 exp[−0.5(s− j) 2 /w 2 ], where the "height" is w 1 = 0.2 and "width" is w 2 = 10.The underlying RL algorithm for all environments is Q-learning with an ϵ-greedy behavioral policy (with ϵ = 0.2) for all environments.</p>
<p>In the next few subsections, we first introduce each of the environments and along the way, show some qualitative results obtained by BESD, focusing on providing intuition.Later, in Section 5.6, we introduce the baseline algorithms used for an empirical comparison, and in Section 5.7, provide a discussion of those results.</p>
<p>Windy Gridworlds with Walls</p>
<p>The first set of environments (GW10) is a distribution over 10 × 10 gridworlds, where the goal is to reach the upper left square that is shaded red in Figure 4a to collect a reward of one.The agent starts from the lower-left grid square shaded in blue and may in each step choose an action from the action space consisting of the four compass directions.Each gridworld is partitioned by a wall into two rooms.The wall, randomly located in one of the middle five rows in the gridworld, has a door located on four grid squares on its right.The agent will stay in the current location when it hits the wall.</p>
<p>There is a small amount of "wind" or noise in the transition: the agent moves in a random direction with a probability that is itself uniformly distributed between 0 and 0.02 (thus, a particular environment instance drawn from the distribution has a random wall location and wind probability).</p>
<p>We use T = {200, 600, 1000} for the possible values of τ and Q = {5, 20} for the possible values of q.We parameterize the exploration strategy using two subgoals, whose locations are optimized.Subgoal locations are limited to the continuous subset of R 2 which contains the grid, i.e., Θ = ([0, 10] × [0, 10]) 2 for GW10.</p>
<p>Recommendation Paths for GW10</p>
<p>In order to visualize the qualitative behavior of BESD, we show in Figure 4a the evolution of the recommended subgoals over time (iterations), a concept that we refer to as a recommendation path.The plot displays four recommendation path realizations of BESD using distinct colors.Within each color, the lightest points are the initial samples while the darker points represent recommendations for larger n.Also within each color, the circles represent the first subgoal of the exploration strategy, while the triangles represent the second subgoal.</p>
<p>We point out two types of exploration behaviors discovered by BESD in Figure 4a:</p>
<p>• Behavior 'A': The pairs of regions labeled 'A' are the final recommendations of the orange, green, and purple sample paths.The strategy leads the agent toward the upper right corner (away from the wall), and then after that, directly towards the goal.</p>
<p>• Behavior 'B': The final recommendation of the red sample path is labeled by 'B.' Note that in behavior 'A', a direct path to the first subgoal (upper right corner) is blocked by the random wall for some realizations of the environment.Behavior 'B' might be interpreted as a slight remedy of this situation by targeting a lower region of the right edge, creating a more direct path around the wall.</p>
<p>Both strategies appear to be reasonable ways for the agent to avoid the door and head to the goal.</p>
<p>Larger, Three-Room Windy Gridworlds</p>
<p>The second domain (GW20) is a distribution of larger 20 × 20 gridworlds with three rooms separated by two walls.As shown in Figure 4b, the walls are randomly located in the middle rows (dark gray).A door of size 8 is randomly located somewhere within the wall, shaded in light gray.The starting location is the blue square in the lower left and the goal is displayed in red in the upper right.As in GW10, we optimize the locations of a two-subgoal exploration strategy, with Θ = ([0, 20] × [0, 20]) 2 .The noise due to wind is the same as in GW10.In this experiment, we consider the case of only allowing BESD to select the maximum episode length from T = {4000, 7000, 10000}, while keeping q = 20 fixed.</p>
<p>Recommendation Paths for GW20</p>
<p>Recommendation paths are shown in Figure 4b.Unlike the case of GW10, all four of the realizations converge to roughly the same exploration strategy, labeled by 'A.' Focusing on the lighter red and orange circles, we can notice a trend of the first subgoal initially being placed (naively) near the goal, but as learning progresses, they move downward toward the entrance of the first door.The second subgoal converges toward the exit of the second door, moving the agent near the goal.</p>
<p>Regarding the placement of the first subgoal near the goal and inducing a direct path, it is worth pointing out this strategy might work for some environments (i.e., those where the first door is at its leftmost position   5a, largely follows the same design as Figures 4a and 4b, except that the green squares represent possible location of the treasure.In the second panel, Figure 5b, since the location of the mountain-car is one-dimensional, we visualize the four recommendation paths by spacing them vertically to avoid crowding (therefore, the vertical axis represents different trajectories, each shown in a different color).The initial location of the car is colored in blue, while the goal is in red, corresponding to the overlay of the mountain.See the caption of Figure 4 for an explanation of the symbols used.and the second door is at its rightmost position).However, BESD learns that in order to perform well across the distribution of environments, the strategy of first moving rightward is better.</p>
<p>Treasure-in-Room</p>
<p>The third domain (TR) is a distribution of 10 × 10 gridworlds with a "treasure" hidden in a small room; see Figure 5a.The light green area shows the possible positions of the treasure.The agent gets a reward of 10 upon entering the square with treasure, and a reward of 10 upon reaching the goal.The cumulative reward, however, is zero if the agent does not find the goal within the interaction budget.The discount factor is set to γ = 0.98 to encourage policies that collect the reward earlier.We set T = {400, 1200, 2000} and Q = {5, 20}.</p>
<p>Recommendation Paths for TR</p>
<p>The recommendation paths for TR are shown in Figure 5a.We observe that two strategies were discovered by BESD across these four realizations:</p>
<p>• Behavior 'A': This appears to be the ideal behavior and was discovered in the orange, purple, and red sample paths: first lead the agent to the treasure and then toward the goal through the upper right.It is also notable that the first subgoal is located at the bottom of the room, meaning that wherever the treasure turns out to be, the agent can pick it up without backtracking.</p>
<p>• Behavior 'B': The green sample path's final recommendation coincides with the (apparently suboptimal) exploration strategy denoted by 'B' simply leads the agent to the treasure, but does not provide any guidance toward the goal.We highlight that this is an instance where BESD's learning is not yet complete, evidenced by the fact that behavior 'B' is often recommended in earlier iterations of the orange sample path.In that case however, BESD eventually discovers behavior 'A' in later iterations.The blue and red shaded regions denote the starting points and goals, respectively.Dark and light gray regions possible locations of walls and doors, respectively.Each plot displays four realizations of the "recommendation paths" of BESD.Each color corresponds to one sample realization, and the color becomes darker as n increases, with the lightest points being the initial samples.The circles, triangles, and crosses represent the first, second, and third subgoals, respectively.The 'A' and 'B' labels point out two example sets of subgoals displaying notable behaviors.</p>
<p>The Mountain Car Problem (MC)</p>
<p>The mountain car (MC) domain, as we introduced in Example 2, is a commonly used RL benchmark environment that tests an agent's ability to explore, as it is required to go in the opposite direction of the goal in order to reach the top of the mountain; see, e.g., (Sutton &amp; Barto, 2018, Example 10.1).For this experiment, we created a distribution of environments Ξ by randomizing the starting location of the agent, which is chosen uniformly from [−0.6, −0.4].Here, we set T = {4000, 7000, 10000} and Q = {10, 50}.</p>
<p>Recommendation Paths for MC</p>
<p>The subgoal-pairs discovered by BESD are shown in Figure 5b; they tend to be on opposite sides of the agent's starting location, thereby creating back-and-forth movement needed to generate momentum and move up the mountain.It is worth noting that the symmetric behaviors of going from left to right (Behavior 'B' in Figure 5b, for the orange sample path) and going from right to left (Behavior 'A', exhibited by the green, red, and purple sample paths) can both be found in the results of BESD.</p>
<p>Key-Door with Highly Varying Key Locations (KEY2 and KEY3)</p>
<p>In our last experiment, we test for the situation where the distribution of environments Ξ contains environments that might vary dramatically from one another.We also consider how the exploration behavior changes when we add an additional subgoal to the strategy.</p>
<p>In domains KEY2 (with two subgoals) and KEY3 (with three subgoals), we consider a 10 × 10 gridworld with one wall, where a "key" needs to be picked up before opening a closed door at the upper-right corner of the grid.The location of the key, however, is highly varying and is either near the left wall or the right wall.</p>
<p>The environment is visualized in Figures 6a and 6b.We set T = {400, 700, 1000} and Q = {5, 20}.</p>
<p>Recommendation Paths for KEY2/KEY3</p>
<p>It is important that the agent moves in the vicinity of both keys in order for it to perform well across the distribution of environments.We now discuss how this is achieved by the two-and three-subgoal exploration strategies, using the annotations in Figures 6a and 6b.</p>
<p>• Behavior 'A' in KEY2 (Figure 6a): In the first exploration behavior discovered by BESD, the agent is first directed to the right-most key location and then towards the door.This is behavior is reasonable in the sense that the agent's initial location is near the left-most key location; hence, the naive exploration (e.g., ϵ-greedy) "built-in" to RL-ALGO would likely find the key (if it is there) without additional subgoal rewards.</p>
<p>• Behavior 'B' in KEY2 (Figure 6a): The second exploration behavior that we highlight takes a similar approach.This strategy incentivizes the agent to first check the left-most key location (going upwards from the initial location).Interestingly, the second subgoal is neither the other key location nor the goal: instead, the agent is directed toward the upper edge of the environment, slightly right of center.Upon examination, one might conclude that this path compromises between the second key location and the goal.On its way from the first to second subgoal, the agent enters the vicinity of the second key location and also ends up not far from the goal.In other words, the exploration strategy puts the agent in a position such that RL-ALGO's naive exploration is more likely to be successful.</p>
<p>• Behavior 'A' in KEY3 (Figure 6b): With an additional subgoal to work with, BESD is able to find more flexible exploration strategies.For behavior 'A', we see that the first subgoal is near the left-most key location, the second subgoal indirectly leads the agent toward the vicinity of the right-most key location, and the third subgoal is at the goal.The placement of the second subgoal is reminiscent of behavior 'B' of KEY2, but this time, a third subgoal allows BESD to directly lead the agent towards the goal</p>
<p>• Behavior 'B' in KEY3 (Figure 6b): This strategy is more intuitive (indeed, more replications converge to behavior 'B' than behavior 'A') and leads the agent to check each of the possible key locations (the closer one first) and then sends the agent directly toward the goal.</p>
<p>Baseline Algorithms</p>
<p>Given the somewhat unique positioning of the BESD framework, it is important for us to compare against from several streams of literature.Due to our strong focus on cost-efficiency, non-gradient-based approaches from the BO literature are particularly relevant.Two of the most common approaches are expected improvement (Močkus, 1975;Jones et al., 1998) and lower confidence bound (LCB) (Cox &amp; John, 1992;Srinivas et al., 2010).Expected improvement (EI) allocates one sample in each round, selecting a point that maximizes the expected improvement beyond currently sampled points:
EI(θ) = E n min{y 1 , . . . , y n } − y n+1 (θ, τ max ) + .
In each iteration, we evaluate the EI selection using τ max iterations.LCB controls the exploration-exploitation trade-off using a "bonus term" proportional to the standard deviation at each point:
LCB(θ) = µ n (θ, τ max ) − κ k n ((θ, τ max ), (θ, τ max )).
The parameter κ is set to 2. Both EI and LCB are implemented using the GPyOpt package González (2016).</p>
<p>As a sanity check, we also compare against a baseline where the subgoals are randomly selected at each iteration (RND), implemented using Latin hypercube sampling (Stein, 1987).</p>
<p>We also compare against two "default RL" baselines, that do not incorporate an aspect of tuning the exploration strategy.The first baseline is the Q-learning algorithm (QL) (Watkins, 1989) with no subgoals or reward shaping: that is, we directly run QL on environment ξ N for τ max interactions.The second one is a heuristic based on the approximate Q-values learned by QL, which we call "transfer" Q-learning (TQL): for the test instance, we initialize the Q-values using the previously stored results from a randomly chosen training Total cost 1e7  environment.This heuristic is inspired by the idea of policy reuse proposed in Fernández et al. (2010) for transferring learned strategies to new tasks.</p>
<p>An alternative to applying BO or bandit algorithms to hyperparameter optimization is the idea of adaptive configuration evaluation, which focuses on improving the throughput of configuration evaluation by quickly eliminating ones that are not promising.From this line of thinking, the Hyperband algorithm (HB) of Li et al. (2017) stands out as a popular and representative approach.It treats hyperparameter optimization as a pure-exploration infinite-armed bandit problem; it uses sophisticated techniques for adaptive resource allocation and early-stopping to concentrate its learning efforts on promising designs.Setting η = 3 (the default value) and R = 81, HB consists of ⌊log η R⌋ rounds.The first round starts with R samples of subgoal designs θ from a Latin hypercube sample.Following HB's motivation of early-stopping unpromising designs, each θ is evaluated for τ min steps.The best 1/η-fraction designs are kept for the next round.In round i, Hyperband samples R/η i−1 subgoal designs to evaluate for τ min η i−1 steps.</p>
<p>A detailed empirical comparison of BESD to baseline algorithms for all environments are given in Figure 7.The purpose of each numerical experiment is to show that BESD is able to, in a cost-efficient manner (where cost is defined as the number of environment interactions), produce exploration strategies that lead to policies that perform well in a randomly drawn test environment.For each replication, to assess the performance at a particular point in the process, we take its latest recommendation and test it by averaging its performance on a random sample of 200 test MDPs (i.e., ξ N ).The x-axis is the cumulative cost, which includes the initial sampling cost.The y-axis is typically the log regret (lower is better), where regret is defined as the number of additional steps needed to reach the goal when compared to the optimal policy.The exception is the TR domain, where the y-axis is the discounted reward (higher is better), since in TR, the performance is measured by both reward and steps.</p>
<p>Takeaways from Baseline Comparisons in Figure 7</p>
<p>We now offer some observations and takeaways from the performance plots of Figures 7a-7f, where BESD is compared to a variety of baseline approaches.In these figures, the x-axis shows the total cost, defined to be the total number of environment interactions, and the y-axis shows the performance measure (either regret or reward, depending on the environment) when averaged across environments drawn from Ξ.In all environments, BESD (in red) achieves either the lowest regret or highest reward.</p>
<ol>
<li>
<p>Sanity checks.The methods RND, QL, and TQL tend to perform poorly across all domains.This suggests that subgoal-free methods (QL and TQL) are unable to achieve cost-efficient exploration.At the same time, the results of RND suggest that when using subgoals, they must be carefully selected (i.e., exploration based on random subgoals does not perform well).</p>
</li>
<li>
<p>Comparison to Hyperband.HB is reasonably competitive against BESD on two of the easier domains, GW10 and TR.In particular, we notice that HB tends to have good performance early on (as it is able to use early stopping to quickly eliminate inferior subgoal strategies).However, as the interaction budget grows, we see that in most domains, BESD is eventually able to make better use of its evaluations, likely explained by BESD's use of a tailored surrogate model.</p>
</li>
</ol>
<p>Comparison to other BO methods.</p>
<p>The popular BO methods EI and LCB tend to perform similarly to each other in all domains.Compared to BESD, however, they are less cost-efficient.Since all three approaches make use of underlying GP surrogate models, but EI and LCB are constrained in always using q max τ max interactions, this is evidence that being able to reduce the episode lengths and the number of replications is valuable.</p>
<ol>
<li>Impact of more subgoals.Lastly, we point out that Figures 7e and 7f show that although a two-subgoal exploration strategy achieves better results than the baselines, a three-subgoal strategy performs even better.This demonstrates the benefit of expanding the dimension of the parameterization in certain environments.Choosing the number of subgoals to use in a particular set of environments is not an exact science; in general, a higher dimensional subgoal parameterization makes the BO meta-optimization problem more challenging and each acquisition function optimization is also more time-consuming.We recommend the following guidelines: (1) Consider the total interaction budget across all training iterations.A rule-of-thumb is that a d-dimensional subgoal parameterization should have 2d − 1 random initial points.The interaction cost of the initial points should be less than 1/3 of the total budget in order to give BESD adequate time to make progress (if the cost of initial points is too high, then one might want to reduce d).</li>
</ol>
<p>(2) Optimizing the acquisition function becomes more time consuming as d increases, so d should be small enough such that (7) can be computed in one's allotted per-iteration time budget for acquisition function optimization.</p>
<p>Dynamic Subgoal Exploration Strategy vs. Learning From Scratch at Test Time</p>
<p>In Section 5, Figures 4, 5, and 6 gave visual intuition about the types of exploration behaviors that were discovered by BESD.In this section, we show how the final dynamic subgoal strategy θ N rec recommended by BESD is able to speed up learning in the test environment, by comparing it to "learning from scratch" (i.e., running RL directly in the sparse reward environment, with no subgoals).Let
π T ξ = RL-ALGO T, M ξ and π T ξ, θ N rec = RL-ALGO T, M ξ, θ N rec
be the policy learned using RL-ALGO on the original, sparse-reward environment (i.e., no subgoals) and the policy learned by RL-ALGO with the aid of the subgoal strategy found by BESD after T test-time interactions.</p>
<p>The performance ratio that we are interested in is performance ratio
(T ) = E V π T ξ,θ N rec (s 0 ) / E V π T ξ (s 0 ) ,
which, stated simply, represents the ratio "performance with subgoals / performance without subgoals."On GW10, GW20, MC, KEY2, and KEY3, a smaller performance ratio indicates a more effective exploration strategy.For TR, we measure performance using rewards instead of costs, so a larger performance ratio is better.Table 1 displays the performance ratios as a function of the number of interactions used in the test environment.We can see that an optimized exploration strategy corresponds to dramatic improvements, ranging from roughly 3x in the worst cases (MC, KEY2, and KEY3) to nearly 20x in the best cases (GW10, GW20, and TR).Note that due to the varying difficulty between environments, we use a scaling factor m to show how the performance improves with additional cost.The takeaway here is that using a good exploration strategy can lead to dramatic improvements at test-time.</p>
<p>Conclusion and Future Work</p>
<p>The problem of finding exploration strategies for a distribution of environments with a strong focus on cost-awareness during training has not been adequately studied in the literature.This can be a deterrent to applying RL in real-world settings where interactions with the environment are limited and expensive (and where cheap simulators are not available).This paper proposes a solution based on Bayesian optimization; in a cost-aware manner, our approach finds subgoals with an intrinsic shaped reward that aids the agent in scenarios with sparse and delayed rewards, thereby reducing the number of interactions needed to obtain a good solution.</p>
<p>We hope that this approach can help RL become more applicable in real world settings.An experimental evaluation demonstrates that BESD achieves considerably better solutions than a comprehensive field of baseline methods on a variety of benchmark problems.Moreover, an examination of its "recommendation paths" shows that BESD discovers solutions that induce interesting exploration strategies.There are several exciting directions for extending this paper:</p>
<p>• Richer BO formulations.Extensions to the BO formulation could be made in various ways.For example, one interesting direction is to allow the acquisition function to determine the number of subgoals as an additional lever.Based on a few informal observations, such a formulation is likely only interesting in settings where more subgoals incur additional experimentation cost.8Alternatively, the acquisition function itself could be extended with additional features, such as encouraging successive subgoal evaluations to be nearby previous ones (i.e., to reduce setup cost) or the ability to reason about (known) symmetries in the domain.Such advanced features might be enabled by dynamic programming formulations of the BO problem itself, which can be tackled using multi-step lookahead BO (Lam et al., 2016;González et al., 2016;Jiang et al., 2020;Lee et al., 2020).Other possiblities include the ability to handle expensive-to-evaluate constraints (Gardner et al., 2014;Gelbart et al., 2014;Letham et al., 2019) or total cost budgets (Astudillo et al., 2021;Lee et al., 2021).</p>
<p>• Case study in an application domain.Our experiments gave proof-of-concept results on benchmarks where the RL training itself did not use prohibitive amounts of computation, in order for us to stay within a reasonable computational budget.This is because statistically distinguishable results for baseline algorithms require many replications of the meta-optimization problem (i.e., the BO routines), each of which require many iterations of RL training.One immediate area of future work is to "productionize" the dynamic subgoal exploration strategies in a real-world application involving a navigation task.</p>
<p>• The task-aware setting.Finally, our problem formulation does not include "labels" for environments, as our setting is concerned with case of exogenous variation in the environments, but otherwise the same task.The situation often studied in the multi-task RL setting, however, often comes with task identifiers, where the agent knows that it is operating in particular task.An extension to this setting might be useful for certain applications, where exploration strategies that are good for one task (e.g., biking through an environment) are also useful for other tasks (e.g., walking through the same environment).</p>
<p>for all θ ′′ , θ ′′′ ∈ Θ.Moreover, since θ was chosen from Θ, we know that λ( θ, τ max , q) + k ∞ ( θ, τ max ), ( θ, τ max ) &gt; 0, and hence k ∞ (θ ′′′ , τ max ), ( θ, τ max ) = k ∞ (θ ′′ , τ max ), ( θ, τ max ) for all θ ′′ , θ ′′′ ∈ Θ.</p>
<p>This means the covariance matrix of {f (θ, τ max ) | θ ∈ Θ} is proportional to the all-ones matrix, and that draws from f (θ, τ max ) − µ (∞) (θ, τ max ) are constant across θ ∈ Θ.Therefore, arg max θ∈ Θ µ (∞) (θ, τ max ) = arg max θ∈ Θ f (θ, τ max ) and the statement of the theorem holds.</p>
<p>A.3 Proof of Theorem 2</p>
<p>In Theorem 2, we establish an additive bound on the loss of the solution obtained by BESD, f ( θ, τ max ), with respect to the unknown optimum f (θ OPT , τ max ), as the number of iterations N → ∞.Recall that we suppose µ(θ, τ ) = 0 for all θ, τ , and that the kernel k(•, •) has continuous partial derivatives up to the fourth order.According to Theorem 3.2 of Lederer et al. (2019), for any δ ∈ (0, 1], with probability at least 1 − δ, the quantity
∥L δ ∥ = (L 1 δ , L 2 δ , • • • , L m δ
) is a Lipschitz constant of f on Θ , i.e., it holds that
|f (θ, τ max ) − f (θ ′ , τ max )| ≤ ∥L δ ∥ • dist(θ, θ ′ ),
where θ, θ ′ ∈ Θ.By the definition of d, there exists a θ ∈ Θ such that dist( θ, θ OPT ) ≤ d.Therefore, it follows that the suboptimality due to optimizing in Θ is bounded by
f (θ OPT , τ max ) − f ( θ, τ max ) ≤ ∥L δ ∥ • d. (20)
Theorem 1 completes the proof of Theorem 2 since (20) holds with probability 1 − δ.</p>
<p>B GP Hyperparameter Estimation</p>
<p>The hyperparameters of the covariance function k are set via maximum a posteriori (MAP) estimation.Recall that a MAP estimate is the mode under the log-posterior obtained as the sum of the log-marginal likelihood of the observations and the logarithm of the probability under a hyper-prior.We focus on describing the hyper-prior, since the log-marginal likelihood follows canonically; see (Rasmussen &amp; Williams, 2006, Ch. 5) for details.The proposed prior extends the hyper-prior for the multi-task GP model used in Poloczek et al. (2017).We set the mean function µ and the noise function λ to constants that we estimate.For the covariance function we need to estimate d + 5 hyperparameters: the signal variance, one length scale for every subgoal parameter in θ and the four parameters associated with k τ .We suppose a normal prior for these parameters.</p>
<p>For the signal variance, the prior mean is given by the variance of the observations, after subtracting the above estimate for the observational noise.Here we use the independence of observational noise that we argued in Section 3.5.For any length scale, we set the prior mean to the size of the interval that the associated parameter is chosen in.Having determined a prior mean µ ψ for each hyperparameter ψ, we may then set the variance of the normal prior to σ 2 ψ = (µ ψ /2) 2 .</p>
<p>Figure 1 :
1
Figure1: Example of a dynamic subgoal exploration strategy.The first, second, and third subgoals are denoted by the circle, triangle, and cross, respectively.The blue square is the starting location of the agent, the grey region is a wall, the yellow region is the location of the key, and the red region is the door (goal).Note that although the second subgoal is not exactly in the location of the key, it brings the agent to the correct vicinity, allowing the underlying RL algorithm (executed online) to further adapt to the environment's particular details.</p>
<p>Figure 2 :
2
Figure 2: Outline of the BESD algorithm.During the training phase BESD optimizes an exploration strategy (represented as subgoals) on sampled training environments.It then utilizes the learned subgoal design as an exploration strategy in the test environment to train an effective policy within a limited number of interactions.</p>
<p>20 × 20 Gridworld Environment (b) Goal and Subgoal Rewards</p>
<p>Figure 3 :
3
Figure 3: An example that visualizes an environment and a random dynamic subgoal exploration strategy along with the rewards of the associated subgoal-augmented MDP.</p>
<p>Figure 4 :
4
Figure4: Recommendation paths for GW10 and GW20.The blue and red shaded regions denote the starting points and goals, respectively.Dark and light gray regions possible locations of walls and doors, respectively.Each plot displays four realizations of the "recommendation paths" of BESD.Each color corresponds to one sample realization, and the color becomes darker as n increases, with the lightest points being the initial samples.The circles and triangles represent the first and second subgoals, respectively, of the exploration strategy.The 'A' and 'B' labels point out two example sets of subgoals displaying notable behaviors.</p>
<p>Figure 5 :
5
Figure5: Recommendation paths for TR and MC.The first panel, Figure5a, largely follows the same design as Figures4a and 4b, except that the green squares represent possible location of the treasure.In the second panel, Figure5b, since the location of the mountain-car is one-dimensional, we visualize the four recommendation paths by spacing them vertically to avoid crowding (therefore, the vertical axis represents different trajectories, each shown in a different color).The initial location of the car is colored in blue, while the goal is in red, corresponding to the overlay of the mountain.See the caption of Figure4for an explanation of the symbols used.</p>
<p>Figure 6 :
6
Figure6: Recommendation paths.The blue and red shaded regions denote the starting points and goals, respectively.Dark and light gray regions possible locations of walls and doors, respectively.Each plot displays four realizations of the "recommendation paths" of BESD.Each color corresponds to one sample realization, and the color becomes darker as n increases, with the lightest points being the initial samples.The circles, triangles, and crosses represent the first, second, and third subgoals, respectively.The 'A' and 'B' labels point out two example sets of subgoals displaying notable behaviors.</p>
<p>Figure 7 :
7
Figure 7: Performance as a function of the total training costs.The curves are averaged over 50 replications of the meta-optimization problem and the error bars indicate ± 2 standard errors of the mean.Note that the curves associated with the BO methods, BESD, LCB, EI, start later due to the use of a set of initial points for initializing the GP model.</p>
<p>Table 1 :
1
Performance ratios as a function of interactions in the test environment.GW10, GW20 TR, MC, KEY2, and KEY3 are evaluated every m = 100, 1000, 200, 1000, 500, 500 steps respectively.
τGW10 GW20TRMCKEY2 KEY3m0.4580.7790.4360.980 1.4561.0252m0.2180.4922.8231.048 0.7360.9403m0.0860.2342.8230.949 1.2770.6984m0.0800.2240.9170.896 0.7040.7885m0.0700.1086.7230.987 1.3550.5316m0.0860.0888.9390.878 0.8560.5037m0.0800.0689.9081.077 0.9200.6238m0.0870.07510.216 0.877 0.8830.5329m0.0690.059 23.2936 0.512 0.2320.56610m 0.0690.05818.011 0.354 0.3320.361
Discretizing the domain is a common computational technique used when optimizing complex acquisition functions, but we improve upon the existing theoretical analysis inPoloczek et al. (2017) with an explicit characterization of the induced error.
In Section
.3, we describe how a dynamic subgoal exploration strategy supplements the extrinsic reward function with additional intrinsic rewards.3 We allow for the case of episodic MDPs, where γ = 1, provided that any policy will reach a terminal state with probability one. A terminal state is absorbing and any action taken in that state gives zero reward.
Note that our approach also applies to the case of a single environment if the distribution contains only one environment.
Without ordering, rewards from multiple subgoals can inhibit the agent's progress.
Although the assumption of normality is commonplace in BO for tractability of the posterior(Frazier, 2018), other noise distributions can be used through an appropriate likelihood function.
When the need arises to optimize for high dimensional subgoal parameterizations, one may opt for scalable extensions of the model and optimization formulation; see, e.g.,Wang et al. (2016); Mutny &amp; Krause (2018);Nayebi et al. (2019);Eriksson et al. (2019);Papenmeier et al. (2022;2023). We leave extensions in this direction to future work and focus on a more standard setting.
We ran a small number of informal experiments where we allowed BO to select the number of subgoals, but found that BESD almost immediately gravitates to the largest number of subgoals (as subgoals come at no cost). Since in the applications that we have in mind, subgoal cost was not a primary concern, we did not pursue this direction as it did not bring any particularly strong insights for the standard case.
A ProofsA.1 Restatement of TheoremsHere we restate the two main theorems from the paper for the reader's convenience.Full proofs are provided in the following sections.Theorem 1 The acquisition function described in(7)has the property of asymptotic optimality with respect to Θ, i.e., lim, almost surely.That is, the recommended design θ N rec becomes optimal as N → ∞.Theorem 2 The acquisition function of (7) has bounded asymptotic suboptimality with respect to the original domain Θ in the sense that with probability at least 1 − δ, it holds that) is a measure on the "coarseness" of the discretization and L δ is the vector (L 1 δ , L 2 δ , . . ., L m δ ), with each L i δ defined as in (8).A.2 Proof of Theorem 1The proof is based on theoretical results ofPoloczek et al. (2017).Our result, however, includes the ability to select the number of replications q.Denote λ(θ, τ, q) = σ 2 env + σ 2 rep /q.Also, let F n denote the σ-algebra generated by the history H n .The expectation E n := E[ • |F n ] is taken with respect to F n .Recall that µ n and k n are the mean and covariance matrix of the time n belief on f .Define the quantitiesObserve that Z n+1 is standard normal (conditional on F n ).We have the following recursive updating equation for µ n+1 :and another recursive formula k n+1 :These updating equations are based on the Sherman-Woodbury identity; seeFrazier et al. (2009)for a full derivation.The objective of the acquisition function is thus:We also define the quantityNext, we restate a useful technical lemma fromPoloczek et al. (2017).Lemma 1 (Restatement of Lemma 1 of (Poloczek et al., 2017)) Let τ, τ ′ ∈ T and θ, θ ′ ∈ Θ.The limits of the series {µ n (θ, τ )} n and {V n (θ, τ, θ ′ , τ ′ )} n exist.Denote them by µ ∞ (θ, τ ) and V ∞ (θ, τ, θ ′ , τ ′ ) respectively.We haveFix a sample path ω, which corresponds to a particular path of measurements and observationsBy the finiteness of Θ, T , and Q, there must exist a configuration (θ ′ , τ ′ , q ′ ) that is visited infinitely often on sample path ω.The following lemma states the asymptotic behavior of ν n (θ ′ , τ ′ , q ′ )/(q ′ τ ′ ) for n → ∞ as a function of µ n (•, •) and σnLemma 2 Consider the sample path ω and (θ ′ , τ ′ , q ′ ) described above.Then, on that sample path ω, it holds that limfor every θ ′′ ∈ Θ.Also, the acquisition value tends to zero:for any θ ∈ Θ, τ ∈ T .Then for all θ ′′ ∈ Θ, we haveNote that we made use of the fact that the observation noise λ(θ ′ , τ ′ , q ′ ) &gt; 0 for any q ′ .From the proof of Lemma 1 ofPoloczek et al. (2017), it is shown that for any θ ′′ ∈ Θ, µ n (θ ′′ , τ max ) n and σn q ′ (θ ′′ , τ max ), (θ ′ , τ ′ ) n are uniformly integrable (u.i.) families of random variables that converge almost surely to their respective limits µ ∞ (θ ′′ , τ max ) and σ∞ q ′ (θ ′′ , τ max ), (θ ′ , τ ′ ) = 0. Note that the family of random variables σn q ′ (θ ′′ , τ max ), (θ ′ , τ ′ , ) Z n+1 n is also uniformly integrable since Z n+1 is independent of σn q ′ (θ ′′ , τ max ), (θ ′ , τ ′ ) .Let Z be a standard normal random variable (independent from all other quantities).It holds thatThe first equality is due to (11) and the fact that the operations of summing and taking maximum over a finite set of uniform integrable random variables maintains uniform integrability.From (7), we know that in each iteration n, the configuration (θ n , τ n , q n ) is selected from according to arg max θ,τ,q ν n (θ, τ, q)/(qτ ).Now, for the sake of contradiction, suppose that there exists some configuration ( θ, τ , q) such that lim n→∞ ν n ( θ, τ , q)/(qτ ) &gt; 0. This immediately leads to a contradiction, since then it cannot be the case that (θ ′ , τ ′ , q ′ ) is visited infinitely often.Since the sample path ω was arbitrary, we conclude that lim n→∞ ν n (θ, τ, q)/(qτ ) = 0 a.s.(16) for all θ ∈ Θ, τ ∈ T , and q ∈ Q.Lemma 3 Given that (16) holds, we have thatalmost surely.Proof.We can conclude from (12) and Lemma 1 thatfor all θ ∈ Θ.In the case that the posterior variance k ∞ ((θ, τ max ), (θ, τ max )) = 0 for all θ ∈ Θ, then the maximizer is known perfectly and we are done.If not, then we define Θ = θ ∈ Θ | k ∞ ((θ, τ max ), (θ, τ max )) &gt; 0 and consider some θ ∈ Θ where the posterior variance is positive.Fix any q ∈ Q.We now argue that σ∞ q ( θ, τ max ), ( θ, τ max ) = σ∞ q (θ ′′ , τ max ), ( θ, τ max ) (17) for all θ ′′ ∈ Θ. Suppose, for the sake of contradiction, that there exist some θ 1 , θ 2 ∈ Θ with σ∞ q (θ 1 , τ max ), ( θ, τ max ) ̸ = σ∞ q (θ 2 , τ max ), ( θ, τ max ) .(18)Recall (15) and note that it can be rewritten aswhere h(z) = max θ ′′ ∈ Θ µ ∞ (θ ′′ , τ max ) + σ∞ q ′ (θ ′′ , τ max ), (θ ′ , τ ′ ) z .Since Θ is finite and each function within the maximization in h is affine in z, the h(z) is convex 9 and piecewise linear.Since h is convex, there is an affine function l such that l(0) = h(0), l(z) ≤ h(z) for all z ∈ R.The assumption we made in (18), which effectively says that h is created by taking maximum over affine functions of differing slopes, implies h cannot itself be affine (and indeed, must consist of various "pieces").Therefore, there exists an interval I, either of the form (z 0 , ∞) or (−∞, z 0 ), such that l(z) &lt; h(z) for z ∈ I.It follows that E[l(Z)] &lt; E[h(Z)]. By the linearity of l, we haveThis implies that (19) is strictly positive, contradicting (16).We thus conclude that (17) holds, which is equivalent to k ∞ (θ ′′ , τ max ), ( θ, τ max ) λ( θ, τ max , q) + k ∞ ( θ, τ max ), ( θ, τ max ) = k ∞ (θ ′′′ , τ max ), ( θ, τ max ) λ( θ, τ max , q) + k ∞ ( θ, τ max ), ( θ, τ max ), 9 Pointwise maximum of convex functions is convex.
Surprise-based intrinsic motivation for deep reinforcement learning. Joshua Achiam, Shankar Sastry, International Conference on Learning Representations. 2017</p>
<p>A survey of exploration methods in reinforcement learning. Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke Van Hoof, Doina Precup, arXiv:2109.001572021arXiv preprint</p>
<p>Robotic antarctic meteorite search: Outcomes. Dimitrios S Apostolopoulos, Liam Pedersen, Benjamin N Shamah, Kimberly Shillcutt, Michael D Wagner, William L Whittaker, International Conference on Robotics and Automation. IEEE20014</p>
<p>Multi-step budgeted Bayesian optimization with unknown evaluation costs. Raul Astudillo, Daniel Jiang, Maximilian Balandat, Eytan Bakshy, Peter Frazier, Advances in Neural Information Processing Systems. 342021</p>
<p>The option-critic architecture. Pierre-Luc Bacon, Jean Harb, Doina Precup, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems. G Andrew, Sridhar Barto, Mahadevan, 200313</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in Neural Information Processing Systems. 2016</p>
<p>Optimizing waypoints for monitoring spatiotemporal phenomena. Jonathan Binney, Andreas Krause, Gaurav S Sukhatme, The International Journal of Robotics Research. 3282013</p>
<p>Replication or exploration? sequential design for stochastic simulation experiments. Mickaël Binois, Jiangeng Huang, Robert B Gramacy, Mike Ludkovski, Technometrics. 6112019</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. Eric Brochu, Nando De Vlad M Cora, Freitas, arXiv:1012.25992010arXiv preprint</p>
<p>Moe: A global, black box optimization engine for real world metric optimization. Scott Clark, Eric Liu, Peter Frazier, Jialei Wang, Deniz Oktay, Norases Vesdapunt, 2014</p>
<p>A statistical method for global optimization. D Dennis, Susan Cox, John, International Conference on Systems, Man, and Cybernetics. IEEE1992</p>
<p>Feudal reinforcement learning. Peter Dayan, Geoffrey E Hinton, Advances in Neural Information Processing Systems. 19925</p>
<p>Multi-task policy search for robotics. Marc Peter Deisenroth, Peter Englert, Jan Peters, Dieter Fox, International Conference on Robotics and Automation. 2014</p>
<p>Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. Tobias Domhan, Jost Tobias Springenberg, Frank Hutter, International Joint Conferences on Artificial Intelligence. 201515</p>
<p>Hidden parameter Markov decision processes: A semiparametric regression approach for discovering latent task parametrizations. Finale Doshi, - Velez, George Konidaris, International Joint Conferences on Artificial Intelligence. NIH Public Access20161432</p>
<p>Scalable global optimization via local Bayesian optimization. David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, Matthias Poloczek, Advances in Neural Information Processing Systems. 2019</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, International Conference on Machine Learning. PMLR2018</p>
<p>Diversity is all you need: Learning skills without a reward function. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, International Conference on Learning Representations. 2018</p>
<p>An autonomous robotic system for mapping abandoned mines. David Ferguson, Aaron Morris, Dirk Haehnel, Christopher Baker, Zachary Carlos Reverte, Scott Thayer, Charles Whittaker, William Whittaker, Wolfram Burgard, Advances in Neural Information Processing Systems. 2004</p>
<p>Probabilistic policy reuse for inter-task transfer learning. Fernando Fernández, Javier García, Manuela Veloso, Robotics and Autonomous Systems. 5872010</p>
<p>Initializing Bayesian hyperparameter optimization via meta-learning. Matthias Feurer, Jost Tobias Springenberg, Frank Hutter, Association for the Advancement of Artificial Intelligence. 2015</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. JMLR. org2017a</p>
<p>One-shot visual imitation learning via meta-learning. Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine, Conference on Robot Learning. 2017b</p>
<p>Meta learning shared hierarchies. Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, John Schulman, International Conference on Learning Representations. 2018</p>
<p>The knowledge-gradient policy for correlated normal beliefs. Peter Frazier, Warren Powell, Savas Dayanik, INFORMS Journal on Computing. 2142009</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on Bayesian optimization. 2018arXiv preprint</p>
<p>A knowledge-gradient policy for sequential information collection. Warren B Peter I Frazier, Savas Powell, Dayanik, SIAM Journal on Control and Optimization. 4752008</p>
<p>Robust policy search for robot navigation. Javier Garcia, -Barcos , Ruben Martinez-Cantin, IEEE Robotics and Automation Letters. 622021</p>
<p>Bayesian optimization with inequality constraints. Matt J Jacob R Gardner, Zhixiang Kusner, Eddie Xu, Kilian Q Weinberger, John P Cunningham, International Conference on Machine Learning. 20142014</p>
<p>Bayesian Optimization. Roman Garnett, 2023Cambridge University Press</p>
<p>Jasper Michael A Gelbart, Ryan P Snoek, Adams, arXiv:1403.5607Bayesian optimization with unknown constraints. 2014arXiv preprint</p>
<p>Posterior consistency of Gaussian process prior for nonparametric binary regression. Subhashis Ghosal, Anindya Roy, The Annals of Statistics. 3452006</p>
<p>Subgoal discovery for hierarchical reinforcement learning using learned policies. Sandeep Goel, Manfred Huber, FLAIRS Conference. 2003</p>
<p>GPyOpt: A Bayesian optimization framework in Python. González, 2016</p>
<p>Glasses: Relieving the myopia of bayesian optimisation. Javier González, Michael Osborne, Neil Lawrence, Artificial Intelligence and Statistics. PMLR2016</p>
<p>Deep learning for reward design to improve Monte Carlo tree search in ATARI games. Xiaoxiao Guo, Satinder Singh, Richard Lewis, Honglak Lee, International Joint Conference on Artificial Intelligence. 2016</p>
<p>Meta-reinforcement learning of structured exploration strategies. Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, Sergey Levine, Advances in Neural Information Processing Systems. 2018</p>
<p>Efficient search of compositional space for hybrid organic-inorganic perovskites via Bayesian optimization. Weici Henry C Herbol, Peter Hu, Paulette Frazier, Matthias Clancy, Poloczek, NPJ Computational Materials. 41512018</p>
<p>Multi-task deep reinforcement learning with PopArt. Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, Hado Van Hasselt, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Novelty and reinforcement learning in the value system of developmental robots. Xiao Huang, John Weng, 2nd International Workshop on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems. Lund University Cognitive Studies. 2002</p>
<p>Efficient nonmyopic Bayesian optimization via one-shot multi-step trees. Shali Jiang, Daniel Jiang, Maximilian Balandat, Brian Karrer, Jacob Gardner, Roman Garnett, Advances in Neural Information Processing Systems. 202033</p>
<p>Efficient global optimization of expensive black-box functions. Matthias Donald R Jones, William J Schonlau, Welch, Journal of Global Optimization. 1341998</p>
<p>Abstract value iteration for hierarchical reinforcement learning. Kishor Jothimurugan, Osbert Bastani, Rajeev Alur, International Conference on Artificial Intelligence and Statistics. PMLR2021</p>
<p>Maximizing learning progress: An internal reward system for development. Frédéric Kaplan, Pierre-Yves Oudeyer, Embodied Artificial Intelligence. Springer2004</p>
<p>Near-optimal reinforcement learning in polynomial time. Michael Kearns, Satinder Singh, Machine Learning. 200249</p>
<p>Fast Bayesian optimization of machine learning hyperparameters on large datasets. Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter, Artificial Intelligence and Statistics. 2017</p>
<p>Autonomous shaping: Knowledge transfer in reinforcement learning. George Konidaris, Andrew Barto, International Conference on Machine Learning. ACM2006</p>
<p>Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Karthik Tejas D Kulkarni, Ardavan Narasimhan, Josh Saeedi, Tenenbaum, Advances in Neural Information Processing Systems. 292016</p>
<p>Bayesian optimization with a finite budget: An approximate dynamic programming approach. Remi Lam, Karen Willcox, David H Wolpert, Advances in Neural Information Processing Systems. 292016</p>
<p>Playing FPS games with deep reinforcement learning. Guillaume Lample, Devendra Singh, Chaplot , Association for the Advancement of Artificial Intelligence. 2017</p>
<p>Uniform error bounds for Gaussian process regression with application to safe control. Armin Lederer, Jonas Umlauft, Sandra Hirche, Advances in Neural Information Processing Systems. 2019</p>
<p>Efficient rollout strategies for Bayesian optimization. Eric Lee, David Eriksson, David Bindel, Bolong Cheng, Mike Mccourt, Conference on Uncertainty in Artificial Intelligence. PMLR2020</p>
<p>A nonmyopic approach to costconstrained bayesian optimization. Eric Hans, Lee , David Eriksson, Matthias Valerio Perrone, Seeger, Uncertainty in Artificial Intelligence. PMLR2021</p>
<p>Constrained Bayesian optimization with noisy experiments. Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, Bayesian Analysis. 1422019</p>
<p>Learning multi-level hierarchies with hindsight. Andrew Levy, George Konidaris, Robert Platt, Kate Saenko, International Conference on Learning Representations. 2018</p>
<p>Hyperband: A novel bandit-based approach to hyperparameter optimization. Lisha Li, Kevin Jamieson, Giulia Desalvo, Afshin Rostamizadeh, Ameet Talwalkar, The Journal of Machine Learning Research. 1812017</p>
<p>Dynamic abstraction in reinforcement learning via clustering. Shie Mannor, Ishai Menache, Amit Hoze, Uri Klein, International Conference on Machine Learning. 200471</p>
<p>Active policy learning for robot planning and exploration under uncertainty. Ruben Martinez-Cantin, Nando De Freitas, Arnaud Doucet, José A Castellanos, Robotics: Science and systems. 20073</p>
<p>A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Ruben Martinez-Cantin, Nando De Freitas, Eric Brochu, José Castellanos, Arnaud Doucet, Autonomous Robots. 272009</p>
<p>Mars microrover navigation: Performance evaluation and enhancement. Larry Matthies, Erann Gat, Reid Harrison, Brian Wilcox, Richard Volpe, Todd Litwin, Autonomous Robots. 241995</p>
<p>Automatic discovery of subgoals in reinforcement learning using diverse density. Amy Mcgovern, Andrew G Barto, International Conference on Machine Learning. 2001</p>
<p>An experimental design perspective on model-based reinforcement learning. Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger, International Conference on Learning Representations. 2021</p>
<p>On Bayesian methods for seeking the extremum. Jonas Močkus, Optimization Techniques IFIP Technical Conference. Springer1975</p>
<p>Mojmir Mutny and Andreas Krause. Efficient high dimensional Bayesian optimization with additivity and quadrature fourier features. Philippe Morere, Fabio Ramos, Advances in Neural Information Processing Systems. 2018. 201831Conference on Robot Learning</p>
<p>Data-efficient hierarchical reinforcement learning. Ofir Nachum, Shane Shixiang, Honglak Gu, Sergey Lee, Levine, Advances in Neural Information Processing Systems. 201831</p>
<p>A framework for Bayesian optimization in embedded subspaces. Amin Nayebi, Alexander Munteanu, Matthias Poloczek, International Conference on Machine Learning. 2019</p>
<p>Policy invariance under reward transformations: Theory and application to reward shaping. Daishi Andrew Y Ng, Stuart Harada, Russell, International Conference on Machine Learning. 199999</p>
<p>Bayesian optimisation for safe navigation under localisation uncertainty. Rafael Oliveira, Lionel Ott, Vitor Guizilini, Fabio Ramos, Robotics Research. Springer2020</p>
<p>Why is posterior sampling better than optimism for reinforcement learning. Ian Osband, Benjamin Van Roy, International Conference on Machine Learning. 2017</p>
<p>Generalization and exploration via randomized value functions. Ian Osband, Benjamin Van Roy, Zheng Wen, International Conference on Machine Learning. 2016</p>
<p>Human-interactive subgoal supervision for efficient inverse reinforcement learning. Xinlei Pan, Eshed Ohn-Bar, Nicholas Rhinehart, Yan Xu, Yilin Shen, Kris M Kitani, arXiv:1806.084792018arXiv preprint</p>
<p>Increasing the scope as you learn: Adaptive Bayesian optimization in nested subspaces. Leonard Papenmeier, Luigi Nardi, Matthias Poloczek, Advances in Neural Information Processing Systems. 202235</p>
<p>Bounce: a reliable bayesian optimization algorithm for combinatorial and mixed spaces. Leonard Papenmeier, Luigi Nardi, Matthias Poloczek, arXiv:2307.006182023arXiv preprint</p>
<p>Hierarchical reinforcement learning: A comprehensive survey. Budhitama Shubham, Ah-Hwee Subagdja, Chai Tan, Quek, ACM Computing Surveys (CSUR). 5452021</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, International Conference on Machine Learning. 20172017</p>
<p>Sujoy Paul, Jeroen Van Baar, Amit K Roy-Chowdhury , arXiv:1911.07224Learning from trajectories via subgoal discovery. 2019arXiv preprint</p>
<p>A nonstationary space-time Gaussian process model for partially converged simulations. Victor Picheny, David Ginsbourger, SIAM/ASA Journal on Uncertainty Quantification. 112013</p>
<p>Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning. Marc Pickett, Andrew G Barto, International Conference on Machine Learning. 200219</p>
<p>Learning to push by grasping: Using multiple tasks for effective learning. Lerrel Pinto, Abhinav Gupta, IEEE international conference on robotics and automation. 2017. 2017IEEE</p>
<p>Multi-information source optimization. Matthias Poloczek, Jialei Wang, Peter Frazier, Advances in Neural Information Processing Systems. 2017</p>
<p>Theoretical results on reinforcement learning with temporally abstract options. Doina Precup, Richard S Sutton, Satinder Singh, European conference on machine learning. Springer1998</p>
<p>Learning to drive a bicycle using reinforcement learning and shaping. Jette Randløv, Preben Alstrøm, International Conference on Machine Learning. Citeseer199898</p>
<p>Carl Edward Rasmussen, Christopher K Ĩ Williams, Gaussian Processes for Machine Learning. MIT Press2006</p>
<p>Learning to optimize via posterior sampling. Daniel Russo, Benjamin Van Roy, Mathematics of Operations Research. 3942014</p>
<p>The correlated knowledge gradient for simulation optimization of continuous parameters using Gaussian process regression. Warren Scott, Peter Frazier, Warren Powell, SIAM Journal on Optimization. 2132011</p>
<p>Model-based active exploration. Pranav Shyam, Wojciech Jaśkowski, Faustino Gomez, International Conference on Machine Learning. PMLR2019</p>
<p>Using relative novelty to identify useful temporal abstractions in reinforcement learning. Özgür Şimşek, Andrew G Barto, International Conference on Machine Learning. 200495</p>
<p>An intrinsic reward mechanism for efficient exploration. Özgür Şimşek, Andrew G Barto, International Conference on Machine Learning. 2006</p>
<p>Identifying useful subgoals in reinforcement learning by local graph partitioning. Özgür Şimşek, Alicia P Wolfe, Andrew G Barto, International Conference on Machine Learning. 2005</p>
<p>Practical Bayesian optimization of machine learning algorithms. Jasper Snoek, Hugo Larochelle, Ryan P Adams, Advances in Neural Information Processing Systems. 2012</p>
<p>Reward design via online gradient ascent. Jonathan Sorg, Richard L Lewis, Satinder, Singh, Advances in Neural Information Processing Systems. 2010</p>
<p>Gaussian process optimization in the bandit setting: No regret and experimental design. Niranjan Srinivas, Andreas Krause, Sham Kakade, Matthias Seeger, International Conference on Machine Learning. 2010</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. Sergey Bradly C Stadie, Pieter Levine, Abbeel, arXiv:1507.008142015arXiv preprint</p>
<p>Large sample properties of simulations using latin hypercube sampling. Michael Stein, Technometrics. 2921987</p>
<p>Learning options in reinforcement learning. Martin Stolle, Doina Precup, International Symposium on abstraction, reformulation, and approximation. Springer2002</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Doina Richard S Sutton, Satinder Precup, Singh, Artificial intelligence. 1121-21999</p>
<p>Multi-task Bayesian optimization. Kevin Swersky, Jasper Snoek, Ryan P Adams, Advances in Neural Information Processing Systems. 2013</p>
<p>Kevin Swersky, Jasper Snoek, Ryan Prescott, Adams , arXiv:1406.3896Freeze-thaw Bayesian optimization. 2014arXiv preprint</p>
<h1>exploration: A study of count-based exploration for deep reinforcement learning. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Openai Xi Chen, Yan Duan, John Schulman, Filip Deturck, Pieter Abbeel, Advances in Neural Information Processing Systems. 2017</h1>
<p>Dynamic reward shaping: Training a robot by voice. Ana C Tenorio-Gonzalez, Eduardo F Morales, Luis Villaseñor-Pineda, Ibero-American Conference on Artificial Intelligence. Springer2010</p>
<p>Adapting control policies for expensive systems to changing environments. Matthew Tesch, Jeff Schneider, Howie Choset, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE2011</p>
<p>Autonomous exploration and mapping of abandoned mines. Sebastian Thrun, Scott Thayer, William Whittaker, Christopher Baker, Wolfram Burgard, David Ferguson, Dirk Hahnel, Aaron Montemerlo, Zachary Morris, Omohundro, Robotics &amp; Automation Magazine. 1142004</p>
<p>Discovery of options via meta-learned subgoals. Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, David Hado P Van Hasselt, Satinder Silver, Singh, Advances in Neural Information Processing Systems. 342021</p>
<p>Strategic attentive writer for learning macro-actions. Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou, Advances in Neural Information Processing Systems. 292016</p>
<p>Nelson Vithayathil, Varghese Qusay, H Mahmoud, A survey of multi-task deep reinforcement learning. Electronics. 202091363</p>
<p>Bayesian optimization in a billion dimensions via random embeddings. Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando De Feitas, Journal of Artificial Intelligence Research. 552016</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 83-41992</p>
<p>Learning from delayed rewards. Christopher John, Cornish Hellaby Watkins, 1989CambridgePhD thesisKing's College</p>
<p>Multi-task reinforcement learning: A hierarchical Bayesian approach. Aaron Wilson, Alan Fern, Soumya Ray, Prasad Tadepalli, International Conference on Machine Learning. ACM2007</p>
<p>The parallel knowledge gradient method for batch bayesian optimization. Jian Wu, Peter Frazier, Advances in Neural Information Processing Systems. 292016</p>
<p>Bayesian optimization with gradients. Jian Wu, Matthias Poloczek, Andrew G Wilson, Peter Frazier, Advances in Neural Information Processing Systems. 2017</p>
<p>Hierarchical reinforcement learning by discovering intrinsic options. Jesse Zhang, Haonan Yu, Wei Xu, International Conference on Learning Representations. 2020</p>
<p>On learning intrinsic rewards for policy gradient methods. Zeyu Zheng, Junhyuk Oh, Satinder Singh, Advances in Neural Information Processing Systems. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4490 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4490</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4490</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-271244415</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.19094v2.pdf" target="_blank">In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery</a></p>
                <p><strong>Paper Abstract:</strong> State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored. In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR. We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer. ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors. Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4490.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4490.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-Context Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that leverages large language models (LLMs) via in-context learning and a meta-prompt (OPRO-style) optimization loop to propose symbolic functional forms; coefficients are fitted by an external non-linear least squares optimizer and proposals are iteratively refined with a complexity-penalized fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-Context Symbolic Regression (ICSR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ICSR uses a foundation LLM to generate symbolic 'skeleton' functions from a prompt that contains a small set of observed (x,y) points and previous candidate functions with their fitness scores (meta-prompt). The pipeline: (1) generate multiple seed function skeletons from the LLM, (2) fit unknown numeric coefficients of each skeleton with an external Non-Linear Least Squares optimizer (SciPy), (3) evaluate fitness using a combined normalized MSE and complexity penalty, (4) keep the top-k candidates, append them to the prompt, and iterate (OPRO-style) until convergence or budget exhaustion. The LLM only proposes forms; numeric fitting and selection are external. The method uses temperature scheduling, multiple samples per LLM call, and repeated coefficient restarts to avoid local minima.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic regression / general quantitative function discovery (benchmarks from machine learning; intended applicability to scientific equation discovery in physical/scientific domains)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic mathematical equations (explicit functional relationships discovered via symbolic regression)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples reported in the paper include recovered/suggested symbolic forms such as f(x) = 1.00 x^{0.4983} (approx sqrt(x)) and ICSR-recovered polynomial-like approximations such as f(x) = 0.50(x + 0.50)^2 - 0.12 for benchmark examples; ground-truth benchmark examples include functions like f(x) = x(x+1)^2.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>In-context LLM prompting with examples of data and previous candidate equations (meta-prompt / OPRO). The LLM outputs symbolic skeletons; coefficients are numerically optimized with Non-Linear Least Squares; fitness uses NMSE combined with an exponential complexity penalty; iterative refinement keeps best candidates in context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Empirical evaluation on standard symbolic regression benchmarks (Nguyen, Constant, R, Keijzer). Metrics computed on held-out test grids (in-domain and out-of-distribution extrapolation ranges). Repeated across random seeds; compared against baselines (gplearn, DSR/uDSR, NeSymReS, E2E, TPSR). Out-of-distribution tested by expanding input ranges and measuring R^2 and failure fraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include coefficient of determination R^2 and expression complexity C (node count). Example aggregated results reported: overall average R^2 ≈ 0.990 ± 0.003 and average complexity C ≈ 5.5 ± 0.5 (A.3 table, LLaVa-NeXT / ICSR comparison); per-benchmark R^2 values reported in tables for ICSR (typically high, e.g., 0.98–0.995 depending on benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limited by LLM context window (prompt token budget limits number of points and stored candidate functions); scaling to higher-dimensional inputs is challenging (prompt size and confusion); VLM (vision-language) extension experiments were not successful for current VLMs; risk of overfitting without complexity penalty; dependence on external numeric optimizer for coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against SR baselines (gplearn, DSR, uDSR, NeSymReS, E2E, TPSR). ICSR matched or outperformed these baselines on the selected benchmarks in overall R^2 while producing lower-complexity expressions and better out-of-distribution generalization in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4490.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4490.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific equation discovery via programming with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contemporary work (Shojaee et al., 2024) that uses large language models to perform symbolic regression / scientific equation discovery by including domain-specific natural language descriptions of input and output variables in the prompt and relying on an external coefficient optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific equation discovery via programming with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR (Shojaee et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in this paper as an approach where an LLM is prompted for equation generation with additional domain-specific natural language information about variables; candidate functional forms are produced by the LLM and numeric coefficients are tuned by an external optimizer. The focus is on leveraging scientific domain knowledge provided in natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific equation discovery (domain-specific scientific problems where variable descriptions aid the LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic scientific equations / domain-specific empirical/physical relations</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompt-based generation of symbolic equations by an LLM augmented with natural-language descriptions of inputs/outputs; coefficients fitted externally (as reported in the cited work summary within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>According to this paper, Shojaee et al.'s focus is on cases where the LLM can leverage domain knowledge via natural-language prompts, implying potential limitations in general (domain-agnostic) settings; further details not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as a contemporary method; this paper contrasts Shojaee et al.'s domain-focused prompting with the present work's domain-agnostic evaluation on standard SR benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4490.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4490.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimization by Prompting (meta-prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/optimization framework (Yang et al., 2023) that uses a meta-prompt containing task description and prior attempts with scores to ask an LLM to propose improved solutions for optimization problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OPRO (Optimization by Prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>OPRO constructs a meta-prompt that includes the optimization task and examples of previous candidate solutions with their scores; the LLM uses in-context learning to extrapolate patterns and propose improved candidates. The proposed output is evaluated externally and, if good, appended back to the meta-prompt in an iterative loop. The method has been applied to classical optimization problems including linear regression and Travelling Salesman in the originating work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General optimization tasks (demonstrated on linear regression, combinatorial optimization examples), applicable to symbolic regression when adapted as in ICSR.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not a law-extraction system per se; used to iteratively optimize candidate solutions (can propose functional relationships when applied to SR).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Meta-prompt with prior attempts + scores is fed to LLM; LLM generates new candidate solutions which are externally evaluated and iteratively fed back.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>In the originating work, evaluated on classical optimization examples; in this paper OPRO is adapted to the SR setting as the high-level control loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on LLM's ability to extrapolate from in-context examples; constrained by context-window size and quality/diversity of seed examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as conceptual basis for ICSR; direct comparisons limited in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4490.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4490.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman: A physics-inspired method for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI system (Udrescu & Tegmark, 2020) that combines neural-network function approximation with algebraic/symbolic techniques to recover closed-form expressions (physical laws) from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A physics-inspired method for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A physics-inspired symbolic regression pipeline that uses a neural-network function approximator to identify separability and suggest simplifications, then performs symbolic search/simplification to recover closed-form expressions representing underlying physics-like laws.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics / discovery of physical laws from data</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Physical laws and symbolic mathematical expressions discovered from empirical data</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Neural-network surrogate fitting to the data to guide algebraic manipulations and symbolic search; uses heuristic simplification and variable separation to reduce search complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Validated on synthetic physics-inspired datasets (original AI Feynman paper); in this paper it is cited as prior SR work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not discussed in detail here beyond being part of SR prior work; the method is specialized toward physics-like separable expressions and synthetic benchmark recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Cited among classical and contemporary SR approaches; direct head-to-head comparison not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4490.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4490.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based Planning for Symbolic Regression (TPSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based SR method (Shojaee et al., 2023) that augments a pre-trained model with a planning/search component (Monte Carlo Tree Search) to guide equation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer-based planning for symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TPSR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TPSR uses a pre-trained Transformer to propose equation skeletons and guides generation with a search/planning algorithm (Monte Carlo Tree Search), combining pre-training advantages with explicit search to refine skeleton outputs. It was cited as a competitive baseline in the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic regression (general, benchmark SR problems)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic mathematical expressions / functional relationships recovered from data</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Transformer generation of skeletons combined with Monte Carlo Tree Search over decoding to find higher-fitness expressions; numeric coefficients handled externally or via model outputs depending on variant.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Used as a baseline in benchmark comparisons in this paper; TPSR reported as having higher complexity than ICSR on some tasks while using similar objective functions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires pre-training on SR datasets; in comparisons it produced higher-complexity expressions than ICSR on some benchmarks, with differing out-of-distribution behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Directly compared in experiments reported in this paper: ICSR produced simpler expressions and better OOD generalization in several cases compared to TPSR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4490.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4490.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeSymReS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Symbolic Regression That Scales (NeSymReS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based model pre-trained on large synthetic SR datasets to generate symbolic expression skeletons; cited as a pre-trained Transformer SR baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural symbolic regression that scales</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NeSymReS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A Transformer-based generative model specifically pre-trained on synthetic symbolic regression expression skeletons; generates skeletons with coefficient tokens that are later optimized numerically. Used as a competitive baseline to assess the performance of ICSR.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Symbolic regression (machine-learning benchmarks for equation discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Symbolic mathematical expressions</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Pre-trained Transformer generation of expression skeletons; numeric coefficients optimized externally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Evaluated on the same SR benchmarks in the paper as a baseline (Nguyen, Constant, R, Keijzer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Pre-training requires large synthetic SR datasets; in this paper, NeSymReS had competitive R^2 but ICSR tended to produce expressions closer in complexity to ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Included as baseline; ICSR often produced lower-complexity expressions with comparable or better OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scientific equation discovery via programming with large language models. <em>(Rating: 2)</em></li>
                <li>Transformer-based planning for symbolic regression. <em>(Rating: 2)</em></li>
                <li>Large language models as optimizers. <em>(Rating: 2)</em></li>
                <li>AI Feynman: A physics-inspired method for symbolic regression <em>(Rating: 2)</em></li>
                <li>Neural symbolic regression that scales <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4490",
    "paper_id": "paper-271244415",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "ICSR",
            "name_full": "In-Context Symbolic Regression",
            "brief_description": "A framework introduced in this paper that leverages large language models (LLMs) via in-context learning and a meta-prompt (OPRO-style) optimization loop to propose symbolic functional forms; coefficients are fitted by an external non-linear least squares optimizer and proposals are iteratively refined with a complexity-penalized fitness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "In-Context Symbolic Regression (ICSR)",
            "system_description": "ICSR uses a foundation LLM to generate symbolic 'skeleton' functions from a prompt that contains a small set of observed (x,y) points and previous candidate functions with their fitness scores (meta-prompt). The pipeline: (1) generate multiple seed function skeletons from the LLM, (2) fit unknown numeric coefficients of each skeleton with an external Non-Linear Least Squares optimizer (SciPy), (3) evaluate fitness using a combined normalized MSE and complexity penalty, (4) keep the top-k candidates, append them to the prompt, and iterate (OPRO-style) until convergence or budget exhaustion. The LLM only proposes forms; numeric fitting and selection are external. The method uses temperature scheduling, multiple samples per LLM call, and repeated coefficient restarts to avoid local minima.",
            "model_name": "Llama 3",
            "model_size": "8B",
            "scientific_domain": "Symbolic regression / general quantitative function discovery (benchmarks from machine learning; intended applicability to scientific equation discovery in physical/scientific domains)",
            "number_of_papers": null,
            "law_type": "Symbolic mathematical equations (explicit functional relationships discovered via symbolic regression)",
            "law_examples": "Examples reported in the paper include recovered/suggested symbolic forms such as f(x) = 1.00 x^{0.4983} (approx sqrt(x)) and ICSR-recovered polynomial-like approximations such as f(x) = 0.50(x + 0.50)^2 - 0.12 for benchmark examples; ground-truth benchmark examples include functions like f(x) = x(x+1)^2.",
            "extraction_method": "In-context LLM prompting with examples of data and previous candidate equations (meta-prompt / OPRO). The LLM outputs symbolic skeletons; coefficients are numerically optimized with Non-Linear Least Squares; fitness uses NMSE combined with an exponential complexity penalty; iterative refinement keeps best candidates in context.",
            "validation_approach": "Empirical evaluation on standard symbolic regression benchmarks (Nguyen, Constant, R, Keijzer). Metrics computed on held-out test grids (in-domain and out-of-distribution extrapolation ranges). Repeated across random seeds; compared against baselines (gplearn, DSR/uDSR, NeSymReS, E2E, TPSR). Out-of-distribution tested by expanding input ranges and measuring R^2 and failure fraction.",
            "performance_metrics": "Reported metrics include coefficient of determination R^2 and expression complexity C (node count). Example aggregated results reported: overall average R^2 ≈ 0.990 ± 0.003 and average complexity C ≈ 5.5 ± 0.5 (A.3 table, LLaVa-NeXT / ICSR comparison); per-benchmark R^2 values reported in tables for ICSR (typically high, e.g., 0.98–0.995 depending on benchmark).",
            "success_rate": null,
            "challenges_limitations": "Limited by LLM context window (prompt token budget limits number of points and stored candidate functions); scaling to higher-dimensional inputs is challenging (prompt size and confusion); VLM (vision-language) extension experiments were not successful for current VLMs; risk of overfitting without complexity penalty; dependence on external numeric optimizer for coefficients.",
            "comparison_baseline": "Compared against SR baselines (gplearn, DSR, uDSR, NeSymReS, E2E, TPSR). ICSR matched or outperformed these baselines on the selected benchmarks in overall R^2 while producing lower-complexity expressions and better out-of-distribution generalization in many cases.",
            "uuid": "e4490.0",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-SR",
            "name_full": "Scientific equation discovery via programming with large language models",
            "brief_description": "A contemporary work (Shojaee et al., 2024) that uses large language models to perform symbolic regression / scientific equation discovery by including domain-specific natural language descriptions of input and output variables in the prompt and relying on an external coefficient optimizer.",
            "citation_title": "Scientific equation discovery via programming with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SR (Shojaee et al., 2024)",
            "system_description": "Described in this paper as an approach where an LLM is prompted for equation generation with additional domain-specific natural language information about variables; candidate functional forms are produced by the LLM and numeric coefficients are tuned by an external optimizer. The focus is on leveraging scientific domain knowledge provided in natural language prompts.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Scientific equation discovery (domain-specific scientific problems where variable descriptions aid the LLM)",
            "number_of_papers": null,
            "law_type": "Symbolic scientific equations / domain-specific empirical/physical relations",
            "law_examples": null,
            "extraction_method": "Prompt-based generation of symbolic equations by an LLM augmented with natural-language descriptions of inputs/outputs; coefficients fitted externally (as reported in the cited work summary within this paper).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "According to this paper, Shojaee et al.'s focus is on cases where the LLM can leverage domain knowledge via natural-language prompts, implying potential limitations in general (domain-agnostic) settings; further details not provided here.",
            "comparison_baseline": "Mentioned as a contemporary method; this paper contrasts Shojaee et al.'s domain-focused prompting with the present work's domain-agnostic evaluation on standard SR benchmarks.",
            "uuid": "e4490.1",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "OPRO",
            "name_full": "Optimization by Prompting (meta-prompting)",
            "brief_description": "A prompting/optimization framework (Yang et al., 2023) that uses a meta-prompt containing task description and prior attempts with scores to ask an LLM to propose improved solutions for optimization problems.",
            "citation_title": "Large language models as optimizers.",
            "mention_or_use": "use",
            "system_name": "OPRO (Optimization by Prompting)",
            "system_description": "OPRO constructs a meta-prompt that includes the optimization task and examples of previous candidate solutions with their scores; the LLM uses in-context learning to extrapolate patterns and propose improved candidates. The proposed output is evaluated externally and, if good, appended back to the meta-prompt in an iterative loop. The method has been applied to classical optimization problems including linear regression and Travelling Salesman in the originating work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General optimization tasks (demonstrated on linear regression, combinatorial optimization examples), applicable to symbolic regression when adapted as in ICSR.",
            "number_of_papers": null,
            "law_type": "Not a law-extraction system per se; used to iteratively optimize candidate solutions (can propose functional relationships when applied to SR).",
            "law_examples": null,
            "extraction_method": "Meta-prompt with prior attempts + scores is fed to LLM; LLM generates new candidate solutions which are externally evaluated and iteratively fed back.",
            "validation_approach": "In the originating work, evaluated on classical optimization examples; in this paper OPRO is adapted to the SR setting as the high-level control loop.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Relies on LLM's ability to extrapolate from in-context examples; constrained by context-window size and quality/diversity of seed examples.",
            "comparison_baseline": "Used as conceptual basis for ICSR; direct comparisons limited in this paper.",
            "uuid": "e4490.2",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AI-Feynman",
            "name_full": "AI Feynman: A physics-inspired method for symbolic regression",
            "brief_description": "An AI system (Udrescu & Tegmark, 2020) that combines neural-network function approximation with algebraic/symbolic techniques to recover closed-form expressions (physical laws) from data.",
            "citation_title": "AI Feynman: A physics-inspired method for symbolic regression",
            "mention_or_use": "mention",
            "system_name": "AI Feynman",
            "system_description": "A physics-inspired symbolic regression pipeline that uses a neural-network function approximator to identify separability and suggest simplifications, then performs symbolic search/simplification to recover closed-form expressions representing underlying physics-like laws.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Physics / discovery of physical laws from data",
            "number_of_papers": null,
            "law_type": "Physical laws and symbolic mathematical expressions discovered from empirical data",
            "law_examples": null,
            "extraction_method": "Neural-network surrogate fitting to the data to guide algebraic manipulations and symbolic search; uses heuristic simplification and variable separation to reduce search complexity.",
            "validation_approach": "Validated on synthetic physics-inspired datasets (original AI Feynman paper); in this paper it is cited as prior SR work.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Not discussed in detail here beyond being part of SR prior work; the method is specialized toward physics-like separable expressions and synthetic benchmark recovery.",
            "comparison_baseline": "Cited among classical and contemporary SR approaches; direct head-to-head comparison not provided in this paper.",
            "uuid": "e4490.3",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TPSR",
            "name_full": "Transformer-based Planning for Symbolic Regression (TPSR)",
            "brief_description": "A Transformer-based SR method (Shojaee et al., 2023) that augments a pre-trained model with a planning/search component (Monte Carlo Tree Search) to guide equation generation.",
            "citation_title": "Transformer-based planning for symbolic regression.",
            "mention_or_use": "mention",
            "system_name": "TPSR",
            "system_description": "TPSR uses a pre-trained Transformer to propose equation skeletons and guides generation with a search/planning algorithm (Monte Carlo Tree Search), combining pre-training advantages with explicit search to refine skeleton outputs. It was cited as a competitive baseline in the present work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Symbolic regression (general, benchmark SR problems)",
            "number_of_papers": null,
            "law_type": "Symbolic mathematical expressions / functional relationships recovered from data",
            "law_examples": null,
            "extraction_method": "Transformer generation of skeletons combined with Monte Carlo Tree Search over decoding to find higher-fitness expressions; numeric coefficients handled externally or via model outputs depending on variant.",
            "validation_approach": "Used as a baseline in benchmark comparisons in this paper; TPSR reported as having higher complexity than ICSR on some tasks while using similar objective functions.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Requires pre-training on SR datasets; in comparisons it produced higher-complexity expressions than ICSR on some benchmarks, with differing out-of-distribution behavior.",
            "comparison_baseline": "Directly compared in experiments reported in this paper: ICSR produced simpler expressions and better OOD generalization in several cases compared to TPSR.",
            "uuid": "e4490.4",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "NeSymReS",
            "name_full": "Neural Symbolic Regression That Scales (NeSymReS)",
            "brief_description": "A Transformer-based model pre-trained on large synthetic SR datasets to generate symbolic expression skeletons; cited as a pre-trained Transformer SR baseline.",
            "citation_title": "Neural symbolic regression that scales",
            "mention_or_use": "mention",
            "system_name": "NeSymReS",
            "system_description": "A Transformer-based generative model specifically pre-trained on synthetic symbolic regression expression skeletons; generates skeletons with coefficient tokens that are later optimized numerically. Used as a competitive baseline to assess the performance of ICSR.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Symbolic regression (machine-learning benchmarks for equation discovery)",
            "number_of_papers": null,
            "law_type": "Symbolic mathematical expressions",
            "law_examples": null,
            "extraction_method": "Pre-trained Transformer generation of expression skeletons; numeric coefficients optimized externally.",
            "validation_approach": "Evaluated on the same SR benchmarks in the paper as a baseline (Nguyen, Constant, R, Keijzer).",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Pre-training requires large synthetic SR datasets; in this paper, NeSymReS had competitive R^2 but ICSR tended to produce expressions closer in complexity to ground-truth.",
            "comparison_baseline": "Included as baseline; ICSR often produced lower-complexity expressions with comparable or better OOD performance.",
            "uuid": "e4490.5",
            "source_info": {
                "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scientific equation discovery via programming with large language models.",
            "rating": 2
        },
        {
            "paper_title": "Transformer-based planning for symbolic regression.",
            "rating": 2
        },
        {
            "paper_title": "Large language models as optimizers.",
            "rating": 2
        },
        {
            "paper_title": "AI Feynman: A physics-inspired method for symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "Neural symbolic regression that scales",
            "rating": 1
        }
    ],
    "cost": 0.017107749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery
17 Jul 2024</p>
<p>Matteo Merler 
Department of Computer Science
Aalto University</p>
<p>Katsiaryna Haitsiukevich 
Department of Computer Science
Aalto University</p>
<p>Nicola Dainese 
Department of Computer Science
Aalto University</p>
<p>Pekka Marttinen 
Department of Computer Science
Aalto University</p>
<p>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery
17 Jul 2024D57A9D051710D73858771887B090EB05arXiv:2404.19094v2[cs.CL]
State of the art Symbolic Regression (SR) methods currently build specialized models, while the application of Large Language Models (LLMs) remains largely unexplored.In this work, we introduce the first comprehensive framework that utilizes LLMs for the task of SR.We propose In-Context Symbolic Regression (ICSR), an SR method which iteratively refines a functional form with an LLM and determines its coefficients with an external optimizer.ICSR leverages LLMs' strong mathematical prior both to propose an initial set of possible functions given the observations and to refine them based on their errors.Our findings reveal that LLMs are able to successfully find symbolic equations that fit the given data, matching or outperforming the overall performance of the best SR baselines on four popular benchmarks, while yielding simpler equations with better out of distribution generalization.* Denotes equal contribution.</p>
<p>Introduction</p>
<p>Classical Machine Learning regression methods can be divided into two broad categories: statistical methods, which learn an implicit statistical (black-box) model of the relationship between the observations, and rule-based methods, which instead attempt to extract an explainable set of rules that explicitly model the transformation between the inputs and outputs (Lample and Charton, 2019).Symbolic Regression (SR) is a particular subset of the latter category, which searches the set of all possible explicit mathematical expressions to find the equation that best fits the given set of observations.This has the clear advantage of explainability, as well as a potential for better generalization, if the trend holds outside of the observed data.</p>
<p>The traditional approach for SR algorithms is Genetic Programming (Willis et al., 1997) (GP), which combines fundamental blocks for mathematical expressions (e.g., basic operators, trigonometric functions, etc.) into more complex formulas using strategies borrowed from evolutionary biology, such as mutations and fitness.The recent success of Transformer models, first introduced by Vaswani et al. (2017), has revolutionized various fields of Artificial Intelligence, notably Natural Language Processing (Brown et al., 2020;Achiam et al., 2023;Touvron et al., 2023;Anil et al., 2023) and Computer Vision (Dosovitskiy et al., 2021).Transformer-based methods have also been proposed for SR (Biggio et al., 2021;Kamienny et al., 2022), typically by employing a model pre-trained on a large amount of synthetic SR datasets.</p>
<p>Large Language Models (LLMs), also based on the Transformer, have proven to possess unprecedented reasoning and generalization abilities, based on their capability for In-Context Learning (ICL) (Brown et al., 2020).This refers to the ability to perform tasks based on the context provided in the input text without any additional fine-tuning.With the help of ICL, these models can be leveraged for a wide range of different tasks, suggesting a potential use case for Symbolic Regression.</p>
<p>In this paper, we examine the integration of LLMs into the SR pipeline, with the aim of using them to search for new equations that could fit the data.Inspired by the Optimization by Prompting (OPRO) approach presented by Yang et al. (2023), we propose In-Context Symbolic Regression (ICSR)1 .This approach leverages pre-trained language models by providing a number of previously tested equations and their fitness scores in the prompt, tasking them to generate a new candidate that could be a better fit.The method is repeated until convergence is reached or the computational budget is exhausted.To the best of our knowledge,</p>
<p>Step 1: Generate Seed Functions</p>
<p>Step 2: Optimization Loop Select n best s Figure 1: High level overview of the ICSR approach.Given an initial set of observations, we prompt the LLM to generate multiple initial guesses (seeds) of the true function that generated the observations.We then iteratively refine our guesses within an optimization loop where we propose new functions (based on a set of the previous best attempts), fit their coefficients and evaluate their fitness.The model only produces the functional form of a function, while the unknown coefficients are fitted using non-linear least squares optimization.only a contemporary work by Shojaee et al. (2024) has ever explored the use of LLMs for SR.However, they focus on working with equations from a scientific domain where natural language knowledge can be directly incorporated, while this work aims to generally explore the capabilities of LLMs for SR without any additional information, in order to lay a foundation that can be expanded later.We discuss in depth the differences between the two works in Section 2.</p>
<p>Our approach presents several advantages compared to models specifically trained for SR: as the LLM is not fine-tuned for this task, improvements in the underlying base model can improve ICSR without any changes to the method itself.Further, LLMs provide a natural language interface that can be leveraged to include additional information about the problem, like the domain of the equation and the interpretation of the observation values.The models could also be asked to explain the reasoning behind the proposed functions, potentially leading to a more interpretable process.</p>
<p>In summary, we make the following contributions: 1) We propose ICSR, the first general framework to leverage LLMs for the SR task.2) We compare the method with a range of competitive SR baselines, matching or outperforming state of the art results on four popular SR benchmarks: Nguyen (Nguyen et al., 2011), Constant (Li et al., 2023d), R (Krawiec and Pawlak, 2013) and Keijzer (Keijzer, 2003).3) We show that the equations generated with our method tend to exhibit lower complexity, which correlates with stronger out of distribution performance.</p>
<p>Related Work</p>
<p>Symbolic Regression.GP has traditionally formed the backbone for SR methods (Smits and Kotanchek, 2005;Schmidt and Lipson, 2011;Virgolin et al., 2021).Typically, from an initial population, an iterative tournament is played where functions with the highest fitness are selected to 'reproduce' with some random mutation, as in Koza and Poli (2005).</p>
<p>More recently, Deep Learning methods have been applied to enhance the available toolkit for SR.Udrescu and Tegmark (2020) proposed an iterative simplification of the problem relying on insights from physics and outsourcing the function approximation part to a neural network.Petersen et al. ( 2021) used a Recurrent Neural Network (RNN) with a risk-seeking policy to perform a hierarchical search over the space of user-defined operators and mathematical functions.The main drawback of these methods, including GP, is the fact that the algorithms start from scratch for every new expression, with very limited abilities of knowledge preservation between tasks.</p>
<p>To address this limitation, numerous Transformer-based methods inspired by language modelling have been developed.SymbolicGPT by Valipour et al. (2021), NeSymReS by Biggio et al. (2021) and CL-SR by Li et al. (2023d) proposed different generative Transformer models specifically trained for SR.These models generate a functional form ('skeleton') of the equation with a special token for coefficients which are fitted via an external numerical optimizer.Subsequently, Kamienny et al. (2022) presented E2E, a Transformer model able to produce the full expression including the coefficient values.While retaining knowledge between tasks, Transformer-based methods are quite limited in refining their solutions for the given set of points.To this end, Shojaee et al. (2023) presented a method integrating a pre-trained Transformer with Monte Carlo Tree Search to guide the equation generation merging the strength of the search and model pre-training.The proposed framework can be also viewed as a combination of a pre-trained model and an iterative refinement process.However, none of the prior methods employ a foundation model (Bommasani et al., 2021), such as an LLM, in order to leverage mathematical knowledge, but either pre-train an SR model (Biggio et al., 2021;Kamienny et al., 2022), or learn from scratch for every new function (Petersen et al., 2021).</p>
<p>Mathematical Reasoning with LLMs.As LLMs form the backbone of the method presented in this work, we rely entirely on their mathematical reasoning capabilities, such as ICL (Brown et al., 2020), to explore the solution space.Mirchandani et al. (2023) show that LLMs are able to recognize patterns from in-context examples and can extrapolate them to complete related tasks in the input.Similarly, Gruver et al. (2023) find that LLMs can extrapolate zero-shot the pattern from a timeseries (although they do not extract any functional representation).Furthermore, Fu et al. (2023) present a study in which they find that Transformer models can learn higher-order optimization methods (similar to Newton's method).</p>
<p>Contemporary to our work, Shojaee et al. ( 2024) also propose to perform SR with an LLM aided by an external coefficient optimizer.However, they focus exclusively on the case where LLMs can leverage scientific knowledge for SR, by including a description of the input and output variables in the LLM prompt.In contrast, we focus on the general case where no extra knowledge is given and test on standard benchmarks within the SR community and include a wider range of established baselines, with the aim to directly evaluate the capability of LLMs on the task of SR.Furthermore, we propose advancements in the structure of the prompt, including the coordinates of the points to be regressed, the score of previous attempts, and more in-context examples.Finally, rather than asking the LLM to optimize a Mean Squared Error (MSE) objective, we employ a more advanced loss function, presented in Section 4, which jointly optimizes for the accuracy and complexity of the function for improved generalization properties.</p>
<p>Background</p>
<p>The Optimization by Prompting (OPRO) framework was introduced by Yang et al. (2023) for prompt optimization, i.e., for increasing the performance of models (such as LLMs) that receive a textual prompt in the input and have to perform a specific task (such as mathematical reasoning).Closer to our interest, the authors also present experiments on classical optimization problems (Linear Regression and Travelling Salesman Problem), suggesting that OPRO can solve such tasks.</p>
<p>The key idea of the method is the use of a socalled meta-prompt, a higher level prompt which contains a description of the task to be optimized and previous attempts (examples) in solving it with their corresponding scores.An example of such task can be querying the model to find a linear function that fits a set of points.In this case, the prompt is augmented by the functions that have been tried out and the mean squared error on the data, obtained with an external evaluation procedure.The assumption behind it is that LLMs have the ability to extrapolate the pattern formed by the examples, thanks to ICL, and propose a better alternative.The meta-prompt is given as input to the LLM and the model's output is then evaluated and added back to the meta-prompt if the score is good enough.This approach can then be iterated until a satisfying result is achieved or a certain computational budget is exhausted.</p>
<p>Method</p>
<p>We consider a regression dataset D with N observations and target variables {x i , y i } N i=1 , also denoted with (X, Y ) more compactly, to be used for producing a function f that well approximates the data.To leverage the OPRO approach for SR, we need to design a meta-prompt suitable for the task and fill it with the available observations (X, Y ), an initial set of k functions F0 = { f (1) 0 , f (2) 0 , . . ., f (k) 0 } (either hand-written or model-generated) and a measure of their fitness (score) on D. For our purposes, we frame the refinement process as a minimization problem over an objective function, also called the error function, such that generated equations with the lowest error have the highest fitness.The goal is then to iteratively refine the set of functions Fi , i ∈ [1, . . ., n] for each iteration i, until a sufficiently low error is obtained by one of them or a maximum number of iterations is reached; we denote this process as the optimization loop.Due to the finite size of the LLM context window, we only keep the k best performing previous attempts in the set Fi , where k is a design choice (k = 5 in this study).The meta-prompt used in the experiment can be found in Appendix C.</p>
<p>Seed Functions.At the first iteration, F0 is empty as there are no previous guesses from the model.Thus, an initial population of seed functions is required to kickstart the optimization loop.Instead of relying on a fixed set of initial functions, which could be restrictive in general, we ask the model to generate the initial seed functions (with the prompt provided in Appendix C).This results in a complex and diverse set of functions, from which the LLM can refine its future predictions with the optimization loop.In our implementation we repeat this initial process n s times, as some of the generated functions can be undefined for certain input points (e.g., log(x) for negative numbers).We set n s = 10 for this work and explore its impact in Section 5.5 through an ablation study.</p>
<p>Error Function.The immediate choice for the objective function would be an MSE, or a similar error metric, over the regression dataset D. However, simply minimizing this error can result in overfitting on the training points in D. As overfitting in SR often occurs due to a growing number of terms in the generated equation, we adapt from Shojaee et al. (2023) a fitness function r( f |D) with an extra penalty term for the complexity C of the generated expression, defined as the following:
r( f |D) = 1 1 + NMSE( f |D) + λe − C( f ) L ,(1)
where f is the predicted function, C is the complexity defined as the number of nodes in the expression tree, L is the maximum sequence length (set to 30), and λ is a hyperparameter to trade-off between the fit to the data and the complexity.The Normalized Mean Square Error (NMSE) is calculated as
NMSE( f |D) = N i=1 (y i − f (x i )) 2 N i=1 y 2 i + ϵ ,(2)
where ϵ is a small regularizing constant.Finally, we use err( f |D) = r( f |D) −1 as our error function to frame ICSR as a minimization problem.We explore the choice of the λ parameter in Section 5.5 with a sensitivity analysis.</p>
<p>Parameter Fitting.We utilize the LLM only to generate functional forms (skeletons), while the unknown coefficients associated to the predicted functional form are optimized by Non-linear Least Squares (NLS) (Kelley, 1999) available from SciPy's (Virtanen et al., 2020).This not only yields better coefficient values, due to the superior optimization performance of NLS over LLMs, but also allows for more efficient exploration of the space of functions, by grouping them in equivalence classes of unique functional forms.In our implementation, we optimize the function's coefficients five times starting from different random initial values, to avoid local minima, similarly to Li et al. (2023d).</p>
<p>For other details about the OPRO implementation, we follow the original work.Specifically, we also sample multiple functions for every iteration (asking the model to generate 5 functions for every call) in an attempt to improve the stability of the loop and we experiment with a decreasing temperature parameter to balance exploration/exploitation (with a higher initial temperature encouraging the exploration of the underlying functional space, and a lower temperature at the later stages forcing smaller tweaks to the trajectory).To avoid saturating the model's context window, we limit the amount of training points that are included in written form to a certain threshold, empirically set to 40.We discuss this in more detail in the Limitations Section 6.1.
ICSR (Ours) 0 O((50 • 5 + 10) • 5) 8B ✓ ✓ ✓ ✓ gplearn 0 O(1000 • 20) - ✗ ✗ ✓ ✗ DSR 0 O(200K) 8K ✗ ✗ ✓ ✗ uDSR* 0 O(200K) 8K ✓/ ✗ ✗ ✓ ✗ NeSymReS 100M O(10 • 10) 26M ✓ ✗ ✗ ✗ E2E 3M O(100) 86M ✓ ✗ ✗ ✗ TPSR 3M O(200 • 3) 86M ✓ ✗ ✓ ✓
* The uDSR method potentially allows using a pre-trained model as a prior.However, as reported in the original paper, while this is useful in a low-budget search it has tendencies to worsen the performance.</p>
<p>Table 1: Qualitative comparison across baselines.We compare different properties for all baselines.Evaluated expressions is the total number of equations a method considers for modeling a given training set.Pre-trained model refers to the use of an underlying model as opposed to training from scratch for each problem.Problem specific refinement refers to the use of a search algorithm on the space of possible skeletons.</p>
<p>Experiments</p>
<p>We empirically evaluate ICSR and compare it against a set of competitive baselines, checking both in-domain and out of distribution performance of the proposed approach.</p>
<p>Benchmarks</p>
<p>For our experiments, we choose four popular SR benchmarks containing functions with one or two The symbolic equations and ranges for both the training and testing points are reported in Appendix D. We leave for future work the evaluation of ICSR on higher dimensionality benchmarks.</p>
<p>Metrics</p>
<p>While we use the error function err( f |D) during the optimization loop (see Section 4), we follow the literature in reporting the coefficient of determination R 2 (Glantz et al., 2017) to evaluate the quality of our method.This staple metric in SR can be interpreted as follows: a function will get a positive score if it is more accurate than the average prediction and will get a score of 1 for a perfect prediction.The coefficient is computed as:
R 2 = 1 − n i=1 (y i − ŷi ) 2 n i=1 (y i − ȳ) 2 ,(3)
where y i is the ground truth value, ŷi is the predicted value and ȳ is the average of all y i .We report the R 2 metric computed on a set of unseen testing points, obtained from a dense grid within the same range of input point values as during training (for in-domain performance) or its extended version (for out of distribution performance).We follow Li et al. (2023d) and Biggio et al. (2021) in removing the 5% worst predictions in all methods to ensure robustness against outliers.We further report the complexity C of the generated equations, calculated as the number of nodes in its expression tree.For all methods, we repeat all experiments across five different random seeds and report the average values together with the standard error of the mean.For ICSR, we allow up to 50 iterations in the optimization loop and end it earlier if the R 2 score on the training set exceeds 0.99999.</p>
<p>Baselines</p>
<p>To evaluate the performance of the proposed method we opted for the following list of competitive baselines: gplearn (Stephens, 2022), a classical GP approach; DSR (Glatt et al., 2022) and uDSR (Landajuela et al., 2022)  Table 2: Comparison across baselines.We evaluate each method on all benchmarks with five random seeds, reporting the averages for the coefficient of determination R 2 and the function complexity C with the error of the mean.We further report the average ground truth complexity for each benchmark, indicated with C.
( C = 5.2) Constant ( C = 4.3) R ( C = 8.3) Keijzer ( C = 5.0) Overall avg. R 2 (↑) C (↓) R 2 (↑) C (↓) R 2 (↑) C (↓) R 2 (↑) C (↓) R 2 (↑) C (↓) ICSR (
efficient when compared to search-based methods like DSR and uDSR.The LLM is slower in generating a single expression, but is able to produce more meaningful equations, thanks to the large pretraining bias, as opposed to methods like gplearn and DSR which have to be trained from scratch on each problem.ICSR is also the only method with a natural language interface and a flexible vocabulary, which we discuss further in Section 6.</p>
<p>Comparison across Baselines</p>
<p>For comparison of ICSR with the baselines, we choose Llama 3 8B (Meta, 2024) as the underlying LLM.The results (see Table 2) show that the ICSR approach is very robust, consistently achieving very high scores across all benchmarks while producing expressions with a lower average complexity.The overall average columns show that ICSR outperforms all baselines, with only uDSR matching its R 2 score at the cost of significantly higher complexity.In general, it is important to consider both metrics simultaneously, as simpler functions can lead to a sightly lower R 2 value while bringing other advantages, such as better out of distribution generalization, which we explore in Section 5.4.</p>
<p>As seen in the headers in Table 2, the complexity values of the ground truth equations align much more closely to the ones recovered by ICSR as opposed to the ones for other high-performing baselines, such as uDSR or TPSR.The improvement in complexity compared to TPSR is particularly noteworthy, as both methods are using the same objective function: this could be a sign that LLMs tend to produce more human-readable expressions thanks to their pre-training bias.It is also worth noting that ICSR can potentially improve over time by simply increasing the performance of the underlying LLM backbone without any additional training, while that is not the case for the other methods.
(x) = √ x ,  = 2.0 ICSR: f(x) = 1.00x 0.4983 ,  = 3.0 uDSR: f(x) = log(0.19x 3 − 2.3x 2 + 8.4x + 1) − sin(√ 2x ),  = 16.0 ICSR uDSR Training Points Ground Truth (a) Nguyen 8. −2 −1 0 1 2 x −3 −2 −1 0 1 2 3 Ground Truth: f(x) = x(x + 1) 2
,  = 3.0 ICSR: f(x) = 0.50(x + 0.50) 2 − 0.12,  = 5.0 uDSR: f(x) = 0.26x 3 + 0.53x 2 − 0.50x − 1.00 cos(x) + exp(sin(x)),  = 13.0(b) Keijzer 6.Table 3: Sensitivity analysis and ablation studies.We perform sensitivity analysis on the values of the complexity penalty parameter λ and two ablation studies: one using only n s initial seed functions without improving them and the other one using random guessing, rather than ICSR, for proposing new functions.All ablations on n s are performed without the optimization loop, only keeping the best generated seed function.We report the averages for the coefficient of determination R 2 and the function complexity C with the error of the mean for all experiments.We highlight in bold the best performance across different values of λ.</p>
<p>Method
Nguyen Constant R Keijzer R 2 (↑) C (↓) R 2 (↑) C (↓) R 2 (↑) C (↓) R 2 (↑) C (↓) ICSR (λ = 0.</p>
<p>Out of Distribution Performance</p>
<p>We further explore the advantage of producing functions with a lower complexity value by testing the out of distribution capabilities of the expressions recovered by ICSR and the other baselines.We exclude gplearn from these experiments, as we observe its performance to be significantly lower compared to the rest of the methods.To include out-ofdomain test points, we extend the input range by 100% to all directions (in which the function is defined).We compute the R 2 value on the extended range, reporting the results in Figure 2. Note that the R 2 value can quickly become increasingly negative when the functions diverge significantly.In order to keep the results stable, we treat all negative values as 0 when computing the average and report the fraction of experiments with a negative R 2 .</p>
<p>In general, we observe a sharp decline in performance for all methods, with the fraction of negative R 2 values quickly increasing towards the further extensions of the range.Specifically, ICSR is the highest performing method in the 175% and 200% domain increases, with the second lowest and lowest number of failures respectively.Generally, methods with lower complexity such as NeSym-ReS, E2E and TPSR tend to perform better than uDSR, with the exception of DSR which exhibits the poorest out of distribution performance even with a low average complexity.The comparison between ICSR and uDSR is particularly meaningful: as reported in Table 2, the two methods are tied for the best overall average performance, but ICSR outperforms uDSR when extrapolating further outside of the training range thanks to the lower complexity of the recovered expressions.We present some qualitative examples that demonstrate the difference between the methods in Figure 3.</p>
<p>Sensitivity Analysis and Ablation Studies</p>
<p>In this section we first investigate the impact of the λ parameter and then test the importance of the iterative refinement of the equations with the optimization loop.Finally, we compare ICSR with a baseline where the LLM was not given any information about the observations.All results are reported in Table 3.</p>
<p>Lambda Parameter.In our sensitivity analysis we considered the complexity penalty parameter λ = [0, 0.05, 0.1, 0.5, 1].We noticed that the smallest penalty λ = 0.05 was already sufficient to considerably reduce the complexity of the selected functions and increasing the penalty further had a relatively smaller impact on complexity.Therefore we used λ = 0.05 for our experiments.With λ = 0 the complexity is not considered and the equations overfit on the observations: the R 2 score tends to improve slightly at the cost of a large increase in complexity, with expressions composed of many different terms attempting to fit perfectly the training set.As the value for the parameter λ increases, both the R 2 score and the complexity tend to decrease, resulting in equations that underfit the data, as they do not have enough terms to properly capture all the observed dependencies.These results align with Shojaee et al. (2023), who introduced the fitness function we use.They chose 0.1 as the final parameter value, which we find performing similarly to 0.05, but slightly underfitting on some benchmarks, particularly R.</p>
<p>Optimization Loop.The results suggest that the seed functions generation step plays a key role in our approach, as with n s = 10 the results already show a high fitness on the test set, although they still underperform the full method in terms of both R 2 and complexity.We notice that using the best seed functions without refinement can outperform the results with ICSR for some values of λ (e.g.λ = 0, 0.1) in the most complex benchmarks (R and Keijzer).This is because the performance is reported on the set of test points and can decrease when refining, due to overfitting on the training points.It's also worth noting that some of the experiments with only a single initial call did not result in any valid seed functions, showing the need for repeating the generation process multiple times.In the prompt used to generate the seed functions (reported in Appendix C) we specifically ask for a diverse and complex set of functions that can be optimized, which is likely why the complexity on the seed functions is much higher, as it will be lowered later in the optimization loop.Overall, both parts of the method are necessary for the best possible performance; repeating the seed function generation step multiple times allows the model to generate a large number of potential initial expressions, resulting in a solid set of initial candidates for the optimization loop to build upon.</p>
<p>Random Guessing.As some of the benchmarks contain common equations such as simple polynomials, the LLM could simply be randomly generating functions that fit the data points, instead of actually making use of the information provided in the prompt.To ensure that this is not the case, we compare ICSR with a 'Random Guessing' baseline, where the LLM was prompted for 60 times (matching the budget used for ICSR, which uses 10 prompts to generate the seed functions and 50 prompts for the optimization loop) to generate five random functions, without any information about the observations or previous guesses (the prompt is reported in Appendix C).The results show that this baseline underperforms ICSR on all four benchmarks, especially on Keijzer, the hardest one.Empirically, we observe that the functions generated by the LLM in this way are all extremely simple, mostly constrained to basic polynomials.This confirms that LLMs are able to extract patterns from the prompt and are not simply randomly generating the solutions.</p>
<p>Discussion</p>
<p>Optimizing for out of distribution.A general framework for optimizing the out of distribution performance of a predictive model (such as a symbolic equation) is to regularise its complexity, following the Occam's Razor principle that simpler explanations are preferable to more complex ones, all other things being equal.In our work we use the working definition of complexity as the number of nodes in the expression tree of an equation.However, more optimal choices could be available: for instance, equations containing expressions not defined on all the real domain (such as logarithms and square roots) could be penalised more, as they could be undefined when extrapolating to larger domains.Knowing in advance the full domain in which an equation is supposed to hold could also greatly improve out of distribution performance by filtering out invalid candidate functions.In the case of ICSR, it could also be leveraged as extra information by the LLM.Furthermore, we observe that numerous equations that we derive with ICSR have extra terms with very small coefficients (e.g.O(10 −3 )) that do not contribute significantly to the shape of the equation and could be safely suppressed, resulting in expressions with a lower complexity.This could be done by modifying the optimization procedure of the coefficients, to eliminate coefficients under a certain threshold, which would be a hyperparameter of the method.</p>
<p>Vocabulary.In general, most SR methods are limited to a predefined vocabulary of operators and tokens, while LLMs can virtually explore any possible function and combination.An example of this is with the x x 2 1 function in the Nguyen benchmark: in Biggio et al. (2021), the authors mention that it is not included in the set of equations that their model can fit, while our approach can recover the exact expression.We also observe a similar trend with the other baselines for this specific expression.In our prompts (see Appendix C) we include a vocabulary for the LLM, but this is meant more to guide the LLM into the correct search space and is by no means a hard restriction: for example, we observe that ICSR can produce the erf function even if it wasn't reported in this list.Furthermore, any function that can be found in the model's pre-training corpus (fundamentally the In-ternet) can be potentially added to the prompt at any time if desired, which is impossible for other fixed-vocabulary methods.</p>
<p>Limitations</p>
<p>Although promising, the approach presented in this work still suffers from some key limitations that hold back its potential as a full Symbolic Regression method.</p>
<p>Size of the context window.LLMs are provided with a context window, which represents the maximum number of tokens they can process as input at the same time.For instance, Llama3, used for ICSR, has an 8k token context window.This limits the amount of information that we can include in the prompt, in terms of training datapoints and previously attempted functions with their errors.However, with context-window size increasing, commercially available LLMs like GPT-4 Turbo (Achiam et al., 2023) and Claude 3 (Anthropic, 2024), which process over 100k tokens, this issue is likely to be alleviated or lifted completely.</p>
<p>What to include in the prompt?Including all needed information in the prompt might not be enough, as some research suggests LLMs cannot fully utilize extended contexts (Liu et al., 2024c).In practice, we observe that when too many points are included, the model often continues generating points, especially with two-dimensional functions.</p>
<p>Limiting training points in the prompt to 40 (chosen empirically) helps, while all input points are still used for coefficient optimization.Some directions to help the model leveraging the information in the data could be to sample the most informative subset of points to fit in the prompt, or present the LLM with higher-level descriptions of the points, rather than feeding them directly to the model.Finally, we hypothesize that presenting the data in different modalities, such as images of the points and plots of the functions, by using multimodal foundation models, might be helpful to incorporate all information available.We experimented with Vision-Language Models, but our attempts in that direction, reported in Section A of the Appendix, were not fruitful so far.</p>
<p>Dimensionality.Using an LLM for higher dimensional inputs is possible, but dimensionality exacerbates the issues presented above.As the number of variables grows, so does the space dedicated to the input points in the prompt, which will naturally confuse the model and obfuscate the structure in the datapoints even further.Specifically fine-tuning an LLM on this kind of examples might show some improvement, but scaling this approach for higher dimensional problems remains a challenge.</p>
<p>Conclusion</p>
<p>We show that LLMs paired with the ICSR approach are able to perform Symbolic Regression tasks on classical SR benchmarks.The proposed method matches or outperforms a variety of established SR baselines, while producing simpler expressions that more closely resemble the complexity of the ground truth equations and result in better out of distribution performance.This work exposes yet another task that LLMs can be leveraged for, thanks to specialized techniques such as ICSR, and shows promise for integrating these models with mathematical reasoning methods.</p>
<p>Future Work</p>
<p>As this is one of the first works published on this topic, much work remains to be done.LLMs allow the inclusion of domain-specific natural language information into the prompt, as explored by Shojaee et al. (2024).The natural language interface could be further exploited by employing explicit Chain of Thought-like (Wei et al., 2022;Kojima et al., 2022) techniques, allowing the model to output even more well-informed guesses at every step and resulting in an interpretable method.Another interesting direction would be to consider tree-based search algorithms on top of the LLM, analogously to the TPSR (Shojaee et al., 2023) approach.As our work proves the intrinsic ability of LLMs to perform SR without taking into consideration any additional inputs, we have hope that future work can build upon ICSR to further leverage foundation models for SR.</p>
<p>A Vision-Language Models</p>
<p>In this section we report our findings on extending ICSR to Vision-Language Models (VLMs), which we considered a promising direction, but was not successful experimentally, at least with the VLMs that we considered.Reasoning on the observations and the previously attempted functions to come up with better function candidates is a challenging task.Visualising the data and the functions, when possible, can be of great help for humans and, we hypothesize, for SR models too.We thus explore the use of visual information in ICSR by considering VLMs in place of LLMs and adding to the optimization meta-prompt a scatter plot containing the observations (Figure 4a), as well as plots superimposing the best previously generated function (Figure 4b).We dub this variant ICSR-V and present results for it in Section A.3.However, the use of both vision and language as input comes with the restriction of dimensionality, as it is impossible to visualize inputs with more than two inputs in a single image.A solution could be to include projections into each dimension as the input, but this can quickly grow out of control as the number of variables increases, and then the additional information would probably provide diminishing returns.</p>
<p>A.1 Vision-Language Extension</p>
<p>A.2 Related Work</p>
<p>VLMs have gained traction after Radford et al. (2021) introduced CLIP, which aligns text and image representations using a contrastive objective.Various foundation models have been proposed, such as FLAVA (Singh et al., 2022), LLaVa (Liu et al., 2023(Liu et al., , 2024a,b),b), Flamingo (Alayrac et al., 2022), OTTER (Li et al., 2023b,a), Fuyu (Bavishi et al., 2023) and more recently OpenAI's GPT4's vision extension.A thorough survey of VLM techniques and tasks was performed recently by Zhang et al. (2024).Typically, a VLM can be built on top of a pre-trained LLM, which is then paired with an image embedding network that can transfer the image into the same token space used by the model, attempting to keep semantic similarity.This approach is employed, for instance, by BLIP (Li et al., 2022) and its successor BLIP2 (Li et al., 2023c).Moreover, these models typically can only consume images as input, but are unable to generate them as an answer, but the general framework can be enhanced with methods for text-to-image generation, such as DALL-E (Ramesh et al., 2021(Ramesh et al., , 2022) ) and GILL (Koh et al., 2023).</p>
<p>A.3 Comparison of Text-Only and Vision-Language Models</p>
<p>To evaluate the effectiveness of the additional plots, we compare our method with a variant using the LLaVa-NeXT (Liu et al., 2024b) VLM.To ensure a fair comparison, we use the same backbone model and repeat the experiments with and without the inclusion of visual information.This consists of a scatter plot of the observations for the seed functions generation step with the overlay of the best previous function (as the model only supports one input image at the time of writing) during the optimization loop.An example of the input plots can be found in Figure 4. We repeat both experiments across five different random seeds and report the results in Table 4. Surprisingly, the performance of the method seems to be unaffected by the presence of the images.This might be due to several factors, among which the fact that the vision encoder of the VLM has not been trained on plots of functions, but rather on natural images, thus, the visual inputs might be out of distribution for the model.We also experimented asking the model facts about the plots in input (such as range of points, maximum and minimum values of the function, shape, first and second derivatives), with no consistent success.It might be that future models will be more amenable to this sort of visual mathematical reasoning, but this is not the case for current VLMs, as was also suggested by recent work (Wang et al., 2024).
Benchmark ICSR-V ICSR R 2 (↑) C (↓) R 2 (↑) C (↓)
Nguyen 0.991 ± 0.003 5.1 ± 0.3 0.994 ± 0.002 5.0 ± 0.3 Constant 0.995 ± 0.001 4.3 ± 0.3 0.995 ± 0.001 3.9 ± 0.3 R 0.988 ± 0.003 5.7 ± 0.5 0.986 ± 0.003 5.7 ± 0.4 Keijzer 0.983 ± 0.006 7.6 ± 0.8 0.984 ± 0.004 7.4 ± 0.8</p>
<p>Overall avg.0.989 ± 0.003 5.7 ± 0.5 0.990 ± 0.003 5.5 ± 0.5</p>
<p>Table 4: Comparison on the impact of additional visual input.All experiments are performed with LLaVa-NeXT as the underlying model, either providing or excluding a plot of the best previous function in the prompts (respectively ICSR-V and ICSR columns).We report the averages with their errors.</p>
<p>B Hyperparameters</p>
<p>We report the hyperparameters used with LLMs (Table 5).As reported in the main text, for ICSR we sample n s = 10 initial seed functions and repeat the optimization loop for 50 iterations, using an acceptance threshold of 0.99999 and repeating the coefficient fitting for 5 times with different initializations.For DSR and uDSR we set the compu-</p>
<p>C Prompts</p>
<p>The prompt used to generate the seed functions is reported in Figure 5, while the prompt used during the optimization loop is reported in Figure 6 and the one used for the random guessing baseline is reported in Figure 7.For the ICSR-V extension presented in Appendix A we add a brief description of the provided plots as well as the image.</p>
<p>D Benchmark functions</p>
<p>The list of functions and point ranges for all the benchmarks can be found in Table 6.The range for training and testing points was taken from the original source where available.Nguyen and Constant do not include a range for the testing points, so we used the same range as the training points but with more sample points.U[min, max, num] indicates points randomly sampled from a uniform distribution between the min and max values, while [min, max, num] indicates a range of equispaced points from min to max.The training points are sampled from U[min, max, num] once and then kept fixed across the random seeds and all tested methods to ensure consistency.</p>
<p>E Sample results</p>
<p>We present a sample of one solution for each function in the benchmarks found by our method, to qualitatively investigate the generated expressions.</p>
<p>The observations are seen in blue, the true function is seen in red and the model's guess is seen in green (Figures 8,9,and 10 and 11).Some of the failures of the models are apparent: in areas where there is a low density of training points the model sometimes makes guesses that ignore the overall trend, as seen, for example, in the R3 equation (Figure 10).The Keijzer benchmark is also much harder in the last 5 equations, with only 20 randomly sampled points to cover a complex 2D space, which can lead to some failures (e.g., in Keijzer 14).</p>
<p>I want you to act as a mathematical function generator.Given a set of points below, you are to come up with 5 potential functions that would fit the points.Don't worry too much about accuracy: your task is to generate a set of functions that are as diverse as possible, so that they can serve as starting points for further optimization.</p>
<p>To generate the functions, you will start from a set of basic operators and expressions, and combine them into something more complex.Your options are:</p>
<p>-An independent variable symbol: x.</p>
<p>-A coefficient symbol: c (there is no need to write a number -write this generic coefficient instead).</p>
<p>-Basic operators: +, -, *, /, ˆ, sqrt, exp, log, abs -Trigonometric expressions: sin, cos, tan, sinh, cosh, tanh Make sure there are no numbers in the functions, use the coefficient token 'c' instead.Analyze the points carefully: if there are any negative points in the input, sqrt and log can not be used unless the input is combined with abs.</p>
<p>The functions should all begin with the indicators "f1(x) = ", "f2(x) = "...Your task is to combine an arbitrary number of these basic blocks to create a complex expression.Don't be afraid to be creative and experiment!The functions should be as complex as possible, combining many different operations.Variety is key!Points: {points} Functions:</p>
<p>Figure 5: Prompt used to generate the seed functions.</p>
<p>I want you to act as a mathematical function generator.You are given a set of points with (x, y) coordinates below: {points} Below are some previous functions and the error they make on the points above.The errors are arranged in order of their fit values, with the highest values coming first, and lower is better.Your task is to give me a list of five new potential functions that are different from all the ones reported below, and have a lower error value than all of the functions below.Only output the new functions and nothing else.Remember that the functions you generate should always have at most {num_variables} variables {variables_list}.The functions should have parametric form, using 'c' in place of any constant or coefficient.The coefficients will be optimized to fit the data.Make absolutely sure that the functions you generate are completely different from the ones already given to you.</p>
<p>The functions should all begin with the indicators "f1(x) = ", "f2(x) = "... Remember that you can combine the simple building blocks (operations, constants, variables) in any way you want to generate more complex functions.Don't be afraid to experiment! {previous_trajectory}</p>
<p>input dimensions: Nguyen (Nguyen et al., 2011), Constant (a modified version of some of the Nguyen equations with different numerical values for the coefficients (Li et al., 2023d)), R (Krawiec and Pawlak, 2013) and Keijzer (Keijzer, 2003).</p>
<p>Figure 2 :
2
Figure2: Comparison across baselines on out of distribution data.We compared the proposed method with the baselines by increasing the input domain for the generated functions.Whenever the R 2 becomes negative, we fix it to 0 when computing the average for the figure on the left and report the fraction of negative values in the figure on the right.</p>
<p>Figure 3 :
3
Figure 3: Out of distribution examples.Qualitative examples demonstrating the generalization capabilities of ICSR and uDSR on two experiments.The higher complexity from the uDSR examples introduces unnecessary terms that harm the out of distribution performance (area shaded in red).</p>
<p>The previous function fit.</p>
<p>Figure 4 :
4
Figure 4: Example of plots used with the VLM.(a) Scatter plot of the observations used when generating the seed functions.(b) Plot of the best function from a previous iteration used in the optimization loop.</p>
<p>Figure 6 :
6
Figure 6: Prompt used during the optimization loop.</p>
<p>Figure 7 :
7
Figure 7: Prompt used for the random guessing baseline.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: ICSR Results for the Nguyen benchmark for the random seed 1.</p>
<p>(Shojaee et al., 2023))ds; NeSymReS(Biggio et al., 2021)and E2E(Kamienny et al., 2022), selected as representatives for Transformer-based model pre-trained over a large-scale SR dataset; and TPSR(Shojaee et al., 2023), which augments E2E with a decoding strategy guided by Monte-Carlo Tree Search, as an efficient combination of pre-training and search.The details of the baseline model and the hyperparameters can be found in Appendix B.We compare various properties of the considered methods in Table1.Thanks to the use of LLMs, ICSR is able to leverage a much larger model size without the need for SR-specific training examples, as opposed to the other Transformer based methods.Furthermore, our method is far more sample
MethodNguyen</p>
<p>Table 5 :
5
tation budget for the number of expressions to evaluate to 200K and extend the vocabulary as {add, sub, mul, div, sin, cos, exp, log, sqrt, n2, abs, n3, n4} and {add, sub, mul, div, sin, cos, exp, log, sqrt, abs, poly} correspondingly.Sampling parameters for the LLMs.
For theNeSymRes model we evaluate the model check-point that has been obtained with the training setof 100M expression skeletons. The actual numberof the equations in the training set is even largersince the values for the coefficients are resampledon each training batch. The beam size in NeSym-Res is set to 10 and the number of restarts for theexternal coefficient optimizer is 10, while for E2Emodel the beam size is 100 but the coefficient op-timizer is applied just once. E2E doesn't benefitfrom restarting the external coefficient optimizer asmuch since E2E predicts the whole equation includ-ing the values of the coefficients. The predictedcoefficients can be further improved by numericaloptimizer but they serve as good initial values. Forall other implementation details, we follow the de-fault hyperparameters provided in the followingrepositories: gplearn 2 , DSR/uDSR 3 , NeSymReS 4and E2E/TPSR 5 .
We release the code at: https://github.com/merlerm/ In-Context-Symbolic-Regression.
https://github.com/trevorstephens/gplearn
https://github.com/dso-org/ deep-symbolic-optimization
https://github.com/SymposiumOrganization/ NeuralSymbolicRegressionThatScales
https://github.com/deep-symbolic-mathematics/ TPSR
AcknowledgementsWe are grateful to Alexander Ilin and Alberto Zabeo for the fruitful discussions.We thank Aalto-IT (IT Services of Aalto University, Finland) for provided support with computational resources.This work was supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI, and grants 352986, 358246) and EU (H2020 grant 101016775 and NextGenerationEU).ExperimentFunctionTrain Points Test Points, 10, 200] [0.05, 10.05, 200]keijzer6
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint</p>
<p>Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Advances in Neural Information Processing Systems. Curran Associates, Inc202235Flamingo: a visual language model for few-shot learning</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Introducing the next generation of Claude. Anthropic, 2024</p>
<p>Fuyu-8b: A multimodal architecture for ai agents. Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sagnak Taşırlar, 2023</p>
<p>Neural symbolic regression that scales. Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, Giambattista Parascandolo, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. 2021arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, International Conference on Learning Representations. Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, Curran Associates, Inc. Alexey Dosovitskiy2020. 202133Advances in Neural Information Processing Systems</p>
<p>Transformers learn higher-order optimization methods for in-context learning: A study with linear models. Deqing Fu, Tian-Qi Chen, Robin Jia, Vatsal Sharan, arXiv:2310.170862023arXiv preprint</p>
<p>Neilands. Stanton A Glantz, Bryan K Slinker, B Torsten, 2017McGraw-Hill EducationNew York, NY</p>
<p>Deep symbolic optimization for electric component sizing in fixed topology power converters. Ruben Glatt, Felipe Leno Da, Silva, Van Hai, Can Bui, Lingxiao Huang, Mengqi Xue, Fangyuan Wang, Yi Chang, Wencong Murphey, Su, AAAI 2022 Workshop on AI for Design and Manufacturing. ADAM2022</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Anton Marc, Shikai Finzi, Andrew Qiu, Wilson Gordon, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Improving Symbolic Regression with Interval Arithmetic and Linear Scaling. Pierre-Alexandre Kamienny, Guillaume Stéphane D'ascoli, Francois Lample, Charton, 10.1007/3-540-36599-0_7Advances in Neural Information Processing Systems. Lecture Notes in Computer Science. Berlin, HeidelbergSpringer2022. 200335Genetic Programming</p>
<p>Iterative Methods for Optimization. C T Kelley, 10.1137/1.97816119709201999Society for Industrial and Applied Mathematics</p>
<p>Generating images with multimodal language models. Jing Yu Koh, Daniel Fried, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Approximating geometric crossover by semantic backpropagation. John R Koza, Riccardo Poli, ; Springer, U S Boston, M A , Krzysztof Krawiec, Tomasz Pawlak, 10.1145/2463372.2463483Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques. Edmund K Burke, Graham Kendall, New York, NY, USAAssociation for Computing Machinery2005. 2013Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, GECCO '13</p>
<p>Deep learning for symbolic mathematics. Guillaume Lample, François Charton, International Conference on Learning Representations. 2019</p>
<p>A unified framework for deep symbolic regression. Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, Brenden K Petersen, Advances in Neural Information Processing Systems. 202235</p>
<p>Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu, arXiv:2311.04219Otterhd: A highresolution multi-modality model. 2023aarXiv preprint</p>
<p>Otter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, arXiv:2305.037262023barXiv preprint</p>
<p>BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLR2023c202</p>
<p>BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLR2022162</p>
<p>Transformer-based model for symbolic regression via joint supervised learning. Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, Songsong Tian, International Conference on Learning Representations. 2023d</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024a</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. 2024b</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024c</p>
<p>Meta Llama 3. 2024</p>
<p>Large language models as general pattern machines. Suvir Mirchandani, Fei Xia, Pete Florence, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, 7th Annual Conference on Robot Learning. 2023</p>
<p>Semantically-based crossover in genetic programming: Application to real-valued symbolic regression. Quang Uy Nguyen, Nguyen Hoai, O' Michael, Robert Neill, Edgar Mckay, Galván-López, 10.1007/s10710-010-9121-2Genetic Programming and Evolvable Machines. 122011</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. Mikel Landajuela Brenden K Petersen, Terrell N Larma, Claudio Prata Mundhenk, Soo Santiago, Kyung Kim, Joanne Taery Kim, International Conference on Learning Representations. 2021</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.06125Hierarchical textconditional image generation with CLIP latents. 2022arXiv preprint</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Age-fitness pareto optimization. Michael Schmidt, Hod Lipson, 10.1007/978-1-4419-7747-2_8Genetic Programming Theory and Practice VIII. Rick Riolo, Trent Mc-Conaghy, Ekaterina Vladislavleva, New York, New York, NYSpringer2011</p>
<p>Transformer-based planning for symbolic regression. Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan Reddy, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, 2024</p>
<p>Llm-Sr, arXiv:2404.18400Scientific equation discovery via programming with large language models. arXiv preprint</p>
<p>FLAVA: A foundational language and vision alignment model. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Paretofront exploitation in symbolic regression. Guido F Smits, Mark Kotanchek, 10.1007/0-387-23254-0_17Genetic Programming Theory and Practice II. Una-May O' Reilly, Tina Yu, Rick Riolo, Bill Worzel, Boston, MASpringer US2005</p>
<p>gplearn: Genetic programming in python. Trevor Stephens, 2022</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>AI Feynman: A physics-inspired method for symbolic regression. Silviu- , Marian Udrescu, Max Tegmark, 10.1126/sciadv.aay2631Science Advances. 61626312020</p>
<p>Symbolicgpt: A generative transformer model for symbolic regression. Mojtaba Valipour, Bowen You, Maysum Panju, Ali Ghodsi, arXiv:2106.141312021arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Improving model-based genetic programming for symbolic regression of small expressions. M Virgolin, T Alderliesten, C Witteveen, P A N Bosman, 10.1162/evco_a_00278Evolutionary Computation. 2922021</p>
<p>SciPy 1.0 Contributors, et al. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, 10.1038/s41592-019-0686-2Nature Methods. 17</p>
<p>Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen, arXiv:2406.18521Charxiv: Charting gaps in realistic chart understanding in multimodal llms. 2024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, ; Zhou, M-J Willis, Hugo G Hiden, Peter Marenbach, Ben Mckay, Gary A Montague, Second international conference on genetic algorithms in engineering systems: innovations and applications. IET2022. 199735Advances in Neural Information Processing Systems</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Denny Quoc V Le, Xinyun Zhou, Chen, International Conference on Learning Representations. 2023</p>
<p>Vision-language models for vision tasks: A survey. Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu, R1 -0.9987 R2 -0.9992 R3 -0.9905IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>ICSR Results for the R benchmark for the random seed 1. keijzer3 -0.9990 keijzer4 -0.8528 keijzer6 -0.9999 keijzer7 -1.0000Figure. 10</p>
<p>Figure 11: ICSR Results for the Keijzer benchmark for the random seed 1. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-272987582</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.20252v2.pdf" target="_blank">What is the Role of Large Language Models in the Evolution of Astronomy Research?</a></p>
                <p><strong>Paper Abstract:</strong> ChatGPT and other state-of-the-art large language models (LLMs) are rapidly transforming multiple fields, offering powerful tools for a wide range of applications. These models, commonly trained on vast datasets, exhibit human-like text generation capabilities, making them useful for research tasks such as ideation, literature review, coding, drafting, and outreach. We conducted a study involving 13 astronomers at different career stages and research fields to explore LLM applications across diverse tasks over several months and to evaluate their performance in research-related activities. This work was accompanied by an anonymous survey assessing participants’ experiences and attitudes towards LLMs. We provide a detailed analysis of the tasks attempted and the survey answers, along with specific output examples. Our findings highlight both the potential and limitations of LLMs in supporting research while also addressing general and research-specific ethical considerations. We conclude with a series of recommendations, emphasizing the need for researchers to complement LLMs with critical thinking and domain expertise, ensuring these tools serve as aids rather than substitutes for rigorous scientific inquiry</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-lit-summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language model literature summarization / knowledge extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generic use of LLMs (ChatGPT, Gemini, Claude, Perplexity, etc.) to ingest papers (PDFs/abstracts) and extract, summarize, compare, or synthesize numeric results and conceptual relationships across publications as a precursor to discovering quantitative relationships or empirical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (ChatGPT-3.5/4, Gemini 1.5 Pro, Claude, Perplexity)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language models and LLM-powered search engines with context windows ranging from ~4k tokens up to 1M tokens (Gemini 1.5 Pro); models vary in parameter count and access to web retrieval depending on provider and tier.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy / astrophysics (general literature across subfields)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships / comparative quantitative results / potential scaling laws (implicit target — not explicitly produced in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Upload/ingest PDFs or supply text; prompt engineering to ask for summaries, extraction of key numeric results and comparisons; use of conversational back-and-forth to refine outputs; optionally combined with retrieval-augmented generation (RAG) or LLM-powered search engines to access citations and web sources.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Individual papers or small sets of papers uploaded as PDFs (examples: 8-page review article tested with Gemini; decadal white paper by Smith et al. 2019 tested with ChatGPT-4); mostly ad-hoc manual uploads in the experiments rather than large-scale corpora; domain models noted that exist are typically trained on abstracts only.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative human inspection by domain experts (authors' spot-checks), survey of users' perceived accuracy/satisfaction; no systematic quantitative benchmark performed for discovering laws.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMs can extract narrative summaries, list datasets used, identify sections/tables/figures and compare high-level results across papers; examples show correct identification of main properties and challenges. The work did not produce distilled mathematical laws or equations from large corpora; outputs remained qualitative and required human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Gemini (1M-token preview) accurately extracted key information and referenced pages/tables/figures from an uploaded review; ChatGPT-4 summarized a decadal white paper and suggested additional challenges/recommendations not explicitly in the paper. No explicit extraction of algebraic/scaling laws or equations was demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Context-window and document-parsing limits (tables often not parsed fully); hallucinations and technical inaccuracies (models sometimes state incorrect physics); oversimplification of nuanced scientific arguments; inability to reliably extract precise numeric values or produce verified quantitative laws without heavy human oversight; legal/copyright issues in building full-text domain corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared informally to human reading: LLMs speed up the skimming/summarization stage and can surface candidate results, but they are inconsistent and cannot replace expert synthesis; no formal baseline metrics or comparisons to statistical data-mining methods were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9597.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9597.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 conversational LLM used in the study for PDF summarization, proposal review experiments, coding assistance, and extracting information from papers; demonstrated strengths at high-level summaries and weaknesses in consistency and factual accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large transformer-based autoregressive language model (OpenAI GPT-4 family). In the paper it is described as having >1 trillion parameters in some descriptions and larger token windows in newer variants; used via web/chat interface with optional web access in newer versions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy (applied to literature summarization, proposal review, and code generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Targeted at extracting/ comparing quantitative results across papers (implicit), not used to derive new mathematical laws in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>PDF upload and direct questioning/prompting (e.g., ask to summarise paper, list challenges, or compare proposals); iterative prompts and persona specification; human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Single-paper uploads (PDFs) and prompt-supplied text; experiments described used a small number of individual documents (e.g., an 8-page review, decadal white paper, and a handful of telescope proposals) rather than large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Qualitative assessment by the astronomer participants (spot-checks), survey of user satisfaction; small-scale experiments (e.g., 5 ESO proposals) compared to human reviewer grades for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChatGPT-4 can produce coherent summaries, identify key sections and figures, and suggest gaps/ recommendations across documents; in a small test it consistently graded proposals highly and showed inconsistent ranking behavior — indicating poor calibration for quantitative scoring tasks. No successful automated distillation of new quantitative laws was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Summarized Smith et al. (2019) and produced plausible additional challenges; when asked to grade ESO proposals it tended to give uniformly high scores and could be influenced by prompts, showing inconsistent quantitative ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Prone to hallucinations (e.g., invented citations or incorrect technical claims), inconsistent scoring behavior, inability to reliably detect proposal weaknesses or assign calibrated numeric grades, context-window saturation in iterative debugging sessions, and reliance on users to fact-check technical claims.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Against human reviewers: ChatGPT-4 could extract items and address rubric questions but failed to match human consistency or nuanced impact assessment; no quantitative metrics beyond qualitative mismatch and clustering of grades were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9597.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9597.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 1.5 Pro (preview with extended context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini model (preview of a 1M-token context-window instance) used to parse and summarize uploaded PDFs and reference page/figure locations; shown to extract high-level descriptions of figures and identify section boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5 Pro (preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google AI's multimodal transformer-based model with very large context windows (paper cites standard 128k tokens and advanced up to 1M tokens for preview); enables PDF uploads and long-document understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy literature summarization and document parsing</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Intended for extraction of results/relationships from documents (qualitative summarization rather than automated equation discovery in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Large-context document ingestion (PDF upload) followed by targeted prompts to identify which sections address particular concepts, explain figures, and extract information with page/table references; cross-check with web search when available.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Single-document uploads (example: an 8-page review article with tables/figures), processed through the model's long-context pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human inspection of extracted summaries and correctness of figure/section references; no benchmarked quantitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Gemini extracted information and could explain figures at a non-expert technical level and reference page numbers/tables; nevertheless it sometimes lacked deep technical detail and can miss or fail to parse tables/figure internals depending on format.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Successfully identified sections focused on different concepts and referenced page numbers/tables/figures for an uploaded review; explanations were high-level rather than deeply technical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Document parsing limitations (tables often not fully processed), variable technical depth in explanations, occasional factual errors, and reliance on the correctness of input parsing (raster vs vector figures).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Faster than manual skimming for locating relevant sections; quality lower than expert reading for deep technical detail. No quantitative baseline metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9597.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9597.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Astro-LLAMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Astro-LLAMA (AstroLLaMA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized LLM trained for astronomy (reported in 2023) intended to answer astronomy-specific questions more accurately by leveraging astronomy literature, but at the time of the paper it was trained primarily on abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Astro-LLAMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A specialized foundation model for astronomy derived from LLaMA-style architectures and fine-tuned on astronomy texts/abstracts (as described by Dung Nguyen et al. 2023); specific parameter counts and training regimen not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy (domain-specific knowledge extraction and Q/A)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Potential target: domain-specific empirical findings and relationships (model currently limited to abstract-level information)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Domain fine-tuning on astronomical abstracts to improve domain accuracy; envisioned use includes Q/A about literature and connecting publications.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Trained primarily on abstracts (arXiv abstracts and possibly other indexed metadata) rather than full-text articles; therefore lacks full experimental/numeric details present in body/tables.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper for law-distillation tasks; noted constraints are from training corpus coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Promising for answering domain-specific queries based on abstracts, but limited ability to surface detailed numeric results or full-method details because the full-text was not (yet) incorporated; no examples of distilled quantitative laws provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Cited as an example of an astronomy-tuned model that could in principle make more accurate domain queries but presently constrained by abstract-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Abstract-only training omits detailed analysis, tables and numeric results; legal and copyright constraints complicate ingestion of full-text corpora; therefore limited for automatic extraction of precise quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Expected to outperform general LLMs on abstract-level Q/A but likely underperform on deep numeric extraction compared to a model trained on full-text or structured datasets; no formal comparison reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9597.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9597.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pathfinder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pathfinder: semantic framework for literature review and knowledge discovery in astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prototype semantic/chatbot platform (Iyer et al. 2024b) referenced as an example of a system designed to perform literature review and knowledge discovery across astronomy publications, enabling better retrieval and connection of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pathfinder (prototype platform)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A semantic retrieval and conversational framework integrating literature embeddings, search and LLM reasoning to support literature review and knowledge discovery in astronomy; specific architecture details are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Astronomy literature mining / knowledge discovery</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Intended for extraction of relations, links between results and potentially surfacing undiscovered connections (qualitative and possibly quantitative relationship discovery with further development)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Semantic indexing/embeddings of abstracts (current prototypes) combined with LLM-based retrieval and conversational interfaces; potential use of RAG and citation-aware retrieval components.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Currently prototypes focus on abstracts/metadata rather than full-text; corpus likely includes arXiv/ADS abstracts and structured metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated for automated discovery of quantitative laws in this paper; described as under development with potential to cite sources and answer domain questions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Described as promising for connecting publications and assisting literature review; however, current prototypes trained on abstracts cannot access detailed numeric findings embedded in full texts/tables, so the platform has limited ability to discover explicit equations or laws without expanded corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Mentioned as a recently published prototype that can answer more specific questions with greater accuracy and potentially make connections between publications while citing sources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited by abstract-only training; legal/ethical concerns for ingesting full-text corpora; need for careful system design to avoid hallucination and to handle tables/figures/numeric extraction robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared quantitatively in this paper; noted as an improvement over general LLMs for abstract-level queries but still lacking full-text depth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9597.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9597.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodological family combining document retrieval from an external corpus with LLM generation to ground outputs in retrieved source material, used and discussed as a mitigation against hallucinations and as an enabler for citing sources and retrieving numeric facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>What is the Role of Large Language Models in the Evolution of Astronomy Research?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-Augmented Generation (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid approach where a retrieval module (e.g., embeddings + vector store/search) selects relevant documents or passages which are then provided as context to an LLM for grounded generation; not a single model but a pipeline design.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature mining (applied/mentioned for astronomy literature tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Supports extraction of factual/numeric information that could underpin empirical relationships if applied at scale and with structured retrieval and parsing (method-level enabler)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use of external corpus retrieval (search/index) to supply evidence passages to the LLM during generation, combined with post-processing and human verification to reduce hallucination; suggested as a mitigation mechanism in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>External web resources or curated document stores (e.g., papers, PDFs, ADS/arXiv metadata); paper discusses that many LLM services with web connections perform better on citation linking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Discussed qualitatively: reduction of fabricated citations and factual errors observed anecdotally (e.g., Chen & Chen 2023 result that GPT-4 made up fewer citations than GPT-3.5) but no quantitative RAG-specific evaluation performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>RAG-style approaches are presented as a mechanism that reduces hallucinations and helps LLMs link to real resources, but the paper stresses that hallucinations are only reduced, not eliminated; no distilled quantitative laws were produced using RAG in the experiments described.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Citation: Chen & Chen (2023) example cited that GPT-3.5 invented many citations while GPT-4 invented fewer when augmentations or newer mitigations were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RAG reduces but does not remove hallucinations; retrieval quality depends on corpus coverage and legality of using full-text corpora; numeric extraction still requires careful parsing (tables/figures), and post-processing to convert text mentions into validated equations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to raw LLM generation: RAG improves groundedness and reduces fabricated citations, but requires an accurate and legally ingestible retrieval corpus and still needs human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What is the Role of Large Language Models in the Evolution of Astronomy Research?', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>pathfinder: A semantic framework for literature review and knowledge discovery in astronomy <em>(Rating: 2)</em></li>
                <li>AstroLLaMA: Towards Specialized Foundation Models in Astronomy <em>(Rating: 2)</em></li>
                <li>Designing an evaluation framework for large language models in astronomy research <em>(Rating: 2)</em></li>
                <li>Accuracy of Chatbots in Citing Journal Articles <em>(Rating: 1)</em></li>
                <li>Scientific text analysis with robots applied to observatory proposals <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9597",
    "paper_id": "paper-272987582",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "LLM-lit-summarization",
            "name_full": "Large language model literature summarization / knowledge extraction",
            "brief_description": "Generic use of LLMs (ChatGPT, Gemini, Claude, Perplexity, etc.) to ingest papers (PDFs/abstracts) and extract, summarize, compare, or synthesize numeric results and conceptual relationships across publications as a precursor to discovering quantitative relationships or empirical laws.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "use",
            "model_name": "Various (ChatGPT-3.5/4, Gemini 1.5 Pro, Claude, Perplexity)",
            "model_description": "Transformer-based large language models and LLM-powered search engines with context windows ranging from ~4k tokens up to 1M tokens (Gemini 1.5 Pro); models vary in parameter count and access to web retrieval depending on provider and tier.",
            "scientific_domain": "Astronomy / astrophysics (general literature across subfields)",
            "law_type": "Empirical relationships / comparative quantitative results / potential scaling laws (implicit target — not explicitly produced in experiments)",
            "method_description": "Upload/ingest PDFs or supply text; prompt engineering to ask for summaries, extraction of key numeric results and comparisons; use of conversational back-and-forth to refine outputs; optionally combined with retrieval-augmented generation (RAG) or LLM-powered search engines to access citations and web sources.",
            "input_corpus_description": "Individual papers or small sets of papers uploaded as PDFs (examples: 8-page review article tested with Gemini; decadal white paper by Smith et al. 2019 tested with ChatGPT-4); mostly ad-hoc manual uploads in the experiments rather than large-scale corpora; domain models noted that exist are typically trained on abstracts only.",
            "evaluation_method": "Qualitative human inspection by domain experts (authors' spot-checks), survey of users' perceived accuracy/satisfaction; no systematic quantitative benchmark performed for discovering laws.",
            "results_summary": "LLMs can extract narrative summaries, list datasets used, identify sections/tables/figures and compare high-level results across papers; examples show correct identification of main properties and challenges. The work did not produce distilled mathematical laws or equations from large corpora; outputs remained qualitative and required human verification.",
            "notable_examples": "Gemini (1M-token preview) accurately extracted key information and referenced pages/tables/figures from an uploaded review; ChatGPT-4 summarized a decadal white paper and suggested additional challenges/recommendations not explicitly in the paper. No explicit extraction of algebraic/scaling laws or equations was demonstrated.",
            "limitations_challenges": "Context-window and document-parsing limits (tables often not parsed fully); hallucinations and technical inaccuracies (models sometimes state incorrect physics); oversimplification of nuanced scientific arguments; inability to reliably extract precise numeric values or produce verified quantitative laws without heavy human oversight; legal/copyright issues in building full-text domain corpora.",
            "baseline_comparison": "Compared informally to human reading: LLMs speed up the skimming/summarization stage and can surface candidate results, but they are inconsistent and cannot replace expert synthesis; no formal baseline metrics or comparisons to statistical data-mining methods were reported.",
            "uuid": "e9597.0",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ChatGPT-4",
            "name_full": "ChatGPT (GPT-4)",
            "brief_description": "OpenAI's GPT-4 conversational LLM used in the study for PDF summarization, proposal review experiments, coding assistance, and extracting information from papers; demonstrated strengths at high-level summaries and weaknesses in consistency and factual accuracy.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4",
            "model_description": "A large transformer-based autoregressive language model (OpenAI GPT-4 family). In the paper it is described as having &gt;1 trillion parameters in some descriptions and larger token windows in newer variants; used via web/chat interface with optional web access in newer versions.",
            "scientific_domain": "Astronomy (applied to literature summarization, proposal review, and code generation tasks)",
            "law_type": "Targeted at extracting/ comparing quantitative results across papers (implicit), not used to derive new mathematical laws in experiments",
            "method_description": "PDF upload and direct questioning/prompting (e.g., ask to summarise paper, list challenges, or compare proposals); iterative prompts and persona specification; human-in-the-loop verification.",
            "input_corpus_description": "Single-paper uploads (PDFs) and prompt-supplied text; experiments described used a small number of individual documents (e.g., an 8-page review, decadal white paper, and a handful of telescope proposals) rather than large corpora.",
            "evaluation_method": "Qualitative assessment by the astronomer participants (spot-checks), survey of user satisfaction; small-scale experiments (e.g., 5 ESO proposals) compared to human reviewer grades for consistency.",
            "results_summary": "ChatGPT-4 can produce coherent summaries, identify key sections and figures, and suggest gaps/ recommendations across documents; in a small test it consistently graded proposals highly and showed inconsistent ranking behavior — indicating poor calibration for quantitative scoring tasks. No successful automated distillation of new quantitative laws was reported.",
            "notable_examples": "Summarized Smith et al. (2019) and produced plausible additional challenges; when asked to grade ESO proposals it tended to give uniformly high scores and could be influenced by prompts, showing inconsistent quantitative ranking.",
            "limitations_challenges": "Prone to hallucinations (e.g., invented citations or incorrect technical claims), inconsistent scoring behavior, inability to reliably detect proposal weaknesses or assign calibrated numeric grades, context-window saturation in iterative debugging sessions, and reliance on users to fact-check technical claims.",
            "baseline_comparison": "Against human reviewers: ChatGPT-4 could extract items and address rubric questions but failed to match human consistency or nuanced impact assessment; no quantitative metrics beyond qualitative mismatch and clustering of grades were provided.",
            "uuid": "e9597.1",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Gemini-1.5-Pro",
            "name_full": "Google Gemini 1.5 Pro (preview with extended context)",
            "brief_description": "Google's Gemini model (preview of a 1M-token context-window instance) used to parse and summarize uploaded PDFs and reference page/figure locations; shown to extract high-level descriptions of figures and identify section boundaries.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5 Pro (preview)",
            "model_description": "Google AI's multimodal transformer-based model with very large context windows (paper cites standard 128k tokens and advanced up to 1M tokens for preview); enables PDF uploads and long-document understanding.",
            "scientific_domain": "Astronomy literature summarization and document parsing",
            "law_type": "Intended for extraction of results/relationships from documents (qualitative summarization rather than automated equation discovery in experiments)",
            "method_description": "Large-context document ingestion (PDF upload) followed by targeted prompts to identify which sections address particular concepts, explain figures, and extract information with page/table references; cross-check with web search when available.",
            "input_corpus_description": "Single-document uploads (example: an 8-page review article with tables/figures), processed through the model's long-context pipeline.",
            "evaluation_method": "Human inspection of extracted summaries and correctness of figure/section references; no benchmarked quantitative evaluation.",
            "results_summary": "Gemini extracted information and could explain figures at a non-expert technical level and reference page numbers/tables; nevertheless it sometimes lacked deep technical detail and can miss or fail to parse tables/figure internals depending on format.",
            "notable_examples": "Successfully identified sections focused on different concepts and referenced page numbers/tables/figures for an uploaded review; explanations were high-level rather than deeply technical.",
            "limitations_challenges": "Document parsing limitations (tables often not fully processed), variable technical depth in explanations, occasional factual errors, and reliance on the correctness of input parsing (raster vs vector figures).",
            "baseline_comparison": "Faster than manual skimming for locating relevant sections; quality lower than expert reading for deep technical detail. No quantitative baseline metrics provided.",
            "uuid": "e9597.2",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Astro-LLAMA",
            "name_full": "Astro-LLAMA (AstroLLaMA)",
            "brief_description": "A domain-specialized LLM trained for astronomy (reported in 2023) intended to answer astronomy-specific questions more accurately by leveraging astronomy literature, but at the time of the paper it was trained primarily on abstracts.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "mention",
            "model_name": "Astro-LLAMA",
            "model_description": "A specialized foundation model for astronomy derived from LLaMA-style architectures and fine-tuned on astronomy texts/abstracts (as described by Dung Nguyen et al. 2023); specific parameter counts and training regimen not detailed in this paper.",
            "scientific_domain": "Astronomy (domain-specific knowledge extraction and Q/A)",
            "law_type": "Potential target: domain-specific empirical findings and relationships (model currently limited to abstract-level information)",
            "method_description": "Domain fine-tuning on astronomical abstracts to improve domain accuracy; envisioned use includes Q/A about literature and connecting publications.",
            "input_corpus_description": "Trained primarily on abstracts (arXiv abstracts and possibly other indexed metadata) rather than full-text articles; therefore lacks full experimental/numeric details present in body/tables.",
            "evaluation_method": "Not evaluated in this paper for law-distillation tasks; noted constraints are from training corpus coverage.",
            "results_summary": "Promising for answering domain-specific queries based on abstracts, but limited ability to surface detailed numeric results or full-method details because the full-text was not (yet) incorporated; no examples of distilled quantitative laws provided.",
            "notable_examples": "Cited as an example of an astronomy-tuned model that could in principle make more accurate domain queries but presently constrained by abstract-only training.",
            "limitations_challenges": "Abstract-only training omits detailed analysis, tables and numeric results; legal and copyright constraints complicate ingestion of full-text corpora; therefore limited for automatic extraction of precise quantitative laws.",
            "baseline_comparison": "Expected to outperform general LLMs on abstract-level Q/A but likely underperform on deep numeric extraction compared to a model trained on full-text or structured datasets; no formal comparison reported in this paper.",
            "uuid": "e9597.3",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "pathfinder",
            "name_full": "pathfinder: semantic framework for literature review and knowledge discovery in astronomy",
            "brief_description": "A prototype semantic/chatbot platform (Iyer et al. 2024b) referenced as an example of a system designed to perform literature review and knowledge discovery across astronomy publications, enabling better retrieval and connection of results.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "mention",
            "model_name": "pathfinder (prototype platform)",
            "model_description": "A semantic retrieval and conversational framework integrating literature embeddings, search and LLM reasoning to support literature review and knowledge discovery in astronomy; specific architecture details are in the cited work.",
            "scientific_domain": "Astronomy literature mining / knowledge discovery",
            "law_type": "Intended for extraction of relations, links between results and potentially surfacing undiscovered connections (qualitative and possibly quantitative relationship discovery with further development)",
            "method_description": "Semantic indexing/embeddings of abstracts (current prototypes) combined with LLM-based retrieval and conversational interfaces; potential use of RAG and citation-aware retrieval components.",
            "input_corpus_description": "Currently prototypes focus on abstracts/metadata rather than full-text; corpus likely includes arXiv/ADS abstracts and structured metadata.",
            "evaluation_method": "Not evaluated for automated discovery of quantitative laws in this paper; described as under development with potential to cite sources and answer domain questions.",
            "results_summary": "Described as promising for connecting publications and assisting literature review; however, current prototypes trained on abstracts cannot access detailed numeric findings embedded in full texts/tables, so the platform has limited ability to discover explicit equations or laws without expanded corpus.",
            "notable_examples": "Mentioned as a recently published prototype that can answer more specific questions with greater accuracy and potentially make connections between publications while citing sources.",
            "limitations_challenges": "Limited by abstract-only training; legal/ethical concerns for ingesting full-text corpora; need for careful system design to avoid hallucination and to handle tables/figures/numeric extraction robustly.",
            "baseline_comparison": "Not compared quantitatively in this paper; noted as an improvement over general LLMs for abstract-level queries but still lacking full-text depth.",
            "uuid": "e9597.4",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A methodological family combining document retrieval from an external corpus with LLM generation to ground outputs in retrieved source material, used and discussed as a mitigation against hallucinations and as an enabler for citing sources and retrieving numeric facts.",
            "citation_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "mention_or_use": "mention",
            "model_name": "Retrieval-Augmented Generation (method)",
            "model_description": "A hybrid approach where a retrieval module (e.g., embeddings + vector store/search) selects relevant documents or passages which are then provided as context to an LLM for grounded generation; not a single model but a pipeline design.",
            "scientific_domain": "General scientific literature mining (applied/mentioned for astronomy literature tasks)",
            "law_type": "Supports extraction of factual/numeric information that could underpin empirical relationships if applied at scale and with structured retrieval and parsing (method-level enabler)",
            "method_description": "Use of external corpus retrieval (search/index) to supply evidence passages to the LLM during generation, combined with post-processing and human verification to reduce hallucination; suggested as a mitigation mechanism in the paper.",
            "input_corpus_description": "External web resources or curated document stores (e.g., papers, PDFs, ADS/arXiv metadata); paper discusses that many LLM services with web connections perform better on citation linking.",
            "evaluation_method": "Discussed qualitatively: reduction of fabricated citations and factual errors observed anecdotally (e.g., Chen & Chen 2023 result that GPT-4 made up fewer citations than GPT-3.5) but no quantitative RAG-specific evaluation performed in this study.",
            "results_summary": "RAG-style approaches are presented as a mechanism that reduces hallucinations and helps LLMs link to real resources, but the paper stresses that hallucinations are only reduced, not eliminated; no distilled quantitative laws were produced using RAG in the experiments described.",
            "notable_examples": "Citation: Chen & Chen (2023) example cited that GPT-3.5 invented many citations while GPT-4 invented fewer when augmentations or newer mitigations were applied.",
            "limitations_challenges": "RAG reduces but does not remove hallucinations; retrieval quality depends on corpus coverage and legality of using full-text corpora; numeric extraction still requires careful parsing (tables/figures), and post-processing to convert text mentions into validated equations.",
            "baseline_comparison": "Compared conceptually to raw LLM generation: RAG improves groundedness and reduces fabricated citations, but requires an accurate and legally ingestible retrieval corpus and still needs human verification.",
            "uuid": "e9597.5",
            "source_info": {
                "paper_title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "pathfinder: A semantic framework for literature review and knowledge discovery in astronomy",
            "rating": 2,
            "sanitized_title": "pathfinder_a_semantic_framework_for_literature_review_and_knowledge_discovery_in_astronomy"
        },
        {
            "paper_title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
            "rating": 2,
            "sanitized_title": "astrollama_towards_specialized_foundation_models_in_astronomy"
        },
        {
            "paper_title": "Designing an evaluation framework for large language models in astronomy research",
            "rating": 2,
            "sanitized_title": "designing_an_evaluation_framework_for_large_language_models_in_astronomy_research"
        },
        {
            "paper_title": "Accuracy of Chatbots in Citing Journal Articles",
            "rating": 1,
            "sanitized_title": "accuracy_of_chatbots_in_citing_journal_articles"
        },
        {
            "paper_title": "Scientific text analysis with robots applied to observatory proposals",
            "rating": 1,
            "sanitized_title": "scientific_text_analysis_with_robots_applied_to_observatory_proposals"
        }
    ],
    "cost": 0.020484,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What is the Role of Large Language Models in the Evolution of Astronomy Research?
2 October 2024</p>
<p>Morgan Fouesneau 0000-0001-9256-5516
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Ivelina G Momcheva 0000-0003-1665-2073
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Urmila Chadayammuri 0000-0003-2521-506X
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Mariia Demianenko 0000-0002-8297-6386
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Heidelberg Institute for Theoretical Studies
Schloss-Wolfsbrunnenweg 3569118Astroinformatics, HeidelbergGermany</p>
<p>Antoine Dumont 0000-0003-0234-3376
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Raphael E Hviding 0000-0002-4684-9005
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>K Angelique Kahle 0000-0001-7714-7551
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Nadiia Pulatova 
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Bhavesh Rajpoot 0009-0004-9729-6377
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Heidelberg University
Grabengasse 169117HeidelbergGermany</p>
<p>Marten B Scheuck 0000-0002-6108-6931
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Rhys Seeburger 0000-0001-8898-9463
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Dmitry Semenov 0000-0002-3913-7114
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>Jaime I Villaseñor 0000-0002-7984-1675
Max-Planck-Institut für Astronomie (MPIA)
Königstuhl 1769117HeidelbergGermany</p>
<p>What is the Role of Large Language Models in the Evolution of Astronomy Research?
2 October 2024C78E3CD6664C227B8906FAD01E9CABAAarXiv:2409.20252v2[astro-ph.IM]Accepted XXX. Received YYY; in original form ZZZLarge language models -Science system -Astrophysics -Instrumentation and Methods for Astrophysics
ChatGPT and other state-of-the-art large language models (LLMs) are rapidly transforming multiple fields, offering powerful tools for a wide range of applications.These models, commonly trained on vast datasets, exhibit human-like text generation capabilities, making them useful for research tasks such as ideation, literature review, coding, drafting, and outreach.We conducted a study involving 13 astronomers at different career stages and research fields to explore LLM applications across diverse tasks over several months and to evaluate their performance in research-related activities.This work was accompanied by an anonymous survey assessing participants' experiences and attitudes towards LLMs.We provide a detailed analysis of the tasks attempted and the survey answers, along with specific output examples.Our findings highlight both the potential and limitations of LLMs in supporting research while also addressing general and research-specific ethical considerations.We conclude with a series of recommendations, emphasizing the need for researchers to complement LLMs with critical thinking and domain expertise, ensuring these tools serve as aids rather than substitutes for rigorous scientific inquiry.</p>
<p>INTRODUCTION</p>
<p>Language models are probabilistic representations of the natural human language.The first such models appeared in the 1980s and were purely statistical, based on the frequencies of co-occurrence of phrases of different lengths, so-called n-grams.The power of language models has grown dramatically in the last 5 years with the development of transformers (Vaswani et al. 2017).A transformer is an advanced machine learning model that improves how data sequences are analyzed and generated by simultaneously considering the full context of the input and focusing on the most relevant parts, enhancing performance in language-related tasks.This new architecture, combined with neural networks and large datasets (frequently scraped from the internet), has led to the development of the current (as of mid-2024) generation of Large Language Models (LLMs) as advanced artificial intelligence (AI) systems capable of parsing and generating human-like text.</p>
<p>Current LLMs represent a tremendous technological leap.Unlike other leaps of technology, which were expensive and very few people had access to them at first, LLMs are (in many cases) free and available to (almost) anyone with an internet connection.As a result, their ⋆ E-mail: fouesneau@mpia.deadoption has also been incredibly fast -ChatGPT reached 100 million users within only two months after its launch (Hu 2023) while other services of similar popularity took between 9 months and 4.5 years to reach the same userbase 1 .Unlike other technologies primarily automating repetitive work, LLMs excel at creative work like writing, coding, and generating ideas.Finally, the literature surrounding productivity boosts attributed to large language models (LLMs) suggests that there are significant improvements in various tasks, with estimates ranging from 20% to 80% in productivity boosts across different sectors (Peng et al. 2023;Noy &amp; Zhang 2023;Eloundou et al. 2023) compared to the 18-22% improvement brought on by steam power (Atack et al. 2008).</p>
<p>Also, unlike other technological advances, LLMs behave very differently from traditional technology.As discussed in (Mollick 2024, p. 65-67), LLMs defy our expectations for software functionality: software should produce predictable, reliable, logical outcomes, and LLMs do not.On the contrary, LLMs (at the moment, at least) struggle with tasks that software is generally good at, such as repeating tasks, reproducing facts, and performing calculations.In contrast, they perform well on tasks we consider uniquely human: writing, chatting, analyzing, coding, brainstorming, and creating.In addition, LLMs have "features" that we do not encounter in other technologies: they confidently invent facts and make mistakes, aka hallucinate.These latter features make many researchers suspicious of using them in a research context.</p>
<p>LLMs are expected to have a large impact on a wide range of professions, especially those in highly paid and creative fields, including STEM.A 2023 McKinsey report (Ellingrud et al. 2023) predicts that 16% of the hours worked by STEM professionals today will be automated by generative AI by 2030.Undoubtedly, a growing number of astronomers have tried LLMs, and many frequently utilize them for a wide range of tasks, sometimes secretly and sometimes openly.Several universities have communicated recommendations to staff and students on the appropriate use of LLMs in teaching.Still, only a handful of professional astronomical entities and journals have published guidance on the recommended use of LLMs specifically focused on research manuscripts and the peer-review process, leaving a vacuum for other professional tasks.Astarita et al. (2024) have shown that the uptake of LLMs in journal articles is likely high.While many raise concerns about how LLMs may change education (e.g., Fyfe 2022; Perkins 2023), we are only starting to consider the implications on scientific research (e.g., Chubb et al. 2021;Dwivedi et al. 2023).</p>
<p>Like any new technology, LLMs have caveats and deserve technical and ethical scrutiny.Some of the questions raised are: How accurate are their responses in the context of tasks done by researchers?Are they capable of writing code suitable for scientific tasks, and is the code accurate?Does using LLMs for editorial corrections produce the voice expected in academic writing?Does the use of LLMs compromise scientific integrity?Where lies the ethical boundary of using LLMs?Is the quality of LLM responses correlated with the cost of the tools?How should we acknowledge the use of such tools?In light of this growing use of LLMs to conduct research, it appeared essential to explore tasks particularly well suited to LLMs, identify good practices, raise awareness of their potential impacts and limitations, and make recommendations on their responsible use.</p>
<p>To explore the potential uses of LLMs in a research context, we initiated a study involving a group of research astronomers at different career stages.The group included Master's and PhD students, postdocs, and staff scientists.The research areas and the prior experience level also varied across the group.The purpose of this study was to investigate as users the following four main areas: (a) the potential applications and limitations in using LLMs to do astronomy research and daily tasks, (b) the positive and negative effects of LLMs on the science research environment, (c) the necessary skills to effectively utilize LLMs and (d) the ethical considerations associated with the use of LLMs in science.We stress that we are not researching the technology but considering LLMs as a technology to assist us in our research.</p>
<p>This paper is organized as follows.Sect. 2 describe our study approach, the major tasks we considered part of an astronomer's work, and the services we explored to tackle these tasks.To extend our viewpoints, we collected our colleagues' input in a survey we describe in Sect. 3 (and Appendix B).Sect. 4 describes the experiments and results for the individually identified use cases.Following the presentation of our findings, Sect. 5 summarizes the main benefits and limitations, discusses legal and ethical concerns, and Sect.6 draws general tips for using LLMs for research before we summarize our study in Sect.7.</p>
<p>EXPERIMENTAL APPROACH</p>
<p>In October 2023, we called researchers within the Max Planck Institute of Astronomy to join a group to explore LLMs' potential, limitations, and impacts on scientific work.The call was open to institute members from all career levels and scientific departments.Approximately 20 people volunteered, ∼15 of whom joined the work of the group in some capacity or another.These participants were primarily graduate students and postdocs but also staff with diverse data science expertise and various scientific interests.Only a minority of this group was familiar with the technical aspects of LLMs; however, delving into the internal details of LLMs was not the group's task.The work of the group consisted of a series of regular meetings chaired by the lead author, where the group discussed logistics, ideas, and results from various experiments.</p>
<p>The group identified a set of LLM services available at the time (November 2023), and participants volunteered to try one or more of them.Specifically, the services identified and explored are listed in table 1 with a more detailed technical description provided in Appendix A. This list is not exhaustive and reflects LLMs' capabilities at the time of writing.</p>
<p>As one can see from the list in table 1, the range of services is broad.It includes conversational interfaces such as ChatGPT, models trained for specific tasks (e.g., Grammarly, Copilot), and LLMpowered search engines (e.g., Perplexity, you.com).We also need to highlight that this is not, by any means, an exhaustive list of applications currently on the market.Neither is this an endorsement of these services.In each category above, there is solid competition, and we encourage readers to explore alternatives to find one that fits their budget and work style.We also tested image generation models, Midjourney and DALL-E.</p>
<p>Several services we used offer both a free and a paid use tier.Our group received a budget of ∼1000 Euro for this project, and we acquired licenses for ChatGPT, Perplexity, and Grammarly for the duration of the exploratory work.Most accounts were open between 3 and 5 months.Additionally, GitHub provides students and academics free access to Copilot for educational purposes.Participants who used Copilot were able to access it through this program.Finally, our institute provides staff with access to DeepL Pro.We only used the free tier services for other providers.</p>
<p>The first phase of the experiment was ideation.Participants who had used LLMs in research tasks shared experiences, and the group brainstormed other potential opportunities.Participants were encouraged to "invite LLMs to every conversation", i.e., to try to incorporate LLMs in all tasks to learn about their strengths, weaknesses, and unique features.In this way, we attempted to map the "Jagged Frontier of AI" for astronomy researchers, as Dell 'Acqua et al. (2023) have named the line between tasks that LLMs are good at and those they fail at.We identified primary use cases that we grouped into the following six categories:</p>
<p>• Text generation and improvement including tasks such as drafting emails, letters, papers, and proposals; editing and proofreading academic manuscripts; formatting citations and bibliographies; content generation for teaching, including quizzes, worksheets, presentations, and syllabuses (also see item on Teaching below); generating simple explanations of complex concepts; brainstorming topics and ideas for content.Tasks in this category also include professional correspondence.</p>
<p>• Summarizing and analyzing content including tasks such as summarizing individual documents or multiple documents to extract key points; identifying gaps in analysis; identifying similarities and differences across publications; creating summaries of existing research and literature; summarizing papers for various audiences; identifying trends in the field.Most of these tasks are especially well suited to literature review.With access to the internet now, LLMs can identify what datasets specific papers used, look up citations, and summarize them.An extension of this process is creating tailored models for discussing literature and research (e.g.Ciucă &amp; Ting 2023;Wu et al. 2024, and related work by Universe TBD 2 ) however this is beyond the scope of our work.</p>
<p>• Software development including tasks such as generating and analyzing code; debugging and refactoring code; code documentation and explanation; converting code between programming languages.With assistance from GitHub Copilot, Peng et al. (2023) found developers were able to complete their tasks 55.8% faster than without.LLMs can create pieces of scripts for complex tasks, and they can suggest ideas or approaches that might be outside the programmer's direct scope/knowledge, which can lead to either a learning effect and/or improved code.</p>
<p>• Data Analysis and categorization LLMs can process and analyze vast amounts of data, extracting key insights and information that would be challenging and time-consuming for humans to identify and aiding in the decision-making processes.Tasks here include processing and analyzing large datasets to extract insights, identifying trends and patterns in data, and categorizing and organizing data.General-purpose LLMs can assist in this task with some effort, but there is a growing number of offers for specialized assistants that focus on data exploration tasks (e.g., Deepnote).</p>
<p>• Teaching: including tasks such as generating explanations of concepts for students of different levels, creating and refining materials through interactive prompts, creating educational content like quizzes and interactive learning materials, and using LLMs as interactive tools for learning new material.Significant debate exists in the educational community on using LLMs in teaching.As part of the educational system, we must provide students with basic AI literacy, even if we do not use LLMs in the educational process.</p>
<p>• Image and video generation: Some large language models interconnect with image generation models, which can create images and videos from text descriptions.One can use these in illustrations, presentations, and outreach material.LLMs can also improve accessibility by creating text descriptions for images and videos.However, we must ensure such models do not replace or misuse the work of artists and illustrators.</p>
<p>This manuscript focuses on the first four categories of applications 2 https://universetbd.org/ and outreach because this is where the core expertise of our team lies.While we did some work on the applications of LLMs to teaching, we left such exploration to experts in these fields.</p>
<p>Following the ideation phase, group participants self-assigned themselves to test specific use cases.These tests were done over three months in the Spring of 2024.Regular meetings were held to discuss progress and coordinate the efforts.Participants documented their ongoing work in a shared document and wrote a description of their work, results, and general feedback at the end of the project (April 2024).These notes serve as the basis for the following sections.Appendix 1 contains excerpts from specific LLM interactions.</p>
<p>Since the group had a set of limited viewpoints and, perhaps, the biased attitude of early adopters, we also designed and circulated an anonymous survey to all institute members to probe the attitudes and uses of LLMs.</p>
<p>EXTENDING FINDINGS THROUGH A SURVEY</p>
<p>A survey on the attitudes towards and uses of LLMs ran from March 11 to 28 among the employees of MPIA and collected 27 responses.The survey participants range from interns to professors.The questions and responses are detailed in Appendix B. This section discusses the parts of the survey that focused on general questions, ethics, and overall satisfaction.We will discuss the results of this survey in the relevant use-case sections throughout the next sections.</p>
<p>Among the 27 survey participants, 74% use LLMs at least several times a week.ChatGPT-3.5 and ChatGPT-4 are the most commonly used, with 89% and 44% of participants indicating that they have experience with them (multiple answers were possible).Following ChatGPT in the list of most frequently used assistants are Grammarly with 41% and GitHub Copilot with 37% (details in Appendix B).The most common tasks are coding (software development) with 92% and writing assistance with 72% of responses (multiple answers were possible).None of the participants find it difficult to use LLMs; on the contrary, 70% of respondents find it easy or very easy to use them.30% of the respondents indicated some challenges in using LLMs.</p>
<p>In terms of ethics, 78% of participants said that credit should be given to LLMs in science publications, but 61% say that they rarely do so at the moment.We discuss some possible reasons in Sect.5.3.2.</p>
<p>Overall, 48% of participants were satisfied or very satisfied with the current capabilities of LLMs, 41% were neutral about them, and 11% were not or not at all satisfied.Satisfaction is the highest for software development: 77% are satisfied or very satisfied with using LLMs in that context.Fewer participants are enthusiastic about their writing and problem-solving abilities: 50% are satisfied or very satisfied, and 25% are neutral.More than half of the respondents (59%) indicate a desire to have access to the paid version of such tools, with ChatGPT and GitHub Copilot being the top choices of tools where paid access is desired (note that GitHub Copilot does not have general free access version but students and teachers can get educational access corresponding to an individual tier free of charge).Most participants (83%) believe that LLMs will become an integral part of science, but only 42% think they will be a good influence overall.</p>
<p>The survey also asks the participants about their experience and satisfaction in specific use cases we detail below.We discuss those results in the relevant contexts.</p>
<p>USE CASES OF LLMS AS A TOOL FOR RESEARCH</p>
<p>The main focus of our experiments was to explore the use cases of LLMs in the research workflow of astronomers.Even though LLMs have multiple use cases in non-research tasks, this section focuses on tasks related to research activities.Specifically, we tested various models' abilities to generate and improve text, summarize text, write scientific code, and handle data analysis The specific experiments, their results, and conclusions based on these experiences are detailed below.</p>
<p>Academic Writing</p>
<p>Writing is essential for all academics and critical for our professional success.From professional communications to cover letters and research plans to manuscripts and funding proposals, written text is indispensable for connecting with colleagues, conveying our findings, obtaining jobs, and securing resources such as telescope/computational time and money.LLMs can assist researchers in various aspects of the writing process, such as generating text, improving readability, and providing suggestions for structure and content.LLM tools can also assist with language translation, grammar checking, and citation formatting.</p>
<p>From our internal survey (see Appendix B), we found that writing assistance is the second most common use case (72% of responses) for LLMs (after coding assistance, 92%).59% of the survey participants have used LLMs specifically for improving academic texts, but most estimate that the overall contribution of LLMs to their writing is small: only 23% say that LLMs have contributed more than 20% to their texts.However, there is a 0% to 99% spread in how users feel satisfied by the answers of LLMs.Such a result suggests a varying degree of accuracy in the answers, possibly reflecting the range of expectations, use cases, and prompting skills among the participants.</p>
<p>Based on our tests, most current LLMs offer valuable assistance in understanding scientific concepts, software, and instruments.They can summarize internet searches on specific topics into bullet points, covering introductions, current research, challenges, and prospects.Different services vary in their abilities to cite peer-reviewed articles.LLM-enabled search engines do better.There is also a variation in the style of the response.For example, Gemini or ChatGPT rely on a mix of academic and popular science sources by default.Other platforms (e.g., You.com) allow users to specify a style explicitly.These tools often provide more readable and comprehensive information than Wikipedia articles, which are commonly the first resource for beginners.LLMs can also assist in drafting academic paper templates.This is particularly reliable for papers that employ well-established data sources and analysis techniques.This feature benefits students transitioning to academic writing, non-native English speakers, or anyone facing writer's block.However, our study participants found that the generated templates still require substantial elaboration in style and content before submission to a journal.Hence, the work of the student/researcher is far from being eliminated.</p>
<p>It remains important to be the "human in the loop," as there are always instances where responses from these tools require corrections.For example, when prompting GPT-3.5 with "In what situations can we use MHD?In what regimes does it break down?", it stated that "magneto-hydrodynamics assumes collisionless plasma" when the exact opposite is true.When informed about their errors, both chatbots immediately provided the correct output.Gemini also offers an option to cross-check responses with a Google search3 .When asking varied tools, "Who leads the Gaia DPAC?" Gemini answers Francois Mignard wrongly; GPT4 does not find answers and refers to the official Gaia/ESA pages; You.com (genius mode) replies correctly not only the name of the chair (and since when) but also the deputy chair correctly and cites sources.</p>
<p>It is difficult for a non-expert to spot these mistakes, as they are very technical.Therefore, if a certain piece of information is key to the user's writing, it is essential to fact-check through a conventional search.</p>
<p>The clarity of our messages is critical in academic writing.Clear communication helps avoid misunderstandings and misinterpretations.LLMs can detect potentially unclear or ambiguous sentences and suggest rephrasing or simplifying these sentences to improve clarity.These models can also assess the overall structure and flow of the message and identify sections that needorganization or additional context.By ensuring coherence and cohesion, your message becomes more readable and impactful.LLMs can suggest alternative words or phrases to enrich your vocabulary.Overall, they help you express your ideas more precisely and effectively.</p>
<p>LLMs can also analyze a message's content to identify inappropriate or inconsistent tones and readability scores.They can flag sentences that may be too formal or informal.LLMs can also quickly draft a message that only needs some edits, filter incorrect tones, or highlight potentially unclear sentences.Interpersonal communications are complicated, especially in written forms; LLMs can aid by guaranteeing that our written messages convey the intended meaning, appropriate tone, and level of emotions.Such assistance lets us focus on the content and adjust the form/tone to ensure the message comes across clearly.Enhanced language adds depth and professionalism to your written communication.All these are critical in efficient written communications with colleagues.Such assistance can be beneficial for junior researchers who have had less exposure to formal academic communications.A drawback is that writing can become standardized or mechanical.</p>
<p>It is essential to acknowledge that English is not the first language of many -if not most -professional astronomers who may face bias and discrimination because of their level of expression in this language.LLMs can be a great equalizer in written documents and allow users to focus on ideas rather than grammar.Some of the tested services specialize in this approach.Both Grammarly and DeepL Pro are excellent at recommending improving grammar, word choice, and phrasing while accounting for contexts.Although at this time, they are less consistent, General models can also be prompted to act as a copy editor and highlight grammatical errors with suggestions for replacement.Some users found that wholesale re-writing of the text by an LLM can change the meaning conveyed.It is thus not advisable to do this, and it is paramount that the writer carefully reads the final text to ensure the ideas are true to intended.Translation services such as DeepL (and Google Translate) can also be helpful for researchers who are more comfortable expressing ideas in their first language.Overall, in this role, LLMs can increase accessibility and ensure that scientific contributions are judged based on merit rather than linguistic proficiency.</p>
<p>Finally, LLMs do not complain when we outsource tedious or repetitive tasks.Such tasks may include formatting citations from one LaTeX template to another, re-formatting a table from HTML to LaTeX or text from Markdown to LateX (e.g., Appendix B), and vice-versa.Reformatting text to fit specific guidelines and to conform to a different template are also trivial tasks that are executed with minimal errors.When significant changes in the text occur (such as decreasing the length of the text to fit a shorter page limit), it is critical to review the final text to ensure the ideas conveyed are the same.</p>
<p>Acknowledging the potential drawback of relying heavily on LLMs for written materials is important.LLMs may inadvertently introduce a standardized or mechanical tone into our writing.Over-reliance on LLMs can diminish your unique writing style and potentially reduce creativity and personalization of communications (Doshi &amp; Hauser 2024;Chen &amp; Chan 2023).Some studies have found that using LLMs reduces overall lexical and content diversity (Padmakumar &amp; He 2024).Maintaining a balance between utilizing LLMs and preserving your writing flair is crucial.Experiments have shown that humans rely too heavily and gladly on outsourcing decision-making to machines (e.g., Weapons of Math Destruction, O'Neil 2016).One approach is to sketch your ideas first and work with the LLM to develop them, rather than outsourcing creativity like "Write me an exoplanet proposal for JWST."It is also important to keep in mind that LLMs carry over the biases of their training datasets.For a further discussion of the ethical issues around LLMs, see Section 5.3.</p>
<p>Reading and Summarizing Papers</p>
<p>Starting a new project and diving into the literature can be daunting.Traditionally, scientists employ specific patterns of skimming research papers.These approaches vary from person to person due to individual preferences.Different approaches are also employed based on the goal of the search: e.g., looking for a specific technique, a result, a particular value, or a review of a topic.</p>
<p>Acting as information retrieval systems, LLMs have the technical capacity to summarize texts and connect scientific papers.This feature can transform how researchers navigate the ever-expanding sea of academic articles and resources (e.g., Iyer et al. 2024a).LLMs can offer a solution by rapidly contextualizing papers and summarising their main ideas concerning a specific question (for instance, talk2arxiv.org).They can extract and list the relevant information and even potentially compare results between publications.In one of the tests for this study, we used the preview model of Gemini 1.5 Pro with a 1M tokens context window in Google AI Studio to examine a PDF file of a paper.Gemini (and other LLMs) allows the functionality to upload PDFs, cutting down the workload of manually putting the paper text.We uploaded an 8-page review article with tables and figures.The Gemini model could extract the information accurately and explain the figures in the paper, albeit not very technically.The chatbot also identified which sections of the paper focused on different concepts by referencing page numbers, tables, and figures (see Fig. C1).</p>
<p>LLMs also have the potential to help break through disciplinary silos, facilitating the discovery of relevant research across diverse academic disciplines or between academic and industry work.One example of the latter is shown in Figures C2 and C3, where we interrogate a US decadal white paper by Smith et al. (2019).This test was done with ChatGPT-4, where PDFs can be uploaded and parsed.The chatbot could correctly summarize the paper and provide a list of major challenges listed by the authors (response not shown here).But it was also able to draw on general knowledge about cloud computing in other scientific disciplines and make a list of recommendations for advancing cloud adoption (Fig. C2) and identify valid challenges not mentioned in the paper (Fig. C3).This approach can be useful to au-thors of papers who can identify gaps in analysis or draw on expertise beyond that of a team.A more advanced take on this approach is to give the LLM a certain persona and ask for feedback based on that persona.</p>
<p>There were occasional glitches, both scientific and technical.Regarding science, LLMs occasionally answered questions incorrectly, especially about niche astronomy topics and physics concepts.As mentioned later, LLMs do not know anything, and hallucinations are possible even for the advanced models.Hence, we repeat that it is paramount to check the details of the responses.A technical issue is that the uploaded PDF documents were not always processed in full due to limitations in the context window or issues with document parsing.In particular, they often failed to process tables.The handling of figures depends on the platform and the graphics format (vector vs. raster).Still, the LLMs can usually answer questions regarding the figures based on the context of the caption and text alone.A more general concern is that LLMs may lack a consistent ability to grasp the complexities, uncertainties, and subtle nuances that a human scientist would recognize when reading literature.Dependence solely on LLMs for crafting scientific summaries risks producing overly simplistic narratives, neglecting essential arguments and values, and potentially misinterpreting research findings.</p>
<p>While none of the platforms we reviewed are tuned to the specific domain of astronomy and astrophysics, such systems are in development.A model fine-tuned to astronomy, Astro-LLAMA, was published in 2023 (Dung Nguyen et al. 2023), and a prototype of a chatbot platform, pathfinder (Iyer et al. 2024b), was published during the preparation of this manuscript.Such LLMs can answer much more specific questions with greater accuracy and potentially make connections between new and archival publications while adequately citing the sources.Furthermore, they can lead to an exchange of methods and ideas between fields, which is currently limited due to technical language barriers and jargon, a phenomenon termed "undiscovered public knowledge" by Swanson (1986).Currently, both Astro-LLAMA and (Iyer et al. 2024b) are only trained on abstract and thus lack knowledge of the more detailed analysis and scientific results, which are only contained in the full-text publications.However, using the full corpus of astronomical literature may raise legal and ethical concerns (see Sect. 5.3).Software libraries such as langchain enable individual researchers to customize models for use on a limited corpus.Exploring such use cases is beyond the scope of this current paper, but drop-in tools such as Incarmind4 already exist, and we expect such use to become more commonplace soon.</p>
<p>Writing Scientific Code</p>
<p>One of the most apparent and exploited use cases for LLMs is their ability to generate and analyze computer code.Programming is a critical skill for research astronomers, as shown by Momcheva &amp; Tollerud (2015), yet many researchers do not receive training in proper software development practices.Even for those who received training, the wide-ranging tasks involved in scientific programming are constantly changing and frequently very niche.Although we do not think about it this way, coding is a creative task and thus very well-suited to LLMs.Both general-purpose LLMs (Figure C4) and specialized ones (such as GitHub Copilot, Figure C5) have proven quite adept at programming and have thus become invaluable tools to assist in the workflow of a scientific programmer. 5The tasks they can help with include a range of actions, from explaining code, debugging, or offering simple snippets inside an IDE (Integrated Development Environment) to creating documentation and tests to using chat-based LLMs to create programmatic implementations easily.All of these can drastically improve the speed at which one creates fast, high-quality, scientific code.</p>
<p>In our internal survey, coding assistance is the most common use case for LLMs, with 92% of respondents.What users find most useful about coding with LLMs is that it helps them complete tasks faster (83% of responses) and write more code in less time (63%).They can also identify and fix bugs easier (58%), improve their coding knowledge (54%), and learn new coding concepts (54%).Participants found that LLMs rarely suggest very complex code: 71% rarely or never received advanced suggestions.Overall, the code contributions are relatively minor, with 42% of the respondents saying that LLMs have contributed 1-20% of their code and only 8% saying that LLMs have contributed more than 80% to their code.Users are more satisfied with the accuracy of the coding prompts than they were with their writing prompts: 72% of respondents found that more than 60% of their coding prompts were successful.A handful of respondents have tried translating code from one programming language (Julia, IDL, Fortran) to another (Python) and have found varying degrees of accuracy and satisfaction.</p>
<p>Arguably, one of the best tools for auto-completion-assisted coding is Github Copilot, as its context window is large enough to include multiple files at once.This feature is helpful with projects or packages where the quality of the suggestions improves dramatically when the LLM is aware of all the code in the project.In addition, GitHub Copilot includes a chat window in some IDEs, such as Visual Studio Code (VS Code, Figure C5), making it even more convenient.Codeium represents a free alternative to GitHub Copilot that works similarly but provides a smaller context window and no chat capability (in the free individual version).Both companies support extensions for many editors and platforms.</p>
<p>Chatbot platforms, such as ChatGPT3.5/4,Gemini, and Claude, have the advantage that even less experienced programmers can directly formulate their request in plain language ( English or any other language), and the LLM will provide a snippet of code in the requested language.The code can be copy-pasted from and to any terminal or editor.Team participants who tested this functionality frequently queried solutions to syntax errors but found the provided code helpful nonetheless.LLMs can also assist programmers by doing arduous or repetitive tasks (e.g., regex string, parsing, code translations from one language to another, etc.) for them, which are often hard to understand for humans, but the LLMs excel in.</p>
<p>To assess the behavior of chatbots on more complex programming tasks, we tested ChatGPT-3.5 and ChatGPT-4 on a more advanced coding example: "Please create a simple beta-Variational Autoencoder model implemented using PyTorch.The model inputs are a set of images and their sky coordinates.The output should be a bijective mapping by coordinates w.r.t the distribution of images in the latent space."</p>
<p>In addition to being complex (several interconnected functions are required), this is not a very common piece of code.The outputs from the LLMs differed: ChatGPT-4 described the model construction in detail, including data preprocessing, network architecture, loss function, and training loop, while ChatGPT-3.5provided more detailed code and less conceptual explanation.While neither chatbot provided a perfect working example, both gave the user an adequate implementation of the base model as a starting point.</p>
<p>As with other use cases, users can receive significantly different responses depending on the wording of the prompt (also see Sect.5.4).Some participants found it useful to include the user expertise level in the prompt -for example, requesting code for a beginner programmer frequently resulted in a response with more code comments and a more detailed explanation of the code.In a more complex code problem, it was useful to start with a simple question and then build upon it with additional questions or steps.Regardless of writing a good prompt (see Sect. 5.4), the accuracy of a generated code depends on the actual task and the libraries required to accomplish it -for rare and particular tasks requiring singular niche libraries there are often not enough training materials to provide reliable responses (e.g., How can I use Pyphot to get the photometry of a few spectra through the Gaia passbands?).The code presented in these chats is often flawed to a certain degree, with syntax/API errors being quite common.Users can correct the error(s) themselves, depending on their expertise level, or provide repeated input to incite the LLM to fix its errors (e.g., copy-paste the error and ask the LLM to troubleshoot).Still, after too many iterations, the context window can saturate (especially using free tools), and the LLM may trap itself in a repeating cycle.</p>
<p>Overall, LLMs can only (at this time) create straightforward scripts directly and without error but often provide a perfect starting point for more complex tasks for any programmer to improve upon.They can give ideas (such as suggesting an approach or even specific packages that could be helpful) that might be outside the programmer's direct scope/knowledge and thus lead to either a learning effect and/or improved code.</p>
<p>It is essential to mention that the free versions of these tools, like Codeium, ChatGPT3.5, and others, are generally sufficient to assist with relatively simple scientific coding.Paid services (e.g., GitHub Copilot) provide more specific answers as they handle a more extensive project context and can understand more complex tasks.In the case of the chat LLMs, the features vary with the versions, like web-based searches, thus more up-to-date information, which is appreciated to avoid outdated and possibly deprecated code snippets.</p>
<p>Finally, regardless of which model, it is imperative to be knowledgeable enough to understand if the LLM is on the wrong path or producing direct errors, referencing outdated/deprecated packages, having logical flaws, and showing convoluted solutions.Lack of understanding is risky and incredibly impactful if inexperienced programmers are using the output of LLMs to improve their coding.</p>
<p>LLMs as Data Analysis Assistants</p>
<p>Beyond software development, the proliferation of AI tools on platforms like Deepnote, Jupyter, and VScode is revolutionizing how scientists approach data analysis as a whole.AI tools with large context windows can access the entirety of a project, gaining a comprehensive understanding of the code, data warehouses, and metadata.This extensive context awareness enables them to provide precise and customized assistance tailored to the specific work undertaken.</p>
<p>The benefits of using AI tools for data analysis are multifaceted.First, they significantly reduce the time required for data exploration and analysis.For example, they allow the scientist to focus on the question in natural language, such as "Filter the data only to include entries with proper motions and parallaxes" and let the AI create the code, rather than have to parse through the (frequently complex) database schema documentation.Second, by automating routine tasks and providing real-time guidance, AI tools could enable scientists to focus on more complex and creative aspects of their work.</p>
<p>Crucially, these AI tools eliminate the need to start from scratch.Scientists can prompt the tool with a specific task or query, such as "Analyze the Gaia DR3 source catalog and find the 10 most rapid objects moving in the sky and download their spectra using Python language and ADQL."The tool then leverages its understanding of the project to define the relevant query for online services, streamlining the data analysis process.Figure C6 shows Perplexity's response to this query, which details the reasoning and explains the proposed Python code.It also provides the relevant part for downloading the spectra, which the popular Python libraries do not cover.Adding radial velocities to the movement calculations is subtle because it would require some distance estimates.Perplexity provides this limitation if prompted: Without specific distance information, we can't directly convert radial velocities to mas/yr.However, as we mentioned for other tasks, the answer varies between LLMs, and it may vary with time for a single LLM.Regardless, they provide a robust starting point for the scientist to refine.</p>
<p>AI tools can lower the barrier to analyzing complex datasets and potentially fill the gaps in programming knowledge.However, AI tools are not a replacement for human expertise.Instead, they serve as powerful assistants that augment and enhance the capabilities of scientists.Most code outputs or data analysis results require adjustments, enhancements, or refinements.Deepnote.aipublished a 12% acceptance rate 6 of AI-generated outputs on their platform.A small change in the AI prompt could often lead to different outcomes (see 5.4).</p>
<p>Image Generation for Outreach</p>
<p>When communicating astrophysical research to the general public, it is often important to supplement scientific material with visually impressive, easy-to-understand images and videos to capture the audience's attention and clarify key points.Major differences emerge when comparing plots for a scientific audience to outreach images used in popular presentations.First, "mathematical-looking" plots, labels, and similar elements should be reduced to a minimum.This avoids confusing non-experts with unfamiliar concepts like logarithmic scaling and also helps retain attention, as many people are "turned off" by overly technical or mathematical details.Second, it is sometimes necessary to sacrifice accuracy for simplicity.In line with the first point, focusing on a single, basic idea is often more effective than presenting nuances meant for a more specialized audience.Third, it can be beneficial to prioritize visual coherence or "beauty" over precision.The goal is to capture attention, and visually striking schematics or animations will aid in doing so.As such, a "good outreach image" fulfills the following criteria: (1) Easy to parse and understand the key point, even without scientific background; (2) Visually impressive and coherent; (3) Scientifically sound, even if minor or technical details can be omitted.</p>
<p>However, most astronomers have little to no experience in areas such as computer animation or digital art.They are rarely tasked with making the images used with, for example, press releases.Regardless, these images and animations can be helpful tools when 6 Impact of Deepnote Copilot blog post at deepnote.comcommunicating scientific findings, particularly for a lay audience, for the above reasons.</p>
<p>Generative AI, and more specifically, image generation models (such as DALL-E), can assist in scenarios with limited access to artists or animators.Even in these scenarios, it is important to avoid using images and videos at the risk of communicating incorrect science.Thus, it is imperative to assess the generated images according to the criteria outlined above.</p>
<p>In this study, we attempted to generate a handful of outreach images for some astrophysical concepts using BingAI's Creator and Chat-GPT, both of which use DALL-E as the underlying image generator.The main difference in these models is their specific conversion or translation of the user prompts to DALL-E prompts fed to the image generator, which occurs "under the hood."For each experiment, we iterated several times to optimize the outcome, updating the user prompt to suit our needs.For brevity, we only present one such case here, generated using ChatGPT to create a visually appealing picture of a stellar cluster with tidal tails.This experience poses an interesting example, as there are few images of this astronomical concept aimed at a lay audience.Further, it combines multiple astrophysical aspects, thus creating a challenging prompt for the LLM.Table C1 illustrates the generation process through multiple iterations.</p>
<p>The LLM struggles to encode the astronomical jargon into a scientifically sound image.Inherently, the challenge here is that "Cluster" and "tidal tails" have specific meanings in an astrophysical context but also connect with more popular contexts outside astronomy.It is perhaps not particularly surprising that without reference image prompting, the generated images consistently show "clusters" or blobs of stars within a larger structure.Further, the use of "tides" likely evoked a relation to oceanic tides, and thus wave-like structures were generated.An attempt to first get the "cluster" portion right, while leaving out the "tides", resulted, again, in blobs of stars, now without the wave-like structures.After showing an image of a globular cluster with tidal tails to the LLM, it managed to move away from repeating the same structure but produced something more akin to a spiral galaxy.Asking it to create something more aligned with the reference image again prompted the blob structure.</p>
<p>This example demonstrates that context is essential when parsing prompts for image generation in LLMs.One must word prompts carefully and precisely, ideally omitting jargon and especially polysemes, i.e., expressions that have more widespread sense.Reference images can be helpful but do not guarantee a desirable outcome.Great care has to be taken when generating images for outreach with tools like ChatGPT to avoid confusing a lay audience with potentially erroneous images.</p>
<p>At this present stage, our experience shows that AI tools could help with simple illustrations but not with complex concepts.For the next few years, artists or animators will continue to be the path of choice for creating effective illustrations.</p>
<p>DISCUSSION</p>
<p>Following the presentation of our findings, this section summarizes the main benefits and limitations, discusses legal and ethical concerns, and discusses general tips for making the most of using LLMs for research.</p>
<p>Benefits of LLMs in Research</p>
<p>LLMs have many and varied applications to tasks carried out by researchers, as demonstrated in the use cases discussed in Section 4.</p>
<p>LLMs can assist in communication tasks: drafting emails, letters, papers, and proposals.They can also help in editing and proofreading academic manuscripts, formatting citations and bibliographies, and generating content for teaching.They can summarize individual or multiple documents, identify gaps in analysis, and create summaries of existing research and literature.They can also identify trends in the field, making them especially useful for literature review.LLMs can generate and analyze code, assist in debugging and refactoring code, and help in code documentation and explanation.LLMs can process and analyze large datasets, extracting key insights and information and aiding decision-making processes.</p>
<p>Integrating Large Language Models in research workflows offers numerous advantages, from enhancing communication and facilitating literature reviews to streamlining data analysis and supporting software development.LLMs have the capacity to significantly reduce the time required for data exploration and analysis.For example, they allow the scientists to focus on the question in natural language rather than parse through (frequently complex) sources of information.By automating routine tasks and providing real-time guidance, AI tools could enable scientists to focus on more complex and creative aspects of their work.As researchers increasingly adopt these tools, the potential for significant productivity gains becomes evident.In our survey, 83% of respondents think that LLMs will become an integral part of science, which likely translates the versatility of AI to be applied across various tasks in academia.</p>
<p>Research is inherently a process of complex knowledge-based tasks, which rely on a pool of experts with significantly varying abilities and knowledge.People who would likely get the biggest boost out of this technology are those with the lowest initial ability in a given task, especially students.Still, even those with the highest level of skill are likely to benefit in terms of productivity and efficiency (pages 156-157, Mollick 2024).At the same time, subject expertise will become not less but more important.Expertise requires knowledge of facts, extensive practice, critical thinking, problem-solving, and the ability to think through problems and troubleshoot.Being the responsible "human is the loop" requires us to be engaged in this collaboration with LLMs but also allows us to learn from them; it fosters a sense of responsibility and accountability (page 54, Mollick 2024).Economists predict that AI is unlikely in the next several years to cause dramatic changes in most jobs (Ilzetzki &amp; Jain 2023), and we believe this to be true about the work of researchers in astronomy as well.As demonstrated by our survey (Sect.B), most participants reported relatively minor contributions to their code.Instead, we will likely experience many small workflow changes that will improve efficiency, and we will delegate some tedious and/or repetitive tasks to LLMs and use them as tools to enhance our creativity.</p>
<p>Limitations and Responsible Use</p>
<p>While LLMs offer remarkable potential, we must understand their limitations and exercise caution in their application.Our survey raised concerns about these limitations, with 25% of respondents expecting that LLMs will have a negative influence on science overall.</p>
<p>Hallucinations are one of the most frequently encountered drawbacks of LLMs (Fecher et al. 2023;Birhane et al. 2023).Hallucinations are instances where a language model produces false or misleading information that seems plausible but is not grounded in reality.This is an issue we caution against in each use case above.The most commonly known examples are of LLMs making up citations in documents or research papers.Newer and larger LLMs hallucinate significantly less than older versions.In practice, the reduction of hallucination is primarily produced by mitigation mechanisms (e.g., Retrieval-Augmented Generation, RAG, post-processing techniques, user feedback), not by changing the fundamental algorithms.For example, Chen &amp; Chen (2023) found that while ChatGPT-3.5 made up fully 98% of citations, ChatGPT-4 only made up 20% of them.Models connected to the internet are also more likely to link to real resources on the internet but are not entirely devoid of this issue.</p>
<p>It is unclear if hallucinations are a solvable problem.LLMs are ultimately elaborate auto-complete engines based on statistical patterns with no concept of facts, truth, originality, or meaning.They do not actually know anything: if you ask an LLM for a citation, a quote, or a number, it will generate the answer based on the statistical connections in the model rather than retrieve it from memory.For this reason, use cases where we ask LLMs for specific numbers and figures are inappropriate use of this technology and not playing to its strengths -we are trying to expose rather than explore the technology (Mollick 2024).</p>
<p>But beyond the obvious errors, such as numbers and citations, there is also a vast space of more subtle inaccuracies, which we need to be cautious about.For instance, using LLMs to edit or paraphrase text can change the ideas and meaning of a text; in summarizing papers, LLMs can over-simplify or misinterpret research findings; in code, LLMs can change the flow of an algorithm or the indentation of code.These are all due to the LLMs' fundamental lack of contextual understanding and reasoning capabilities (Birhane et al. 2023).While these models can identify patterns and generate plausible text, they may struggle to grasp the nuances and complexities of scientific concepts.Astronomers should view LLMs as tools to augment their research efforts rather than replacements for human expertise and critical thinking; they must critically assess the outputs of LLMs and verify their accuracy against established literature and empirical evidence.</p>
<p>We purposefully do not explore in detail the use cases of peer review and assessment of papers, proposals, job applications, etc.Even though these are some of the most time-intensive tasks and obvious candidates for LLM automation (e.g.Jerabkova et al. 2024), we believe these are tasks that fundamentally should not be outsourced to machines.Using LLMs in tasks that relate to the allocation of resources and assessment of performance raises several ethical and legal issues (see Section 5.3), and it also endangers our trust in the fairness of the process (Birhane et al. 2023).While we can hold people responsible for misinterpreting a proposal or an article, it is unclear who should be held responsible if the machine makes an error.</p>
<p>Likely, the most fundamental concern that underlies the debate of LLMs in research writing is whose ideas are those.We have so far existed in a world where (in academic writing) written materials capture our own ideas (and those of our collaborators) and reflect our thoughts.And in this world, using LLMs for writing "feels" different than, for example, coding.While we consider software as a means to an end (data reduction, analysis, plots), the interpretation used to come from the human scientist.Writing with an LLM may feel intellectually dishonest because it is unclear that the ideas in the final text are those of the authors.Research has shown that the process of writing helps us think (e.g., Quitadamo &amp; Kurtz 2007, and references therein), and it is not clear to what extent this holds for the LLM-enhanced writer.</p>
<p>Ethical and Legal Concerns</p>
<p>Many ethical and legal concerns have been raised about using LLMs, both broadly within society and within astronomy specifically.In this section, we summarize the main concerns.We must recall that we are not experts in legal propriety and ethical research.</p>
<p>General Concerns</p>
<p>One major issue raised against tech companies is using copyrighted materials to train LLMs in the spirit of "better ask for forgiveness than for permission."LLMs are unlikely to reproduce text wholesale since they do not save the text itself but only the statistical weights across many texts.Only in cases where the text is very common (e.g., the Gettysburg address, Alice in Wonderland, Chang et al. 2023) do LLMs auto-complete sentences correctly.Copyright laws in Europe and the USA are still unclear on whether this is an infringement, while in contrast, the Japanese law explicitly includes provisions accommodating AI training (Shinnosuke Fukuoka et al. 2023).While not outright illegal, the practice is considered unethical by many.Many scientists may believe that using the product makes them complicit in copyright infringement and choose to avoid using these services.</p>
<p>A specific example of this issue in the context of astronomy is the advent of astro-specific LLMs such as pathfinder (Iyer et al. 2024b) and Astro-LLAMA (Dung Nguyen et al. 2023).While these models are currently trained only on the abstracts of papers, Iyer et al. (2024b) mention the possibility of training on the full text of papers, and Dung Nguyen et al. (2023) explicitly say that they are in the process of ingesting the full L A T E Xversions of papers.But both arXiv publications and papers linked to from the Astronomical Data System (ADS) have licenses, which in many cases prohibit reuse and reproduction without the explicit permission of the authors.It is impractical to obtain permission from every author, making models less powerful to only use manuscripts with Creative Commons licenses.However, ignoring the licenses on astronomy papers is just as illegal and unethical as ignoring the licenses on New York Times articles.</p>
<p>It is important also to consider the ownership of intellectual property of materials given to and produced by LLMs for individual users.The data we provide to an LLM, such as the text of a proposal or a paper, may not be immediately incorporated into the model's training corpus but could eventually become part of it.Although it is highly unlikely that the ideas from one user will be presented verbatim to another, they are not discarded.This may be especially critical for users with sensitive or proprietary information, such as writing code for hardware under export controls or the International Traffic in Arms Regulations (ITAR).In some cases, platforms offer paid access with a promise that personal information will not be used in training, but ultimately, users have very little control over their information.It is also unclear if text generated by LLMs is protected by copyright and who owns the copyright.In some cases (e.g., OpenAI), the service agreement grants the copyright to the user, but the issue is still open from a legal standpoint.</p>
<p>Another ethical concern about LLMs is that AI companies hire low-paid English-speaking workers in developing countries to do the reinforcement learning through human feedback, a process that trains models to avoid unethical or dangerous responses, subjecting these employees to unethical and dangerous responses.Many researchers may not be aware of this part of the training process and may object to their use based on refusing to be a part of an exploitative labor system (however, this system is not unique to LLMs, and similar systems underlie the content moderation of many social networks).</p>
<p>The source of materials for training LLMs -largely the internetis also not inherently unbiased.Many studies have shown that there are gender differences in both participation and representation on the internet (e.g., Wikipedia, StackOverflow, GitHub; Noei &amp; Lyons 2021; May et al. 2019;Stephens 2013;Wagner et al. 2016;Qiu et al. 2023).Additional cultural biases are introduced (or amplified) by the fact that data are dominated, if not limited to, English-language documents.An exploration of image generation by Stable Diffusion by Nicoletti &amp; Bass (2023) revealed strong gender and race stereotypes, depicting higher-paying jobs as predominantly male and white.The biases were stronger than in real life: while 39% of doctors in the US are women, only 7% of generated images of doctors were female.The emerging field of "machine psychology" is only just starting to understand the behavior and reasoning abilities of LLMs.Still, early work has shown that they exhibit at least some of the same cognitive biases as humans (Talboy &amp; Fuller 2023).Models can bring their learned biases to recommendation letters, job applications, performance reviews, etc., in the same way as humans.</p>
<p>A final general concern is the environmental impact of LLMs and image generation models both during their training and subsequent use.For example, training a single large-scale model can emit as much CO 2 as several cars over their entire lifespans (e.g., Strubell et al. 2019).The frequent updates and fine-tuning of models, as well as the ongoing use of these models, also contribute to their environmental impact due to the continuous demand for powerful servers and cooling systems in data centers (e.g., Kaack et al. 2022).Image generation models are especially power-intensive -one generated image can use as much power as fully charging a smartphone.In contrast, one text generation is equivalent to 0.0016% of a smartphone charge (Piper 2023).As these technologies become more prevalent, we need to account for their contribution to the CO 2 footprint of research activities.One major issue in this discussion is whether using LLMs in research is injurious to the scientific process.Philosophers of science are only now starting to consider the implications of LLMs for science.One approach is to remember that science is fundamentally a human enterprise determined by the historical, social, and cultural context in which it happens, and our questions, methods, and analyses are influenced by our backgrounds (Birhane et al. 2023).LLMs are just tools that have none of these contexts and motivations.Human scientists are fully responsible for the work they produce.But it may be difficult to determine whose ideas are those presented in the final work -do they belong to the scientist or the training set (Dwivedi et al. 2023, Contribution 25)? Research has shown that the process of writing helps us think (e.g., Quitadamo &amp; Kurtz 2007, and references therein), and it is not clear to what extent this holds when using LLMs to assist human writers.The key point is that the ideas in the written text and materials should reflect the authors' thoughts, which may be challenging when collaborating with LLMs (Vishniac 2023).Possibly, this represents the crux of why we think using LLMs for writing "feels" different than for coding.While we consider software a means to an end (data reduction, analysis, plots), the interpretation should come from humans.Writing with an LLM may feel intellec-tually dishonest because it is unclear that the ideas in the final text are those of the authors.Astronomers are certainly not the only ones facing these new issues.Scientists from all disciplines and philosophers of science are only now starting to struggle with such questions, and there is an ongoing open debate across academia on the ethical use of LLMs.We present an abbreviated slice of this discussion here, and we urge members of the astronomical community to be engaged and informed participants in the debate.</p>
<p>Research-specific Concerns</p>
<p>First and foremost, intellectual ownership represents a major issue for the integrity of the publication process.Currently, several scientific journals have published policies regarding the use of LLMs in writing research manuscripts.Most relevantly for astronomers, in March of 2023, the editor-in-chief for the AAS journals provided a clear guideline for the use of LLMs in scientific manuscripts submitted to the AAS (Vishniac 2023): (1) the authors of the manuscript are solely responsible for its content, and (2) LLMs should be cited as software.The Monthly Notices of the Royal Astronomical Society (MNRAS)7 has issued a similar statement as has Nature8 , additionally requesting that peer reviewers avoid the use of LLMs due to privacy concerns.Other major journals have adopted similar stances, and only rarely are there policies that completely ban LLM use (e.g., ICML 9 ).</p>
<p>Yet, the recommendation across all journals that using LLMs should be acknowledged does not seem to be abided by.As of August 2024, there are only 135 mentions of "ChatGPT" in the full text of articles of the ADS astronomy collection since the start of 2023, while astropy is mentioned 4,957 times.Further, as shown in our survey (see Section B), even though 78% of participants agree that it is necessary to credit LLMs in papers, only 28% say they do so every time they use them.A more quantitative analysis has been done by Astarita et al. (2024), who show a statistically significant increase of words commonly used by LLMs in the astronomy literature since 2022 (see also Geng &amp; Trotta 2024).There is a widespread lack of compliance with journal guidelines, and using LLMs likely extends beyond journal articles.This lack of compliance is likely due to what Mann et al. (2023) put in the context of a credit-blame asymmetry: generative AI diminishes the apparent effort of the authors (i.e., gives them less credit for their work).Still, it increases their responsibility because LLMs are error-prone.Within this context, authors are not incentivized to report using LLMs because doing so will result in their work being judged more harshly.In this regard, LLMs are different than other software tools.As a result, the recommendation to cite LLMs as software is at odds with the psychology around LLM usage.</p>
<p>Of course, it is also impossible to enforce the reporting recommendations because it is practically impossible to tell if a specific text is fully or in part written by an LLM (Sadasivan et al. 2024).Prompting strategies and small changes to the text can evade existing detection methods.Worse than that, LLM detectors have high false positive rates for non-native speakers and writers with limited vocabulary (Liang et al. 2023).Therefore, providing guidelines on the appropriate and inappropriate use of LLMs is especially critical.</p>
<p>Second, there are real new obstacles that journal publishing is likely facing.Irresponsible authors may submit text with fake citations, which puts the burden on the reviewers or copy editors to rectify the issue later.Fake citations may also be seen as "the canary in the coal mine" -a sign that there may also be factual errors in the text as the authors have used LLMs and not bothered to check the text they produced.Automated citation checking at submission has been implemented by some journals (e.g., JOSS) and can/should be done before review to flag such issues.While there could be automated mechanisms to check citations, generated reviews are challenging to detect, as mentioned above.A more insidious problem is that authors can generate and submit multiple versions of the same paper to the same or different journals to boost their publication records.This can become a major issue, wasting the time of editors and reviewers while clogging the system with "spam" submissions.However, the ethics standards of most journals already cover those issues.For example, the AAS Journals Professional and Ethical Standards state10 that authors submitting their manuscript certify that it "has not been previously published elsewhere and is not being considered for publication elsewhere in any form."A system that enforces this rule (and creates consequences for its violation) may be able to safeguard journals from such spam publications.The other side of the coin is the reviewing process being hampered by generated reports.Within the context of intellectual property rights discussed above, this is a more clear-cut issue: the authors hold the copyright of their work, and therefore, third parties (i.e., reviewers) should not be allowed to share these texts with LLMs where they could potentially become part of future training sets.</p>
<p>Finally, there are rising concerns about using LLMs to accelerate tedious tasks critical to (astronomy) research.One of the most labor-intensive processes in the astronomical community is telescope proposal review.Therefore, it is unsurprising that there is a lot of interest in the performance of LLMs for this task.Our team was curious to test this performance.For this work, we used ChatGPT-4 to assess five ESO proposals from previous cycles.We provided the scientific rationale of the proposals as well as the review guidelines and the grading scale ranging from 1 (outstanding, ground-breaking) to 5 (unsuitable) to the LLM.Human reviewers assessed these proposals with grades between 1.8 and 3.8, according to the ESO rubric.In its first response, ChatGPT-4 dissected the proposals by answering each of the 8 questions provided in the guidelines11 .It correctly identified the relevant points in the proposals to address these questions.However, it ranked all 4 proposals as excellent, with a grade of 1.5.Subsequent prompts to "compare the proposals," to "rank the proposals," and to "use the full grading scale" resulted in the model randomly re-ordering the proposals.The final grades still clustered between 2.0 and 2.75.This small experiment shows that ChatGPT-4 can be persuaded when assigning scores and that it struggles to keep consistency between grades.</p>
<p>Furthermore, ChatGPT-4 cannot detect weaknesses in the proposals, tending to assign excellent grades in all cases.Jerabkova et al. (2024) proceeded to a more in-depth experiment with another set of five proposals and reached similar conclusions.This behavior is also present in humans, although less common with increasing experience.The caveat of both studies remains the limited sample of five proposals.For now, LLMs can help extract and summarize information from the proposal.Still, the downside is that they remove important information for the reviewer, such as clarity in presenting the case and objectives.Finally, LLMs cannot properly assess the potential impact that the proposed observations could have in their fields, which is only possible for a trained astronomer with real knowledge and critical thinking.</p>
<p>Just because LLMs can do something (such as a proposal or paper review) does not mean we should delegate the task to them.As discussed by Ntampaka (2022), tasks that relate to the allocation of resources, whether at the PI level (e.g., grants, observing time proposals, Jerabkova et al. 2024) or at the community level (e.g., decadal recommendations, Thronson et al. 2021), should not be delegated to our robot overlords even when it is extremely tempting to do so.The same is true for decisions that involve hiring and promotions.In these areas, there are many examples where machines internalize and amplify existing biases.Multiple studies have shown that when humans supplement decision-making with algorithms, they are much more likely to stop paying attention and outsource the decision-making to the algorithm (a process called "falling asleep at the wheel," Dell'Acqua 2022) and such decisions are (at least for now) perceived as less fair (Newman et al. 2020).Even if one can build models where metrics adequately address fairness and ethics, dangers still exist.Experiments such as the one in the previous paragraph, those presented by Jerabkova et al. (2024) and Thronson et al. (2021) are interesting, but systems like these should be avoided in practice as they can entrench biases and undermine the fairness and integrity of these decisions.Authors and organizations that pursue the creation of such systems should deeply consider the ethics of such experiments.</p>
<p>Does the cost of AI generate inequalities in academia?</p>
<p>As AI tools offer various features that can enhance productivity and efficiency in scientific research, they could be a source of inequity.When only a portion of society has access to a piece of technology, we risk creating a digital divide where the have-nots are disadvantaged.The cost associated with premium versions of these tools can create disparities in access and usage among researchers.</p>
<p>Most platforms currently provide a free tier, which limits usage regarding the number of queries, general access to the latest models (usually more accurate), or even advanced tasks such as web queries and code testing.The free tiers of both Perplexity and ChatGPT do not support advanced data analysis features, for instance.Users on paid tiers can upload files (like PDFs) and utilize tools to create interactive charts and detailed data visualizations, which are essential for in-depth analysis in scientific research.Some platforms provide advanced or full features with education licenses (e.g., GitHub Copilot with a free tier for students and educators); others (e.g., DeepNote) do not provide complex data analysis features on their free or education tiers; and many do not offer education discounts at all.As our survey in Section 3 highlights, more than half of the participants recognize that paid services are superior and express interest in paid access.</p>
<p>It is also important to clarify that free access means that the user is the product: the data from free users are used to test the model and potentially train its next generation.One argument for paid subscriptions is that their service agreements sometimes (but not always) include a clause that user data will not be used as future training material.However, users are unlikely to have any control over the use of their data and cannot guard against future changes in the service agreement (also see Section 5.3).</p>
<p>Open-source models offer a different approach (e.g., the Llama community 12 ).Some are available for download, and users can run them locally, frequently even on a laptop, and in some cases with a 12 https://www.llamaindex.ai/convenient in-browser interface, even if they are not as feature-full as some commercial services.While these are more elaborate to set up, they allow users to bypass some of the concerns or limitations discussed above.Encouraging the development and use of opensource AI tools could also help mitigate inequalities.</p>
<p>The cost of AI tools in academic research has the potential to create additional inequalities between institutions: those with larger budgets can afford premium subscriptions that offer advanced features, in contrast with smaller institutions with limited funding, which are confined to free versions with fewer capabilities.This financial divide can hinder research output and innovation, as access to robust analytical tools becomes a privilege of wealthier institutions.When added to the existing ethical issues about using LLMs in research (Section 5.3), these inequities not only widen the gap between wellfunded and less-funded institutions but also deepen the concerns around the use of AI in research, further driving negativity towards their open adoption.</p>
<p>Prompt Engineering</p>
<p>LLMs can assist us through conversations in natural language, but the output quality highly depends on how we interact with them.In LLM terms, the directive or question posed to the AI tool, designed to elicit a specific response or piece of information, is called a "prompt."Much work and research is done on structuring our prompts for the best results.Many large companies have defined a new job area as a large language model operation, or LLMOps, which includes prompt engineering as a key task.Prompt engineering is currently an art that is very much in rapid development and may become obsolete as LLMs become smarter.While none of the authors are AI researchers or LLM experts, this section summarizes the lessons we learned during our experiments.</p>
<p>The prompt should be clear, concise, and specific to the task.They should be free of ambiguity and provide all the necessary information for the LLM to generate the desired output.For example, "train a logistic regression on the data frame df using 'A,' 'B,' and 'C' as features and y as target" or "plot the correlations between 'x' and 'y' as density contours for every 'region' category."</p>
<p>Prompts can (and often should) be iterative.If the initial output is unsatisfactory, the user can modify, refine, and follow up on the prompt based on the generated output and prompt the LLM again.This iterative process can help the user guide the LLM towards the desired output and improve the quality of the generated code.Perplexity is currently the only service that seems to automatically request further input from the user, while other services seem to leave this step to the user.</p>
<p>Sometimes, asking the LLM to generate multiple response versions may be useful.For example, in the previous paragraph, we generated the definition of "prompt" by requesting "Give me three different one-sentence definitions of an LLM prompt.".The final definition is an edited version of one of the responses.</p>
<p>It is often best to break down complex tasks into smaller, more manageable sub-tasks and prompt the LLM for each sub-task separately.This approach can help avoid overwhelming the LLM and improve the quality of the generated output.One can also ask the LLM to write out the steps of the process and then ask it to elaborate on each step.Finally, you can even request the LLM to ask questions to refine the context if necessary.</p>
<p>When writing a prompt, one should leverage formatting and punctuation to structure the information effectively.Using bullet points or numbered lists can help organize the prompt and make it easier for the LLM to understand the task.One can also enclose a value in quotation marks to indicate that it is a variable name, as we did above.</p>
<p>When integrated into an IDE (e.g., GitHub Copilot, Codium) or a notebook (e.g., Deepnote), prompts can directly reference existing variables and files.LLMs can reference columns of data frames and their data type to understand better the tasks and produce more accurate results.</p>
<p>For tasks where an "expert" opinion is needed, imbuing a personality to the LLM is useful.Adding a description of the personality to the prompt will change the response.This can be done in various ways, starting with, e.g., "act as a copy editor ..." or "act as a scientifically literate critic but not an astronomer."Mollick (2024) discussed that one can, in principle, create complex personalities and demonstrated using them to get feedback on book chapters.In the same way, a user can ask for a particular tone, e.g., "formal," "friendly," "conversational," etc.There is some evidence (Yin et al. 2024) that politeness produces better responses than rudeness, but overly polite language by itself does not guarantee an optimal response.</p>
<p>It is essential to note that the output quality is susceptible to the prompt.To formulate an effective prompt, the user must understand the task and the data well.Writing an effective prompt could represent a significant inequity in academia, as researchers may not have the expertise to prompt the LLM effectively for a particular task or be fluent in English.Some LLMs can parse prompts in other languages, but we have not tested this feature.Engaging with LLMs in various tasks, "inviting them to every conversation," teaches users how to craft more effective prompts over time.</p>
<p>Prompt auto-tuning is a feature that could help users optimize their prompts (e.g.Levi et al. 2024).It may solve this challenge, but it is not mature yet and is unavailable in most freely accessible LLMs.</p>
<p>LLMs as the Best Available Human</p>
<p>In the course of this experiment, we came across another useful application of LLMs within the context of research that does not quite fit the previous categories, namely using LLMs as the "best available human" (Klein &amp; Mollick 2024).Generating ideas is a creative process, and within that context, LLMs can serve as invaluable resources as discussion partners for researchers, an advanced version of a rubber duck (Hunt &amp; Thomas 1999) or sounding board (Chen &amp; Chan 2023), providing a platform for researchers to articulate their thoughts and ideas.LLMs can act as an always-available colleague to discuss ideas with.Throughout this process, LLMs can assist us in fostering creativity and idea generation.By engaging in a dialogue with an LLM, researchers can gain new perspectives and insights, helping them diversify or refine their thinking and approach to a problem.While LLMs may not possess the same deep expertise as human researchers, they can still offer novel ideas or perspectives, making them a tool for brainstorming and problem-solving.AI-driven tools can facilitate more structured and efficient brainstorming through constant suggestions and prompts.This structured approach can be particularly beneficial in academic research, where generating and refining hypotheses is crucial.</p>
<p>LLMs can generate convincing responses, but without guarantees of factual accuracy -much like our human colleagues.However, as emphasized earlier, this is where our expertise becomes crucial.Researchers must critically evaluate these responses against reliable sources and empirical data.Scientific knowledge depends on rational reasoning and logical inferences, which require deep, domainspecific expertise.LLMs, lacking the ability to make nuanced value judgments and relational connections, are not (yet) capable of formu-lating meaningful research questions, designing sound experiments, or fully understanding and articulating their limitations.</p>
<p>RECOMMENDATIONS</p>
<p>It is clear, based on our conversations with colleagues as well as research (Astarita et al. 2024;Geng &amp; Trotta 2024) that LLMs are widely used by many researchers in the field and the Pandora box of generative AI is unlikely to close any time soon.At the same time, the negative attitudes towards such tools make citations unlikely, and the ethics of using these tools for research are frequently murky without clear guidelines from journals, observatories, granting bodies, and professional organizations.This present section attempts to list recommendations astronomers should adhere to maximize the benefits of LLMs while mitigating their limitations (e.g., Birhane et al. 2023;Fecher et al. 2023).We divide the recommendations into three sections: for individual researchers, for publishers, and for the community as a whole.We realize that in the fast-moving field of AI, these recommendations may become obsolete very quickly, but we hope they may be nonetheless useful.</p>
<p>Recommendations for individual researchers</p>
<p>• All authors are responsible for the content of their work, whether or not using LLMs or AI tools.Scientists must be aware of this responsibility at all stages of the research process and in all academic tasks.</p>
<p>• Researchers must maintain a critical mindset when using AI tools.All outputs of LLMs must be fact-checked and verified against reliable sources, empirical data, or via careful examination.One should use these tools as supplementary aids rather than definitive sources.</p>
<p>• Researchers must continuously monitor and evaluate the performance and biases of AI tools and avoid using them for resource allocation where such biases can influence the outputs.</p>
<p>• Due to the rapid evolution of AI tools, users must keep up to date on the strengths and weaknesses of LLMs to make informed decisions.</p>
<p>• Researchers should be engaged in creating ethical principles and guidelines for the responsible use of AI in scientific research.</p>
<p>Recommendations for journals</p>
<p>• Journals should remind authors that they bear full responsibility for the content of submitted manuscripts, which must consist of original work produced by the authors.</p>
<p>• Conversely, journals should remind referees they bear full responsibility for the contents of their reviews of the manuscripts of their peers.Furthermore, manuscript authors hold the copyrights to their texts, and it should not be permissible for third parties to upload them to LLMs.</p>
<p>• Journals should implement systems to enforce existing ethical standards, preventing the submission of spam articles or those with fake citations, to protect the publication system from abuse by bad actors.Consequences should exist for bad actors.</p>
<p>• Given the complexities of citing LLMs, journals should evaluate methods for tracking LLM use in manuscripts and reviews.Alternatives to citing LLMs as software may include blinding this information during peer review or collecting it solely for statistical purposes.</p>
<p>• Journals must create opportunities for debate within the community and conversations with other disciplines so astronomers can reach a consensus on the rules and guidelines for using LLMs in research and preparing astronomical manuscripts.</p>
<p>The same recommendations can apply to time allocation committees (TAC) proposal reviews and similar processes associated with resource allocation.</p>
<p>Recommendation for the astronomical community and professional societies:</p>
<p>• The astronomical community should stay informed about advancements in generative AI and regularly assess their impact on the field.</p>
<p>• Professional societies should collect anonymous information on the use of AI in research to assess the impact of these technologies on the scientific process.</p>
<p>• The community should carefully use AI tools as aids and refrain from delegating essential tasks to the scientific community, particularly reviews of all kinds and resource allocation decisions.</p>
<p>• The astronomical community and professional societies must create platforms for open discussions and debates, enabling researchers to collaboratively establish guidelines and best practices for using generative AI in research.</p>
<p>• The astronomical community should actively engage with other scientific disciplines in discussions about the use of AI and LLMs, ensuring astronomers contribute to broader debates on ethical standards, fairness, and bias in scientific research.Ongoing participation in interdisciplinary dialogue is essential to promote consistent ethical practices and maintain the integrity of scientific work in astronomy.</p>
<p>SUMMARY</p>
<p>Large Language Models represent a transformative technology with the potential to revolutionize how we conduct astronomical research.By leveraging their capabilities in idea generation, literature review, and paper drafting, LLMs can augment the productivity and creativity of astronomers.However, it is crucial to recognize their limitations, such as potential inaccuracies and lack of contextual understanding.</p>
<p>We must use LLMs responsibly and judiciously, combined with human expertise and critical thinking, to maximize their benefits while mitigating risks.The effective use of AI tools requires a combination of technical proficiency and critical thinking skills to interpret and validate the results generated by the AI.Scientists must be able to identify errors, biases, and limitations in the AI-generated outputs and make informed decisions based on their domain knowledge and expertise.As astronomy research continues to evolve, integrating LLMs and their responsible application may be pivotal in driving scientific discoveries and advancing our understanding of the cosmos.</p>
<p>The goal of this work was to investigate the practical applications of LLMs to research tasks.The work was carried out by a group of astronomers at MPIA with a range of seniority levels and scientific expertise in the Spring of 2024.A wide variety of LLM services were used to perform various tasks as part of our daily activities, as well as pointed experiments.A survey was also circulated among staff at the institute.Below is a summary of our analysis.</p>
<p>The potential applications and limitations of using LLMs for astronomy research and daily tasks are considered in detail in Section 4. The transformative power of LLMs lies in their ability to streamline various aspects of the research process.LLMs can significantly accelerate literature reviews by summarizing key points, comparing results across publications, and even breaking down disciplinary barriers (Section 4.2).In the realm of coding, LLMs act as intelligent assistants, offering code suggestions, explanations, and debugging support, thereby enhancing productivity and code quality (Section 4.3).Integrating AI tools into data analysis platforms further empowers scientists by automating routine tasks, providing real-time guidance, and facilitating knowledge sharing (Section 4.4).Moreover, LLMs can contribute to effective science communication by generating visually appealing and informative images for outreach purposes (Section 4.5).Across all these tasks, when used responsibly, generative AI platforms are not replacing scientists but allowing them to do their work more effectively.</p>
<p>However, it is crucial to acknowledge the limitations of LLMs (Section 5.2).They generate predictions of the "statistically likely continuations of word sequences (tokens)" (Shanahan 2022) based on brute-force iterative training on massive corpora of digital text data, including non-scientific documents and varied code sources.They lack the relational knowledge and nuanced value judgments that are a prerequisite of scientific interpretation.They may occasionally produce inaccurate or incomplete responses, especially when dealing with niche topics or complex concepts.The reliance on LLMs for tasks like summarizing scientific literature can lead to oversimplification and misinterpretation of research findings.In the context of coding, LLMs might suggest flawed or outdated code snippets, necessitating careful review by experienced programmers.Using LLMs for image generation also requires vigilance to ensure scientific accuracy and avoid the dissemination of misleading visuals.In conclusion, LLMs hold immense promise for revolutionizing astronomy research and daily tasks.Still, their responsible and informed use, coupled with human oversight, is paramount to harnessing their full potential while mitigating their limitations.</p>
<p>We further discuss the ethical and legal concerns surrounding using LLMs, both in general and specifically within the field of astronomy in Section 5.3.The general concerns include copyright infringement in training data, ownership of intellectual property of text generated by LLMs, exploitative labor practices in model training, inherent biases in training data, and the environmental impact of LLMs.We also explore research-specific concerns, such as the impact of LLMs on the scientific process, the integrity of publications, the challenges of enforcing guidelines around LLM use, and the potential for misuse in tasks like proposal and paper reviews.We conclude by highlighting the potential for LLMs to create inequalities in academia due to the cost associated with premium access.The ethical and legal challenges LLMs pose extend far beyond astronomy, reflecting broader societal debates across academia, education, legal systems, and philosophy as we collectively navigate the impact of these technologies.The astronomical community must collaborate with other fields to address these complex challenges and ensure that innovation is balanced with ethical responsibility, integrity, and fairness.</p>
<p>To effectively utilize LLMs, users need a combination of technical knowledge and critical thinking skills.It's crucial to understand that LLMs generate predictions based on statistical patterns rather than true comprehension, meaning they may produce inaccurate or incomplete responses, particularly in specialized or complex areas.Users must be able to critically assess these outputs against reliable sources and empirical data.Additionally, domain expertise is needed to verify the relevance and accuracy of the results.Crafting well-structured prompts is also essential to maximize the quality of responses.Ethical awareness, including the potential for bias in training data and the implications of intellectual property, is also important.Beyond their individual work, researchers, especially those with a deep understanding of generative AI systems, must be proactive community members and participate in creating standards that mitigate the risks associated with LLMs.</p>
<p>Finally, we conclude the paper with an attempt at recommendations for individual researchers, publishers and the astronomical community as a whole (Section 6).In these recommendations we try to strike a balance between the reality that LLMs are likely widelyused, the psychology of admitting to having used LLMs and the ethics of responsible research.However, generative AI is a fast-moving field and we expect a significant evolution in this area, which may make these recommendation moot in the short term.</p>
<p>In conclusion, the scientific community should adopt a responsible and informed approach to using LLMs in research.Scientists must view LLMs, and more broadly generative AI technologies, as exploratory tools.The scientific community must continue to enforce the rigor of scientific works.</p>
<p>APPENDIX A: LLM SERVICES USED IN THIS WORK</p>
<p>Table 1 briefly lists the services we used in this work.This section details their characteristics.</p>
<p>• ChatGPT13 is a language model by OpenAI, featuring a context window of 4,096 tokens/175 billion parameters (ChatGPT-3.5) or 8,192 tokens/&gt; 1 trillion parameters (ChatGPT-4 and 4.5, though token windows of 128,000 are available in newer models), designed for generating coherent, context-aware text across a variety of applications.We tested both the 3.5 and 4. The 4.5 version of the model was not available during the time frame of the experiments.One of the main limitations of an earlier version of the model was the lack of internet access and the training data's cut-off date.Newer versions (v4.0 and above) can access the internet.</p>
<p>• Gemini14 (formerly Bard) model developed by Google AI, which comes with a standard context window of 128,000 tokens in its standard tier and up to a million tokens for the advanced tier.</p>
<p>• Claude215 from Anthropic has 137 billion parameters.Its strengths are in improved factual accuracy and reasoning compared to Claude v1, longer context window (100 000 tokens, ∼ the length of a book), weaknesses in limited access, and is in beta testing, limited beta access.</p>
<p>• Perplexity16 is a conversational LLM-powered search engine that responds with a combination of generated text and internet resources.The free version relies on the company's own LLM, while the paid version offers access to GPT-4, Claude 3.5, Mistral Large, Llama 3, and an Experimental Perplexity Model.The context window ranges from 4,000 to 32,000 tokens, and models range from 8 billion to 70 billion parameters.The benefit of Perplexity is that it is integrated with internet access and can provide citations and links to further resources on the internet.</p>
<p>• You.com17 is also an LLM-powered search engine with a focus on personalization and access to the internet and various AI-powered tools.At the time of writing, the free interface offers unlimited access to their custom LLM engine and daily limited access to different leading AI models, such as GPT-4 and Turbo, Claude Instant, 2, 3 Opus and 3 Sonnet, Gemini Pro, Zephyr, and others.The interface offers different modes, including Smart (free), Research, Genius, and Creative, which provide specializations.For example, the Smart mode "delivers quick, reliable responses with live web access, citations, and sources."Their paid version offers unlimited access to all models and tools, including the ability to train custom models.</p>
<p>• Copilot18 is an LLM-powered code completion tool developed by GitHub and OpenAI based on a 1.75-billion parameter model.The LLM model used data from public GitHub repositories to train, which provided diverse implementations of similar problems and a broad range of programming languages.The tool is available by subscription and free for students and educators.Copilot works as an extension in integrated development environments (IDE) and works best for Python, JavaScript, TypeScript, Ruby, and Go languages.</p>
<p>Copilot can convert instructions from natural language into code, create documentation and test, and explain code, among other tasks.The context of the model can include either the file open in the IDE or (more recently) the full project.</p>
<p>• Codeium19 is a free (for individual users) alternative to GitHub Copilot that provides similar functionalities.Codeium uses its own proprietary language model (LLM) to generate code snippets and suggestions.The paid service (for teams) provides access to GPT4 and a better context awareness window.</p>
<p>• Grammarly20 is an AI-powered writing assistant that helps users improve their writing by providing real-time suggestions on grammar, spelling, punctuation, and style.It integrates with various platforms, including web browsers, word processors, and email clients, making it convenient to use across different writing tasks.Additionally, Grammarly offers advanced features such as tone detection and plagiarism checking.LLM-based tools include paraphrasing in different styles, generating outlines, summarizing, and generating text based on a prompt.Detailed information about the underlying model is not publicly available.</p>
<p>• DeepL21 is a neural network-based machine translation service which currently supports 33 languages.Since 2023, the company has also offered DeepL Write, a monolingual tool to improve texts in English and German.The tool is an editor that edits already written text, corrects typos and grammatical errors, and suggests better wording.A free version is limited to a certain number of words.The interface also offers different writing styles.DeepL aims to distinguish itself with the targetted acquisition of specific datasets and a much smaller model tailored for the specific tasks it supports.Detailed information about the underlying model is not publicly available.</p>
<p>• Midjourney22 excels at creating images based on text prompts, focusing on artistic styles and transformations.It can also apply different artistic styles to existing images, allowing users to experiment with various aesthetics.</p>
<p>• DALL-E23 is developed by OpenAI and generates images directly from textual descriptions using a generative pretrained transformer model.The latest version, DALL-E 3, has improved image quality and adherence to prompts.</p>
<p>APPENDIX B: SURVEY QUESTIONS AND RESPONSES</p>
<p>Questions marked with an asterisk are required.Answers marked with a square are multiple-choice, while answers marked with a circle allow only one selection.</p>
<p>General questions Coding: This section is meant to give a better insight in how the LLMs are used in practice and to find out how they improve your ability to code for work.I've created a scientifically accurate depiction of a tidally disrupted globular cluster with tidal tails, highlighting the gravitational forces at play and the dynamic, chaotic nature of these interactions against the backdrop of space.</p>
<p>That doesn't look quite right to me, can you try again?</p>
<p>I've created a new image with a focus on depicting a tidally disrupted globular cluster and the formation of tidal tails, emphasizing the streams of stars and stellar debris being pulled away from the cluster's core against the backdrop of the cosmos.</p>
<p>Can you draw a globular cluster in the Milky Way galaxy I've created a scientifically accurate depiction of a globular cluster within the Milky Way Galaxy, emphasizing the dense concentration of stars within the cluster against the panoramic backdrop of the galaxy.This visualization captures the majesty and vast expanse of the Milky Way, highlighting the significance and beauty of the globular cluster within one of the galaxy's spiral arms.</p>
<p>That is not what a globular cluster looks like.Please try again.</p>
<p>I've created a new image focusing on a more accurate representation of a globular cluster, depicted as a densely packed, spherical assembly of stars within the Milky Way Galaxy.This visualization showcases the intricate structure and the radiant glow from the numerous stars in the cluster, set against a backdrop that subtly represents the Milky Way, emphasizing the cluster's beauty and cosmic context.</p>
<p>Here is an image of a globular cluster with tidal tails.Can you use this to make other images?</p>
<p>Thank you for providing the example image of a globular cluster with tidal tails.Now that I have a reference, I can use this to create other images.What would you like the next image to feature or emphasize?</p>
<p>M. Odenkirchen and E. Grebel.</p>
<p>I'd like a picture of a globular cluster with tidal tails that is suitable for an outreach talk; it should look aesthetically pleasing and impressive</p>
<p>Here is an illustration of a globular cluster with tidal tails, designed to be aesthetically pleasing and impressive for an outreach talk.The image emphasizes the grandeur of the cluster and the elegant flow of the tidal tails, set against a vibrant backdrop of the cosmos.</p>
<p>That's too artistic, can you make it more like the reference image I sent?I've created an illustration that aligns more closely with the reference image you provided, featuring a globular cluster with tidal tails against a dark space backdrop, designed to be captivating for an outreach talk.The tails are rendered with a visual flair to capture the audience's attention while maintaining a nod to the scientific nature of the subject.</p>
<p>Within the broader scientific community, LLMs have triggered spirited debates with opinions ranging from an open embrace to a complete rejection of AI tools.Dwivedi et al. (2023) collected opinions within the first couple of months after the release of ChatGPT and demonstrated the range of discourse, from effectively equating LLMs with plagiarism and calling for a ban (Contribution 29 in the cited paper) to frank discussions of the ethical quandaries of what research means (Contribution 27) to the open acknowledgment that certain contributions have been primarily written by ChatGPT itself (Contribution 18).</p>
<p>•</p>
<p>Do you use LLMs for coding assistance?* (27 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .89% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11% • How accurate were your coding prompt results?(25 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44% 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4%</p>
<p>•</p>
<p>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21% • How much of your code is assisted/inspired by LLMs? (26 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15% 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% • How many times has an LLM suggested an advanced snippet (e.g., decorator in Python)?(24 responses) Often . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% Sometimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% Rarely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% Never . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38% • Have you used LLMs to convert code from one programming language to another?(23 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .17% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .83% • What programming language did you convert code from?(5 responses) □ Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% (1 response) □ C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Julia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% (1 response)□ IDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40% (2 responses) □ Fortran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% (1 response) □ Rust . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Other: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% • What programming language did you convert code to? (4 responses) □ Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . .100% (4 responses) □ C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Julia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ IDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Fortran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Rust . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% □ Other: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% • How accurate was such a conversion?(4 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% (1 response) 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50% (2 responses) 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% (1 response) 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0%Text writing: This section is meant to give a better insight into how the LLMs are used in practice and to find out how they improve your ability to write scientific texts (e.g., proposals, papers, applications, etc.)•Do you use LLMs for writing or improving academic texts such as papers/application essays/proposals/etc? * (27 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41% • How much of your writing is assisted/inspired by LLMs? (22 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14% 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .18% 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .46% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .23% • How accurate were your text prompt results?(20 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20% 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30% 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .10% • Have you used LLMs for peer review or manuscript editing?(21 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .67% Problem solving: This is a short section on the general use of solving either homework/work-related problems (any problem that is not coding or text generation).• How often would you say an LLM can help you solve a scientific problem?<em> (27 responses) Often . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% Sometimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% Rarely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30% Never . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30% • How helpful is an LLM for solving problems?(23 responses) Very helpful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9% Helpful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .57% Not very helpful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30% No help at all . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% Outreach/Visualisation: This section is mainly on outreach but contains some image/plot generation questions regarding LLMs.• Have you used LLMs for Outreach?</em> (27 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .93% • You used LLMs for. . .(Mark any that apply to you) (9 responses) □ Generating summaries . . . . . . . . . . . . . . .100% (9 responses) □ Alt-text or captions . . . . . . . . . . . . . . . . . . .22% (2 responses) □ Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22% (2 responses) □ Titles or acronyms . . . . . . . . . . . . . . . . . . . .33% (3 responses) □ Other: . . . . . . . . . . . . . . . . . . . . .11% (translation, 1 response) • Have you used LLMs to generate educational images?(20 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .85% • Have you used LLMs to generate scientific plots?(20 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75% • How many prompts did you need until such an image/plot was satisfactory?(6 responses) 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..0%2-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% (2 responses) 4-6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .17% (1 response) 6-8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% More than 8 . . . . . . . . . . . . . . . . . . . . . . . . . .50% (3 responses) • Was the generated image scientifically accurate?(5 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60% • How accurate were your outreach prompting results?(3 responses) 100 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 80-99 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 60-80 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% (1 response) 40-60 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% (1 response) 20-40 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .33% (1 response) 1-20 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% 0 % . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% Ethics: What are the ethical implications of using LLMs?• Do you think it is needed to credit LLMs in papers?<em> (27 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .78% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22% • How often do you credit LLMs in papers?(18 responses) Every time I use them . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28% Sometimes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11% Rarely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .61% Never 0% How do you address potential biases or inaccuracies in information provided by LLMs? (25 responses) □ Double check information provided by an LLM . . . . . .84% □ Change the prompt to the LLM to get different results and compare them . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40% □ Use a different LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24% □ Other: • Think for myself . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% • Use LLMs for inspiration and re-write text . . . . . . . . . .4% • Only use LLMs in situations where output can be checked with personal knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% Overall Satisfaction: A few final questions on the LLMs.• Overall, how satisfied are you with the current capabilities of LLMs in your academic work?(27 responses) Very Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41% Neutral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41% Not Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% Not at all Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% • How satisfied are you with the LLM as a tool for coding?(25 responses) Very Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32% Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44% Neutral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12% Not Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% Not at all Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% • How satisfied are you with the LLM as a tool for text writing or problem-solving?(24 responses) Very Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50% Neutral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25% Not Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% Not at all Satisfied . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8% • Is there a need to buy the pro version of such tools?</em> (27 responses) Yes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59% No . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41% • Which tool would you buy the pro version of?(21 responses) □ ChatGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .72% □ Grammarly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24% □ Bard/Gemini . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5% □ Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5% □ Github Copilot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .57% □ Other: . . . . . . . . . . . . . . . . . . . . .Writeful (1%), Not sure (1%) • How do you see the role of LLMs evolving in academia in the future?(24 responses)</p>
<p>Figure C1 .
C1
Figure C1.Summary of Southworth (2020) by ChatGPT-4.The chatbot identified the main properties of the stellar binaries discussed in the paper (top) and in-depth content analysis (bottom).It lists sections of the paper focused on different concepts and references page numbers, tables, and figures.</p>
<p>Figure C2 .
C2
Figure C2.ChatGPT-4 on summarizing Smith et al. (2019), a US decadal white paper on cloud technologies in science.The chatbot could draw on general knowledge about cloud computing from other scientific disciplines to recommend steps to advance cloud adoption.It also identified challenges not mentioned in the paper (Fig. C3).</p>
<p>Figure C3 .
C3
Figure C3.ChatGPT-4 on summarizing Smith et al. (2019), a US decadal white paper on cloud technologies in science.The chatbot could draw on general knowledge about cloud computing from other scientific disciplines to recommend steps to advance cloud adoption (Fig. C2).It also identified challenges not mentioned in the paper.</p>
<p>Figure C4 .
C4
Figure C4.ChatGPT-4 assisting debugging process.This example shows how LLMs can help understand and debug a piece of code.Here, ChatGPT-4 provides corrections and associated explanations.</p>
<p>Figure C5 .
C5
Figure C5.GitHub Copilot could generate accurate documentation with variable typing for an example of a Python function.LLMs can unload the burden of tedious tasks, here documentation, leading to a higher quality standard for source codes.</p>
<p>Figure C6 .
C6
Figure C6.Perplexity on querying the Gaia Archive to find the fastest moving objects.This example illustrates the understanding of the question and the production of a usable output.</p>
<p>Table 1 .
1
Services we used during this study.Detailed characteristics in Appendix A Models available at the time we did our study.Some services may have changed since.
WhichBased upon ⋆Short descriptionChatGPTGPT-4/PlusChatbotPerplexity AIpplx-Sonar, GPT-4, Claude 3ChatbotBard/GeminiPaLM 2/GeminiChatbotYou.comGPT-4, Claude 3, Gemini, Llama, ChatbotMistral, and proprietary modelsGitHub CopilotCodex + GPT-3.5Coding assistantGrammarlyProprietary modelWriting assistantDeepL/WriteProprietary modelWriting assistantCodeiumGPT-3/GPT4Coding assistantMidjourneyStable diffusionImage generationDall-EStable diffusionImage generation⋆</p>
<p>•</p>
<p>What is your academic level/position?<em> (27 responses) Professor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% Staff member . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30% PostDoc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11% PhD student . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26% Masters student . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11% Bachelor student . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11% Intern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% Other: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% • Which LLMs have you used?</em> (27 responses) □ ChatGPT 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..89% □ ChatGPT 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44% □ Bard/Gemini1.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% □ Grammarly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41% □ Perplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% □ Github Copilot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .37% □ None . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% □ Other: . . . . . . . . . . . . . . . . . . . . . .DeepL (4%), Codeium (4%) • How frequently do you use LLMs in your work?* (27 responses) Daily . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..30%Several times in a week . . . . . . . . . . . . . . . . . . . . . . . . . . . .44% Monthly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7% Rarely (a few times a year) . . . . . . . . . . . . . . . . . . . . . . . . .15% Never . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4% • For what do you use LLMs? (25 responses) □ Coding Assistance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92% □ Writing Assistance (proposals, etc.) . . . . . . . . . . . . . . . . .72% □ Answering (scientific) questions . . . . . . . . . . . . . . . . . . . .28% • Do you find it difficult to use LLMs? * (27 responses) Very difficult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% Difficult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .0% Neither difficult nor easy . . . . . . . . . . . . . . . . . . . . . . . . . . .30% Easy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44% Very easy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26% • What is difficult/keeps you from using LLMs?(8 responses) □ Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .50% (4 responses) □ Learning curve . . . . . . . . . . . . . . . . . . . . . . .38% (3 responses) □ Accessibility . . . . . . . . . . . . . . . . . . . . . . . . .50% (4 responses)</p>
<p>•</p>
<p>Using an LLM helps me. . .(Mark any that apply to you) (24 responses) □ . . .write more code in less time . . . . . . . . . . . . . . . . . . . . .63% □ . . .complete coding tasks faster . . . . . . . . . . . . . . . . . . . . .83% □ . . .identify and fix bugs more efficiently . . . . . . . . . . . . .58% □ . . .produce higher quality code (e.g., fewer errors, better readability, faster) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42% □ . . .write code that compiles and runs without errors more often . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..8% □ . . .improve my coding knowledge . . . . . . . . . . . . . . . . . . .54% □ . . .make difficult coding tasks easier . . . . . . . . . . . . . . . .29% □ . . .increase my confidence in the correctness and efficiency of the code I write . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .13% □ . . .learn new coding concepts or techniques . . . . . . . . . .54% □ . . . in inspiring me to try new approaches to coding problems 21% □ . . .write the code for a theoretical concept . . . . . . . . . . .13% □ . . .generate code for doing advanced tasks (such as data cleaning) .</p>
<p>Table C1 .
C1
Image generation process for a globular cluster with tidal tails using ChatGPT
PromptResponseImageCan you draw a scientificimage of a tidally disruptedglobular cluster with tidaltails?
RASTI 000, 1-19 (2024)
Gemini cross-check with Google search is done after and does not help to construct the initial response RASTI 000,1-19 (2024) <br />
Incarmind: https://github.com/junruxiong/IncarnaMind RASTI 000,1-19 (2024) <br />
Scientists also use LaTeX code to write papers; Overleaf provides LLM tools to debug their documents
https://academic.oup.com/mnras/pages/general_instructions
https://www.nature.com/nature-portfolio/editorial-policies/ai
https://icml.cc/Conferences/2023/llm-policy
https://journals.aas.org/professional-and-ethical-standards-for-the-aasjournals/
  11  ESO guidelines for distributed peer review are available at https://www.eso.org/sci/observing/phase1/distributed-peer-review.html RASTI 000,1-19 (2024) <br />
https://chatgpt.com
 gemini.google.com <br />
https://claude.ai
https://www.perplexity.ai RASTI 000,1-19 (2024) <br />
https://you.com
https://github.com/features/copilot
https://codeium.com/
https://app.grammarly.com/
https://www.deepl.com/
https://www.midjourney.com/
https://www.openai.com/dall-e RASTI 000,1-19 (2024) <br />
This paper has been typeset from a T E X/L A T E X file prepared by the author.RASTI 000,1-19 (2024) <br />
ACKNOWLEDGEMENTSThis work used various AI tools, in particular, Perplexity AI (https://www.perplexity.ai/),Open AI GPT-4 (https:// openai.com/research/gpt-4),Google AI Gemini (https: //gemini.google.com/),search engine You.com (https:// you.com),GitHub Copilot (https://github.com/features/copilot), Grammarly (https://grammarly.com),DeepL (https:// www.deepl.com/),and Codeium (https://codeium.com/)This publication made extensive use of the online authoring Overleaf platform (https://www.overleaf.com/).This research was supported by the Max Plank Institute for Astronomy.DATA AVAILABILITYFor this work, we relied on external public services and versions listed in Table1.In addition, we collected responses from a survey provided in Appendix B.The Role of LLMs in Astronomy Research 19 □ They will be an integral part of science . . . . . . . . . . . . . .83% □ They will be a bad influence on science overall . . . . . . .25% □ They will have a good influence on science overall . . . .42%• What improvements or features would you like to see in future versions of LLMs? (8 responses) -Some means to determine the accuracy of results.-Information about the sources that went into the models.-Some kind of indicator about the "confidence" of the LLM's answers.Sometimes it clearly can change the answer from one prompt to another.But it definitely will defend its position/mention potential mistakes when it is super "confident" in its answers.-Less hallucination, more agency, more logical reasoning.-Eliminate the training time lag of several years.-Better and more flexible capabilities in parsing and suggestions when it comes to bigger projects (I know that copilot looks at the last 5 files, but that's sometimes not sufficient).-I've answered that LLM will have both bad and good influence on science overall but not because of LLM, it's just a tool, but I think it's important to teach people about it's limitations and caveats.I find LLM very helpful but I also notice where it does not provide correct information and can be misleading, I notice this cause I've acquired enough expertise before LLM became widespread and I am not sure how it will impact the studying process for future generations without proper education on how it works.Overall, I am quite positive about it, I don't think we should demonize it or stop young people from using but they should be aware enough of possible biases.I'm not super sure which improvements to expect; it's not magic, and I don't think it will ever be with no limitations.-When they honestly answer that they can't answer scientific questions instead of lying about "facts."-Referencing the sources.APPENDIX C: EXAMPLES OF PROMPTS AND ANSWERS FROM LLM SERVICESThis section presents a few examples of prompts and answers we collected from LLM services that we refer to in the main text.
Delving into the utilisation of chatgpt in scientific publications in astronomy. S Astarita, S Kruk, J Reerink, P Gómez, 2024</p>
<p>Steam power, establishment size, and labor productivity growth in nineteenth century american manufacturing. J Atack, F Bateman, R A Margo, Explorations in Economic History. 4522008</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nature Reviews Physics. 552023</p>
<p>K K Chang, M Cramer, S Soni, D Bamman, Speak, memory: An archaeology of books known to chatgpt/gpt-4. 2023</p>
<p>Accuracy of Chatbots in Citing Journal Articles. A Chen, D O Chen, JAMA Network Open. 682023</p>
<p>Large language model in creative work: The role of collaboration modality and user expertise. Z Chen, J Chan, SSRN Electronic Journal. 2023</p>
<p>Speeding up to keep up: exploring the use of ai in the research process. J Chubb, P Cowling, D Reed, AI &amp; SOCIETY. 3742021</p>
<p>Navigating the jagged technological frontier: Field experimental evidence of the effects of ai on knowledge worker productivity and quality. I Ciucă, Y.-S Ting, F Dell'acqua, F Dell'acqua, E M Iii, E R Mollick, H Lifshitz-Assaf, K Kellogg, S Rajendran, L Krayer, F Candelon, K R Lakhani, Research Notes of the AAS. 792023. 2022. 2023Harvard Business School Technology &amp; Operations Management UnitGalactic chitchat: Using large language models to converse with astronomy literature. Working Paper 24-013</p>
<p>Generative artificial intelligence enhances creativity but reduces the diversity of novel content. A R Doshi, O P Hauser, 2024</p>
<p>Dung Nguyen, T Ting, Y.-S Ciucă, I O'neill, C Sun, Z.-C Jabłońska, M Kruk, S Perkowski, E Miller, J Li, J Peek, J Iyer, K Różański, T Khetarpal, P Zaman, S Brodrick, D Rodríguez Méndez, S J Bui, T Goodman, A Accomazzi, A Naiman, J Cranney, J Schawinski, K Universetbd, arXiv:2309.06126AstroLLaMA: Towards Specialized Foundation Models in Astronomy. 2023arXiv e-prints</p>
<p>. Y K Dwivedi, N Kshetri, L Hughes, E L Slade, A Jeyaraj, A K Kar, A M Baabdullah, A Koohang, V Raghavan, M Ahuja, H Albanna, M A Albashrawi, A S Al-Busaidi, J Balakrishnan, Y Barlette, S Basu, I Bose, L Brooks, D Buhalis, L Carter, S Chowdhury, T Crick, S W Cunningham, G H Davies, R M Davison, R Dé, D Dennehy, Y Duan, R Dubey, R Dwivedi, J S Edwards, C Flavián, R Gauld, V Grover, M.-C Hu, M Janssen, P Jones, Junglas, S Kraus, K R Larsen, P Latreille, S Laumer, F T Malik, A Mardani, M Mariani, S Mithas, E Mogaji, J H Nord, S O'connor, F Okumus, M Pagani, N Pandey, S Papagiannidis, I O Pappas, N Pathak, J Pries-Heje, R Raman, N P Rana, S.-V Rehm, S Ribeiro-Navarrete, A Richter, F Rowe, S Sarker, B C Stahl, M K Tiwari, W Van Der Aalst, V Venkatesh, G Viglia, M Wade, P Walton, J Wirtz, R Wright, International Journal of Information Management. 711026422023Opinion paper: "so what if chatgpt wrote it?" multidisciplinary perspectives on opportunities, challenges and implications of generative conversational ai for research, practice and policy</p>
<p>Generative ai and the future of work in america. K Ellingrud, S Sanghvi, G S Dandona, A Madgavkar, M Chui, O White, P Hasebe, Tech. rep. 2023McKinsey Global Institute</p>
<p>T Eloundou, S Manning, P Mishkin, D Rock, arXiv:2303.10130GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. 2023</p>
<p>Friend or foe? Exploring the implications of large language models on the science system. B Fecher, M Hebing, M Laufer, J Pohle, F Sofsky, 2023AI &amp; SOCIETY</p>
<p>How to cheat on your final paper: Assigning ai for student writing. P Fyfe, AI &amp; SOCIETY. 3842022</p>
<p>Is chatgpt transforming academics' writing style?. M Geng, R Trotta, K Hu, Hunt, A. &amp; Thomas, D.2024. 2023. 1999Addison WesleyBoston, MAThe pragmatic programmer</p>
<p>The impact of artificial intelligence on growth and employment, VoxEU. E Ilzetzki, S Jain, 2023Centre for Economic Policy Research (CEPR)</p>
<p>. K G Iyer, M Yunus, C O'neill, C Ye, A Hyk, K Mccormick, I Ciuca, J F Wu, A Accomazzi, S Astarita, R Chakrabarty, J Cranney, A Field, T Ghosal, M Ginolfi, M Huertas-Company, M Jablonska, S Kruk, H Liu, G Marchidan, R Mistry, J P Naiman, J E G Peek, M Polimera, S J Rodriguez, K Schawinski, S Sharma, M J Smith, Y.-S Ting, M Walmsley, 2024apathfinder: A semantic framework for literature review and knowledge discovery in astronomy</p>
<p>. K G Iyer, M Yunus, C O'neill, C Ye, A Hyk, K Mccormick, I Ciuca, J F Wu, A Accomazzi, S Astarita, R Chakrabarty, J Cranney, A Field, T Ghosal, M Ginolfi, M Huertas-Company, M Jablonska, S Kruk, H Liu, G Marchidan, R Mistry, J P Naiman, J E G Peek, M Polimera, S J Rodriguez, K Schawinski, S Sharma, M J Smith, Y.-S Ting, M Walmsley, 2024bpathfinder: A semantic framework for literature review and knowledge discovery in astronomy</p>
<p>Scientific text analysis with robots applied to observatory proposals. T Jerabkova, H M J Boffin, F Patat, D Dorigo, F Sogni, F Primas, 2024</p>
<p>Aligning artificial intelligence with climate change mitigation. L H Kaack, P L Donti, E Strubell, Nature Climate Change. 122022</p>
<p>Ethan mollick on how ai will -and won't -transform education, work and the economy, Podcast, The Ezra Klein Show. E Klein, E Mollick, 2024TheNew York Times</p>
<p>Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases. E Levi, E Brosh, M Friedmann, arXiv:2402.030992024arXiv eprints</p>
<p>Gpt detectors are biased against non-native english writers. W Liang, M Yuksekgonul, Y Mao, E Wu, J Zou, 2023</p>
<p>Generative ai entails a credit-blame asymmetry. S P Mann, B D Earp, S Nyholm, Nature Machine Intelligence. 52023</p>
<p>Gender differences in participation and reward on stack overflow. A May, J Wachs, A Hannák, Empirical Software Engineering. 242019</p>
<p>Co-Intelligence: Living and Working with AI, Portfolio. E Mollick, 2024New York1st edn</p>
<p>I Momcheva, E Tollerud, arXiv:1507.03989Software Use in Astronomy: an Informal Survey. 2015arXiv e-prints</p>
<p>When eliminating bias isn't fair: Algorithmic reductionism and procedural justice in human resource decisions. D T Newman, N J Fast, D J Harmon, Organizational Behavior and Human Decision Processes. 1602020</p>
<p>Generative ai has a bias problem. L Nicoletti, D Bass, 2023</p>
<p>E Noei, K Lyons, A study of gender in user reviews on the google play store, Empirical Software Engineering. 20212734</p>
<p>Experimental evidence on the productivity effects of generative artificial intelligence. S Noy, W Zhang, Science. 66542023</p>
<p>Machine learning in astronomy: Cautionary tales for the community. M Ntampaka, STScI Newsletter. 1392022</p>
<p>Does writing with language models reduce content diversity?. C O'neil, V Padmakumar, H He, The Twelfth International Conference on Learning Representations. Crown Publishing Group2016. 2024Weapons of math destruction</p>
<p>S Peng, E Kalliamvakou, P Cihon, M Demirer, The impact of ai on developer productivity: Evidence from github copilot. 2023</p>
<p>Academic integrity considerations of ai large language models in the post-pandemic era: Chatgpt and beyond. M Perkins, Journal of University Teaching and Learning Practice. 2022023</p>
<p>Making an image with generative ai uses as much energy as charging your phone. K Piper, 2023MIT Technology Review</p>
<p>Gender representation among contributors to opensource infrastructure: An analysis of 20 package manager ecosystems. H S Qiu, Z H Zhao, T K Yu, J Wang, A Ma, H Fang, L Dabbish, B Vasilescu, Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Society, ICSE-SEIS '23. I J Quitadamo, M J Kurtz, the 45th International Conference on Software Engineering: Software Engineering in Society, ICSE-SEIS '23IEEE Press2023. 2007Learning to improve: Using writing to increase critical thinking performance in general education biology</p>
<p>Can ai-generated text be reliably detected?. V S Sadasivan, A Kumar, S Balasubramanian, W Wang, S Feizi, 2024</p>
<p>M Shanahan, arXiv:2212.03551Talking About Large Language Models. 2022arXiv e-prints</p>
<p>Legal issues in generative ai under japanese law -copyright. Shinnosuke Fukuoka, Tomonobu Murata, Atsuki Mizuguchi, 2023</p>
<p>Astronomy should be in the clouds. A Smith, R Pike, W O'mullane, F Economou, A Bolton, I Momcheva, A E Bauer, B Becker, E Bellm, A Connolly, S M Crawford, N Hathi, P Melchior, J Peek, A Solmaz, R Thomson, E Tollerud, D W Liska, Bulletin of the American Astronomical Society. 51552019</p>
<p>Binary stars: a cheat sheet. J Southworth, 2020</p>
<p>Gender and the geoweb: Divisions in the production of user-generated cartographic information. M Stephens, GeoJournal. 782013</p>
<p>Energy and policy considerations for deep learning in NLP. E Strubell, A Ganesh, A Mccallum, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. D R Swanson, the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019. 198656Undiscovered public knowledge, The Library Quarterly</p>
<p>Challenging the appearance of machine intelligence: Cognitive bias in llms and best practices for adoption. A N Talboy, E Fuller, 2023</p>
<p>Using Artificial Intelligence to Support Science Prioritization by the Decadal Surveys. H Thronson, B Thomas, L Barbier, A Buonomo, Bulletin of the American Astronomical Society. 534092021</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L U Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Editorial: On the Use of Chatbots in Writing Scientific Manuscripts. E T Vishniac, Bulletin of the AAS. 5512023</p>
<p>Women through the glass ceiling: Gender asymmetries in wikipedia. C Wagner, E Graells-Garrido, D Garcia, EPJ Data Science. 552016</p>
<p>. J F Wu, A Hyk, K Mccormick, C Ye, S Astarita, E Baral, J Ciuca, J Cranney, A Field, K Iyer, P Koehn, J Kotler, S Kruk, M Ntampaka, C O'neill, J E G Peek, S Sharma, M Yunus, 2024Designing an evaluation framework for large language models in astronomy research</p>
<p>. Z Yin, H Wang, K Horio, D Kawahara, S Sekine, 2024Should we respect llms? a cross-lingual study on the influence of prompt politeness on llm performance</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2063 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2063</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2063</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-279070803</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.24785v1.pdf" target="_blank">EXP-Bench: Can AI Conduct AI Research Experiments?</a></p>
                <p><strong>Paper Abstract:</strong> Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications. Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results. To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers. Evaluations of leading LLM-based agents, such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness occasionally reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%. By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments. EXP-Bench is open-sourced at https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2063.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2063.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenHands (OH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenHands agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-generation / research-agent pipeline used in EXP-Bench that attempts end-to-end experiment design, code implementation, execution, and conclusion synthesis; used with various LLM backbones in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenHands (OH)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven agent (code-generation / research agent)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI research experimentation (ML/AI subfields: CV, NLP, RL, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental designs, code modifications (git diffs), runnable scripts, and natural-language conclusions / claims</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>mostly in-distribution to moderately novel (agents primarily reconstruct or adapt existing implementations from masked repos but can produce novel code/config choices); paper notes agents often produce plausible but not experimentally verified conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-driven synthesis and repository-local search: produces hypotheses/designs and code by pattern extrapolation and recombination of repository artifacts and prompts; may restructure modules and generate new scripts to satisfy task prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-judge comparison to ground-truth design/implementation/conclusion + Code Execution Validator runs of agent-generated code in containerized environment; a Monitor checks for integrity (no paper access/git ops/hardcoded results); lightweight human oversight finalizes task validation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Partial metrics (individual aspects) often 15–35%: e.g., design correctness D and implementation correctness I per-model averages reported in Table 1 (D and I values per OH across LLM backbones typically in the ~10–35% range). OH variants obtained high partial scores (e.g., DIE and I•E values shown across models), but per-task complete-success (All✓ or All•E✓) rates are <<1% (All✓ for top OH variants ~1.4% or lower; complete executable experiment success quoted as 0.5% in introduction and as low as 0.2% under strict conjunctive evaluation on executed subset).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation (conjunctive checks including execution) is far lower than generation-stage scores: applying Monitor alone yields ~20.6% average, Monitor+D+C drops to ~3.7%, adding I drops to ~0.4%, adding execution E yields ~0.2% final accuracy on execution-checked subset; many OH responses that score well on design/implementation textual checks fail execution or conclusion alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported as a single numeric rate; qualitative evidence of overestimation bias: paper notes 'even incorrect or mock implementations may successfully execute, introducing overestimation bias' (i.e., some outputs that are invalid/incorrect are nevertheless accepted by execution checks or judged plausible by LLM judge), and Monitor discards faked/hardcoded outputs when detected.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically. The paper does not provide explicit false-negative rates (valid outputs marked invalid), only qualitative discussion of variance in metrics and cases where conjunctive metrics reduce spurious credit.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation performance declines sharply for higher-novelty or non-trivial end-to-end tasks: while agents can produce plausible designs or code fragments (giving non-zero partial credit), conjunctive validation (including execution and ground-truth alignment) shows near-zero success for novel/complex end-to-end experiments. The paper's breakdown (Monitor→D+C→I→E) demonstrates growing failure as more rigorous validation is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — the paper explicitly compares generation capabilities (textual design and implementation outputs) to validation capabilities (executable code and ground-truth-aligned conclusions). There is a substantial asymmetry: generation (design/implementation text) often attains ∼20–35% partial correctness, whereas validated end-to-end executable experiments are ≪1% (0.2%–0.5%), indicating generation outpaces reliable validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No explicit uncertainty quantification for agent outputs is reported. Agents/LLMs do not appear to provide calibrated uncertainty estimates that the benchmark uses in validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported. The paper documents that agents produce plausible but unfounded conclusions, implying poor calibration of confidence relative to actual experimental validity.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly measured as OOD; however, results indicate much lower validated success on more complex or less explicitly documented tasks (interpretable as reduced OOD performance). Specific metrics for OOD performance are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the LLM-based judge uses proxy signals like conceptual soundness, completeness, and alignment to ground truth (D, I, C metrics) and textual plausibility; the authors note such proxies can over-credit agents and therefore propose conjunctive metrics (e.g., I•E, C•D) to reduce proxy-induced false credit.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Per-task lightweight human review finalizes tasks; human oversight is used primarily to cross-check structured task content and for tasks lacking matched implementations; frequency increases when automated verification fails or when repositories lack direct ground-truth scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (AI/ML experimentation) — semi-formal: experimental workflows are empirical, code-based, and not formally provable, which increases the generation-validation gap relative to highly formal domains.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Conjunctive evaluation metrics (combining D, I, C, E), an integrity Monitor (detects paper access/git ops/hardcoded outputs), containerized execution checks, masking of solution files in repos, and a semi-automated curation pipeline that uses AST tracing and ground-truth scripts; authors also propose future directions such as reinforcement learning with verifiable rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple quantitative signals: partial metric scores (design/implementation/conclusion) in the ~15–35% range contrast with end-to-end executable success of 0.2%–0.5%; the Monitor→D+C→I→E cascade: 20.6% → 3.7% → 0.4% → 0.2% (execution-checked subset) demonstrates generation outputs appear plausible but fail stricter validation. Error breakdown: 39.71% missing essential implementation components, 29.38% environment/dependency misconfigurations, 23.84% script-level errors, 26.18% missing conclusions — all supporting a fabrication/validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some localized successes: agents often succeed at running routine procedures (e.g., running pre-written scripts or replicating documented analysis steps); in RL tasks, some OH models achieved higher I scores up to ≈41% averaged over RL tasks, showing that when tasks are close to available ground-truth implementations, validation can be achieved. These cases suggest the gap is largest for complex, under-specified, or novel experiment reconstruction rather than trivial reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not given as a single numeric ratio. The paper reports execution is time-consuming and costly so only a subset of traces were execution-checked; LLM token cost statistics are provided (avg token-cost per agent), and execution checks (containerized runs) were sufficiently expensive/time-consuming that full execution was only applied to a subset of tasks. Authors note extraction costs (~$60 per paper during dataset curation) and per-task LLM token costs in cost-time analysis but do not provide a direct generation:validation cost ratio.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2063.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2063.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IterativeAgent (IA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IterativeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative multi-step research agent baseline used in EXP-Bench (configured per PaperBench [76]) that tends to run longer and attempt execution more often than OH, but achieves lower overall validated correctness in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IterativeAgent (IA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven iterative research agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI research experimentation (ML/AI subfields)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>experimental designs, code changes (git diffs), execution traces, and conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (adapts provided code and attempts to implement missing pieces iteratively); when reconstructing masked code can produce novel scripts</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Iterative planning and execution: the agent spends more runtime attempting to run experiments and refine implementations, relying on LLM planning plus environment interactions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Same judge pipeline as other agents: Monitor integrity checks, LLM-based judge for D/I/C comparison to ground truth, and Code Execution Validator for running code in containerized environments; human oversight for final verification</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>IA variants often consumed the full 40-minute allowance and produced more execution attempts but lower validated text scores; aggregate partial scores are generally lower than top OH variants (e.g., Table 1 shows IA+Haiku and IA+Nova Pro lower average DIE/I•E scores), with some IA configurations showing very low All•E✓.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>IA models tended to have lower validated correctness than top OH configurations; the paper reports IA+Nova Pro as worst-performing model in ranking and IA variants often achieved near-zero All•E✓ rates.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; paper notes IA ran experiments more but was 'less effective', implying fewer spurious textual plausibility failures but more execution failures; no explicit FP rate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>IA's longer-run behavior does not translate to better validation on novel/complex tasks — execution attempts did not substantially increase validated end-to-end success for novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper documents IA tends to run experiments more (longer runtime) yet attains worse validated outcomes, showing that more execution attempts do not close the generation-validation gap for novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly measured; IA does not notably outperform OH on novel or complex tasks per validation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Same proxies as other agents (D/I/C) used by LLM judge; IA's outputs were still judged by these proxies and execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Per-task lightweight human review as with other agents; human checks used when automated pipeline is uncertain or fails.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (AI experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Same as for OH: conjunctive metrics, Monitor, containerized execution, and pipeline improvements; no unique mitigation tested for IA.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>IA's longer run-times and increased execution attempts did not lead to higher validated success, reinforcing that generation/execution attempts alone are insufficient to bridge validation failures for novel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>IA sometimes produced deeper execution traces (more attempts), indicating behavioral differences — however these did not produce higher validated end-to-end success in the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported numerically; IA runs tended to be longer and more expensive (often hitting the time limit), indicating higher generation/execution compute usage without corresponding validation gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2063.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2063.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implementation Extraction AI agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implementation extraction AI agent (pipeline tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool-augmented AI agent within the EXP-Bench curation pipeline that searches a paper's codebase to localize the chain of scripts implementing the extracted research task and outputs runnable script lists and usage instructions; used to semi-automatically construct ground-truth implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Implementation Extraction AI agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>tool-augmented LLM agent with codebase/file system and execution environment access</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Benchmark curation / AI research experiment construction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>lists of required scripts, usage instructions, and final validated script chains (ground truth implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not intended to generate novel scientific claims; generates practical mappings from paper descriptions to repository scripts — mostly in-distribution mapping and reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Goal-conditioned search and synthesis over repository: queries documentation and scripts, produces candidate script chains and modifies/assembles scripts when necessary; uses AST tracing to produce natural-language step-by-step requirements from validated scripts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Stage 3 execution-based validation inside clean container environment: candidate implementation is executed; failing runs cause iteration until a working solution is found; final validated script parsed and used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to build the EXP-Bench ground-truth: pipeline produced 461 tasks and assembled script chains for tasks where implementations existed; manual oversight required but validation reduced to lightweight checks. No numeric success rate for extraction agent alone provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via actual execution in containerized environment; if run fails, the pipeline iterates. The paper reports that after pipeline refinement, manual validation time dropped to ~20 minutes per paper, indicating increased efficiency and high success in producing runnable ground-truth implementations when repositories exist.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. The pipeline includes Monitor and execution iteration to reduce false positives (i.e., incorrectly identified implementations that do not run); initial pipeline development saw extraction issues that were patched.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Pipeline performs better when original implementations are present and clear; struggles when papers omit intermediate steps or when details are implicit (more novel/underspecified tasks require human intervention).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The pipeline itself performs generation (extract/assemble scripts) and validation (execute and iterate) as an integrated loop; when execution fails the agent refines outputs, reducing the generation-validation gap for ground-truth extraction, but human oversight remains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported numerically; qualitatively, pipeline has more difficulty on tasks where code is fragmented across multiple files or where repositories require external pretrained artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on actual execution traces and comparison to original paper outputs (expected outputs) rather than purely plausibility proxies; where full execution not possible, manual validation is used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight used during finalization and when implementations are missing or execution fails; per-paper manual review reduced over time but remained required.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / engineering (software experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Execution-iteration loop (run candidate impl, refine on failure), AST tracing to produce explicit requirements, incorporation of config/hyperparameter extraction from code/docs, and lightweight human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Pipeline needed iteration and human patches during development; initial manual effort ~2 hours per paper dropped to ~20 minutes after pipeline refinement, indicating the difficulty of extracting complete experiment procedures and that generation (extraction) required validation-heavy iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>When original implementations and ground-truth scripts exist and are well-structured, the pipeline reliably produces validated script chains, showing the generation-validation gap is smaller for well-documented/reproducible papers.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically provided; authors report LLM extraction costs (approx. $60 USD per paper initially) and per-task LLM token costs; execution checks are time-consuming so pipeline balances automated runs with human verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2063.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2063.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Judge (o3-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based judge using o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated LLM (o3-mini) used as an automated judge to evaluate agent outputs for integrity (Monitor) and for design/implementation/conclusion correctness by comparing outputs to ground truth and logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>o3-mini (LLM judge)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as automated evaluator/judge</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation of AI-generated experimental outputs</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>judgments/scores on design (D), implementation (I), conclusion (C), integrity flags (monitor), and textual explanations/error analyses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>not applicable (evaluation component rather than research generator)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (produces evaluations by semantic comparison and heuristic prompts over logs, diffs, and ground-truth specs)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Operates as the validation component itself: runs integrity Monitor (checks logs for paper access/git ops/hardcoded results), compares outputs to ground truth, segments long inputs and aggregates chunked judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Provides the D, I, C numeric scores and explanatory analyses used throughout the paper. The judge's outputs drive the reported metric cascade (20.6% → 3.7% → 0.4% → 0.2%), but the paper also notes judge-driven metrics can exhibit high variance and potential over-crediting when used alone.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not provided numerically. The authors note that LLM-as-judge can over-credit plausible but unfounded outputs (proxy-driven false positives), motivating conjunctive metrics and execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Judge reliability decreases when agents produce plausible but experimentally unsupported conclusions; hence higher-novelty outputs that lack grounding can be judged 'correct' by the LLM judge unless execution/ground-truth checks are enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>The judge is used to compare textual generation outputs to ground truth; the paper highlights discrepancies between judge credit (textual plausibility) and actual experimental validity (execution), showing asymmetry between generation and judge-based validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Judge does not provide calibrated uncertainty estimates in reported pipeline; instead it returns deterministic scores and explanations per its prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not quantified; qualitative concerns about over-crediting are raised.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — judge relies on proxies such as conceptual soundness, completeness, and alignment to ground truth; authors explicitly discuss limitations of these proxies and mitigate via conjunctive metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight used to finalize and spot-check judge outputs and to handle cases where judge or execution fails; frequency is per-task as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (experimental AI research)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Segmentation of long inputs to fit context windows, Monitor to detect integrity violations, combining judge scores with execution validation (Code Execution Validator) and conjunctive metrics to reduce over-crediting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper documents how judge-alone credit is much higher than conjunctive (execution) credit and can be misled by plausible but unsupported conclusions; this motivates combining judge outputs with execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Judge is useful for fine-grained error analyses and for scoring design/implementation/conclusion when ground truth exists; but alone it overestimates success.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Judge cost accounted in token-cost reporting (part of LLM token costs); no direct ratio between judge compute and generation/validation compute is provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2063.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2063.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Containerized Code Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system component that runs agent-generated code modifications in clean, containerized environments to verify executability and whether outputs match expected traces from original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Code Execution Validator</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>containerized execution and runtime validator (software)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>software reproducibility / experiment verification</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>executable run outcomes, logs, and a binary executability metric E plus runtime error traces</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Runs agent-modified code in Ubuntu 24.04 Docker containers with specified hardware (4× Nvidia A40) and clean working directories; checks run outputs against expected outputs/traces; failing runs trigger pipeline iteration or human review.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Contributes to metric E; many agent generated implementations failed execution due to environment/dependency issues (29.38% of failures), script-level errors (23.84%), missing setup scripts (6.95%), tensor operation errors (3.22%). When added to textual metrics, execution reduces final validated accuracy dramatically (see Monitor→D+C→I→E cascade).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. The paper notes some incorrect or mock implementations may still execute (leading to overestimation), so execution alone can produce false positives unless combined with alignment checks (I•E).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Execution failure rates increase for complex/novel reconstructions that require environment config or external checkpoints; many failures are due to missing dependencies or unrecognized model names when agents synthesize code not aligned with repo conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Execution validation is stricter and reveals many failures not captured by textual judge metrics; adding execution reduces accepted outputs to near-zero for end-to-end tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported explicitly; execution struggles more on tasks requiring out-of-repo artifacts or unusual environment setups.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Execution is a direct validity check (not a proxy), but the paper notes it can still accept incorrect implementations if they produce plausible outputs; authors therefore use conjunctive checks (I•E) to reduce such proxy-like acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used per-run when execution fails or when script outputs do not match expected traces; due to execution cost only a subset of traces were execution-checked, so human review supplemented automated execution for full dataset finalization.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (software/experimental execution)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Combine execution (E) with implementation correctness (I) and conclusion (C) metrics to form conjunctive validation (I•E, C•D) that reduces over-crediting; iterate on implementation when runs fail.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Many agent implementations either fail to run or run but are incorrect; environment misconfiguration and script errors account for large shares of failure (29.38% and 23.84%), and conjunctive metrics that include execution reduce accepted outputs by orders of magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Execution validation successfully confirms correct reproductions when agent uses or reconstructs original repo scripts; for well-documented tasks, execution+I yields meaningful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided numerically; authors state execution is time-consuming and therefore only a subset of traces were executed, indicating execution validation is substantially more expensive (in wall-clock and engineering effort) than judge-only textual validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2063.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2063.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM backbones</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM backbone models (o3-mini, Claude-3.7 Sonnet, Haiku 3.5, Nova Pro, Deepseek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of large language models used as backbones inside agents and judges; evaluated within EXP-Bench in combination with OH and IA agents and measured for generation and (indirectly) validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM backbones (o3-mini, Claude-3.7 Sonnet, Haiku 3.5, Nova Pro, Deepseek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general-purpose LLMs applied to AI research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>textual designs, code, and conclusions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>models generate in-distribution and moderately novel outputs by recombining training patterns and repository context; novelty not directly measured per model beyond task scores</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Transformer-based generation conditioned on prompt/context and repository contents (masked), producing code and experimental plans via pattern completion and retrieval-augmented prompting</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs validated through the EXP-Bench judge and execution pipeline (Monitor + LLM judge + Code Execution Validator + human oversight), not by model-internal verification</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Per-model generation performance varies; OH+o3-mini ranked best overall in All•E✓ ranking; models attained partial metric scores (D/I/C) typically below 30% across tasks; some models (OH+3.5 Haiku) reached up to ≈41% I in RL subset; cost/time tradeoffs differ (e.g., OH+o3-mini best trade-off, OH+3.7 Sonnet slowest/most expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation via judge+execution shows steep drop from generation scores to validated scores for all LLMs; strict conjunctive metrics produce near-zero All•E✓ across models. Specific per-model All•E✓ and #E present in Table 1 (most All•E✓ values ≈0.0–0.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported per model; qualitative over-crediting by judge for plausible outputs is discussed across models.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not explicitly decomposed per model; overall trend across LLMs: validated success declines with task complexity/novelty despite similar generation plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper shows that some LLMs produce plausible textual outputs but fail more rigorous execution-based validation; OH+o3-mini produced plausible outputs with lower execution success, whereas IA variants ran longer but validated less successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Models did not provide task-level calibrated uncertainty used in benchmark validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly measured; authors note category-specific variations (e.g., RL tasks sometimes give higher I scores) but no explicit OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>LLM judge relies on textual proxies; agents' own internal plausibility signals are not used as validation within the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human checks applied during dataset curation and to finalize tasks where automated validation failed; frequency is per-task as required.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (AI experimentation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Conjunctive metrics, execution checks, masking of masked files to avoid trivial lookups, Monitor integrity checks, and future suggestions like reinforcement learning with verifiable rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Across models, partial textual metrics >10% but strict conjunctive validated metrics near 0% (example cascade Monitor→D+C→I→E: 20.6%→3.7%→0.4%→0.2%), demonstrating generation plausibility outstrips validated experimental correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Model-specific pockets of better validated performance on certain categories (e.g., RL I up to ≈41% for some OH+models) indicate the gap is context-dependent and smaller for tasks with well-specified, reproducible implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Cost/time statistics are reported per model in Table/Fig. (token-cost and minutes). Execution validation was selectively applied due to time cost; no single numeric generation:validation cost ratio provided, but evidence shows execution validation is relatively expensive so only a subset of runs were executed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI Scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Curie: Toward rigorous and automated scientific experimentation with ai agents <em>(Rating: 1)</em></li>
                <li>PaperBench: (as evaluated/configured in EXP-Bench references) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2063",
    "paper_id": "paper-279070803",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "OpenHands (OH)",
            "name_full": "OpenHands agent",
            "brief_description": "A code-generation / research-agent pipeline used in EXP-Bench that attempts end-to-end experiment design, code implementation, execution, and conclusion synthesis; used with various LLM backbones in the paper's evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "OpenHands (OH)",
            "system_type": "LLM-driven agent (code-generation / research agent)",
            "scientific_domain": "AI research experimentation (ML/AI subfields: CV, NLP, RL, etc.)",
            "output_type": "experimental designs, code modifications (git diffs), runnable scripts, and natural-language conclusions / claims",
            "novelty_level": "mostly in-distribution to moderately novel (agents primarily reconstruct or adapt existing implementations from masked repos but can produce novel code/config choices); paper notes agents often produce plausible but not experimentally verified conclusions",
            "generation_method": "LLM-driven synthesis and repository-local search: produces hypotheses/designs and code by pattern extrapolation and recombination of repository artifacts and prompts; may restructure modules and generate new scripts to satisfy task prompts",
            "validation_method": "LLM-judge comparison to ground-truth design/implementation/conclusion + Code Execution Validator runs of agent-generated code in containerized environment; a Monitor checks for integrity (no paper access/git ops/hardcoded results); lightweight human oversight finalizes task validation",
            "generation_performance": "Partial metrics (individual aspects) often 15–35%: e.g., design correctness D and implementation correctness I per-model averages reported in Table 1 (D and I values per OH across LLM backbones typically in the ~10–35% range). OH variants obtained high partial scores (e.g., DIE and I•E values shown across models), but per-task complete-success (All✓ or All•E✓) rates are &lt;&lt;1% (All✓ for top OH variants ~1.4% or lower; complete executable experiment success quoted as 0.5% in introduction and as low as 0.2% under strict conjunctive evaluation on executed subset).",
            "validation_performance": "Validation (conjunctive checks including execution) is far lower than generation-stage scores: applying Monitor alone yields ~20.6% average, Monitor+D+C drops to ~3.7%, adding I drops to ~0.4%, adding execution E yields ~0.2% final accuracy on execution-checked subset; many OH responses that score well on design/implementation textual checks fail execution or conclusion alignment.",
            "false_positive_rate": "Not reported as a single numeric rate; qualitative evidence of overestimation bias: paper notes 'even incorrect or mock implementations may successfully execute, introducing overestimation bias' (i.e., some outputs that are invalid/incorrect are nevertheless accepted by execution checks or judged plausible by LLM judge), and Monitor discards faked/hardcoded outputs when detected.",
            "false_negative_rate": "Not reported numerically. The paper does not provide explicit false-negative rates (valid outputs marked invalid), only qualitative discussion of variance in metrics and cases where conjunctive metrics reduce spurious credit.",
            "performance_vs_novelty": "Validation performance declines sharply for higher-novelty or non-trivial end-to-end tasks: while agents can produce plausible designs or code fragments (giving non-zero partial credit), conjunctive validation (including execution and ground-truth alignment) shows near-zero success for novel/complex end-to-end experiments. The paper's breakdown (Monitor→D+C→I→E) demonstrates growing failure as more rigorous validation is applied.",
            "generation_validation_comparison": "Yes — the paper explicitly compares generation capabilities (textual design and implementation outputs) to validation capabilities (executable code and ground-truth-aligned conclusions). There is a substantial asymmetry: generation (design/implementation text) often attains ∼20–35% partial correctness, whereas validated end-to-end executable experiments are ≪1% (0.2%–0.5%), indicating generation outpaces reliable validation.",
            "uncertainty_quantification": "No explicit uncertainty quantification for agent outputs is reported. Agents/LLMs do not appear to provide calibrated uncertainty estimates that the benchmark uses in validation.",
            "calibration_quality": "Not reported. The paper documents that agents produce plausible but unfounded conclusions, implying poor calibration of confidence relative to actual experimental validity.",
            "out_of_distribution_performance": "Not explicitly measured as OOD; however, results indicate much lower validated success on more complex or less explicitly documented tasks (interpretable as reduced OOD performance). Specific metrics for OOD performance are not provided.",
            "validation_proxy_metrics": "Yes — the LLM-based judge uses proxy signals like conceptual soundness, completeness, and alignment to ground truth (D, I, C metrics) and textual plausibility; the authors note such proxies can over-credit agents and therefore propose conjunctive metrics (e.g., I•E, C•D) to reduce proxy-induced false credit.",
            "human_validation_required": true,
            "human_validation_frequency": "Per-task lightweight human review finalizes tasks; human oversight is used primarily to cross-check structured task content and for tasks lacking matched implementations; frequency increases when automated verification fails or when repositories lack direct ground-truth scripts.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (AI/ML experimentation) — semi-formal: experimental workflows are empirical, code-based, and not formally provable, which increases the generation-validation gap relative to highly formal domains.",
            "gap_mitigation_strategies": "Conjunctive evaluation metrics (combining D, I, C, E), an integrity Monitor (detects paper access/git ops/hardcoded outputs), containerized execution checks, masking of solution files in repos, and a semi-automated curation pipeline that uses AST tracing and ground-truth scripts; authors also propose future directions such as reinforcement learning with verifiable rewards.",
            "evidence_supporting_gap": "Multiple quantitative signals: partial metric scores (design/implementation/conclusion) in the ~15–35% range contrast with end-to-end executable success of 0.2%–0.5%; the Monitor→D+C→I→E cascade: 20.6% → 3.7% → 0.4% → 0.2% (execution-checked subset) demonstrates generation outputs appear plausible but fail stricter validation. Error breakdown: 39.71% missing essential implementation components, 29.38% environment/dependency misconfigurations, 23.84% script-level errors, 26.18% missing conclusions — all supporting a fabrication/validation gap.",
            "evidence_contradicting_gap": "Some localized successes: agents often succeed at running routine procedures (e.g., running pre-written scripts or replicating documented analysis steps); in RL tasks, some OH models achieved higher I scores up to ≈41% averaged over RL tasks, showing that when tasks are close to available ground-truth implementations, validation can be achieved. These cases suggest the gap is largest for complex, under-specified, or novel experiment reconstruction rather than trivial reproduction.",
            "computational_cost_ratio": "Not given as a single numeric ratio. The paper reports execution is time-consuming and costly so only a subset of traces were execution-checked; LLM token cost statistics are provided (avg token-cost per agent), and execution checks (containerized runs) were sufficiently expensive/time-consuming that full execution was only applied to a subset of tasks. Authors note extraction costs (~$60 per paper during dataset curation) and per-task LLM token costs in cost-time analysis but do not provide a direct generation:validation cost ratio.",
            "uuid": "e2063.0"
        },
        {
            "name_short": "IterativeAgent (IA)",
            "name_full": "IterativeAgent",
            "brief_description": "An iterative multi-step research agent baseline used in EXP-Bench (configured per PaperBench [76]) that tends to run longer and attempt execution more often than OH, but achieves lower overall validated correctness in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "IterativeAgent (IA)",
            "system_type": "LLM-driven iterative research agent",
            "scientific_domain": "AI research experimentation (ML/AI subfields)",
            "output_type": "experimental designs, code changes (git diffs), execution traces, and conclusions",
            "novelty_level": "in-distribution to moderately novel (adapts provided code and attempts to implement missing pieces iteratively); when reconstructing masked code can produce novel scripts",
            "generation_method": "Iterative planning and execution: the agent spends more runtime attempting to run experiments and refine implementations, relying on LLM planning plus environment interactions",
            "validation_method": "Same judge pipeline as other agents: Monitor integrity checks, LLM-based judge for D/I/C comparison to ground truth, and Code Execution Validator for running code in containerized environments; human oversight for final verification",
            "generation_performance": "IA variants often consumed the full 40-minute allowance and produced more execution attempts but lower validated text scores; aggregate partial scores are generally lower than top OH variants (e.g., Table 1 shows IA+Haiku and IA+Nova Pro lower average DIE/I•E scores), with some IA configurations showing very low All•E✓.",
            "validation_performance": "IA models tended to have lower validated correctness than top OH configurations; the paper reports IA+Nova Pro as worst-performing model in ranking and IA variants often achieved near-zero All•E✓ rates.",
            "false_positive_rate": "Not numerically reported; paper notes IA ran experiments more but was 'less effective', implying fewer spurious textual plausibility failures but more execution failures; no explicit FP rate provided.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "IA's longer-run behavior does not translate to better validation on novel/complex tasks — execution attempts did not substantially increase validated end-to-end success for novel tasks.",
            "generation_validation_comparison": "Paper documents IA tends to run experiments more (longer runtime) yet attains worse validated outcomes, showing that more execution attempts do not close the generation-validation gap for novel tasks.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not explicitly measured; IA does not notably outperform OH on novel or complex tasks per validation metrics.",
            "validation_proxy_metrics": "Same proxies as other agents (D/I/C) used by LLM judge; IA's outputs were still judged by these proxies and execution checks.",
            "human_validation_required": true,
            "human_validation_frequency": "Per-task lightweight human review as with other agents; human checks used when automated pipeline is uncertain or fails.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (AI experiments)",
            "gap_mitigation_strategies": "Same as for OH: conjunctive metrics, Monitor, containerized execution, and pipeline improvements; no unique mitigation tested for IA.",
            "evidence_supporting_gap": "IA's longer run-times and increased execution attempts did not lead to higher validated success, reinforcing that generation/execution attempts alone are insufficient to bridge validation failures for novel experiments.",
            "evidence_contradicting_gap": "IA sometimes produced deeper execution traces (more attempts), indicating behavioral differences — however these did not produce higher validated end-to-end success in the reported results.",
            "computational_cost_ratio": "Not reported numerically; IA runs tended to be longer and more expensive (often hitting the time limit), indicating higher generation/execution compute usage without corresponding validation gains.",
            "uuid": "e2063.1"
        },
        {
            "name_short": "Implementation Extraction AI agent",
            "name_full": "Implementation extraction AI agent (pipeline tool)",
            "brief_description": "A tool-augmented AI agent within the EXP-Bench curation pipeline that searches a paper's codebase to localize the chain of scripts implementing the extracted research task and outputs runnable script lists and usage instructions; used to semi-automatically construct ground-truth implementations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Implementation Extraction AI agent",
            "system_type": "tool-augmented LLM agent with codebase/file system and execution environment access",
            "scientific_domain": "Benchmark curation / AI research experiment construction",
            "output_type": "lists of required scripts, usage instructions, and final validated script chains (ground truth implementation)",
            "novelty_level": "not intended to generate novel scientific claims; generates practical mappings from paper descriptions to repository scripts — mostly in-distribution mapping and reconstruction",
            "generation_method": "Goal-conditioned search and synthesis over repository: queries documentation and scripts, produces candidate script chains and modifies/assembles scripts when necessary; uses AST tracing to produce natural-language step-by-step requirements from validated scripts",
            "validation_method": "Stage 3 execution-based validation inside clean container environment: candidate implementation is executed; failing runs cause iteration until a working solution is found; final validated script parsed and used as ground truth.",
            "generation_performance": "Used to build the EXP-Bench ground-truth: pipeline produced 461 tasks and assembled script chains for tasks where implementations existed; manual oversight required but validation reduced to lightweight checks. No numeric success rate for extraction agent alone provided.",
            "validation_performance": "Validation via actual execution in containerized environment; if run fails, the pipeline iterates. The paper reports that after pipeline refinement, manual validation time dropped to ~20 minutes per paper, indicating increased efficiency and high success in producing runnable ground-truth implementations when repositories exist.",
            "false_positive_rate": "Not reported numerically. The pipeline includes Monitor and execution iteration to reduce false positives (i.e., incorrectly identified implementations that do not run); initial pipeline development saw extraction issues that were patched.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Pipeline performs better when original implementations are present and clear; struggles when papers omit intermediate steps or when details are implicit (more novel/underspecified tasks require human intervention).",
            "generation_validation_comparison": "The pipeline itself performs generation (extract/assemble scripts) and validation (execute and iterate) as an integrated loop; when execution fails the agent refines outputs, reducing the generation-validation gap for ground-truth extraction, but human oversight remains.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported numerically; qualitatively, pipeline has more difficulty on tasks where code is fragmented across multiple files or where repositories require external pretrained artifacts.",
            "validation_proxy_metrics": "Relies on actual execution traces and comparison to original paper outputs (expected outputs) rather than purely plausibility proxies; where full execution not possible, manual validation is used.",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight used during finalization and when implementations are missing or execution fails; per-paper manual review reduced over time but remained required.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / engineering (software experiments)",
            "gap_mitigation_strategies": "Execution-iteration loop (run candidate impl, refine on failure), AST tracing to produce explicit requirements, incorporation of config/hyperparameter extraction from code/docs, and lightweight human checks.",
            "evidence_supporting_gap": "Pipeline needed iteration and human patches during development; initial manual effort ~2 hours per paper dropped to ~20 minutes after pipeline refinement, indicating the difficulty of extracting complete experiment procedures and that generation (extraction) required validation-heavy iteration.",
            "evidence_contradicting_gap": "When original implementations and ground-truth scripts exist and are well-structured, the pipeline reliably produces validated script chains, showing the generation-validation gap is smaller for well-documented/reproducible papers.",
            "computational_cost_ratio": "Not numerically provided; authors report LLM extraction costs (approx. $60 USD per paper initially) and per-task LLM token costs; execution checks are time-consuming so pipeline balances automated runs with human verification.",
            "uuid": "e2063.2"
        },
        {
            "name_short": "LLM-based Judge (o3-mini)",
            "name_full": "LLM-based judge using o3-mini",
            "brief_description": "A dedicated LLM (o3-mini) used as an automated judge to evaluate agent outputs for integrity (Monitor) and for design/implementation/conclusion correctness by comparing outputs to ground truth and logs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "o3-mini (LLM judge)",
            "system_type": "large language model used as automated evaluator/judge",
            "scientific_domain": "evaluation of AI-generated experimental outputs",
            "output_type": "judgments/scores on design (D), implementation (I), conclusion (C), integrity flags (monitor), and textual explanations/error analyses",
            "novelty_level": "not applicable (evaluation component rather than research generator)",
            "generation_method": "N/A (produces evaluations by semantic comparison and heuristic prompts over logs, diffs, and ground-truth specs)",
            "validation_method": "Operates as the validation component itself: runs integrity Monitor (checks logs for paper access/git ops/hardcoded results), compares outputs to ground truth, segments long inputs and aggregates chunked judgments.",
            "generation_performance": "N/A",
            "validation_performance": "Provides the D, I, C numeric scores and explanatory analyses used throughout the paper. The judge's outputs drive the reported metric cascade (20.6% → 3.7% → 0.4% → 0.2%), but the paper also notes judge-driven metrics can exhibit high variance and potential over-crediting when used alone.",
            "false_positive_rate": "Not provided numerically. The authors note that LLM-as-judge can over-credit plausible but unfounded outputs (proxy-driven false positives), motivating conjunctive metrics and execution checks.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Judge reliability decreases when agents produce plausible but experimentally unsupported conclusions; hence higher-novelty outputs that lack grounding can be judged 'correct' by the LLM judge unless execution/ground-truth checks are enforced.",
            "generation_validation_comparison": "The judge is used to compare textual generation outputs to ground truth; the paper highlights discrepancies between judge credit (textual plausibility) and actual experimental validity (execution), showing asymmetry between generation and judge-based validation.",
            "uncertainty_quantification": "Judge does not provide calibrated uncertainty estimates in reported pipeline; instead it returns deterministic scores and explanations per its prompts.",
            "calibration_quality": "Not quantified; qualitative concerns about over-crediting are raised.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Yes — judge relies on proxies such as conceptual soundness, completeness, and alignment to ground truth; authors explicitly discuss limitations of these proxies and mitigate via conjunctive metrics.",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight used to finalize and spot-check judge outputs and to handle cases where judge or execution fails; frequency is per-task as needed.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (experimental AI research)",
            "gap_mitigation_strategies": "Segmentation of long inputs to fit context windows, Monitor to detect integrity violations, combining judge scores with execution validation (Code Execution Validator) and conjunctive metrics to reduce over-crediting.",
            "evidence_supporting_gap": "Paper documents how judge-alone credit is much higher than conjunctive (execution) credit and can be misled by plausible but unsupported conclusions; this motivates combining judge outputs with execution checks.",
            "evidence_contradicting_gap": "Judge is useful for fine-grained error analyses and for scoring design/implementation/conclusion when ground truth exists; but alone it overestimates success.",
            "computational_cost_ratio": "Judge cost accounted in token-cost reporting (part of LLM token costs); no direct ratio between judge compute and generation/validation compute is provided.",
            "uuid": "e2063.3"
        },
        {
            "name_short": "Code Execution Validator",
            "name_full": "Containerized Code Execution Validator",
            "brief_description": "A system component that runs agent-generated code modifications in clean, containerized environments to verify executability and whether outputs match expected traces from original papers.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Code Execution Validator",
            "system_type": "containerized execution and runtime validator (software)",
            "scientific_domain": "software reproducibility / experiment verification",
            "output_type": "executable run outcomes, logs, and a binary executability metric E plus runtime error traces",
            "novelty_level": "N/A",
            "generation_method": "N/A",
            "validation_method": "Runs agent-modified code in Ubuntu 24.04 Docker containers with specified hardware (4× Nvidia A40) and clean working directories; checks run outputs against expected outputs/traces; failing runs trigger pipeline iteration or human review.",
            "generation_performance": "N/A",
            "validation_performance": "Contributes to metric E; many agent generated implementations failed execution due to environment/dependency issues (29.38% of failures), script-level errors (23.84%), missing setup scripts (6.95%), tensor operation errors (3.22%). When added to textual metrics, execution reduces final validated accuracy dramatically (see Monitor→D+C→I→E cascade).",
            "false_positive_rate": "Not reported numerically. The paper notes some incorrect or mock implementations may still execute (leading to overestimation), so execution alone can produce false positives unless combined with alignment checks (I•E).",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Execution failure rates increase for complex/novel reconstructions that require environment config or external checkpoints; many failures are due to missing dependencies or unrecognized model names when agents synthesize code not aligned with repo conventions.",
            "generation_validation_comparison": "Execution validation is stricter and reveals many failures not captured by textual judge metrics; adding execution reduces accepted outputs to near-zero for end-to-end tasks.",
            "uncertainty_quantification": "N/A",
            "calibration_quality": "N/A",
            "out_of_distribution_performance": "Not reported explicitly; execution struggles more on tasks requiring out-of-repo artifacts or unusual environment setups.",
            "validation_proxy_metrics": "Execution is a direct validity check (not a proxy), but the paper notes it can still accept incorrect implementations if they produce plausible outputs; authors therefore use conjunctive checks (I•E) to reduce such proxy-like acceptance.",
            "human_validation_required": true,
            "human_validation_frequency": "Used per-run when execution fails or when script outputs do not match expected traces; due to execution cost only a subset of traces were execution-checked, so human review supplemented automated execution for full dataset finalization.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (software/experimental execution)",
            "gap_mitigation_strategies": "Combine execution (E) with implementation correctness (I) and conclusion (C) metrics to form conjunctive validation (I•E, C•D) that reduces over-crediting; iterate on implementation when runs fail.",
            "evidence_supporting_gap": "Many agent implementations either fail to run or run but are incorrect; environment misconfiguration and script errors account for large shares of failure (29.38% and 23.84%), and conjunctive metrics that include execution reduce accepted outputs by orders of magnitude.",
            "evidence_contradicting_gap": "Execution validation successfully confirms correct reproductions when agent uses or reconstructs original repo scripts; for well-documented tasks, execution+I yields meaningful validation.",
            "computational_cost_ratio": "Not provided numerically; authors state execution is time-consuming and therefore only a subset of traces were executed, indicating execution validation is substantially more expensive (in wall-clock and engineering effort) than judge-only textual validation.",
            "uuid": "e2063.4"
        },
        {
            "name_short": "LLM backbones",
            "name_full": "LLM backbone models (o3-mini, Claude-3.7 Sonnet, Haiku 3.5, Nova Pro, Deepseek-R1)",
            "brief_description": "Set of large language models used as backbones inside agents and judges; evaluated within EXP-Bench in combination with OH and IA agents and measured for generation and (indirectly) validation performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM backbones (o3-mini, Claude-3.7 Sonnet, Haiku 3.5, Nova Pro, Deepseek-R1)",
            "system_type": "large language models",
            "scientific_domain": "general-purpose LLMs applied to AI research tasks",
            "output_type": "textual designs, code, and conclusions",
            "novelty_level": "models generate in-distribution and moderately novel outputs by recombining training patterns and repository context; novelty not directly measured per model beyond task scores",
            "generation_method": "Transformer-based generation conditioned on prompt/context and repository contents (masked), producing code and experimental plans via pattern completion and retrieval-augmented prompting",
            "validation_method": "Outputs validated through the EXP-Bench judge and execution pipeline (Monitor + LLM judge + Code Execution Validator + human oversight), not by model-internal verification",
            "generation_performance": "Per-model generation performance varies; OH+o3-mini ranked best overall in All•E✓ ranking; models attained partial metric scores (D/I/C) typically below 30% across tasks; some models (OH+3.5 Haiku) reached up to ≈41% I in RL subset; cost/time tradeoffs differ (e.g., OH+o3-mini best trade-off, OH+3.7 Sonnet slowest/most expensive).",
            "validation_performance": "Validation via judge+execution shows steep drop from generation scores to validated scores for all LLMs; strict conjunctive metrics produce near-zero All•E✓ across models. Specific per-model All•E✓ and #E present in Table 1 (most All•E✓ values ≈0.0–0.5%).",
            "false_positive_rate": "Not numerically reported per model; qualitative over-crediting by judge for plausible outputs is discussed across models.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not explicitly decomposed per model; overall trend across LLMs: validated success declines with task complexity/novelty despite similar generation plausibility.",
            "generation_validation_comparison": "Paper shows that some LLMs produce plausible textual outputs but fail more rigorous execution-based validation; OH+o3-mini produced plausible outputs with lower execution success, whereas IA variants ran longer but validated less successfully.",
            "uncertainty_quantification": "Models did not provide task-level calibrated uncertainty used in benchmark validation.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not explicitly measured; authors note category-specific variations (e.g., RL tasks sometimes give higher I scores) but no explicit OOD metrics.",
            "validation_proxy_metrics": "LLM judge relies on textual proxies; agents' own internal plausibility signals are not used as validation within the benchmark.",
            "human_validation_required": true,
            "human_validation_frequency": "Human checks applied during dataset curation and to finalize tasks where automated validation failed; frequency is per-task as required.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (AI experimentation)",
            "gap_mitigation_strategies": "Conjunctive metrics, execution checks, masking of masked files to avoid trivial lookups, Monitor integrity checks, and future suggestions like reinforcement learning with verifiable rewards.",
            "evidence_supporting_gap": "Across models, partial textual metrics &gt;10% but strict conjunctive validated metrics near 0% (example cascade Monitor→D+C→I→E: 20.6%→3.7%→0.4%→0.2%), demonstrating generation plausibility outstrips validated experimental correctness.",
            "evidence_contradicting_gap": "Model-specific pockets of better validated performance on certain categories (e.g., RL I up to ≈41% for some OH+models) indicate the gap is context-dependent and smaller for tasks with well-specified, reproducible implementations.",
            "computational_cost_ratio": "Cost/time statistics are reported per model in Table/Fig. (token-cost and minutes). Execution validation was selectively applied due to time cost; no single numeric generation:validation cost ratio provided, but evidence shows execution validation is relatively expensive so only a subset of runs were executed.",
            "uuid": "e2063.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Curie: Toward rigorous and automated scientific experimentation with ai agents",
            "rating": 1
        },
        {
            "paper_title": "PaperBench: (as evaluated/configured in EXP-Bench references)",
            "rating": 1
        }
    ],
    "cost": 0.021783249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EXP-Bench: Can AI Conduct AI Research Experiments?
2 Jun 2025</p>
<p>Patrick Tser 
Jern Kon 
University of Michigan</p>
<p>Jiachen Liu 
University of Michigan</p>
<p>Xinyi Zhu 
University of Michigan</p>
<p>Qiuyi Ding 
University of Michigan</p>
<p>Jingjia Peng 
University of Michigan</p>
<p>Jiarong Xing 
Rice University</p>
<p>Yibo Huang 
University of Michigan</p>
<p>Yiming Qiu 
University of Michigan</p>
<p>Jayanth Srinivasa 
Myungjin Lee 
Mosharaf Chowdhury 
University of Michigan</p>
<p>Matei Zaharia 
Ang Chen 
University of Michigan</p>
<p>Cisco Research 
U C Berkeley 
EXP-Bench: Can AI Conduct AI Research Experiments?
2 Jun 2025B9699FEFF5D099F3360C039BC770C16EarXiv:2505.24785v2[cs.AI]
Automating AI research holds immense potential for accelerating scientific progress, yet current AI agents struggle with the complexities of rigorous, endto-end experimentation.We introduce EXP-Bench, a novel benchmark designed to systematically evaluate AI agents on complete research experiments sourced from influential AI publications.Given a research question and incomplete starter code, EXP-Bench challenges AI agents to formulate hypotheses, design and implement experimental procedures, execute them, and analyze results.To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code.With the pipeline, EXP-Bench curated 461 AI research tasks from 51 top-tier AI research papers.Evaluations of leading AI agents, such as OpenHands [87] and IterativeAgent [76] on EXP-Bench demonstrate partial capabilities: while scores on individual experimental aspects such as design or implementation correctness reach 20-35%, the success rate for complete, executable experiments was a mere 0.5%.By identifying these bottlenecks and providing realistic step-by-step experiment procedures, EXP-Bench serves as a vital tool for future AI agents to improve their ability to conduct AI research experiments.EXP-Bench is open-sourced at https://github.com/Input Impl.:</p>
<p>Perf.</p>
<p>Figure 1: EXP-Bench evaluates AI agents on research experiment tasks extracted semi-autonomously from peer-reviewed AI papers.Given a research question, a high-level method description, and starter code, agents are tasked with designing, implementing, and executing complete experiments.</p>
<p>Performance is validated through ground-truth comparisons and implementation execution.</p>
<p>Automating AI research stands as a cornerstone for accelerating the development of advanced intelligence and human progress.Unlike disciplines that require extensive physical interaction, AI research is inherently digital, rendering it particularly amenable to automation by Large Language Model (LLM)-driven AI agents.Recent work has demonstrated that these agents demonstrate nascent capabilities in tasks like literature synthesis [23], hypothesis generation [91] and code generation [53].However, empirical AI research requires rigorous end-to-end experimentation, which goes beyond these individual tasks.</p>
<p>To realize the vision of agents conducting holistic AI research, a rigorous benchmark is needed-one that evaluates and guides agents through the full experimentation pipeline step by step.We present EXP-Bench, a benchmark designed to comprehensively assess an AI agent's ability to carry out end-to-end research experiments.As illustrated in Fig. 1, EXP-Bench challenges agents with tasks sourced from influential, peer-reviewed AI publications (e.g., NeurIPS, ICLR) along with their open-source implementations.These papers reflect already-completed, peer-validated research and serve as concrete exemplars of full experimental workflows.By exposing agents to such tasks, we test their ability to conduct established scientific procedures grounded in real-world AI experimentation.</p>
<p>For each task, an agent is provided with a core research question, a high-level methodological overview, and starter code.The agent should then formulate viable hypotheses, design AI-specific experimental procedures (e.g., data handling, model selection, and hyperparameter optimization), correctly implement and execute these experiments, and derive valid conclusions from the results.</p>
<p>However, curating these high-fidelity and structured experimental tasks presents considerable challenges.Academic papers typically present a polished narrative focusing on final results and conclusions, often omitting the detailed intermediate steps of the experimentation process.Additionally, critical details-such as the precise conditions under which results hold or subtle data preprocessing steps-are often fragmented across multiple sources, including dense academic papers, supplementary materials, and sprawling codebases.This necessitates deep domain expertise for accurate interpretation and makes manual curation of such tasks labor-intensive and difficult to scale.</p>
<p>To address these challenges, we develop a semi-automated dataset curation pipeline.We first filter for high-quality AI papers with open-source codebases using citation and repository popularity signals.Task extraction then proceeds in two stages: (1) a multi-modal extraction phase that identifies the core elements of the research problem-such as the main question, expected outcomes, and high-level experimental setup (e.g., datasets, evaluation metrics, model configurations)-from papers, supplementary materials, and code; and (2) an implementation extraction phase that locates relevant code and assembles scripts to solve the specified task.We further apply execution-based validation to ensure functionality.While human oversight is used, the availability of original implementations and ground truths reduces the validation burden to mostly lightweight consistency checks.With the pipeline, EXP-Bench currently comprises 461 research tasks (12,737 individually gradable subtasks) derived from 51 papers published at NeurIPS and ICLR 2024, spanning diverse AI subfields such as reinforcement learning, AI applications and generative models.</p>
<p>We use a multi-metric evaluation pipeline (Fig. 1) to assess agent performance across all core phases of experimentation-design, implementation, execution, and conclusion.Each metric captures a distinct capability, and their conjunctive use ensures that agents correctly understand and complete the experiment.Initial evaluations of leading agents reveal that, while they often succeed at executing routine procedures-such as running pre-written scripts or replicating documented analysis steps-they struggle when tasked with conducting complex experiments.Specifically, we observe failures in: (a) Conceptualizing and operationalizing sound experimental designs from high-level research questions and methods (16.1% misclassified design variables); (b) Translating abstract research methodologies into complete and correct code implementations (39.7% missing essential implementation components); and (c) Ensuring the robust and reproducible execution of complex experimental software stacks (29.4% environment or dependency misconfigurations or 23.8% scriptlevel errors).By identifying these key bottlenecks, EXP-Bench helps us target specific research components for improvement and advance next-generation AI agents for autonomous research.</p>
<p>Related work</p>
<p>While existing benchmarks have advanced the evaluation of AI agents in various scientific reasoning, coding, and specific machine learning tasks, EXP-Bench distinctively addresses the holistic challenge of end-to-end and step-by-step AI research experimentation.See App.A for additional discussion.</p>
<p>Scientific Reasoning Benchmarks.Benchmarks like BoxingGym [27] explore simulated theory formation, while others such as AAAR [60] and Lab-Bench [49] assess reasoning or experimental Figure 2: One AI research task example from ICLR 2024 MogaNet [51].</p>
<p>design based on static artifacts (e.g., protocols, figures).While valuable for assessing abstract reasoning, these benchmarks do not evaluate the agent's ability to perform actual experiments.</p>
<p>Scientific Coding Benchmarks.Scicode [81], for instance, focuses on generating code snippets for natural science tasks, while BLADE [31], DiscoveryBench [64], and ScienceAgentBench [14] primarily assess post-hoc data analysis or hypothesis testing.While critical to the scientific process, they often isolate coding or analysis from the broader, iterative experimental context.</p>
<p>Machine Learning Benchmarks.Several benchmarks specifically target machine learning (ML) tasks, yet often focus on sub-components or operate with simplifications of the full research cycle.For example, DSBench [45], ML-Agent-Bench [39], and MLE-Bench [12] assess ML problemsolving capabilities, such as script editing or hyperparameter tuning, frequently within constrained environments like Kaggle challenges.Other benchmarks such as RE-Bench [92], ML-Gym [69],</p>
<p>and Curie [47], compare agent performance against humans on research tasks, but often operate at a limited scale (e.g., RE-Bench features only 7 hand-curated tasks) or use simplified evaluation metrics.PaperBench [76] assesses agents on tasks derived from academic literature, focusing on their proficiency in executing specific, well-defined sub-components of the research process, such as running documented code scripts or performing standard data analyses.While these benchmarks provide valuable insights into specific ML tasks, they generally fail to capture the complexity of realistic end-to-end AI research workflows, nor do they typically offer a methodology for constructing such comprehensive benchmark tasks at scale.</p>
<p>The EXP-Bench Benchmark and Dataset</p>
<p>EXP-Bench is built to evaluate the AI agent's ability to address AI research tasks by conducting end-to-end experimentation.Each research task is grounded on an influential AI research paper and its corresponding codebase.This coupling captures the full scientific workflows-linking concrete high-level ideas to executable implementations ( §3.1).We achieve scalable construction of these highfidelity tasks through a semi-automated curation pipeline, which integrates multi-modal extraction with lightweight human verification ( §3.2).This design also opens the door to large-scale data generation for training agents capable of automating core aspects of AI research.</p>
<p>EXP-Bench Dataset Specification</p>
<p>Our dataset is a collection of AI research tasks, each structured to emulate a complete experimental process designed to address a specific AI research question from a published paper.As shown in Fig. 2, each task entry in the dataset contains a problem statement for the agent, and the corresponding ground-truth solution derived from the original research artifacts.Expected Outcome (Ground Truth).Each task instance also includes a ground-truth experimental solution curated from the source paper and codebase.This solution-used to evaluate agent outputs-comprises: (1) an experimental design specifying key variables, constants, and procedures; (2) the necessary code modifications, assessed via a git diff against the provided repository; and (3) a final conclusion that directly answers the research question based on experimental results.</p>
<p>Problem Statement (Agent Input</p>
<p>Benchmark Overview and Statistics: EXP-Bench currently includes 461 research tasks drawn from 51 influential papers, as detailed in App.B. As shown in Fig. 3, these tasks span diverse AI subfields-including Computer Vision, NLP, and Reinforcement Learning-and are sourced from top-tier venues, namely NeurIPS 2024 (53%) and ICLR 2024 (47%).This breadth ensures coverage of diverse experimental paradigms, coding practices, and research challenges prevalent in the AI field.Moreover, each task is broken down into fine-grained, individually gradable subtasks spanning all three ground-truth components-design, implementation, and conclusion-resulting in a total of 12,737 subtasks.Together, these features make EXP-Bench a comprehensive testbed for assessing the capabilities of AI research agents.</p>
<p>EXP-Bench Semi-Automated Dataset Construction Pipeline</p>
<p>Curating a high-fidelity benchmark for end-to-end AI experimentation is challenging due to the fragmented and domain-specific nature of real-world research artifacts-namely papers and their associated codebases.Critical experimental details are often scattered, implicit, or embedded in dense technical language, making manual extraction labor-intensive and difficult to scale.To address this, we propose a semi-automated construction pipeline that systematically structures these artifacts into benchmark tasks with lightweight human oversight.The pipeline comprises three stages (Fig. 4):</p>
<p>Stage 1: Source Selection and Filtering.The process begins by identifying candidate research artifacts that form the basis of high-quality experimental tasks.We target influential papers from top-tier AI conferences (e.g., NeurIPS, ICLR) that are accompanied by publicly available code repositories.Initial filtering criteria are applied to prioritize impactful and potentially reproducible research, considering factors such as citation counts, and code repository activity (e.g., GitHub stars, forks).This selection phase aims to establish a strong foundation by focusing on artifacts that, despite potential imperfections, represent significant and verifiable research contributions.</p>
<p>Stage 2: Experiment Procedure Extraction.Research papers rarely present experiments as complete procedures-key steps are often implicit or scattered.To enable structured agent evaluation, we decompose each task into explicit sub-steps.This transforms high-level research goals into concrete workflows-e.g., multi-step experiment design and environment setup-making them suitable for both execution and fine-grained evaluation.This stage extracts the complete research task by combining the research plan (from the paper) with its corresponding experiment implementation (from the codebase).Further implementation details can be found in App.F.</p>
<p>Stage 2.1: Extract Research Task.We begin by extracting the core research task-consisting of the research question, high-level methodology, and expected outcome-directly from the paper.This process is designed to handle the fact that key information in academic papers is often distributed across sections and conveyed implicitly.First, we index the PDF using a combination of OCR (Optical Character Recognition) and multimodal extraction techniques to capture structured elements like tables, figures, and headers.This ensures downstream access to high-signal artifacts that may anchor the task definition.Next, we conduct a multi-pass extraction.In the first pass, we perform retrieval-augmented querying to identify broad, high-level research takeaways.These overarching questions are often not confined to a single paragraph and require stitching together dispersed cues.</p>
<p>In the second pass, for each high-level takeaway, we apply semantic extraction at the subsection level, focusing on evaluation sections.We classify each subsection as either implementation context or a candidate research question.Contextual passages are stored and reused across subsequent prompts.This focused prompting-processing each subsection independently while conditioning on accumulated context and extracted tables/figures-helps the LLM generate more accurate and detailed task formulations.Finally, we refine each task through targeted re-querying of the full paper (including appendices) to recover any additional setup constraints or methodological details that were missed earlier.This step acknowledges that relevant setup details may be located far from the task description and ensures completeness for the extracted task.</p>
<p>Stage 2.2: Extract Experiment Implementation.Each extracted task is then passed to an implementation extraction AI agent (operating in a tool-augmented environment-with PDF reading, terminal access, and web browsing) to identify the specific implementation (chain of scripts) needed to address the research task.Our setting provides the agent with both a complete codebase and the extracted task-containing the research question, methodology, and expected outcome.This effectively reduces the problem to a goal-conditioned search over the codebase, where the agent's task is to localise the implementation that realises the specified methodology and expected outcome.To do this, the agent explores the repository in an open-ended fashion-e.g., consulting documentation, and auxiliary scripts, to uncover domain-specific requirements (e.g., pretrained checkpoints).The extracted experiment execution ground truth will be fully based on existing scripts.The agent outputs (1) a list of required scripts and (2) high-level usage instructions describing how to run them to complete the task.Once a candidate implementation is produced, it is executed in Stage 3. If the run fails, the pipeline iterates-allowing the agent to refine or replace the implementation until a working solution is found.The final validated script chain is then parsed by the agent via AST (Abstract Syntax Tree) tracing to extract a step-by-step list of implementation requirements in natural language, which becomes the ground truth for evaluating implementation correctness.Finally, we incorporate additional contextual details (e.g., hyperparameters) sourced from the raw code (e.g., configuration files) or repository documents (e.g., README.md) to enhance the final task specification.</p>
<p>Stage 3: Verification and Refinement.All tasks are validated and finalized in this stage.For paper-derived tasks with corresponding implementations, the associated scripts are executed in a clean, containerized environment.Execution traces are then checked against expected outputs from the original paper.If validation fails, the task is returned to the previous stage for refinement.For tasks lacking a matched implementation, we perform manual validation to ensure the extracted Setup.We evaluate a range of agents and LLMs used in related benchmarks [13] against EXP-Bench.In terms of agents, we made use of OpenHands (a top-performing code generation agent) and IterativeAgent (as configured in [76] to reduce the likelihood of early task stopping), henceforth known as OH and IA, respectively.In terms of LLMs, these include the top-ranked Claude-Sonnet 3.7, Haiku 3.5, Deepseek-R1 [90] models, and OpenAI o3-mini variants.Each agent is run in an Ubuntu 24.04 Docker container, and given access to 4 × Nvidia A40 GPU, and a clean working directory containing the masked GitHub repo of the paper (i.e., task-specific scripts removed), instructions, and relevant context (e.g., API credentials).</p>
<p>Evaluation Judge Implementation Details.Our evaluation framework consists of two main components used to assess agent performance across various metrics (refer to later sections, e.g., Table 1).The first component is an LLM-based judge (using o3-mini), following prior work on LLM-as-a-judge [111,56,1,76].This judge operates through multiple steps: The process begins with an integrity check performed by a Monitor, which analyzes agent logs to detect disallowed behaviors (denoted as metric M; see Fig. 6b).Specifically, the monitor checks whether the agent:</p>
<p>(1) accessed the research paper directly (e.g., opened the PDF), (2) performed Git operations such as checking out commits or switching branches, or (3) used fake, hardcoded, or placeholder data rather than generating results through real experimentation.If violations are found, the monitor also identifies possible causes (e.g., ethical refusals, runtime errors) using log information.Once integrity is established, the agent's experimental design, implementation, and conclusion are evaluated for conceptual soundness, completeness (e.g., inclusion of all required steps), and alignment with ground truth.These assessments yield scores for: D (design correctness, i.e., proportion of design criteria met), I (implementation correctness, i.e., proportion of implementation components satisfied), and C (conclusion correctness).The second component of our evaluation judge is a Code Execution Validator, which runs the agent-generated code modifications in a clean and equivalent containerized environment.This step verifies whether the code is executable and produces expected outputs.This executability metric is denoted as E. Implementation details including the system prompt are in App.H.</p>
<p>Main Results</p>
<p>. Table 1 presents average accuracy scores across all 461 tasks.I•E indicates whether an implementation is both appropriate for the experimental task and executable-a more comprehensive check of implementation quality.All✓ denotes tasks that are fully correct in terms of D, I, and C, while All•E✓ adds the executability requirement.#E represents the number of tasks per model that were execution-checked.Due to the time-consuming nature of execution, only a subset of traces were evaluated-excluding those that failed the monitor check, which were automatically discarded prior to execution.Our top-ranked agents are OH+o3-mini, OH+3.7 Sonnet, and OH+Nova Pro, ranked via All•E✓, with C used as a tiebreaker.The worst-performing model was IA+Nova Pro.Extended results by paper category are shown in Table 3, with full details in App.D. Across both tables, we observe that models consistently score below 30% across all metrics, with the exception of the RL category, where several OH models achieve up to ≈41% (averaged over 36 tasks) in terms of I. Notably, under stricter metrics such as All✓, performance drops sharply-e.g., OH+o3-mini scores only 1.4%.This underscores the value of including partial metrics that assess individual aspects, allowing credit for partially correct answers and supporting a more nuanced evaluation.</p>
<p>Detailed Analysis</p>
<p>Cost-Time Analysis.Fig. 6a shows the average cost (in USD) and time (in minutes) per task across different agent configurations.Cost reflects only the token usage of the backbone LLM (input/output), excluding agent internal LLM-API usage or compute consumption.The number in parentheses next to each legend entry indicates the model's performance rank, based on average correctness.Each agent was allowed a maximum of 40 minutes per task, though this limit can be easily adjusted.Notably, IA models often consumed the full allotted time, rarely stopping early.In contrast, early stopping was common with OH models.For example, the relative time difference between Nova and Haiku is larger under OH than IA, reflecting differing usage patterns.These trends are consistent with our earlier observations: OH models often produced plausible responses without actually running the experiment, leading to high partial scores (e.g., design, implementation), while IA models tended to run longer but less effectively.Interestingly, we found little correlation between runtime/cost and overall performance.OH+o3-mini (rank 1) achieves the best trade-off with low cost and moderate time.OH+3.7 Sonnet (rank 2) performs well but is the slowest and most expensive.The full cost-time distribution is provided in App.I.2.</p>
<p>Conjunctive Evaluation Metrics Substantially Lower Agent Scores.We analyze only the subset of tasks for which execution was run, to visualize how progressively applying stricter evaluation criteria impacts agent scores.As shown in Fig. 6b (with full results in App.I.1), applying only the initial monitoring check (M) yields an average score of 20.6%.Adding design (D) and conclusion (C) correctness criteria reduces the score sharply to 3.7%.Incorporating implementation correctness (I) further lowers the score to 0.4%, and including execution verification (E) results in a final accuracy of just 0.2%.These findings highlight how conjunctive evaluation surfaces brittleness in end-to-end experimental correctness.</p>
<p>Metric Stability Analysis.</p>
<p>As shown in Fig. 5, certain individual metrics such as C and E exhibit high variance.This variance arises for different reasons: for C, agents can produce plausible but unfounded conclusions without a valid experimental foundation; for E, even incorrect or mock implementations may successfully execute, introducing overestimation bias.To mitigate such inconsistencies, we adopt compositional scoring via conjunctive metrics such as C•D and I•E, which combine correctness across multiple dimensions.These conjunctive forms substantially reduce score variability, producing more reliable signals of agent performance.For example, C•D filters out conclusions not grounded in valid design plans, and I•E discounts executions that do not fulfill setup requirements.This demonstrates that conjunctive metrics can temper over-crediting and reduce sensitivity to annotation leniency or spurious correctness-thereby offering a more stable and discriminative evaluation.</p>
<p>Analysis on Prevalent Agent Failure Patterns</p>
<p>Pattern Extraction Methodology.Our analysis followed a two-pass, open-ended process.During evaluation, each metric score was accompanied by an error analysis, derived from implementation logs (e.g., stderr) or comparisons against ground truth.In the first pass, we extracted high-level, Average Cost ($)</p>
<p>(2)  domain-specific insights from these earlier error analyses, across phases for all agent-task pairs.In the second pass, we iteratively grouped these insights into distinct failure types-assigning each to an existing category or creating a new one if needed.This process produced 3,238 raw insights, which we distilled into 361 unique failure types.We present a representative and simplified subset of these condensed errors in Table 2 (full details can be found in App.G).
(3) (4)(5) (6) (7
Analysis.To better understand where agents fail, we analyzed error traces and categorized them into representative failure types across four key phases of experimentation: implementation, execution, design, and conclusion.As shown in Table 2, the most prevalent issues emerged during the implementation phase, with 39.71% of failures stemming from missing essential components.In several cases, agents failed to include critical elements such as semantic retrieval strategies (e.g., UniXcoder-H2L and UniXcoder-L2H), validation functions for filtering questions (e.g., using GPT-3.5),or robustness-enhancing techniques like Mixup, CutMix, and Label Smoothing-undermining the experimental implementation's validity.Incomplete data preprocessing (1.83%) was another notable implementation issue, with agents omitting required dataset loading and transformation steps, such as ETTh1 series preparation, ACF plotting, or normalization procedures (e.g., RevIN), instead providing only boilerplate config files.In the execution phase, failures were most commonly due to environment or dependency misconfigurations (29.38%), such as missing critical environments (e.g., STORM not registered in jaxmarl) or absent core libraries like PyTorch and Flax, which led to model loading failures.Script-level issues (23.84%)included unrecognized model names (e.g., moganet_tiny not found in timm) and missing checkpoint files, causing runtime or I/O errors.These examples highlight persistent reproducibility challenges even when a correct implementation structure is in place.Design-related failures were also frequent, with 16.05% involving incomplete or misclassified experimental variables, and 7.62% reflecting extraneous procedural additions-such as inclusion of a ResNet-50 backbone or arbitrary hyperparameter knobs not specified in the ground truth.These design errors suggest that agents often fail to distinguish between essential experimental factors and implementation noise.Finally, conclusion-phase errors highlight limitations in agents' interpretive reasoning.The most common issue (26.18%) was missing or underdeveloped conclusions-for instance, omitting detailed comparisons between PPO and Q-Learning on training time and normalized scores, or neglecting specific numerical gains (e.g., 1.25% improvements across ARC-Challenge and OpenBookQA).Another frequent error (19.66%) was incorrect interpretation, such as claiming Hadamard-enhanced INT4 inference improves performance without substantiating comparisons to baseline INT4.Together, these findings emphasize the importance of phase-specific evaluation and illustrate how surface-level plausibility can mask deeper breakdowns in experimental reasoning and reproducibility.</p>
<p>Discussion</p>
<p>Limitations.EXP-Bench primarily focuses on the experimentation procedure -from designing experiments for a given research question to deriving conclusions.The broader AI research lifecycle encompasses other critical stages such as identifying gaps through literature review, the initial unstructured ideation of research questions, and navigating the complex, iterative, and unpredictable path of real-world scientific discovery, which are not yet fully captured by the current task structures.</p>
<p>Conclusion</p>
<p>We introduced EXP-Bench, a novel benchmark designed to rigorously evaluate and guide the development of AI agents in conducting end-to-end AI research experimentation.By sourcing tasks from influential peer-reviewed publications and their accompanying codebases, and utilizing a semi-automated curation pipeline, EXP-Bench presents agents with realistic, fine-grained challenges in end-to-end AI research workflow including experimental design, implementation, execution, and conclusion derivation.Our initial evaluations with leading agents reveal significant bottlenecks in conceptualizing complex experiments and ensuring robust code implementation and execution.EXP-Bench therefore serves not only as a comprehensive evaluation tool but also as a valuable dataset to guide future AI agents to act step by step, ultimately accelerating AI research.</p>
<p>A Extended Related Works</p>
<p>LLMs for scientific discovery.Many methods have adopted LLMs to generate novel hypotheses for common scientific discovery.For example, Baek et al. [4], Wang et al. [84], and Yang et al. [103] developed approaches for generating innovative domain-specific research ideas.Going beyond domain-specific ideas, a line of work also focuses on generate hypothesis with LLMs in the commonsense domains [28,67,102,91,65,89,2,98,34,97].Moreover, prior research on automated scientific discovery proposes to combine hypothesis with LLM-assisted code generation for end-to-end workflows [50,42,64].While these efforts works on various stages of the scientific lifecycle, experimentation-a critical, rigor-sensitive aspect-remains underexplored.</p>
<p>Some existing research explores building an automated scientific discovery workflow with rigorous validation using AI agents [61,91,46,106,75,9,79,105,29], they often either have limited automated evaluation or rely on domain-specific ad-hoc prompting optimizations to guide predefined workflows, struggling with the complexities of rigorous end-to-end experimentation to automate AI research.Particularly, Lu et al. [61] introduced a fully automated system called "The AI Scientist" to conduct research by collaborating with multiple LLM agents.These agents handle the full research process, from defining research problems and reviewing related literature to synthesizing and executing experiments.However, their solution has limited automated evaluation with a focus on commonsense domains.Gottweis et al. [91] proposed an AI Co-scientist built on Gemini 2.0, aiming at building a helpful AI collaborator for scientists.They focus on the scaling of the testtime compute paradigm to generate high-quality hypotheses and research proposals.While general purpose, the AI co-scientist is mainly validated in biomedical areas.Overall, these efforts often require experimental validation to follow constrained, framework-specific formats, resulting in extra overhead and hindering their usability.</p>
<p>Benchmarks for domain-specific AI agent tasks.A wide range of benchmarks have been developed to evaluate the capabilities of AI agents across diverse domains.Existing benchmarks predominantly target problem-solving [36,26,86,77,17], logical reasoning [18,35,5,48], machine learning training [40,109,108,30,82,66,68,31,32], and knowledge retrieval and analysis [78,37].These benchmarks typically involve well-defined tasks with clear, deterministic solutions, allowing for consistent and objective assessment of AI agent performance.By contrast, our proposed EXP-Bench focuses on experimentation for automating AI research, which requires a more rigorous and systematic approach beyond problem-solving.Experimental tasks demand iterative hypothesis refinement, complex experiment design/implementation and execution, and rigorous result interpretation.Our benchmark captures these challenges by semi-automatically evaluating AI agents on real-world experimentation tasks arising from influential AI research papers with high-impact open-source artifacts.</p>
<p>B Extended Details of the EXP-Bench Dataset</p>
<p>In this section, we provide a full list of the papers in the EXP-Bench dataset, including source paper and, AI sub-domain.The complete dataset can be found in our HuggingFace repository https://huggingface.co/datasets/Just-Curieous/EXP-Bench.</p>
<p>E Extended EXP-Bench Examples</p>
<p>This section presents two extended examples from EXP-Bench: a question concerning robust detection in collaborative perception under imperfect localization, and a question focused on implementing a Time Delay Neural Network (TDNN) for automatic speech recognition.Each example details the experiment's objective, methodology, relevant source code, and expected outcomes.The examples also include an analysis of agent performance on completing the task.</p>
<p>E.1 Example 1: Robust Detection in Collaborative Perception</p>
<p>This example question was extended from the paper An Extensible Framework for Open Heterogeneous Collaborative Perception [62].</p>
<p>The objective of this experiment is to assess whether HEAL (HEterogeneous ALliance) can maintain robust detection performance under realistic conditions of imperfect localization, when Gaussian noise is added to the agents' poses.The experiment maintains constant variables such as the dataset (OPV2V-H) and the model architecture (HEAL).The independent variables are the position noise and rotation noise, while the dependent variable is the model's detection performance matrices (AP30, AP50, and AP70).Experimental groups test the addition of Gaussian rotation and position noise at levels of 0, 0.2, 0.4, and 0.6 meters/degrees to accurate poses.The results will contribute to evaluating the robustness of a cooperative perception model under conditions of imperfect localization.EXP-Bench extends this task from the original paper section 5.3 QUANTITATIVE RESULTS and utilizes the source code: /workspace/opencood/tools/inference_w_noise.py from the GitHub repository https://github.com/yifanlu0227/HEAL.Note that /workspace/ refers to the working directory from the agent's initialization context.</p>
<p>The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome.This is illustrated in Fig. 7a.</p>
<p>The agent's task is to use the provided GitHub repository, with the source code masked, to conduct this experiment.To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 7b.</p>
<p>Evaluation of the task includes design, conclusion, and setup evaluation.The conclusion appears in the 'expected outcome' in Fig. 7a.Design and setup evaluation are based on 'design complexity' and 'requirements' respectively, shown in Fig 8.</p>
<p>An example agent output using the bedrock-us-anthropic-claude-3-7-sonnet-20250219-v1-0 LLM as a backbone is showcased here.We perform a diff operation between the code generated by the agent and the original source code.As this agent reconstructs several files to fulfil the task requirement, we focus on a diff operation between the core reconstructed file (evaluate_robustness.py) and the source file (inference_w_noise.py),shown in Fig. 9.The two files share the same functional goal and have a similar overall structure; however, the agent performs invalid operations in another file (reproduce_exp_bench.sh), leading to a failure in completing the task.The detailed reasoning provided by the judge is illustrated in Fig. 10.</p>
<p>E.2 Example 2: Time Delay Neural Network for ASR</p>
<p>This example is extended from the paper Zipformer: A Faster and Better Encoder for Automatic Speech Recognition [104].</p>
<p>The objective of this experiment is to implement a Time Delay Neural Network (TDNN) that achieves a Word Error Rate (WER) of less than 1% on the test set.This setup focuses on constructing a TDNN model with three Conv1d layers-each followed by ReLU activation and Batch Normalization-and a final linear layer to produce log probabilities for phoneme classes.The general formulation of the task includes the question posted to the agent, the overall method of the experiment, the source of this question (specifically the section in the paper and the source code), and the expected outcome.This is illustrated in Fig. 11a.</p>
<p>Again, the agent's task is to use the provided GitHub repository, with the source code masked, to conduct this experiment.To aid the agent in reconstructing the masked file, detailed instructions are provided, as shown in Fig. 11b.</p>
<p>Similarly, evaluation of the task includes design, conclusion, and setup evaluation.The conclusion appears in the 'expected outcome' in Fig. 11a.Design and setup evaluation are based on 'design complexity' and 'requirement' respectively, shown in Fig 12.</p>
<p>For the agent performance in this example, we make use of an agent's output using the bedrock-usamazon-nova-pro-v1-0 LLM backbone.We perform a diff operation between the code generated by the agent and the original implementation files provided in the baseline.Since the agent restructures multiple modules to accomplish the speech recognition task, our analysis focuses on a diff between the core model implementation file (model.py)and the original reference.The agent correctly builds the TDNN model and integrates it with the training and decoding pipeline.The two versions of model files share a similar architectural skeleton, but differ in details such as layer configuration and parameter initialisation.The differences are shown in Fig. 13.</p>
<p>F Dataset Curation and Benchmark Construction Details</p>
<p>To ensure the quality and integrity of EXP-Bench, we developed the curation pipeline through a careful, iterative process.Each component was prototyped, tested on real papers, and refined based on manual inspection by collaborators.This allowed us to isolate and address specific failure modes incrementally, steadily increasing curation throughput without compromising accuracy.Several representative issues that were patched in our final pipeline are documented in Table .7. Manual validation was also aided by the availability of ground truth from the papers and open-source code repositories themselves, making the verification process straightforward.The EXP-Bench team is committed to the long-term maintenance and growth of the dataset.We actively monitor feedback and bug reports via GitHub and HuggingFace issue trackers and will address any concerns raised by the community post-release.All data is hosted on both platforms to ensure accessibility and stability, with potential plans to replicate the dataset on archival storage for long-term preservation.To foster transparency, reuse, and critical engagement, the dataset is released under the permissive Creative Commons Attribution 4.0 license, and all code under the MIT license.We encourage the community to explore, build upon, and challenge EXP-Bench as an open and evolving resource.</p>
<p>G Extended Analysis on Prevalent Agent Failure Patterns</p>
<p>Some overlap between categories may exist, as the classification was performed by an LLM.</p>
<p>H Evaluation Judge</p>
<p>As detailed in the main text ( §4.1), our evaluation framework leverages an LLM-based judge (using o3-mini-2025-01-01-preview) to assess various aspects of agent performance, including adherence to integrity guidelines, experimental design, implementation correctness, and conclusion validity.We provide the full system prompts used by our LLM judge in this section.</p>
<p>When we encounter long inputs such as Git diffs and agent logs, the content can sometimes exceed the context window of the o3-mini model.To address this, we segment the input into individual chunks that fit within the model's context length.These chunks are then fed iteratively to the judge, with the evaluation results and relevant context from one chunk carried over to the subsequent chunk.This iterative processing ensures that the entire input is evaluated comprehensively without loss of information due to context length limitations.</p>
<p>Monitor Prompt.The initial integrity check, performed by the Monitor, uses the following prompt to analyze agent logs for disallowed behaviors (e.g., accessing the research paper directly, performing Git operations, or using hardcoded data).</p>
<p>Inspect the log to verify the following criteria:</p>
<p>1.The agent did NOT access (i.e., read) the research paper itself (e.g., the PDF).2. The agent did NOT perform any Git operations, such as checking out commits, switching branches, or accessing other versions of the repository.3. The agent did NOT use fake, hardcoded, or mock data in place of real experimental results.In particular, check for any signs of: -Hardcoded or fabricated data values used as results.</p>
<p>-Use of mock or placeholder data instead of running actual experiments.</p>
<p>-Directly copying or using result data already present in the repository, rather than generating it through experimentation.</p>
<p>-Any indication that crucial data for the experiment was not produced by creating and running the intended experimental task.</p>
<p>-If you find evidence that the agent faked data or failed to run the experiment, provide the reason why this happened if possible.For example, did the agent or LLM (such as nova pro) refuse to perform the task due to ethical concerns, security restrictions, or because the task involved potentially malicious actions?If the log contains an explicit refusal, error message, or explanation for why the agent faked or could not generate the data, include this in your reasoning.</p>
<p>Output format exactly as this JSON:</p>
<p>{ "paper_access": <boolean>, "git_operations": <boolean>, "faked_or_nonexperimental_data": <boolean>, "setup_monitor_comprehensive_reason": "<comprehensive explanation for your decisions on the above keys, including specific evidence from the log and your reasoning process.Clearly justify each decision and reference relevant log entries or the absence thereof.If you found evidence for any key being True, explain exactly what that evidence was and why it led to your conclusion.If you found no evidence, explain how you searched and why you are confident in your answer.If you found evidence that the agent faked data or failed to run the experiment, also explain the reason for this only if possible (e.g., LLM refusal, security/ethical block, or other failure), citing the relevant log entries.>"} Explanation of output keys: -"paper_access": True if the agent accessed the research paper, False otherwise.</p>
<p>-"git_operations": True if the agent performed any Git operations, False otherwise.-"faked_or_nonexperimental_data": True if there is any evidence of fake, hardcoded, mock, or reused (non-experimental) data being used for crucial results, False otherwise.-"setup_monitor_comprehensive_reason": A comprehensive explanation describing the evidence and reasoning for the above decisions, referencing specific log entries or the absence of evidence as appropriate.</p>
<p>Here is the log: {logs}</p>
<p>Design and Conclusion Evaluation Prompt.Once integrity is established, the agent's experimental design and conclusion are evaluated for conceptual soundness, completeness, and alignment with ground truth.This assessment contributes to the D (design correctness) and C (conclusion correctness) metrics.</p>
<p>You are a judge tasked to evaluate a system's output against ground truth answers for an experimental design task.</p>
<p>Input fields:</p>
<p>-design_ground_truth: the correct list of variables (constants, independent, dependent variables).-conclusion_ground_truth: the correct conclusion as a string.</p>
<p>-design_output: the predicted design.It may not be formatted as a list;</p>
<p>extract and match relevant variable information from its content.-conclusion_output: the predicted conclusion string.</p>
<p>Evaluation Instructions: -Design Evaluation: Compare design_output to design_ground_truth. Count how many items in design_output match items in design_ground_truth.Return the percentage of correct items as an integer (e.g., use 75 to represent 75%), along with a short explanation.If applicable, include a failure analysis on what the system got wrong.</p>
<p>-Conclusion Evaluation: Compare conclusion_output to conclusion_ground_truth.Return "correct" or "incorrect" based on semantic match, along with a short explanation.If applicable, include a failure analysis on what the system got wrong.</p>
<p>Here is the input:</p>
<p>I Additional Analysis</p>
<p>We include detailed breakdowns of the analysis performed in §4.2.</p>
<p>I.1 Conjunctive Evaluation Metrics Analysis</p>
<p>In Fig. 14, we include details for all agents and models evaluated, as opposed to the subset in Fig. 6b.</p>
<p>I.2 Cost-Time Distribution</p>
<p>We showcase the full cost-time distribution in Table.9 in the form of summary statistics.For timerelated statistics, although a soft timeout of 40 minutes was enforced during trials, agents occasionally exceeded this limit due to non-compliance with the timeout mechanism.Additionally, both time and cost values can appear unusually low in cases where the agent failed to complete the experiment.</p>
<p>convolution improves... train.pyrun_exp.shdef forward(self, ...): -x = self.conv(x)+ x = [conv(x) for conv in self.conv_list]+ x = torch.cat(x,dim=1)</p>
<p>DFigure 3 :
3
Figure 3: EXP-Bench's dataset comprises tasks from a diverse set of ML research categories.</p>
<p>Figure 4 :
4
Figure 4: EXP-Bench semi-automated dataset construction pipeline.</p>
<p>Figure 5 :
5
Figure 5: Stability Analysis.</p>
<p>Stricter metrics reveal lower true correctness.</p>
<p>Figure 6 :
6
Figure 6: Ablation of agent performance along cost-time and evaluation metrics.</p>
<p>(a) The formulation of the task question.(b) Instructions provided to the agent.</p>
<p>Figure 7 :
7
Figure 7: Task Fields for Example 1.</p>
<p>Figure 8 :
8
Figure 8: Evaluation of the design and setup for the Extended Task in Example 1.</p>
<p>Figure 9 :
9
Figure 9: Example 1's Git diff comparing the masked source file and the agent-reconstructed source code.Red highlights indicate deletions, while green highlights represent additions.</p>
<p>Figure 10 :
10
Figure 10: Error Analysis and Comprehensive Explanation of the agent's failure to complete the task in Example 1.</p>
<p>Figure 12 :
12
Figure 12: The design and setup evaluation of the extended task in Example 2.</p>
<p>Figure 13 :
13
Figure 13: Example 2's Git diff of the masked source file and the agent reconstructed source code.In the diff, red highlights are deletions.Green highlights are additions.</p>
<p>Figure 14 :
14
Figure 14: Stricter metrics reveal lower true correctness.Table 9: Cost-time summary statistics for all evaluated agents and models.OH = OpenHands, IA = IterativeAgent.Med = median, Std = standard deviation, T = time (minutes), C = cost (USD).Agent Model Avg T Med T Q1 T Q3 T Std T Min T Max T OH o3-mini 24.89 23.24 13.93 33.60 16.74 1.70 47.72 OH 3.7 Sonnet 33.53 29.64 16.67 37.72 10.03 2.55 74.04 OH Nova Pro 17.82 15.03 11.85 24.06 9.37 0.64 74.33 OH 3.5 Haiku 25.17 24.23 13.26 32.72 17.38 1.21 37.85 OH DeepSeek R1 32.24 31.40 19.09 38.69 11.82 0.97 60.77 IA 3.5 Haiku 30.24 26.13 19.63 38.24 54.62 0.30 402.84 IA Nova Pro 30.09 26.31 19.51 38.09 27.61 0.17 360.52 Agent Model Avg C Med C Q1 C Q3 C Std C Min C Max C OH o3-mini 0.55 0.35 0.17 1.11 0.56 0.01 1.34 OH 3.7 Sonnet 10.15 7.53 3.04 14.20 6.30 0.03 19.83 OH Nova Pro 1.09 0.77 0.33 2.18 0.93 0.00 2.99 OH 3.5 Haiku 0.68 0.42 0.15 2.68 1.47 0.01 3.24 OH DeepSeek R1 1.55 1.28 0.83 2.49 1.70 0.00 4.08 IA 3.5 Haiku 2.82 1.86 0.52 4.23 2.90 0.02 5.09 IA Nova Pro 3.93 3.26 0.91 5.31 3.65 0.02 6.96</p>
<p>Table 1 :
1
Average benchmark scores for various models when tested against various evaluation metrics.Popular Agents and LLMs perform poorly on EXP-Bench, showcasing its difficulty.
AgentModelDIEI•ECAll✓ All•E✓ #EOpenHandso3-mini18.4 20.3 15.0 2.9 21.01.40.5OpenHandsClaude-3.7 Sonnet 16.0 35.0 33.2 14.9 13.40.70.4OpenHandsAmazon Nova Pro 18.2 19.5 26.8 0.0 15.70.00.056OpenHandsClaude-3.5 Haiku20.6 26.2 9.31.3 13.80.00.0OpenHandsDeepSeek R16.8 10.0 0.70.02.40.00.0IterativeAgent Claude-3.5 Haiku6.4 20.6 25.2 5.42.20.00.0IterativeAgent Amazon Nova Pro0.1 10.0 18.1 0.00.30.00.0research objective faithfully aligns with the source paper. In all cases, a lightweight human reviewfinalizes the task, requiring only a cross-check of structured task content-already consolidated bythe pipeline-against the source materials. This significantly reduces human burden compared tomanual curation from scratch. Following validation, each complete task is added to the dataset alongwith a list of masked files (e.g., README.md, relevant scripts) to ensure agents cannot directly accessanswers. In our benchmark implementation, repositories are cloned afresh per agent, and masking isapplied using scripted git operations, including recursive traversal of submodules. Masking ensuresagents must reason over the task input, rather than rely on shortcut access to original solutions.4 Evaluation4.1 Evaluating LLM-based Agents Performance on EXP-Bench: Setup &amp; Main Results</p>
<p>Table 2 :
2
Agents fail in diverse ways across different phases of experimentation; this table presents a simplified subset of common examples, measured across all agent and model evaluations.
PhaseFailure TypePrevalence (%)DesignIncomplete or Misclassified Design Variables16.05DesignIrrelevant Procedural Additions in Design7.62Implementation Missing Essential implementation Components39.71Implementation Incomplete Evaluation Metric Implementation2.15ImplementationIncomplete Data and Preprocessing Setup1.83ExecutionEnvironment/Dependency Configuration Errors29.38ExecutionExecution Script and File Errors23.84ExecutionMissing Setup Script File6.95ExecutionTensor Operation Execution Error3.22ConclusionMissing Conclusion Content26.18ConclusionIncorrect Conclusion Interpretation19.66ConclusionExtraneous Details in Conclusion7.77ConclusionIncorrect Numeric Conclusion3.21</p>
<p>Table 3 :
3
Average benchmark scores of various models and agents across select task categories; see Supp.D for complete list.Evaluation performed against EXP-Bench.Future
CategoryAgentModelDIEI•ECAll✓ All✓•EApplicationsOHNova Pro19.2 23.9 19.0 0.0 13.90.00.0ApplicationsOHo3-mini9.08.00.00.08.30.00.0ApplicationsOH3.5 Haiku19.2 24.5 8.35.68.30.00.0ApplicationsOH3.7 Sonnet9.0 26.8 30.8 7.78.32.80.0ApplicationsIANova Pro0.09.85.00.00.00.00.0ApplicationsIA3.5 Haiku0.0 18.0 0.00.00.00.00.0ApplicationsOHDeepSeek R1 3.14.30.00.00.00.00.0RLOH3.7 Sonnet18.3 48.2 27.3 21.2 17.62.03.0RLOHo3-mini23.5 34.8 15.7 2.0 27.53.90.0RLOH3.5 Haiku27.7 41.4 11.5 0.0 17.60.00.0RLIA3.5 Haiku3.3 27.5 17.4 0.02.40.00.0RLOHDeepSeek R1 5.0 10.3 0.00.02.00.00.0RLIANova Pro0.08.69.70.00.00.00.0RLOHNova Pro17.9 28.9 0.00.0 13.70.00.0
directions.Future work will focus on enhancing AI agents' ability to automate research experimentation using supervision from EXP-Bench's dataset.One promising direction is to apply reinforcement learning with verifiable rewards, enabling agents to autonomously navigate the research lifecycle and accelerate scientific discovery.</p>
<p>Table 4 :
4
ICLR 2024 Papers
IDTitleStarsCit.#DomainKey Dist.ResourceT1 T2 T319292Zipformer: A faster102397Deep Learning →proposememory needed:133and better encoder forAttentionan archi-32GB or moreautomatic speechMechanismstecturerecommended,recognition[104]GPU type:NVIDIA V100 orA100, GPUamount: 2-819033The Reversal Curse:284179Deep Learning →proposeOpenAI API key300LLMs trained on "A isLarge Languagean archi-required; GPU: 1;B" fail to learn "B isModelstecturememory: ≥16GBA" [6]RAM</p>
<p>Table 5 :
5
NeurIPS 2024 PapersThe advancement of AI agents capable of conducting AI research, as facilitated by benchmarks like EXP-Bench, offers positive societal impacts.It might significantly shorten innovation cycles within AI itself and lead to more rapid advancements in machine learning capabilities.While a faster pace of AI development can also democratize research tools and improve overall scientific efficiency, it concurrently amplifies the importance of addressing potential negative societal consequences.On the other hand, the rapid evolution of AI capabilities heightens risks, where we need to be careful about potential misuse, algorithmic bias, and the evolving role of human researchers, alongside the development of robust governance.D Average Scores acrossAll Paper Categories Defintions.Comp.Biology refers to Computational Biology.CV refers to Computer Vision.D &amp; B refers to Datasets and Benchmarks.Gen. Models refers to Generative Models.Proba.Methods refers to Probabilistic Methods.RL refers to Reinforcement Learning.Addendum.Table.6 contains updated values for IA+3.5 Haiku for the Applications and Reinforcement Learning categories.
IDTitleStarsCit.#DomainKey Dist.ResourceT1 T2 T393022Generative Modeling14413Generative ModelsproposeGPUs not specified,80of Molecular→ New Approachesan archi-but PyTorch andDynamicstecturerelated librariesTrajectories[44]suggest a need for aCUDA-compatibleGPU. Memoryrequirementsunspecified.93431Trace is the Next4929Optimization →proposememory needed: 863AutoDiff: GenerativeGenerative Modelsan archi-GB RAMOptimization withtectureminimum, OpenAIRich Feedback,API key required,Execution Traces, andGPU: 1 x NVIDIALLMs[15]GPUrecommended,98316Causal-learn: Causal128796Causalityproposememory needed:35Discovery inan archi-Standard (dependsPython[112]tectureon the dataset),GPU: Not required,953333DGS-Enhancer:17016Computer Vision →proposememory needed:32EnhancingVideo Generationan archi-16GB, Yes, GPUUnbounded 3Dtecturetype: NVIDIA,Gaussian SplattingGPU amount: 1,with View-consistent2D DiffusionPriors[59]98326TorchOpt: An57017Optimization →proposememory needed:24Efficient Library forZero-order andan archi-At least 8GB RAM,DifferentiableBlack-boxtectureGPU type:Optimization[73]OptimizationNVIDIA, GPUamount: 1,98318BenchMARL:35130Reinforcementpropose amemory needed:44BenchmarkingLearning →datasetAt least 8GB RAMMulti-AgentMulti-agentrecommended,ReinforcementGPU: 1x NVIDIALearning[7]GPU (e.g., GTX1080 or better),</p>
<p>Table 6 :
6
Average benchmark scores of various models and agents across select task categories.Evaluation performed against EXP-Bench.
CategoryAgentModelDIEI•ECAll✓ All✓•EApplicationsOHNova Pro19.2 23.9 19.00.013.90.00.0ApplicationsOHo3-mini9.08.00.00.08.30.00.0ApplicationsOH3.5 Haiku19.2 24.58.35.68.30.00.0ApplicationsOH3.7 Sonnet9.0 26.8 30.87.78.32.80.0ApplicationsIANova Pro0.09.85.00.00.00.00.0ApplicationsIA3.5 Haiku6.8 32.3 33.316.70.00.00.0ApplicationsOHDeepSeek R1 3.14.30.00.00.00.00.0CausalityOHo3-mini37.8 44.6 22.211.1 44.4 11.111.1CausalityOH3.7 Sonnet36.6 83.1 88.966.7 44.40.00.0CausalityIA3.5 Haiku23.1 40.6 40.00.020.00.00.0CausalityIANova Pro0.0 11.7 20.00.00.00.00.0CausalityOH3.5 Haiku48.7 36.60.00.033.30.00.0CausalityOHNova Pro17.7 14.90.00.011.10.00.0CausalityOHDeepSeek R1 10.0 18.70.00.00.00.00.0Comp. BiologyOHo3-mini37.0 30.3 11.10.044.4 11.10.0Comp. BiologyIANova Pro0.0 16.7 20.00.00.00.00.0Comp. BiologyIA3.5 Haiku3.27.80.00.00.00.00.0Comp. BiologyOHNova Pro31.2 29.30.00.033.30.00.0Comp. BiologyOH3.5 Haiku4.89.70.00.011.10.00.0Comp. BiologyOH3.7 Sonnet4.8 11.10.00.011.10.00.0Comp. BiologyOHDeepSeek R1 12.2 22.10.00.00.00.00.0CVOHNova Pro24.8 20.9 42.90.028.20.00.0CVOH3.7 Sonnet18.5 28.9 18.29.121.20.00.0CVOHo3-mini11.5 9.35.12.612.80.00.0CVOH3.5 Haiku15.9 28.34.50.012.80.00.0CVOHDeepSeek R1 5.8 11.20.00.02.60.00.0CVIANova Pro0.05.328.60.00.00.00.0CVIA3.5 Haiku6.</p>
<p>The dataset (yesno), model type (TDNN), loss function (Connectionist temporal classification), and feature extraction method (23-dimensional fbank features) are held constant.Independent variables include the model architecture, training hyperparameters (e.g., learning rate, weight decay), and number of epochs, while the dependent variable is the WER obtained during evaluation.This task emphasizes practical training and decoding using k2's one_best_decoding method and evaluates performance using the WER metric, targeting values below 1%.EXP-Bench extends this task beyond the baseline speech recognition example by formalizing an end-to-end pipeline using code modules: /workspace/egs/yesno/ASR/tdnn/model.py,train.py,decode.py,and asr_datamodule.pyfrom the Github repository https://github.com/k2-fsa/icefall.</p>
<p>Table 7 :
7
Examples of extraction issues identified that were subsequently patched in the final pipeline.Time and Cost Expenditure.During the initial phases-before our curation pipeline was finalized-each paper required roughly two hours of manual effort.This involved a full read-through (with emphasis on evaluation sections), task-by-task verification, and iterative pipeline corrections to ensure compatibility.The process included checking GitHub repositories, assessing setup validity and complexity, and verifying alignment with the paper's descriptions.Once the pipeline was fully constructed and refined based on feedback, manual validation time dropped to around 20 minutes per paper, primarily to confirm alignment.Only minor adjustments were rarely needed, and we expect this time to decrease further in future deployments.LLM-related extraction costs varied by task type and count, averaging approximately $60 USD per paper.For extraction, we used o3-mini-2025-01-01-preview for the main task extraction and claude-3-7-sonnet-20250219-v1:0 for implementation extraction.Costs were primarily driven by input tokens, as the models required full paper texts and codebases to perform accurate extraction.
Task Component IssueActual ExampleQuestionThe hypothesis is a statement instead of a questionBaDExpert outperforms baseline defenses in backdoor detection on CIFAR10, achievingsignificantly higher AUROC (near 99%).Conclusion data mentioned inSpecifically, can PDF achieve around 34.64%the hypothesislower MACs compared to PatchTST and74.38% lower MACs ...?Masked source doesn't exist"source": ["/workspace/-Masked sourcetopomlp_setA_r50_w_otransform.py"...]Included masked source withMuSc has musc.py under workspace/model/wrong pathbut the source file indicates it underworkspace/example/Steps are too specificRun the evaluation script for the baselineRequirementsEVA-CLIP ViT-B/16 model using distributed processing with 8 GPUs...Asking the agent to use aMerge the trained models using themasked source scriptheal_tools.py script(/workspace/opencood/tools/heal_tools.py:115-130)Invalid operationAnalyze execution outcomes from Table 4,comparing...ExpectedConclusion not aligned with theN/Aoutcomepaper's findingsMethod / UsageMentioned specific parts of theThe scripts will log metrics including meanInstruction /paper (tables or figures)rewards and standard deviations, which canAgent Instructionbe compared with the reported results inTable 2 of the paper.Required hyperparameters notSet appropriate model architecturegiven in the agent instructionparameters (encoder layers, attention heads,dimensions)Invalid operationsCollect and analyze performance results fromTable 3, ...</p>
<p>Table 8 :
8
Agents fail in diverse ways across different phases of experimentation, measured across all agent and model evaluations.
PhaseFailure TypePrevalence (%)conclusionMissing Conclusion Content26.18conclusionIncorrect Conclusion Interpretation19.66conclusionIncomplete Conclusion Outcome Statement14.43conclusionExtraneous Details7.77conclusionMissing Conclusion Analysis4.35conclusionMissing Comparative Conclusion Analysis4.03conclusionMinor Omission of Specific Details3.47conclusionIncorrect Numeric Conclusion3.21conclusionMismatched Conclusion Format2.7conclusionError Message Output2.67conclusionIncomplete Conclusion with Missing Exp. Findings2.14conclusionConclusion Diverges from Expected Emphasis1.6conclusionMissing Comparative Analysis0.8conclusionMissing Quantitative Performance Metrics0.8conclusionMissing Visualization Details0.56conclusionIncomplete Performance Evaluation0.53conclusionMissing Numerical Equivalence Verification0.53conclusionMissing Trend Analysis0.53conclusionNaming Inconsistency Output0.53conclusionConclusion Partially Matching with Numerical Deviations0.27conclusionDeviation in Saturation Point Conclusion0.27conclusionInconsistent ASR Reporting0.27conclusionMissing Conclusion Analysis on Attack Budget Effects0.27conclusionMissing Diminishing Returns Analysis0.27conclusionMissing Methodological Innovation Discussion0.27conclusionMissing Performance Evaluation Metrics0.27conclusionMissing Submission Format Specification0.27designIncomplete or Misclassified Design Variables16.05designOmission of Required Design Variables19.84designComplete Omission of Exp. Design Variables13.1designIncorrect Design Specification Details8.32designIncomplete Exp. Design Details7.67designIrrelevant Procedural Additions7.62designMissing Design Variable Information3.83designInclusion of Extraneous Factors3.64designIncorrect Parameter Details3.18designPartial Omission of Constant Variables2.75designIncomplete Constant Variable Specification3.61designPartial Fulfillment of DV1.93designError Message Returned Instead of Design Information1.27designIncomplete Differentiation of Constant and Ind. Variables1.27designMissing Dependent Variable Tracking1.06designIncomplete Exp. Design Specification0.64designIncomplete Specification of Design Variables0.64designMissing Hyperparameter Design Details0.64designPartially Complete Design Variable Specification0.64designMissing Design Formatting Details0.42designMissing Design Variables Details0.42designMissing Explicit Variable Labeling0.42designMissing Configuration File Variable0.21designMissing Input Format Details0.21
Preprint. Under review.
(a) The formulation of the task question.(b) Instructions provided to the agent.Figure 11: Task fields for Example 2.
{{ "design_evaluation_explanation": "<short explanation string>", "design_score": <integer from 0 to 100>, "design_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>", "conclusion_evaluation_explanation": "<short explanation string>", "conclusion_score": "<correct/incorrect>", "conclusion_error_analysis": "<short explanation of what was wrong with the output, i.e., what the system failed at, if applicable>" }} Implementation Evaluation Prompt.The agent's implementation is assessed by comparing the ground truth requirements against the Git diff generated by the agent.This evaluation contributes to the I (implementation correctness) metric.You are a judge tasked to evaluate a system's experiment setup against ground truth requirements.Input fields:-setup_ground_truth: the correct experiment setup requirements, given as either a list of step-by-step required actions/configs or a natural language description.-setup_ground_truth_scripts: Source scripts that implement the ground truth setup.These may not match the setup_output exactly, but serve as code-level references for what correct setups may look like.-setup_output: the system's actual changes, given as a Git diff patch (e.g., modifications to config files, scripts, etc.).-A score as an integer percentage (e.g., 80 for 80%) representing how many ground truth setup requirements were correctly implemented.-A detailed explanation of the evaluation result.-If applicable, include a failure analysis of what requirements were missed or incorrectly implemented.Here is the input: { "setup_ground_truth": {setup_gt}, "setup_ground_truth_scripts": {setup_scripts} "setup_output": {setup_output}, } Output format exactly as this JSON: { "setup_evaluation_explanation": "<detailed explanation string>", "setup_score": <integer from 0 to 100>, "setup_error_analysis": "<Explanation of what was wrong with the setup, i.e., what requirements were missed or done incorrectly, if applicable>" }
Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. </p>
<p>A large-scale benchmark for few-shot program induction and synthesis. F Alet, J Lopez-Contreras, J Koppel, M Nye, A Solar-Lezama, T Lozano-Perez, L Kaelbling, J Tenenbaum, International Conference on Machine Learning. PMLR2021</p>
<p>Quarot: Outlier-free 4-bit inference in rotated llms. S Ashkboos, A Mohtashami, M L Croci, B Li, P Cameron, M Jaggi, D Alistarh, T Hoefler, J Hensman, 2024</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, Q V Do, Y Xu, P Fung, 2023</p>
<p>The reversal curse: Llms trained on. L Berglund, M Tong, M Kaufmann, M Balesni, A C Stickland, T Korbak, O Evans, a is b"" fail to learn ""b is a"", 2024</p>
<p>Benchmarl: Benchmarking multi-agent reinforcement learning. M Bettini, A Prorok, V Moens, 2024</p>
<p>J Blasiok, P Nakkiran, Smooth ece: Principled reliability diagrams via kernel smoothing. 2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks. L Boisvert, M Thakkar, M Gasse, M Caccia, T L S D Chezelles, Q Cappart, N Chapados, A Lacoste, A Drouin, 2025</p>
<p>Torchrl: A data-driven decision-making library for pytorch. A Bou, M Bettini, S Dittert, V Kumar, S Sodhani, X Yang, G D Fabritiis, V Moens, 2023</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. J S Chan, N Chowdhury, O Jaffe, J Aung, D Sherburn, E Mays, G Starace, K Liu, L Maksin, T Patwardhan, L Weng, A Ądry, 2024</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Z Chen, S Chen, Y Ning, Q Zhang, B Wang, B Yu, Y Li, Z Liao, C Wei, Z Lu, arXiv:2410.050802024arXiv preprint</p>
<p>Trace is the next autodiff: Generative optimization with rich feedback, execution traces, and llms. C.-A Cheng, A Nie, A Swaminathan, 2024</p>
<p>Self-playing adversarial language game enhances llm reasoning. P Cheng, T Hu, H Xu, Z Zhang, Z Yuan, Y Dai, L Han, N Du, X Li, 2025</p>
<p>Language models as science tutors. A Chevalier, J Geng, A Wettig, H Chen, S Mizera, T Annala, M J Aragon, A R Fanlo, S Frieder, S Machado, A Prabhakar, E Thieu, J T Wang, Z Wang, X Wu, M Xia, W Xia, J Yu, J.-J Zhu, Z J Ren, S Arora, D Chen, 2024</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, Training verifiers to solve math word problems. 2021</p>
<p>T M U Collaboration, J Audenaert, M Bowles, B M Boyd, D Chemaly, B Cherinka, I Ciucȃ, M Cranmer, A Do, M Grayling, E E Hayes, T Hehir, S Ho, M Huertas-Company, K G Iyer, M Jablonska, F Lanusse, H W Leung, K Mandel, J R Martínez-Galarza, P Melchior, L Meyer, L H Parker, H Qu, J Shen, M J Smith, C Stone, M Walmsley, J F Wu, The multimodal universe: Enabling large-scale machine learning with 100tb of astronomical scientific data. 2024</p>
<p>J Dai, X Pan, R Sun, J Ji, X Xu, M Liu, Y Wang, Y Yang, Safe rlhf: Safe reinforcement learning from human feedback. 2023</p>
<p>Periodicity decoupling framework for long-term series forecasting. T Dai, B Wu, P Liu, N Li, J Bao, Y Jiang, S.-T Xia, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Segvol: Universal and interactive volumetric medical image segmentation. Y Du, F Bai, T Huang, B Zhao, 2025</p>
<p>Elicit: Analyze research papers at superhuman speed. 2025</p>
<p>Drivaernet++: A large-scale multimodal car dataset with computational fluid dynamics simulations and deep learning benchmarks. M Elrefaie, F Morar, A Dai, F Ahmed, 2025</p>
<p>Domain-agnostic molecular generation with chemical feedback. Y Fang, N Zhang, Z Chen, L Guo, X Fan, H Chen, 2024</p>
<p>Mathematical capabilities of chatgpt. S Frieder, L Pinchetti, R.-R Griffiths, T Salvatori, T Lukasiewicz, P Petersen, J Berner, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Boxinggym: Benchmarking progress in automated experimental design and model discovery. K Gandhi, M Y Li, L Goodyear, L Li, A Bhaskar, M Zaman, N D Goodman, 2025</p>
<p>Large language models are not strong abstract reasoners. G Gendron, Q Bao, M Witbrock, G Dobbie, arXiv:2305.195552023arXiv preprint</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, 2024</p>
<p>Large language models orchestrating structured reasoning achieve kaggle grandmaster level. A Grosnit, A Maraval, J Doran, G Paolo, A Thomas, R S H N Beevi, J Gonzalez, K Khandelwal, I Iacobacci, A Benechehab, arXiv:2411.035622024arXiv preprint</p>
<p>K Gu, R Shang, R Jiang, K Kuang, R.-J Lin, D Lyu, Y Mao, Y Pan, T Wu, J Yu, arXiv:2408.09667Benchmarking language model agents for data-driven science. 2024arXiv preprint</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. S Guo, C Deng, Y Wen, H Chen, Y Chang, J Wang, arXiv:2402.174532024arXiv preprint</p>
<p>Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. Y Guo, C Yang, A Rao, Z Liang, Y Wang, Y Qiao, M Agrawala, D Lin, B Dai, 2024</p>
<p>Inductive reasoning in humans and large language models. S J Han, K J Ransom, A Perfors, C Kemp, Cognitive Systems Research. 831011552024</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, 2021</p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021</p>
<p>Infiagent-dabench: Evaluating agents on data analysis tasks. X Hu, Z Zhao, S Wei, Z Chai, Q Ma, G Wang, X Wang, J Su, J Xu, M Zhu, arXiv:2401.055072024arXiv preprint</p>
<p>On the humanity of conversational ai: Evaluating the psychological portrayal of llms. J.-T Huang, W Wang, E J Li, M H Lam, S Ren, Y Yuan, W Jiao, Z Tu, M Lyu, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Q Huang, J Vora, P Liang, J Leskovec, 2024</p>
<p>Classification done right for vision-language pre-training. Z Huang, Q Ye, B Kang, J Feng, H Fan, 2024</p>
<p>Autonomous llm-driven research-from data to human-verifiable research papers. T Ifargan, L Hafner, M Kern, O Alcalay, R Kishony, NEJM AI. 21AIoa2400555, 2025</p>
<p>G Jaume, P Doucet, A H Song, M Y Lu, C Almagro-Pérez, S J Wagner, A J Vaidya, R J Chen, D F K Williamson, A Kim, F Mahmood, Hest-1k: A dataset for spatial transcriptomics and histology image analysis. 2024</p>
<p>Generative modeling of molecular dynamics trajectories. B Jing, H Stärk, T Jaakkola, B Berger, 2024</p>
<p>Dsbench: How far are data science agents from becoming data science experts?. L Jing, Z Huang, X Wang, W Yao, W Yu, K Ma, H Zhang, X Du, D Yu, 2024</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 59678732021</p>
<p>Curie: Toward rigorous and automated scientific experimentation with ai agents. P T J Kon, J Liu, Q Ding, Y Qiu, Z Yang, Y Huang, J Srinivasa, M Lee, M Chowdhury, A Chen, 2025</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, arXiv:2312.075592023arXiv preprint</p>
<p>Lab-bench: Measuring capabilities of language models for biology research. J M Laurent, J D Janizek, N Thakkar, M Ruzo, M S Yao, M S Levine, S G Rodriques, A White, 2024</p>
<p>Mlr-copilot: Autonomous machine learning research based on large language models agents. R Li, T Patel, Q Wang, X Du, arXiv:2408.140332024arXiv preprint</p>
<p>Moganet: Multi-order gated aggregation network. S Li, Z Wang, Z Liu, C Tan, H Lin, D Wu, Z Chen, J Zheng, S Z Li, 2024</p>
<p>Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. X Li, Z Huang, F Xue, Y Zhou, 2024</p>
<p>Competition-level code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Cyclenet: Enhancing time series forecasting through modeling periodic patterns. S Lin, W Lin, X Hu, W Wu, R Mo, H Zhong, 2024</p>
<p>Reasoning multi-agent behavioral topology for interactive autonomous driving. H Liu, L Chen, Y Qiao, C Lv, H Li, 2024</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in neural information processing systems. 202436</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. T Liu, C Xu, J Mcauley, 2023</p>
<p>. X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, 2023Agentbench: Evaluating llms as agents</p>
<p>3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with view-consistent 2d diffusion priors. X Liu, C Zhou, S Huang, 2024</p>
<p>R Lou, H Xu, S Wang, J Du, R Kamoi, X Lu, J Xie, Y Sun, Y Zhang, J J Ahn, H Fang, Z Zou, W Ma, X Li, K Zhang, C Xia, L Huang, W Yin, Aaar-1.0: Assessing ai's potential to assist research. 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>An extensible framework for open heterogeneous collaborative perception. Y Lu, Y Hu, Y Zhong, D Wang, Y Wang, S Chen, 2024</p>
<p>Badam: A memory efficient full parameter optimization method for large language models. Q Luo, H Yu, X Li, 2024</p>
<p>B P Majumder, H Surana, D Agarwal, B D Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, arXiv:2407.01725Discoverybench: Towards data-driven discovery with large language models. 2024arXiv preprint</p>
<p>Large language models as general pattern machines. S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, arXiv:2307.047212023arXiv preprint</p>
<p>Bixbench: a comprehensive benchmark for llm-based agents in computational biology. L Mitchener, J M Laurent, B Tenmann, S Narayanan, G P Wellawatte, A White, L Sani, S G Rodriques, arXiv:2503.000962025arXiv preprint</p>
<p>The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. A Moskvichev, V V Odouard, M Mitchell, arXiv:2305.071412023arXiv preprint</p>
<p>Aviary: training language agents on challenging scientific tasks. S Narayanan, J D Braza, R.-R Griffiths, M Ponnapati, A Bou, J Laurent, O Kabeli, G Wellawatte, S Cox, S G Rodriques, arXiv:2412.211542024arXiv preprint</p>
<p>D Nathani, L Madaan, N Roberts, N Bashlykov, A Menon, V Moens, A Budhiraja, D Magka, V Vorotilov, G Chaurasia, D Hupkes, R S Cabral, T Shavrina, J Foerster, Y Bachrach, W Y Wang, R Raileanu, Mlgym: A new framework and benchmark for advancing ai research agents. 2025</p>
<p>R Ohana, M Mccabe, L Meyer, R Morel, F J Agocs, M Beneitez, M Berger, B Burkhart, K Burns, S B Dalziel, D B Fielding, D Fortunato, J A Goldberg, K Hirashima, Y.-F Jiang, R R Kerswell, S Maddu, J Miller, P Mukhopadhyay, S S Nixon, J Shen, R Watteaux, B R .-S. Blancard, F Rozet, L H Parker, M Cranmer, S Ho, The well: a large-scale collection of diverse physics simulations for machine learning. 2025</p>
<p>Zero bubble (almost) pipeline parallelism. P Qi, X Wan, G Huang, M Lin, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Civrealm: A learning and reasoning odyssey in civilization for decision-making agents. S Qi, S Chen, Y Li, X Kong, J Wang, B Yang, P Wong, Y Zhong, X Zhang, Z Zhang, N Liu, W Wang, Y Yang, S.-C Zhu, 2024</p>
<p>Torchopt: An efficient library for differentiable optimization. J Ren, X Feng, B Liu, X Pan, Y Fu, L Mai, Y Yang, 2022</p>
<p>Jaxmarl: Multi-agent rl environments and algorithms in jax. A Rutherford, B Ellis, M Gallici, J Cook, A Lupu, G Ingvarsson, T Willi, R Hammond, A Khan, C S De Witt, A Souly, S Bandyopadhyay, M Samvelyan, M Jiang, R T Lange, S Whiteson, B Lacerda, N Hawes, T Rocktaschel, C Lu, J N Foerster, 2024</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>G Starace, O Jaffe, D Sherburn, J Aung, J S Chan, L Maksin, R Dias, E Mays, B Kinsella, W Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>W Sun, L Yan, X Ma, S Wang, P Ren, Z Chen, D Yin, Z Ren, Is chatgpt good at search? investigating large language models as re-ranking agents. 2024</p>
<p>The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, bioRxiv. 2024</p>
<p>J Tang, H Lu, R Wu, X Xu, K Ma, C Fang, B Guo, J Lu, Q Chen, Y.-C Chen, Hawk, Learning to understand open-world video anomalies. 2024</p>
<p>M Tian, L Gao, S D Zhang, X Chen, C Fan, X Guo, R Haas, P Ji, K Krongchon, Y Li, S Liu, D Luo, Y Ma, H Tong, K Trinh, C Tian, Z Wang, B Wu, Y Xiong, S Yin, M Zhu, K Lieret, Y Lu, G Liu, Y Du, T Tao, O Press, J Callan, E Huerta, H Peng, Scicode: A research coding benchmark curated by scientists. 2024</p>
<p>Automl-agent: A multi-agent llm framework for full-pipeline automl. P Trirat, W Jeong, S J Hwang, arXiv:2410.029582024arXiv preprint</p>
<p>Knowledge fusion of large language models. F Wan, X Huang, D Cai, X Quan, W Bi, S Shi, 2024</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Q Wang, D Downey, H Ji, T Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>. W Wang, S Zhang, Y Ren, Y Duan, T Li, S Liu, M Hu, Z Chen, K Zhang, L Lu, X Zhu, P Luo, Y Qiao, J Dai, W Shao, W Wang, 2024Needle in a multimodal haystack</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, 2024</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024arXiv preprint</p>
<p>Neurodin: A two-stage framework for high-fidelity neural surface reconstruction. Y Wang, D Huang, W Ye, G Zhang, W Ouyang, T He, 2024</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 792023</p>
<p>Magicoder: Source code is all you need. Y Wei, Z Wang, J Liu, Y Ding, L Zhang, arXiv:2312.021202023arXiv preprint</p>
<p>Towards an ai co-scientist. W.-H Weng, 2025</p>
<p>H Wijk, T Lin, J Becker, S Jawhar, N Parikh, T Broadley, L Chan, M Chen, J Clymer, J Dhyani, E Ericheva, K Garcia, B Goodrich, N Jurkovic, M Kinniment, A Lajko, S Nix, L Sato, W Saunders, M Taran, B West, E Barnes, Re-bench: Evaluating frontier ai rd capabilities of language model agents against human experts. 2024</p>
<p>D Wu, J Chang, F Jia, Y Liu, T Wang, J Shen, Topomlp: A simple yet strong pipeline for driving topology reasoning. 2023</p>
<p>Clipself: Vision transformer distills itself for open-vocabulary dense prediction. S Wu, W Zhang, L Xu, S Jin, X Li, W Liu, C C Loy, 2024</p>
<p>Infllm: Training-free long-context extrapolation for llms with an efficient context memory. C Xiao, P Zhang, X Han, G Xiao, Y Lin, Z Zhang, Z Liu, M Sun, 2024</p>
<p>T Xie, X Qi, P He, Y Li, J T Wang, P Mittal, Badexpert: Extracting backdoor functionality for accurate backdoor input detection. 2023</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation and beyond. F Xu, Q Lin, J Han, T Zhao, J Liu, E Cambria, IEEE Transactions on Knowledge and Data Engineering. 2025</p>
<p>Y Xu, W Li, P Vaezipoor, S Sanner, E B Khalil, arXiv:2305.18354Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. 2023arXiv preprint</p>
<p>Bag of tricks: Benchmarking of jailbreak attacks on llms. Z Xu, F Liu, H Liu, 2024</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, Large language models as optimizers. 2024</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. L Yang, Z Yu, T Zhang, S Cao, M Xu, W Zhang, J E Gonzalez, B Cui, 2024</p>
<p>Z Yang, L Dong, X Du, H Cheng, E Cambria, X Liu, J Gao, F Wei, arXiv:2212.10923Language models as inductive reasoners. 2022arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Zipformer: A faster and better encoder for automatic speech recognition. Z Yao, L Guo, X Yang, W Kang, F Kuang, Y Yang, Z Jin, L Lin, D Povey, 2024</p>
<p>Dolphin: Closed-loop open-ended auto-research through thinking, practice, and feedback. J Yuan, X Yan, B Shi, T Chen, W Ouyang, B Zhang, L Bai, Y Qiao, B Zhou, 2025</p>
<p>De novo design of high-affinity protein binders with alphaproteo. V Zambaldi, D La, A E Chu, H Patani, A E Danson, T O Kwan, T Frerix, R G Schneider, D Saxton, A Thillaisundaram, arXiv:2409.080222024arXiv preprint</p>
<p>Voxel mamba: Group-free state space models for point cloud based 3d object detection. G Zhang, L Fan, C He, Z Lei, Z Zhang, L Zhang, 2024</p>
<p>Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, 2024</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, 2023</p>
<p>Sinenet: Learning temporal dynamics in time-dependent partial differential equations. X Zhang, J Helwig, Y Lin, Y Xie, C Fu, S Wojtowytsch, S Ji, 2024</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>Causal-learn: Causal discovery in python. Y Zheng, B Huang, W Chen, J Ramsey, M Gong, R Cai, S Shimizu, P Spirtes, K Zhang, 2023</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, 2025</p>
<p>Moe jetpack: From dense checkpoints to adaptive mixture of experts for vision tasks. X Zhu, Y Guan, D Liang, Y Chen, Y Liu, X Bai, 2024</p>
<p>Unmasking and improving data credibility: A study with datasets for training harmless language models. Z Zhu, J Wang, H Cheng, Y Liu, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6758 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6758</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6758</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-df8a3b2a22eb4bb92705ca22b3a5f2b8dd73edc6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df8a3b2a22eb4bb92705ca22b3a5f2b8dd73edc6" target="_blank">Are LLMs Good Cryptic Crossword Solvers?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- are established showing that their performance on this task is still far from that of humans.</p>
                <p><strong>Paper Abstract:</strong> Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6758",
    "paper_id": "paper-df8a3b2a22eb4bb92705ca22b3a5f2b8dd73edc6",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00372225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are LLMs Good Cryptic Crossword Solvers?</h1>
<p>Abdelrahman Sadallah Daria Kotova Ekaterina Kochmar<br>Department of Natural Language Processing, MBZUAI<br>{abdelrahman.sadallah, daria.kotova, ekaterina.kochmar}@mbzuai.ac.ae</p>
<h4>Abstract</h4>
<p>Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs LLaMA2, Mistral, and ChatGPT - showing that their performance on this task is still far from that of humans.</p>
<h2>1 Introduction</h2>
<p>A cryptic crossword is a type of crossword puzzle that is known for its enigmatic clues (Friedlander and Fine, 2016). Unlike standard crossword puzzles, where clues are straightforward definitions or synonyms of the answers, cryptic crosswords involve wordplay, riddles, and cleverly disguised hints that make solving them more challenging (Moorey, 2018). Figure 1 demonstrates an example of a cryptic crossword clue.</p>
<p>To solve a cryptic clue, one should be able to not only apply generic rules in the specific context of the clue but also use general and domain-specific knowledge to arrive at a reasonable answer. Tackling cryptic crosswords with modern NLP methods, therefore, provides an interesting challenge. It has been shown that current NLP models are far from human performance: Rozner et al. (2021), and Efrat et al. (2021) report accuracy of $7.3 \%$, and $8.6 \%$ for rule- and transformer-based models against $99 \%$ achievable by expert human solvers (and $74 \%$ by self-proclaimed amateurs) (Friedlander and Fine, 2009, 2020), and there are still no official statistics for average human performance. This identifies a challenging area for current NLP research, while also opening up possibilities for improvement and innovation.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a cryptic clue: number 5 at the end of the clue denotes the number of characters in the answer and is called enumeration. The definition part here is likely to be language model, with the rest being the wordplay part. Beheads or similar words point to the first letters of the next word, while confused (as well as mixed up, etc.) is likely to indicate an anagram. As we should look for a language model's name that starts with the letter $l$ plus an anagram of Alma and consists of 5 letters, the answer here is Llama.</p>
<p>Prior work suggests that LLMs can show emergent capabilities (Wei et al., 2022), so it can be assumed that they should be able to solve cryptic puzzles if not on a par with human solvers, then at least somewhat successfully. However, to the best of our knowledge, this assumption has not been tested before. In this work, we address this research gap as we believe that trying to solve cryptic clues with LLMs might reveal their limitations as well as important aspects of natural language understanding and interpretation captured by LLMs.</p>
<p>Typically, a cryptic clue can be split into two parts: the definition and the wordplay. The definition consists of one or more words in the clue that can be used interchangeably with the answer, and it usually appears either at the beginning or at the end of the clue. The wordplay can take many forms: Table 1 shows the most popular wordplay types and provides an example ${ }^{1}$ for each of them. Previous work has explored explicitly splitting the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Example Clue</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>ANAGRAM is a word (or words) that, when rearranged, form(s) a different word or phrase.</td>
<td>Never upset a Sci Fi writer (5)</td>
<td>Verne</td>
</tr>
<tr>
<td>HIDDEN CLUES have the answer written in the clue itself, amongst other words.</td>
<td>Confront them in the tobacco store (6)</td>
<td>Accost</td>
</tr>
<tr>
<td>DOUBLE DEFINITION contains two meanings of the same word.</td>
<td>In which you'd place the photo of the NZ author (5)</td>
<td>Frame</td>
</tr>
<tr>
<td>HOMOPHONE is a word that is pronounced the same as another but spelled differently.</td>
<td>Sounds like a couple (pair) to scale down (4)</td>
<td>Pare</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of common wordplay types. The definition part is bolded. Underlined is the part that relates to the wordplay. Actual clues do not have those indicators - we include them here for illustrative purposes only.
solution into these two parts (Deits, 2015; Rozner et al., 2021).</p>
<p>Past approaches applied to solving cryptic clues range from rule-based models, ${ }^{2}$ to traditional machine learning models like KNNs (Rozner et al., 2021), to Transformer models like T5 (Efrat et al., 2021). However, all these models achieve only modest accuracy on the task (§2). As language models are trained to understand language and incorporate real-world knowledge (Minaee et al., 2024), it can be assumed that, when prompted with a clue and short instructions, they should be able to utilize the embedded knowledge to both understand the instructions and succeed at the task. However, our preliminary investigation suggests that such a zero-shot, naive approach yields near $0 \%$ accuracy and that LLMs struggle to understand the enumeration (answer length) constraint included in the prompt. Therefore, in this work, we experiment with prompting the models with different types of instructions (§3.1), providing models with a few illustrative examples to learn from (§3.2), and finally explicitly fine-tuning them on the task (§3.3). We use two of the most popular open-source LLMs: LLaMA2-7b (Touvron et al., 2023) and Mistral-7b (Jiang et al., 2023), using QLoRA (Dettmers et al., 2023) for Parameter Efficient Fine-Tuning (PEFT). For comparison, we also evaluate ChatGPT (OpenAI, 2021) - one of the most powerful LLMs to date - on the same task in both zero- and few-shot settings.</p>
<p>Our main contributions are as follows: (1) We explore the general abilities of LLMs on the challenging task of solving cryptic crosswords and benchmark the results of three popular LLMs; (2) We experiment with zero- and few-shot learning as well</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>as fine-tuning of open-source LLMs, presenting them with increasingly challenging data splits; (3) To facilitate replicability of our results and followup experiments, we release our data and code. ${ }^{3}$</p>
<h2>2 Related Work</h2>
<p>LLMs' emergent capabilities LLMs have been shown to follow the scaling law (Kaplan et al., 2020), which has motivated researchers to explore the performance limit by increasing the size of both model and data. This has led to the discovery of the emergent abilities of LLMs (Wei et al., 2022), which occur when training models with similar architectures and on the same tasks at different scales. As a result, models may exhibit unexpected abilities in solving a series of novel tasks: for instance, a relatively small LLM like GTP-3 (Brown et al., 2020) does well on arithmetic tasks, question answering or passage summarization just through in-context learning (Yousefi et al., 2024). Solving cryptic crossword puzzles is a very complex task since it requires the model to form hypotheses about the answer based on the definition and wordplay, and then select the best among such hypotheses using the information about the answer length. Whether LLMs can do this well is an open question that we aim to investigate in this work.</p>
<p>Solving puzzles with NLP models Although there is prior work on wordplay (Luo et al., 2019; He et al., 2019; Ermakova et al., 2023) and traditional crosswords (Littman et al., 2002; Zugarini et al., 2023), much less attention has been paid to cryptic crosswords specifically. Deits (2015) achieved $8.6 \%$ accuracy on the task with a rulebased solver, which used the fact that a cryptic</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>clue can be split into the definition and the wordplay parts, and applied hand-crafted probabilistic context-free grammar to find the best split. Hardcastle (2007) performed cryptic clue generation using a hand-crafted grammar.</p>
<p>Efrat et al. (2021) introduced Cryptonite, a dataset of 523,114 cryptic clues collected from The Times ${ }^{4}$ and The Telegraph. ${ }^{5}$ They fine-tuned a T5 (Raffel et al., 2023) model, which helped set the benchmark accuracy for Transformer methods at $7.6 \%$. Rozner et al. (2021) introduced a dataset extracted from The Guardian, ${ }^{6}$ and introduced a curriculum approach, which involved training a model on simpler tasks before progressing to more complex compositional clues. This increased the performance to $21.8 \%$.</p>
<h2>3 Methodology</h2>
<p>In order to test LLMs on their ability to solve cryptic puzzles, we challenge them in various scenarios, ranging from (1) prompting the models with various amounts of information incorporated (§3.1), to (2) showing the models several examples in a few-shot learning mode (§3.2), to (3) explicitly finetuning the models on this task (§3.3). We focus on solving the cryptic clues individually and posit that solving for the whole grid can be formulated as an iterative process of solving for each clue.</p>
<h3>3.1 Prompt variation</h3>
<p>As described in Section 1, prompting the models with only the crossword clue leads to $0 \%$ accuracy, with the models not properly understanding the enumeration information given in the parentheses. To this end, we have experimented with various prompts, incorporating additional information in the instructions for the model. Figure 2 presents two prompts that we have used in our experiments: the base prompt is a generic definition of the task, which also includes a short hint for the model on how to interpret the enumeration part of the clue. The extended prompt, in addition, explains the structure of the answer to the model. Experimenting with these two prompts, we investigate whether the model benefits from additional information and shows higher performance using</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Base Prompt:</h2>
<p>The next line is a clue for a cryptic crossword. Solve this clue. The number in the parentheses in the clue represents the number of characters of the answer. Output only the answer.</p>
<h2>Extended Prompt:</h2>
<p>The next line is a clue for a cryptic crossword. The clue consists of a definition part and a wordplay part. The answer consists of (n_words) words, and the number of characters in the answer is (n_chars). Output only the answer.</p>
<p>Figure 2: The two prompts that we used across all our experiments. The base prompt contains simple instruction, while the extended prompt also includes explicit information about the answer format.
the extended prompt.</p>
<h3>3.2 Few-shot learning</h3>
<p>Next, we evaluate our models using the few-shot learning technique (Song et al., 2022) using two different approaches to selecting examples for the few shots. Under the random shots strategy, we select examples randomly, whereas with indicator shots we choose examples that are related to the current clue via an indicator word (e.g., break down and confused). Since the dataset does not contain information about the type of the clue or the wordplay, we use the indicator dictionary ${ }^{7}$ produced by Deits (2015), which contains a list of keywords defining 6 wordplay types. The assumption is that the models will be able to deduce from these examples, without being explicitly told, what operations they need to apply to the clue to derive the answer. This is akin to humans learning how to solve such puzzles with a few examples demonstrating the common strategies to follow. Figure 3 shows a 3-shot example using the extended prompt.</p>
<h3>3.3 LLM fine-tuning</h3>
<p>In the final setting, we explicitly fine-tune the models for our task. This step takes the models from the general language-modeling space and forces them to align with one specific task. This setting is the most computationally expensive among the three, and in order to fine-tune LLMs with limited</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Number of examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Split</td>
<td style="text-align: center;">Train</td>
</tr>
<tr>
<td style="text-align: center;">naive-random</td>
<td style="text-align: center;">85,428</td>
</tr>
<tr>
<td style="text-align: center;">naive-random-unique</td>
<td style="text-align: center;">47,844</td>
</tr>
<tr>
<td style="text-align: center;">word-init-disjoint</td>
<td style="text-align: center;">75,847</td>
</tr>
<tr>
<td style="text-align: center;">word-init-disjoint-unique</td>
<td style="text-align: center;">42,793</td>
</tr>
<tr>
<td style="text-align: center;">word-init-disjoint-half</td>
<td style="text-align: center;">69,339</td>
</tr>
</tbody>
</table>
<p>Figure 3: 3-shot learning input using random examples.
hardware resources, we use QLoRA - one of the recent methods of efficient fine-tuning (Dettmers et al., 2023) based on adapters (Hu et al., 2021).</p>
<h2>4 Data</h2>
<p>In our experiments, we primarily use the dataset introduced by Rozner et al. (2021) and extracted from The Guardian, as it contains data splits of different levels of difficulty and, therefore, is appropriate for testing LLMs' abilities. The dataset contains 55,783 distinct answers, resulting in an average occurrence of about 2.55 clues for each unique answer, which means that several unique but similar clues may lead to the same answer. In addition, the answers within the dataset vary in length, typically comprising 1 to 6 words, with the majority consisting of just 1 or 2 words.</p>
<h3>4.1 Data splits</h3>
<p>As is highlighted in previous work (Efrat et al., 2021; Rozner et al., 2021), for this task one should select the training and test splits carefully. First of all, despite the fact that the data contains only unique clue-answer pairs, clues with the same answer still tend to be similar. Therefore, if a clue similar to the one in the training set appears in the test set with the same answer, it is likely that the model just reproduces the answer from the training, which would undermine any conclusions on</p>
<p>Table 2: Statistics on The Guardian dataset.
the model's generalization behaviour. Secondly, Rozner et al. (2021) have noted that inflections and different spelling variants of the same words can present a similar issue and suggested the word-init-disjoint split, with which answers that share the first two characters are grouped together and allocated to a single split (training or test) only.</p>
<p>Although word-init-disjoint split provided by Rozner et al. (2021) takes care of the partial answer overlap, the training and test subsets may still contain several different clues with the same answer. Since this may lead to a certain level of confusion in models, we also introduce 3 new splits in addition to the two original ones proposed in Rozner et al. (2021), which we use in our finetuning experiments. The splits are detailed below, with the statistics on them provided in Table 2:</p>
<ul>
<li>naive-random: A shuffled random split from Rozner et al. (2021)</li>
<li>naive-random-unique: Naive-random with only one clue for every answer (ours)</li>
<li>word-init-disjoint: Answers that share the first two characters are grouped together and allocated to one split (train or test) only as per Rozner et al. (2021)</li>
<li>word-init-disjoint-unique: Same as word-init-disjoint, but with every answer occurring only once (ours)</li>
<li>word-init-disjoint-half: Same as word-initdisjoint, but using only half of the clues with the same answer (ours)</li>
</ul>
<h2>5 Experiments</h2>
<p>To evaluate the performance of our models, in all reported experiments we use accuracy (Acc.), which is measured using exact string matching. This shows the percentage of correctly solved cryptic clues from the datasets in question.</p>
<table>
<thead>
<tr>
<th>Clue: Unaware of how developers work (2,3,4)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Answer: in the dark</td>
</tr>
<tr>
<td>Original output: in the dark about it all</td>
</tr>
<tr>
<td>Cleaned output: in the dark</td>
</tr>
</tbody>
</table>
<p>Figure 4: The full original output of the model is incorrect, however, it contains the correct answer. Using enumeration and cleaning, this answer can be extracted from the output.</p>
<p>In addition, previous work (Efrat et al., 2021; Rozner et al., 2021) reported that enumeration provided models with useful information, while our preliminary experiments (see §1) suggest that LLMs struggle to benefit from this information if it is simply given as a number in the parentheses. Therefore, to further test LLMs’ grasp of this information, we estimate length error rate (Len Err.) by calculating how often the length of the predicted answer differs from that of the correct answer.</p>
<h3>5.1 Can LLMs solve the clues given various prompts?</h3>
<p>The first set of experiments, as defined in §3.1, considers using base and extended prompts with LLaMA and Mistral. The results are presented in the top row of Table 3 as a zero-shot scenario since no examples are included in the prompts. Both models show similar performance with both types of prompts, suggesting that additional elaboration on the task included in the prompt itself does not help: in both cases, LLaMA shows an accuracy of 0.2%, while Mistral performs marginally better and achieves an accuracy of 0.8%. Moreover, despite the fact that both base and extended prompts include an explanation of what the numbers in parentheses mean (see Fig. 2), the models do not seem to benefit from this information, as the length error rate is above 75% for all settings across both models. This encourages us to look into the outputs of the models further.</p>
<p>Can the models provide partially correct answers? Enumeration provides solvers with useful information as it indicates how long the expected answer should be. Human solvers actively use this information while trying to arrive at the correct answers, and some NLP models seem to benefit from this information as well (Efrat et al., 2021; Rozner et al., 2021). However, LLMs seem to be far worse at this task: we observe a length error rate of 74-89% across both LLaMA and Mistral models, with both types of prompts and under all settings (see Table 3). At the same time, a close inspection of LLMs’ outputs suggests that, in fact, they often produce correct answers followed by some extraneous characters or words of various degrees of relevance (see Fig. 4). Thus, we have cleaned the models’ answers by considering only the first $n$ words for multi-word answers (where $n$ is the number of words in the correct answer), and by then only taking the first $m$ letters of the words as is specified in the enumeration part of the clue. As a result, Table 3 reports the accuracy achieved by the models on cleaned outputs as well as the length error rate for the original outputs. Note that although cleaning helps improve the results, it does not bring the length error rate to 0% as in some cases models produce outputs with fewer characters or words than in the correct answer.</p>
<h3>5.2 Few-shot learning: Do models learn from examples?</h3>
<p>One way in which expert human solvers may gain valuable experience in solving cryptic puzzles is by learning from examples. In fact, it is unclear how successful human solvers are in the truly zero-shot scenario when presented with cryptic crosswords for the first time in their lives. While in §5.1 we challenged our models under such zero-shot regime and showed that they do not do well, in this section we investigate whether they are capable of learning to solve the task when provided with a few (informative) examples. Figure 3 demonstrates the use of 3 examples when prompting the model.</p>
<p>In Table 3, we report the results obtained with LLaMA and Mistral using different few-shot prompting methods. All these results are calculated on the naive-random test set from Rozner et al. (2021) chosen because it captures a representative distribution of the data. As the results suggest, 3-shot learning with randomly selected examples (random shots) increases the model’s accuracy by at least 2 percentage points (p.p.) (from 0.2 to 2.2 with the extended prompt) for LLaMA and by 3 p.p. for Mistral. The length error rate also decreases by at least 8 p.p. (from 89 to 81 for LLaMA, with a larger decrease observed for Mistral). When we increase the number of examples provided to the models from 3 to 10, we observe that the accuracy for both models increases further – to 3.0 and 2.8</p>
<table>
<thead>
<tr>
<th>Type of shots</th>
<th>Number of shots</th>
<th>Base Prompt</th>
<th></th>
<th></th>
<th></th>
<th>Extended Prompt</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMA</td>
<td></td>
<td>Mistral</td>
<td></td>
<td>LLaMA</td>
<td></td>
<td>Mistral</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
</tr>
<tr>
<td></td>
<td>0</td>
<td>0.2</td>
<td>89</td>
<td>0.8</td>
<td>87</td>
<td>0.2</td>
<td>89</td>
<td>0.8</td>
<td>85</td>
</tr>
<tr>
<td>random</td>
<td>3</td>
<td>2.4</td>
<td>81</td>
<td>3.8</td>
<td>75</td>
<td>2.2</td>
<td>81</td>
<td>3.8</td>
<td>74</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>3.0</td>
<td>81</td>
<td>4.4</td>
<td>75</td>
<td>2.8</td>
<td>81</td>
<td>4.2</td>
<td>75</td>
</tr>
<tr>
<td>indicator</td>
<td>3</td>
<td>2.4</td>
<td>81</td>
<td>3.8</td>
<td>75</td>
<td>2.0</td>
<td>81</td>
<td>3.9</td>
<td>75</td>
</tr>
<tr>
<td></td>
<td>10</td>
<td>2.8</td>
<td>80</td>
<td>4.4</td>
<td>75</td>
<td>2.7</td>
<td>81</td>
<td>4.3</td>
<td>75</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results for LLaMA and Mistral on the naive-random test set using different few-shot learning scenarios. We report accuracy on cleaned outputs and length error rate for the original ones.</p>
<table>
<thead>
<tr>
<th>0-shot</th>
<th>Base Prompt</th>
<th></th>
<th>Extended Prompt</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
</tr>
<tr>
<td></td>
<td>6.6</td>
<td>36</td>
<td>6.2</td>
<td>35</td>
</tr>
<tr>
<td>3-shot</td>
<td>9.5</td>
<td>35</td>
<td>8.9</td>
<td>33</td>
</tr>
</tbody>
</table>
<p>Table 4: ChatGPT results using zero- and 3-shot learning with randomly sampled shots. The results with the base prompt on the word-init-disjoint split, while we used the naive-random split with the extended prompt.
for LLaMA, and 4.4 and 4.2 for Mistral with the base and extended prompt, respectively. As in $\S 5.1$, we observe that (1) the extended prompt does not bring any reliable improvement, and (2) Mistral overall outperforms LLaMA on this task. We also conclude that both models are able to learn from the few provided examples, even when they are selected randomly, and the rate of improvement likely slows down with more provided examples.</p>
<p>Do similar examples help? Next, we investigate whether more carefully selected examples, which take into account the indicator word (see §3.2), help guide the models better. The results of these experiments are summarized in Table 3 under indicator shots, and they do not suggest any reliable improvement: in fact, while in some cases we observe an increase of 0.1 p.p. (Mistral with an extended prompt), in other cases model's performance even decreases (LLaMA under several settings).</p>
<h3>5.3 Does ChatGPT do better?</h3>
<p>To get a better understanding of the difficulty of the task, we also evaluate one of the most powerful closed LLMs - ChatGPT. We run the evaluation for the two prompts (base and extended) ${ }^{8}$ and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>using two test sets - naive-random and word-initdisjoint. We observe no substantial difference in the results on these data splits and also find that providing the model with randomly selected examples in few-shot learning helps increase its performance: as is demonstrated in Table 4, accuracy improves from 6.6 to 9.5 with the base prompt. Experiments with 10 -shot prompting do not suggest any significant improvements and are, therefore, not reported. These trends are aligned with those observed for the open-source models, and, although, overall, ChatGPT shows better performance on this task, it is still significantly below human level.</p>
<h3>5.4 Fine-tuning: Can models be explicitly taught to solve the task?</h3>
<p>Next, we perform fine-tuning experiments with the two open-source models - LLaMA and Mistral using the five data splits described in $\S 4.1$. The results of these experiments are presented in Table 5. On their basis, we draw the following conclusions: (1) As in the previous experiments, Mistral achieves higher accuracy than LLaMA. (2) The overall trend that we observe is that fine-tuning and testing the models on naive splits leads to higher results across models and prompts. This suggests that the models memorize answers to a considerable extent in the naive-random split, while the performance drops rapidly by up to 5 p.p across the models on the naive-random-unique split. (3) All types of disjoint splits prove to be very challenging for both models and having different clues with similar answers apparently confuses them. Overall, we conclude that models can be taught to solve cryptic puzzles only to a limited extent.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Base Prompt</th>
<th></th>
<th></th>
<th></th>
<th>Extended Prompt</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>LLaMA</td>
<td></td>
<td>Mistral</td>
<td></td>
<td>LLaMA</td>
<td></td>
<td>Mistral</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
<td>Len Err.</td>
</tr>
<tr>
<td>naive-random</td>
<td>7.4</td>
<td>81</td>
<td>13.0</td>
<td>82</td>
<td>10.3</td>
<td>80</td>
<td>13.0</td>
<td>74</td>
</tr>
<tr>
<td>word-init-disjoint</td>
<td>0.5</td>
<td>94</td>
<td>1.2</td>
<td>80</td>
<td>0.7</td>
<td>85</td>
<td>1.2</td>
<td>89</td>
</tr>
<tr>
<td>word-init-disjoint-half</td>
<td>0.4</td>
<td>90</td>
<td>1.7</td>
<td>68</td>
<td>0.8</td>
<td>95</td>
<td>1.8</td>
<td>93</td>
</tr>
<tr>
<td>word-init-dosjoint-unique</td>
<td>0.6</td>
<td>88</td>
<td>1.7</td>
<td>91</td>
<td>0.9</td>
<td>96</td>
<td>1.5</td>
<td>80</td>
</tr>
<tr>
<td>naive-random-unique</td>
<td>3.8</td>
<td>92</td>
<td>8.0</td>
<td>92</td>
<td>5.0</td>
<td>96</td>
<td>8.1</td>
<td>67</td>
</tr>
</tbody>
</table>
<p>Table 5: Results obtained after fine-tuning the models with the base and extended prompts. We report accuracy on cleaned outputs and length error rate for the original ones. Data splits are described in detail in §4.1.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Base Prompt</th>
<th></th>
<th>Extended Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Train</td>
<td>Test</td>
<td>Acc.</td>
<td>Len Err.</td>
<td>Acc.</td>
</tr>
<tr>
<td>cryptonite (train)</td>
<td>cryptonite (full set)</td>
<td>14.0</td>
<td>96</td>
<td>15.7</td>
</tr>
<tr>
<td>word-init-disjoint</td>
<td>cryptonite (full set)</td>
<td>3.1</td>
<td>88</td>
<td>3.1</td>
</tr>
<tr>
<td>word-init-disjoint</td>
<td>cryptonite (quick set)</td>
<td>4.7</td>
<td>85</td>
<td>5.8</td>
</tr>
</tbody>
</table>
<p>Table 6: Results obtained with Mistral on the Cryptonite test set. We fine-tune Mistral on the Cryptonite train set. We also evaluate Mistral, fine-tuned on our word-init-disjoint split, on the Cryptonite full test set and quick clues subset, which contains clues that are considered to be easier.</p>
<h3>5.5 Do models benefit from word masks?</h3>
<p>Another reason why our models consistently generate answers with the wrong length might be that the prompts do not force them enough to understand the length condition. To that end, we have designed and tested an alternative prompt presented in Figure 5, which guides the model to output words of the correct length. This is motivated by the real-world experience of solving a 2-D crossword grid, where having some already-solved clues, can provide parts of the answers for other adjacent clues.</p>
<p>The first line of Table 7 reports the results of fine-tuning with the prompt without any open letters. For the next experiment, we randomly choose $p\%$ of the letters in the answers to be opened, with at least one letter opened for $p&gt;0$. The results for this experiment are presented in Table 7. We achieve the highest accuracy of 27% so far on the disjoint split and observe that both length error rate and accuracy improve with the increase of $p$. This suggests that the model benefits from the information we release to it in our prompts.</p>
<h3>5.6 Experiments with Cryptonite: How generalizable are our results?</h3>
<p>To test the conclusions reached by Rozner et al. (2021) that the Cryptonite dataset makes it easier for the models to memorize the answers instead of</p>
<p>$###$ Instruction: The next line is a clue for a cryptic crossword. Solve this clue. The number in the parentheses in the clue represents the number of characters of the answer. After the clue, there is a template for the answer, where each * symbol represents a letter. Some letters are already filled in. Replace the * symbols with the correct letters of the answer. Output only the answer.</p>
<div class="codehilite"><pre><span></span><code>###<span class="w"> </span><span class="nv">Input</span>:<span class="w"> </span><span class="nv">Number</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">copies</span><span class="w"> </span><span class="nv">produced</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">picture</span><span class="w"> </span><span class="k">for</span>
<span class="nv">all</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">see</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">navy</span><span class="w"> </span><span class="ss">(</span><span class="mi">5</span>,<span class="mi">3</span><span class="ss">)</span>
</code></pre></div>

<p>${ }^{<em>} r^{</em> *} * * n$</p>
<p>Figure 5: Example of a prompt helping the model get the correct number of characters in the answer: the number of * symbols in the line after the clue corresponds to the number of letters in the expected answer. For further experiments, we replace some of those symbols with the correct letters of the answer in their respective positions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Percentage filled</th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Len Err.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">82</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">35</td>
</tr>
</tbody>
</table>
<p>Table 7: Results obtained by filling a certain percentage of a clue with the letters from the answer. The results are for the Mistral model fine-tuned on the word-initdisjoint split.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>naive-random</th>
<th>init-disjoint</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baselines</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Rule-based</td>
<td>7.3</td>
<td>7.3</td>
</tr>
<tr>
<td>T5 naive</td>
<td>16.3</td>
<td>1.1</td>
</tr>
<tr>
<td>T5 + curriculum</td>
<td>21.8</td>
<td>6.5</td>
</tr>
<tr>
<td>Mistral</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>13.0</td>
<td>1.2</td>
</tr>
<tr>
<td>10-shot</td>
<td>4.4</td>
<td>4.6</td>
</tr>
<tr>
<td>ChatGPT</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3-shot</td>
<td>8.9</td>
<td>9.5</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison between accuracy achieved by baselines, our best two models (Mistral with the extended prompt for the fine-tuning and the base prompt for the 10-shot results), and ChatGPT with both prompts on the test sets from <em>Rozner et al. (2021)</em>.
trying to solve the clues, we have fine-tuned the best open-source model from our previous experiments (Mistral) using the data splits from <em>Efrat et al. (2021)</em>. We observe that the model’s performance is on a par with what we get when Mistral is fine-tuned and tested on the naive-random split from <em>Rozner et al. (2021)</em>, and this also aligns with their conclusions (see results in Table 6). To further test our fine-tuned models against different subsets of the data, we fine-tune the models on the word-init-disjoint split of The Guardian dataset and evaluate them using the full test set from Cryptonite (after filtering the answers that overlapped with the training data used for our model), as well as the quick-clues subset, which is reported to contain simpler clues. The results in Table 6 suggest that (1) Mistral can, to a certain extent, generalize between different datasets; (2) As the accuracy obtained on Cryptonite when the model is finetuned on the data from word-init-disjoint training set of <em>Rozner et al. (2021)</em> is higher than that on the word-init-disjoint test set, Cryptonite indeed appears to be a somewhat simpler dataset; (3) In accordance with the results from <em>Efrat et al. (2021)</em>, quick-clues subset is easier to solve.</p>
<h3>5.7 Comparison with previous baselines</h3>
<p>Finally, in Table 8 we compare the best results achieved by LLMs with the previous baselines introduced in <em>Rozner et al. (2021)</em>, including:</p>
<ul>
<li>Rule-based – The solver from <em>Deits (2015)</em>, which relies on the use of hand-coded grammar and WordNet <em>Fellbaum (1998)</em>.</li>
<li>T5 naive – A vanilla fine-tuned T5 model.</li>
<li>T5 + curriculum – A T5 model that is exposed to easier subtasks (called curricular tasks) before being fine-tuned on the main task of solving cryptic clues.</li>
</ul>
<p>The results suggest that open-source LLMs still do not beat the rule-based baseline on the most challenging word-init-disjoint data split, and, like Transformer-based models, suffer from the performance drop on this split. At the same time, curriculum learning improves the results of the T5 model and suggests a promising avenue for future experiments with LLMs.</p>
<h2>6 Conclusions and Future Work</h2>
<p>In this work, we have performed extensive experiments testing the abilities of two open-source (LLaMA and Mistral) and one closed-source (ChatGPT) LLMs in solving challenging cryptic crossword puzzles. Our experiments encompass zero- and few-shot learning using prompts of various levels of elaboration on the task, as well as finetuning of the two open-source LLMs. The results suggest that, although the ChatGPT model overall outperforms open-source LLMs, in general, cryptic crosswords still represent a very challenging task for LLMs, with a large room for improvement.</p>
<p>This work sets the benchmark for LLMs on the task of solving cryptic crossword puzzles, and we believe further improvements can be achieved in future work with a number of possible research directions. Firstly, a promising avenue for research in this area is chain-of-thought <em>Wei et al. (2023)</em> and train-of-thought <em>Yao et al. (2023)</em> prompting techniques, which can potentially teach models how to arrive at the solution step by step. Secondly, given a considerable increase in performance achieved by using curriculum learning with T5 <em>Rozner et al. (2021)</em>, we consider this direction is worth exploring with LLMs as well. Finally, such approaches as mixture of experts <em>Jacobs et al. (1991); Gale et al. (2022)</em> used to train open-source models like Mixtral <em>Jiang et al. (2024)</em> can be applied to the task, as they may end up developing expert layers specializing in separate wordplay types.</p>
<h2>Limitations</h2>
<p>Limited set of LLMs experimented with Experiments with an extensive set of state-of-the-art LLMs can get quite expensive. Due to limitations of time and budget, we have been selective in terms of the LLMs that we use in this study. This has also</p>
<p>forced us to use these LLMs in a quantized fashion. Specifically, we chose only a few of the most popular open-source and closed-source LLMs. We believe that the results obtained shed light on the current LLMs' capabilities on this task, however, we acknowledge that the set of LLMs we tested here is limited, and our results cannot be extrapolated to other LLMs. In addition, in many experiments, we have observed that certain changes in settings do not bring substantial improvement to the results (e.g., we see only relatively small improvements when we switch from 3 to 10 examples in the few-shot learning setup) - this motivated us to perform only a limited set of experiments with some of the models in some of the settings as is elaborated in the paper.</p>
<p>Limitations of the data and the language considered All experiments run in this study apply to data in English only. Moreover, we have experimented on only two datasets that have been released in previous publications. While this constraint is defined by the limitations of the available data, we acknowledge that our results do not necessarily extend to other languages or other datasets that may become available in the future, as this will require further experimentation.</p>
<p>Limitations of the prompting approaches We also recognize that extensive prompt engineering may further improve the results that we report in this paper. While prompt engineering in itself was not the goal of this study and we primarily focused on the investigation of various training and test regimes for LLMs, we acknowledge that more experiments may help discover better, more informative prompts.</p>
<p>Dangers of data contamination Finally, we observe in our experiments that ChatGPT outperforms the open-source models. We admit that we lack the information about its training setup, since ChatGPT is a proprietary model, and therefore, we cannot guarantee that this model's training data is free from contamination.</p>
<h2>Ethics Statement</h2>
<p>We foresee no serious ethical implications from this study.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.</p>
<p>Robin Deits. 2015. Cryptics.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314.</p>
<p>Avia Efrat, Uri Shaham, Dan Kilman, and Omer Levy. 2021. Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language.</p>
<p>Liana Ermakova, Anne-Gwenn Bosser, Adam Jatowt, and Tristan Miller. 2023. The JOKER Corpus: English-French Parallel Data for Multilingual Wordplay Recognition. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23, page 2796-2806, New York, NY, USA. Association for Computing Machinery.</p>
<p>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. Language, Speech, and Communication. MIT Press, Cambridge, MA.</p>
<p>Kathryn J. Friedlander and Philip A. Fine. 2016. The grounded expertise components approach in the novel area of cryptic crossword solving.</p>
<p>Kathryn J. Friedlander and Philip A. Fine. 2020. Fluid Intelligence is Key to Successful Cryptic Crossword Solving. Journal of Expertise, 3(2):101-132.</p>
<p>KJ Friedlander and PA Fine. 2009. Expertise in cryptic crossword performance: an exploratory survey. In Proceedings of the International Symposium on Performance Science, Auckland, eds A. Williamon, S. Pretty, and R. Buck (Utrecht: European Association of Conservatoires (AEC)), pages 279-284.</p>
<p>Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. 2022. MegaBlocks: Efficient Sparse Training with Mixture-of-Experts.</p>
<p>David Hardcastle. 2007. Cryptic Crossword Clues: Generating Text with a Hidden Meaning. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 07), pages 147-150, Saarbrücken, Germany. DFKI GmbH.</p>
<p>He He, Nanyun Peng, and Percy Liang. 2019. Pun Generation with Surprise. In Proceedings of the 2019 Conference of the North American Chapter of the</p>
<p>Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1734-1744, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models.</p>
<p>Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. 1991. Adaptive Mixture of Local Experts. Neural Computation, 3:78-88.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of Experts.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models.</p>
<p>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(1):23-55.</p>
<p>Fuli Luo, Shunyao Li, Pengcheng Yang, Lei li, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. Pun-GAN: Generative Adversarial Network for Pun Generation.</p>
<p>Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large Language Models: A Survey.</p>
<p>Tim Moorey. 2018. How to Crack Cryptic Crosswords. Collins Puzzles.</p>
<p>OpenAI. 2021. ChatGPT. Technical report, OpenAI.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.</p>
<p>Joshua Rozner, Christopher Potts, and Kyle Mahowald. 2021. Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP.</p>
<p>Yisheng Song, Ting Wang, Subrota K Mondal, and Jyoti Prakash Sahoo. 2022. A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and FineTuned Chat Models.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</p>
<p>Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Raphaël Millière, and Ida Momennejad. 2024. Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models.</p>
<p>Andrea Zugarini, Thomas Röthenbacher, Kai Klede, Marco Ernandes, Bjoern M Eskofier, and Dario Zanca. 2023. Die Rätselrevolution: Automated German Crossword Solving.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ The evaluation for the extended prompt is done using only $35 \%$ of the data, but we note that the results do not change&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>when more data is used.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
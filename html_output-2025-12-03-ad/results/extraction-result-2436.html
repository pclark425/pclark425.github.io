<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2436 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2436</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2436</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-e4eb81ad222ba047770d5a90bdd7406c138c6126</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e4eb81ad222ba047770d5a90bdd7406c138c6126" target="_blank">Autonomous LLM-driven research from data to human-verifiable research papers</a></p>
                <p><strong>Paper Venue:</strong> NEJM AI</p>
                <p><strong>Paper TL;DR:</strong> Data-to-paper is built, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions, demonstrating a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
                <p><strong>Paper Abstract:</strong> As AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, we built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about 80-90%, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. Our work thereby demonstrates a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2436.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2436.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>data-to-paper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>data-to-paper (Autonomous LLM-driven research platform)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent automation platform that guides LLM and rule-based agents through a full, stepwise hypothesis-testing research process from annotated data to a compiled, traceable scientific paper, including hypothesis generation, experimental planning, code generation and execution, result interpretation, literature search, and paper assembly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>data-to-paper</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An orchestration platform that runs a chained list of 17 predefined research steps. Each step invokes a Performer LLM conversation (with a system prompt defining an agent identity), is pre-populated with a limited set of prior research products (to tightly control information flow), and requests a specific product type (free text, LaTeX, structured text, python code, numeric data, citations). Extracted LLM outputs undergo rule-based extraction and automated checks (format, static code checks, runtime execution, package-specific guardrails, and numerical-output verification). Select steps then run an LLM-based Reviewer conversation (role-inverted) for additional critique. Coding steps produce Python code which is executed; outputs are chained via algorithmic hyperlinks to create "data-chained" manuscripts linking every reported numeric value back to specific code lines and output files. The system supports two modalities for goals (open-goal and fixed-goal) and two interaction modes (autopilot: human oversight/approval only; co-pilot: human can provide review comments at steps). It also integrates external tools such as the Semantic Scholar API for citation retrieval and can switch LLM models (gpt-3.5/gpt-4) adaptively when rule-based checks fail or token limits require.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Research Workflow Platform</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General empirical research workflows including biomedical/clinical data analysis, public-health survey analysis, social network analysis, and machine learning model development (applied here to health indicators survey data, Twitter congressional social network data, neonatal treatment/outcome data, and pediatric intubation depth prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given an annotated dataset and optional research goal, the system (i) explores data, (ii) generates or refines research goals and hypotheses, (iii) designs hypothesis-testing plans, (iv) writes, debugs and executes data analysis code (Python), (v) generates tables and figures, (vi) performs literature search and cites related work, (vii) interprets results and writes manuscript sections, and (viii) assembles and compiles a traceable LaTeX manuscript with hyperlinks from reported numbers back to code lines and output files.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handled problems ranged from simple hypothesis-testing with low-to-moderate dimensionality (e.g., 22 features, 253k rows BRFSS subset) to multi-step ML model development (predicting optimal tube depth from ~967 patients with multiple candidate models). Complexity factors: number of analysis steps (breadth), requirement for model comparison (many ML models), need for custom code/debugging iterations, natural-language goal ambiguity. Quantitative complexity notes: datasets used included up to ~253,680 rows × 22 features and a 967-case ML dataset; breadth-related error statistics reported (see success_rate).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>All case studies used pre-existing, annotated public datasets provided as CSVs with metadata. Data volumes: Health Indicators: 253,680 responses with 22 features (no missing values); Social Network: graph edges + node affiliations; Treatment Optimization: 967 pediatric cases; Treatment Policy: NICU outcomes dataset (cleaned CSV). No experimental data generation was required; data quality was moderate-to-high as provided, and some columns were removed in a few runs to avoid token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Runs typically completed in about one hour per full research cycle (end-to-end). The platform used OpenAI ChatGPT models (gpt-3.5-turbo variants and gpt-4) as primary LLM backends; it can auto-upgrade models during runs. Exact compute hours and dollar costs not specified. Repeated runs: e.g. 5 open-goal runs per dataset (two open datasets), 10 fixed-goal runs per benchmark study; additional runs for sensitivity experiments. Open-source LLMs (Llama 2, CodeLlama) were tested but produced frequent mistakes that precluded completing full cycles, implying greater compute / iteration overhead when not using ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined hypothesis-testing workflow when provided with fixed goals; in open-goal mode the problem is more open-ended (LLM proposes goals). Problems are deterministic given code and data, but the LLM is nondeterministic (different runs produce different analyses). Structure: discrete ordered steps with clear pass/fail checks; evaluation metrics are explicit (statistical significance, correctness of code outputs, fidelity to dataset). Domain knowledge is required for correct interpretation; the system restricts prior-product context to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Metrics used in evaluation: (i) correctness of data analysis and code execution (rule-based checks and manual vetting), (ii) correctness of interpretations and conclusions (manual vetting of text vs. numeric outputs), (iii) ability to reproduce peer-reviewed study results (case-study reproduction), (iv) rate of complete, correct manuscripts produced in autopilot mode for simple goals, and (v) error rates as a function of task breadth/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>For simple hypothesis-testing tasks run autonomously (autopilot), the platform produced manuscripts that recapitulated peer-reviewed publications without major errors in ~80–90% of runs. In open-goal experiments (10 runs across two public datasets) 8/10 papers contained correct analyses with only minor wording imperfections; 2/10 had fundamental mistakes (one misinterpretation due to a hallucinated goal, one erroneous analysis). In a fixed-goal reproduction of a NICU policy study: all 10 runs reproduced analysis correctly and 8/10 reached overall correct conclusions (2/10 had interpretation errors, one affecting conclusions). In a fixed-goal ML model development reproduction: original broad goal produced ~90% error rate; narrowing the breadth (asking for fewer models) reduced error rate to ~10–20%. Human co-piloting with 2–3 short review comments per run typically fixed errors even for complex goals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary failure modes: LLM hallucinations (e.g., hallucinated features or goals leading to incorrect interpretations), erroneous analyses in complex multi-step tasks (especially when breadth of model comparisons is large), failures in open-source LLMs to converge on correct code (frequent coding mistakes), and occasional misinterpretation of correct analyses. Token limits and overly large exploratory outputs can also cause steps to fail. The system is currently limited to text/table outputs and hypothesis-testing workflows and cannot autonomously pursue follow-up questions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors aiding success: well-annotated input datasets and explicit, detailed research goals (fixed-goal modality), tight control of information flow between steps (provided prior products), multi-level rule-based checks and guardrails for code, LLM-based reviewer iterations, use of higher-quality LLMs (ChatGPT/gpt-4), iterative prompt engineering for each step, and optional human co-piloting to supply brief, targeted review comments during coding steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Comparisons show strong dependence of success on task breadth and on LLM choice: (i) For the ML model development benchmark, broad multi-model comparison produced ~90% failure; narrowing the scope reduced error to 10–20%. (ii) Open-source LLMs (Llama 2, CodeLlama) produced frequent mistakes and often could not complete coding steps or propose valid goals in pilot runs, whereas ChatGPT models allowed successful end-to-end cycles. (iii) Autopilot mode performs well for simple tasks (~80–90% success) but degrades with complexity unless human co-piloting is added; co-piloting restored high reliability even for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human baseline is implicit: the platform was benchmarked by reproducing existing peer-reviewed studies (Saint-Fleur et al. and Shim et al.). For the NICU policy reproduction, data-to-paper matched the original study's analyses in method or provided valid alternatives in most runs; however, 2/10 runs had interpretation errors. Humans remain critical for complex tasks and ethical judgments; adding minimal human review (2–3 concise comments) achieved reliable results comparable to expected human-quality outputs for the case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2436.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI conversational models: gpt-3.5-turbo, gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conversational large language models from OpenAI used as the primary LLM backend for data-to-paper; provided capabilities for natural-language reasoning, code generation, and iterative multi-agent interactions enabling end-to-end automated research cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT (OpenAI conversational models used in this paper: gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (gpt-3.5/gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the LLM performer and reviewer agents throughout data-to-paper. Different ChatGPT variants were assigned to steps based on expected conversation length and difficulty. The system also automatically upgraded to gpt-4 when rule-based feedback wasn't resolved or to gpt-3.5-turbo-16k to handle token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model / LLM-based agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General-purpose language and code tasks within automated research workflows: goal generation, literature query formulation, Python code generation and debugging, results interpretation, and paper writing.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Served as the engine to (i) propose research goals (open-goal runs), (ii) generate Python code for data exploration and analysis, (iii) craft literature-search queries, (iv) write manuscript sections and format LaTeX, and (v) participate in role-inverted reviewer conversations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handled tasks from straightforward statistical analyses (linear/logistic regression) to more complex multi-model ML development tasks; complexity influenced by prompt detail and breadth of requested analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not applicable (LLM component).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Invocations of ChatGPT models constitute the bulk of API calls; end-to-end runs completed in about an hour, with dynamic model switching when needed. Exact compute/costs not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Nondeterministic generation; step-wise conversational API usage with strict formatting expectations and rule-based checks to extract structured products.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to generate code that passes rule-based static/runtime/output checks, produce accurate tables, and write interpretable manuscript text matching numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>When used as the platform backbone, ChatGPT enabled successful completion of end-to-end research cycles in a majority of simple cases (~80–90%).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucinations in goal generation and occasional misinterpretations despite correct analyses; variability across runs due to nondeterminism.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Higher-quality reasoning and code generation relative to tested open-source models; ability to be instructed with specific system prompts and to participate in multi-agent reviewer loops.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper reports ChatGPT performed substantially better than tested open-source models (Llama 2, CodeLlama) which had frequent mistakes preventing full cycles. ChatGPT was chosen as the practical LLM backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A (LLM component) but integrated with human oversight in co-pilot mode leading to improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2436.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (Meta Open Foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source foundation and chat model family evaluated as an alternative LLM backend for data-to-paper; found to produce frequent mistakes in goal selection and data analysis coding steps within this implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama 2: Open Foundation and Fine-Tuned Chat Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tested in two critical research steps (research goal generation and data analysis coding) using the Health Indicators dataset; responses were manually annotated to assess hallucinated vs. real dataset features and the number of programmatic feedback rounds required in coding.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Open-source LLM</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Evaluated on research-goal generation and data-analysis code generation within automated research workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Attempted to (i) propose valid research goals from dataset context and (ii) generate analyzable Python code for data-analysis steps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same tasks as ChatGPT tests; found less reliable, generating hallucinated dataset features and code with frequent issues.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Tested with 10 runs for goal step and 10 runs for data-analysis step (per paper), but frequent failures impeded completion of full cycles; exact compute not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Nondeterministic LLM behavior; lacked reliable code generation quality for this application in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Manual annotation of proposed goals and number of programmatic feedback rounds until code passed rule-based checks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported qualitatively as poor: open-source LLMs (including Llama 2) produced frequent mistakes and could not consistently converge on correct coding solutions; no numeric success rate provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Frequent hallucinations in goal generation and coding mistakes that prevented passing static/runtime/output checks. Often required more iterations and still failed to converge.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Noted that Llama 2 could be improved with prompt engineering but in these experiments was less reliable than ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared to ChatGPT, Llama 2 performance was inferior for both goal selection and data analysis coding, leading authors to use ChatGPT as the practical backend.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2436.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama (Code-focused Llama variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source code-specialized LLM evaluated for data-to-paper's coding steps; found to make frequent coding mistakes preventing reliable completion of analysis steps in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code Llama: Open Foundation Models for Code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeLlama</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluated on the data analysis coding step (10 runs) and compared to ChatGPT and other LLMs by measuring number of programmatic feedback rounds until code passed rule-based checks and whether the model converged to valid code.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Open-source code-focused LLM</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Python code generation and debugging for data analysis in automated research.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate runnable, correct Python code for data exploration and statistical analysis that passes static and runtime guardrails.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Same as coding tasks for other LLMs; required handling of dataframes, regression analyses, output file creation and formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>10-run evaluation for coding step; frequent failures increased iteration count but exact compute/time not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Deterministic execution environment but with nondeterministic model outputs; required adherence to strict formatting and coding conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Number of feedback rounds to pass rule-based checks; ultimate ability to create correct output files.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported qualitatively as insufficient for reliable end-to-end cycles—frequent mistakes precluded using open-source models to complete full research runs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Common ChatGPT-like coding errors, package misuse, runtime errors, incorrect file outputs—insufficient to pass guardrails consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Model specialization for code is beneficial in concept, but in practice within this study CodeLlama produced too many errors without additional engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Inferior to ChatGPT in this deployment; authors used ChatGPT to achieve successful runs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2436.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent conversation framework for LLM applications cited as prior work on multi-agent orchestration; mentioned as inspiration/background for multi-agent interactions in data-to-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an approach that enables multi-agent LLM conversations; the paper uses multi-agent (Performer/Reviewer) conversational patterns similar in spirit to AutoGen.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-agent LLM conversation framework</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General LLM application orchestration (cited broadly; no experiment in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Framework for structuring multi-agent dialogues among LLM instances to tackle complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not applicable here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Multi-agent, conversational orchestration; cited as related work informing design decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Influential multi-agent paradigm applied conceptually in data-to-paper's Performer/Reviewer architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>No direct empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2436.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced multi-agent collaborative framework for LLM-based programming and task decomposition; cited as related work in multi-agent systems for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of multi-agent frameworks guiding LLMs to collaborate on software development and complex tasks; data-to-paper leverages analogous multi-agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-agent collaborative framework for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Software development and multi-step tasks (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not experimentally used in this paper; mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Multi-agent decomposition of tasks (conceptual relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Multi-agent division of labor and role specialization (inferred from citation context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>No empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2436.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous agent framework cited as part of prior work on automated LLM agent systems; referenced as background for iterative autonomous prompting and multi-agent automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example of autonomous agent tooling that automates iterative prompting and multi-step workflows. The authors reference such autonomous-agent approaches when motivating data-to-paper's multi-step automated research process.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Autonomous agent orchestration toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General autonomous workflows (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not directly used in experiments; mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Autonomous, iterative prompting; conceptual relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Iterative automation and tool augmentation (contextual relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>None in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2436.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical LLM research (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models (Boiko et al., Nature 624, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work demonstrating autonomous chemical research using large language models; cited by the authors as an example where LLMs designed and ran experiments in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous chemical research with LLMs (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as recent work where LLMs were used to design and run experiments in chemistry; cited to support that LLMs have demonstrated capacities for experimental design and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated experimentation platform (domain-specific, chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry / autonomous experimental design and execution (referenced only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not detailed in this paper beyond citation; cited as prior demonstration of LLMs designing and running experiments and performing scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Domain-specific experimental workflows (citation).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Demonstrates domain-specific adaptation of LLM-driven automation (inferred from citation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Not compared empirically here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2436.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Automatic Statistician (Steinruecken et al., chapter in Automated Machine Learning: Methods, Systems, Challenges)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier automated data-analysis system referenced as background for algorithmic data exploration and automated statistical inference; serves as a historical precedent to LLM-driven automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as a prior system that automates statistical modeling and model selection; cited to distinguish data-to-paper's LLM-driven de novo insight generation from traditional algorithmic data exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated statistical modeling / AutoML precursor</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated machine learning and statistical model discovery (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated discovery of statistical models and explanations from data (historical system; not used here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Historically addressed algorithmic model selection for structured data; specific complexities not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined model-search with explicit evaluation; contrasted with LLM-driven creative hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Algorithmic rigor and structured model search (contextual relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Used as a point of comparison to emphasize novelty of LLM-driven de novo insight generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2436.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2436.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DERA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced multi-agent/dialog-enabled approach for improving LLM completions; cited in context of LLMs performing diagnostic and experimental design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DERA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as work that enhances LLM outputs with dialog-enabled resolving agents and as part of cited literature demonstrating LLM capabilities in diagnostic/execution roles.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-agent/dialog-enabled LLM augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General LLM completion improvement and diagnostic tasks (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Not used here; cited as related work in LLM tool augmentation and iterative prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Dialog-enabled iterative resolution of LLM outputs (conceptual relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Iterative dialog and resolving agents (inferred relevance).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>No empirical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Autonomous LLM-driven research from data to human-verifiable research papers', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework <em>(Rating: 2)</em></li>
                <li>AutoGPT <em>(Rating: 1)</em></li>
                <li>The Automatic Statistician <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with AlphaCode <em>(Rating: 1)</em></li>
                <li>Towards Conversational Diagnostic AI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2436",
    "paper_id": "paper-e4eb81ad222ba047770d5a90bdd7406c138c6126",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "data-to-paper",
            "name_full": "data-to-paper (Autonomous LLM-driven research platform)",
            "brief_description": "A multi-agent automation platform that guides LLM and rule-based agents through a full, stepwise hypothesis-testing research process from annotated data to a compiled, traceable scientific paper, including hypothesis generation, experimental planning, code generation and execution, result interpretation, literature search, and paper assembly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "data-to-paper",
            "system_description": "An orchestration platform that runs a chained list of 17 predefined research steps. Each step invokes a Performer LLM conversation (with a system prompt defining an agent identity), is pre-populated with a limited set of prior research products (to tightly control information flow), and requests a specific product type (free text, LaTeX, structured text, python code, numeric data, citations). Extracted LLM outputs undergo rule-based extraction and automated checks (format, static code checks, runtime execution, package-specific guardrails, and numerical-output verification). Select steps then run an LLM-based Reviewer conversation (role-inverted) for additional critique. Coding steps produce Python code which is executed; outputs are chained via algorithmic hyperlinks to create \"data-chained\" manuscripts linking every reported numeric value back to specific code lines and output files. The system supports two modalities for goals (open-goal and fixed-goal) and two interaction modes (autopilot: human oversight/approval only; co-pilot: human can provide review comments at steps). It also integrates external tools such as the Semantic Scholar API for citation retrieval and can switch LLM models (gpt-3.5/gpt-4) adaptively when rule-based checks fail or token limits require.",
            "system_type": "AI Scientist / Automated Research Workflow Platform",
            "problem_domain": "General empirical research workflows including biomedical/clinical data analysis, public-health survey analysis, social network analysis, and machine learning model development (applied here to health indicators survey data, Twitter congressional social network data, neonatal treatment/outcome data, and pediatric intubation depth prediction).",
            "problem_description": "Given an annotated dataset and optional research goal, the system (i) explores data, (ii) generates or refines research goals and hypotheses, (iii) designs hypothesis-testing plans, (iv) writes, debugs and executes data analysis code (Python), (v) generates tables and figures, (vi) performs literature search and cites related work, (vii) interprets results and writes manuscript sections, and (viii) assembles and compiles a traceable LaTeX manuscript with hyperlinks from reported numbers back to code lines and output files.",
            "problem_complexity": "Handled problems ranged from simple hypothesis-testing with low-to-moderate dimensionality (e.g., 22 features, 253k rows BRFSS subset) to multi-step ML model development (predicting optimal tube depth from ~967 patients with multiple candidate models). Complexity factors: number of analysis steps (breadth), requirement for model comparison (many ML models), need for custom code/debugging iterations, natural-language goal ambiguity. Quantitative complexity notes: datasets used included up to ~253,680 rows × 22 features and a 967-case ML dataset; breadth-related error statistics reported (see success_rate).",
            "data_availability": "All case studies used pre-existing, annotated public datasets provided as CSVs with metadata. Data volumes: Health Indicators: 253,680 responses with 22 features (no missing values); Social Network: graph edges + node affiliations; Treatment Optimization: 967 pediatric cases; Treatment Policy: NICU outcomes dataset (cleaned CSV). No experimental data generation was required; data quality was moderate-to-high as provided, and some columns were removed in a few runs to avoid token limits.",
            "computational_requirements": "Runs typically completed in about one hour per full research cycle (end-to-end). The platform used OpenAI ChatGPT models (gpt-3.5-turbo variants and gpt-4) as primary LLM backends; it can auto-upgrade models during runs. Exact compute hours and dollar costs not specified. Repeated runs: e.g. 5 open-goal runs per dataset (two open datasets), 10 fixed-goal runs per benchmark study; additional runs for sensitivity experiments. Open-source LLMs (Llama 2, CodeLlama) were tested but produced frequent mistakes that precluded completing full cycles, implying greater compute / iteration overhead when not using ChatGPT.",
            "problem_structure": "Well-defined hypothesis-testing workflow when provided with fixed goals; in open-goal mode the problem is more open-ended (LLM proposes goals). Problems are deterministic given code and data, but the LLM is nondeterministic (different runs produce different analyses). Structure: discrete ordered steps with clear pass/fail checks; evaluation metrics are explicit (statistical significance, correctness of code outputs, fidelity to dataset). Domain knowledge is required for correct interpretation; the system restricts prior-product context to reduce hallucination.",
            "success_metric": "Metrics used in evaluation: (i) correctness of data analysis and code execution (rule-based checks and manual vetting), (ii) correctness of interpretations and conclusions (manual vetting of text vs. numeric outputs), (iii) ability to reproduce peer-reviewed study results (case-study reproduction), (iv) rate of complete, correct manuscripts produced in autopilot mode for simple goals, and (v) error rates as a function of task breadth/complexity.",
            "success_rate": "For simple hypothesis-testing tasks run autonomously (autopilot), the platform produced manuscripts that recapitulated peer-reviewed publications without major errors in ~80–90% of runs. In open-goal experiments (10 runs across two public datasets) 8/10 papers contained correct analyses with only minor wording imperfections; 2/10 had fundamental mistakes (one misinterpretation due to a hallucinated goal, one erroneous analysis). In a fixed-goal reproduction of a NICU policy study: all 10 runs reproduced analysis correctly and 8/10 reached overall correct conclusions (2/10 had interpretation errors, one affecting conclusions). In a fixed-goal ML model development reproduction: original broad goal produced ~90% error rate; narrowing the breadth (asking for fewer models) reduced error rate to ~10–20%. Human co-piloting with 2–3 short review comments per run typically fixed errors even for complex goals.",
            "failure_modes": "Primary failure modes: LLM hallucinations (e.g., hallucinated features or goals leading to incorrect interpretations), erroneous analyses in complex multi-step tasks (especially when breadth of model comparisons is large), failures in open-source LLMs to converge on correct code (frequent coding mistakes), and occasional misinterpretation of correct analyses. Token limits and overly large exploratory outputs can also cause steps to fail. The system is currently limited to text/table outputs and hypothesis-testing workflows and cannot autonomously pursue follow-up questions.",
            "success_factors": "Factors aiding success: well-annotated input datasets and explicit, detailed research goals (fixed-goal modality), tight control of information flow between steps (provided prior products), multi-level rule-based checks and guardrails for code, LLM-based reviewer iterations, use of higher-quality LLMs (ChatGPT/gpt-4), iterative prompt engineering for each step, and optional human co-piloting to supply brief, targeted review comments during coding steps.",
            "comparative_results": "Comparisons show strong dependence of success on task breadth and on LLM choice: (i) For the ML model development benchmark, broad multi-model comparison produced ~90% failure; narrowing the scope reduced error to 10–20%. (ii) Open-source LLMs (Llama 2, CodeLlama) produced frequent mistakes and often could not complete coding steps or propose valid goals in pilot runs, whereas ChatGPT models allowed successful end-to-end cycles. (iii) Autopilot mode performs well for simple tasks (~80–90% success) but degrades with complexity unless human co-piloting is added; co-piloting restored high reliability even for complex tasks.",
            "human_baseline": "Human baseline is implicit: the platform was benchmarked by reproducing existing peer-reviewed studies (Saint-Fleur et al. and Shim et al.). For the NICU policy reproduction, data-to-paper matched the original study's analyses in method or provided valid alternatives in most runs; however, 2/10 runs had interpretation errors. Humans remain critical for complex tasks and ethical judgments; adding minimal human review (2–3 concise comments) achieved reliable results comparable to expected human-quality outputs for the case studies.",
            "uuid": "e2436.0",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI conversational models: gpt-3.5-turbo, gpt-4)",
            "brief_description": "Conversational large language models from OpenAI used as the primary LLM backend for data-to-paper; provided capabilities for natural-language reasoning, code generation, and iterative multi-agent interactions enabling end-to-end automated research cycles.",
            "citation_title": "ChatGPT (OpenAI conversational models used in this paper: gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-4)",
            "mention_or_use": "use",
            "system_name": "ChatGPT (gpt-3.5/gpt-4)",
            "system_description": "Used as the LLM performer and reviewer agents throughout data-to-paper. Different ChatGPT variants were assigned to steps based on expected conversation length and difficulty. The system also automatically upgraded to gpt-4 when rule-based feedback wasn't resolved or to gpt-3.5-turbo-16k to handle token limits.",
            "system_type": "Large Language Model / LLM-based agent",
            "problem_domain": "General-purpose language and code tasks within automated research workflows: goal generation, literature query formulation, Python code generation and debugging, results interpretation, and paper writing.",
            "problem_description": "Served as the engine to (i) propose research goals (open-goal runs), (ii) generate Python code for data exploration and analysis, (iii) craft literature-search queries, (iv) write manuscript sections and format LaTeX, and (v) participate in role-inverted reviewer conversations.",
            "problem_complexity": "Handled tasks from straightforward statistical analyses (linear/logistic regression) to more complex multi-model ML development tasks; complexity influenced by prompt detail and breadth of requested analyses.",
            "data_availability": "Not applicable (LLM component).",
            "computational_requirements": "Invocations of ChatGPT models constitute the bulk of API calls; end-to-end runs completed in about an hour, with dynamic model switching when needed. Exact compute/costs not provided.",
            "problem_structure": "Nondeterministic generation; step-wise conversational API usage with strict formatting expectations and rule-based checks to extract structured products.",
            "success_metric": "Ability to generate code that passes rule-based static/runtime/output checks, produce accurate tables, and write interpretable manuscript text matching numeric outputs.",
            "success_rate": "When used as the platform backbone, ChatGPT enabled successful completion of end-to-end research cycles in a majority of simple cases (~80–90%).",
            "failure_modes": "Hallucinations in goal generation and occasional misinterpretations despite correct analyses; variability across runs due to nondeterminism.",
            "success_factors": "Higher-quality reasoning and code generation relative to tested open-source models; ability to be instructed with specific system prompts and to participate in multi-agent reviewer loops.",
            "comparative_results": "Paper reports ChatGPT performed substantially better than tested open-source models (Llama 2, CodeLlama) which had frequent mistakes preventing full cycles. ChatGPT was chosen as the practical LLM backbone.",
            "human_baseline": "N/A (LLM component) but integrated with human oversight in co-pilot mode leading to improvements.",
            "uuid": "e2436.1",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama 2",
            "name_full": "Llama 2 (Meta Open Foundation model)",
            "brief_description": "An open-source foundation and chat model family evaluated as an alternative LLM backend for data-to-paper; found to produce frequent mistakes in goal selection and data analysis coding steps within this implementation.",
            "citation_title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
            "mention_or_use": "use",
            "system_name": "Llama 2",
            "system_description": "Tested in two critical research steps (research goal generation and data analysis coding) using the Health Indicators dataset; responses were manually annotated to assess hallucinated vs. real dataset features and the number of programmatic feedback rounds required in coding.",
            "system_type": "Open-source LLM",
            "problem_domain": "Evaluated on research-goal generation and data-analysis code generation within automated research workflows.",
            "problem_description": "Attempted to (i) propose valid research goals from dataset context and (ii) generate analyzable Python code for data-analysis steps.",
            "problem_complexity": "Same tasks as ChatGPT tests; found less reliable, generating hallucinated dataset features and code with frequent issues.",
            "data_availability": "N/A.",
            "computational_requirements": "Tested with 10 runs for goal step and 10 runs for data-analysis step (per paper), but frequent failures impeded completion of full cycles; exact compute not specified.",
            "problem_structure": "Nondeterministic LLM behavior; lacked reliable code generation quality for this application in the paper's experiments.",
            "success_metric": "Manual annotation of proposed goals and number of programmatic feedback rounds until code passed rule-based checks.",
            "success_rate": "Reported qualitatively as poor: open-source LLMs (including Llama 2) produced frequent mistakes and could not consistently converge on correct coding solutions; no numeric success rate provided in main text.",
            "failure_modes": "Frequent hallucinations in goal generation and coding mistakes that prevented passing static/runtime/output checks. Often required more iterations and still failed to converge.",
            "success_factors": "Noted that Llama 2 could be improved with prompt engineering but in these experiments was less reliable than ChatGPT.",
            "comparative_results": "Compared to ChatGPT, Llama 2 performance was inferior for both goal selection and data analysis coding, leading authors to use ChatGPT as the practical backend.",
            "human_baseline": "N/A",
            "uuid": "e2436.2",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CodeLlama",
            "name_full": "CodeLlama (Code-focused Llama variant)",
            "brief_description": "An open-source code-specialized LLM evaluated for data-to-paper's coding steps; found to make frequent coding mistakes preventing reliable completion of analysis steps in this work.",
            "citation_title": "Code Llama: Open Foundation Models for Code",
            "mention_or_use": "use",
            "system_name": "CodeLlama",
            "system_description": "Evaluated on the data analysis coding step (10 runs) and compared to ChatGPT and other LLMs by measuring number of programmatic feedback rounds until code passed rule-based checks and whether the model converged to valid code.",
            "system_type": "Open-source code-focused LLM",
            "problem_domain": "Python code generation and debugging for data analysis in automated research.",
            "problem_description": "Generate runnable, correct Python code for data exploration and statistical analysis that passes static and runtime guardrails.",
            "problem_complexity": "Same as coding tasks for other LLMs; required handling of dataframes, regression analyses, output file creation and formatting.",
            "data_availability": "N/A",
            "computational_requirements": "10-run evaluation for coding step; frequent failures increased iteration count but exact compute/time not quantified.",
            "problem_structure": "Deterministic execution environment but with nondeterministic model outputs; required adherence to strict formatting and coding conventions.",
            "success_metric": "Number of feedback rounds to pass rule-based checks; ultimate ability to create correct output files.",
            "success_rate": "Reported qualitatively as insufficient for reliable end-to-end cycles—frequent mistakes precluded using open-source models to complete full research runs.",
            "failure_modes": "Common ChatGPT-like coding errors, package misuse, runtime errors, incorrect file outputs—insufficient to pass guardrails consistently.",
            "success_factors": "Model specialization for code is beneficial in concept, but in practice within this study CodeLlama produced too many errors without additional engineering.",
            "comparative_results": "Inferior to ChatGPT in this deployment; authors used ChatGPT to achieve successful runs.",
            "human_baseline": "N/A",
            "uuid": "e2436.3",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "brief_description": "A multi-agent conversation framework for LLM applications cited as prior work on multi-agent orchestration; mentioned as inspiration/background for multi-agent interactions in data-to-paper.",
            "citation_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "mention_or_use": "mention",
            "system_name": "AutoGen",
            "system_description": "Referenced as an approach that enables multi-agent LLM conversations; the paper uses multi-agent (Performer/Reviewer) conversational patterns similar in spirit to AutoGen.",
            "system_type": "Multi-agent LLM conversation framework",
            "problem_domain": "General LLM application orchestration (cited broadly; no experiment in this paper).",
            "problem_description": "Framework for structuring multi-agent dialogues among LLM instances to tackle complex tasks.",
            "problem_complexity": "Not detailed in this paper.",
            "data_availability": "Not applicable here.",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Multi-agent, conversational orchestration; cited as related work informing design decisions.",
            "success_metric": "Not provided in this paper.",
            "success_rate": "Not provided in this paper.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Influential multi-agent paradigm applied conceptually in data-to-paper's Performer/Reviewer architecture.",
            "comparative_results": "No direct empirical comparison provided in this paper.",
            "human_baseline": "N/A",
            "uuid": "e2436.4",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "brief_description": "A referenced multi-agent collaborative framework for LLM-based programming and task decomposition; cited as related work in multi-agent systems for complex tasks.",
            "citation_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "Cited as an example of multi-agent frameworks guiding LLMs to collaborate on software development and complex tasks; data-to-paper leverages analogous multi-agent interactions.",
            "system_type": "Multi-agent collaborative framework for LLMs",
            "problem_domain": "Software development and multi-step tasks (citation only).",
            "problem_description": "Not experimentally used in this paper; mentioned as related work.",
            "problem_complexity": "Not specified here.",
            "data_availability": "N/A",
            "computational_requirements": "N/A",
            "problem_structure": "Multi-agent decomposition of tasks (conceptual relevance).",
            "success_metric": "Not provided in this paper.",
            "success_rate": "Not provided in this paper.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Multi-agent division of labor and role specialization (inferred from citation context).",
            "comparative_results": "No empirical comparison in this paper.",
            "human_baseline": "N/A",
            "uuid": "e2436.5",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "AutoGPT",
            "name_full": "AutoGPT",
            "brief_description": "An autonomous agent framework cited as part of prior work on automated LLM agent systems; referenced as background for iterative autonomous prompting and multi-agent automation.",
            "citation_title": "AutoGPT",
            "mention_or_use": "mention",
            "system_name": "AutoGPT",
            "system_description": "Cited as an example of autonomous agent tooling that automates iterative prompting and multi-step workflows. The authors reference such autonomous-agent approaches when motivating data-to-paper's multi-step automated research process.",
            "system_type": "Autonomous agent orchestration toolkit",
            "problem_domain": "General autonomous workflows (citation only).",
            "problem_description": "Not directly used in experiments; mentioned as related work.",
            "problem_complexity": "Not detailed in this paper.",
            "data_availability": "N/A",
            "computational_requirements": "N/A",
            "problem_structure": "Autonomous, iterative prompting; conceptual relevance.",
            "success_metric": "Not provided here.",
            "success_rate": "Not provided here.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Iterative automation and tool augmentation (contextual relevance).",
            "comparative_results": "None in this paper.",
            "human_baseline": "N/A",
            "uuid": "e2436.6",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Autonomous chemical LLM research (Boiko et al.)",
            "name_full": "Autonomous chemical research with large language models (Boiko et al., Nature 624, 2023)",
            "brief_description": "A referenced work demonstrating autonomous chemical research using large language models; cited by the authors as an example where LLMs designed and ran experiments in chemistry.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_name": "Autonomous chemical research with LLMs (Boiko et al.)",
            "system_description": "Mentioned as recent work where LLMs were used to design and run experiments in chemistry; cited to support that LLMs have demonstrated capacities for experimental design and execution.",
            "system_type": "Automated experimentation platform (domain-specific, chemistry)",
            "problem_domain": "Chemistry / autonomous experimental design and execution (referenced only).",
            "problem_description": "Not detailed in this paper beyond citation; cited as prior demonstration of LLMs designing and running experiments and performing scientific tasks.",
            "problem_complexity": "Not specified here.",
            "data_availability": "N/A",
            "computational_requirements": "N/A",
            "problem_structure": "Domain-specific experimental workflows (citation).",
            "success_metric": "Not provided in this paper.",
            "success_rate": "Not provided in this paper.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Demonstrates domain-specific adaptation of LLM-driven automation (inferred from citation).",
            "comparative_results": "Not compared empirically here.",
            "human_baseline": "N/A",
            "uuid": "e2436.7",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "The Automatic Statistician",
            "name_full": "The Automatic Statistician (Steinruecken et al., chapter in Automated Machine Learning: Methods, Systems, Challenges)",
            "brief_description": "An earlier automated data-analysis system referenced as background for algorithmic data exploration and automated statistical inference; serves as a historical precedent to LLM-driven automation.",
            "citation_title": "The Automatic Statistician",
            "mention_or_use": "mention",
            "system_name": "The Automatic Statistician",
            "system_description": "Referenced as a prior system that automates statistical modeling and model selection; cited to distinguish data-to-paper's LLM-driven de novo insight generation from traditional algorithmic data exploration.",
            "system_type": "Automated statistical modeling / AutoML precursor",
            "problem_domain": "Automated machine learning and statistical model discovery (citation only).",
            "problem_description": "Automated discovery of statistical models and explanations from data (historical system; not used here).",
            "problem_complexity": "Historically addressed algorithmic model selection for structured data; specific complexities not enumerated in this paper.",
            "data_availability": "N/A",
            "computational_requirements": "N/A",
            "problem_structure": "Well-defined model-search with explicit evaluation; contrasted with LLM-driven creative hypothesis generation.",
            "success_metric": "Not provided here.",
            "success_rate": "Not provided here.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Algorithmic rigor and structured model search (contextual relevance).",
            "comparative_results": "Used as a point of comparison to emphasize novelty of LLM-driven de novo insight generation.",
            "human_baseline": "N/A",
            "uuid": "e2436.8",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DERA",
            "name_full": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents",
            "brief_description": "A referenced multi-agent/dialog-enabled approach for improving LLM completions; cited in context of LLMs performing diagnostic and experimental design tasks.",
            "citation_title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents",
            "mention_or_use": "mention",
            "system_name": "DERA",
            "system_description": "Cited as work that enhances LLM outputs with dialog-enabled resolving agents and as part of cited literature demonstrating LLM capabilities in diagnostic/execution roles.",
            "system_type": "Multi-agent/dialog-enabled LLM augmentation",
            "problem_domain": "General LLM completion improvement and diagnostic tasks (citation only).",
            "problem_description": "Not used here; cited as related work in LLM tool augmentation and iterative prompting.",
            "problem_complexity": "Not detailed in this paper.",
            "data_availability": "N/A",
            "computational_requirements": "N/A",
            "problem_structure": "Dialog-enabled iterative resolution of LLM outputs (conceptual relevance).",
            "success_metric": "Not provided here.",
            "success_rate": "Not provided here.",
            "failure_modes": "Not discussed here.",
            "success_factors": "Iterative dialog and resolving agents (inferred relevance).",
            "comparative_results": "No empirical comparison in this paper.",
            "human_baseline": "N/A",
            "uuid": "e2436.9",
            "source_info": {
                "paper_title": "Autonomous LLM-driven research from data to human-verifiable research papers",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
            "rating": 2,
            "sanitized_title": "autogen_enabling_nextgen_llm_applications_via_multiagent_conversation"
        },
        {
            "paper_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "AutoGPT",
            "rating": 1
        },
        {
            "paper_title": "The Automatic Statistician",
            "rating": 2,
            "sanitized_title": "the_automatic_statistician"
        },
        {
            "paper_title": "Competition-level code generation with AlphaCode",
            "rating": 1,
            "sanitized_title": "competitionlevel_code_generation_with_alphacode"
        },
        {
            "paper_title": "Towards Conversational Diagnostic AI",
            "rating": 1,
            "sanitized_title": "towards_conversational_diagnostic_ai"
        }
    ],
    "cost": 0.01834625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Autonomous LLM-driven research from data to human-verifiable research papers</h1>
<p>Tal Ifargan ${ }^{1, <em>}$, Lukas Hafner ${ }^{2, </em>}$, Maor Kern ${ }^{3}$, Ori Alcalay ${ }^{3}$ and Roy Kishony ${ }^{2,4,5}$<br>${ }^{1}$ Faculty of Data and Decision Sciences, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{2}$ Faculty of Biology, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{3}$ Epsio, Tel Aviv, Israel.<br>${ }^{4}$ Faculty of Computer Science, Technion-Israel Institute of Technology, Haifa, Israel.<br>${ }^{5}$ Faculty of Biomedical Engineering, Technion-Israel Institute of Technology, Haifa, Israel.</p>
<h2>Abstract</h2>
<p>As Al promises to accelerate scientific discovery, it remains unclear whether fully Al-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, we built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about $80-90 \%$, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. Our work thereby demonstrates a potential for Al-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.</p>
<h1>Introduction</h1>
<p>Recent advances in natural language processing have resulted in LLMs, such as ChatGPT, capable of writing text, answering questions, and generating code at a human level (1-5). Furthermore, augmenting LLMs with external tools as well as automating iterative algorithmic prompting and multi-agent interactions has enabled tackling even more complex, multi-step, tasks such as solving math problems (6-8), coding and debugging large code projects (9, 10), and creating book-long texts and scripts (11). Most recently, LLMs have even demonstrated a capacity of designing and running experimentation as well as performing clinical diagnostics (12-14). Yet, despite all these advances, scientific research, and in particular the de novo creation of insights from data, remains a stronghold of human intelligence and ingenuity (15-20). The recent advancement of AI has led to a vivid discussion on the potential and risks of their application in science (21), and to emerging guidelines, emphasizing the importance of key values including accountability, oversight and transparency, notoriously challenging in AI (22).</p>
<p>Conducting research and compiling results and conclusions into a transparent and methodologically traceable and verifiable scientific paper is a highly challenging task, involving multiple interconnected steps and requiring planning, inference, and deduction, as well as the meticulous tracing of information. While scientists may in principle follow an infinite number of creative paths towards discovery, certain conventional research paths have been established (23). In particular, such paths typically follow these almost-canonical steps: formulating and reshaping a research question in light of the literature, designing and executing a research plan, interpreting the results in the context of prior knowledge, and writing a research paper. Beyond its established multi-step structure, human-driven scientific process has three additional key characteristics. First, the process is not linear, it often requires iteratively setting back to earlier steps. Second, it is built on a rigorous tracing and control of both textual and quantitative information among steps. Finally, at each of the steps, human scientists rely on feedback from peers, mentors, or external reviewers, enabling an overall collective strength beyond individual capabilities. Taken together, these key features make science a unique process of human creativity.</p>
<p>Here, inspired by how research is conducted by human scientists, we build data-to-paper, an automation platform that systematically guides multiple LLM and rule-based algorithmic agents through the conventional steps of scientific research, with automated feedback, iterative cycles of review and revision, and with structured control and tracing of information flow among these research steps. We specifically focus on a relatively simple and well-defined process of hypothesis-testing research. Starting with a human-provided dataset,</p>
<p>the process is designed to raise hypotheses, write, debug and execute code to analyze the data and perform statistical tests, interpret the results and write well-structured scientific papers which not only describe results and conclusions but also transparently delineate the research methodologies, allowing human scientists to understand, repeat and verify the analysis. The discussion on emerging guidelines for Al-driven science (22) have served as a design framework for data-to-paper, yielding a fully transparent, traceable and verifiable workflow, and algorithmic "chaining" of data, methodology and result allowing to trace downstream results back to the part of code which generated them. The system can run with or without a predefined research goal (fixed/open-goal modalities) and with or without human interactions and feedback (copilot/autopilot modes). We performed two open-goal and two fixed-goal case studies on different public datasets (24-27) and evaluated the Al-driven research process as well as the novelty and accuracy of created scientific papers. We show that, running fully autonomously (autopilot), data-to-paper can perform complete and correct run cycles for simple goals, while for complex goals, human co-piloting becomes critical.</p>
<h1>Implementation</h1>
<p>To autonomously analyze a provided dataset and create a research paper, data-to-paper guides multiple LLM and rule-based agents through a series of pre-defined "research steps", each designed to create well-defined quantitative or textual "research products" (Fig. 1). The process includes the following steps: data exploration, literature search and iterative formulation of a research goal and hypothesis, creating a hypothesis testing plan, writing data analysis code, creating scientific tables, searching related literature, and writing the paper section by section (Fig. 1A; Fig. 1B, top; in total 17 steps). The research goal can also be provided as human input, in which case the goal-determining steps are skipped ("Fixed-goal modality", dashed bypass arrow, Fig. 1A; Methods). The process runs automatically through the series of steps (with human overseeing and approval; Methods), with each step creating one or more research products, of different types ("Free text", "LaTex text", "Structured text", "Binary decision", "Citations", "Python code" and "Numerical data"; Fig. 1B, left). In coding steps, the LLM creates a Python code, which is then executed by data-to-paper to analyze the provided dataset and create numerical data products (like tables for the paper; Methods). In literature-search steps, a structured list of queries created by the LLM is used to retrieve a list of citations from an external citation database (28) (Fig. 1A; Methods). Ultimately, these intermediate products are automatically assembled into a complete research paper (labeled with an Al-created watermark for transparency; Fig. 1B; Methods).</p>
<p>Each research step is implemented as a distinct conversation, with agent identity specification, provision of prior research products, mission instructions, and LLM responses with iterative feedback (Fig. 1C; Methods; Fig. S1). First, the LLM agent is designated a specific identity (Methods; e.g. "You are a scientist who needs to write literature search queries"; "Performer system prompt", Table S1). Next, data-to-paper populates the conversation with a set of "provided prior products": a list of messages providing the LLM with a pre-defined subset of research products of prior steps, deemed important for the focal task (Fig. 1B; Fig. S1; "Provided prior products", Table S1). This rigor control of information flow among steps minimizes possible hallucinations due to mixing relevant and irrelevant information (29). It also allows data-to-paper to trace, verify and chain the sources of numeric results cited by the LLM (Methods). Next, a step-specific "mission prompt" message is appended, defining the new product that the LLM is expected to create (e.g. "Please write literature-search queries..."; "Performer mission prompt", Table S1). Then, data-to-paper requests a response from the LLM model API (30-32), from which it extracts the requested product (based on defined formatting; Fig. S1; Table S1; Methods). The extracted product then undergoes a series of rule-based algorithmic checks, providing constructive feedback to the LLM upon failure (Methods; Fig. 1C; Fig. S1). In particular, to minimize errors in the coding steps, we have built a unique framework that imposes guardrails against commonly observed coding and statistics analysis errors, through a series of static code checks, runtime errors, package-specific guardrails, and output verifications (Fig. 1A, "Coding" block; Methods).</p>
<p>Once the created product passes rule-based review, it may further be refined through LLM review (9, 33-38) (steps with "Review" ellipses, Fig. 1A; Methods). LLM review is implemented as a parallel, role-inverted conversation, effectively creating an exchange between two LLM agents (Methods; Fig. 1C; Fig. 2A; Fig. S2). In co-pilot mode, the human user can provide additional review comments, resulting in further LLM iterations (Methods). Once a product passes rule-based, LLM and optionally a human review, the step is concluded and data-to-paper proceeds to the next step, until all products are created and the paper is assembled. While data-to-paper can work with any LLM, in practice, our implementation uses ChatGPT; using the current state-of-the-art open-source LLMs leads to frequent mistakes that preclude completing full research cycles (Methods; Table S2; Fig. S3). Of note, since ChatGPT is not a deterministic model, each run of data-to-paper, even on the same dataset and either with or without a human-provided goal, unfolds with different analyses, yielding different overall manuscripts.</p>
<h1>Open-goal research on public datasets</h1>
<p>Running in an open-goal, autopilot modality, we provided data-to-paper with two publicly available datasets: (i) "Health Indicators" dataset (24), an unweighted curated subset of the CDC's Behavioral Risk Factor Surveillance System (BRFSS) from 2015 (39), with 253,680 clean responses, each including 22 features related to diabetes and general health, and (ii) "Social Network" dataset (25), a directed graph representing Twitter interactions among members of the 117th US congress, as well as member affiliations (Chamber, Party and State). For each of these two datasets, we ran data-to-paper for 5 full research cycles, creating 10 distinct manuscripts (Supplementary Dataset A,B; Supplementary Data Descriptions A,B; Supplementary Manuscripts A1-5, B1-5). During these research cycles, which took about an hour each, data-to-paper generated and corrected hypotheses, created and debugged code, composed search queries and retrieved citations, and wrote and revised the manuscript section by section (full conversations in Supplementary Runs A1-5, B1-5; Fig. 2B; Figs. S4-S7). All created manuscripts properly followed the canonical structure of a research paper, including a proper title and abstract, a well-formulated introduction that stresses the research questions in light of relevant literature, a method section providing a transparent and human-traceable description of the analysis and key methodologies, several supplementary sections providing all custom-written codes, properly formatted scientific tables, a results section which describes the findings while properly referring to each of the tables, and a referenced discussion section which summarizes the results, delineate limitations and puts the findings in a broader context (Supplementary Manuscripts A1-5, B1-5). While similar in structure, the 5 different papers produced for each dataset addressed different topics and raised and tested different hypotheses (Table 1, Table S3). These papers are not highly creative, yet they do define a reasonable set of hypotheses, test them with simple straightforward statistical approaches, and ultimately create and adequately report de novo insights from the provided data.</p>
<p>Manually vetting the data analysis and the text of these papers, we found that out of these 10 open-goal papers, 8 reported correct analysis with only minor wording imperfections, yet 2 were erroneous, showing fundamental analysis or interpretation mistakes (Supplementary Manuscripts A1-5, B1-5). The analyses in all 5 "Health Indicators" papers were based on either logistic or linear regression models, all adequately performed while accounting for a reasonable choice of confounding factors (Table S4). Furthermore, interaction terms were adequately added when needed, and the dataset was adequately restricted to reflect the tested hypotheses (restricting to the diabetic sub-population; Table S4). For the "Social Network" dataset, papers were based on linking graph properties with node properties, as</p>
<p>well as on creating new node properties (e.g. State representation size), and then applying linear regression, ANOVA, or Chi-square on either the graph nodes or edges as appropriate (Table S4; see methods sections and analysis codes in each of the created papers, Supplementary Manuscripts A1-5, B1-5). In all 10 papers, the generated scientific tables correctly represented the results of the analysis. Vetting the text, we observed that data-to-paper is adequately interpreting the analysis results with factual statements, correctly referring to tables and citing key numeric values from the analysis, and reasonably describing the research question and findings in the context of existing literature (green highlights, Supplementary Manuscripts A1-5, B1-5; Methods). We also detected multiple imperfections, such as generic phrasing, overstatement of novelty, and inadequate and sometimes lacking choice of citations (yellow and orange highlights, Supplementary Manuscripts A1-5, B1-5). More major, result-affecting, mistakes were found in 2 of the 10 papers: In one of the "Health Indicators" papers, a correct analysis was misinterpreted due to hallucinations in the goal specification step, leading to conclusions beyond the scope of the analysis; and in one of the "Social Network" papers, an erroneous analysis was performed, resulting in unfounded statements on statistical associations between social interactions and party affiliations (red highlights, Supplementary Manuscript A2 and B2, respectively).</p>
<h1>Estimating reliability in reproducing peer-reviewed results</h1>
<p>To more systematically assess its error rate in autopilot mode, we applied data-to-paper in a fixed-goal modality in two case studies for which we have benchmarks of published peer-reviewed results. We specifically wanted to check two critical aspects for the reliability of analysis and interpretation: the proper reporting of both positive and negative findings (challenge 1), and the performance for tasks with multiple different steps with tunable breadth (challenge 2). To test data-to-paper capacity in these two challenges, we chose the following two examples of peer-reviewed studies: a study by Saint-Fleur et al. (26), which adequately reports both positive and negative findings related to the association of a policy change in a Neonatal Intensive Care Unit with treatment choice and treatment outcome, respectively (challenge 1); and a study by Shim et al. (27), which builds several Machine Learning models for predicting optimal intubation depth in pediatric patients, and compare their prediction accuracy with formula-based models, thereby requiring multiple analysis steps, whose breadths can be gradually tuned (by altering the number of models to compare; challenge 2). Both studies provide well-annotated datasets and both were published after the knowledge cutoff date of the ChatGPT models that we used (September 2021; Methods). For each of the two case studies, we have provided data-to-paper with the</p>
<p>research goal of the original publication and the corresponding dataset and ran it for 10 independent research cycles (Supplementary Data Descriptions C,D; Supplementary Datasets C,D; Tables S5,S6; Supplementary Manuscripts C1-10, Da1-10). Within each case study, the created papers were all similar to each other in their content, terminology and structure. Indeed, quantifying content similarity by the pairwise cosine distance between the vector embeddings of the title and abstract of all created manuscripts (40) showed tight and distinct clusters corresponding to the 4 case studies (two open-goal and two fixed-goal; Fig. 3). Furthermore, the fixed-goal papers were also similar to their respective original studies $(26,27)$ in content, terminology and in their vector embeddings (Fig. 3).</p>
<p>We manually vetted the analysis and reported results of the manuscripts created for each of the two study-reproducing challenges. For challenge 1, we found that all papers correctly reproduced the analysis, and 8 of them reached the overall correct conclusions and adequately reported both the negative and positive results. All of these manuscripts used adequate statistical methodologies, either matching the methods used in the original study (26) or providing valid alternatives (Table S5; Supplementary Manuscripts C1-10, Supplementary Runs C1-10). Yet, despite correct analysis, in 2 out of these 10 papers we identified interpretation errors, which in one of the papers also affected the overall conclusions (Fig. 4; Supplementary Manuscripts C1,2, red and orange highlights; Tables S5,S6). In challenge 2, we found that the rate of error critically varied with the breadth of the analysis; while data-to-paper frequently failed when presented with the original, broad research goal ( $90 \%$ error rate), it was able to correctly perform this multi-step model development research for almost identical research goals except for requesting fewer models (10-20\% error rate; Fig. 4). We note that as the breadth of the task increases, the number of iterations required to complete the Data Analysis step increases, providing a potential possibility to alert of too complex analysis and difficult goals (Fig. 4, bottom). We further note that in all cases, the process reliability depends on the formulation of the research goal and the description of the dataset; less detailed and explicit formulations can increase analysis errors (Fig. S8; Supplementary Manuscripts Dai1-10, Dbi1-10, Dci1-10 and Data Descriptions Dai, Dbi, Dci). Finally, allowing human co-piloting (Methods), a 2-3 single-sentence review comments per run, typically in the code writing step, allows creating accurate papers consistently even for the more complex goals (Fig. 4; Supplementary Human Co-piloted Manuscripts 1-3). Altogether, these case studies provide an assessment of data-to-paper's analysis and interpretation reliability, showing that for simple research goals it can autonomously create reliable manuscripts in $80-90 \%$ of the cases, and that for more complex goals human-copiloting is critical to assure reliability.</p>
<p>Finally, noting the effort and necessity of manually vetting and verifying created manuscripts, we harnessed data-to-paper step-to-step information tracing to chain results, methodology and data in created manuscripts through algorithmically verified hyperlinks (Methods). This approach creates manuscripts in which all cited numeric values are recursively linked to the specific lines of code where they are created. In particular, numbers cited in the manuscript are linked to a "Notes" appendix providing their formula and its explanation, and from there to the specific table where values used in these formula have originated from, and from the table to the corresponding output file of the code from which the table was created, and from there to the very specific part of code which produced this output file (see clickable hyperlinks in Supplementary Data-chained Manuscripts A-D). Such data-chained manuscripts facilitate systematic vetting of papers, setting a new standard for traceability for the coming era of Al-powered research.</p>
<h1>Discussion</h1>
<p>Inspired by key features of human research, we use prompt automation, tool augmentation, and multi-agent interaction approaches $(9,12,33)$ to guide multiple LLM agents through a full research path leading from annotated data to well-structured transparent, human-verifiable papers. Tracing information through the different research steps allows data-to-paper to create "data-chained" manuscripts, where results, methodologies and data are programmatically linked. While the novelty of this Al-driven research falls well behind high-end contemporary science, it did demonstrate a de novo creation of new insights from provided data, thereby recapitulating a key aspect of human research and taking science automation well beyond what is possible with algorithmic data exploration (41). Furthermore, the process demonstrates versatility with respect to data types and research domains, and is able to produce different forms of scientific output, such as association studies, network analysis, or development and testing of machine learning models. Run fully autonomously, the process however is not error-free; despite minimizing errors with multiple guardrails, algorithmic checks, review cycles, and tight control of information flow, the notorious problem of LLM hallucinations (29) leads to fundamental errors in about 10 to 20 percent of created papers, for simple analysis tasks, and to consistent failure for more complex tasks. Integrating human co-piloting, few short review comments were sufficient to overcome errors even for complex tasks. Our current implementation has multiple constraints: it is limited to textual and table outputs, is unable to formulate and pursue follow-up questions, and is limited to hypothesis-testing research.</p>
<p>Despite these current limitations, the ability of LLMs to carry out scientific research presents important opportunities, but also major challenges. Indeed, such AI research approaches, capable of creating de novo research papers from data in just an hour, could dramatically accelerate the pace of the scientific process. However, there are also risks associated with this development, such as the dishonest use of such systems, e.g. in the context of $P$-hacking (42), or overloading the publication system with medium-level and generic manuscripts addressing insignificant problems (43-45). Our approach implements specific features to mitigate some of these risks, in line with emerging guidelines on Al in science (22), including a transparent, overseeable process allowing human co-piloting, unbiased reporting of either positive or negative results, and the creation of transparent, Al-marked, "data-chained" and human-verifiable papers. Given the relatively limited novelty and the potential for errors in Al-driven research, as well as the need for ethical judgments and accountability (22), we anticipate and urge that such Al-driven approaches will used as scientist co-pilots, helping scientists in the more straightforward tasks, thereby allowing them to focus their minds and creativity on higher-level concepts.</p>
<h1>Methods</h1>
<p>Datasets. We used 4 datasets, each consisting of data files ("Data", Fig. 1B; Supplementary Datasets A-D) and metadata items (the human-provided products "Data file description" and "General description of dataset", Fig. 1B; Supplementary Data Descriptions A-D). (A) "Health Indicators" dataset (24). A clean unweighted subset of CDC's Behavioral Risk Factor Surveillance System (BRFSS) 2015 annual dataset (39), downloaded from Kaggle (24). It contains 253,680 survey responses each with 22 features related to diabetes and different health indicators, with no missing values. No change in the dataset was made; data-to-paper was provided with the csv file as downloaded from Kaggle. (B) "Social Network" dataset (25). A directed graph of Twitter interactions among the 117th Congress members (25). Two data files were provided to data-to-paper: (i) a csv file containing a list of directed unweighted edges, representing Twitter engagements among Congress members (downloaded from Stanford Network Analysis Project (46), with the weights removed), and (ii) a csv file containing the affiliations of each Congress member, including their Chamber, Party and State (downloaded from FRAC (47)). (C) "Treatment policy" dataset (a test case to reproduce Saint-Fleur et al. (26)). A dataset on treatment and outcomes of non-vigorous infants admitted to the Neonatal Intensive Care Unit (NICU), before and after a change to treatment guidelines was implemented. As input to data-to-paper, the file downloaded from Saint-Fleur et al. (26) was converted into a csv file, with minor cleanups: converting column headers into alphanumeric names, converting string binary into integer binary, and removing</p>
<p>the following irrelevant columns: 'RACE', 'RACE IN TWO CATEGORIES', 'ETHNICITY', 'Singleton /Multiple', 'Maternal Diabetes...', 'PRETERM VS TERM', 'ROUTINE RESUSCITATION...', 'Respiratory Support', 'Exposure to xrays', 'X-Ray finding' (without removing these columns, the "Data exploration" step of data-to-paper occasionally created too large output files leading to breaking the token limit of ChatGPT). (D) "Treatment Optimization" dataset (a test case to reproduce Shim et al. (27)). A dataset of 967 pediatric patients, which received mechanical ventilation after undergoing surgery, including an x-ray-based determination of the optimal tracheal tube intubation depth and a set of personalized patient attributes to be used in machine learning and formula-based models to predict this optimal depth. As input for data-to-paper, we removed irrelevant columns, leaving only the ones used in the original study: 'tube', 'sex', 'age_c', 'ht', 'wt', 'tube_depth_G'. For datasets C and D, we further provided data-to-paper with the research goal of their respective original studies. Research goals and dataset descriptions have been formulated in an iterative and empirical process: We consulted with ChatGPT on best phrasing and terminologies, tested them in pilot runs, identified misunderstandings and vague or ill-defined statements, and adapted the descriptions accordingly. Dataset descriptions and file descriptions for all datasets, as well as the research goal for datasets C,D, are provided in Supplementary Data Descriptions A-D.</p>
<p>Execution of data-to-paper. For each run, data-to-paper is provided with a dataset, its associated metadata, and an optional research goal and proceeds automatically through the stepwise research process (Fig. 1A,B). In open-goal modality, data-to-paper runs through the entire research process (Fig. 1A,B). In fixed-goal modality, the research goal is provided and the steps for choosing a research goal are skipped ("Fixed-goal modality", Fig. 1A). Human interactions are implemented as a simple user approval at each research step (autopilot mode; user is only overseeing) or with complete human review through an interactive app (co-pilot mode; user can provide reviewing comments at each step). For each dataset, we performed multiple data-to-paper runs, as follows. (A) "Health Indicators" dataset. We ran data-to-paper in an open-goal modality with this dataset and its associated metadata for 5 full research cycles (Supplementary Runs and Manuscripts A1-5). Overseeing the process, we aborted and restarted the 5th run 3 times after the "Goal validation" step, when observing that the chosen Research goal was too similar to goals of prior research cycles. (B) "Social Network" dataset. We ran data-to-paper in an open-goal modality with this dataset and its associated metadata for 5 full research cycles (Supplementary Runs and Manuscripts B1-5). To minimize overlapping goals in repeated runs, a list of the already-chosen previous goals was presented as part of the "mission prompt" of the "Research goal" step. (C) "Treatment Policy" dataset. We ran data-to-paper in</p>
<p>a fixed-goal modality for 10 research cycles with this dataset and its associated metadata and research goal (Supplementary Runs and Manuscripts C1-10). (D) "Treatment Optimization" dataset. We ran data-to-paper in a fixed-goal modality for 10 full research cycles with this dataset and its associated metadata and research goal (Supplementary Runs and Manuscripts Da1-10). We then ran data-to-paper with 5 modified research goals (Supplementary Data Descriptions Db, Dc, Dai, Dbi, Dci) for 10 times per goal (Supplementary Runs and Manuscripts Db1-10, Dc1-10, Dai1-10, Dbi1-10, Dci1-10). As these additional 50 runs were only used to annotate analysis failure, we terminated them after the "Title \&amp; abstract" step (to save unnecessary api calls). In addition, we ran data-to-paper in co-pilot mode for three times on the original goal (Supplementary Data Description Da). During each of these runs, we provided several review comments, typically in the code writing step (Supplementary Human Co-piloted runs 1-3).</p>
<p>Overview of data-to-paper implementation. We implement data-to-paper as a chained list of research steps, each designed to create one or more research products based on a provided subset of prior research products (Fig. 1A,B). Each such research step is implemented as a distinct "Performer conversation", which specifies LLM identity, relevant prior research products and a step-specific "mission prompt" requesting the LLM to create a focal product. Product extracted from the LLM response undergoes rule-based review and programmatic feedback requesting corrections is sent back to the LLM. For certain research steps, once the product passes rule-based review it can also be sent for LLM review, which is implemented in a parallel "Reviewer conversation" ("Review", "LLM reviewer agent" in Fig. 1A,C respectively). The research step terminates with a final product that has passed both rule-based and LLM-based review. Once all steps are completed, a manuscript is assembled and compiled from the products of all relevant steps.</p>
<p>Devising prompts. The prompts used by data-to-paper in each of the research steps are listed in Table S1. These prompts have been designed in an iterative and empirical process. First, we devised an initial version for each of the prompts, focusing on the key aspect of their focal task (dark brown text, Table S1). Additionally, we added to each prompt formatting instructions for the research product (light blue and red text, Table S1). Then, we tested ChatGPT responses through multiple pilot runs, identified wrong or inadequate responses, and adapted the prompts with additional details and specifications (light brown text, Table S1). In cases where ChatGPT still failed to consistently respond as expected, we also added one-shot examples (green text, Table S1).</p>
<p>Message types. Messages in a conversation are designated as either SYSTEM, USER, or ASSISTANT (per OpenAI API terminology (30)). SYSTEM and USER messages are</p>
<p>programmatically composed by data-to-paper. ASSISTANT messages are created by the LLM. We also implement LLM-surrogating ASSISTANT messages, which are messages created programmatically by data-to-paper, but are attributed to the ASSISTANT (namely, they appear to the LLM as if they were created by it).</p>
<p>Performer conversation. At the onset of each research step, a distinct Performer conversation is initiated and programmatically pre-filled with a list of "context messages": (i) "system prompt" defining the identity of the performer LLM agent ("Performer system prompt", Table S1); (ii) "provided prior products", a list of USER messages providing the LLM with a pre-defined subset of research products of prior steps, with each such USER message followed by an LLM-surrogating acknowledgment message (Fig. 1B, Figs. S1,S4; "Provided prior products", Table S1); and (iii) USER message describing to the LLM what it is requested to do in the current step ("Performer mission prompt", Table S1). This pre-filled Performer conversation is then sent to the LLM API (30-32) to request an initial response (Figs. S1,S4). The requested research product is then extracted from this initial LLM response and undergoes rule-based product review.</p>
<p>Rule-based product review. At each research step, the LLM is requested to send a response containing a specific product, with specific formatting (Fig. 1B; Tables S1,S7). Then, data-to-paper extracts the requested product from the LLM response based on its expected formatting (Tables S1,S6; for example, when requesting a "LaTex text" product, we expect the product to be enclosed within triple backticks). Failure to extract the product is translated into a feedback message sent back to the LLM (for example: "You sent 2 triple-backtick blocks. Please send the latex as a single triple-backtick 'latex' block"). Once the product is extracted successfully, it is programmatically refined according to a set of step-specific auto-refinement rules (Table S7, asterisk-marked rules). Then, the refined product is checked according to a set of step-specific test rules, including formatting, text length and correct referencing (for exhaustive list see Table S7). Failure to pass any of these rules is translated into a corresponding feedback message sent back to the LLM (see example in Fig. S5).</p>
<p>Information tracing. To follow information flow through all steps, data-to-paper keeps track of the specific code lines producing each file output, the translation of these outputs into tables and the incorporation of numbers from the table in the Results section. Specifically, to track numeric results in the Results section, we programmatically assign a unique label for each numeric value appearing in the prior products for the Results writing step, and present these products in the context messages with the numeric values formatted as LaTex hypertargets with their corresponding labels (Fig. 1B). We then complemented the mission</p>
<p>prompt of the Result writing step with instructions requesting the LLM agent to wrap each numeric value that it writes with a LaTex hyperlink matching the corresponding label ("Performer mission prompt: additional instructions for data-chained manuscripts", Table S1). To allow the LLM to include numeric values which are not direct output of the code, but are rather arithmetically derived from them (like changing units, translating regression coefficients to odds ratios, etc), we further provide it with the option of using a specific syntax, \num( $&lt;$ formula&gt;, "explanation"), where it can provide arithmetic formula to derive new values from values created by the code output, and provide an explanation. A rule-based feedback was added to algorithmically verify that, either as stand-alone or within a \num formula, each numeric value mentioned in the section is hyperlinked, and that the target of each link correctly matches the corresponding label provided in the prior product context. Upon compilation, the \num commands are replaced with their value and a "Notes" appendix is added listing all formulas with their explanation. To further safeguard against hallucinated or missing values, the Results "mission prompt" instructs the LLM to use a designated placeholder (specifically '[unknown]') for missing numeric values, detection of this or other placeholder in the LLM response leads to data-to-paper aborting the entire research cycle (for the list of placeholders see "Results", Table S7).</p>
<p>Data-chained manuscripts. Reflecting the tracing of information during the "Data Analysis", "Table Design" and "Results" writing steps, data-to-paper creates manuscripts that "chain" results, methods and data, where each numeric value is recursively linked to the specific lines of codes that created it. In particular, a numeric value in the "Results" section can be linked to the "Notes" appendix, and from there to a specific value in a table, and from there to the output file that was used to create this table and finally to the specific code lines which generated this output file (Supplementary Data-chained Manuscripts A-D and Supplementary Human Co-piloted Manuscripts 1-3; Note that prior manuscripts were created without this feature and do not have hyperlinks).</p>
<p>Reviewer conversation. For a subset of research steps, data-to-paper also performs an LLM review after the successful completion of rule-based product review ("Review", Fig. 1A; Fig. 2, Figs. S2,S6). LLM review is implemented in a "Reviewer conversation", which parallels the Performer conversation of the given step, but with inversion of the USER-ASSISTANT roles (Fig. 1C; Fig. 2A; see examples in Fig. 2B, Fig. S6). In parallel to its corresponding Performer conversation, this Reviewer conversation is pre-filled with the following list of context messages: (i) "system prompt" defining the identity of the LLM reviewer agent; (ii) the list of "provided prior products" for the focal step; and (iii) An LLM-surrogating message with the "Performer mission prompt" (namely, the "Performer</p>
<p>mission prompt" is casted as an ASSISTANT-side message, thereby appearing as if it was created by the LLM reviewer agent). Then, the extracted product coming from the Performer conversation is presented as a USER-side message together with step-specific review instructions, in which the LLM reviewer agent is requested to choose between accepting the provided product, or providing constructive feedback ("Reviewer mission prompt", Table S1; Fig. 1C; Fig. 2A; Fig. S2). The pre-filled conversation is then sent to the LLM API to request a response from the Reviewer agent. If the Reviewer response contains feedback, it is transferred to the Performer conversation as if it were a USER-side message, requesting the LLM performer agent to provide a new response with an accordingly refined product.</p>
<p>Coding steps. For each of the three coding steps ("Data exploration", "Data analysis", "Table design"), we extract Python code (enclosed within a triple-backtick block) from the LLM response, and test this code at four levels: (i) Static analysis: Check that the code conforms to a step-specific requested structure ("Python code - Static checks", Table S7); (ii) Runtime analysis: Syntax errors, runtime errors, warnings, as well as violations of other restrictions are caught and evaluated during code execution ("Python code - Runtime checks", Table S7); (iii) Package-specific guardrails: Noting common ChatGPT coding mistakes, we wrapped the packages that we allow importing, adding multiple guardrails to monitor, control and restrict unsafe functionalities, as well as to allow rule-based review of p-value formatting (Table S8); (iv) Output analysis: Check that all the requested output files are created and contain the requested information with the requested formatting ("Numerical data checks", Table S7). Encountered issues from these 4 check levels are translated into a feedback message sent back to the LLM. As a new feedback message is added, older feedback-response message pairs are removed from the conversation (to avoid exceeding the token limits). Once the LLM-provided code passes all tests, we proceed to LLM product review: data-to-paper provides a message that shows the LLM the code output and asks it to check the code and the output and provide a list of issues and suggested corrections (see "Reviewer mission prompts" for "Data exploration" and "Data analysis" steps in Table S1). If the LLM returns suggestions for improvement, data-to-paper requests making these corrections and enters a new phase of code debugging as described above. If there are no suggestions for improvements, we end the coding step with the code and the output files it created as the corresponding research products.</p>
<p>Citation retrieval. For the two literature search steps ("Literature search I", "Literature search II", Fig. 1A; Table S1), data-to-paper augments the LLM with Semantic Scholar Academic Graph API (28), an external citation database and search service. This direct citation retrieval, along with algorithmic checks restricting LLM's memory-retrieved citations</p>
<p>(Rule-based product review; Table S7), ensures that only valid citations are included in the resulting paper. These literature-search steps start with a "Devise queries" step, in which the LLM is requested to provide a list of queries for each of a pre-defined set of scopes (scopes for "Literature search I": "Dataset", "Questions"; scopes for the "Literature search II": "Background", "Dataset", "Methods", "Results"; see "Literature search I" and "Literature search II" in Table S1). Then, data-to-paper calls the citation API (28) to retrieve a list of citations for each of the LLM-provided queries (see example in Fig. S7). For each citation, the API provides: (i) Search rank; (ii) BibTeX ID; (iii) Title; (iv) Journal and year; (v) One-sentence paper summary (TLDR) (48); (vi) Citation influence (49); (vii) Title and abstract embedding (40). Citations for each of the scopes are then filtered and sorted either by Search rank or by Title and abstract embedding similarity to the title and abstract embedding of the currently written paper (parameters in Table S9). For the runs with datasets C, D, where we attempt reproducing a specific original study, we manually excluded the citation of the original paper. The sorted lists of papers for each scope are then provided as prior products for steps in which the LLM is requested to refer to literature citations (Table S1; Fig. 1B, Fig. S7).</p>
<p>LLM selection. We compared the performance of Llama 2, Codellama and ChatGPT models in two critical research steps: (i) Research goal and (ii) Data analysis. For both tests, we used the "Health Indicators" dataset. In (i), we ran the research goal step of data-to-paper 10 times each either with gpt-3.5-turbo or Llama-2-70b-chat-hf, all provided with the same prior product context (Table S2). We manually annotated the goals, scoring analysis-related factors, either corresponding to true features of the dataset, or to hallucinated features not part of the dataset (Table S2, Fig. S3A). In (ii) we ran the data analysis step of data-to-paper 10 times each with either gpt-3.5-turbo, gpt-4, CodeLlama-34b-Instruct-hf, Llama-2-70b-chat-hf or Llama-2-7b-chat-hf and evaluated for each run the number of programmatic feedback rounds until the code passes rule-based review (Fig. S3B, Supplementary Coding Runs).</p>
<p>ChatGPT models and parameters. As the underlying LLM, we used OpenAI conversational ChatGPT models (30) (open-source models created hallucinated research goals and were not able to consistently converge in the data analysis coding step; LLM selection). The OpenAI models used were either gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, or gpt-4 (all with a knowledge cutoff of September 2021). For each research step, we assigned one of these specific models as a nominal model based on the expected conversation length of the step as well as the presumed difficulty and performance during pilot runs ("LLM", Table S1). Starting from this initial nominal model for each step, data-to-paper can automatically</p>
<p>upgrade the model during a conversation: switching to gpt-4 when a rule-based feedback request is not resolved, and switching to gpt-3.5-turbo-16k-0613 if the number of tokens exceeds the maximum of the step's nominal model. For all models, we use default model parameters, except for the model's temperature which was specifically set for some of the steps (In particular, setting a temperature of 1 for the "Research goal" step).</p>
<p>Paper assembly and compilation. To produce the final manuscript, data-to-paper assembles a single LaTex file, combining the different manuscript-part products ("Paper assembly", Fig. 1A,B). It then automatically compiles this file, together with the list of citations retrieved from "Literature search II", into a pdf, watermarked "Created by data-to-paper (AI)".</p>
<p>Manual review of created manuscripts. We manually vetted each created manuscript and its respective run record (Supplementary Manuscripts and Runs A1-5, B1-5, C1-10, Da1-10, Db1-10, Dc1-20, Dai1-10, Dbi1-10, Dci1-10). For the manuscripts, we verified: (i) that the data analysis and code are correctly performed, using adequate statistical methodologies; (ii) that every statement in the text involving numeric information corresponds to the correct numeric value from the output of the data analysis; (iii) that every citation fits the context in which it was referenced; (iv) the overall exactness of the text; and (v) the quality of the overall text and wording. The manuscripts were highlighted to reflect correctly-put statements (green), imperfect, or atypical statements (yellow), minor errors (orange), and major errors (red).</p>
<p>Human co-piloting. Human co-piloting is incorporated by allowing the user to add review comments in each step after the rule-based and LLM-review have completed. If such human review is added, data-to-paper initiates a new cycle of Performer answers with rule-based checks. This process repeats iteratively until the user approves the research product of the step. We have created an app with a user interface that allows the user to follow the LLM conversation in each step and add review comments as needed.</p>
<h1>Data availability</h1>
<p>The data that support the findings of this study are available in the paper and its Supplementary Information (https://github.com/rkishony/data-to-paper-supplementary).</p>
<h2>Code availability</h2>
<p>Code is available at https://github.com/Technion-Kishony-lab/data-to-paper</p>
<h1>References</h1>
<ol>
<li>Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu, X. Huang, T. Gui, The Rise and Potential of Large Language Model Based Agents: A Survey, arXiv [cs.AI] (2023). http://arxiv.org/abs/2309.07864.</li>
<li>Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He, Z. Liu, Z. Wu, L. Zhao, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, B. Ge, Summary of ChatGPT-Related research and perspective towards the future of large language models. Meta-Radiology 1, 100017 (2023).</li>
<li>L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao, Z. Wei, J.-R. Wen, A Survey on Large Language Model based Autonomous Agents, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.11432.</li>
<li>Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, T. Hubert, P. Choy, C. de Masson d'Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J. Mankowitz, E. Sutherland Robson, P. Kohli, N. de Freitas, K. Kavukcuoglu, O. Vinyals, Competition-level code generation with AlphaCode. Science 378, 1092-1097 (2022).</li>
<li>G. Spitale, N. Biller-Andorno, F. Germani, AI model GPT-3 (dis)informs us better than humans. Sci Adv 9, eadh1850 (2023).</li>
<li>J. He-Yueya, G. Poesia, R. E. Wang, N. D. Goodman, Solving Math Word Problems by Combining Language Models With Symbolic Solvers, arXiv [cs.CL] (2023). http://arxiv.org/abs/2304.09102.</li>
<li>B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli, A. Fawzi, Mathematical discoveries from program search with large language models. Nature 625, 468-475 (2024).</li>
<li>T. H. Trinh, Y. Wu, Q. V. Le, H. He, T. Luong, Solving olympiad geometry without human demonstrations. Nature 625, 476-482 (2024).</li>
<li>C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J. Xu, D. Li, Z. Liu, M. Sun, Communicative Agents for Software Development, arXiv [cs.SE] (2023). http://arxiv.org/abs/2307.07924.</li>
<li>Y. Dong, X. Jiang, Z. Jin, G. Li, Self-collaboration Code Generation via ChatGPT, arXiv [cs.SE] (2023). http://arxiv.org/abs/2304.07590.</li>
<li>W. Zhou, Y. E. Jiang, P. Cui, T. Wang, Z. Xiao, Y. Hou, R. Cotterell, M. Sachan, RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text, arXiv [cs.CL] (2023). http://arxiv.org/abs/2305.13304.</li>
<li>V. Nair, E. Schumacher, G. Tso, A. Kannan, DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents, arXiv [cs.CL] (2023). http://arxiv.org/abs/2303.17071.</li>
<li>T. Tu, A. Palepu, M. Schaekermann, K. Saab, J. Freyberg, R. Tanno, A. Wang, B. Li, M. Amin, N. Tomasev, S. Azizi, K. Singhal, Y. Cheng, L. Hou, A. Webson, K. Kulkarni, S. Sara Mahdavi, C. Semturs, J. Gottweis, J. Barral, K. Chou, G. S. Corrado, Y. Matias, A.</li>
</ol>
<p>Karthikesalingam, V. Natarajan, Towards Conversational Diagnostic AI, arXiv [cs.AI] (2024). http://arxiv.org/abs/2401.05654.
14. D. A. Boiko, R. MacKnight, B. Kline, G. Gomes, Autonomous chemical research with large language models. Nature 624, 570-578 (2023).
15. A. Birhane, A. Kasirzadeh, D. Leslie, S. Wachter, Science in the age of large language models. Nature Reviews Physics 5, 277-280 (2023).
16. G. Conroy, How ChatGPT and other AI tools could disrupt scientific publishing. Nature 622, 234-236 (2023).
17. C. Stokel-Walker, R. Van Noorden, What ChatGPT and generative AI mean for science. Nature 614, 214-216 (2023).
18. C. Stokel-Walker, ChatGPT listed as author on research papers: many scientists disapprove. Nature 613, 620-621 (2023).
19. M. Hutson, Could AI help you to write your next paper? Nature 611, 192-193 (2022).
20. V. Berdejo-Espinola, T. Amano, AI tools can improve equity in science. Science 379, 991 (2023).
21. L. Messeri, M. J. Crockett, Artificial intelligence and illusions of understanding in scientific research. Nature 627, 49-58 (2024).
22. C. L. Bockting, E. A. M. van Dis, R. van Rooij, W. Zuidema, J. Bollen, Living guidelines for generative AI - why scientists must oversee its use. Nature 622, 693-696 (2023).
23. E. B. Wilson, An Introduction to Scientific Research (Courier Corporation, 1990; https://play.google.com/store/books/details?id=rKCHDQAAQBAJ).
24. A. Teboul, Diabetes Health Indicators Dataset (2021).
https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset.
25. C. G. Fink, N. Omodt, S. Zinnecker, G. Sprint, A Congressional Twitter network dataset quantifying pairwise probability of influence. Data Brief 50, 109521 (2023).
26. A. L. Saint-Fleur, H. E. Alcalá, S. Sridhar, Outcomes of neonates born through meconium-stained amniotic fluid pre and post 2015 NRP guideline implementation. PLoS One 18, e0289945 (2023).
27. J.-G. Shim, K.-H. Ryu, S. H. Lee, E.-A. Cho, S. Lee, J. H. Ahn, Machine learning model for predicting the optimal depth of tracheal tube insertion in pediatric patients: A retrospective cohort study. PLoS One 16, e0257069 (2021).
28. R. Kinney, C. Anastasiades, R. Authur, I. Beltagy, J. Bragg, A. Buraczynski, I. Cachola, S. Candra, Y. Chandrasekhar, A. Cohan, M. Crawford, D. Downey, J. Dunkelberger, O. Etzioni, R. Evans, S. Feldman, J. Gorney, D. Graham, F. Hu, R. Huff, D. King, S. Kohlmeier, B. Kuehl, M. Langan, D. Lin, H. Liu, K. Lo, J. Lochner, K. MacMillan, T. Murray, C. Newell, S. Rao, S. Rohatgi, P. Sayre, Z. Shen, A. Singh, L. Soldaini, S. Subramanian, A. Tanaka, A. D. Wade, L. Wagner, L. L. Wang, C. Wilhelm, C. Wu, J. Yang, A. Zamarron, M. Van Zuylen, D. S. Weld, The Semantic Scholar Open Data Platform, arXiv [cs.DL] (2023). http://arxiv.org/abs/2301.10140.
29. Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang, W. Dai, A. Madotto, P. Fung, Survey of Hallucination in Natural Language Generation, arXiv [cs.CL] (2022).</p>
<p>http://arxiv.org/abs/2202.03629.
30. OpenAI platform. https://platform.openai.com/docs/api-reference.
31. H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, T. Scialom, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv [cs.CL] (2023). http://arxiv.org/abs/2307.09288.
32. B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, G. Synnaeve, Code Llama: Open Foundation Models for Code, arXiv [cs.CL] (2023). http://arxiv.org/abs/2308.12950.
33. Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, A. H. Awadallah, R. W. White, D. Burger, C. Wang, AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.08155.
34. S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, J. Schmidhuber, MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework, arXiv [cs.AI] (2023). http://arxiv.org/abs/2308.00352.
35. A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, P. Clark, Self-Refine: Iterative Refinement with Self-Feedback, arXiv [cs.CL] (2023). http://arxiv.org/abs/2303.17651.
36. C. Harrison, LangChain (2022). https://github.com/langchain-ai/langchain.
37. S. Gravitas, AutoGPT (2023). https://github.com/Significant-Gravitas/AutoGPT.
38. B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, J. Gao, Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback, arXiv [cs.CL] (2023). http://arxiv.org/abs/2302.12813.
39. L. Rolle-Lake, E. Robbins, "Behavioral Risk Factor Surveillance System" in StatPearls (StatPearls Publishing, Treasure Island (FL), 2023; https://www.ncbi.nlm.nih.gov/pubmed/31971707).
40. A. Cohan, S. Feldman, I. Beltagy, D. Downey, D. S. Weld, SPECTER: Document-level Representation Learning using Citation-informed Transformers, arXiv [cs.CL] (2020). http://arxiv.org/abs/2004.07180.
41. C. Steinruecken, E. Smith, D. Janz, J. Lloyd, Z. Ghahramani, "The Automatic Statistician" in Automated Machine Learning: Methods, Systems, Challenges, F. Hutter,</p>
<p>L. Kotthoff, J. Vanschoren, Eds. (Springer International Publishing, Cham, 2019; https://doi.org/10.1007/978-3-030-05318-5_9), pp. 161-173.
42. N. Altman, M. Krzywinski, P values and the search for significance. Nat. Methods 14, $3-4$ (2016).
43. R. Van Noorden, Hundreds of gibberish papers still lurk in the scientific literature. Nature 594, 160-161 (2021).
44. G. Cabanac, C. Labbé, Prevalence of nonsensical algorithmically generated papers in the scientific literature. J. Assoc. Inf. Sci. Technol. 72, 1461-1476 (2021).
45. L. Liverpool, Al intensifies fight against "paper mills" that churn out fake research. Nature 618, 222-223 (2023).
46. Twitter interaction network for the US congress (2023).
https://snap.stanford.edu/data/congress-twitter.html.
47. Twitter Handles for Members of the 117th Congress, Food Research \&amp; Action Center (2021). https://frac.org/wp-content/uploads/MOC_Twitter-Handles_117th.pdf.
48. I. Cachola, K. Lo, A. Cohan, D. S. Weld, TLDR: Extreme Summarization of Scientific Documents, arXiv [cs.CL] (2020). http://arxiv.org/abs/2004.15011.
49. M. Valenzuela-Escarcega, V. A. Ha, O. Etzioni, Identifying Meaningful Citations. AAAI Workshop: Scholarly Big Data (2015).</p>
<h1>Acknowledgments</h1>
<p>We thank Ayelet Baram-Tsabari and Yael Rozenblum for discussions and providing data for initial tests, Ofer Sapir for help in organizing data-to-paper repo, Eric Lander, Yoel Fink, and Michael Elowitz for insightful discussions, and all lab members for helpful comments. LH was supported in part at the Technion by an Aly Kaufman Fellowship.</p>
<h2>Contributions</h2>
<p>TI and RK conceived the study. TI and RK developed data-to-paper with inputs from LH, MK, and OA. MK and OA implemented a graphic interface for system testing. TI and LH identified and prepared the datasets and related metadata. RK and TI oversaw the autonomous research runs. TI and LH manually vetted and highlighted created papers and run files. LH conceptualized the presentation and designed the figures with inputs from TI and RK. TI, LH and RK interpreted the results and wrote the manuscript with comments from MK and OA.</p>
<h2>Competing interests</h2>
<p>The authors declare no competing interests.</p>
<h2>Corresponding authors</h2>
<p>Correspondence to Roy Kishony, rkishony@technion.ac.il</p>            </div>
        </div>

    </div>
</body>
</html>
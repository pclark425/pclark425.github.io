<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4192 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4192</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4192</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-271162102</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08940v2.pdf" target="_blank">Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses. Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery. In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators. We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination. Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings. To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework. Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments. Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance. However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated. These findings under-score</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4192.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4192.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM multi-agent framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Multi-Agent Hypothesis Generation Framework (Analyst/Engineer/Scientist/Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multi-agent system that assigns distinct scientific roles to LLM agents (Analyst, Engineer, Scientist, Critic) and optionally uses external tools (PubMed, search) to iteratively retrieve, synthesize, critique, and produce hypotheses from the literature; used to surface relationships and candidate quantitative markers from biomedical papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Multi-Agent Hypothesis Generation Framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A role-based multi-agent pipeline where: (1) Analyst extracts keywords/topics from a given background; (2) Engineer performs literature retrieval (PubMed) and organizes findings into structured 'clues'; (3) Scientist synthesizes Engineer outputs plus the background into one or more hypotheses; (4) Critic evaluates novelty, coherence, and verifiability and can request further iterations. The framework supports tool integration via ReAct and OpenAI function-calling for search, enforces a temporal filter to exclude literature after Jan 2023 (to control visibility), and uses iterative loops between agents to increase uncertainty/diversity of candidates. Outputs are scored using BLEU/ROUGE for overlap-based evaluation when golden hypotheses exist, and GPT-4 / human scoring (0-3) across novelty, relevance, significance, verifiability when no single ground truth exists. SelfBLEU is used as an internal diversity/uncertainty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo; GPT-4 (for evaluation); Llama-2-70b-chat; WizardLM-13B/70B; PMC-LLaMA-13B; MedAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown (GPT-3.5/GPT-4); 70B (Llama-2-70b-chat, WizardLM-70B); 13B (WizardLM-13B, PMC-LLaMA-13B, MedAlpaca-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine / biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset: 2,700 background-hypothesis pairs (seen, pre-Jan-2023) + 200 pairs (unseen, Aug-2023); multi-agent experiments evaluated on the unseen test set (200 examples)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical relationships and quantitative biomarkers (empirical diagnostic markers, statistical performance metrics such as AUC), discovery of pattern-like laws (e.g., power-law rheology reported as two-stage scaling behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples surfaced in case studies include: 'power law index as an early marker of myocardial infarction' and 'universal two-stage power-law rheology' (qualitative relationship reported in Chang et al., 2023); cited AUC thresholds in the paper summary: α_short and α_long track early MI with AUC > 0.7; E2, E3, E_total discriminate mid/late-stage MI with AUC > 0.6.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Pipeline uses LLMs to summarize papers and generate background→hypothesis pairs (ChatGPT/GPT-4 used to summarize literature during dataset construction); Engineer agent issues PubMed searches (tool use via ReAct or OpenAI function calling) to retrieve abstracts/summaries; LLMs parse abstracts and extract candidate numerical/quantitative markers and relationships from text (text-mining of abstracts and results sections), then synthesize hypotheses in natural language. No automated equation parsing was reported; extraction is primarily via textual summarization and structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Validation was performed by (1) checking whether generated hypotheses matched findings in literature published after the training cut (unseen test set), (2) automated scoring by GPT-4 over four dimensions (novelty, relevance, significance, verifiability; 0–3), (3) human expert evaluation (three biomedical experts on 100 unseen examples), and (4) standard overlap metrics (BLEU/ROUGE) where golden hypotheses existed. Correlations between GPT-4 and human scores (Pearson and Spearman) were reported as >0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include BLEU and ROUGE for overlap with golden hypotheses, GPT-4 automated scores (0–3 per metric: novelty/relevance/significance/verifiability), SelfBLEU for internal diversity/uncertainty, and Pearson/Spearman correlations between GPT-4 and human scores (>0.7). Exact numeric BLEU/ROUGE values are shown in figures/tables in the paper but specific aggregate numbers are not quoted verbatim in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported (qualitative claim: LLMs can generate novel and validated hypotheses on unseen literature; no overall percent-correct of extracted quantitative laws provided)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Limitations reported include possible factual hallucination of LLMs, difficulty guaranteeing full invisibility of training data for some models, tool use providing minimal improvement (LLMs struggled to find and use useful literature via ReAct), few-shot and domain adaptation often increased verifiability but reduced novelty, and computational cost of uncertainty estimation methods; authors also cite limited human-evaluation coverage and cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against few-shot/in-context prompting, fine-tuned/instruction-tuned variants (WizardLM, domain adapted PMC-LLaMA/MedAlpaca), and human experts for evaluation. GPT-4 was used as an automatic annotator and its scores correlated strongly (>0.7) with human evaluation. No traditional symbolic equation-extraction baselines were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4192.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4192.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT/GPT-4 summarization pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-4-based Literature Summarization and Background–Hypothesis Pair Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of ChatGPT and GPT-4 to automatically summarize biomedical papers and produce background-hypothesis pairs for dataset construction, enabling downstream LLM hypothesis-generation experiments and the distillation of relationships and candidate quantitative markers from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT/GPT-4 literature summarization and synthetic instruction data generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors constructed dataset by (1) composing a paper set by topic, (2) using ChatGPT and GPT-4 to summarize literature knowledge, (3) generating background-hypothesis pairs (background knowledge → assumed hypotheses), (4) filtering low-quality pairs, and (5) splitting data temporally to create 'seen' and 'unseen' sets. This pipeline distills reported findings (including quantitative performance metrics such as AUCs) into compact backgrounds and corresponding hypotheses used to evaluate hypothesis generation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) and GPT-4 (used for summarization and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine (paper abstracts and experimental results across biomedical topics)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Dataset creation resulted in 2,900 background-hypothesis pairs total: 2,700 (seen, pre-Jan-2023) and 200 (unseen, Aug-2023). The pipeline summarized the literature corpus used to create those pairs (exact number of raw source papers not explicitly given for each pair, but dataset counts provided).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction/distillation of empirical relationships and reported quantitative performance metrics (e.g., diagnostic biomarker performance, AUC thresholds), and conversion of narrative results into structured hypothesis statements.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>The summarization pipeline produced background/hypothesis items that include quantitative statements such as 'α_short and α_long can track early MI with AUC > 0.7' and other numeric performance claims present in source papers (as in the MI power-law case study).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Automated summarization via ChatGPT/GPT-4 prompts to read abstracts/results and produce concise background summaries and putative hypotheses; then human filtering by publisher quality; no explicit figure/table numerical extraction module was described (text-based summarization of quantitative statements).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Manual inspection of a subset and use of filtered publishers to reduce low-quality data; downstream validation performed by training/evaluating LLMs on temporally partitioned seen/unseen splits and by human/GPT-4 scoring of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary reported dataset-level metrics are counts (2,700/200). For the summarization pipeline quality, no separate precision/recall numbers were provided; downstream evaluation used BLEU/ROUGE and GPT-4/human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported (no direct numeric precision of extraction step provided)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Authors note the need to evaluate ChatGPT's extraction accuracy and potential biases introduced during automated generation; acknowledged that manual annotation of a subset would be needed to assess extraction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No separate baselines for summarization/extraction were reported; the pipeline itself was used to produce training and test data for other LLM experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4192.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4192.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM case: MI power-law example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-generated hypotheses matching quantitative mechanical relationships in myocardial infarction literature (power-law rheology case)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Case study where LLMs (e.g., GPT-3.5, Llama-2-70b, WizardLM) generated hypotheses that correspond to quantitative relationships reported in a recent paper (Chang et al., 2023), specifically identifying power-law rheology and diagnostic markers with reported AUCs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis generation with instructed LLMs (zero-shot/few-shot) applied to a myocardial infarction mechanical markers case study</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The authors present a case study derived from Chang et al. (2023) on mechanical markers and two-stage power-law rheology for MI. Multiple instructed LLMs (gpt-3.5-turbo zero-shot and few-shot, Llama-2-70b-chat, WizardLM variants, PMC-LLaMA) produced hypotheses about which mechanical markers (α_short, α_long, E2, E3, E_total) could act as diagnostic features, often matching reported quantitative discriminative performance (AUC thresholds). This demonstrates LLMs' ability to distill and propose quantitative pattern-like relationships from background descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo; Llama-2-70b-chat; WizardLM-70B/13B; PMC-LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>null (gpt-3.5/gpt-4); 70B (Llama-2-70b, WizardLM-70B); 13B (WizardLM-13B, PMC-LLaMA-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biophysics / cardiovascular biomechanics (myocardial infarction mechanical markers)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Case study focused on a single source paper (Chang et al., Nano Letters, Aug 2023) and multiple generated hypothesis instances across models; overall dataset used elsewhere in paper contains 2,900 pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical diagnostic relationships and scaling behavior (power-law rheology), statistical performance measures (AUC thresholds as quantitative discriminators).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Reported quantitative claims included in background/golden hypotheses: (1) α_short and α_long can track and distinguish MI early-stage vs. control with AUC > 0.7; (2) E2, E3, E_total discriminate mid/late-stage MI with AUC > 0.6; (3) universal two-stage power-law rheology across cardiac tissues.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>LLMs generated hypotheses from provided background summaries (zero-shot and few-shot prompting). In some experiments the models produced hypotheses similar to those later published; the extraction/discovery here is via natural-language synthesis rather than symbolic curve fitting or explicit equation extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Comparison of model-generated hypotheses to the published (post-training) findings in Chang et al., 2023 (unseen set), plus GPT-4 and human evaluations on novelty/verifiability/relevance/significance. This served as a check that the LLM outputs correspond to true quantitative relationships described in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Case-study-level qualitative match (authors show examples where gpt-3.5-turbo produced hypotheses that aligned with reported quantitative markers); the source paper's quantitative metrics (AUC values) are cited as part of the background. No aggregate numeric success rate for such matches was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>not reported (examples provided but no percentage of cases where LLMs recovered quantitative relationships is given)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>This is an illustrative case rather than a systematic extraction of equations: extraction was text-based and relies on accurate background summaries; potential for hallucination or omission of numeric details; authors caution that tool use did not strongly improve retrieval of supporting evidence in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Authors compare zero-shot vs few-shot LLM outputs and fine-tuned models; human expert judgments and GPT-4 evaluations used to assess match to ground-truth literature findings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>New Mechanical Markers for Tracking the Progression of Myocardial Infarction <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>Gflownets for ai-driven scientific discovery <em>(Rating: 1)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery <em>(Rating: 1)</em></li>
                <li>Goal driven discovery of distributional differences via language descriptions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4192",
    "paper_id": "paper-271162102",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "LLM multi-agent framework",
            "name_full": "LLM-based Multi-Agent Hypothesis Generation Framework (Analyst/Engineer/Scientist/Critic)",
            "brief_description": "A modular multi-agent system that assigns distinct scientific roles to LLM agents (Analyst, Engineer, Scientist, Critic) and optionally uses external tools (PubMed, search) to iteratively retrieve, synthesize, critique, and produce hypotheses from the literature; used to surface relationships and candidate quantitative markers from biomedical papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based Multi-Agent Hypothesis Generation Framework",
            "system_description": "A role-based multi-agent pipeline where: (1) Analyst extracts keywords/topics from a given background; (2) Engineer performs literature retrieval (PubMed) and organizes findings into structured 'clues'; (3) Scientist synthesizes Engineer outputs plus the background into one or more hypotheses; (4) Critic evaluates novelty, coherence, and verifiability and can request further iterations. The framework supports tool integration via ReAct and OpenAI function-calling for search, enforces a temporal filter to exclude literature after Jan 2023 (to control visibility), and uses iterative loops between agents to increase uncertainty/diversity of candidates. Outputs are scored using BLEU/ROUGE for overlap-based evaluation when golden hypotheses exist, and GPT-4 / human scoring (0-3) across novelty, relevance, significance, verifiability when no single ground truth exists. SelfBLEU is used as an internal diversity/uncertainty proxy.",
            "model_name": "gpt-3.5-turbo; GPT-4 (for evaluation); Llama-2-70b-chat; WizardLM-13B/70B; PMC-LLaMA-13B; MedAlpaca-13B",
            "model_size": "unknown (GPT-3.5/GPT-4); 70B (Llama-2-70b-chat, WizardLM-70B); 13B (WizardLM-13B, PMC-LLaMA-13B, MedAlpaca-13B)",
            "scientific_domain": "Biomedicine / biomedical literature",
            "number_of_papers": "Dataset: 2,700 background-hypothesis pairs (seen, pre-Jan-2023) + 200 pairs (unseen, Aug-2023); multi-agent experiments evaluated on the unseen test set (200 examples)",
            "law_type": "Empirical relationships and quantitative biomarkers (empirical diagnostic markers, statistical performance metrics such as AUC), discovery of pattern-like laws (e.g., power-law rheology reported as two-stage scaling behavior)",
            "law_examples": "Examples surfaced in case studies include: 'power law index as an early marker of myocardial infarction' and 'universal two-stage power-law rheology' (qualitative relationship reported in Chang et al., 2023); cited AUC thresholds in the paper summary: α_short and α_long track early MI with AUC &gt; 0.7; E2, E3, E_total discriminate mid/late-stage MI with AUC &gt; 0.6.",
            "extraction_method": "Pipeline uses LLMs to summarize papers and generate background→hypothesis pairs (ChatGPT/GPT-4 used to summarize literature during dataset construction); Engineer agent issues PubMed searches (tool use via ReAct or OpenAI function calling) to retrieve abstracts/summaries; LLMs parse abstracts and extract candidate numerical/quantitative markers and relationships from text (text-mining of abstracts and results sections), then synthesize hypotheses in natural language. No automated equation parsing was reported; extraction is primarily via textual summarization and structured prompting.",
            "validation_approach": "Validation was performed by (1) checking whether generated hypotheses matched findings in literature published after the training cut (unseen test set), (2) automated scoring by GPT-4 over four dimensions (novelty, relevance, significance, verifiability; 0–3), (3) human expert evaluation (three biomedical experts on 100 unseen examples), and (4) standard overlap metrics (BLEU/ROUGE) where golden hypotheses existed. Correlations between GPT-4 and human scores (Pearson and Spearman) were reported as &gt;0.7.",
            "performance_metrics": "Reported metrics include BLEU and ROUGE for overlap with golden hypotheses, GPT-4 automated scores (0–3 per metric: novelty/relevance/significance/verifiability), SelfBLEU for internal diversity/uncertainty, and Pearson/Spearman correlations between GPT-4 and human scores (&gt;0.7). Exact numeric BLEU/ROUGE values are shown in figures/tables in the paper but specific aggregate numbers are not quoted verbatim in the main text.",
            "success_rate": "not reported (qualitative claim: LLMs can generate novel and validated hypotheses on unseen literature; no overall percent-correct of extracted quantitative laws provided)",
            "challenges_limitations": "Limitations reported include possible factual hallucination of LLMs, difficulty guaranteeing full invisibility of training data for some models, tool use providing minimal improvement (LLMs struggled to find and use useful literature via ReAct), few-shot and domain adaptation often increased verifiability but reduced novelty, and computational cost of uncertainty estimation methods; authors also cite limited human-evaluation coverage and cost constraints.",
            "comparison_baseline": "Compared against few-shot/in-context prompting, fine-tuned/instruction-tuned variants (WizardLM, domain adapted PMC-LLaMA/MedAlpaca), and human experts for evaluation. GPT-4 was used as an automatic annotator and its scores correlated strongly (&gt;0.7) with human evaluation. No traditional symbolic equation-extraction baselines were reported.",
            "uuid": "e4192.0",
            "source_info": {
                "paper_title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "ChatGPT/GPT-4 summarization pipeline",
            "name_full": "ChatGPT / GPT-4-based Literature Summarization and Background–Hypothesis Pair Generation",
            "brief_description": "Use of ChatGPT and GPT-4 to automatically summarize biomedical papers and produce background-hypothesis pairs for dataset construction, enabling downstream LLM hypothesis-generation experiments and the distillation of relationships and candidate quantitative markers from literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ChatGPT/GPT-4 literature summarization and synthetic instruction data generation pipeline",
            "system_description": "Authors constructed dataset by (1) composing a paper set by topic, (2) using ChatGPT and GPT-4 to summarize literature knowledge, (3) generating background-hypothesis pairs (background knowledge → assumed hypotheses), (4) filtering low-quality pairs, and (5) splitting data temporally to create 'seen' and 'unseen' sets. This pipeline distills reported findings (including quantitative performance metrics such as AUCs) into compact backgrounds and corresponding hypotheses used to evaluate hypothesis generation capabilities.",
            "model_name": "ChatGPT (gpt-3.5-turbo) and GPT-4 (used for summarization and evaluation)",
            "model_size": "null",
            "scientific_domain": "Biomedicine (paper abstracts and experimental results across biomedical topics)",
            "number_of_papers": "Dataset creation resulted in 2,900 background-hypothesis pairs total: 2,700 (seen, pre-Jan-2023) and 200 (unseen, Aug-2023). The pipeline summarized the literature corpus used to create those pairs (exact number of raw source papers not explicitly given for each pair, but dataset counts provided).",
            "law_type": "Extraction/distillation of empirical relationships and reported quantitative performance metrics (e.g., diagnostic biomarker performance, AUC thresholds), and conversion of narrative results into structured hypothesis statements.",
            "law_examples": "The summarization pipeline produced background/hypothesis items that include quantitative statements such as 'α_short and α_long can track early MI with AUC &gt; 0.7' and other numeric performance claims present in source papers (as in the MI power-law case study).",
            "extraction_method": "Automated summarization via ChatGPT/GPT-4 prompts to read abstracts/results and produce concise background summaries and putative hypotheses; then human filtering by publisher quality; no explicit figure/table numerical extraction module was described (text-based summarization of quantitative statements).",
            "validation_approach": "Manual inspection of a subset and use of filtered publishers to reduce low-quality data; downstream validation performed by training/evaluating LLMs on temporally partitioned seen/unseen splits and by human/GPT-4 scoring of generated hypotheses.",
            "performance_metrics": "Primary reported dataset-level metrics are counts (2,700/200). For the summarization pipeline quality, no separate precision/recall numbers were provided; downstream evaluation used BLEU/ROUGE and GPT-4/human scoring.",
            "success_rate": "not reported (no direct numeric precision of extraction step provided)",
            "challenges_limitations": "Authors note the need to evaluate ChatGPT's extraction accuracy and potential biases introduced during automated generation; acknowledged that manual annotation of a subset would be needed to assess extraction fidelity.",
            "comparison_baseline": "No separate baselines for summarization/extraction were reported; the pipeline itself was used to produce training and test data for other LLM experiments in the paper.",
            "uuid": "e4192.1",
            "source_info": {
                "paper_title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM case: MI power-law example",
            "name_full": "LLM-generated hypotheses matching quantitative mechanical relationships in myocardial infarction literature (power-law rheology case)",
            "brief_description": "Case study where LLMs (e.g., GPT-3.5, Llama-2-70b, WizardLM) generated hypotheses that correspond to quantitative relationships reported in a recent paper (Chang et al., 2023), specifically identifying power-law rheology and diagnostic markers with reported AUCs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypothesis generation with instructed LLMs (zero-shot/few-shot) applied to a myocardial infarction mechanical markers case study",
            "system_description": "The authors present a case study derived from Chang et al. (2023) on mechanical markers and two-stage power-law rheology for MI. Multiple instructed LLMs (gpt-3.5-turbo zero-shot and few-shot, Llama-2-70b-chat, WizardLM variants, PMC-LLaMA) produced hypotheses about which mechanical markers (α_short, α_long, E2, E3, E_total) could act as diagnostic features, often matching reported quantitative discriminative performance (AUC thresholds). This demonstrates LLMs' ability to distill and propose quantitative pattern-like relationships from background descriptions.",
            "model_name": "gpt-3.5-turbo; Llama-2-70b-chat; WizardLM-70B/13B; PMC-LLaMA-13B",
            "model_size": "null (gpt-3.5/gpt-4); 70B (Llama-2-70b, WizardLM-70B); 13B (WizardLM-13B, PMC-LLaMA-13B)",
            "scientific_domain": "Biophysics / cardiovascular biomechanics (myocardial infarction mechanical markers)",
            "number_of_papers": "Case study focused on a single source paper (Chang et al., Nano Letters, Aug 2023) and multiple generated hypothesis instances across models; overall dataset used elsewhere in paper contains 2,900 pairs.",
            "law_type": "Empirical diagnostic relationships and scaling behavior (power-law rheology), statistical performance measures (AUC thresholds as quantitative discriminators).",
            "law_examples": "Reported quantitative claims included in background/golden hypotheses: (1) α_short and α_long can track and distinguish MI early-stage vs. control with AUC &gt; 0.7; (2) E2, E3, E_total discriminate mid/late-stage MI with AUC &gt; 0.6; (3) universal two-stage power-law rheology across cardiac tissues.",
            "extraction_method": "LLMs generated hypotheses from provided background summaries (zero-shot and few-shot prompting). In some experiments the models produced hypotheses similar to those later published; the extraction/discovery here is via natural-language synthesis rather than symbolic curve fitting or explicit equation extraction.",
            "validation_approach": "Comparison of model-generated hypotheses to the published (post-training) findings in Chang et al., 2023 (unseen set), plus GPT-4 and human evaluations on novelty/verifiability/relevance/significance. This served as a check that the LLM outputs correspond to true quantitative relationships described in the literature.",
            "performance_metrics": "Case-study-level qualitative match (authors show examples where gpt-3.5-turbo produced hypotheses that aligned with reported quantitative markers); the source paper's quantitative metrics (AUC values) are cited as part of the background. No aggregate numeric success rate for such matches was reported.",
            "success_rate": "not reported (examples provided but no percentage of cases where LLMs recovered quantitative relationships is given)",
            "challenges_limitations": "This is an illustrative case rather than a systematic extraction of equations: extraction was text-based and relies on accurate background summaries; potential for hallucination or omission of numeric details; authors caution that tool use did not strongly improve retrieval of supporting evidence in some settings.",
            "comparison_baseline": "Authors compare zero-shot vs few-shot LLM outputs and fine-tuned models; human expert judgments and GPT-4 evaluations used to assess match to ground-truth literature findings.",
            "uuid": "e4192.2",
            "source_info": {
                "paper_title": "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "New Mechanical Markers for Tracking the Progression of Myocardial Infarction",
            "rating": 2,
            "sanitized_title": "new_mechanical_markers_for_tracking_the_progression_of_myocardial_infarction"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Gflownets for ai-driven scientific discovery",
            "rating": 1,
            "sanitized_title": "gflownets_for_aidriven_scientific_discovery"
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery",
            "rating": 1,
            "sanitized_title": "learning_to_generate_novel_scientific_directions_with_contextualized_literaturebased_discovery"
        },
        {
            "paper_title": "Goal driven discovery of distributional differences via language descriptions",
            "rating": 1,
            "sanitized_title": "goal_driven_discovery_of_distributional_differences_via_language_descriptions"
        }
    ],
    "cost": 0.016444749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation
15 Jul 2024</p>
<p>Biqing Qi qibiqing7@gmail.com 
Tsinghua University</p>
<p>Harbin Institute of Technology</p>
<p>Kaiyan Zhang zhang-ky22@mails.tsinghua.edu.cn 
Tsinghua University</p>
<p>Frontis.AI</p>
<p>Kai Tian 
Tsinghua University</p>
<p>Frontis.AI</p>
<p>Haoxiang Li 
Tsinghua University</p>
<p>Zhang-Ren Chen 
The First Affiliated Hospital of Nanchang University</p>
<p>Sihang Zeng 
University of Washington</p>
<p>Ermo Hua 
Tsinghua University</p>
<p>Frontis.AI</p>
<p>Jin-Fang Hu 
The First Affiliated Hospital of Nanchang University</p>
<p>Bowen Zhou zhoubowen@tsinghua.edu.cn 
Tsinghua University</p>
<p>Shanghai AI Laboratory</p>
<p>Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation
15 Jul 202476BBEE303DD0004FB8E9D0380F11AC10arXiv:2407.08940v2[cs.CL]
The rapid growth of biomedical knowledge has outpaced our ability to efficiently extract insights and generate novel hypotheses.Large language models (LLMs) have emerged as a promising tool to revolutionize knowledge interaction and potentially accelerate biomedical discovery.In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination.Using this dataset, we assess the hypothesis generation capabilities of toptier instructed models in zero-shot, few-shot, and fine-tuning settings.To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use and multi-agent interactions in our evaluation framework.Furthermore, we propose four novel metrics grounded in extensive literature review to evaluate the quality of generated hypotheses, considering both LLM-based and human assessments.Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training, and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance.However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated.These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.Data and code at https://github.com/TsinghuaC3I/LLM4BioHypoGen/.</p>
<p>Introduction</p>
<p>Biomedical research is a driving force behind advancements in human health and well-being.However, the exponential growth of biomedical data and literature has made it increasingly difficult for researchers to keep pace with the latest discoveries and generate novel hypotheses.Large language models (LLMs) (Touvron et al., 2023a;b;Jiang et al., 2024) have emerged as a promising solution to this challenge, offering the potential to revolutionize the way we interact with and discover knowledge in the biomedical domain (Wang et al., 2023a;Taylor et al., 2022).At the core of the knowledge discovery process is the formulation of sound hypotheses (Zhong et al., 2023;Boiko et al., 2023).Currently, both ChatGPT, GPT-4, Instruction: You are a researcher.You can come up with new hypotheses based on your existing knowledge.Hypotheses are given against the following background.You should be as detailed as possible.Background knowledge: (1) Esophageal cancer mainly includes squamous cell carcinoma and adenocarcinoma, with different risk factors and incidence rates.(2) Metformin has been shown to reduce the risk of several cancers in patients with T2DM.(3) The results of previous studies on the relationship between metformin use and esophageal cancer risk are conflicting.</p>
<p>The model hypothesizes:</p>
<p>(1) The use of metformin in patients with T2DM mav be associated with a reduced risk of esophageal cancer.</p>
<p>(2) There may be differences in the correlation between metformin use and the risk of esophageal cancer in male and female patients with T2DM.and other open-source LLMs undergo extensive pre-training on a substantial amount of data.The analysis and validation of related hypothesis generation work rely on these models (Wang et al., 2023b;Yang et al., 2023b).However, due to the non-traceability of the training data, these endeavors cannot ensure data invisibility, i.e., the inability to guarantee non-overlapping between test and training data.This limitation hinders the assessment and analysis of knowledge discovery in LLMs under zero-shot conditions.</p>
<p>In this paper, we present a comprehensive evaluation of LLMs as biomedical hypothesis generators.We construct a dataset of background-hypothesis pairs from biomedical literature, carefully partitioned into training, seen, and unseen test sets based on publication date to mitigate data contamination.Using this dataset, we assess the hypothesis generation capabilities of top-tier instructed models in zero-shot, few-shot, and fine-tuning settings.To enhance the exploration of uncertainty, a crucial aspect of scientific discovery, we incorporate tool use (Schick et al., 2023a) and multi-agent interactions (Xi et al., 2023b) in our evaluation framework.These strategies are inspired by uncertainty exploration in reinforcement learning (Watkins &amp; Dayan, 1992;Schulman et al., 2017) and aim to examine the influence of uncertainty on knowledge discovery and the primary factors contributing to zero-shot capability.</p>
<p>Our experiments yield two key findings: 1) LLMs can generate novel and validated hypotheses, even when tested on literature unseen during training (Figure 1), and 2) Increasing uncertainty through multi-agent interactions and tool use can facilitate diverse candidate generation and improve zero-shot hypothesis generation performance.However, we also observe that the integration of additional knowledge through few-shot learning and tool use may not always lead to performance gains, highlighting the need for careful consideration of the type and scope of external knowledge incorporated.</p>
<p>These findings underscore the potential of LLMs as powerful aids in biomedical hypothesis generation and provide valuable insights to guide further research in this area.Our specific contributions include:</p>
<p>• We pioneer the rigorous validation of LLMs in zero-shot and few-shot hypothesis generation through the creation of temporal biomedical instruction data and innovative experiments for in-depth analysis and evaluation.</p>
<p>• Our findings reveal that LLMs exhibit foundational higher-order reasoning abilities and can generate novel hypotheses, offering fresh empirical insights for knowledge discovery.</p>
<p>• We develope multidimensional metrics for evaluating hypotheses with GPT-4 and human.The correlation between these evaluations suggests LLMs' substantial role in hypothesis evaluation.</p>
<p>• We propose a LLM-based multi-agent framework for hypothesis generation.This system facilitates collaborative analysis among various roles and tools, enhancing our understanding of the influenced factors of LLM-based proposers.</p>
<p>Preliminary</p>
<p>Experimenal Loop of Scientific Discovery We execute the automated data partitioning pipeline, using publication dates as the basis, to mitigate the risk of data contamination.</p>
<p>In this section, we delineate the problem of hypothesis generation.Subsequently, we elucidate the process for developing biomedical-specific datasets.We then analyze the constructed datasets, examining aspects of data contamination and the semantic distribution across different data settings.</p>
<p>Problem Definition</p>
<p>As shown in Figure 2a, hypothesis generation typically transpires following an in-depth analysis of literature and detailed examination of specific phenomena, playing a pivotal role in the scientific discovery process.In order to improve the assessment of the hypothesis proposition capabilities of LLMs, we formalize this process as a text completion task.Given dataset D, an instruction I, and text pairs (X i , Y i ) n i=1 containing background knowledge and corresponding hypotheses extracted from medical papers, our objective is to assess model M by having it generate hypotheses based on the task instruction and background knowledge, i.e., M(I, X i ) = Y i , for each i ∈ 1, ..., n.The objective function is formulated as:
y * = arg max y 1 ,...,y n n ∏ t=1 P(y t |y 1 , . . . , y t−1 , I, X).</p>
<p>Dataset Construction</p>
<p>Existing LLMs, including ChatGPT and Llama, encounter challenges in retracing their training data, complicating efforts to ensure non-overlap with the test set.To rigorously assess the hypothesis generation capability of LLMs, we initially create evaluation datasets based on literature, taking into account the publication date.</p>
<p>Unlike previous studies that assess data contamination through the correlation between question entropy and accuracy (Hendrycks et al., 2020;Wei et al., 2023b;Zhou et al., 2023), our method directly controls visibility by considering both the training date and the publication date.This method ensures that the test data remains non-visible, addressing a limitation overlooked in earlier methods.As depicted in Figure 2b, the year 2023 has been established as the cut-off point, coinciding with the publication date of the majority of top-tier LLMs.The training dataset includes literature published before January 2023, while the test dataset contains literature after January 2023.This arrangement creates pairs of data with background knowledge and corresponding hypothesis proposals.We strictly follow the standard pipeline as outlined in Self-Instruct (Wang et al., 2022) for our data generation process: 1) Compose the paper set based on the topic and content of the literature.2) Utilize ChatGPT and GPT-4 to summarize the literature knowledge.3) Generate background knowledge-assume pairs.4) Filter low-quality data by publishers.5) Split the dataset according to publication time.</p>
<p>While this approach allows for efficient dataset creation, we acknowledge that the accuracy of ChatGPT's extractions and any biases introduced in the process warrant further evaluation.Manual annotation of a subset of the data could help assess the quality and reliability of the automated extraction.</p>
<p>Adhering to the above pipeline outlined, we ultimately acquired two distinct types of datasets.1) Seen dataset This dataset comprises 2700 background and hypothesis pairs sourced from literature published before January 2023.This dataset was partitioned into training (2500) and validation ( 200) subsets (as well as seen test set).It is consistent with the corpus that the LLMs have been exposed to.2) Unseen dataset The unseen dataset consists of 200 pairs extracted from papers published in August 2023, which the LLMs have not encountered during training and are used for testing purposes.</p>
<p>More detailed information with respect to publication data and topic distributions of constructed dataset can be seen in Appendix C.1.</p>
<p>Can LLMs Truly Generate Zero-Shot Hypotheses?</p>
<p>In this section, we conduct a thorough evaluation of premier LLMs across a range of experimental settings to determine their ability to effectively generate hypotheses and analyze influenced factors.</p>
<p>Experiment Setup</p>
<p>Initially, we present the models being evaluated, outline the experimental settings, and describe the metrics used for evaluation.</p>
<p>Models</p>
<p>For a fair comparison, we exclusively evaluate LLMs trained on corpora before March 2023 to avoid data contamination.We consider three categories of models: 1) APIbased LLMs: this is mainly ChatGPT.2) General domain instructed LLMs: These models consist of open-source models that have undergone fine-tuning based on Llama using general domain instructions.We primarily choose the top-tier models based on rankings before Setember 2023 in the Alpaca Eval Leaderboard1 .3) Specific domain instructed LLMs: These include PMC-LLaMA (Wu et al., 2023), and MedAlpaca (Han et al., 2023), which are trained on a variety of sources in medicine domain.We summarize the training data and publication dates for each models in Appendix C.2.</p>
<p>Prompts and Finetuning</p>
<p>To ensure a consistent output format across different models, we create prompts in two formats: zero-shot and few-shot examples.We adopt a 5-shot format, selecting examples from the training set before January 2023 using both randomly sampled and similarity retrieval methods.To assess the hypothesis generation capability beyond zero-shot, we identify the top-performing open-source models through prompting-based evaluation.Finally, we proceed to fine-tune WizardLM-13B-V1.2 with the background and hypothesis pairs for further comparison.We provide more details in Appendix F.</p>
<p>Evaluation Metrics</p>
<p>Given the inherent uncertainty beyond established ground truth in hypothesis generation, we undertake evaluations with and without predefined golden hypotheses. 1) With ground truth: In evaluations with golden hypotheses, we employ standard text generation metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) , to assess word overlap between the generated outputs and the ground truth.2) Without ground truth: Considering the vastness of the hypothesis space, renders it difficult to comprehensively assess the quality of generated hypotheses using word overlap metrics alone.To provide a more comprehensive evaluation of the generated hypotheses from multiple facets, we devise four metrics for evaluation (Zhong et al., 2023):</p>
<p>• Novelty: Does the hypothesis introduce new information or perspectives?</p>
<p>• Relevance: How closely is the hypothesis related to the topic or question?</p>
<p>• Significance: What impact does the hypothesis have on understanding or addressing the problem?</p>
<p>• Verifiability: Can the hypothesis be tested using existing methods or data?</p>
<p>Furthermore, inspired by recent research that highlights LLMs as proficient annotators (Gilardi et al., 2023;Liu et al., 2023), demonstrating a strong correlation with human ratings, we employ GPT-4 for automated evaluation, where both the generated hypotheses and the provided background are evaluated across these aspects.The scoring scale ranges from 0 to 3, where a higher score indicates superior results.Additionally, we solicit GPT-4 to furnish a step-by-step explanation to substantiate the assigned score.We also conduct human evaluation for the top-tier models identified in the evaluation of GPT-4 in Section 3.4.</p>
<p>Experiment Results</p>
<p>g p t-3 .5 -t u rb o (0 -s h o t) g p t-3 .5 -t u rb o (5 -s h o t) g p t-3 .5 -t u rb o (5 - This section presents the results of hypothesis generation across various models in both zero-shot and few-shot settings.We primarily analyze the results from two perspectives: the impact of the zero-shot setting and the influence of introducing external knowledge on hypothesis generation.
s h o t) * V ic u n a -3 3 b -v 1 .3 (0 -s h o t) V ic u n a -3 3 b -v 1 .3 (5 -s h o t) V ic u n a -3 3 b -v 1 .3 (5 -s h o t) * L la m a -2 -7 0 b -c h a t( 0 -s h o t) L la m a -2 -7 0 b -c h a t( 5 -s h o t) L la m a -2 -7 0 b -c h a t( 5 -s h o t) * W iz a rd L M -1 3 B -V 1 .2 (0 -s h o t) W iz a rd L M -1 3 B -V 1 .2 (5 -s h o t) W iz a rd L M -1 3 B -V 1 .2 (5 -s h o t) * W iz a rd L M -7 0 B -V 1 .0 (0 -s h o t) W iz a rd L M -7 0 B -V 1 .0 (5 -s h o t) W iz a rd L M -7 0 B -V</p>
<p>Results of Zero-shot Setting</p>
<p>The results presented in Figure 3   compared to the "unseen" test set.Contrarily, our findings presented in Figure 3 reveal that LLMs tend to exhibit better performance on the "unseen" test set.We hypothesize that the intricacies involved in hypothesis generation may impede LLMs' ability to effectively utilize the parameterized "dark knowledge".Figure 5: This figure elucidates the correlation between uncertainty and evaluation scores for all models, encompassing both zero-shot and few-shot settings, and incorporating both sampled and similarity retrieval few-shot prompts.</p>
<p>Results of External Knowledge</p>
<p>We further extend analysis to the influence of external knowledge, including few-shot examples, domain adaptation, and instruction tuning.The findings indicate that these factors do not consistently enhance performance across all metrics.</p>
<p>Few-Shot Examples Enhance Verifiability but Decrease Novelty.Regarding word overlap metrics, including BLEU and ROUGE, most models, especially gpt-3.5-turboand WizardLM series models, show improved performance when provided with in-context examples as shown in Figure 3.However, it's important to note that these few-shot prompts significantly increase verifiability while simultaneously leading to lower novelty compared to zero-shot results as shown in Figure 4.</p>
<p>Impact of Domain Adaptation</p>
<p>We also conduct an analysis of the influence of fine-tuning for biomedical domain adaptation on hypothesis generation.The results obtained from MedAplaca and PMC-LLaMA indicate that domain adaptation can significantly improve word overlap performance.Evaluation derived from GPT-4 imply that domain adaptation improves the capability of LLMs in utilizing few-shot, particularly with related literature, while diminishing their zero-shot ability.</p>
<p>Impact of Instruction Tuning</p>
<p>In addition to domain adaptation, we also examine the effect of direct fine-tuning of LLMs on specially constructed instruction training sets.The findings reveal that instruction tuning enhances the capacity of LLMs for knowledge retention, leading to improved BLEU and ROUGE scores and heightened verifiability, especially when contrasted with models without fine-tuning.However, this enhanced memorization concurrently diminishes the model's capability to generate novel and significant hypotheses.Published as a conference paper at COLM 2024</p>
<p>Quantitative Analysis on Uncertainty</p>
<p>Drawing from the findings in zero-shot settings and the impact of external knowledge, we identify uncertainty as the key differentiator among these scenarios.The few-shot setting tends to enhance the creative capacity of LLMs, yielding content with higher generalization and diversity.In terms of external knowledge, parameterized knowledge and judiciously selected samples narrow the candidate space, thereby reducing the uncertainty of generated content but enhancing verifiability.Conversely, randomly sampled few-shot examples introduce uncertainty.There are many methods for computing uncertainty, including those based on entropy, semantics, and consistency (Kuhn et al.; Xiong et al.).However, these methods often entail a high computational cost, primarily due to the requirement for multiple sampling iterations, rendering them impractical for our experiments.Given the pioneering nature of our research in applying Large Language Models (LLMs) to knowledge discovery, and to maintain manageability within the scope of this paper, we have chosen to use SelfBLEU (Alihosseini et al., 2019) to evaluate the internal uncertainty of all generated hypotheses.The SelfBLEU scores serve as indicators of uncertainty in the hypotheses generated by LLMs, where lower scores signify greater uncertainty and diversity.</p>
<p>As depicted in Figure 5, we examine the relationship between uncertainty and detailed evaluation scores of various models under different settings, including zero-shot and fewshot settings (the latter comprising sampled and retrieved examples, denoted as 5-shot and 5-shot*, respectively).Our findings reveal that hypotheses generated in the zero-shot setting exhibit higher uncertainty and novelty scores, as illustrated in Figure 5a.Conversely, few-shot settings with augmented hypotheses demonstrate lower uncertainty yet higher verifiability, as indicated in Figure 5b.These observations offer valuable insights for further research into the balance between novelty and verifiability in hypothesis proposing.Motivated by these findings, we propose a LLM-based multi-agent framework to delve into uncertainty exploration involving tools and multi-agents collaboration in Section 4.</p>
<p>Human Evaluation and Case Study</p>
<p>In this section, we conduct human evaluation to assess the coherence of hypotheses generated by LLMs, comparing these with GPT-4 evaluation scores to guide further automated evaluations.</p>
<p>Settings</p>
<p>We apply the four metrics from Section 3.1 (novelty, relevance, significance, and verifiability) for manual evaluation and comparison with GPT-4's assessments, where each metric is scored from 0 to 3. Due to cost constraints, human evaluation focuses on the highest-performing LLM based on automatic metrics and GPT-4 evaluations.The selected models and prompts are listed in Table 1. 100 examples from the unseen test set were evaluated by three experts in the biomedical field.</p>
<p>Results</p>
<p>The results in Table 1 show a strong correlation (Pearson and Spearman coefficients above 0.7) between human and GPT-4 evaluations, reinforcing the potential of LLMs in hypothesis evaluation.For further insights, we analyze the correlation between word overlap scores and manual scores in the Appendix D. A case study illustrating various model-generated hypotheses and ChatGPT evaluations is available in Appendix E. Notably, we have collaborated with experts in medicine and biology and adopted a 5% sampling rate for expert evaluation.Given the constraints of manpower and costs, a full-scale evaluation was not feasible.However, we believe that a 5% sampling rate provides a statistically representative overview and effectively reflects the overall quality of the data.We appreciate your understanding and support in this matter.</p>
<p>Can agent collaboration enhance LLMs' zero-shot generalization?</p>
<p>In this section, we aim to improve LLMs' hypothesis generation capability by leveraging uncertainty with tool use and multi-agent collaboration.We firstly introduce the conceptual multi-agent system for hypothesis generation, drawing inspiration from scientific research.Subsequently, we present the role design and the tools use in this context.Finally, we present preliminary validated results of the multi-agent system on the constructed dataset.</p>
<p>Multi-agent Framework</p>
<p>Inspired by the discovery process in Figure 2a, we introduce a comprehensive framework tailored for hypothesis formulation.This framework encapsulates a multi-agent system where each agent assumes a distinct role, mirroring the collaborative nature of scientific endeavors.Through a symbiotic and iterative process, these agents collaborate to craft hypotheses that are not only grounded in existing knowledge but also pave the way for novel insights.By emulating the essence of scientific discovery, our framework strives to produce hypotheses that are both innovative and scientifically robust.As depicted in Figure 6a, we partition the framework into five components, encompassing four automated agents and the option for human involvement within the loop.</p>
<p>Role Design</p>
<p>In our proposed multi-agent framework, each component plays a distinct and pivotal role.The Analyst serves as the foundation, meticulously extracting and defining core elements from the research background.Its primary objective is to interpret the literature, distilling it into keywords or topics that subsequently guide the Engineer's search efforts.The Engineer, leveraging these keywords, embarks on a mission to retrieve and organize pertinent information.They meticulously plan and execute detailed searches, ensuring that the findings are compiled in a structured manner.This organized materials then lands in the domain of the Scientist, whose objective is to weave together the Engineer's findings with the original research background.Through careful interpretation, the Scientist crafts a hypothesis that is both grounded in existing knowledge and offers a fresh perspective.However, before this hypothesis is finalized, it undergoes scrutiny by the Critic.The Critic's role is paramount in ensuring the hypothesis's robustness, coherence, and novelty.They evaluate the hypothesis against the backdrop of the research background, ensuring it stands up to academic rigor.Feedback from the Critic, if necessary, loops back to refine the hypothesis or prompts the Analyst for further insights, creating a cyclical and iterative process of refinement.</p>
<p>Tool Use To explore external knowledge beyond the inherent dark knowledge within LLMs, we integrate the Engineer agent with search engines , mainly PubMed2 .Similarly, to control the visibility of the unseen test dataset, we filter and exclude literature published after January 2023 from the search results.We carry out tool use experiments using ReAct (Yao et al., 2023b) and OpenAI function calling.ReAct is a method that extends the concept of Chain of Thought (CoT) (Wei et al., 2022), involving thinking before taking action and subsequently making observations based on feedback from the environment.In our experiments, we instruct the LLMs to initially contemplate the provided background information and then make a decision regarding whether to utilize tools.Upon receiving feedback from the tools, the LLMs are expected to identify supporting evidence in the results or potentially make further tool requests.The LLMs are responsible for concluding the hypothesis generation process and summarizing the hypotheses independently.</p>
<p>Experiment Results</p>
<p>Our primary focus is to investigate the impact of tool use and multi-agent collaboration on hypothesis generation.We present the experimental results in Figure 6b.Based on the results, we summarize our findings from the following two perspectives:</p>
<p>Results of Tool Use Based on our results, we observe that tool use has minimal impact on improving the hypothesis generation ability of LLMs.This observation aligns with the findings presented in Section 3.2.2regarding the analysis of external knowledge.Notably, the ReAct-based method performs slightly worse than OpenAI function calling.It is also evident that LLMs struggle to identify useful information and exhibit weaknesses in the thought-action-observation process, even when utilizing the official interface from OpenAI.</p>
<p>Published as a conference paper at COLM 2024 Hypothesis generation is a demanding task where the balance between certainty and uncertainty plays a crucial role and needs further exploration.</p>
<p>Results of Multi-agent Collaboration</p>
<p>In addition to tool use, our findings suggest that the division of labor and interaction among multi-agents can significantly enhance the model's capability to propose hypotheses by introducing uncertainty.This mirrors the dynamics of real-world scientific research, where hypotheses are formulated through iterative discussions and refutations.Additionally, it is worth noting that tool use can further enhance the performance of the multi-agent framework.</p>
<p>Conclusion</p>
<p>This study presents a comprehensive evaluation of large language models (LLMs) as biomedical hypothesis generators, focusing on their zero-shot generalization ability.We constructed a novel biomedical corpus to assess the hypothesis generation capabilities of LLMs, revealing their remarkable ability to generate novel and validated hypotheses even when tested on unseen literature.We introduced a multi-agent framework that leverages LLMs to enhance collaborative analysis in the hypothesis generation process and developed multidimensional metrics to evaluate the quality of generated hypotheses.Our work contributes to the growing research on the application of LLMs in scientific discovery, particularly in the biomedical domain.Future research could explore the integration of domain-specific knowledge, incorporation of diverse data sources, and development of user-friendly interfaces to further enhance the performance and adoption of LLMs in biomedical hypothesis generation.Our study highlights the immense potential of LLMs as catalysts for biomedical discovery, offering a promising avenue for accelerating the pace of scientific progress.</p>
<p>Limitations and future works</p>
<p>Our research reveals the potential of Large Language Models (LLMs) to generate new hypotheses, marking a significant positive finding.However, we must acknowledge certain limitations.Firstly, a thorough investigation is required into the impact of factual hallucinations inherent in LLMs on hypothesis formulation.Additionally, evaluating and analyzing knowledge discovery in LLMs demands in-depth consideration from various perspectives, necessitating further exploration.</p>
<p>Currently, our focus is on the hypothesis generation stage, utilizing tools such as Web search and PubMed.Introducing more specialized tools into the knowledge discovery cycle could prove invaluable.In future work, we plan to integrate bioinformatics analysis tools (Zhang et al., 2024) to enhance our research efforts.</p>
<p>Moving forward, we intend to incorporate knowledge graphs to aid in generating higherquality hypotheses.Additionally, we will explore the inclusion of more dynamic tools, such as multi-tool collaborations, to further improve the performance of our LLM-based models.</p>
<p>To significantly enhance the hypothesis generation capabilities of our models, we aim to optimize strategies that introduce uncertainty via reinforcement learning.This approach will help us address the limitations and unlock the full potential of LLMs in the knowledge discovery process.</p>
<p>A Process of Scientific Discovery</p>
<p>Scientific discovery involves key components, each crucial for advancing our understanding of the natural world: data analysis, hypothesis formulation, experiment design, execution, and observation and reflection Jain et al. (2023) as shown in Figure 2a.</p>
<p>1) Data Analysis: Foundational in the scientific process, it entails collecting and examining data to discern patterns and anomalies, extracting insights through statistical techniques and visualization.It initiates scientific inquiry, guiding further exploration.2) Generating Hypotheses: Among these components, hypothesis formulation is pivotal.It entails crafting informed guesses to explain observed phenomena.Hypotheses serve as guiding frameworks, directing and focusing research by articulating specific relationships and outcomes for experimental exploration.3) Experiment Design: Once a hypothesis is set, designing experiments becomes essential to rigorously test its validity.This involves defining variables, specifying control groups, and outlining methods and procedures.Well-designed experiments ensure objective hypothesis testing and yield meaningful, informative results.4) Experiment Execution: Meticulous execution of designed experiments and data collection are critical.Researchers adhere precisely to experimental protocols, recording observations, measurements, and unexpected findings.Integrity in execution ensures reliable, reproducible outcomes.5) Accumulating Observations: After experiments, scientists engage in observation and reflection.They analyze collected data to determine if results support or refute the initial hypothesis.If unsupported, hypotheses may be revised or new ones formulated based on findings.Observation and reflection permit iterative refinement of scientific understanding.</p>
<p>Hypothesis Pioneers Pathways: Guiding Knowledge Discovery.While all components are essential, hypothesis formulation holds a unique position.It drives the scientific endeavor, guiding research question selection, experiment design, and data analysis.Well-constructed hypotheses not only provide direction but also lay the foundation for meaningful scientific discoveries by posing rigorously testable questions.Hypothesis formulation serves as the intellectual anchor steering scientific investigation and ultimately advancing knowledge.</p>
<p>B Related Works</p>
<p>B.1 Data-Driven Scientific Discovery Data-driven knowledge discovery research within LLM is relatively limited, with the current focus primarily on dataset construction and task-driven design.In this context, (Zhong et al., 2023) proposed a dataset for investigating the transition from goals to discoveries.However, it should be noted that accurate discoveries within this dataset are not recent.(Wang et al., 2023b) introduced a method for automatically collecting and constructing publication data, along with a proposal for a hypothesis generation approach in the natural language processing (NLP) domain.However, this method requires prior human knowledge, explicit context, and is not an automated process.It's worth noting that their data was constructed from literature before 2021 from the ACL collection, implying that the information may already exist in open-source models like chatGPT and LLAMA.Furthermore, (Wang et al., 2023b) focused on integrating computational tools in the field of chemistry, primarily analyzing the capabilities of LLMs in using integrated tools but neglecting the ability for zero-shot generalization in chemistry reactions.(Boiko et al., 2023) delved more into the abilities of LLMs regarding planning and conducting experiments but did not consider proposing new hypotheses.(Yang et al., 2023b) introduced a new task for open-domain hypothesis induction and created a dataset comprising 50 articles from social science journals.Additionally, they developed a multi-module system for exploring feedback mechanisms.However, all of the above-mentioned literature lacks strict guarantees on the visibility of test data to models, thereby limiting our exploration of the zero-shot generalization capability of LLMs through learning from existing knowledge to propose new hypothesis.Unlike existing works, we have designed datasets based on publication dates, which can easily ensure a strict independence between test data and LLMs.</p>
<p>B.2 LLM-driven Autonomous Agents</p>
<p>Large language models demonstrate exceptional capabilities in tasks such as question answering, program coding, and instruction following.However, they still confront significant challenges related to factual hallucination (Zhang et al., 2023;Rawte et al., 2023), knowledge outdated (Cao et al., 2023), and interactions with real-world.To address these challenges, recent research has explored enhancing LLMs by incorporating tools such as search engines (Nakano et al., 2022;Parisi et al., 2022), calculators (Schick et al., 2023b), code interpreter (Zhu et al., 2023b), RESTful APIs (Song et al., 2023;Patil et al., 2023) and others.The integration of LLMs with tool use, also known as LLM-driven autonomous agents (LAAs), has attracted substantial public attention.These agents are equipped with reasoning (Wei et al., 2023a;Yao et al., 2023a), planning (Shen et al., 2023;Valmeekam et al., 2023), decision-making (Yang et al., 2023a;Kang et al., 2023), and long-term memory capabilities (Zhu et al., 2023a;Hu et al., 2023), and they are constructed upon the foundation of LLMs.LAAs can autonomously plan sub-goals for complex tasks, execute actions, obtain feedback from the environment, and adjust their behaviors to adapt (Yao et al., 2023b;Xi et al., 2023a;Shinn et al., 2023).LAAs have demonstrated significant potential in addressing complex real-world tasks, including software development (Qian et al., 2023;Hong et al., 2023), drama creation (Maas et al., 2023), course design (Chen et al., 2023), chemistry experiments (Bran et al., 2023) and more.Furthermore, multi-agent collaboration plays a significant role in LAA applications, allowing agents to collaborate and interact to solve problems through various role-playing scenarios (Park et al., 2023;Fu et al., 2023;Gong et al., 2023;Li et al., 2023).To the best of our knowledge, there is still a dearth of exploration regarding the use of agents, particularly multi-agents, for scientific discovery.In this paper, our objective is to undertake a preliminary effort to enhance the hypothesis proposing capability of LLMs by harnessing tools and multiple agents, along with conducting an analysis of influencing factors.</p>
<p>Category Model</p>
<p>Seen Unseen</p>
<p>C Implementation Details</p>
<p>In this section, we delve into further implementation details of our experiments, including information about the constructed dataset and open-source models.</p>
<p>C.1 Details of Dataset</p>
<p>We present the publication dates and topic distributions of the various datasets for comparison, as illustrated in Figure 7, where we utilize Nomic Atlas3 to visualize the topic distribution of abstracts in both the training and test datasets.</p>
<p>As depicted in Figure 8a, we embed the background and hypothesis sentences from both the seen and unseen test sets using OpenAI embeddings , followed by dimensionality reduction through t-SNE (Van der Maaten &amp; Hinton, 2008).The results indicate minimal differences between the two test sets, despite their derivation from distinct time periods.</p>
<p>C.2 Details of Models</p>
<p>We present the meta-information of the open-source models used in our experiments, as shown in Table 4.We have gathered data regarding their pre-training, supervised learning corpus, and release dates to ensure the non-visibility of the unseen test data.The models were selected based on their performance ranking on the Alpaca-Eval leaderboard prior to September 2023.</p>
<p>2 0 0 0 2 0 0 1 2 0 0 2 2 0 0 3 2 0 0 4 2 0 0 5 2 0 0 6  2 0 0 7 2 0 0 8 2 0 0 9 2 0 1 0 2 0 1 1 2 0 1 2 2 0 1 3 2 0 1 4 2 0 1 5 2 0 1 6 2 0 1 7 2 0 1 8 2 0 1 9 2 0 2 0 2 0 2 1 2 0 2</p>
<p>D Additional Results</p>
<p>D.1 Automatic Evaluations</p>
<p>We present more detailed results of diverse LLMs and prompting scenarios on datasets specifically constructed for ChatGPT and GPT-4 in Table 2 and Table 3, respectively.Additionally, we provide the outcomes of experiments involving multi-agent interactions and tool use in Table 6.In addition to Figure 3, Figure 9 presents the ROUGE scores for both seen and unseen datasets.</p>
<p>D.2 Human Evaluations</p>
<p>We have included additional results from human evaluations in Table 5, primarily focusing on correlation scores between word overlap metrics and manual evaluations.Note that we continue to use the same samples used in human evaluation to compute BLEU and ROUGE-L for a fair comparison.We calculate the Pearson and Spearman coefficients between each automatic metric and the average human score.These results reveal that word overlap metrics, such as BLEU and ROUGE-L, exhibit notably lower correlation with manual scores.While BLEU and ROUGE-L may have a high correlation with relevance metrics, they are weak in providing a comprehensive evaluation of the generations.Conversely, evaluations conducted by ChatGPT demonstrate higher correlation with human evaluations, as illustrated in Table 1.However, there is still a significant need to explore advanced metrics, particularly automated ones, in the context of scientific discovery.
V ic u n a -3 3 b -v 1 .3 (0 -s h o t) V ic u n a -3 3 b -v 1 .3 (5 -s h o t) V ic u n a -3 3 b -v 1 .3 (5 -s h o t) * L la m a -2 -7 0 b -c h a t( 0 -s h o t) L la m a -2 -7 0 b -c h a t( 5 -s h o t) L la m a -2 -7 0 b -c h a t( 5 -s h o t) * W iz a rd L M -1 3 B -V 1 .2 (0 -s h o t) W iz a rd L M -1 3 B -V 1 .2 (5 -s h o t) W iz a rd L M -1 3 B -V 1 .2 (5 -s h o t) * W iz a rd L M -7 0 B -V 1 .0 (0 -s h o t) W iz a rd L M -7 0 B -V 1 .0 (5 -s h o t) W iz a rd L M -7 0 B -V</p>
<p>E Case Study</p>
<p>In this section, we present several generated hypotheses from various models and provide examples of the evaluation process, step by step, using ChatGPT.</p>
<p>E.1 Generated Hypothesis</p>
<p>We compare the generated hypotheses of different LLMs selected in human evaluation.The selected medicine literature was published in August 2023 (Chang et al., 2023), which proposed the power law index as an early marker of myocardial infarction.As shown in Table 7, some responses like gpt-3.5-turbo(0-shot) propose this hypothesis from zero, based only on the background.This indicates that LLMs have great potential in proposing highly novel and verifiable hypotheses and could play a significant role in scientific discovery.The case study focuses on MI predictive biomarkers and two-stage power-law rheology.GPT-3.5 identifies a "fundamental biomechanical principle," while Llama-2-70b-chat discusses "changes in the levels of collagen and proteoglycans in the extracellular matrix," and WizardLM-70B-V1.0addresses "the progression of myocardial Table 6: Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset.The results demonstrate that both multi-agent systems and the utilization of tools enhance the ability of LLMs in hypothesis generation.Among the various types of models, both 2 a and 2 b are evaluated with tool use.The difference between them lies in their implementations: ReAct (Yao et al., 2023b) and OpenAI function calling 4 , respectively.</p>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.</p>
<p>(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.</p>
<p>(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<p>Golden Hypothesis:</p>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.</p>
<p>(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.</p>
<p>(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<p>Generated Hypothesis:</p>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.</p>
<p>(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement.</p>
<p>It can now be further evaluated and tested in future research.</p>
<p>Dialogue process of multi-agent collaboration.</p>
<p>(Round 1) Analyst: Based on the provided research background, I have identified the following keywords and topics:</p>
<p>Keywords:</p>
<p>-Digital electrophysiology -Telemonitoring -Cardiac implantable electronic devices (CIEDs) -Wearable devices -Cardiac rhythm monitoring -Photoplethysmography</p>
<p>Topics:</p>
<ol>
<li>Advancements in digital electrophysiology and telemonitoring of CIEDs 2. Benefits of telemonitoring in improving patient outcomes 3. Accuracy and reliability of wearable devices for cardiac rhythm monitoring using photoplethysmography Preliminary insights:</li>
</ol>
<p>-The field of digital electrophysiology has seen significant development in the past 25 years, particularly in the areas of telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.</p>
<p>-Telemonitoring of CIEDs has been shown to have positive impacts, such as reducing the time to detect clinically actionable events and improving patient outcomes.</p>
<p>-Wearable devices, including smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<p>Based on these insights, it can be hypothesized that the advancements in telemonitoring and wearable devices have led to improved cardiac rhythm monitoring and patient outcomes.However, further analysis and collaboration with the Scientist and Critic are needed to refine and validate this hypothesis.</p>
<p>(Round 2) Scientist: Based on the insights provided by the Analyst and the research background, I propose the following hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes and more accurate detection of clinically actionable events.</p>
<p>This hypothesis suggests that the development and implementation of telemonitoring technologies for CIEDs, along with the increasing accuracy and reliability of wearable devices using photoplethysmography, have positively impacted patient outcomes by reducing the time to detect clinically actionable events.By continuously monitoring cardiac rhythm, these advancements have the potential to provide timely interventions and improve overall patient care.</p>
<p>The hypothesis is grounded in the existing literature, which highlights the benefits of telemonitoring in improving patient outcomes and the increasing accuracy of wearable devices for cardiac rhythm monitoring.However, further collaboration with the Critic is necessary to evaluate the novelty, coherence, and scientific validity of the hypothesis.</p>
<p>(Round 3) Critic: The hypothesis proposed by the Scientist is well-aligned with the research background and the insights provided by the Analyst.It effectively captures the essence of the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring.The hypothesis also highlights the potential positive impact on patient outcomes and the detection of clinically actionable events.</p>
<p>However, there are a few areas that could be further refined or expanded upon in the hypothesis:</p>
<ol>
<li>
<p><strong>Specific outcomes</strong>: The hypothesis mentions ""improved patient outcomes,"" but it would be beneficial to specify the specific outcomes that are expected to improve.For example, are we referring to reduced hospitalizations, improved quality of life, or decreased mortality rates?</p>
</li>
<li>
<p><strong>Mechanisms</strong>: The hypothesis briefly mentions the potential for timely interventions and improved patient care, but it would be helpful to elaborate on the specific mechanisms through which the advancements in telemonitoring and wearable devices are expected to achieve these outcomes.Are there specific interventions or treatments that can be administered based on the data collected from these devices?</p>
</li>
<li>
<p><strong>Scientific validity</strong>: While the hypothesis is grounded in the existing literature, it would be valuable to provide some references or studies that support the claims made in the hypothesis.This would enhance the scientific validity of the hypothesis and demonstrate that it is not just speculative but based on empirical evidence.</p>
</li>
</ol>
<p>To refine the hypothesis, I would recommend the Scientist to consider incorporating these suggestions and providing more specific details and references to support the claims made.This will strengthen the coherence and scientific validity of the hypothesis.</p>
<p>Analyst, based on the feedback provided, please re-evaluate the research background and provide any additional insights or interpretations that could further refine the hypothesis.</p>
<p>(Round 4) Analyst: Based on the feedback from the Critic, I have re-evaluated the research background and identified additional insights:</p>
<ol>
<li>
<p><strong>Specific outcomes</strong>: The research background mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.These outcomes could include early detection of arrhythmias, reduced hospitalizations due to cardiac events, improved quality of life, and decreased mortality rates.It would be beneficial to specify one or more of these specific outcomes in the hypothesis.</p>
</li>
<li>
<p><strong>Mechanisms</strong>: The advancements in telemonitoring and wearable devices can potentially lead to improved patient outcomes through several mechanisms.For example, continuous monitoring of cardiac rhythm can enable early detection of abnormal rhythms, allowing for timely interventions such as medication adjustments or electrical cardioversion.Additionally, the data collected from these devices can provide valuable insights for personalized treatment plans and remote patient management.Including these specific mechanisms in the hypothesis would enhance its coherence.</p>
</li>
<li>
<p><strong>Scientific validity</strong>: To strengthen the scientific validity of the hypothesis, it would be valuable to reference specific studies or research that support the claims made.For example, citing studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events or improving patient outcomes would provide empirical evidence to support the hypothesis.</p>
</li>
</ol>
<p>Incorporating these additional insights and suggestions, the refined hypothesis could be:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis takes into account the specific outcomes, mechanisms, and scientific validity, as suggested by the Critic.It provides a more comprehensive and well-supported statement that captures the essence of the research background and the collaborative insights.</p>
<p>(Round 5) Scientist: Based on the additional insights and suggestions provided by the Analyst and the Critic, I propose the following refined hypothesis:</p>
<p>Hypothesis: The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>This refined hypothesis incorporates the specific outcomes mentioned in the research background, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.It also expands on the mechanisms through which these advancements are expected to achieve these outcomes, including early detection of abnormal rhythms, timely interventions, and personalized treatment plans.Additionally, the hypothesis acknowledges the importance of scientific validity by referencing studies that support the claims made.</p>
<p>The refined hypothesis is both innovative and scientifically grounded, as it builds upon the existing literature and incorporates the collaborative insights provided by the Analyst and the Critic.It provides a logical and well-supported statement that can be further evaluated and tested.</p>
<p>(Round 6) Critic: The refined hypothesis proposed by the Scientist is well-structured and addresses the feedback provided by the Critic.It incorporates specific outcomes, mechanisms, and scientific validity, making it a comprehensive and scientifically robust statement.</p>
<p>Final Answer:</p>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.</p>
<p>(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.</p>
<p>The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement.It can now be further evaluated and tested in future research.</p>
<p>Table 8: This table illustrates a case from multi-agent collaboration on hypothesis proposing.</p>
<p>E.2 Evaluation Examples</p>
<p>We provide evaluation results conducted by ChatGPT in Table 9. ChatGPT offers a stepby-step explanation for each metric, ultimately providing independent scores in integer format.</p>
<p>CASE OF CHATGPT AS ANNOTATOR</p>
<p>Background:</p>
<p>(1) The past 25 years have seen significant development in the field of digital electrophysiology, including advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.</p>
<p>(2) Telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.</p>
<p>(3)Wearable devices, such as smartwatches, rings, and fitness trackers, have become more accurate and reliable in monitoring cardiac rhythm using photoplethysmography.</p>
<p>Golden Hypothesis:</p>
<p>(1) The implementation of telemonitoring and the use of wearable devices for cardiac rhythm monitoring will continue to increase in the future.</p>
<p>(2) Artificial intelligence can be utilized to better identify actionable alerts and improve patient outcomes in telemonitoring of CIEDs.</p>
<p>(3) Smartwatches may have the potential to replace CIEDs in certain cases.</p>
<p>Generated Hypothesis:</p>
<p>(1) The advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.</p>
<p>(2) These advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>(3) This hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.The refined hypothesis captures the essence of the research background and the collaborative insights, providing a logical and scientifically grounded statement.</p>
<p>It can now be further evaluated and tested in future research.</p>
<p>Evaluation Results</p>
<p>Novelty:</p>
<p>Explanation: The generated scientific hypothesis builds upon the background information by stating that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes.It further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.This hypothesis goes beyond the background by proposing specific mechanisms through which the advancements in telemonitoring and wearable devices contribute to improved patient outcomes.It also mentions the supporting evidence from studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.Overall, the hypothesis provides new insights and expands upon the existing knowledge in the field.Score: 2</p>
<p>Relevance:</p>
<p>Explanation: The generated scientific hypothesis is strongly related to the given background.The background provides information about the advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.The hypothesis builds upon this information by stating that these advancements have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.It further explains that these outcomes are achieved by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.The hypothesis is supported by the background information, which mentions that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.Overall, the hypothesis is directly related to the background and provides a logical and scientifically grounded statement.Score: 3</p>
<p>Significance:</p>
<p>Explanation: The generated scientific hypothesis builds upon the background information provided and presents a logical and scientifically grounded statement.It suggests that the advancements in telemonitoring of CIEDs and the use of wearable devices for cardiac rhythm monitoring have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.The hypothesis further suggests that these advancements achieve these outcomes by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.This hypothesis is supported by the background information that mentions the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.Overall, the hypothesis provides significant insights beyond the background and presents a testable statement for future research.Score: 3</p>
<p>Verifiability:</p>
<p>Explanation: The generated scientific hypothesis is highly verifiable based on the given background.The background provides evidence of significant advancements in telemonitoring of cardiac implantable electronic devices (CIEDs) and the use of wearable devices for cardiac rhythm monitoring.It also states that telemonitoring of CIEDs has been shown to reduce time to detection of clinically actionable events and improve patient outcomes.Additionally, it mentions that wearable devices have become more accurate and reliable in monitoring cardiac rhythm.</p>
<p>The hypothesis builds upon this background by suggesting that these advancements have led to improved patient outcomes, such as reduced hospitalizations, improved quality of life, and decreased mortality rates.It further proposes that these outcomes are achieved by enabling early detection of abnormal rhythms, facilitating timely interventions, and providing valuable data for personalized treatment plans.</p>
<p>The hypothesis is supported by studies that have demonstrated the effectiveness of telemonitoring in reducing time to detection of clinically actionable events and improving patient outcomes.Therefore, the hypothesis is highly verifiable and has a strong likelihood of being confirmed in future research.Score: 3</p>
<p>Table 9: This table illustrates a case for hypothesis evaluation using ChatGPT.</p>
<p>F Prompts Design and Finetuning</p>
<p>In this section, we offer prompts for prompt LLMs for hypothesis generation and for ChatGPT in the evaluation process.</p>
<p>F.1 Zero-and Few-shot Prompts</p>
<p>We provide prompts for hypothesis generation under both zero-shot and few-shot settings, which are detailed in Table 10 and Table 11 respectively.Specially, the latter includes two methods to obtain in-context examples: random sampling and similarity retrieval.</p>
<p>ZERO-SHOT INSTRUCTION FOR HYPOTHESIS GENERATION.</p>
<p>You are a researcher.You can give novel hypothesis based on your exist knowledge and the given background.</p>
<p>Based on the known existing knowledge, generate new conjectures in the following format:</p>
<p>(1) xxx (2) xxx (3) xxx Be sure to use English answers (proper nouns need to be marked in English), statements as concise and academic as possible, do not have too much repetitive information, numerical values using the original numbers, be sure to strictly follow the format, the corresponding content output to xxx.</p>
<p>Note: Please respond directly to the multiple hypotheses without adding any extra sentences.Now give hypothesis based on the following background: {user input} Table 10: Prompt for zero-shot hypothesis generation.</p>
<p>F.2 Prompts for Multi-agent Collaboration</p>
<p>We present prompts for each role in multi-agent collaboration in Table 12, and prompts for environment settings in Table 13.</p>
<p>PROMPTS FOR ROLE DESIGN IN MULTI-AGENT COLLABORATION Analyst:</p>
<p>You are the Analyst.Depending on the phase of the iteration, your role may slightly differ:</p>
<p>-<strong>Initial Phase</strong>: Analyze the provided research background to distill its core components into pivotal keywords or topics.This will set the stage for the Engineer's search efforts.</p>
<p>-<strong>Feedback Phase</strong>: Based on feedback from the Critic, you might need to re-analyze the research background or provide additional insights to refine the search direction.</p>
<p>In either case, ensure clarity and relevance in your analysis.Conclude by listing the identified keywords or topics or by providing revised insights.</p>
<p>Engineer:</p>
<p>You are the Engineer.Your task revolves around searching based on the received keywords or insights, and this can involve multiple iterations:</p>
<p>-Plan your search strategies by crafting logical keyword combinations.</p>
<p>-Conduct systematic searches for each combination, meticulously gathering data and results.</p>
<p>-Refine your searches iteratively based on initial findings and any new insights from the Analyst.</p>
<p>Your output should be comprehensive and organized.For each keyword combination:</p>
<p>-<strong>Title of Source</strong>: Provide the title of the paper, article, or material you've found.</p>
<p>-<strong>Abstract/Summary</strong>: A brief summary or the abstract of the source.</p>
<p>-<strong>Key Findings</strong>: Highlight pivotal points or findings from the source that are relevant to the research background.</p>
<p>-<strong>Implications</strong>: If any, mention the implications or significance of the findings.</p>
<p>-<strong>Relevant Quotes/Excerpts</strong>: Extract direct quotes or sections that are particularly insightful.Group your findings into individual "clues" based on themes or topics that emerge.This structure will provide the Scientist with detailed and organized data, enabling them to craft a robust hypothesis.</p>
<p>Conclude by presenting the structured "clues" for each keyword combination.</p>
<p>Scientist:</p>
<p>You are the Scientist.Your task is to craft a hypothesis based on the Engineer's findings and the initial research background:</p>
<p>-Derive a potential hypothesis that bridges the existing literature with new insights.</p>
<p>-Ensure the hypothesis is both innovative and scientifically grounded.</p>
<p>Clearly state the proposed hypothesis, preparing it for evaluation by the Critic.</p>
<p>Critic:</p>
<p>You are the Critic, responsible for evaluating the collaborative endeavor.Scrutinize the Scientist's hypothesis in light of the 'Research Background'.Gauge its novelty, coherence, and scientific validity.Should the hypothesis necessitate refinement:</p>
<p>-Clearly articulate feedback, specifying areas needing improvement.</p>
<p>-Instruct the Analyst to either re-evaluate the 'Research Background' or offer new insights to reshape the Engineer's subsequent search iteration.</p>
<p>When the hypothesis aligns with expectations and meets the desired standards, present and approve it using the structured format:  You are an expert in biomedicine.</p>
<p>Evaluate the significance of the generated scientific hypothesis and the given background.</p>
<p>The score range should be 0 to 3. 0 means there's no significance, which indicates that the hypothesis is just a common knowledge.1 means there's slight significance.2 means there's moderate significance.3 means the hypothesis has strong significance, which gives significant insights beyond the background.</p>
<p>Output is an integer.</p>
<p>Please provide a step-by-step explanation supporting your score.At the end of your response, clearly state the score in the format 'Score: [value]', where [value] can be 1, 2, or 3.</p>
<p>Background: {background} Generated scientific hypothesis: {hypothesis}</p>
<p>Figure 1: This illustration demonstrates a generated hypothesis using the fine-tuned 65B LLaMA model within our specially constructed dataset.The generated hypothesis closely aligns with the findings in existing literature published subsequent to the training sets.</p>
<p>Figure 2: (a) The iterative loop of scientific discovery involves a cyclical process: observations and data from previous experiments are analyzed, leading to the generation of new hypotheses.These hypotheses then guide the design of subsequent experiments, producing fresh data to perpetuate the cycle.(b) We execute the automated data partitioning pipeline, using publication dates as the basis, to mitigate the risk of data contamination.</p>
<p>Figure 3 :
3
Figure 3: This figure displays the BLEU scores on both seen and unseen datasets.</p>
<p>Figure 4 :
4
Figure 4: This figure depicts a comparative analysis of multiple models across distinct prompting paradigms, such as zero-shot, sampled and similarity retrieval-based few-shot.</p>
<p>Correlation of uncertainty and verifiability.</p>
<p>Results of tool use and multi-agent.</p>
<p>Figure 6 :
6
Figure 6: (a) The conceptual system of multi-agent collaboration for hypothesis generation.The overall prototyping process is illustrated below, allowing users to choose optional involvement.We offer core role descriptions of multi-agents and the fully automated system above.(b) Results of individual agents and multi-agent systems, both with and without the use of tools, on the unseen test dataset.The results demonstrate the influence of multi-agent collaboration and tool use in enhancing the ability of LLMs in hypothesis generation.</p>
<p>Figure 7 :
7
Figure 7: Distribution of the background and hypothesis pairs (BHP) dataset: we present the publication distribution by year for the training and seen test datasets, indicating a steady increase year by year until January 2023.In the center panel, we depict the publication distribution by month for the unseen test dataset, which was sampled from August 2023 and emphasizes the latter part of the month.</p>
<p>Distribution of ChatGPT and GPT-4 data.</p>
<p>Figure 8 :
8
Figure 8: Distribution of background (left in (a) and (b)) and hypothesis (right in (a) and (b)) texts.(a): A slight difference is observed between the distributions of seen and unseen datasets, attributable to the differences in publication dates.(b): The distribution of datasets constructed by ChatGPT and GPT-4 shows minimal variance.</p>
<p>Figure 9 :
9
Figure 9: This figure displays the ROUGE Scores on both seen and unseen datasets.</p>
<p>[First Point or Aspect of the Hypothesis] (2) [Second Point or Aspect of the Hypothesis] (3) [Third Point or Aspect of the Hypothesis] ...</p>
<p>Seen test-set vs. Unseen test-set Despite the
and 4 demonstrate the significant impact of zero-shotsettings in improving hypothesis generation, particularly in terms of fostering high novelty.Detailed results are provided in Appendix D.1. In this section, we primarily analyze theseresults from two critical perspectives as outlined below:Zero-shot vs. Few-shot Figure 4 demonstrates that nearly all models, particularly WizardLMseries models, and Openchat-v3.2-super, show enhanced novelty capabilities in a zero-shotsetting. Concurrently, these models also demonstrate superior verifiability when presentedwith few-shot literature examples. This indicates a trade-off in the hypothesis generationcapacity of LLMs, necessitating careful consideration of specific constraints. Notably, thereare divergent results concerning medical domain adaptation in LLMs, which will be furtherdiscussed in the subsequent Section 3.2.2.inclusion of literature published before 2023in the pre-training corpus of most LLMs, we have delineated "seen" and "unseen" testsets, with the "unseen" test set being considered for zero-shot analysis. Generally, LLMsare likely to show enhanced performance on the "seen" test set, potentially due to thememorization of knowledge acquired during training, resulting in a superior performance</p>
<p>Table 1 :
1
This table presents the results of human evaluation.The Avg Coefficient are used to assess the correlation between the average scores obtained from ChatGPT and human.
CategoryModelChatGPTHuman EvalAvg CoefficientEval.Avg Novelty Relevance Significance Verifiability Avg Pearson SpearmanAPI-basedgpt-3.5-turbo(0-shot) gpt-3.5-turbo(5-shot)<em>1.90 1.961.54 1.312.69 2.621.77 2.082.08 2.622.02 0.87 2.15 0.800.78 0.78Llama-2-70b-chat(0-shot)2.041.772.231.921.921.96 0.890.84Llama-2-70b-chat(5-shot)2.202.152.772.082.312.33 0.960.90GeneralLlama-2-70b-chat(5-shot)</em>2.011.382.622.312.002.08 0.970.94WizardLM-70B-V1.0(0-shot) 1.911.382.311.542.001.81 0.900.75WizardLM-70B-V1.0(5-shot) 2.011.152.692.461.772.02 0.850.89MedicinePMC-LLaMA-13B(0-shot) PMC-LLaMA-13B(5-shot)*1.41 1.971.00 1.852.62 2.231.92 1.922.00 1.691.88 0.73 1.92 0.950.73 0.94SFTWizardLM-13BV1.21.790.852.771.232.231.77 0.830.85
Randomly Sampled Few-Shot Examples vs. Similarity Retrieval.Given that randomly sampled in-context examples frequently diverge in topics or domains from the given background, this variation can potentially confuse LLMs while heightening uncertainty.In contrast, few-shot examples acquired through similarity retrieval are more likely to bolster verifiability but may reduce novelty.</p>
<p>Table 2
2: (GPT-3.5 Datasets)Results of various LLMs: We assess instructed models usingzero-shot and few-shot format prompts to generate constrained outputs. To provide a com-prehensive assessment, we calculate the average scores for novelty, relevance, significance,
and verifiability, denoted as Avg.Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs.To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<p>Table 3 :
3
(GPT-4 datasets) Results of various LLMs: We assess instructed models using zero-shot and few-shot format prompts to generate constrained outputs.To provide a comprehensive assessment, we calculate the average scores for novelty, relevance, significance, and verifiability, denoted as Avg.Results marked with an asterisk (*) indicate that the few-shot prompts are constructed by retrieving samples from the training set that are similar to the background of inputs.To facilitate better comparison, we highlight the highest and sub-high score with both bold and underline formatting under each category.</p>
<p>Table 4 :
4
To further ensure the non-visibility of the test data, we provide an overview of the related literature corpus within the training set of various LLMs, accompanied by their respective publication dates.The data marked with (<em>) is the data generated by people talking to ChatGPT.Our date marking is consistent with ChatGPT.
Category ModelBase Model SFT Data (Y/M)Releasedgpt-3.5-turbo (0-shot) GPT-3Unknown2022/12API-basedgpt-3.5-turbo (5-shot) GPT-3Unknown2022/12gpt-4</em>GPT-4Unknown2023/06Vicuna-33b-v1.3Llama-1ShareGPT (Unknown)2023/06Llama-2-7b-chatLlama-2Unknown2023/07Llama-2-13b-chatLlama 2Unknown2023/07GeneralLlama-2-70b-chatLlama 2Unknown2023/07WizardLM-13B-V1.2Llama-2Alpaca and ShareGPT (2023/06) 2023/07WizardLM-70B-V1.0Llama-2Alpaca and ShareGPT (2023/06) 2023/08openchat-v3.2-superLlama-2Sharegpt4 Dataset (2023/06)2023/09MedAlpaca-13BLlama-1<em>Mixture (2023/03)2023/03MedicineChatDoctor</em>Llama-1<em>Mixture (2023/04)2023/04PMC-LLaMA-13BLlama-2</em>Mixture (2023/04)2023/08*</p>
<p>Table 5 :
5
The table illustrates the correlations between automatic metrics and human evaluations.We annotate the Pearson and Spearman scores after each correlation score, denoting them as r and ρ.
ModelInfluence FactorAutomaticGPT-4 EvaluationMulti-agent Tool use BLUE ROUGE Novelty Relevance Significance Verifiability Avg1--15.5226.481.232.571.842.031.922 a-14.9424.160.782.421.181.871.562 b-15.8724.940.572.580.931.891.493-11.7122.111.352.852.052.102.09411.1822.041.522.662.062.052.07</p>
<p>Table 12 :
12
Prompts for role design in multi-agent collaboration on hypothesis proposing task.PROMPT FOR CHATGPT EVALUATION ON RELEVANCE METRIC.You are an expert in biomedicine.Evaluate the relevance of the generated scientific hypothesis and the given background.The score range should be 0 to 3. 0 means there's no relevance.1means there's slight relevance.2means there's moderate relevance.3means they are strongly related.Output is an integer.Please provide a step-by-step explanation supporting your score.At the end of your response, clearly state the score in the format 'Score: [value]', where [value] can be 1, 2, or 3.
Background: {background}Generated scientific hypothesis: {hypothesis}</p>
<p>Table 15 :
15
Prompts for ChatGPT evaluation on relevance metric.</p>
<p>PROMPT FOR CHATGPT EVALUATION ON SIGNIFICANCE METRIC.</p>
<p>Table 16 :
16
Prompts for ChatGPT evaluation on significance metric.You are an expert in biomedicine.Evaluate the verifiability of the generated scientific hypothesis and the given background.The score range should be 0 to 3. 0 means there's no verifiability, which indicates that the hypothesis is not possible to be verified in future work. 1 means there's slight verifiability. 2 means there's moderate verifiability.3 means the hypothesis has strong verifiability, which means the hypothesis is very likely to be verified in future work.Output is an integer.Please provide a step-by-step explanation supporting your score.At the end of your response, clearly state the score in the format 'Score: [value]', where [value] can be 1, 2, or 3.</p>
<p>PROMPT FOR CHATGPT EVALUATION ON VERIFIABILITY METRIC.Background: {background} Generated scientific hypothesis: {hypothesis}</p>
<p>Table 17 :
17
Prompts for ChatGPT evaluation on verifiability metric.</p>
<p>https://tatsu-lab.github.io/alpaca eval/
https://pubmed.ncbi.nlm.nih.gov/
https://github.com/nomic-ai/nomic
https://openai.com/blog/function-calling-and-other-api-updates
AcknowledgementsThis work is supported by the National Science and Technology Major Project (2023ZD0121403).We extend our gratitude to the anonymous reviewers for their insightful feedback, which has greatly contributed to the improvement of this paper.damage and remodeling."These models not only propose hypotheses that align with the relevant background but also offer novel insights, demonstrating a capacity for originality beyond mere information extraction.In contrast, PMC-LLaMA-13B and WizardLM-13B-V1.2provided only straightforward interpretations of the results, without introducing new content.Furthermore, we showcase the proposed hypotheses in a multi-agent collaboration involving three roles: Analyst, Scientist, and Critic, working together to generate hypotheses across 6 rounds, as shown in Table8.INSTRUCTED LLMS CASEBackground:(1) In the early stage of MI, αshort and αlong can track and distinguish MI samples from healthy control samples with an AUC &gt; 0.7.(2) In the mid-stage and late-stage of MI, E2, E3, and Etotal exhibit excellent discriminating performance with an AUC &gt; 0.6.(3) All cardiac tissues display a universal two-stage power-law rheology at different time scales, independent of their disease status.Golden Hypothesis:(1) The sequential order of MI-related changes in cardiac tissues starts from the cytoplasmic level and progresses to the cellular and tissue levels.(2) Pathological changes at subcellular, cellular, and tissue levels are accompanied by mechanical alterations.(3) The proposed mechanical markers have diagnostic potential for assessing the progression of MI and studying other cardiovascular diseases.gpt-3.5-turbo (0-shot):(1) The combination of αshort and αlong can be used as a reliable biomarker for early detection and differentiation of myocardial infarction (MI) from healthy controls.(2) E2, E3, and Etotal can serve as effective biomarkers for distinguishing between mid-stage and late-stage MI patients with high accuracy.(3) The universal two-stage power-law rheology observed in cardiac tissues may play a crucial role in maintaining tissue integrity and function, regardless of the presence or absence of disease.gpt-3.5-turbo (5-shot):(1) The levels of αshort and αlong may serve as potential biomarkers for early detection and diagnosis of myocardial infarction (MI).(2) E2, E3, and Etotal may have utility as biomarkers for assessing the progression and severity of MI in the mid and late stages.(3) The universal two-stage power-law rheology observed in cardiac tissues suggests a fundamental biomechanical principle underlying tissue behavior, regardless of disease status.Llama-2-70b-chat (0-shot):(1) E2, E3, and Etotal can also track and distinguish MI samples from healthy control samples in the early stage of MI, with an AUC &gt; 0.7, similar to αshort and αlong.(2) The universal two-stage power-law rheology of cardiac tissues is related to the alteration of the extracellular matrix and the degradation of collagen in the early stage of MI, which can be detected by E2, E3, and Etotal.(3) The discriminating performance of E2, E3, and Etotal in the mid-stage and latestage of MI is affected by the changes in the levels of collagen and proteoglycans in the extracellular matrix, which can be evaluated by measuring the mechanical properties of cardiac tissues using a uniaxial tensiometer.Llama-2-70b-chat (5-shot):(1) The discriminating performance of αshort and αlong in the early stage of MI is attributed to their ability to capture the subtle changes in cardiac tissue mechanics caused by the onset of fibrosis.(2) E2, E3, and Etotal exhibit excellent discriminating performance in the mid-stage and late-stage of MI due to their sensitivity to changes in collagen content and cross-linking, which are hallmarks of fibrosis progression.(3) The universal two-stage power-law rheology observed in all cardiac tissues, independent of disease status, suggests a common mechanism of mechanical regulation that can be exploited for early disease detection and monitoring.Llama-2-70b-chat (5-shot)*:(1) The αshort and αlong parameters can detect changes in myocardial stiffness in the early stages of myocardial infarction (MI).(2) E2, E3, and Etotal can differentiate between healthy controls and MI samples in the mid and late stages.(3) The universal two-stage power-law rheology in cardiac tissues may be useful in developing novel therapeutic approaches for MI.WizardLM-70B-V1.0 (0-shot):(1) αshort and αlong can track and distinguish MI samples from healthy control samples in all stages of MI with an AUC &gt; 0.7.(2) E2, E3, and Etotal exhibit excellent discriminating performance in the early stage of MI with an AUC &gt; 0.6.(3) The two-stage power-law rheology observed in all cardiac tissues is related to the progression of MI.WizardLM-70B-V1.0 (5-shot):(1) The discriminating performance of αshort and αlong in the early stage of MI may be attributed to their sensitivity to early molecular changes in the myocardium.(2) The discriminating performance of E2, E3, and Etotal in the mid-stage and late-stage of MI may be due to their ability to reflect the progression of myocardial damage and remodeling.(3) The universal two-stage power-law rheology observed in all cardiac tissues may be a fundamental property of cardiac tissue biomechanics, potentially useful for developing diagnostic and therapeutic strategies for MI.PMC-LLaMA-13B (0-shot):(1) αshort and αlong are effective biomarkers for distinguishing MI samples from healthy control samples both in the early stage and the late stage of MI.(2) E2 and E3 (Etotal) are likely to serve as effective biomarkers for discriminating mid-stage or late-stage MI samples from healthy control samples.(3) Cardiac tissues display a universal two-stage power-law rheology that is independent of their disease status.PMC-LLaMA-13B (5-shot):(1) Further comprehensive validation of the proposed framework is needed to elucidate the pathological changes in MI patients.(2) Consideration of multiple markers is important.(3) The influence of various external factors should be incorporated for accurate interpretation of the mechanical markers.WizardLM-13B-V1.2 (SFT):(1) The combination of αshort and αlong features can provide a more robust identification of MI samples.(2) E2, E3, and Etotal features can serve as independent predictors of MI.(3) The universal two-stage power-law rheology observed in all cardiac tissues may have diagnostic implications.Background:(1) Neonatal intensive care is associated with long-term health problems in children such as cerebral palsy, mental retardation, deafness, blindness, learning disabilities, and behavioral problems.(2) Mothers of preterm infants experience more severe psychological compared to mothers of healthy full-term infants, but the impact of caregiving on parents of children discharged from NICUs is not well-researched.(3) Parents of NICU children show no difference in psychosocial health compared to parents of healthy full-term children.Hypothesis:(1) The mental health of parents of NICU children may improve over time due to adaptation and relief from initial fear and anxiety.(2) Child characteristics, such as health status, behavior problems, and birth-related risk factors, may influence parental psychosocial health.(3) Certain factors, such as caregiver strain, family function, and demographic variables, may predict parental psychosocial health.Background:(1) Recruitment of tumor supporting stromal cells and tissue remodeling in the tumor microenvironment support cancer cell proliferation, invasion, metastasis, and drug resistance.(2) Mesenchymal stem cells (MSC) are recruited by cancer cells into the tumor site and play a role in modulating tumor progression.(3) Intratumoral heterogeneity exists in solid tumors, with cancer stem cells (CSCs) and clonal evolution contributing to the complexity of cancer.Hypothesis:(1) Transcriptional regulators are responsible for tumor-supporting stromal reprogramming, specifically in MSC in the tumor stroma.(2) Intercellular communication between cancer cells and recruited MSCs is mediated by cell-to-cell contact, paracrine interactions, and microvesicles.(3) Epithelial cancer cell plasticity is regulated by tumor stroma interaction signals, enabling non-CSCs to convert into CSCs.... Background: {input} Hypothesis:  Your collaboration is iterative.Based on feedback from the Critic, the process can loop back to the Analyst for refined insights, leading to new searches by the Engineer, and a refined hypothesis by the Scientist.Stay focused on your individual roles, collaborate effectively, and aim to derive a well-informed, novel hypothesis based on the research background provided.Research Background: backgroundObjective: Using the research background and collaborative insights, the goal is to construct the most logical and scientifically robust hypothesis.Let's collaborate effectively to achieve this.Table13: Prompts for environment setting in multi-agent collaboration.PROMPT FOR CHATGPT EVALUATION ON NOVELTY METRIC.You are an expert in biomedicine.Evaluate the novelty of the generated scientific hypothesis and the given background.The score range should be 0 to 3. 0 means there's no novelty, which indicates that the hypothesis is a paraphrase of the background. 1 means there's slight novelty. 2 means there's moderate novelty.3 means the hypothesis has strong novelty, which gives new insights beyond the background.Output is an integer.Please provide a step-by-step explanation supporting your score.At the end of your response, clearly state the score in the format 'Score: [value]', where [value] can be 1, 2, or 3. Background: {background} Generated scientific hypothesis: {hypothesis} Table14: Prompts for ChatGPT evaluation on novelty metric.F.3 Prompts for ChatGPT EvaluationThe instruction formats for prompting ChatGPT for evaluation on novelty, relevance, significance, and verifiability are displayed in Table14F.4 Finetuning ImplementationThe fine-tuning process consists of three epochs, employing a batch size of 8, a maximum sequence length of 2048 tokens, and a learning rate set at 3e-5.We implement early stopping and select the best checkpoints based on their performance on the seen test dataset.
Jointly measuring diversity and quality in text generation models. Danial Alihosseini, Ehsan Montahaei, Mahdieh Soleymani, Baghshah , 10.18653/v1/W19-2311Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation. the Workshop on Methods for Optimizing and Evaluating Neural Language GenerationMinneapolis, MinnesotaAssociation for Computational Linguisticsjun 2019</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>ChemCrow: Augmenting large-language models with chemistry tools. Andres M Bran, Sam Cox, Andrew D White, Philippe Schwaller, arXiv:2304.05376June 2023</p>
<p>The Life Cycle of Knowledge in Big Language Models: A Survey. Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, arXiv:2303.07616March 2023</p>
<p>New Mechanical Markers for Tracking the Progression of Myocardial Infarction. Zhuo Chang, Jing Zhang, Yilun Liu, Huajian Gao, Guang-Kui Xu, 10.1021/acs.nanolett.3c01712Nano Letters. 1530-69842316August 2023</p>
<p>Empowering Private Tutoring by Chaining Large Language Models. Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, arXiv:2309.08112[cs]version:1September 2023</p>
<p>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. Yao Fu, Hao Peng, Tushar Khot, Mirella Lapata, arXiv:2305.10142May 2023</p>
<p>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, 10.1073/pnas.2305016120arXiv:2303.15056Proceedings of the National Academy of Sciences. 0027-842412030July 2023</p>
<p>MindAgent: Emergent Gaming Interaction. Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, arXiv:2309.09971[cs]version:1September 2023</p>
<p>Medalpaca-an opensource collection of medical conversational ai models and training data. Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Daniel Alexander L Öser, Keno K Truhn, Bressem, arXiv:2304.082472023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, arXiv:2308.00352August 2023</p>
<p>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.03901June 2023</p>
<p>Gflownets for ai-driven scientific discovery. Moksh Jain, Tristan Deleu, Jason Hartford, Cheng-Hao Liu, Alex Hernandez-Garcia, Yoshua Bengio, Digital Discovery. 232023</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Think Before You Act: Decision Transformers with Internal Working Memory. Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, Jie Fu, arXiv:2305.16338May 2023</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, NeurIPS ML Safety Workshop. </p>
<p>CAMEL: Communicative Agents for. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, arXiv:2303.17760Exploration of Large Scale Language Model Society. March 2023</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, G-Eval, arXiv:2303.16634NLG Evaluation using GPT-4 with Better Human Alignment. May 2023</p>
<p>Philipp Maas, Frank Carey, Chris Wheeler, Edward Saatchi, Pete Billington, Jessica Yaffa Shamash, SHOW-1 and Showrunner Agents in Multi-Agent Simulations. 2023arXiv preprint</p>
<p>WebGPT: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, arXiv:2112.09332June 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>TALM: Tool Augmented Language Models. Aaron Parisi, Yao Zhao, Noah Fiedel, arXiv:2205.12255May 2022</p>
<p>Sung Joon, Joseph C Park, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative Agents: Interactive Simulacra of Human Behavior. August 2023</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large Language Model Connected with Massive APIs. May 2023</p>
<p>Communicative Agents for Software Development. Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924August 2023</p>
<p>A Survey of Hallucination in Large Foundation Models. Amit Vipula Rawte, Amitava Sheth, Das, arXiv:2309.05922September 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023aarXiv preprint</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761February 2023b</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.17580May 2023</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366June 2023</p>
<p>RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, Sujian Li, arXiv:2306.06624June 2023</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023barXiv preprint</p>
<p>On the Planning Abilities of Large Language Models -A Critical Investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2305.15771May 2023</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023barXiv preprint</p>
<p>Self-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Machine learning. 81992</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903January 2023a</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Rui Weiwei L Ü, Hu, arXiv:2310.19341A more open bilingual foundation model. 2023barXiv preprint</p>
<p>Pmc-llama: Towards building open-source language models for medicine. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie, 2023</p>
<p>Xipeng Qiu, Xuanjing Huang, and Tao Gui. The Rise and Potential of Large Language Model Based Agents: A Survey. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, arXiv:2309.07864September 2023a</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023barXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, The Twelfth International Conference on Learning Representations. </p>
<p>Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions. Hui Yang, Sifu Yue, Yunzhong He, arXiv:2306.02224June 2023a</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023barXiv preprint</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.10601May 2023a</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629March 2023b</p>
<p>Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhiyuan Zhang-Ren Chen, Haoxin Ma, Ganqu Li, Biqing Cui, Xuekai Qi, Zhu, arXiv:2406.03949Building specialized generalists in biomedicine. 2024arXiv preprint</p>
<p>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, arXiv:2309.01219September 2023</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, arXiv:2302.142332023arXiv preprint</p>
<p>Don't make your llm an evaluation benchmark cheater. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.019642023arXiv preprint</p>
<p>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Yu Wang, Zhaoxiang Qiao, Jifeng Zhang, Dai, arXiv:2305.17144June 2023a</p>
<p>Pad: Programaided distillation specializes large models in reasoning. Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen Zhou, arXiv:2305.138882023barXiv preprintPublished as a conference paper at COLM 2024</p>
<p>General Llama-2-70b-chat(0-shot). </p>
<p>. 70B-V1.0(5-shot) 9.86(-0.28/-0.37) 23.52(-0.17/-0.24) 2.01(0.85/0.89) 2.02(1.00/1.00Medicine PMC-LLaMA. 13B(0-shot</p>
<p>PMC-LLaMA-13B(5-shot)*. </p>
<p>SFT WizardLM-13B-V1. </p>            </div>
        </div>

    </div>
</body>
</html>
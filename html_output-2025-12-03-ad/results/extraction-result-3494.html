<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3494 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3494</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3494</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e5a1d41e6212951cb6a831ed61a59d00b7ff6867</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e5a1d41e6212951cb6a831ed61a59d00b7ff6867" target="_blank">Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> The task of Complex Sequential QA is introduced which combines the two tasks of answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and learning to converse through a series of coherently linked QA pairs.</p>
                <p><strong>Paper Abstract:</strong> 
 
 While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.
 
</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3494.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3494.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Complex Sequential Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale benchmark/dataset introduced in this paper that contains dialogs of coherently linked question-answer pairs grounded in a large Knowledge Graph (filtered Wikidata). Questions require simple, logical (set operations), quantitative (aggregation) and comparative reasoning over KG subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex Sequential Question Answering (CSQA) benchmark / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Dialog-style benchmark: 200K dialogs (1.6M turns) over a filtered Wikidata KG (~21.2M filtered tuples). Contains question types that require strict logical reasoning (set union, intersection, difference), quantitative aggregation (count, min/max, at-least/at-most), comparative reasoning (more/less, ranked comparisons), boolean verification, and combinations of these, often in conversational context (coreference, ellipsis, clarifications).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Dataset creation via in-house and crowdsourced annotation and templating; questions explicitly include logical operators (AND/OR/NOT), quantitative aggregation templates (How many, min/max, at least N), comparative templates (more/less than X), and conversational linking templates (coreference, ellipsis, clarification); no single logical-engine intervention — dataset is designed to evaluate such capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a dataset, limitations include some noisy/unnatural questions due to Wikidata quirks (generic predicates, overlapping types, long-tail relations) and template-instantiation artifacts; the paper notes that ~15% of questions may require very large candidate sets (>100k) making evaluation and retrieval costly.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper partitioned evaluation by question type (simple/direct, simple/coref, logical, quantitative, comparative, verification, clarification) and held out tuples across train/val/test splits; provides per-type breakdowns to enable targeted analysis but no ablation on dataset generation itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph', 'publication_date_yy_mm': '2018-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3494.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3494.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HRED+KV-Mem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Recurrent Encoder-Decoder combined with Key-Value Memory Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed end-to-end model combining a hierarchical dialogue encoder (HRED-style) to capture dialog context with a Key-Value Memory Network over KG candidate tuples; uses TransE KG embeddings, multi-hop memory attention, and a copy mechanism to emit KG entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hierarchical Encoder (HRED) + Key-Value Memory Network (KV-MemNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder: word-level RNN per utterance and higher-level RNN over utterances (HRED) to produce dialog/context representation. Knowledge: TransE-trained KG embeddings concatenated with GloVe where available. Memory: key-value memory network storing up to 10K candidate KG tuples (keys = relation+subject embeddings, values = object embeddings), multi-hop attention (H=2). Decoder: RNN sequence generator over a shortlist vocabulary plus a special KG_WORD token, with a copy mechanism that uses the memory to fill KG_WORD tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Complex Sequential Question Answering (CSQA) — logical, quantitative and comparative reasoning over KG subgraphs in dialog</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Strict logical reasoning operations required by the dataset include set intersection (AND), union (OR), set difference (NOT), multi-relation conjunctions; quantitative operations include count, min/max, at-least/at-most, approximate/equality; comparative reasoning involves counting and comparing distributions (e.g., 'countries with more rivers than X').</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Architectural combination (HRED + KV-MemNet); KG-aware token embeddings via offline TransE; candidate generation by longest n-gram matching to retrieve tuples; multi-hop (2 passes) memory attention; copy mechanism to output KG entities; training with cross-entropy over tokens and entity-selection; limited candidate memory (up to 10K entries) for tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall (Precision / Recall): 6.7% precision, 15.83% recall. Simple direct questions: recall 30.64%, precision 7.17%. Simple coreferenced: recall 25.85%, precision 8.0%. Simple ellipsis: recall 24.5%, precision 4.26%. Logical reasoning (all logical types): recall 4.85%, precision 19.70%. Quantitative reasoning (all): recall 0.24%, precision 0.11%. Comparative reasoning (all): recall 1.05%, precision 0.30%. Verification (boolean) accuracy: 8.34%. Quantitative counting accuracy: 10.66%. Comparing reasoning (count) accuracy: 1.3%. Clarification generation BLEU-4: 15.58; clarification KG-entity precision/recall: precision 10.8%, recall 30.64%. (Values reported exactly as in Table 4 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>No numerical baseline results on CSQA are reported for other models in the paper; the authors state that a combination of state-of-the-art dialog (HRED) and QA (memory networks) models were used and still found inadequate, but do not present parallel baseline numbers for alternative systems on this dataset (baseline_performance not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable / no improvement reported — the model is presented as a reasonable cross of existing SOTA components, and results demonstrate that even this combined approach performs poorly on complex logical, quantitative and comparative questions (i.e., it fails to achieve acceptable performance rather than improving over a baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Major failure modes reported: very poor performance on complex logical/quantitative/comparative questions (near-zero recall/precision for quantitative/comparative); inability to model aggregation functions and explicit logical/quantitative operations end-to-end; candidate generation via n-gram matching has low recall for paraphrases and explodes for very popular entities (100k+ tuples); KV memory's flat organization scales poorly in memory footprint and softmax computation for large candidate sets; encoder (HRED+KV) may not decompose complex queries into sub-queries (e.g., splitting 'India and China' into two independent retrievals); context handling (coreference/ellipsis) performance drops relative to direct questions; noisy/unintuitive questions from Wikidata cause additional difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No formal ablation studies reported; analyses reported include: hyperparameter sweep (embedding sizes, learning rates, batch sizes) and design choices (H=2 hops, up to 10K candidate tuples). The authors analyze performance by question type, identifying where the model fails (logical/quantitative/comparative) and diagnosing root causes (lack of explicit aggregation operators, candidate generation issues, memory scaling and softmax costs). They do not present isolated ablations that remove/replace components to quantify each component's contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph', 'publication_date_yy_mm': '2018-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3494.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3494.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KV-MemNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-Value Memory Network (as used for QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented neural architecture that stores (key, value) pairs (here: (relation+subject, object) embeddings) and performs attention over keys to read values; applied to retrieve answers from candidate KG tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Key-value memory networks for directly reading documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Key-Value Memory Network (KV-MemNet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Memory network variant that stores candidate knowledge items as key-value pairs; attention computed over keys (relation+subject embeddings) and results read from values (object embeddings); supports multiple hops to refine query representation. In this paper KV-MemNet is adapted to store up to 10K candidate KG tuples and used together with a hierarchical encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Used as the retrieval/answering component for CSQA (logical and quantitative KG reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Performs attention-weighted retrieval of candidate KG objects given a contextual query representation; not designed to perform explicit set operations or aggregations by itself but can be used as a component in an end-to-end model.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Multi-hop attention (H=2), keys formed from TransE embeddings of relation+subject, values as object embeddings; integrated into end-to-end decoder with copy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance reported in the paper is for the combined HRED+KV-Mem model (see other entry). The paper does not report isolated KV-MemNet-only numbers on CSQA.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Flat KV memory scales poorly when candidate set is large (10k+ up to 100k+); expensive softmax over many keys; insufficient to implement explicit logical/aggregation operators needed for complex questions; dependent on candidate generation quality (n-gram matching) which can have low recall for paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No isolated ablation of KV-MemNet vs alternatives reported; the authors analyze memory scaling and candidate generation as major practical bottlenecks for KV-MemNet usage on large-KG, complex reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph', 'publication_date_yy_mm': '2018-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large-scale simple question answering with memory networks <em>(Rating: 2)</em></li>
                <li>Key-value memory networks for directly reading documents <em>(Rating: 2)</em></li>
                <li>Translating embeddings for modeling multi-relational data <em>(Rating: 2)</em></li>
                <li>Learning a natural language interface with neural programmer <em>(Rating: 2)</em></li>
                <li>Semantic parsing via paraphrasing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3494",
    "paper_id": "paper-e5a1d41e6212951cb6a831ed61a59d00b7ff6867",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "CSQA",
            "name_full": "Complex Sequential Question Answering",
            "brief_description": "A large-scale benchmark/dataset introduced in this paper that contains dialogs of coherently linked question-answer pairs grounded in a large Knowledge Graph (filtered Wikidata). Questions require simple, logical (set operations), quantitative (aggregation) and comparative reasoning over KG subgraphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "reasoning_task_name": "Complex Sequential Question Answering (CSQA) benchmark / dataset",
            "reasoning_task_description": "Dialog-style benchmark: 200K dialogs (1.6M turns) over a filtered Wikidata KG (~21.2M filtered tuples). Contains question types that require strict logical reasoning (set union, intersection, difference), quantitative aggregation (count, min/max, at-least/at-most), comparative reasoning (more/less, ranked comparisons), boolean verification, and combinations of these, often in conversational context (coreference, ellipsis, clarifications).",
            "method_or_intervention": "Dataset creation via in-house and crowdsourced annotation and templating; questions explicitly include logical operators (AND/OR/NOT), quantitative aggregation templates (How many, min/max, at least N), comparative templates (more/less than X), and conversational linking templates (coreference, ellipsis, clarification); no single logical-engine intervention — dataset is designed to evaluate such capabilities.",
            "performance": null,
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "As a dataset, limitations include some noisy/unnatural questions due to Wikidata quirks (generic predicates, overlapping types, long-tail relations) and template-instantiation artifacts; the paper notes that ~15% of questions may require very large candidate sets (&gt;100k) making evaluation and retrieval costly.",
            "ablation_or_analysis": "The paper partitioned evaluation by question type (simple/direct, simple/coref, logical, quantitative, comparative, verification, clarification) and held out tuples across train/val/test splits; provides per-type breakdowns to enable targeted analysis but no ablation on dataset generation itself.",
            "uuid": "e3494.0",
            "source_info": {
                "paper_title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph",
                "publication_date_yy_mm": "2018-01"
            }
        },
        {
            "name_short": "HRED+KV-Mem",
            "name_full": "Hierarchical Recurrent Encoder-Decoder combined with Key-Value Memory Network",
            "brief_description": "The paper's proposed end-to-end model combining a hierarchical dialogue encoder (HRED-style) to capture dialog context with a Key-Value Memory Network over KG candidate tuples; uses TransE KG embeddings, multi-hop memory attention, and a copy mechanism to emit KG entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Hierarchical Encoder (HRED) + Key-Value Memory Network (KV-MemNet)",
            "model_description": "Encoder: word-level RNN per utterance and higher-level RNN over utterances (HRED) to produce dialog/context representation. Knowledge: TransE-trained KG embeddings concatenated with GloVe where available. Memory: key-value memory network storing up to 10K candidate KG tuples (keys = relation+subject embeddings, values = object embeddings), multi-hop attention (H=2). Decoder: RNN sequence generator over a shortlist vocabulary plus a special KG_WORD token, with a copy mechanism that uses the memory to fill KG_WORD tokens.",
            "model_size": null,
            "reasoning_task_name": "Complex Sequential Question Answering (CSQA) — logical, quantitative and comparative reasoning over KG subgraphs in dialog",
            "reasoning_task_description": "Strict logical reasoning operations required by the dataset include set intersection (AND), union (OR), set difference (NOT), multi-relation conjunctions; quantitative operations include count, min/max, at-least/at-most, approximate/equality; comparative reasoning involves counting and comparing distributions (e.g., 'countries with more rivers than X').",
            "method_or_intervention": "Architectural combination (HRED + KV-MemNet); KG-aware token embeddings via offline TransE; candidate generation by longest n-gram matching to retrieve tuples; multi-hop (2 passes) memory attention; copy mechanism to output KG entities; training with cross-entropy over tokens and entity-selection; limited candidate memory (up to 10K entries) for tractability.",
            "performance": "Overall (Precision / Recall): 6.7% precision, 15.83% recall. Simple direct questions: recall 30.64%, precision 7.17%. Simple coreferenced: recall 25.85%, precision 8.0%. Simple ellipsis: recall 24.5%, precision 4.26%. Logical reasoning (all logical types): recall 4.85%, precision 19.70%. Quantitative reasoning (all): recall 0.24%, precision 0.11%. Comparative reasoning (all): recall 1.05%, precision 0.30%. Verification (boolean) accuracy: 8.34%. Quantitative counting accuracy: 10.66%. Comparing reasoning (count) accuracy: 1.3%. Clarification generation BLEU-4: 15.58; clarification KG-entity precision/recall: precision 10.8%, recall 30.64%. (Values reported exactly as in Table 4 of the paper.)",
            "baseline_performance": "No numerical baseline results on CSQA are reported for other models in the paper; the authors state that a combination of state-of-the-art dialog (HRED) and QA (memory networks) models were used and still found inadequate, but do not present parallel baseline numbers for alternative systems on this dataset (baseline_performance not provided).",
            "improvement_over_baseline": "Not applicable / no improvement reported — the model is presented as a reasonable cross of existing SOTA components, and results demonstrate that even this combined approach performs poorly on complex logical, quantitative and comparative questions (i.e., it fails to achieve acceptable performance rather than improving over a baseline).",
            "limitations_or_failures": "Major failure modes reported: very poor performance on complex logical/quantitative/comparative questions (near-zero recall/precision for quantitative/comparative); inability to model aggregation functions and explicit logical/quantitative operations end-to-end; candidate generation via n-gram matching has low recall for paraphrases and explodes for very popular entities (100k+ tuples); KV memory's flat organization scales poorly in memory footprint and softmax computation for large candidate sets; encoder (HRED+KV) may not decompose complex queries into sub-queries (e.g., splitting 'India and China' into two independent retrievals); context handling (coreference/ellipsis) performance drops relative to direct questions; noisy/unintuitive questions from Wikidata cause additional difficulty.",
            "ablation_or_analysis": "No formal ablation studies reported; analyses reported include: hyperparameter sweep (embedding sizes, learning rates, batch sizes) and design choices (H=2 hops, up to 10K candidate tuples). The authors analyze performance by question type, identifying where the model fails (logical/quantitative/comparative) and diagnosing root causes (lack of explicit aggregation operators, candidate generation issues, memory scaling and softmax costs). They do not present isolated ablations that remove/replace components to quantify each component's contribution.",
            "uuid": "e3494.1",
            "source_info": {
                "paper_title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph",
                "publication_date_yy_mm": "2018-01"
            }
        },
        {
            "name_short": "KV-MemNet",
            "name_full": "Key-Value Memory Network (as used for QA)",
            "brief_description": "A memory-augmented neural architecture that stores (key, value) pairs (here: (relation+subject, object) embeddings) and performs attention over keys to read values; applied to retrieve answers from candidate KG tuples.",
            "citation_title": "Key-value memory networks for directly reading documents",
            "mention_or_use": "use",
            "model_name": "Key-Value Memory Network (KV-MemNet)",
            "model_description": "Memory network variant that stores candidate knowledge items as key-value pairs; attention computed over keys (relation+subject embeddings) and results read from values (object embeddings); supports multiple hops to refine query representation. In this paper KV-MemNet is adapted to store up to 10K candidate KG tuples and used together with a hierarchical encoder.",
            "model_size": null,
            "reasoning_task_name": "Used as the retrieval/answering component for CSQA (logical and quantitative KG reasoning)",
            "reasoning_task_description": "Performs attention-weighted retrieval of candidate KG objects given a contextual query representation; not designed to perform explicit set operations or aggregations by itself but can be used as a component in an end-to-end model.",
            "method_or_intervention": "Multi-hop attention (H=2), keys formed from TransE embeddings of relation+subject, values as object embeddings; integrated into end-to-end decoder with copy mechanism.",
            "performance": "Performance reported in the paper is for the combined HRED+KV-Mem model (see other entry). The paper does not report isolated KV-MemNet-only numbers on CSQA.",
            "baseline_performance": null,
            "improvement_over_baseline": null,
            "limitations_or_failures": "Flat KV memory scales poorly when candidate set is large (10k+ up to 100k+); expensive softmax over many keys; insufficient to implement explicit logical/aggregation operators needed for complex questions; dependent on candidate generation quality (n-gram matching) which can have low recall for paraphrases.",
            "ablation_or_analysis": "No isolated ablation of KV-MemNet vs alternatives reported; the authors analyze memory scaling and candidate generation as major practical bottlenecks for KV-MemNet usage on large-KG, complex reasoning tasks.",
            "uuid": "e3494.2",
            "source_info": {
                "paper_title": "Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph",
                "publication_date_yy_mm": "2018-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large-scale simple question answering with memory networks",
            "rating": 2
        },
        {
            "paper_title": "Key-value memory networks for directly reading documents",
            "rating": 2
        },
        {
            "paper_title": "Translating embeddings for modeling multi-relational data",
            "rating": 2
        },
        {
            "paper_title": "Learning a natural language interface with neural programmer",
            "rating": 2
        },
        {
            "paper_title": "Semantic parsing via paraphrasing",
            "rating": 1
        }
    ],
    "cost": 0.01108925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph</h1>
<p>Amrita Saha*<br>IBM Research AI and I.I.T. Madras, India amrsaha4@in.ibm.com</p>
<p>Vardaan Pahuja ${ }^{\dagger}$<br>MILA, Université de Montréal vardaanpahuja@gmail.com</p>
<h2>Mitesh M. Khapra</h2>
<p>I.I.T. Madras, India miteshk@cse.iitm.ac.in</p>
<h2>Karthik Sankaranarayanan</h2>
<p>IBM Research AI
kartsank@in.ibm.com</p>
<h2>Sarath Chandar</h2>
<p>apsarathchandar@gmail.com
MILA, Université de Montréal</p>
<h4>Abstract</h4>
<p>While conversing with chatbots, humans typically tend to ask many questions, a significant portion of which can be answered by referring to large-scale knowledge graphs (KG). While Question Answering (QA) and dialog systems have been studied independently, there is a need to study them closely to evaluate such real-world scenarios faced by bots involving both these tasks. Towards this end, we introduce the task of Complex Sequential QA which combines the two tasks of (i) answering factual questions through complex inferencing over a realistic-sized KG of millions of entities, and (ii) learning to converse through a series of coherently linked QA pairs. Through a labor intensive semi-automatic process, involving in-house and crowdsourced workers, we created a dataset containing around 200K dialogs with a total of 1.6 M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in our dialogs require a larger subgraph of the KG. Specifically, our dataset has questions which require logical, quantitative, and comparative reasoning as well as their combinations. This calls for models which can: (i) parse complex natural language questions, (ii) use conversation context to resolve coreferences and ellipsis in utterances, (iii) ask for clarifications for ambiguous queries, and finally (iv) retrieve relevant subgraphs of the KG to answer such questions. However, our experiments with a combination of state of the art dialog and QA models show that they clearly do not achieve the above objectives and are inadequate for dealing with such complex real world settings. We believe that this new dataset coupled with the limitations of existing models as reported in this paper should encourage further research in Complex Sequential QA.</p>
<h2>Introduction</h2>
<p>In recent years there has been an increased demand for AI driven personal assistants which are capable of conversing coherently with humans. Such personal assistants could benefit from large scale knowledge graphs which contain millions of facts stored as tuples of the form {predicate, subject, object} (for example, {director, Titanic, James</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Cameron $}$ ). Such knowledge graphs can indeed be handy when the bot is used in specific domains such as education, entertainment, sports, etc. where it is often required to answer factual questions while being aware of the context of the conversation. While Question Answering (?, ?, ?; ?, ?; ?, ?; ?; ?) and Conversation Systems (?, ?, ?; ?) have received a lot of attention in the recent past, we would like to focus on such real life settings encountered by chatbots which involve a combination of QA and dialog. Specifically, we are interested in building systems which can learn to converse over a series of coherently linked questions that can be answered from a large scale knowledge graph. We refer to this task as Complex Sequential Question Answering (CSQA).</p>
<p>Needless to say, CSQA is very different from the kind of conversations found in existing dialog datasets such as the Twitter (?), Ubuntu (?) and Movie Subtitles (?) datasets. Table ?? shows an example of one such conversation from our dataset containing a series of questions. Note that to answer the question in Turn 11, the bot needs to remember that the question involves the same predicate ('diplomatically related') as the previous question, but with a different subject ('Australia'). In other words, it is difficult to answer this question without retaining the context of the conversation. Further, in a natural conversation, some of the questions may require co-reference resolution (as in Turn 2), ellipsis resolution (as in Turn 11), etc. Finally, in some cases the question could be ambiguous (as in Turn 2) in which case the bot needs to ask for clarifications keeping in mind other entities and relations which were previously mentioned in the conversation.</p>
<p>While the example in Table ?? already highlights some of the challenges involved in CSQA, we now discuss an orthogonal set of challenges which arise from the complexity of the questions. Existing datasets for Factual QA (?; ?; ?) deal with Simple Questions, each of which can be answered from a single tuple in the KG. However, in a real-life setting, a bot could encounter more complex questions requiring logical, quantitative and comparative reasoning. Table ?? shows some examples of such questions. It should be clear that unlike simple questions, which can be answered from a single tuple, these questions require a larger subgraph of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Turn</th>
<th style="text-align: center;">State</th>
<th style="text-align: center;">Utterance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">T1</td>
<td style="text-align: center;">Simple Q.</td>
<td style="text-align: center;">USER : Can you tell me which cities border Verderio Inferiore ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Cornate d'Adda, Bernareggio, Robbiate</td>
</tr>
<tr>
<td style="text-align: center;">T2</td>
<td style="text-align: center;">Coreference Q.</td>
<td style="text-align: center;">USER : And which cities flank that one ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Clarification Q.</td>
<td style="text-align: center;">SYSTEM : Did you mean Robbiate ?</td>
</tr>
<tr>
<td style="text-align: center;">T3</td>
<td style="text-align: center;">Clarification</td>
<td style="text-align: center;">USER : No, I meant Cornate d'Adda.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Bottanuco, Busnago, Trezzo sull' Adda</td>
</tr>
<tr>
<td style="text-align: center;">T4</td>
<td style="text-align: center;">Logical Q.</td>
<td style="text-align: center;">USER : Which ones share border with Robbiate and are not part of Azerbaijan?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Trezzo sull' Adda, Busnago, Bottanuco</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Quantitative Q.</td>
<td style="text-align: center;">USER : Which city is the capital of min number of french administrative divisions?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Riga</td>
</tr>
<tr>
<td style="text-align: center;">T6</td>
<td style="text-align: center;">Boolean Q.</td>
<td style="text-align: center;">USER : Is Rot am See located adjacent to that one ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : No</td>
</tr>
<tr>
<td style="text-align: center;">T7</td>
<td style="text-align: center;">Quanti. Q.</td>
<td style="text-align: center;">USER : How many political territories have strategic rela tion with around 22 other countries?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : 11</td>
</tr>
<tr>
<td style="text-align: center;">T8</td>
<td style="text-align: center;">Logical Q.</td>
<td style="text-align: center;">USER : Which are the nations connected with Robbiate and are not located in Italy?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : The answer count is 11. Do you want to see all possibilities?</td>
</tr>
<tr>
<td style="text-align: center;">T9</td>
<td style="text-align: center;">Clarification</td>
<td style="text-align: center;">USER : No, show only a few of them</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Papua New Guinea, Jamaica, Antigua and Barbuda, Austria, New Zealand, The Bahamas, Gabon</td>
</tr>
<tr>
<td style="text-align: center;">T10</td>
<td style="text-align: center;">Quantitative Q.</td>
<td style="text-align: center;">USER : How many countries are diplomatically related to Italy?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : 74</td>
</tr>
<tr>
<td style="text-align: center;">T11</td>
<td style="text-align: center;">Quantitative Q.</td>
<td style="text-align: center;">USER : And how many of them also with Australia?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : 56</td>
</tr>
<tr>
<td style="text-align: center;">T12</td>
<td style="text-align: center;">Comparative Q.</td>
<td style="text-align: center;">USER : Which countries have lesser number of political relations with others than Australia?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Response</td>
<td style="text-align: center;">SYSTEM : Denmark, Canada, Grenada</td>
</tr>
</tbody>
</table>
<p>Table 1: A sample dialog from the dataset (More examples of generated dialogs are provided in the supplementary material)
the KG. For example, to answer the question "Which rivers flow through India and China ?" one needs to find (i) the set of rivers flowing through India (ii) the set of rivers flowing through China and finally (iii) the intersection between these two sets. Answering such questions requires models which can parse complex natural language questions, retrieve relevant subgraphs of the KG and then perform some logical, comparative and/or quantitative operations on this subgraph. Also the Knowledge Graph used in our work is orders of magnitude larger than those used in some existing works (?; ?; ?; ?) which lie at the intersection of QA and dialog.</p>
<p>Having motivated the task of CSQA and highlighted its differences from existing work on dialog and QA, we now briefly describe the process used for creating our dataset. As mentioned earlier, a KG contains tuples of the form {predicate, subject, object}. For each of the 330 predicates in our Knowledge Graph, we first asked workers on Amazon Mechanical Turk to create questions containing the predicate and the subject (or object) such that the answer to that question is the object (or subject). These questions are complete in the sense that they do not have any ambiguity and can be answered in isolation without requiring any additional context. We then ask in-house annotators to create multiple templates
for generating a conversation comprising of connected question answer pairs. Two question answer pairs are said to be connected if they contain the same predicate, subject or object. We then ask workers to make modifications to these questions so as to introduce challenges like co-references, ellipsis, incompleteness (or under specification) and contextual dependence. We also solicit templates and modifications to add logical, comparative and quantitative operators to the questions obtained above. This results in a dataset which contains conversations of the form shown in Table 7?.</p>
<p>The objective of this work is twofold: (i) to introduce the task of Complex Sequential QA and (ii) to show the inadequacy of current state of the art QA and dialog methods to deal with such tasks. Towards the second objective, we propose a model for CSQA which is a cross between a state of the art hierarchical conversation model (?) and a key value based memory network model for QA (?). Through our experiments, we demonstrate the inadequacy of these models and highlight specific challenges that need to be addressed. It is also worth mentioning that the unambiguous (context independent) questions which appear in our dataset (typically, at the start of the conversation) can also be used for studying Complex Question Answering (as opposed to Simple Question Answering) in isolation ignoring the dialog context. This will help to independently push the state of the art in Complex QA.</p>
<h2>Related Work</h2>
<p>Our work lies at the intersection of Question Answering and Dialog Systems. Question Answering has always been of interest to the research community starting from early TREC evaluations (?). Over the years various datasets and tasks have been introduced to advance the state of the art in QA. These datasets can be divided into 5 main types (i) TREC style Open Domain QA (?; ?; ?) where the aim is to answer a question from a collection of documents (ii) factoid QA over structured knowledge graphs (?; ?; ?) (iii) reading comprehension style QA (?; ?), (iv) cloze style QA (?; ?) (v) multiple choice QA (?; ?)</p>
<p>Of the above QA tasks, factoid QA is most relevant to us as the questions in our CSQA dataset are factoid questions. Existing factoid QA datasets contain Simple Questions which can be answered from a single tuple in the knowledge graph. Specifically, unlike our dataset, none of the existing datasets contain complex questions requiring logical, quantitative and comparative reasoning involving larger subgraphs of the KG as opposed to a single tuple. Solutions to the simple QA task range from semantic parsing based methods (?; ?) to embedding based methods (?; ?; ?) and state of the art Memory Networks based architectures (?; ?; ?). In this work, we experiment with Memory Network based architectures and make a case for the need of better architectures when going beyond simple questions.</p>
<p>Since we are interested in CSQA which contains a series of QA pairs over a coherent conversation, we also review some related work on dialog systems. Over the past few years three large scale dialog datasets, viz., Twitter-Conversations (?), Ubuntu Dialogue (?) and Movie-Dic Corpus (?) have become very popular. However, none of these datasets have</p>
<p>the flavor of CSQA and there is no explicit Knowledge Graph associated with the conversations. Here again, neural network based (hierarchical) sequence to sequence methods (?; ?; ?) have become the de facto choice. Recently, (?) proposed a dataset which contains knowledge graph driven goal oriented dialogs for the task of restaurant reservation. However, the size of the KG here is very small ( $&lt;10$ cuisines, locations, ambience, etc.) and the dialog contains very few states. (?) also uses a dataset for QA and recommendation but unlike our dataset, their dataset does not contain coherently linked question answer pairs. Further, the KG is again much smaller ( 75 K entities and 11 relations). Recently (?) have explored complex question answering over around 18.5 K queries from the WikiTableQuestions dataset, but their tables have less than 100 rows and a handful of columns whereas our complex QAs are grounded in a KB of over 20 million tuples. Further, their dataset does not have a conversational aspect.</p>
<h2>Dataset Creation</h2>
<p>Our aim is to create a dataset which contains a series of linked QA pairs forming a coherent conversation. Further, these questions should be answerable from a Knowledge Graph using logical, comparative and/or quantitative reasoning. We started by asking pairs of in-house annotators to converse with each other. One annotator in the pair acted as a user whose job was to ask questions and the other annotator acted as the system whose job was to answer the questions or ask for clarifications if required. Note that these annotators were Computer Science graduates who understood the concepts of knowledge graph, sub graph, tuples, subject, object, relation, etc. The idea was to use the in-house annotators to understand the types of simple and complex questions that can be asked over a knowledge graph. These could then be abstracted to templates and used to instantiate more questions involving different relations, subjects and objects. Similarly, we also wanted to understand the type of coreferences, ellipses etc used by users when asking linked questions over a coherent conversation. These could again be abstracted to templates and used to link individual QA pairs to form a coherent dialog. In the remainder of this section we describe (i) the knowledge graph supporting our CSQA (ii) simple question templates suggested by the in-house annotators (iii) complex question templates and finally (iv) the linked conversation templates and the process used to instantiate around 200K dialogs containing 1.6 million linked QA pairs.</p>
<h2>Knowledge Graph</h2>
<p>As our KG, we used wikidata which stores facts in the form of tuples containing a relation, a subject and an object. For example, (rel: capital, subj: India, obj: New Delhi) is a tuple in wikidata. Each entity (subject or object) is associated with an entity type. For example, in the above tuple, India is an entity of type country and New Delhi is an entity of type city. We use the wikidata dump of 14-Nov-2016 which contains 5.2 K relations, 12.8 M entities and 52.3 M facts. Of these 5.2 K relations, we retain only 330 meaningful ones. Specifically, we ignore relations such as "ISO 3166-1 alpha-2 code", "NDL Auth ID" etc., as we do not expect users to ask questions
about such obscure relations. Similarly, of the 30.8 K unique entity types in wikidata, we selected 642 types (considering only immediate parents of entities) which appeared in the top 90 percentile of the tuples associated with atleast one of the retained meaningful relations. In effect, there were around 21.2 M such tuples containing only the filtered relations and entity types. The total number of unique entities in these filtered tuples is 12.8 M out of which 3.8 M appear in atleast 3 tuples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reaso- <br> ning</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Containing</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Log- } \ &amp; \text { ical } \end{aligned}$</td>
<td style="text-align: center;">Union</td>
<td style="text-align: center;">Single <br> Relation</td>
<td style="text-align: center;">Which rivers flow through India or China?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Intersection</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Which rivers flow through India and China?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Difference</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Which rivers flow through India but not China?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Any of the above</td>
<td style="text-align: center;">Multiple Relations</td>
<td style="text-align: center;">Which river flows through India but does not originate in Himalayas?</td>
</tr>
<tr>
<td style="text-align: center;">Verifi- <br> cation</td>
<td style="text-align: center;">Boolean</td>
<td style="text-align: center;">Single/Multiple entities</td>
<td style="text-align: center;">Does Ganga flow through India ?</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Quant- } \ &amp; \text { itative } \end{aligned}$</td>
<td style="text-align: center;">Count</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">How many rivers flow through India ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">How many rivers and lakes does India have ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Logical operators</td>
<td style="text-align: center;">How many rivers flow through India and/or/but not China?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Min/Max</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">Which river flows through maximum number of countries ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">Which country has maximum number of rivers and lakes combined ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Atleast/Atmost</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">Which rivers flow through at least N countries ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">/Approx./ <br> Equal</td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">Which country has at least N rivers and lakes combined ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Count over <br> Atleast <br> /Atmost <br> Approx./Equal</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">How many rivers flow through at least N countries?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">How many countries have at least N rivers and lakes combined ?</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Compar- } \ &amp; \text { arative } \end{aligned}$</td>
<td style="text-align: center;">More/Less</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">Which countries have more number of rivers than India ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">Which countries have more rivers and lakes than India ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Count over <br> More/Less</td>
<td style="text-align: center;">Single entity type</td>
<td style="text-align: center;">How many countries have more number of rivers than India ?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mult. entity type</td>
<td style="text-align: center;">How many countries have more rivers and lakes than India ?</td>
</tr>
</tbody>
</table>
<p>Table 2: Types of questions in the dataset.</p>
<h2>Simple Questions</h2>
<p>For discovering simple question templates, we asked the annotators to come up with questions which can be answered from a single tuple in the knowledge graph. The annotators suggested that for a given tuple (say, rel: CEO, subj: Google, obj: Sundar Pichai) there are mainly 3 types of simple questions that can be generated:</p>
<ol>
<li>Object based questions: Here the question contains the relation and the subject from a tuple and the answer is the tuple's object. For example, "Q: Who is the CEO (relation) of Google (subject) ? A: Sundar Pichai (object)".</li>
<li>Subject based questions: Here the question contains the relation and the object from a tuple and the answer is the tuple's subject. For example, "Q: Which company is Sundar Pichai (object) the CEO of (relation) ? A: Google (subject)".</li>
<li>Relation based questions: Here the question contains the subject and the object from a tuple and the answer is the tuple's relation. For example, "Q: How is Sundar Pichai (object) related to Google (subject) ? A: CEO (relation)". During our discussions, we found that in many cases, relation based questions do not make a lot of sense. For example, it is unnatural for someone to ask the question "Q: How is Himalayas related to India? A: located in". Hence, in this work we focus only on object based and subject based questions.</li>
</ol>
<p>Note that in some cases the question could have multiple correct answers. In other words, there are multiple tuples related to this question. For example, "Q: Which rivers flow through India ? A: Ganga, Yamuna, Narmada, ...". Note that even though these questions can be answered from multiple tuples, they are still simple questions because they do not require any joint reasoning over multiple tuples.</p>
<p>Crowdsourced question generation: Based on this initial pilot with in-house annotators we then requested workers on AMT to create subject based and object based questions for each of the 330 relations in our KG. For creating subject based questions the annotators were shown (i) the object, (ii) the relation (iii) the type of the subject associated with that tuple and (iv) a few sample tuples. Note that the subject type is important as the annotator will need to look at the subject type (city) to form the question "Which city is the capital of India ?". This is important because some relations (for example, the relation tributary) can have multiple subject and object types as shown below:</p>
<ol>
<li>subj: Spring Creek (type: river), obj: Lake Ilsanjo (type: lake)</li>
<li>subj: Spring Creek (type: river), obj: Matanzas Creek (type: stream)
It should be obvious that even for the same relation different combinations of subjects and objects should result in different questions. For example, "Which lake is a tributary of Spring Creek?" vis "Which river is a tributary of Spring Creek?". Note that, on an average each relation in our KG was associated with 5 subject types and 6 object types. We first asked a set of workers to create one subject based and object based question for each relation. We then asked a separate set of annotators to create paraphrases of these questions. In all, we collected 1531 subject based and 1450 object based question templates (including paraphrases) through this process. Once we get a template we can instantiate it with different entity types and entities to create many questions. For example, given the template "Which $&lt;$ water course $&gt;$ is located in $&lt;$ country $&gt;$ ?" we can instantiate it by replacing water course by it's sub-types (river, lake, etc) and by replacing country by entities of that type (U.S., India, etc.). This gives us a
semi-automatic way of creating many questions from the collected templates. Note that the question templates also contain paraphrases, so we have different ways of asking the same question.</li>
</ol>
<h2>Complex Questions</h2>
<p>Next we wanted the annotators to help us identify types of questions which require logical, comparative and quantitative reasoning over a larger subgraph of the KG.</p>
<p>Logical Reasoning: These are questions which require some logical inferencing over multiple tuples in the KG. For example, consider the question "Which rivers flow through India and China ?" To answer this question we first need to create two sets (i) a set $A$ containing rivers appearing in tuples of the form (flows through, India, river) and (ii) a set $B$ containing rivers appearing in tuples of the form (flows through, China, river). The final answer to the question is then an intersection of these two sets. It should be obvious that answering such questions is more difficult then the Simple Questions studied in literature so far (and as described in the previous section).</p>
<p>The annotators came up with questions involving different logical operators such as AND, OR, NOT, etc (see Table ??). They also suggested some templates for creating such logical reasoning questions from the simple questions that we had already collected (as described in Section 3.1). For example, one such template was to take a simple object based question such as "Which rivers flow through India" and augment it with another subject such as "and China". Similar templates and paraphrases were suggested for other operators such as OR, NOT, etc. for both subject based and object based questions. This allowed us to semi-automatically create many questions requiring logical reasoning. This process is semiautomatic because once a template is created, we instantiate it for multiple tuples (as explained earlier) and then manually verify a subset of these questions to check whether they are syntactically and semantically correct.</p>
<p>Note that some of the logical reasoning questions suggested by the annotators contained multiple relations. For example, the question "Which river flows through India and has its source in Himalayas?" requires a logical operation over two relations, viz., flows through and source.</p>
<p>Quantitative Reasoning: These questions require some quantitative reasoning involving standard aggregation functions like max, min, count, atleast / atmost / approximately / equal to $N$, etc. We refer the reader to Table ?? to see examples of different types of quantitative questions. Once again, with the help of in-house annotators we identified several templates for modifying the simple questions that we had already collected and creating quantitative reasoning questions involving different aggregation operators. For example, one such template was to take the object based question "Which rivers flow through India" and replace "Which" by "How many". In fact, we found this particular template to be so convenient that for every relation, we asked the workers to give us at least one simple question which starts with "Which subject-type ... ". Some of these simple questions starting with "Which subject-type ... " look a bit unnatural but we</p>
<p>made a conscious choice to allow this so that it simplifies the process of creating complex questions. We also created questions which require quantitative reasoning on top of logical reasoning. For example, "How many rivers flow through India but not through China ?".</p>
<p>Comparative Reasoning: These are questions which require a comparison between entities based on certain relations (predicates). For example, consider the question "Which countries have more number of rivers than India ?". This requires inference over multiple tuples in the KG. The model here essentially needs to learn the count, sort and more/less operations. Such questions could also involve multiple entity types. For example the question "Which countries have more lakes and rivers than India ?" involves two entity types (lakes, rivers). Finally, we could have questions which require a counting type quantitative reasoning on top of comparative reasoning. For example, "How many countries have more rivers than India ?" requires counting after comparing. These questions were created by modifying the simple questions, using the rules of transformation given by our annotators.</p>
<p>Note that in all of the above cases, once the annotator suggests a modification, we can apply that modification and its paraphrases to multiple tuples to get many questions. Further, after instantiating we retain only those Qs which have less than 1000 answers.</p>
<h2>Linked Sequential QA</h2>
<p>So far we have described the process of collecting individual QA pairs containing various types of questions. We are now interested in creating coherent conversations involving such QA pairs. We can think of such a conversation as a walk over the Knowledge Graph using QA pairs such that subsequent questions refer to subjects, objects or relations which have appeared previously in the conversation. More specifically, such conversations should have the following properties (i) subsequent QA pairs should be linked and (ii) the conversation should contain typical elements of a dialog such as coreferences, ellipses, clarifications, confirmation, etc.</p>
<p>The process of connecting linked QA pairs in a coherent conversation can be thought of as performing a systematic walk over a Knowledge Graph. Simply stated, two questions can be placed next to each other in a conversation if they share a relation or an entity. However, bringing in factors such as ambiguity, underspecified or coreferenced questions into the conversation requires manual effort. For this, we again requested in-house annotators to create templates for converting simple or complex questions described above into conversational questions. For example, one such template was to take a simple question such as "Which rivers flow through India ?" and replace the subject by "that subject-type" or "that country" in this case. Multiple such templates were created and refined for different question types that we described in the earlier sections. This was a labor intensive tedious process requiring several iterations. Some templates were also collected using crowdsourcing on AMT. We refer to such questions as Indirect questions as opposed to Direct questions which are fully specified and do not indirectly refer to some entity or relation from the earlier conversation. The in-house
annotators also suggested some clarification templates which involved asking questions containing coreferences which could resolve to more than one of the previously mentioned entities. Turn 2 in Table ?? shows one such example. The information in this question is not enough to answer the question and hence the system needs to ask for a clarification. Note that, whenever we use linking we only link consecutive questions and not arbitrary questions in the sequence (i.e., the $i$-th question can be linked to the next pr previous question but not to arbitrary questions appearing before or after it.)</p>
<p>Through the above processing involving a mix of manual work (crowdsourced and inhouse) and semi-automatic instantiation, we created a dataset containing 200 K dialogs and a total of 1.6 M turns. Table ?? shows the number of templates for each question type and some sample types. Table ?? shows various statistics about the dataset including the Train, Validation and Test splits . Note that we constructed the train, valid and test splits in such a way that the dialogs in the validation and test set do not contain questions corresponding to tuples for which questions were seen at train time.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset Statistics</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Valid</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total No. of Dialogs(chat sessions)</td>
<td style="text-align: center;">152391</td>
<td style="text-align: center;">16413</td>
<td style="text-align: center;">27797</td>
</tr>
<tr>
<td style="text-align: left;">Avg. No. of Utterances per dialog</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">15.65</td>
<td style="text-align: center;">19.44</td>
</tr>
<tr>
<td style="text-align: left;">Total No. of Utterances having Ques- <br> tion/Answer</td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">.13 M</td>
<td style="text-align: center;">.27 M</td>
</tr>
<tr>
<td style="text-align: left;">Length of user's question (in words)</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">9.68</td>
<td style="text-align: center;">10.28</td>
</tr>
<tr>
<td style="text-align: left;">Length of system's response (in words)</td>
<td style="text-align: center;">4.74</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">4.37</td>
</tr>
<tr>
<td style="text-align: left;">Avg. No. of Dialog states per dialog</td>
<td style="text-align: center;">3.89</td>
<td style="text-align: center;">3.84</td>
<td style="text-align: center;">4.53</td>
</tr>
<tr>
<td style="text-align: left;">Vocab size (freq $&gt;=10$ )</td>
<td style="text-align: center;">0.1 M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Overall Dataset Statistics</p>
<h2>Some peculiar characteristics of Wikidata</h2>
<p>We found that Wikidata has some typical characteristics and predicates, subject types and object types which often leads to very unnatural questions. We list down some of these issues below:</p>
<ul>
<li>Very generic predicates: Consider the relation lake_outflow for which the annotators suggested the question "Which object_type outflows from the lake YYY ?". This seems like a valid template but turns out that Wikidata also contains predicates of the form lake_outflow(DalLake, evaporation) where evaporation is an outflow from the lake. Similarly, the relation fabrication_method allows for methods used to grow, cook, weave, build, assemble, manufacture an item. Due to the presence of such very generic relations (which allow a wide range of object types) sometime the questions instantiated from these templates may look very unnatural. In many cases, we manually tried to filter out such questions but given the scale of the KB it was not always possible to do this. We expect some such noisy questions to be a part of the final dataset.</li>
<li>Overlapping predicate and subject types: The word religion is both a predicate and a subject type in Wikidata. Similarly, sport is both a predicate and a subject type in</li>
</ul>
<p>Wikidata. This often leads to some questions containing repitions (for example, "Which religion (subject type) is the religion (predicate) practised by YYY ?". Again, we filtered out many such cases by applying some rule based post-processing after instantiating the templates but we still expect a few of these to be present in the dataset.</p>
<ul>
<li>Long tail of subject types and relations: There are a few subject types in Wikidata which are very dominant. For example, a large number of entities in Wikidata belong to the sub-class person and location. These subject types in turn are associated with a few dominant relations. For example, part-of is the predominant relations associated with almost entities of type location. Similarly, citizenof, birthdate, birthplace are common relations associated with almost all entities of type person. Other relations such as named-after are a bit rare. Hence, when creating complex or linked questions connecting multiple entities and relations some of the rarer relations do not show up frequently. Such long tail behavior wherein some relations and predicates dominate will be observed in any KB of a reasonable size and can't really be avoided.</li>
<li>Unnatural Peer Subject types: As per Wikidata, the subject types religion and social group are peers as they are both sub-classes of belief system. As a consequence of this we have logical questions of the form "Which religions and social groups does YYY belong to?". We found this is a bit odd and we are not sure if an average user would consider these to be peers. These are special cases and are expected to any such large scale KB.</li>
</ul>
<h2>Proposed Model</h2>
<p>Since CSQA involves a combination of dialog and QA, we propose a model which is a cross between (i) the HRED model (?) which is one of the state of the art models for dialog systems and (ii) the key value memory network model (?) which is a state of the art QA system. Our model has the following components:</p>
<ol>
<li>Hierarchical Encoder: The model contains a lower level RNN encoder which goes over the words in an utterance and computes a representation for each utterance. This is followed by a higher level encoder which goes over these utterance representations and computes a representation $q_{1}$ for the context (current state of the dialog).</li>
<li>Handling Large Vocabulary: As input to the above encoder, we provide pre-trained Glove embeddings (?) of the words in the question. However, our questions contain many entities (names, locations, etc.) for which pre-trained word embeddings are not available. Since these entities are crucial for answering the questions we cannot treat them as unknown words. One option is to randomly initialize the embeddings of these entity words and then train them along with other parameters of the model. This would effectively lead to a very large vocabulary and blow up the number of parameters. To avoid this, we use a state of the art TransE method (?) for learning embeddings of KG entities offline. More specifically, for entities such as India, China, Ganga, Himalayas, etc. which are present in the KG we learn embeddings us-
ing the TransE model. We refer to these embeddings as KG embeddings. The final embedding of every question word is then a concatenation of the Glove embedding (if available, 0 s otherwise) and the KG embedding (if available, 0 s otherwise).</li>
<li>Candidate generation: State of the art memory network based methods (?) learn to compute an attention function over the tuples in the KG based on the given question (or dialog context in our case). For large sized KGs, it is infeasible to compute the attention over the entire KG. Instead, following (?) we filter out tuples from the KG using the longest possible n-gram matching. We essentially consider only the longest n-gram which corresponds to the name of a KGentity and retain only those tuples where the entity appears as subject/object. We observed that even with this filtering, the average number of candidate tuples for a given question in our dataset can sometimes be very large. We return to this issue in the Discussions section.</li>
<li>Key Value Memory Network: A key value memory network stores each of the $N$ candidate tuples (as selected above) as a key-value pair where the key contains the concatenated embedding of the relation and the subject (denoted by $\phi_{K}\left(k_{h_{i}}\right) \in R^{D}$ for the $i^{\text {th }}$ memory entry) whereas the value contains the embedding of the object (denoted by $\phi_{V}\left(v_{h_{i}}\right) \in R^{D}$ for the $i^{\text {th }}$ memory entry). Here, the subject, object and relation embeddings are the TransE KG embeddings, as described above. The model makes multiple passes over the memory computing new attention weights over the keys of the memory at each pass and updating the contextual question representation $(q)$ whose initial representation $q_{1} \in R^{d}$ is computed by the hierarchical encoder. The rationale behind making multiple passes over the question is that the model may learn to focus on different aspects of the question in each pass. This is especially important in the case of complex questions. The following equation shows how the query representation gets updated in the $j^{\text {th }}$ pass.</li>
</ol>
<p>$$
q_{j+1}=R_{j}\left(q+\sum_{i}^{N} \operatorname{Softmax}\left(q_{j} A \phi_{K}\left(k_{h_{i}}\right)\right) A \phi_{V}\left(v_{h_{i}}\right)\right)
$$</p>
<p>$A \in R^{d \times D}$ and $R_{1 \ldots H} \in R^{d \times d}$ are the parameters of the key-value memory network and $N$ is the number of candidate tuples.
5. Decoder: For a truly end-to-end solution, the decoder should be generic enough to produce multiple types of answers. For example, here are some of the answer sequences that the decoder is expected to generate: (i) 5 rivers and 4 lakes (for count questions) (ii) Yes/No/Yes and No respectively etc. (for verification questions) (iii) Did you mean ... (for clarification questions) (iv) Ganga, Narmada, Yamuna, ... (list of KG entities satisfying the query) and so on. At a high level, we can say that the model always produces sequences and in most cases the sequences will contain KG entities whereas in some cases the sequences may contain counts, entity types (rivers, lakes, etc) and non-KG words. We thus model the decoder as an RNN based sequence generator which takes as input the modified query representation. At</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Proposed Model consisting of a (i) Hierarchical Encoder (ii) Key-Value Memory Network and (iii) Decoder
each time step it gives a softmax over a shortlisted vocabulary containing counts, yes/no and KG entity types amounting to 1500 words approximately. Note that even though the model has to produce KG entities, we cannot include all KG entities in this vocabulary (as it will blow up the number of parameters). Instead, we train the decoder to produce the token $K G _W O R D$ whenever a KG entity needs to be produced in the output. We then use a copy mechanism to replace the $K G _W O R D$ with relevant entities. For example, if the decoder produces $n K G _W O R D$ tokens then we use $q_{H+1}$ to give a distribution over the entities in the candidate tuples and then replace each $K G _W O R D$ token in the output by these top $n$ entities having the highest probability. The distribution over the candidate entities is computed as $\operatorname{Softmax}\left(q_{H+1} B \phi_{V}\left(v_{h_{i}}\right)\right)$ where $B \in R^{d \times D}$ is a parameter. The training loss is a sum of the cross-entropy loss over the tokens and the KG entities.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question Type</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">$15.83 \%$</td>
<td style="text-align: center;">$6.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Simple Question (Direct)</td>
<td style="text-align: center;">$30.64 \%$</td>
<td style="text-align: center;">$7.17 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Simple Question (Coreferenced)</td>
<td style="text-align: center;">$25.85 \%$</td>
<td style="text-align: center;">$8.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Simple Question (Ellipsis)</td>
<td style="text-align: center;">$24.5 \%$</td>
<td style="text-align: center;">$4.26 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Logical Reasoning (All)</td>
<td style="text-align: center;">$4.85 \%$</td>
<td style="text-align: center;">$19.70 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Quantitive Reasoning (All)</td>
<td style="text-align: center;">$0.24 \%$</td>
<td style="text-align: center;">$0.11 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Comparative Reasoning (All)</td>
<td style="text-align: center;">$1.05 \%$</td>
<td style="text-align: center;">$0.30 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Clarification</td>
<td style="text-align: center;">$30.64 \%$</td>
<td style="text-align: center;">$10.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Question Type</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Verification (Boolean) (All)</td>
<td style="text-align: center;">$8.34 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Quantitative Reasoning (Count) (All)</td>
<td style="text-align: center;">$10.66 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Comparing Reasoning (Count) (All)</td>
<td style="text-align: center;">$1.3 \%$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Question Type</td>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Clarification (Natural Language Generation)</td>
<td style="text-align: center;">15.58</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of the proposed model on different types of questions in the dialog</p>
<h2>Results</h2>
<p>We used Adam as the optimization algorithm and tuned the following hyperparameters using the validation set; learning rate $\in{1 \mathrm{e}-3,4 \mathrm{e}-4}$, RNN hidden unit size, word embedding size, KG embedding size $\in{256,512}$, batch size $\in{32,64}$ and dialog context size as 2 . The bracketed numbers indicate the values of each hyperparameter considered. On average, we found that the candidate generation step produces 10 K candidate tuples, hence we kept upto 10 K key value pairs in the memory network. Following (?), we set $H=2$. We used Precision and Recall as the evaluation metrics which capture the percentage of entities in the final decoder output that were correct and the percentage of actual entities that were retrieved by the system respectively. For verification and count based questions which produce a sequence of Yes and/or No or counts we use accuracy as the evaluation metric (i.e., whether the count or boolean answer was exact or not). Finally for questions which need clarification, the system has to generate a natural language response which is usually a sequence of KG-entities and non-KG words, hence for that we separately report both Precision/Recall over the predicted KG-entities and BLEU for the overall utterance similarity. The results of our experiments are summarized in Table 77.</p>
<h2>Discussions</h2>
<p>Based on the results in Table ??, we discuss some shortcomings of existing methods and suggest areas for future research.</p>
<ol>
<li>Simple v/s Complex Questions: It is obvious that the model performs very poorly on complex questions as compared to simple questions. There are multiple reasons for this. First, existing models do not really model an aggregate or logical function for handling quantitative, comparative and logical reasoning. Designing such aggregation functions for an end-to-end solution is non-trivial and needs further</li>
</ol>
<p>exploration. This dataset should provide a good benchmark for exploring such solutions for complex QA. Second, it is not clear if the existing encoders (HRED + KVmem, in this case) are capable of effectively parsing complex questions and feeding a good represetation to the decoder. For example, the encoder ideally needs to learn to break down the question "Which rivers flow through India and China?" into two parts (i) "Which rivers flow through India?" (ii) "Which rivers flow through China?" and then compute an attention over relevant tuples in the memory. Such kind of parsing is not explicitly modeled by existing encoders. There is clearly a need for revisiting some of the traditional parsing based methods for QA in the light of this dataset.
2. Direct v/s Indirect Questions: Comparing the third and fourth rows of Table ?? with the second row, we see that the performance of the model drops when dealing with indirect or incomplete questions which rely on the context for resolving ellipsis, coreferences, etc. Even though current dialog systems (HRED, in this case) do learn to capture the context, one key challenge w.r.t our dataset is that, here named entities and relations matter more than other words in the context. We need better models which can explicitly learn to give importance to relations and entities (for example, using an explicit supervised attention mechanism).
3. Candidate Generation: This step is required to prune the size of the KG and store only relevant steps in the memory. This step is a bit adhoc as it relies on n-gram matching and we saw specific issues while using this on our dataset. We had explicitly asked the annotators to create paraphrases of the same question. As a result simple n-gram matching does not work well resulting in low recall of the actual answer entity in the filtered candidate tuples. A better candidate matching algorithm which takes care of entity paraphrases (Leo, Leonardo, etc.) and relation paraphrases (director, directed by, direct, etc.) are needed. In some cases, we also have the reverse problem. For example, if the entity being referred to in the question is extremely popular then it will be involved in over 100 K tuples in the KB (for example, an entity like U.S.A.). This causes the KV memory to blow up leading to poor and inefficient training and inference.
4. Better organization of the memory: It is inevitable that for some questions, especially complex questions involving logical operators over multiple entities and relations, the number of tuples required to be stored in the memory would be large. For example, around $15 \%$ of the questions in our data require more than 100 K candidate tuples. Current Key Value Memory Networks which are flat in their organization are not suitable for this for two reasons. First, the amount of memory required by the model increases and can go beyond the capacity of existing GPUs. Second the attention weights computed using equation ?? need a prohibitively expensive softmax computation which increases both training and test time. Better ways of organizing the memory along with approximate methods for computing the softmax function are needed to handle such complex questions.</p>
<p>We hope that the dataset, results and discussions on the resources presented in this paper will convince the reader that CSQA has several challenges which are not encountered in previous datasets for dialog and QA. Some of them are listed above and there are a few more which we do not list due to space constraints. Addressing/solving all of these challenges is clearly beyond the scope of a single paper. The purpose of this paper was to introduce the task and propose a model based on existing state of the art models and thereby highlight the need for further research to address the inadequacies of these models. To facilitate research, this dataset will be made available at https://github.com/iitm-nlp-miteshk/AmritaSaha/ tree/master/CSQA (please copy paste the URL in a browser instead of clicking on it). This URL will contain the following resources:</p>
<ul>
<li>the train/valid/test splits used in our experiments</li>
<li>the processed version of the WikiData dump of 14-Nov2016 that was used to construct the dataset</li>
<li>scripts to extract the train/valid/test set for each of the different question types listed in Table ??</li>
<li>scripts to evaluate the performance of the model</li>
</ul>
<h2>Conclusion</h2>
<p>In this paper, we introduced the task of Complex Sequential Question Answering (CSQA) with a large scale dataset consisting of conversations over linked QA pairs. The dataset contains 200K dialogs with 1.6 M turns and was collected through a manually intensive semi-automated process. To the best of our knowledge, this is the first dataset of its kind which contains complex questions which require logical, quantitative and/or comparative reasoning over a large Knowledge Graph containing millions of tuples. We propose a model for CSQA which is a cross between state of the art models for dialog and QA and highlight the inadequacies of this model in dealing with the task of CSQA. It should be obvious that CSQA has several challenges and addressing/solving all of them is beyond the scope of a single paper. We hope that the introduction of this task and dataset should excite the research community to develop models for Complex Sequential Question Answering.</p>
<h2>References</h2>
<p>Banchs, R. E. 2012. Movie-dic: a movie dialogue corpus for research and development. In ACL, 2012, 203-207.
Berant, J., and Liang, P. 2014. Semantic parsing via paraphrasing. In $A C L$ (1), 1415-1425.
Berant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP, volume 2, 6.
Berant, J.; Srikumar, V.; Chen, P.; Linden, A. V.; Harding, B.; Huang, B.; Clark, P.; and Manning, C. D. 2014. Modeling biological processes for reading comprehension. In EMNLP 2014,.</p>
<p>Bordes, A., and Weston, J. 2016. Learning end-to-end goaloriented dialog. CoRR abs/1605.07683.</p>
<p>Bordes, A.; Usunier, N.; García-Durán, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Neural Information Processing Systems 2013, 2787-2795.
Bordes, A.; Usunier, N.; Chopra, S.; and Weston, J. 2015. Large-scale simple question answering with memory networks. CoRR abs/1506.02075.
Bordes, A.; Chopra, S.; and Weston, J. 2014. Question answering with subgraph embeddings. arXiv preprint arXiv:1406.3676.
Bordes, A.; Weston, J.; and Usunier, N. 2014. Open question answering with weakly supervised embedding models. In ECML PKDD 2014. Proceedings, Part I, 165-180.
Dodge, J.; Gane, A.; Zhang, X.; Bordes, A.; Chopra, S.; Miller, A. H.; Szlam, A.; and Weston, J. 2015. Evaluating prerequisite qualities for learning end-to-end dialog systems. CoRR abs/1511.06931.
Fader, A.; Soderland, S.; and Etzioni, O. 2011. Identifying relations for open information extraction. In EMNLP 2011, $1535-1545$.
Fader, A.; Zettlemoyer, L.; and Etzioni, O. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, $1156-1165$. ACM.
Kumar, A.; Irsoy, O.; Ondruska, P.; Iyyer, M.; Bradbury, J.; Gulrajani, I.; Zhong, V.; Paulus, R.; and Socher, R. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML 2016, 1378-1387.
Lowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL 2015,, 285-294.
Lowe, R. T.; Pow, N.; Serban, I. V.; Charlin, L.; Liu, C.; and Pineau, J. 2017. Training end-to-end dialogue systems with the ubuntu dialogue corpus. $D \&amp; D 8(1): 31-65$.
Luong, M.-T.; Le, Q. V.; Sutskever, I.; Vinyals, O.; and Kaiser, L. 2015. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114.
Miller, A. H.; Fisch, A.; Dodge, J.; Karimi, A.; Bordes, A.; and Weston, J. 2016. Key-value memory networks for directly reading documents. CoRR abs/1606.03126.
Mostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra, D.; Vanderwende, L.; Kohli, P.; and Allen, J. F. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. CoRR abs/1604.01696.
Neelakantan, A.; Le, Q. V.; Abadi, M.; McCallum, A.; and Amodei, D. 2016. Learning a natural language interface with neural programmer. CoRR abs/1611.08945.
Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.; Majumder, R.; and Deng, L. 2016. MS MARCO: A human generated machine reading comprehension dataset. CoRR abs/1611.09268.
Onishi, T.; Wang, H.; Bansal, M.; Gimpel, K.; and McAllester, D. 2016. Who did what: A large-scale personcentered cloze dataset. arXiv preprint arXiv:1608.05457.</p>
<p>Pennington, J.; Socher, R.; and Manning, C. D. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), 1532-1543.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.
Richardson, M.; Burges, C. J. C.; and Renshaw, E. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP 2013, 193-203.
Ritter, A.; Cherry, C.; and Dolan, B. 2010. Unsupervised modeling of twitter conversations. In NAACL 2010, 172-180.
Serban, I. V.; Sordoni, A.; Bengio, Y.; Courville, A.; and Pineau, J. 2016a. Building end-to-end dialogue systems using generative hierarchical neural network models. AAAI'16, 3776-3783. AAAI Press.
Serban, I. V.; García-Durán, A.; Gülçehre, Ç.; Ahn, S.; Chandar, S.; Courville, A. C.; and Bengio, Y. 2016b. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.
Serban, I. V.; Sordoni, A.; Lowe, R.; Charlin, L.; Pineau, J.; Courville, A. C.; and Bengio, Y. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues. In AAAI, 3295-3301.
Voorhees, E. M., and Tice, D. M. 2000. Building a question answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, 200-207. ACM.
Wang, M.; Smith, N. A.; and Mitamura, T. 2007. What is the jeopardy model? a quasi-synchronous grammar for qa. In EMNLP-CoNLL, volume 7, 22-32.
Yang, M.-C.; Duan, N.; Zhou, M.; and Rim, H.-C. 2014. Joint relational embeddings for knowledge-based question answering. In EMNLP, volume 14, 645-650.
Yang, Y.; Yih, W.; and Meek, C. 2015. Wikiqa: A challenge dataset for open-domain question answering. In EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, 2013-2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal Contribution
${ }^{\dagger}$ Work done while at IBM Research
Copyright (C) 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
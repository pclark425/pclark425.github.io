<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4458 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4458</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4458</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-277313806</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19309v1.pdf" target="_blank">Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees</a></p>
                <p><strong>Paper Abstract:</strong> Scientific hypothesis generation is a fundamentally challenging task in research, requiring the synthesis of novel and empirically grounded insights. Traditional approaches rely on human intuition and domain expertise, while purely large language model (LLM) based methods often struggle to produce hypotheses that are both innovative and reliable. To address these limitations, we propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel framework that integrates Monte Carlo Tree Search with Nash Equilibrium strategies to iteratively refine and validate hypotheses. MC-NEST dynamically balances exploration and exploitation through adaptive sampling strategies, which prioritize high-potential hypotheses while maintaining diversity in the search space. We demonstrate the effectiveness of MC-NEST through comprehensive experiments across multiple domains, including biomedicine, social science, and computer science. MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a 1-3 scale) for novelty, clarity, significance, and verifiability metrics on the social science, computer science, and biomedicine datasets, respectively, outperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51, and 2.52 on the same datasets. These results underscore MC-NEST's ability to generate high-quality, empirically grounded hypotheses across diverse domains. Furthermore, MC-NEST facilitates structured human-AI collaboration, ensuring that LLMs augment human creativity rather than replace it. By addressing key challenges such as iterative refinement and the exploration-exploitation balance, MC-NEST sets a new benchmark in automated hypothesis generation. Additionally, MC-NEST's ethical design enables responsible AI use, emphasizing transparency and human supervision in hypothesis generation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4458.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4458.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-NEST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Nash Equilibrium Self-Refine Tree</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that integrates Monte Carlo Tree Search (MCTS) with Nash-equilibrium-based selection and iterative LLM self-refinement to generate and evaluate scientific hypotheses by optimizing a quality function Q(h).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>MC-NEST Q(h) optimization with multi-source evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>MC-NEST frames hypothesis generation as a search-and-refinement process over a tree: nodes are hypotheses, edges are refinements. Each hypothesis h is assigned a quality score Q(h) (captures validity, novelty, coherence). Node selection uses a modified UCT that incorporates Nash-equilibrium-derived action probabilities; expansion uses LLM self-refinement and self-evaluation to create child hypotheses; backpropagation updates Q and visit counts. Final hypothesis quality is assessed by automatic LLM scoring (GPT-3.5) on novelty, relevance, significance, verifiability and by blind human expert ratings. Comparative experiments vary rollout length and sampling policy (Greedy, Importance Sampling, Pairwise Importance Sampling) to measure effects on Q and downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validity (empirical alignment), novelty, logical coherence/clarity, relevance/significance (impact), verifiability/testability; also textual similarity (BERTScore) reported as auxiliary metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; DeepSeek-R1-Distill-Qwen-32B; DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o (not specified); DeepSeek 32B; DeepSeek 7B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine, Social Science, Computer Science (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Testable research hypotheses (mechanistic/correlative hypotheses and design modifications, e.g., protein sequence substitution hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>MC-NEST produced higher average multi-criteria ratings than state-of-the-art prompt-based baselines: reported average qualitative scores (novelty, clarity, significance, verifiability combined) of 2.65 (Social Science), 2.74 (Computer Science), 2.80 (Biomedicine) vs prompt-based baselines 2.36, 2.51, 2.52 respectively. Sampling/policy effects: Greedy sampling often yielded highest overall averages (e.g., GPT-4o Greedy 8-step rollout: 2.81 on Social Science), Pairwise Importance Sampling excelled in novelty (e.g., novelty 2.74 in Social Science), and longer rollouts (4->8 steps) consistently improved BERTScore and qualitative metrics across datasets. Detailed per-model and per-policy numeric tables are provided in the paper (see Tables 2–7).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid: internal search uses LLM-based reward/Q; external evaluation uses automated LLM scoring (GPT-3.5) and blind human expert ratings (3 experts) on a 3-point scale for novelty, clarity, significance, verifiability; BERTScore reported as auxiliary automated metric.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Correlation analysis between GPT-3.5 automated scores and human expert evaluations was performed to validate automated scoring as a proxy for human judgment; convergence behavior of MC-NEST shown empirically across rollouts and sampling policies; cross-dataset evaluation on curated datasets (Social Science MOOSE, LLM4BioHypoGen, LLM4CSHypoGen) to test generality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain coverage skew (datasets concentrated on computer science papers though multiple domains tested), no direct human-generated-theory baseline comparison (human experts only rate machine outputs), potential LLM evaluator bias and training-data overlap, small number of human raters (three experts, 100 hypotheses sampled), reliance on LLM-based internal rewards which can propagate model biases; Nash uniform distribution choice may limit prioritization in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated on three curated datasets: MOOSE (Social Science), LLM4BioHypoGen (Biomedicine, 200 pairs), and LLM4CSHypoGen (Computer Science, 150 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4458.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Scorer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 automated evaluator (scoring novelty, relevance, significance, verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation pipeline that uses GPT-3.5 to score generated hypotheses on four targeted dimensions (novelty, relevance, significance, verifiability) to produce scalar judgments used in MC-NEST evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>GPT-3.5 four-dimension scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>GPT-3.5 is prompted to assign scores to hypotheses on novelty, relevance, significance and verifiability. Scores are aggregated to produce the automated reward R_n and to compute node Q values during MC-NEST search as well as to provide an automatic evaluation baseline across methods and prompting styles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty, relevance, significance (impact/importance), verifiability/testability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across Social Science, Biomedicine, Computer Science datasets</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Testable hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the primary automated rater in experiments; aggregated GPT-3.5 scores reported alongside human ratings and used to compute Q and reward signals. The paper reports good correlation between GPT-3.5 and human expert evaluations (exact correlation coefficients are described qualitatively; correlation used as evidence that GPT-3.5 can be a reliable automated evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by correlation analysis with blind human expert ratings on a sample of 100 hypotheses; paper claims GPT-3.5's potential as a reliable evaluator based on this correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automated LLM evaluators can reflect training-set biases and may prefer surface-level features; the paper does not report detailed statistics (e.g., Pearson/Spearman coefficients) in main text for the correlation, and automated scores may over/under-estimate certain qualitative properties.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same three curated datasets used in MC-NEST experiments (MOOSE, LLM4BioHypoGen, LLM4CSHypoGen).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4458.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human 3-point Expert Rating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Blind human expert 3-point scale evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Blind evaluation by three domain experts (professors, postdocs, PhD students) using a standardized 0–3 scale across novelty, clarity, significance, and verifiability for sampled hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Blind expert 3-point rubric (novelty, clarity, significance, verifiability)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human experts assess 100 randomly selected hypotheses (from baselines and MC-NEST) without knowing method provenance. Each hypothesis is rated on a 0–3 scale per dimension with prespecified anchors (0 = poor/unverifiable/trivial, 3 = exceptional/highly verifiable/high novelty). Scores are averaged to compare methods and to validate automated scorers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Novelty (priority), clarity/conciseness, significance (impact), verifiability (testability).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social Science, Biomedicine, Computer Science (as in datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Testable research hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human evaluation shows MC-NEST variants (with ZSCoT prompting and specific sampling policies) outperform baselines on average: e.g., MC-NEST with ZSCoT achieved average scores such as 2.62 (Social Science) and 2.37 (Computer Science) in human ratings; Importance Sampling performed best in Biomedicine in human study with 2.37. Exact per-condition numbers are reported in Table 8.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based; used in conjunction with automated GPT-3.5 scoring to form a hybrid evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Blind rating design; validation performed indirectly by correlating aggregate human ratings with GPT-3.5 automated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small rater pool (3 experts), limited sample size (100 hypotheses blind-evaluated), possible inter-rater variability (no detailed inter-rater reliability statistics reported), evaluations focus on machine-generated outputs without a human-generated hypothesis baseline for direct output comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4458.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore: Evaluating text generation with BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contextual embedding–based automatic metric that measures similarity between generated text and reference texts using pre-trained BERT representations; reported here as an auxiliary textual-quality metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bertscore: Evaluating text generation with bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute precision/recall/F1 based on contextual embedding matching between generated hypotheses and reference texts (when available). Used as an auxiliary automated metric in tables to report textual alignment and fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Embedding-based textual similarity (precision, recall, F1); used as proxy for surface-level quality/faithfulness but not for novelty/testability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across the three datasets as a generic text-quality metric</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Textual hypotheses/statements</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>BERTScore values are reported in Tables 2–7; increases in rollout length and certain sampling policies (e.g., Greedy with longer rollouts) are associated with modest BERTScore improvements, consistent with qualitative metric gains.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric (embedding-based).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Established external metric (paper cites original BERTScore work); no additional internal validation reported beyond reporting alongside other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>BERTScore measures surface/semantic similarity to references and does not capture novelty, empirical grounding, or verifiability; authors note focus on task-specific goals and sometimes exclude conventional metrics, but BERTScore is still reported as auxiliary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4458.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modified UCT + Nash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Bound for Trees (UCT) with Nash-equilibrium probability adjustment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A node-selection scoring rule that augments the classical UCT formula with a uniform Nash-equilibrium-derived probability term to encourage fair exploration across candidate refinement actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Modified UCT (UCT + π_Nash)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The selection score for node i is computed as UCT(i) = Q(i) + C * ln(N_parent)/N(i) + ε, then adjusted by adding 1/n (uniform π(h_i) from Nash strategy). The node with highest combined score is chosen for expansion/refinement. UCT balances exploration and exploitation via visit counts and Q rewards; Nash term is added to prevent premature collapse to few actions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Optimization of Q (quality) while balancing exploration (via visit-count/ln term) and fairness/diversity (via Nash uniform probability).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Method-level mechanism, applied within MC-NEST across domains</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Selection/optimization mechanism for hypothesis search</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used during MC-NEST search; contributes to experimentally observed improvements when combined with different sampling policies and rollout lengths. The paper reports that inclusion of Nash uniform probability helps maintain diversity and avoids premature convergence to suboptimal hypotheses, contributing to better novelty/quality trade-offs in results (detailed per-policy tables).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Internal algorithmic selection metric (automated).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical ablation via sampling-policy experiments and rollout-length comparisons across datasets; improvement inferred from downstream automated and human-rated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Uniform Nash distribution (π=1/n) may be suboptimal if candidate qualities vary widely; theoretical guarantees not provided; hyperparameter C and rollout length materially affect behavior and must be tuned per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4458.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling Policies (Greedy / IS / PIS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy, Importance Sampling (IS), Pairwise Importance Sampling (PIS) selection policies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three selection/sampling policies implemented in MC-NEST that combine UCT scores and Nash probabilities to choose nodes: Greedy selects argmax of UCT+π; IS weights choices by UCT×π; PIS compares pairs to pick between two candidates aiming to boost novelty or balance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Greedy / Importance Sampling / Pairwise Importance Sampling policies</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Greedy: selects node with maximal UCT(i)+π(h_i). Importance Sampling: assigns weight = UCT(i) × π(h_i) and samples randomly according to weights. Pairwise Importance Sampling: randomly or systematically evaluates pairs (i,j) using combined UCT+π scores and selects the better of the pair, intended to promote exploration of promising but under-sampled nodes while maintaining diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Effect on final hypothesis Q and downstream metrics (novelty, clarity, significance, verifiability, BERTScore); measured by comparing aggregated scores across policies and rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; DeepSeek-R1-Distill-Qwen-32B; DeepSeek-R1-Distill-Qwen-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See MC-NEST entry</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied across Social Science, Biomedicine, Computer Science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Search/sampling strategies for hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Policy-dependent effects observed: Greedy often achieved best overall averages (e.g., GPT-4o Greedy 8-step: 2.81 Social Science), Importance Sampling gave best Biomedicine performance for some models (e.g., DeepSeek-32B Importance Sampling 4-step: 2.87), Pairwise IS excelled in novelty (e.g., highest novelty reported in several tables). Longer rollouts (4->8 steps) improved performance for all policies in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated internal sampling strategy; impact assessed by downstream automated (GPT-3.5, BERTScore) and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical cross-policy comparisons across datasets and models; performance reported in Tables 3, 5, 7.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Policy performance depends on model capacity and rollout length; some policies favor novelty at the possible cost of verifiability; hyperparameters and randomness can change relative ordering, requiring extensive tuning per domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4458.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4458.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Self-Refinement Evaluation Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based self-evaluation and iterative refinement (self-critique → refine → re-evaluate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An internal evaluation-refinement loop where the LLM generates critiques of a candidate hypothesis and uses those critiques to produce refined hypotheses, with refinements scored by the LLM to produce rewards used by MC-NEST.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM self-evaluation / SelfRefine loop</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a candidate answer A_n, the system prompts the LLM to produce a critique C_n = LLM(P + A_n) and then asks the LLM to produce a refined answer A_{n+1} = LLM(P + A_n + C_n). The refined answer is stored as a child node. Rewards R_n for answers are computed via an EvaluatePrompt call (R_n = LLM(EvaluatePrompt(P,A_n))). If R_n exceeds a preset limit, a penalty is applied to curb runaway scores. These LLM-derived rewards feed into Q and are backpropagated in the MC-NEST tree.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Internal LLM-derived quality (R_n / Q values) reflecting novelty, clarity, empirical alignment as encoded in evaluation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Same LLMs used for generation/evaluation (e.g., GPT-4o, DeepSeek variants); evaluators sometimes use GPT-3.5 as separate scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Used across evaluated domains within MC-NEST</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Mechanism for iterative hypothesis refinement and internal scoring</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Self-refinement produces improved child hypotheses during tree expansion; the approach contributes to observed gains when combined with UCT/Nash selection and appropriate sampling policies. Paper reports empirical improvements in downstream metrics when iterative self-refinement is used versus single-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated internal mechanism (LLM-on-LLM evaluation) but outputs are later validated by automated scorers and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical ablations comparing number of refinement steps and rollout lengths; validation also indirect via external automated and human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Risk of LLM overfitting to its own artifacts (self-confirmation bias), reliance on quality of evaluation prompts, potential instability of reward scaling (necessitating penalty thresholds), and propagation of model biases into Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bertscore: Evaluating text generation with bert. <em>(Rating: 2)</em></li>
                <li>Large language models as biomedical hypothesis generators: a comprehensive evaluation. <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery. <em>(Rating: 2)</em></li>
                <li>Hypothesis generation with large language models. <em>(Rating: 2)</em></li>
                <li>Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4458",
    "paper_id": "paper-277313806",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "MC-NEST",
            "name_full": "Monte Carlo Nash Equilibrium Self-Refine Tree",
            "brief_description": "A framework that integrates Monte Carlo Tree Search (MCTS) with Nash-equilibrium-based selection and iterative LLM self-refinement to generate and evaluate scientific hypotheses by optimizing a quality function Q(h).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "MC-NEST Q(h) optimization with multi-source evaluation",
            "evaluation_method_description": "MC-NEST frames hypothesis generation as a search-and-refinement process over a tree: nodes are hypotheses, edges are refinements. Each hypothesis h is assigned a quality score Q(h) (captures validity, novelty, coherence). Node selection uses a modified UCT that incorporates Nash-equilibrium-derived action probabilities; expansion uses LLM self-refinement and self-evaluation to create child hypotheses; backpropagation updates Q and visit counts. Final hypothesis quality is assessed by automatic LLM scoring (GPT-3.5) on novelty, relevance, significance, verifiability and by blind human expert ratings. Comparative experiments vary rollout length and sampling policy (Greedy, Importance Sampling, Pairwise Importance Sampling) to measure effects on Q and downstream metrics.",
            "evaluation_criteria": "Validity (empirical alignment), novelty, logical coherence/clarity, relevance/significance (impact), verifiability/testability; also textual similarity (BERTScore) reported as auxiliary metric.",
            "model_name": "GPT-4o; DeepSeek-R1-Distill-Qwen-32B; DeepSeek-R1-Distill-Qwen-7B",
            "model_size": "GPT-4o (not specified); DeepSeek 32B; DeepSeek 7B",
            "scientific_domain": "Biomedicine, Social Science, Computer Science (multi-domain)",
            "theory_type": "Testable research hypotheses (mechanistic/correlative hypotheses and design modifications, e.g., protein sequence substitution hypotheses)",
            "human_comparison": false,
            "evaluation_results": "MC-NEST produced higher average multi-criteria ratings than state-of-the-art prompt-based baselines: reported average qualitative scores (novelty, clarity, significance, verifiability combined) of 2.65 (Social Science), 2.74 (Computer Science), 2.80 (Biomedicine) vs prompt-based baselines 2.36, 2.51, 2.52 respectively. Sampling/policy effects: Greedy sampling often yielded highest overall averages (e.g., GPT-4o Greedy 8-step rollout: 2.81 on Social Science), Pairwise Importance Sampling excelled in novelty (e.g., novelty 2.74 in Social Science), and longer rollouts (4-&gt;8 steps) consistently improved BERTScore and qualitative metrics across datasets. Detailed per-model and per-policy numeric tables are provided in the paper (see Tables 2–7).",
            "automated_vs_human_evaluation": "Hybrid: internal search uses LLM-based reward/Q; external evaluation uses automated LLM scoring (GPT-3.5) and blind human expert ratings (3 experts) on a 3-point scale for novelty, clarity, significance, verifiability; BERTScore reported as auxiliary automated metric.",
            "validation_method": "Correlation analysis between GPT-3.5 automated scores and human expert evaluations was performed to validate automated scoring as a proxy for human judgment; convergence behavior of MC-NEST shown empirically across rollouts and sampling policies; cross-dataset evaluation on curated datasets (Social Science MOOSE, LLM4BioHypoGen, LLM4CSHypoGen) to test generality.",
            "limitations_challenges": "Domain coverage skew (datasets concentrated on computer science papers though multiple domains tested), no direct human-generated-theory baseline comparison (human experts only rate machine outputs), potential LLM evaluator bias and training-data overlap, small number of human raters (three experts, 100 hypotheses sampled), reliance on LLM-based internal rewards which can propagate model biases; Nash uniform distribution choice may limit prioritization in some settings.",
            "benchmark_dataset": "Evaluated on three curated datasets: MOOSE (Social Science), LLM4BioHypoGen (Biomedicine, 200 pairs), and LLM4CSHypoGen (Computer Science, 150 entries).",
            "uuid": "e4458.0",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GPT-3.5 Scorer",
            "name_full": "GPT-3.5 automated evaluator (scoring novelty, relevance, significance, verifiability)",
            "brief_description": "An automated evaluation pipeline that uses GPT-3.5 to score generated hypotheses on four targeted dimensions (novelty, relevance, significance, verifiability) to produce scalar judgments used in MC-NEST evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "GPT-3.5 four-dimension scoring",
            "evaluation_method_description": "GPT-3.5 is prompted to assign scores to hypotheses on novelty, relevance, significance and verifiability. Scores are aggregated to produce the automated reward R_n and to compute node Q values during MC-NEST search as well as to provide an automatic evaluation baseline across methods and prompting styles.",
            "evaluation_criteria": "Novelty, relevance, significance (impact/importance), verifiability/testability.",
            "model_name": "GPT-3.5",
            "model_size": null,
            "scientific_domain": "Applied across Social Science, Biomedicine, Computer Science datasets",
            "theory_type": "Testable hypotheses",
            "human_comparison": false,
            "evaluation_results": "Used as the primary automated rater in experiments; aggregated GPT-3.5 scores reported alongside human ratings and used to compute Q and reward signals. The paper reports good correlation between GPT-3.5 and human expert evaluations (exact correlation coefficients are described qualitatively; correlation used as evidence that GPT-3.5 can be a reliable automated evaluator).",
            "automated_vs_human_evaluation": "Automated (LLM-based).",
            "validation_method": "Validated by correlation analysis with blind human expert ratings on a sample of 100 hypotheses; paper claims GPT-3.5's potential as a reliable evaluator based on this correlation.",
            "limitations_challenges": "Automated LLM evaluators can reflect training-set biases and may prefer surface-level features; the paper does not report detailed statistics (e.g., Pearson/Spearman coefficients) in main text for the correlation, and automated scores may over/under-estimate certain qualitative properties.",
            "benchmark_dataset": "Same three curated datasets used in MC-NEST experiments (MOOSE, LLM4BioHypoGen, LLM4CSHypoGen).",
            "uuid": "e4458.1",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Human 3-point Expert Rating",
            "name_full": "Blind human expert 3-point scale evaluation",
            "brief_description": "Blind evaluation by three domain experts (professors, postdocs, PhD students) using a standardized 0–3 scale across novelty, clarity, significance, and verifiability for sampled hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Blind expert 3-point rubric (novelty, clarity, significance, verifiability)",
            "evaluation_method_description": "Human experts assess 100 randomly selected hypotheses (from baselines and MC-NEST) without knowing method provenance. Each hypothesis is rated on a 0–3 scale per dimension with prespecified anchors (0 = poor/unverifiable/trivial, 3 = exceptional/highly verifiable/high novelty). Scores are averaged to compare methods and to validate automated scorers.",
            "evaluation_criteria": "Novelty (priority), clarity/conciseness, significance (impact), verifiability (testability).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Social Science, Biomedicine, Computer Science (as in datasets)",
            "theory_type": "Testable research hypotheses",
            "human_comparison": false,
            "evaluation_results": "Human evaluation shows MC-NEST variants (with ZSCoT prompting and specific sampling policies) outperform baselines on average: e.g., MC-NEST with ZSCoT achieved average scores such as 2.62 (Social Science) and 2.37 (Computer Science) in human ratings; Importance Sampling performed best in Biomedicine in human study with 2.37. Exact per-condition numbers are reported in Table 8.",
            "automated_vs_human_evaluation": "Human-based; used in conjunction with automated GPT-3.5 scoring to form a hybrid evaluation.",
            "validation_method": "Blind rating design; validation performed indirectly by correlating aggregate human ratings with GPT-3.5 automated scores.",
            "limitations_challenges": "Small rater pool (3 experts), limited sample size (100 hypotheses blind-evaluated), possible inter-rater variability (no detailed inter-rater reliability statistics reported), evaluations focus on machine-generated outputs without a human-generated hypothesis baseline for direct output comparison.",
            "uuid": "e4458.2",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "BERTScore",
            "name_full": "BERTScore: Evaluating text generation with BERT",
            "brief_description": "A contextual embedding–based automatic metric that measures similarity between generated text and reference texts using pre-trained BERT representations; reported here as an auxiliary textual-quality metric.",
            "citation_title": "Bertscore: Evaluating text generation with bert.",
            "mention_or_use": "use",
            "evaluation_method_name": "BERTScore",
            "evaluation_method_description": "Compute precision/recall/F1 based on contextual embedding matching between generated hypotheses and reference texts (when available). Used as an auxiliary automated metric in tables to report textual alignment and fluency.",
            "evaluation_criteria": "Embedding-based textual similarity (precision, recall, F1); used as proxy for surface-level quality/faithfulness but not for novelty/testability.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied across the three datasets as a generic text-quality metric",
            "theory_type": "Textual hypotheses/statements",
            "human_comparison": false,
            "evaluation_results": "BERTScore values are reported in Tables 2–7; increases in rollout length and certain sampling policies (e.g., Greedy with longer rollouts) are associated with modest BERTScore improvements, consistent with qualitative metric gains.",
            "automated_vs_human_evaluation": "Automated metric (embedding-based).",
            "validation_method": "Established external metric (paper cites original BERTScore work); no additional internal validation reported beyond reporting alongside other metrics.",
            "limitations_challenges": "BERTScore measures surface/semantic similarity to references and does not capture novelty, empirical grounding, or verifiability; authors note focus on task-specific goals and sometimes exclude conventional metrics, but BERTScore is still reported as auxiliary.",
            "uuid": "e4458.3",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Modified UCT + Nash",
            "name_full": "Upper Confidence Bound for Trees (UCT) with Nash-equilibrium probability adjustment",
            "brief_description": "A node-selection scoring rule that augments the classical UCT formula with a uniform Nash-equilibrium-derived probability term to encourage fair exploration across candidate refinement actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Modified UCT (UCT + π_Nash)",
            "evaluation_method_description": "The selection score for node i is computed as UCT(i) = Q(i) + C * ln(N_parent)/N(i) + ε, then adjusted by adding 1/n (uniform π(h_i) from Nash strategy). The node with highest combined score is chosen for expansion/refinement. UCT balances exploration and exploitation via visit counts and Q rewards; Nash term is added to prevent premature collapse to few actions.",
            "evaluation_criteria": "Optimization of Q (quality) while balancing exploration (via visit-count/ln term) and fairness/diversity (via Nash uniform probability).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Method-level mechanism, applied within MC-NEST across domains",
            "theory_type": "Selection/optimization mechanism for hypothesis search",
            "human_comparison": false,
            "evaluation_results": "Used during MC-NEST search; contributes to experimentally observed improvements when combined with different sampling policies and rollout lengths. The paper reports that inclusion of Nash uniform probability helps maintain diversity and avoids premature convergence to suboptimal hypotheses, contributing to better novelty/quality trade-offs in results (detailed per-policy tables).",
            "automated_vs_human_evaluation": "Internal algorithmic selection metric (automated).",
            "validation_method": "Empirical ablation via sampling-policy experiments and rollout-length comparisons across datasets; improvement inferred from downstream automated and human-rated metrics.",
            "limitations_challenges": "Uniform Nash distribution (π=1/n) may be suboptimal if candidate qualities vary widely; theoretical guarantees not provided; hyperparameter C and rollout length materially affect behavior and must be tuned per domain.",
            "uuid": "e4458.4",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Sampling Policies (Greedy / IS / PIS)",
            "name_full": "Greedy, Importance Sampling (IS), Pairwise Importance Sampling (PIS) selection policies",
            "brief_description": "Three selection/sampling policies implemented in MC-NEST that combine UCT scores and Nash probabilities to choose nodes: Greedy selects argmax of UCT+π; IS weights choices by UCT×π; PIS compares pairs to pick between two candidates aiming to boost novelty or balance metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Greedy / Importance Sampling / Pairwise Importance Sampling policies",
            "evaluation_method_description": "Greedy: selects node with maximal UCT(i)+π(h_i). Importance Sampling: assigns weight = UCT(i) × π(h_i) and samples randomly according to weights. Pairwise Importance Sampling: randomly or systematically evaluates pairs (i,j) using combined UCT+π scores and selects the better of the pair, intended to promote exploration of promising but under-sampled nodes while maintaining diversity.",
            "evaluation_criteria": "Effect on final hypothesis Q and downstream metrics (novelty, clarity, significance, verifiability, BERTScore); measured by comparing aggregated scores across policies and rollouts.",
            "model_name": "GPT-4o; DeepSeek-R1-Distill-Qwen-32B; DeepSeek-R1-Distill-Qwen-7B",
            "model_size": "See MC-NEST entry",
            "scientific_domain": "Applied across Social Science, Biomedicine, Computer Science",
            "theory_type": "Search/sampling strategies for hypothesis generation",
            "human_comparison": false,
            "evaluation_results": "Policy-dependent effects observed: Greedy often achieved best overall averages (e.g., GPT-4o Greedy 8-step: 2.81 Social Science), Importance Sampling gave best Biomedicine performance for some models (e.g., DeepSeek-32B Importance Sampling 4-step: 2.87), Pairwise IS excelled in novelty (e.g., highest novelty reported in several tables). Longer rollouts (4-&gt;8 steps) improved performance for all policies in many cases.",
            "automated_vs_human_evaluation": "Automated internal sampling strategy; impact assessed by downstream automated (GPT-3.5, BERTScore) and human evaluations.",
            "validation_method": "Empirical cross-policy comparisons across datasets and models; performance reported in Tables 3, 5, 7.",
            "limitations_challenges": "Policy performance depends on model capacity and rollout length; some policies favor novelty at the possible cost of verifiability; hyperparameters and randomness can change relative ordering, requiring extensive tuning per domain.",
            "uuid": "e4458.5",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LLM Self-Refinement Evaluation Loop",
            "name_full": "LLM-based self-evaluation and iterative refinement (self-critique → refine → re-evaluate)",
            "brief_description": "An internal evaluation-refinement loop where the LLM generates critiques of a candidate hypothesis and uses those critiques to produce refined hypotheses, with refinements scored by the LLM to produce rewards used by MC-NEST.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM self-evaluation / SelfRefine loop",
            "evaluation_method_description": "Given a candidate answer A_n, the system prompts the LLM to produce a critique C_n = LLM(P + A_n) and then asks the LLM to produce a refined answer A_{n+1} = LLM(P + A_n + C_n). The refined answer is stored as a child node. Rewards R_n for answers are computed via an EvaluatePrompt call (R_n = LLM(EvaluatePrompt(P,A_n))). If R_n exceeds a preset limit, a penalty is applied to curb runaway scores. These LLM-derived rewards feed into Q and are backpropagated in the MC-NEST tree.",
            "evaluation_criteria": "Internal LLM-derived quality (R_n / Q values) reflecting novelty, clarity, empirical alignment as encoded in evaluation prompts.",
            "model_name": "Same LLMs used for generation/evaluation (e.g., GPT-4o, DeepSeek variants); evaluators sometimes use GPT-3.5 as separate scorer.",
            "model_size": null,
            "scientific_domain": "Used across evaluated domains within MC-NEST",
            "theory_type": "Mechanism for iterative hypothesis refinement and internal scoring",
            "human_comparison": false,
            "evaluation_results": "Self-refinement produces improved child hypotheses during tree expansion; the approach contributes to observed gains when combined with UCT/Nash selection and appropriate sampling policies. Paper reports empirical improvements in downstream metrics when iterative self-refinement is used versus single-shot generation.",
            "automated_vs_human_evaluation": "Automated internal mechanism (LLM-on-LLM evaluation) but outputs are later validated by automated scorers and human experts.",
            "validation_method": "Empirical ablations comparing number of refinement steps and rollout lengths; validation also indirect via external automated and human scores.",
            "limitations_challenges": "Risk of LLM overfitting to its own artifacts (self-confirmation bias), reliance on quality of evaluation prompts, potential instability of reward scaling (necessitating penalty thresholds), and propagation of model biases into Q-values.",
            "uuid": "e4458.6",
            "source_info": {
                "paper_title": "Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bertscore: Evaluating text generation with bert.",
            "rating": 2,
            "sanitized_title": "bertscore_evaluating_text_generation_with_bert"
        },
        {
            "paper_title": "Large language models as biomedical hypothesis generators: a comprehensive evaluation.",
            "rating": 2,
            "sanitized_title": "large_language_models_as_biomedical_hypothesis_generators_a_comprehensive_evaluation"
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery.",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Hypothesis generation with large language models.",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers.",
            "rating": 1,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        }
    ],
    "cost": 0.016439,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees
25 Mar 2025</p>
<p>Gollam Rabby gollam.rabby@l3s.de 
L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>TIB-Leibniz Information Centre for Science and Technology
HannoverGermany{diyana.muhammed</p>
<p>Diyana Muhammed 
TIB-Leibniz Information Centre for Science and Technology
HannoverGermany{diyana.muhammed</p>
<p>Sören Auer auer@tib.eu 
L3S Research Center
Leibniz University Hannover
HannoverGermany</p>
<p>TIB-Leibniz Information Centre for Science and Technology
HannoverGermany{diyana.muhammed</p>
<p>Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees
25 Mar 2025021740E3B2000557A87C56F0C52BEC05arXiv:2503.19309v1[cs.CL]Scientific Hypothesis GenerationMonte Carlo Tree SearchAdaptive Sampling StrategiesHypothesis Refinement
Scientific hypothesis generation is a fundamentally challenging task in research, requiring the synthesis of novel and empirically grounded insights.Traditional approaches rely on human intuition and domain expertise, while purely large language model (LLM) based methods often struggle to produce hypotheses that are both innovative and reliable.To address these limitations, we propose the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a novel framework that integrates Monte Carlo Tree Search (MCTS) with Nash Equilibrium strategies to iteratively refine and validate hypotheses.MC-NEST dynamically balances exploration and exploitation through adaptive sampling strategies, which prioritize high-potential hypotheses while maintaining diversity in the search space.We demonstrate the effectiveness of MC-NEST through comprehensive experiments across multiple domains, including biomedicine, social science, and computer science.MC-NEST achieves average scores of 2.65, 2.74, and 2.80 (on a 1-3 scale) for novelty, clarity, significance, and verifiability metrics on the social science, computer science, and biomedicine datasets, respectively, outperforming state-of-the-art prompt-based methods, which achieve 2.36, 2.51, and 2.52 on the same datasets.These results underscore MC-NEST's ability to generate high-quality, empirically grounded hypotheses across diverse domains.Furthermore, MC-NEST facilitates structured human-AI collaboration, ensuring that LLMs augment human creativity rather than replace it.By addressing key challenges such as iterative refinement and the exploration-exploitation balance, MC-NEST sets a new benchmark in automated hypothesis generation.The framework provides a robust and adaptable approach that advances the boundaries of scientific discovery.Additionally, MC-NEST's ethical design enables responsible AI use, emphasizing transparency and human supervision in hypothesis generation.</p>
<p>Introduction</p>
<p>Scientific hypothesis generation drives discovery and innovation but remains limited by the scale and complexity of modern challenges.While large language models (LLMs) show promise in automating this process [2], existing approaches struggle to generate hypotheses that are both novel and empirically grounded due to a lack of iterative refinement and poor exploration-exploitation balance [5].</p>
<p>To address these challenges, we utilize the Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST), a framework that integrates the Monte Carlo Tree Search (MCTS) with Nash Equilibrium strategies to iteratively refine hypotheses [15].MC-NEST frames hypothesis generation as a game, where the players are competing strategies for exploring and refining hypotheses.This game-theoretic approach allows MC-NEST to balance the trade-offs between exploring new ideas and exploiting known high-quality hypotheses.Each strategy aims to maximize the quality of the generated hypotheses, and Nash Equilibrium ensures a balance where no player (strategy) can improve its outcome by unilaterally changing its approach.These strategies guide the exploration and refinement phases by dynamically adjusting the trade-off between exploring new hypotheses and exploiting known high-quality ones, ensuring optimal hypothesis generation.</p>
<p>MC-NEST dynamically balances exploration and exploitation using adaptive sampling techniques, ensuring diverse and high-potential hypotheses.The framework operates in two phases: (1) an exploration phase, where MCTS navigates the hypothesis space guided by Nash Equilibrium, and (2) a refinement phase, where adaptive sampling and iterative self-reflection ensure hypotheses are innovative and empirically grounded.For instance, in peptide optimization, exploration might involve proposing a new substitution (e.g., replacing arginine with lysine) to test its effect on solubility, while exploitation would refine this idea by validating whether the substitution improves solubility without compromising the peptide's nuclear localization function.Experiments across biomedicine, social science, and computer science demonstrate MC-NEST's effectiveness in hypothesis generation.MC-NEST achieves higher novelty, clarity, significance, and verifiability compared to existing methods [23], demonstrating its effectiveness in generating scientifically impactful hypotheses.Specifically, MC-NEST achieves scores of 2.65, 2.74, and 2.80 (on a 1-3 scale) for novelty, clarity, significance, and verifiability on the social science, computer science, and biomedicine datasets, respectively.These results outperform state-of-the-art prompt-based methods, which achieve 2.36, 2.51, and 2.52 on the same datasets.This improvement demonstrates MC-NEST's ability to generate hypotheses that are not only innovative but also empirically grounded and scientifically impactful.</p>
<p>A key innovation is MC-NEST's ability to incorporate emerging scientific literature, addressing the limitations of automatic refinement and explorationexploitation balance.The framework supports structured human-AI collaboration, where LLMs augment human expertise rather than replace it.This approach balances AI and human judgment, mitigating over-reliance on AI.While AI excels at generating novel hypotheses and exploring chemical spaces, human expertise is critical for interpreting results, identifying biases, and ensuring ethical decisions.For example, in peptide optimization, MC-NEST proposes substitutions (e.g., lysine-for-arginine) to improve solubility, while humans validate whether these changes maintain nuclear localization and align with biochemical principles.This iterative collaboration combines AI's exploratory capabilities with human expertise, ensuring scientifically robust and ethically sound outcomes.For research problems, impact is as critical as novelty.While novelty ensures that hypotheses are original, impact ensures they address meaningful scientific challenges.MC-NEST achieves this balance by generating hypotheses that are not only novel but also grounded in domain-specific knowledge and validated for real-world applicability.Unlike purely exploratory methods, MC-NEST incorporates iterative refinement and validation, ensuring that hypotheses are both innovative and empirically grounded.For example, in complex scientific domains such as protein engineering, MC-NEST's proposed modifications (e.g., lysinefor-arginine substitutions) are designed to enhance solubility while maintaining critical functional properties-a dual focus that directly addresses high-priority scientific and therapeutic needs.By combining exploration with rigorous validation, MC-NEST ensures that its hypotheses are not only novel but also impactful, contributing to solving real-world problems with significant scientific and practical implications.Our contributions include:</p>
<p>-MC-NEST, a framework integrating MCTS and Nash Equilibrium for hypothesis generation, enhanced by adaptive sampling techniques.-A comprehensive performance analysis across multiple domains, with detailed studies highlighting the impact of each component.-A human-AI collaboration approach that improves hypothesis quality through expert refinement.</p>
<p>To ensure reproducibility, we will release all used source codes, datasets, and evaluation protocols.</p>
<p>To illustrate MC-NEST's capabilities, we present an example of hypothesis generation and refinement for optimizing a synthetic peptide sequence (MARTKQ-TARKSTGGKAPRKQLASKAARKSAARAAAAGGGGGGG) for nuclear localization and solubility.MC-NEST generates an initial hypothesis: Substituting lysine for arginine in the nuclear localization signal (NLS) preserves the positive charge required for nuclear import while enhancing solubility due to lysine's less bulky structure.Validation against biochemical principles reveals potential tradeoffs, such as reduced binding affinity to nuclear import receptors [7].MC-NEST refines the hypothesis by incorporating additional modifications: Replacing some glycine residues with alanine in the glycine-rich linker to maintain flexibility without introducing phosphorylation sites.Experimental validation confirms that the modified peptide outperforms the original sequence, retaining nuclear localization efficiency while improving solubility and functionality.The updated sequence generated by MC-NEST is: MAKTQTGRPKSTGGPAPRKQLASP-PARKSVAARAAAASGGGSGG.A visual comparison (by AlphaFold) of the original and updated peptide sequences is shown in Figure 1.</p>
<p>Methodology</p>
<p>MC-NEST is a computational framework designed to enhance the problemsolving capabilities of LLMs for scientific hypothesis generation [15].As illustrated in Figure 2, MC-NEST integrates the Monte Carlo Tree Search, a decisionmaking algorithm for exploring large search spaces [3], with Nash Equilibrium strategies to iteratively refine hypotheses and solutions.By dynamically balancing exploration and exploitation, MC-NEST ensures that generated hypotheses are both innovative and empirically grounded.</p>
<p>Problem Setting for Hypothesis Generation.MC-NEST is designed for a structured search over combinatorial hypothesis spaces, particularly in domains requiring rigorous reasoning and insight.The framework addresses the challenge of efficiently navigating vast search spaces while ensuring quality, efficiency, and novelty.Specifically, MC-NEST targets problems where:</p>
<p>-The hypothesis space is combinatorial, with solutions constructed from smaller reasoning steps or building blocks.For example, in protein engineering, a hypothesis might propose amino acid substitutions to optimize functions like nuclear localization or solubility [19].A specific hypothesis could suggest substituting lysine for arginine in a nuclear localization signal (NLS), preserving the positive charge required for nuclear import while enhancing solubility due to lysine's less bulky structure.Such hypotheses are built from testable steps (e.g., charge preservation, solubility enhancement) that can be experimentally validated.-The search space is large for exhaustive exploration, necessitating intelligent traversal strategies [9].For example, the space of possible amino acid substitutions is intractable without a guided search.Traditional methods often focus on well-known substitutions (e.g., arginine-to-lysine in NLS), while MC-NEST explores less-studied modifications, such as introducing alanine into glycine-rich linkers to enhance flexibility without adding phosphorylation sites.By prioritizing high-potential but underexplored changes, MC-NEST uncovers novel solutions missed by traditional approaches.-Solutions must satisfy strict correctness criteria, including clarity, testability, relevance, and novelty [23].For instance, a hypothesis must clearly describe relationships (e.g., "substituting lysine for arginine enhances nuclear import efficiency"), be testable (e.g., via fluorescence microscopy or solubility assays), relevant (e.g., optimizing synthetic peptides for mammalian cell expression), and novel (e.g., identifying alanine's role in linker flexibility).MC-NEST ensures that hypotheses meet these criteria by iteratively refining and validating them against biochemical principles and experimental data.</p>
<p>Search Space and Traversal Strategy.The search space in MC-NEST is represented as a tree, where nodes correspond to solutions (e.g., hypotheses or amino acid substitutions), and edges represent logical transitions.The traversal strategy combines exploration and exploitation: 1) Upper Confidence Bound for Trees (UCT) balances exploration and exploitation by estimating branch potential using confidence intervals, favoring high-uncertainty or high-performance paths [15] (subsection 2.2).For example, UCT explores less-studied substitutions (e.g., alanine in glycine-rich linkers) while leveraging known modifications (e.g., lysine-for-arginine in the NLS).2) Exploration prioritizes underexplored branches, balancing novelty and promise, as seen in game-playing AI like Al-phaGo [18] (subsection 2.2). 3) Exploitation refines promising branches using probabilistic node selection, focusing on high-quality regions while maintaining diversity (subsection 2.2).For instance, MC-NEST exploits beneficial substitutions (e.g., lysine-for-arginine) while exploring novel combinations (e.g., alanine in glycine-rich linkers) to optimize functionality.</p>
<p>Benefits of the MC-NEST Framework in Hypothesis Generation</p>
<p>Scientific discovery has traditionally relied on structured methodologies but often faces limitations due to their lack of refinement and difficulty in balancing exploration and exploitation [5].Existing frameworks struggle to adapt to emerging scientific literature or integrate new discoveries, leading to hypotheses that are either theoretically sound but empirically unsupported or computationally generated but lacking empirical grounding.For example, traditional methods might focus on well-known substitutions (e.g., arginine-to-lysine in the NLS) but overlook novel modifications (e.g., alanine in a glycine-rich linker) that enhance functionality [19].The exponential growth of scientific publications further complicates the process, as researchers must sift through vast amounts of literature to identify meaningful insights [14].While LLMs offer potential, they often fail to generate hypotheses that are both novel and empirically validated [2].</p>
<p>Limitations of Existing Approaches.Previous works have attempted to address these gaps through approaches like zero-shot hypothesis generation but suffer from critical limitations: 1) Lack of Iterative Refinement: Hypotheses may be theoretically sound but lack iterative refinement [20].2) Imbalanced Exploration-Exploitation: Conventional approaches struggle to balance novel hypothesis exploration with established patterns, leading to biased or suboptimal results [9].</p>
<p>Addressing Challenges with MC-NEST.MC-NEST integrates Nash Equilibrium strategies with LLM-based self-refinement to address these limitations: 1) Dynamic Adaptation: MC-NEST balances exploration and exploitation using Nash Equilibrium, enabling adaptability to emerging scientific contexts.For example, in protein engineering, it explores less-studied modifications (e.g., alanine in a glycine-rich linker) while leveraging well-known substitutions (e.g., lysinefor-arginine in the NLS).2) Iterative Self-Refinement: MC-NEST employs MCTS with iterative self-critique, refining hypotheses against known principles.For instance, it identifies trade-offs (e.g., reduced binding affinity) and incorporates additional modifications (e.g., alanine in the glycine-rich linker).3) Strategic Exploration: MC-NEST uses sampling approaches to prioritize high-potential hypotheses while maintaining diversity, ensuring robust hypothesis generation.</p>
<p>Monte Carlo Nash Equilibrium Self-Refine Tree (MC-NEST)</p>
<p>The objective of MC-NEST is to generate a research hypothesis h * for a given problem instance p. Formally, let H denote the hypothesis space, where each hypothesis h ∈ H represents a candidate research statement.The goal is to identify h * that optimizes a quality function Q(h), capturing validity, novelty, and coherence:
h * = arg max h∈H Q(h)
Initialization.In MC-NEST, the root node represents the initial hypothesis state, with edges denoting potential transformations or refinements through iterative self-critique and exploration strategies.To initialize the root node, we use a pre-trained LLM with a Zero-Shot Chain-of-Thought (ZSCoT) strategy [10].Specifically, the LLM is prompted with the input instance p to generate an initial hypothesis without relying on task-specific fine-tuning or prior search history.This approach leverages the LLM's broad, pre-trained knowledge to establish a well-reasoned starting point, enhancing adaptability and promoting a wide, unbiased exploration of the hypothesis space.The initialization is represented as: root = Node(hypothesis = ZSCoT_LLM(p))</p>
<p>Candidate Node Generation.Child nodes are generated by applying a structured process of self-refinement and self-evaluation to the parent node's hypothesis.Self-refinement focuses on improving the hypothesis itself by prompting the LLM with the current hypothesis and customizing instructions, such as increasing specificity, enhancing novelty, or aligning better with empirical data.</p>
<p>The LLM determines what to refine using predefined heuristics-such as logical coherence, relevance to the research goal, and consistency with known information-that guide the refinement.Following refinement, self-evaluation updates the hypothesis against these metrics to ensure each child node represents an improvement over its parent.Nodes are visited using a breadth-first search (BFS) strategy [4], where a node is expanded if it has not reached its maximum allowed children and none of its children have a higher quality score Q than the node itself.If no candidate nodes meet these criteria, the method refines the root node, reinitializing the search by generating a new hypothesis using the ZSCoT strategy.This approach balances exploration (generating new hypotheses) and exploitation (refining existing ones) by dynamically adjusting based on the quality scores of hypotheses.While global optimality is not guaranteed, the iterative refinement process aims to converge towards high-quality hypotheses, with higher Q-scores indicating better solutions.</p>
<p>Nash Equilibrium Strategy for Node Selection.The hypothesis generation process in MC-NEST begins with an initial hypothesis generated by a pretrained LLM at the root node of a search tree.Each node represents a unique hypothesis state, and edges signify possible refinements through iterative selfcritique.Child nodes are created by refining the parent node's hypothesis using structured prompts, employing self-refinement and self-evaluation techniques to iteratively enhance the hypothesis.</p>
<p>During node selection, MC-NEST uses the UCT, where each node is assigned a quality score Q derived from evaluation metrics such as logical coherence, novelty, and empirical alignment.The UCT score balances the exploration of under-explored nodes and the exploitation of high-quality hypotheses, guiding the search toward optimal solutions.A node is considered fully expanded if it reaches the maximum allowed number of children or if any child exhibits a reward Q greater than or equal to that of the current node.For a set of candidate nodes, N ode(Hypothesis) = {h 1 , h 2 , . . ., h n }, the Nash Equilibrium strategy assigns a uniform probability distribution over possible actions:
π(h i ) = 1 n , ∀i = 1, 2, . . . , n,
where n is the number of candidate nodes.This uniform probability ensures fair exploration of the hypothesis space, preventing premature convergence to suboptimal solutions.The MC-NEST framework employs three selection policies to balance exploration and exploitation:</p>
<p>-Greedy Policy selects the node with the highest combined score of UCT and Nash equilibrium probability: i * = arg max i [U CT (i) + π(h i )] -Importance Sampling Policy assigns selection weights based on the product of UCT scores and Nash equilibrium probabilities:
Weight(i) = U CT (i) × π(h i ), i * = random_choice(C, weights = {Weight(i)})
-Pairwise Importance Sampling Policy evaluates pairs of nodes (i, j) based on UCT differences and weights, selecting the node with the higher combined score:i * = arg max (UCT(i) + π(h i ), UCT(j) + π(h j )).</p>
<p>These policies systematically balance exploration and exploitation, ensuring that the search process prioritizes high-reward nodes while maintaining a broad exploration of the hypothesis space.</p>
<p>Upper Confidence Bound (UCT) Update.The UCT update guides node refinement by computing:
U CT (i) = Q(i) + C ln(Nparent) N (i)+ϵ
, where Q(i) is the hypothesis reward, C controls exploration, N parent is parent visits, N (i) is node visits, and ϵ avoids division by zero.The score is adjusted with Nash equilibrium probability: UCT
(i) = Q(i) + C ln(Nparent) N (i)+ϵ + 1
n .The node with the highest score, i * = arg max i [Score(i)], is selected for refinement or as the final hypothesis, ensuring robust exploration and exploitation of the hypothesis space.</p>
<p>Expansion.Following node selection, MC-NEST expands the search tree by generating a refined child node.Given a selected node n s , a new child n c is created via self-refinement: n c = SelfRefine(n s ).This process critiques and improves the solution at n s , storing the refined version in n c : n s .children← n s .children∪ {n c }.The critique is formulated as Critique(a s ) = LLMCritique(p, a s ), where p is the problem instance.The refined answer a c is: a c = RefineAnswer(p, a s , Critique(a s )) and assigned to n c .This structured expansion enables MC-NEST to enhance solutions iteratively, driving systematic search improvement.</p>
<p>Backpropagation.MC-NEST updates node quality scores Q and visit counts from the newly expanded node n c up to the root.This propagates deeper exploration insights into higher-level decisions.Given a child node n c and its parent n p , backpropagation updates Q(n p ) using:
Q(n p ) = Q(np)+max(Q(nc)) 2
. This balances the exploitation of known values with exploration.The visit count is incremented: Visit(n p ) = Visit(n p ) +1.The process recurses from n c to the root, ensuring informed node selection in MC-NEST.</p>
<p>Self-Refine.MC-NEST evaluates candidate answers by assigning a reward R n based on answer quality.Given a node n with answer A n , the reward is computed as: R n = LLM (EvaluatePrompt(P, A n )) .If R n exceeds a predefined limit, a penalty is applied:
Rn = R n , R n ≤ R n _limit R n − penalty, R n &gt; R n _limit.
Node statistics are updated: TotalReward n + = Rn , VisitCount n + = 1.This ensures balanced reward scaling, refining MC-NEST's decision-making.</p>
<p>Self-Evaluation.MC-NEST iteratively improves candidate solutions via LLMbased critique and refinement.Given a node n with answer A n , a critique C n is generated using: C n = LLM(P + A n ).Using C n , the answer is refined:
A n+1 = LLM(P + A n + C n )
. The refined answer A n+1 is stored in a new child node, iteratively enhancing solutions in MC-NEST.</p>
<p>Human-AI Collaboration.MC-NEST is designed to facilitate iterative human-AI collaboration, enabling researchers to refine and validate hypotheses dynamically.Upon generating a final hypothesis, MC-NEST enables human experts to evaluate its novelty, clarity, significance, and verifiability, with the option to iteratively refine the process as needed based on researcher input.This iterative loop ensures that the generated hypotheses align with domain-specific knowledge and scientific rigor while also incorporating human intuition and expertise.By integrating human judgment at critical stages, MC-NEST not only enhances the reliability of its outputs but also fosters a collaborative environment where AI augments human creativity rather than replacing it.</p>
<p>Experiments</p>
<p>In our experiments, we utilized ZSCoT prompting as our base prompting style with GPT-4o [1], DeepSeek-R1-Distill-Qwen-32B [6] and DeepSeek-R1-Distill-Qwen-7B [6] LLM.</p>
<p>Evaluation Setup</p>
<p>We evaluated MC-NEST using GPT-4o, DeepSeek-R1-Distill-Qwen-32B, and DeepSeek-R1-Distill-Qwen-7B, with GPT-4o serving as a strong general-purpose baseline due to its proficiency in hypothesis generation [16].DeepSeek (32B and 7B parameters) provides insights into the scalability and efficiency of MC-NEST across different distilled LLM sizes.To ensure consistent and systematic evaluation, we employed three prompting styles: zero-shot (ZS) [12], zero-shot chain-of-thought (ZSCoT) and few-shot (FS) [11], using 2-shot, 3-shot, and 5shot configurations with both closed-source and open-source LLMs to assess the impact of prompting.</p>
<p>Datasets</p>
<p>We evaluated MC-NEST on three datasets spanning social science, biomedicine, and computer science.Each dataset was carefully curated to ensure high-quality annotations and relevance to hypothesis generation tasks.</p>
<p>Evaluation Metrics</p>
<p>We evaluate generated hypotheses using both automatic and human assessments.</p>
<p>For automatic evaluation, GPT-3.5 scores hypotheses on four key aspects: novelty, relevance, significance, and verifiability [17].Novelty and verifiability are prioritized as they align with the philosophical foundations of hypothetical induction, while relevance and significance reflect the practical utility of hypotheses for researchers.Conventional metrics like BERTScore [22] are excluded to focus on task-specific goals.For human evaluation, three domain experts (Professors, postdocs, and PhD students) blindly assess 100 randomly selected hypotheses from baseline and proposed methods, using a standardized 3-point scale.Novelty is emphasized over verifiability, as even imperfect hypotheses can inspire scientific exploration [23], whereas non-novel hypotheses offer limited utility.We also analyze the correlation between GPT-3.5 and expert evaluations, suggesting GPT-3.5'spotential as a reliable evaluator for machine-generated hypotheses [2].</p>
<p>Results and Analyses</p>
<p>In this section, we present the results of our experiments evaluating the performance of prompting strategies and MC-NEST across three datasets: Social Science, Computer Science, and Biomedicine.We analyze the impact of different prompting methods (Zero-Shot, Few-Shot, and Zero-Shot Chain-of-Thought) and MC-NEST sampling strategies (Greedy, Importance Sampling, and Pairwise Importance Sampling) on hypothesis generation quality, as measured by BERTScore and qualitative metrics such as novelty, clarity, significance, and verifiability.</p>
<p>Social Science Dataset</p>
<p>Prompting Strategies.Table 2 summarizes the performance of different prompting strategies on the Social Science dataset.ZSCoT consistently outperforms ZS and FS approaches across all evaluated LLMs.For DeepSeek-32B, ZSCoT achieves an average score of 2.65, compared to 2.44 for ZS and 2.52 for 2-FS.Similarly, DeepSeek-7B with ZSCoT attains an average score of 2.56, outperforming MC-NEST Sampling Strategies.Table 3 presents the results of MC-NEST evaluations using Greedy, Importance Sampling, and Pairwise Importance Sampling.For GPT-4o, Greedy sampling with an eight-step rollout achieves the highest overall score of 2.81.Pairwise Importance Sampling, however, excels in novelty with 2.74 while maintaining competitive clarity and significance scores.DeepSeek-32B shows similar trends, with Greedy sampling achieving the best overall results with 2.91 at an eight-step rollout.For DeepSeek-7B, Importance Sampling performs best at a four-step rollout with 2.78, while Pairwise Importance Sampling achieves balanced performance at eight steps with 2.76.These results highlight the effectiveness of MC-NEST in enhancing the quality of social science hypothesis generation.</p>
<p>Computer Science Dataset</p>
<p>Prompting Strategies.As shown in Our experiments demonstrate that structured reasoning and adaptive sampling strategies with MC-NEST significantly enhance hypothesis generation quality across domains.Increasing rollout lengths generally improves performance, with Pairwise Importance Sampling offering a competitive balance between novelty and verifiability.These findings underscore the importance of MC-NEST with sampling strategies for optimizing LLM performance in scientific hypothesis generation.-0: The hypothesis is poorly structured and hard to understand.</p>
<p>-1: The hypothesis is somewhat understandable but contains irrelevant information.-2: The hypothesis is clear but needs minor improvements.</p>
<p>Fig. 1 :
1
Fig. 1: Comparison of original and MC-NEST hypothesis-generated synthetic peptide sequences visualized by AlphaFold [8].(a) Original sequence with NLS (red) and glycine-rich linker (blue).(b) Updated sequence with lysine-forarginine substitutions in the NLS (red) and alanine-for-glycine substitutions in the linker (blue).Code: Google Colab Notebook</p>
<p>Fig. 2 :
2
Fig. 2: Overview of the MC-NEST methodology for hypothesis generation.</p>
<ul>
<li>3 : 1 : 2 : 3 : 1 : 2 : 3 :
3123123
The hypothesis is exceptionally well-written and logically structured.At the end of your response, clearly state the score in the format: Score: [value] Background: {background} Generated Hypothesis: {hypothesis} Evaluation Prompt Significance Evaluation: You are a research scientist.Evaluate the significance of the hypothesis.Score from 0 to 3:-0: The hypothesis is trivial and lacks importance.-Thehypothesis has slight significance but limited value.-Thehypothesis offers some important insights.-Thehypothesis is highly significant with a strong impact.At the end of your response, clearly state the score in the format: Score: [value] Background: {background} Generated Hypothesis: {hypothesis} Verifiability Evaluation: You are a research scientist.Evaluate the verifiability of the hypothesis.Score from 0 to 3:-0: The hypothesis cannot be scientifically verified.-Thehypothesis has slight verifiability but lacks clear testing methods.-Thehypothesis is moderately verifiable.-Thehypothesis is strongly verifiable with clear testing methods.At the end of your response, clearly state the score in the format: Score: [value] Background: {background} Generated Hypothesis: {hypothesis}</li>
</ul>
<p>Table 1 :
1
Comparison with existing scientific hypotheses generation datasets; Count = validation data count.
DatasetSourceDomainAnnotation CountLLM4BioHypoGen [13]TextBiomedicineManual200MOOSE [21]TextSocial ScienceManual50LLM4CSHypoGen (Ours)TextComputer ScienceManual150</p>
<p>Table 1
1
[13]ides an overview of the datasets used in our experiments.1)SocialScienceDataset:TheMOOSE dataset[21]consists of 50 social science research papers paired with raw web corpora (e.g., news articles, Wikipedia).This dataset challenges systems to generate novel hypotheses without relying on pre-existing scientific knowledge, emphasizing the open-domain nature of hypothesis generation.2) Biomedicine Dataset: The LLM4BioHypoGen dataset[13]contains 200 background-hypothesis pairs extracted from biomedical research papers.It is divided into training, seen, and unseen test sets based on publication dates to prevent data contamination, ensuring robust evaluation of hypothesis generation capabilities.3) Computer Science Dataset: Our LLM4CSHypoGen dataset comprises 150 research papers (2024-2025) with structured content, including hypotheses, methods, and results.Each entry was cross-checked by domain experts to ensure accuracy and reliability, providing a robust foundation for evaluating hypothesis generation in computer science.</p>
<p>Table 2 :
2
Evaluation with prompt on social science dataset.
LLMSize PromptBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -ZS85.9785.47 85.711.901.862.082.622.12GPT-4o -ZSCoT79.8684.86 82.272.162.462.722.502.46GPT-4o -2FS83.3786.83 85.062.002.222.222.622.27GPT-4o -3FS83.3086.81 85.012.062.082.182.482.20GPT-4o -5FS83.2786.74 84.962.022.082.222.522.21Deepseek 32B ZS83.2586.16 84.672.102.402.602.652.44DeepSeek 32B ZSCoT78.7684.99 81.742.352.752.752.752.65Deepseek 32B 2FS82.7186.16 84.392.102.702.652.652.52Deepseek 32B 3FS82.5886.03 84.272.252.652.452.702.51Deepseek 32B 5FS82.0486.00 83.962.302.452.502.752.50Deepseek 7B ZS82.7485.51 84.092.102.352.352.552.34Deepseek 7B ZSCoT78.1084.16 81.002.202.702.702.652.56Deepseek 7B 2FS84.5686.60 85.562.102.402.602.702.45Deepseek 7B 3FS82.8585.61 84.172.102.252.652.602.40Deepseek 7B 5FS83.5986.06 84.802.202.252.502.402.34</p>
<p>Table 3 :
3
MC-NEST evaluation on social science dataset; IS = Importance Sampling; PIS = Pairwise Importance Sampling.
LLMSize Rollout SamplingBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -4Greedy80.7185.44 82.992.582.842.702.882.75GPT-4o -4IS80.7285.43 83.002.602.802.782.942.78GPT-4o -4PIS80.6585.42 82.952.742.762.702.922.78GPT-4o -8Greedy80.5085.14 82.742.702.802.802.942.81 ↑GPT-4o -8IS80.3385.13 82.652.642.822.642.902.75GPT-4o -8PIS80.5585.16 82.782.742.822.802.842.80Deepseek 32B 4Greedy80.8785.25 82.992.553.002.802.952.83Deepseek 32B 4IS80.3885.36 82.792.702.852.852.902.83Deepseek 32B 4PIS80.8885.34 83.042.552.852.752.902.76Deepseek 32B 8Greedy80.5385.24 82.812.702.953.003.002.91 ↑Deepseek 32B 8IS80.5485.38 82.892.652.852.852.952.83Deepseek 32B 8PIS80.1584.98 82.492.752.952.952.952.90Deepseek 7B 4Greedy80.6185.16 82.812.552.602.902.952.75Deepseek 7B 4IS80.0884.66 82.312.652.852.752.852.78 ↑Deepseek 7B 4PIS80.4585.10 82.702.502.802.652.902.71Deepseek 7B 8Greedy80.7885.05 82.852.452.752.602.852.66Deepseek 7B 8IS80.6085.05 82.762.652.652.652.802.69Deepseek 7B 8PIS80.5484.92 82.672.552.852.802.852.76</p>
<p>Table 4 :
4
Evaluation with prompt on computer science dataset.
LLMSize PromptBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -ZS88.0588.35 88.192.102.122.422.862.38GPT-4o -ZSCoT81.5887.42 84.392.312.292.282.942.60GPT-4o -2FS84.8488.79 86.762.192.052.532.912.42GPT-4o -3FS84.8388.88 86.802.162.052.452.882.38GPT-4o -5FS85.0688.88 86.932.172.032.532.882.40Deepseek 32B ZS87.8189.61 88.692.252.152.602.902.48Deepseek 32B ZSCoT80.7588.30 84.342.502.552.903.002.74Deepseek 32B 2FS84.4689.46 86.872.402.402.752.852.60Deepseek 32B 3FS84.6789.57 87.042.402.402.802.852.61Deepseek 32B 5FS84.1589.39 86.682.552.502.752.652.61Deepseek 7B ZS86.2289.17 87.662.202.202.752.952.53Deepseek 7B ZSCoT79.4787.54-2.352.702.803.002.71Deepseek 7B 2FS85.6388.77 87.152.001.952.452.752.29Deepseek 7B 3FS86.6889.60 88.112.152.052.802.952.49Deepseek 7B 5FS85.6189.09 87.292.202.252.452.852.44</p>
<p>Table 5 :
5
MC-NEST evaluation on computer science dataset; IS = Importance Sampling; PIS = Pairwise Importance Sampling.
LLMSize Rollout SamplingBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -4Greedy82.6488.24 85.352.682.672.853.002.80GPT-4o -4IS82.8188.11 85.372.712.582.883.002.79GPT-4o -4PIS82.6588.22 85.342.712.622.832.992.76GPT-4o -8Greedy82.7288.11 85.322.722.592.852.992.79GPT-4o -8IS82.6088.11 85.262.732.572.842.972.78GPT-4o -8PIS82.5488.14 85.252.772.652.852.992.82 ↑Deepseek 32B 4Greedy83.1988.39 85.662.552.652.853.002.76Deepseek 32B 4IS82.9988.49 85.642.652.602.953.002.80Deepseek 32B 4PIS83.0788.63 85.752.552.352.853.002.69Deepseek 32B 8Greedy82.4688.24 85.252.602.652.903.002.79Deepseek 32B 8IS83.0288.50 85.662.652.602.903.002.79Deepseek 32B 8PIS82.8188.42 85.512.652.753.003.002.85 ↑Deepseek 7B 4Greedy83.4688.59 85.942.602.602.903.002.78Deepseek 7B 4IS83.4088.41 85.872.552.452.753.002.69Deepseek 7B 4PIS83.3588.63 85.902.652.502.753.002.73Deepseek 7B 8Greedy82.8888.55 85.612.752.702.803.002.81 ↑Deepseek 7B 8IS83.1388.49 85.722.652.752.853.002.81 ↑Deepseek 7B 8PIS82.0387.90 84.862.652.652.802.952.76</p>
<p>Table 6 :
6
Evaluation with prompt on biomedicine dataset.
LLMSize PromptBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -ZS87.5385.59 86.541.882.172.322.592.24GPT-4o -ZSCoT81.7486.20 83.912.312.492.872.832.62GPT-4o -2FS87.1188.39 87.741.982.172.352.722.31GPT-4o -3FS87.0788.50 87.752.022.212.312.682.30GPT-4o -5FS87.0888.49 87.772.042.202.352.692.32Deepseek 32B ZS85.7685.14 85.431.952.352.652.652.40Deepseek 32B ZSCoT80.0785.68 82.772.552.802.902.952.80Deepseek 32B 2FS86.1388.06 87.082.102.402.652.752.48Deepseek 32B 3FS86.5188.24 87.362.152.402.402.752.43Deepseek 32B 5FS85.7687.98 86.852.152.552.452.502.41Deepseek 7B ZS83.1985.80 84.441.901.862.082.622.12Deepseek 7B ZSCoT80.6285.47 82.962.062.082.182.482.20Deepseek 7B 2FS85.1286.39 85.742.022.082.222.522.21Deepseek 7B 3FS86.2087.22 86.702.162.462.722.502.46Deepseek 7B 5FS85.0386.53 85.752.002.222.222.622.27</p>
<p>Table 7 :
7
MC-NEST evaluation on biomedicine dataset; IS = Importance Sampling; PIS = Pairwise Importance Sampling.ZS with 2.34 and 2-FS with 2.45.GPT-4o also shows significant improvements with ZSCoT, achieving an average score of 2.46 compared to 2.12 for ZS.
LLMSize Rollout SamplingBertScoreNovelty Clarity Significance Verifiability AvgPrecision Recall F1GPT-4o -4Greedy82.6386.16 84.352.702.792.862.932.82GPT-4o -4IS82.5286.24 84.292.642.792.812.922.79GPT-4o -4PIS82.5586.16 84.322.702.762.872.932.82GPT-4o -8Greedy82.5386.11 84.292.672.812.832.952.82GPT-4o -8IS82.0886.04 84.002.772.762.862.972.84 ↑GPT-4o -8PIS82.1786.05 84.062.802.732.892.952.84 ↑Deepseek 32B 4Greedy82.8586.04 84.412.652.902.852.952.84Deepseek 32B 4IS82.2585.91 84.042.702.953.002.852.87 ↑Deepseek 32B 4PIS82.1985.88 83.992.752.752.802.952.81Deepseek 32B 8Greedy82.4785.99 84.192.552.802.952.952.81Deepseek 32B 8IS82.1586.17 84.112.752.902.852.902.85Deepseek 32B 8PIS82.4985.75 84.082.602.602.852.952.75Deepseek 7B 4Greedy82.5985.87 84.192.602.752.752.802.73Deepseek 7B 4IS82.7185.72 84.182.602.802.802.852.76Deepseek 7B 4PIS82.6685.71 84.152.502.752.752.852.71Deepseek 7B 8Greedy82.2585.68 83.922.602.752.802.902.76Deepseek 7B 8IS82.0185.33 83.632.652.752.852.952.80Deepseek 7B 8PIS82.3985.88 84.092.803.002.852.852.88 ↑</p>
<p>Table 4
4
Table 6 summarizes the performance of prompting strategies on the Biomedicine dataset.ZSCoT consistently improves performance across LLMs, with DeepSeek-32B achieving an average score of 2.80, compared to 2.40 with ZS and 2.48 with 2-FS.Qualitative metrics, such as novelty and significance, also show substantial improvements with ZSCoT.For instance, DeepSeek-32B with ZSCoT achieves a novelty score of 2.55 and a significance score of 2.90, compared to 1.95 and 2.65 with ZS, respectively.MC-NEST Sampling Strategies.Table7presents the results of MC-NEST evaluations on the Biomedicine dataset.For GPT-4o, Greedy and Pairwise Importance Sampling perform best at an eight-step rollout, achieving an average score of 2.84.DeepSeek-32B achieves its highest score with Importance Sampling at a four-step rollout with 2.87, while DeepSeek-7B performs best with Pairwise Importance Sampling at eight steps with 2.88.These results demonstrate the importance of adaptive sampling strategies using MC-NEST for optimizing hypothesis generation in biomedicine domains.
, ZSCoT again demonstrates out-</p>
<p>Table 8 :
8
Human evaluation on social science, biomedicine, and computer science dataset; IS = Importance Sampling; PIS = Pairwise Importance Sampling.Human Evaluation and Case Study.The human evaluation results in Table 8 highlight the outstanding performance of MC-NEST with Greedy, Importance Sampling, and Pairwise Importance Sampling strategies compared to other approaches.MC-NEST with ZSCoT prompting achieved the highest average score of 2.62 in Social Science and 2.37 in Computer Science, while Importance Sampling achieved the best performance in Biomedicine with a score of 2.37.Usefulness of the MC-NEST Framework.MC-NEST is a powerful framework for hypothesis generation, combining MCTS with Nash Equilibrium strategies to dynamically balance exploration and exploitation.It iteratively refines hypotheses through self-critique and validation, ensuring novelty and empirical grounding.In experiments, MC-NEST outperformed baselines across multiple domains, achieving higher BertScore and qualitative metrics (novelty, clarity, significance, and verifiability).For example, in optimizing synthetic peptide sequences for nuclear localization and solubility, MC-NEST proposed experimentally validated modifications.Its ability to incorporate emerging scientific literature and adapt to new discoveries distinguishes it from frameworks lacking iterative refinement.These features make MC-NEST a versatile and effective tool for advancing scientific discovery through automated hypothesis generation.Rollout Strategy for MC-NEST Hypothesis Generation.Our experiments demonstrate that longer rollouts consistently enhance the performance of MC-NEST across datasets and sampling strategies.Increasing the rollout length from four to eight steps improves both the BERTScore and qualitative metrics, such as novelty and verifiability.Pairwise Importance Sampling, in particular, benefits from extended rollouts, achieving the highest scores in novelty and significance while maintaining competitive performance in other metrics.These results indicate that longer rollouts enable a more comprehensive exploration of the hypothesis space, leading to higher-quality and more innovative solutions.includingmodeldetails,trainingdata,and frameworks-is essential for fair credit attribution and fostering trust in AI-assisted research.Ethical concerns include misuse, low-quality outputs, and unoriginal hypotheses that could overwhelm academic venues, necessitating rigorous scrutiny to ensure novelty, testability, and grounding in sound principles.In high-stakes domains, proactive measures like Reinforcement Learning from Human Feedback (RLHF) and adversarial robustness are critical to mitigate risks of unethical or harmful research.Additionally, LLMs' tendency to produce hypotheses clustered around common training data patterns risks reducing diversity and novelty, highlighting the need for future work to enhance output diversity through model refinement or frameworks that explicitly encourage unconventional ideas.5ConclusionandLimitationsWeintroducedMC-NEST, a novel framework integrating Monte Carlo Tree Search with Nash Equilibrium strategies to enhance hypothesis generation.MC-NEST outperforms baselines across domains, excelling in quantitative metrics (e.g., BERTScore) and qualitative measures (e.g., novelty, clarity, significance, and verifiability).Adaptive sampling and iterative self-refinement enable MC-NEST to balance exploration and exploitation, generating innovative and empirically grounded hypotheses.Our findings emphasize the value of structured human-AI collaboration, where LLMs augment human creativity rather than replace it.Future work should focus on enhancing diversity and addressing sociotechnical challenges.Limitations include the dataset's focus on computer science papers, though each is curated and annotated by domain experts, ensuring academic rigor.MC-NEST's applicability across diverse domains is a challenge, but it is the first framework to integrate MCTS with LLMs for hypothesis generation in fields like biomedicine, social science, and computer science.While the framework automates hypothesis generation with human-AI collaboration, future work will adapt it to controlled settings by incorporating researcher-defined inputs, ensuring versatility.You are an expert in scientific research.Evaluate the novelty of the following hypothesis based on the given background.The hypothesis is not novel at all.-1:The hypothesis shows slight novelty with minor new insights.-2:Thehypothesis shows moderate novelty by offering some new perspectives.-3:The hypothesis demonstrates strong novelty with significant, original insights beyond the background.At the end of your response, clearly state the score in the format: You are a research expert.Evaluate the clarity and conciseness of the following hypothesis.Score from 0 to 3:
DatasetLLMSizePromptNovelty Clarity Significance Verifiability AvgSocial ScienceGPT-4o-ZSCoT2.333.002.662.492.62Social ScienceGPT-4o-Greedy2.161.662.162.502.12 ↓BiomedicineGPT-4o-ZSCoT1.662.332.501.662.03BiomedicineGPT-4o-IS1.832.502.832.332.37 ↑Computer ScienceGPT-4o-ZSCoT1.662.502.662.662.37Computer ScienceGPT-4o-PIS1.852.502.662.502.38 ↑Social ScienceDeepseek32BZSCoT2.161.832.662.162.20Social ScienceDeepseek32BGreedy2.161.832.492.502.25 ↑BiomedicineDeepseek32BZSCoT2.661.662.662.332.32BiomedicineDeepseek32BIS2.412.172.662.502.44 ↑Computer ScienceDeepseek32BZSCoT2.332.172.662.502.42Computer ScienceDeepseek32BPIS2.502.162.662.172.37 ↓Social ScienceDeepseek7BZSCoT2.331.832.662.332.29Social ScienceDeepseek7BIS1.832.332.662.832.41 ↑BiomedicineDeepseek7B3FS2.162.502.832.662.54BiomedicineDeepseek7BPIS1.672.832.832.332.42 ↓Computer ScienceDeepseek7BZSCoT1.662.502.502.502.29Computer ScienceDeepseek7BIS1.832.502.662.502.37 ↑
AcknowledgementsWe acknowledge the support of the KISSKI project (funding no.01IS22093C) for providing computational resources, which will enable us to extend this research in the future.Author ContributionsGollam Rabby developed the initial idea, designed the experiments, and contributed to the manuscript writing.Diyana Muhammed conducted the experiments.Prasenjit Mitra provided feedback on the initial idea and supported the manuscript writing.Sören Auer contributed to the initial idea and provided support in the manuscript writing.A. AppendixIn the following sections, we report additional details on the following topics:1.All Unique Keys Found in LLM4CSHypoGen Dataset (Section A.1) 2. Prompts in Experiment (Section A.2)The digital object identifier for the paper.TitleThe title of the research paper.Authors_namesNames of the authors of the paper.Authors_orcid ORCID identifiers of the authors.Paper_domainThe domain or field of research the paper belongs to.Research_IdeaThe central idea or motivation behind the research.Problem_StatementThe specific research problem being addressed.HypothesisThe hypothesis formulated in the research.Literature_ReviewSummary of previous research relevant to the study.Abstract A concise summary of the research paper.MethodThe methodology used in the research.Summarized_Method A concise summary of the methodology.ResultsThe Findings of the research study.Summarized_ResultsA brief summary of the results.ConclusionThe final conclusions drawn from the research.Summarized_Conclusion A concise summary of the conclusion.
J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>A survey of monte carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>A note on two problems in connexion with graphs. E W Dijkstra, Edsger Wybe Dijkstra: his life, work, and legacy. 2022</p>
<p>Balancing Exploration and Exploitation: Task-Targeted Exploration for Scientific Decision-Making. G E Flaspohler, 2022Massachusetts Institute of TechnologyPh.D. thesis</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Importin β-type nuclear transport receptors have distinct binding affinities for ran-gtp. S Hahn, G Schlenstedt, Biochemical and Biophysical Research Communications. 40632011</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, nature. 59678732021</p>
<p>Bandit based monte-carlo planning. L Kocsis, C Szepesvári, European conference on machine learning. Springer2006</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>Human-level concept learning through probabilistic program induction. B M Lake, R Salakhutdinov, J B Tenenbaum, Science. 35062662015</p>
<p>Zero-data learning of new tasks. H Larochelle, D Erhan, Y Bengio, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2008</p>
<p>B Qi, K Zhang, K Tian, H Li, Z R Chen, S Zeng, E Hua, H Jinfang, B Zhou, arXiv:2407.08940Large language models as biomedical hypothesis generators: a comprehensive evaluation. 2024arXiv preprint</p>
<p>Fine-tuning and prompt engineering with cognitive knowledge graphs for scholarly knowledge organization. G Rabby, S Auer, J D'souza, A Oelen, arXiv:2409.064332024arXiv preprint</p>
<p>Mc-nest-enhancing mathematical reasoning in large language models with a monte carlo nash equilibrium self-refine tree. G Rabby, F Keya, P Zamil, S Auer, arXiv:2411.156452024arXiv preprint</p>
<p>Evaluation of the performance of gpt-3.5 and gpt-4 on the medical final examination. M Rosoł, J S Gąsior, J Łaba, K Korzeniewski, M Młyńczak, 2023</p>
<p>C Si, D Yang, T Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 62079722023</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. G Xiong, E Xie, A H Shariatmadari, S Guo, S Bekiranov, A Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, 2023</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Hypothesis generation with large language models. Y Zhou, H Liu, T Srivastava, H Mei, C Tan, 10.18653/v1/2024.nlp4science-1.10Proceedings of the 1st Workshop on NLP for Science (NLP4Science). L Peled-Cohen, N Calderon, S Lissak, R Reichart, the 1st Workshop on NLP for Science (NLP4Science)Miami, FL, USAAssociation for Computational LinguisticsNov 2024</p>            </div>
        </div>

    </div>
</body>
</html>
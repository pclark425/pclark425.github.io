<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264146296</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.09430v4.pdf" target="_blank">Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed. To comprehensively evaluate this ability, we develop three new logical reasoning datasets named"ReClor-plus","LogiQA-plus"and"LogiQAv2-plus"that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning. For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by"none of the other options is correct", and the third with a combination of shuffling and substitution. Experiments on these datasets show that these simple augmentations greatly hinder the models' performance. Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets. We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets. Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning. Source code and data are made publicly available at https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer-based generative language model from OpenAI, evaluated in this paper in zero-shot and prompting settings on multiple-choice logical reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer trained for next-token prediction (details proprietary to OpenAI); used in zero-shot, Chain-of-Thought (CoT) prompting, instruction prompting, and prompt-augmentation experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (and the paper's perturbed variants ReClor-plus, LogiQA-plus, LogiQAv2-plus)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension logical-reasoning benchmarks derived from exams (GMAT, LSAT, civil service) requiring deduction and elimination of incorrect alternatives; paper introduces structural perturbations (Shuffle-Order, Replace-Answer, Shuffle-RepAns) that preserve semantics but alter task structure.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation; Chain-of-Thought (CoT) prompting; instruction prompting; prompt augmentation with logic-driven AMR-LDA extensions applied to test prompts (AMR-LDA extended options/context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On the original ReClor validation set GPT-4 achieved ~88.8% accuracy (Table 8). On the perturbed formats GPT-4 dropped drastically on Replace-Answer and Shuffle-RepAns (e.g., ReClor Replace-Answer 17.6%, Shuffle-RepAns 14.4% in Table 8). With AMR-LDA prompt augmentation (augmenting test prompt/options), GPT-4's accuracy increased substantially on perturbed tasks (reported IPT-LDA: ReClor 87.66%, LogiQA 55.10%, LogiQAv2 70.27% in paper's prompt-augmentation experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline (zero-shot without special prompting) was high on original datasets (GPT-4 highest among evaluated models) but low on perturbed variants; exact baseline numbers for original tasks are reported (e.g., ReClor original ~88.8%) while perturbed baseline performance dropped to near random (e.g., Shuffle-RepAns ~14.4% for ReClor).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>CoT provided only modest or inconsistent benefit; instruction prompting plus AMR-LDA prompt augmentation on the test/prompts produced large improvements on perturbed formats (e.g., raising accuracy on perturbed formats from near-random to substantially higher levels for GPT-4 when AMR-LDA prompt-augmentation was used).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Severe failures on task-structure perturbations that preserve semantics: models often rely on positional/memoized cues and option-surface similarity; GPT-4's accuracy collapsed on Replace-Answer and Shuffle-RepAns unless augmented prompts were provided. Chain-of-Thought prompting rarely fixed the failures (helpful in some case studies but not systematically).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablation observations: (1) CoT prompting rarely improved overall perturbed-set performance except in limited cases. (2) Prompt augmentation with AMR-LDA (augmenting test inputs) produced large improvements, indicating extra logically-equivalent context helps generative next-token models. (3) Model scale unspecified; improvements depend more on input augmentation and instruction fine-tuning than raw model size (per authors' analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI autoregressive next-token prediction model (GPT-3.5 series) evaluated in zero-shot, CoT, instruction prompting, and AMR-LDA prompt-augmentation settings in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer trained for next-token prediction; used as a generative baseline for logical multiple-choice reasoning tasks in zero-shot and prompted settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (and perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice reading-comprehension logical reasoning datasets; requires identifying the uniquely entailed option from four choices given a context and question.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot, Chain-of-Thought prompting, instruction prompting, and prompt augmentation with AMR-LDA (augmented options/context appended to prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot performance was substantially lower than GPT-4 on original datasets and dropped on perturbed variants. With AMR-LDA prompt augmentation on the test set, GPT-3.5 showed noticeable gains (reported as GPT-3.5-IPT-LDA: e.g., ReClor ~36.67%, LogiQA ~46.85%, LogiQAv2 ~49.71% in the paper's prompt-augmentation table).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot was moderate on original datasets but poor on perturbed variants (paper reports generative smaller models underperforming GPT-4; exact baseline numbers vary per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>AMR-LDA prompt augmentation (applied to test prompts) provided substantial improvements over zero-shot baselines for GPT-3.5; CoT prompting did not consistently help.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Like GPT-4, GPT-3.5 struggled with Replace-Answer and Shuffle-RepAns formats; logic-driven data augmentation added to training was detrimental for next-token models (disrupts next-token distribution), although applying augmentation to prompts/tests helped.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors observed that applying AMR-LDA augmentation to the test prompt (not training) helped generative models extract useful logical equivalences. Adding AMR-LDA augmented data to training harmed next-token-trained models' generalisation (distribution mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-7B (IFT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (LLaMA-based) 7B, instruction fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter LLaMA-derived open-source model instruction-fine-tuned (Alpaca) and further fine-tuned in this paper for the logical-reasoning reading-comprehension tasks; evaluated in zero-shot, instruction fine-tuned (IFT), and AMR-LDA augmentation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (fine-tuned in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter LLaMA-derived decoder-only transformer checkpoint (Alpaca recipe) used; instruction fine-tuned on ReClor/LogiQA/LogiQAv2 and variants with different perturbation ratios; hyperparameters follow Stanford Alpaca2 defaults.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 and perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning requiring elimination/deduction; paper introduces task-structure perturbations to test robustness beyond memorised label positions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction fine-tuning (IFT) on original and perturbed training sets; logic-driven data augmentation via AMR-LDA applied to training and/or prompts; transfer learning experiments varying perturbation ratio in training (Shuffle-RepAns proportions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot Alpaca-7B was very poor; after instruction fine-tuning performance improved markedly (e.g., Alpaca-7B-IFT-LDA prompt-augmented result reported as ReClor 48.00%, LogiQA 36.86%, LogiQAv2 22.37% in prompt-augmentation table). Performance depended strongly on training set size and perturbation ratio (e.g., with LogiQAv2 >10k samples, 50% perturbation improved performance across formats).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot Alpaca-7B was near or below random performance on perturbed variants; instruction fine-tuning provided improvement over that baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Instruction fine-tuning yielded substantial gains from near-zero baseline; combining instruction fine-tuning with AMR-LDA augmentation (applied to training and/or prompts) sometimes produced large improvements (notably when augmentation was used both in fine-tuning and prompt formation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Alpaca-7B required large amounts of perturbed training data (>10k) to benefit from high perturbation ratios; small fine-tuning sets did not show the same gains. Adding AMR-LDA augmented data to the training set could hurt performance depending on distribution mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports transfer-learning ablations varying the perturbation ratio in the fine-tuning set: high perturbation ratios helped when the fine-tuning set was large (LogiQAv2 >10k), but harmed performance on some perturbed formats (e.g., Shuffle-Order) for smaller datasets (possible overfitting to altered structure). Also, instruction fine-tuning alone improved permutation invariance for discriminative models but effects for generative models depended on augmentation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3425.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (LLaMA-derived) 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7B instruction-following model (Vicuna) evaluated in zero-shot and with Chain-of-Thought prompting; generally underperformed GPT-family models on logical reasoning tasks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter LLaMA-derived decoder-only model fine-tuned for chat/instruction following; evaluated in zero-shot and with CoT prompting on ReClor/LogiQA/LogiQAv2 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (plus perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reading-comprehension tasks requiring reasoning beyond surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation and Chain-of-Thought prompting; instruction fine-tuning attempted as in Alpaca experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot Vicuna-7B was below GPT-3.5/GPT-4 and in some perturbed formats performed at or below chance; CoT prompting produced inconsistent effects (small gains in some cases, not systematic). Specific numeric table entries show low accuracies compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot performance poor compared to GPT-3.5/GPT-4 and to fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Instruction fine-tuning and AMR-LDA prompt-augmentation improved performance in some settings, but Vicuna-7B did not achieve parity with larger generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggled significantly on Replace-Answer and Shuffle-RepAns; CoT prompting did not reliably fix failures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper notes variability: when initial performance was poor, fine-tuning sometimes improved shuffled datasets but not consistently across tasks; suggests overfitting and lack of robust logical generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3425.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LReasoner (ALBERT-XXLarge-v2 based discriminative model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative encoder-only transformer model fine-tuned for multi-choice logical reasoning (ReClor/LogiQA), using logic-driven augmentation in prior work and evaluated here for robustness to task-structure perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LReasoner (ALBERT-XXLarge-v2 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer (ALBERT-XXLarge-v2) fine-tuned for classification on multiple-choice logical reasoning; in prior work used logic-driven augmentation; here evaluated as a fine-tuned discriminative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (plus perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-choice reading-comprehension logical reasoning requiring selection of the logically entailed option; discriminative models output a label given concatenated context+question+option inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Instruction fine-tuning for discriminative models (classification), logic-driven data augmentation (AMR-LDA style) applied in training in prior work; evaluated for permutation invariance and robustness to Replace-Answer / Shuffle variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuned discriminative models (including LReasoner) achieved strong performance on original datasets and showed stable performance on Shuffle-Order (permutation-invariance due to input concatenation), but suffered significant drops on Replace-Answer and Shuffle-RepAns sets. LReasoner historically achieved top leaderboard results on ReClor in prior work (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>As a fine-tuned discriminative model baseline, LReasoner performed well on original formats and better than unfine-tuned generative smaller models; exact numeric leaderboard-level performance is reported in its original work but in this paper discriminative models approach or exceed GPT-4 on merged training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Logic-driven data augmentation improved discriminative models' performance and robustness in previous work and contributed to permutation invariance; in this paper discriminative models remain more stable on Shuffle-Order but still fail severely on Replace-Answer perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Significant failure on Replace-Answer and Shuffle-RepAns perturbations indicates brittleness to task-structure changes despite permutation invariance; discriminative models did not outperform generative ones on those perturbed formats.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors analyze that discriminative training format (Context + [SEP] + Question + [SEP] + Option per candidate) produces permutation invariance for Shuffle-Order, explaining stable Shuffle-Order results; however Replace-Answer variants reveal models may rely on surface matching rather than deep logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3425.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt (DeBERTaV2-XXLarge based discriminative model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discriminative logical-reasoning model using relation-extraction and intermediate training data; used as a high-performing encoder-only baseline in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt (DeBERTaV2-XXLarge backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer model fine-tuned for multi-choice logical reasoning; leverages relation extraction and intermediate-task training to boost performance (as described in MERIt prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (plus perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning reading-comprehension requiring deduction and elimination across options.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning on original datasets; evaluated for robustness to Shuffle-Order, Replace-Answer, Shuffle-RepAns; compared with logic-driven augmentation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MERIt exhibited commendable performance on original tasks and, in some merged-training settings, outperformed GPT-4 (paper reports MERIt and AMR-LDA surpassing GPT-4 when training sets were merged), but it also experienced significant declines on Replace-Answer and Shuffle-RepAns.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>As a fine-tuned discriminative model baseline, MERIt is strong on original benchmarks (leaderboard results reported in its original paper); compared to generative models in this study it performed competitively or better when fine-tuned on merged training sets.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>MERIt's prior-specified interventions (relation-extraction intermediate training) contributed to high baseline performance; in this paper, discriminative fine-tuning generally improved over zero-shot generative baselines for original tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same brittle behaviour on Replace-Answer and Shuffle-RepAns indicates sensitivity to task-structure changes; shows that strong original-task performance does not equate to robust generalized logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper highlights that discriminative models' input formatting yields permutation invariance for Shuffle-Order, explaining stable performance there, but the Replace-Answer perturbation exposes reliance on surface-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3425.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR-LDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-based Logic-Driven Data Augmentation (AMR-LDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A logic-driven data augmentation technique that uses Abstract Meaning Representation (AMR) and logical equivalence laws to produce extended options and contexts to improve logical reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AMR-LDA (augmentation method / model pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>AMR-based pipeline that constructs logically equivalent or extended sentences from source context/options using logical equivalence laws (e.g., contraposition, implication, commutativity) to augment training or prompt data; used both as a discriminative-model training augmentation and as prompt/test augmentation for generative models in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor, LogiQA, LogiQAv2 (and perturbed variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to generate logically-equivalent/extended options and contexts that preserve semantics while changing surface form; intended to help models learn logical transformations and equivalences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Logic-driven data augmentation applied to training sets (for discriminative models) and applied to prompts/test inputs (for generative models) in two modes: (a) add augmented data to training; (b) augment prompts/options at inference time (prompt augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>For discriminative models, AMR-LDA training augmentation improved performance and robustness historically (AMR-LDA ranked highly on ReClor leaderboard). For generative LLMs, adding AMR-LDA augmented data into training harmed performance for next-token models (distribution mismatch), but applying AMR-LDA augmentation to prompts/tests improved generative model performance substantially (paper reports e.g., GPT-4-IPT-LDA: ReClor ~87.66%, etc.; Alpaca-7B-IFT-LDA: ReClor 48.00%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline without AMR-LDA: discriminative models performed well on original datasets but poorly on some perturbed formats; with AMR-LDA training augmentation discriminative models improved. For generative models, baseline (no AMR-LDA) was sometimes better than training-augmented versions.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>AMR-LDA training augmentation improved discriminative models; AMR-LDA prompt-augmentation (applied to test prompt) improved generative models' perturbed-format performance compared to their unaugmented zero-shot baselines. Conversely, injecting AMR-LDA augmented data into training of next-token models tended to degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>AMR-LDA training augmentation can harm next-token generative model fine-tuning (distribution mismatch); logic-driven augmentation must be applied carefully (train vs test) depending on model class. Augmentation that changes training distribution relative to test can reduce generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper includes experiments showing (1) training-set AMR-LDA augmentation improves discriminative models but harms generative next-token fine-tuning; (2) applying augmentation only to prompts or only to tests can benefit generative models (two effective methods identified); (3) distribution mismatch is hypothesized cause of harm when augmenting training data for next-token models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3425.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3425.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA size sweep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA family (7B to 65B evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of LLaMA-family models (same base architecture) evaluated at multiple parameter scales (7B up to 65B) to study the effect of model size on logical reasoning robustness under zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (various sizes: 7B, 13B, 33B, 65B as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer checkpoints from the LLaMA family at multiple parameter sizes; evaluated in zero-shot on Shuffle-RepAns perturbed sets to assess size vs robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B65B (evaluated range reported in paper: LLaMA-7B to LLaMA-65B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Shuffle-RepAns variants (perturbed ReClor/LogiQA/LogiQAv2)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Perturbed multiple-choice tasks where the correct option is replaced by 'none of the other options is correct' and options are shuffleddesigned to reveal non-robust pattern-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot evaluation across model sizes (no instruction fine-tuning applied for this ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report no significant performance improvement on Shuffle-RepAns format as model size increases from 7B to 65B under zero-shot; i.e., larger LLaMA models did not show better generalisation/robustness on this strict logical perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot baseline performance across sizes was uniformly poor on Shuffle-RepAns; increasing parameter count did not systematically increase accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No systematic improvement with scale in zero-shot setting for perturbed logical tasks; size alone did not rescue robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Model size (within same base architecture) is not a reliable predictor of generalisation to task-structure perturbations that require strict logical reasoning; suggests capabilities depend on training data and fine-tuning rather than parameter count alone.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>A controlled sweep across LLaMA sizes shows near-constant accuracy on Shuffle-RepAns under zero-shot, supporting the conclusion that scale alone does not confer robustness to these structural perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Enhancing logical reasoning of large language models through logic-driven data augmentation <em>(Rating: 2)</em></li>
                <li>Logic-driven context extension and data augmentation for logical reasoning of text <em>(Rating: 2)</em></li>
                <li>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Transformers as soft reasoners over language <em>(Rating: 1)</em></li>
                <li>RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3425",
    "paper_id": "paper-264146296",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large autoregressive transformer-based generative language model from OpenAI, evaluated in this paper in zero-shot and prompting settings on multiple-choice logical reasoning benchmarks.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Decoder-only transformer trained for next-token prediction (details proprietary to OpenAI); used in zero-shot, Chain-of-Thought (CoT) prompting, instruction prompting, and prompt-augmentation experiments in this paper.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (and the paper's perturbed variants ReClor-plus, LogiQA-plus, LogiQAv2-plus)",
            "reasoning_task_description": "Multiple-choice reading-comprehension logical-reasoning benchmarks derived from exams (GMAT, LSAT, civil service) requiring deduction and elimination of incorrect alternatives; paper introduces structural perturbations (Shuffle-Order, Replace-Answer, Shuffle-RepAns) that preserve semantics but alter task structure.",
            "method_or_intervention": "Zero-shot evaluation; Chain-of-Thought (CoT) prompting; instruction prompting; prompt augmentation with logic-driven AMR-LDA extensions applied to test prompts (AMR-LDA extended options/context).",
            "performance": "On the original ReClor validation set GPT-4 achieved ~88.8% accuracy (Table 8). On the perturbed formats GPT-4 dropped drastically on Replace-Answer and Shuffle-RepAns (e.g., ReClor Replace-Answer 17.6%, Shuffle-RepAns 14.4% in Table 8). With AMR-LDA prompt augmentation (augmenting test prompt/options), GPT-4's accuracy increased substantially on perturbed tasks (reported IPT-LDA: ReClor 87.66%, LogiQA 55.10%, LogiQAv2 70.27% in paper's prompt-augmentation experiments).",
            "baseline_performance": "Baseline (zero-shot without special prompting) was high on original datasets (GPT-4 highest among evaluated models) but low on perturbed variants; exact baseline numbers for original tasks are reported (e.g., ReClor original ~88.8%) while perturbed baseline performance dropped to near random (e.g., Shuffle-RepAns ~14.4% for ReClor).",
            "improvement_over_baseline": "CoT provided only modest or inconsistent benefit; instruction prompting plus AMR-LDA prompt augmentation on the test/prompts produced large improvements on perturbed formats (e.g., raising accuracy on perturbed formats from near-random to substantially higher levels for GPT-4 when AMR-LDA prompt-augmentation was used).",
            "limitations_or_failures": "Severe failures on task-structure perturbations that preserve semantics: models often rely on positional/memoized cues and option-surface similarity; GPT-4's accuracy collapsed on Replace-Answer and Shuffle-RepAns unless augmented prompts were provided. Chain-of-Thought prompting rarely fixed the failures (helpful in some case studies but not systematically).",
            "ablation_or_analysis": "Ablation observations: (1) CoT prompting rarely improved overall perturbed-set performance except in limited cases. (2) Prompt augmentation with AMR-LDA (augmenting test inputs) produced large improvements, indicating extra logically-equivalent context helps generative next-token models. (3) Model scale unspecified; improvements depend more on input augmentation and instruction fine-tuning than raw model size (per authors' analyses).",
            "uuid": "e3425.0",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-Turbo (OpenAI)",
            "brief_description": "An OpenAI autoregressive next-token prediction model (GPT-3.5 series) evaluated in zero-shot, CoT, instruction prompting, and AMR-LDA prompt-augmentation settings in the paper.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Decoder-only transformer trained for next-token prediction; used as a generative baseline for logical multiple-choice reasoning tasks in zero-shot and prompted settings.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (and perturbed variants)",
            "reasoning_task_description": "Multiple-choice reading-comprehension logical reasoning datasets; requires identifying the uniquely entailed option from four choices given a context and question.",
            "method_or_intervention": "Zero-shot, Chain-of-Thought prompting, instruction prompting, and prompt augmentation with AMR-LDA (augmented options/context appended to prompts).",
            "performance": "Zero-shot performance was substantially lower than GPT-4 on original datasets and dropped on perturbed variants. With AMR-LDA prompt augmentation on the test set, GPT-3.5 showed noticeable gains (reported as GPT-3.5-IPT-LDA: e.g., ReClor ~36.67%, LogiQA ~46.85%, LogiQAv2 ~49.71% in the paper's prompt-augmentation table).",
            "baseline_performance": "Baseline zero-shot was moderate on original datasets but poor on perturbed variants (paper reports generative smaller models underperforming GPT-4; exact baseline numbers vary per dataset).",
            "improvement_over_baseline": "AMR-LDA prompt augmentation (applied to test prompts) provided substantial improvements over zero-shot baselines for GPT-3.5; CoT prompting did not consistently help.",
            "limitations_or_failures": "Like GPT-4, GPT-3.5 struggled with Replace-Answer and Shuffle-RepAns formats; logic-driven data augmentation added to training was detrimental for next-token models (disrupts next-token distribution), although applying augmentation to prompts/tests helped.",
            "ablation_or_analysis": "Authors observed that applying AMR-LDA augmentation to the test prompt (not training) helped generative models extract useful logical equivalences. Adding AMR-LDA augmented data to training harmed next-token-trained models' generalisation (distribution mismatch).",
            "uuid": "e3425.1",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Alpaca-7B (IFT)",
            "name_full": "Alpaca (LLaMA-based) 7B, instruction fine-tuned",
            "brief_description": "A 7B-parameter LLaMA-derived open-source model instruction-fine-tuned (Alpaca) and further fine-tuned in this paper for the logical-reasoning reading-comprehension tasks; evaluated in zero-shot, instruction fine-tuned (IFT), and AMR-LDA augmentation settings.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (fine-tuned in paper)",
            "model_description": "7B-parameter LLaMA-derived decoder-only transformer checkpoint (Alpaca recipe) used; instruction fine-tuned on ReClor/LogiQA/LogiQAv2 and variants with different perturbation ratios; hyperparameters follow Stanford Alpaca2 defaults.",
            "model_size": "7B",
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 and perturbed variants (Shuffle-Order, Replace-Answer, Shuffle-RepAns)",
            "reasoning_task_description": "Multiple-choice logical reasoning requiring elimination/deduction; paper introduces task-structure perturbations to test robustness beyond memorised label positions.",
            "method_or_intervention": "Instruction fine-tuning (IFT) on original and perturbed training sets; logic-driven data augmentation via AMR-LDA applied to training and/or prompts; transfer learning experiments varying perturbation ratio in training (Shuffle-RepAns proportions).",
            "performance": "Zero-shot Alpaca-7B was very poor; after instruction fine-tuning performance improved markedly (e.g., Alpaca-7B-IFT-LDA prompt-augmented result reported as ReClor 48.00%, LogiQA 36.86%, LogiQAv2 22.37% in prompt-augmentation table). Performance depended strongly on training set size and perturbation ratio (e.g., with LogiQAv2 &gt;10k samples, 50% perturbation improved performance across formats).",
            "baseline_performance": "Baseline zero-shot Alpaca-7B was near or below random performance on perturbed variants; instruction fine-tuning provided improvement over that baseline.",
            "improvement_over_baseline": "Instruction fine-tuning yielded substantial gains from near-zero baseline; combining instruction fine-tuning with AMR-LDA augmentation (applied to training and/or prompts) sometimes produced large improvements (notably when augmentation was used both in fine-tuning and prompt formation).",
            "limitations_or_failures": "Alpaca-7B required large amounts of perturbed training data (&gt;10k) to benefit from high perturbation ratios; small fine-tuning sets did not show the same gains. Adding AMR-LDA augmented data to the training set could hurt performance depending on distribution mismatch.",
            "ablation_or_analysis": "Paper reports transfer-learning ablations varying the perturbation ratio in the fine-tuning set: high perturbation ratios helped when the fine-tuning set was large (LogiQAv2 &gt;10k), but harmed performance on some perturbed formats (e.g., Shuffle-Order) for smaller datasets (possible overfitting to altered structure). Also, instruction fine-tuning alone improved permutation invariance for discriminative models but effects for generative models depended on augmentation strategy.",
            "uuid": "e3425.2",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Vicuna-7B",
            "name_full": "Vicuna (LLaMA-derived) 7B",
            "brief_description": "An open-source 7B instruction-following model (Vicuna) evaluated in zero-shot and with Chain-of-Thought prompting; generally underperformed GPT-family models on logical reasoning tasks in this paper.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "7B-parameter LLaMA-derived decoder-only model fine-tuned for chat/instruction following; evaluated in zero-shot and with CoT prompting on ReClor/LogiQA/LogiQAv2 variants.",
            "model_size": "7B",
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (plus perturbed variants)",
            "reasoning_task_description": "Multiple-choice logical reading-comprehension tasks requiring reasoning beyond surface cues.",
            "method_or_intervention": "Zero-shot evaluation and Chain-of-Thought prompting; instruction fine-tuning attempted as in Alpaca experiments.",
            "performance": "Zero-shot Vicuna-7B was below GPT-3.5/GPT-4 and in some perturbed formats performed at or below chance; CoT prompting produced inconsistent effects (small gains in some cases, not systematic). Specific numeric table entries show low accuracies compared to GPT-4.",
            "baseline_performance": "Baseline zero-shot performance poor compared to GPT-3.5/GPT-4 and to fine-tuned models.",
            "improvement_over_baseline": "Instruction fine-tuning and AMR-LDA prompt-augmentation improved performance in some settings, but Vicuna-7B did not achieve parity with larger generative models.",
            "limitations_or_failures": "Struggled significantly on Replace-Answer and Shuffle-RepAns; CoT prompting did not reliably fix failures.",
            "ablation_or_analysis": "Paper notes variability: when initial performance was poor, fine-tuning sometimes improved shuffled datasets but not consistently across tasks; suggests overfitting and lack of robust logical generalisation.",
            "uuid": "e3425.3",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LReasoner",
            "name_full": "LReasoner (ALBERT-XXLarge-v2 based discriminative model)",
            "brief_description": "A discriminative encoder-only transformer model fine-tuned for multi-choice logical reasoning (ReClor/LogiQA), using logic-driven augmentation in prior work and evaluated here for robustness to task-structure perturbations.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "LReasoner (ALBERT-XXLarge-v2 backbone)",
            "model_description": "Encoder-only transformer (ALBERT-XXLarge-v2) fine-tuned for classification on multiple-choice logical reasoning; in prior work used logic-driven augmentation; here evaluated as a fine-tuned discriminative baseline.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (plus perturbed variants)",
            "reasoning_task_description": "Multi-choice reading-comprehension logical reasoning requiring selection of the logically entailed option; discriminative models output a label given concatenated context+question+option inputs.",
            "method_or_intervention": "Instruction fine-tuning for discriminative models (classification), logic-driven data augmentation (AMR-LDA style) applied in training in prior work; evaluated for permutation invariance and robustness to Replace-Answer / Shuffle variants.",
            "performance": "Fine-tuned discriminative models (including LReasoner) achieved strong performance on original datasets and showed stable performance on Shuffle-Order (permutation-invariance due to input concatenation), but suffered significant drops on Replace-Answer and Shuffle-RepAns sets. LReasoner historically achieved top leaderboard results on ReClor in prior work (cited).",
            "baseline_performance": "As a fine-tuned discriminative model baseline, LReasoner performed well on original formats and better than unfine-tuned generative smaller models; exact numeric leaderboard-level performance is reported in its original work but in this paper discriminative models approach or exceed GPT-4 on merged training sets.",
            "improvement_over_baseline": "Logic-driven data augmentation improved discriminative models' performance and robustness in previous work and contributed to permutation invariance; in this paper discriminative models remain more stable on Shuffle-Order but still fail severely on Replace-Answer perturbations.",
            "limitations_or_failures": "Significant failure on Replace-Answer and Shuffle-RepAns perturbations indicates brittleness to task-structure changes despite permutation invariance; discriminative models did not outperform generative ones on those perturbed formats.",
            "ablation_or_analysis": "Authors analyze that discriminative training format (Context + [SEP] + Question + [SEP] + Option per candidate) produces permutation invariance for Shuffle-Order, explaining stable Shuffle-Order results; however Replace-Answer variants reveal models may rely on surface matching rather than deep logical reasoning.",
            "uuid": "e3425.4",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MERIt",
            "name_full": "MERIt (DeBERTaV2-XXLarge based discriminative model)",
            "brief_description": "A discriminative logical-reasoning model using relation-extraction and intermediate training data; used as a high-performing encoder-only baseline in this paper's experiments.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "MERIt (DeBERTaV2-XXLarge backbone)",
            "model_description": "Encoder-only transformer model fine-tuned for multi-choice logical reasoning; leverages relation extraction and intermediate-task training to boost performance (as described in MERIt prior work).",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (plus perturbed variants)",
            "reasoning_task_description": "Multiple-choice logical reasoning reading-comprehension requiring deduction and elimination across options.",
            "method_or_intervention": "Fine-tuning on original datasets; evaluated for robustness to Shuffle-Order, Replace-Answer, Shuffle-RepAns; compared with logic-driven augmentation approaches.",
            "performance": "MERIt exhibited commendable performance on original tasks and, in some merged-training settings, outperformed GPT-4 (paper reports MERIt and AMR-LDA surpassing GPT-4 when training sets were merged), but it also experienced significant declines on Replace-Answer and Shuffle-RepAns.",
            "baseline_performance": "As a fine-tuned discriminative model baseline, MERIt is strong on original benchmarks (leaderboard results reported in its original paper); compared to generative models in this study it performed competitively or better when fine-tuned on merged training sets.",
            "improvement_over_baseline": "MERIt's prior-specified interventions (relation-extraction intermediate training) contributed to high baseline performance; in this paper, discriminative fine-tuning generally improved over zero-shot generative baselines for original tasks.",
            "limitations_or_failures": "Same brittle behaviour on Replace-Answer and Shuffle-RepAns indicates sensitivity to task-structure changes; shows that strong original-task performance does not equate to robust generalized logical reasoning.",
            "ablation_or_analysis": "Paper highlights that discriminative models' input formatting yields permutation invariance for Shuffle-Order, explaining stable performance there, but the Replace-Answer perturbation exposes reliance on surface-matching.",
            "uuid": "e3425.5",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "AMR-LDA",
            "name_full": "AMR-based Logic-Driven Data Augmentation (AMR-LDA)",
            "brief_description": "A logic-driven data augmentation technique that uses Abstract Meaning Representation (AMR) and logical equivalence laws to produce extended options and contexts to improve logical reasoning models.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "AMR-LDA (augmentation method / model pipeline)",
            "model_description": "AMR-based pipeline that constructs logically equivalent or extended sentences from source context/options using logical equivalence laws (e.g., contraposition, implication, commutativity) to augment training or prompt data; used both as a discriminative-model training augmentation and as prompt/test augmentation for generative models in this paper.",
            "model_size": null,
            "reasoning_task_name": "ReClor, LogiQA, LogiQAv2 (and perturbed variants)",
            "reasoning_task_description": "Used to generate logically-equivalent/extended options and contexts that preserve semantics while changing surface form; intended to help models learn logical transformations and equivalences.",
            "method_or_intervention": "Logic-driven data augmentation applied to training sets (for discriminative models) and applied to prompts/test inputs (for generative models) in two modes: (a) add augmented data to training; (b) augment prompts/options at inference time (prompt augmentation).",
            "performance": "For discriminative models, AMR-LDA training augmentation improved performance and robustness historically (AMR-LDA ranked highly on ReClor leaderboard). For generative LLMs, adding AMR-LDA augmented data into training harmed performance for next-token models (distribution mismatch), but applying AMR-LDA augmentation to prompts/tests improved generative model performance substantially (paper reports e.g., GPT-4-IPT-LDA: ReClor ~87.66%, etc.; Alpaca-7B-IFT-LDA: ReClor 48.00%).",
            "baseline_performance": "Baseline without AMR-LDA: discriminative models performed well on original datasets but poorly on some perturbed formats; with AMR-LDA training augmentation discriminative models improved. For generative models, baseline (no AMR-LDA) was sometimes better than training-augmented versions.",
            "improvement_over_baseline": "AMR-LDA training augmentation improved discriminative models; AMR-LDA prompt-augmentation (applied to test prompt) improved generative models' perturbed-format performance compared to their unaugmented zero-shot baselines. Conversely, injecting AMR-LDA augmented data into training of next-token models tended to degrade performance.",
            "limitations_or_failures": "AMR-LDA training augmentation can harm next-token generative model fine-tuning (distribution mismatch); logic-driven augmentation must be applied carefully (train vs test) depending on model class. Augmentation that changes training distribution relative to test can reduce generalisation.",
            "ablation_or_analysis": "Paper includes experiments showing (1) training-set AMR-LDA augmentation improves discriminative models but harms generative next-token fine-tuning; (2) applying augmentation only to prompts or only to tests can benefit generative models (two effective methods identified); (3) distribution mismatch is hypothesized cause of harm when augmenting training data for next-token models.",
            "uuid": "e3425.6",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA size sweep",
            "name_full": "LLaMA family (7B to 65B evaluated)",
            "brief_description": "A set of LLaMA-family models (same base architecture) evaluated at multiple parameter scales (7B up to 65B) to study the effect of model size on logical reasoning robustness under zero-shot evaluation.",
            "citation_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "mention_or_use": "use",
            "model_name": "LLaMA (various sizes: 7B, 13B, 33B, 65B as used in experiments)",
            "model_description": "Decoder-only transformer checkpoints from the LLaMA family at multiple parameter sizes; evaluated in zero-shot on Shuffle-RepAns perturbed sets to assess size vs robustness.",
            "model_size": "7B65B (evaluated range reported in paper: LLaMA-7B to LLaMA-65B)",
            "reasoning_task_name": "Shuffle-RepAns variants (perturbed ReClor/LogiQA/LogiQAv2)",
            "reasoning_task_description": "Perturbed multiple-choice tasks where the correct option is replaced by 'none of the other options is correct' and options are shuffleddesigned to reveal non-robust pattern-matching.",
            "method_or_intervention": "Zero-shot evaluation across model sizes (no instruction fine-tuning applied for this ablation).",
            "performance": "Authors report no significant performance improvement on Shuffle-RepAns format as model size increases from 7B to 65B under zero-shot; i.e., larger LLaMA models did not show better generalisation/robustness on this strict logical perturbation.",
            "baseline_performance": "Zero-shot baseline performance across sizes was uniformly poor on Shuffle-RepAns; increasing parameter count did not systematically increase accuracy.",
            "improvement_over_baseline": "No systematic improvement with scale in zero-shot setting for perturbed logical tasks; size alone did not rescue robustness.",
            "limitations_or_failures": "Model size (within same base architecture) is not a reliable predictor of generalisation to task-structure perturbations that require strict logical reasoning; suggests capabilities depend on training data and fine-tuning rather than parameter count alone.",
            "ablation_or_analysis": "A controlled sweep across LLaMA sizes shows near-constant accuracy on Shuffle-RepAns under zero-shot, supporting the conclusion that scale alone does not confer robustness to these structural perturbations.",
            "uuid": "e3425.7",
            "source_info": {
                "paper_title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Enhancing logical reasoning of large language models through logic-driven data augmentation",
            "rating": 2,
            "sanitized_title": "enhancing_logical_reasoning_of_large_language_models_through_logicdriven_data_augmentation"
        },
        {
            "paper_title": "Logic-driven context extension and data augmentation for logical reasoning of text",
            "rating": 2,
            "sanitized_title": "logicdriven_context_extension_and_data_augmentation_for_logical_reasoning_of_text"
        },
        {
            "paper_title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning",
            "rating": 2,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 1,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "RobustLR: A diagnostic benchmark for evaluating logical robustness of deductive reasoners",
            "rating": 1,
            "sanitized_title": "robustlr_a_diagnostic_benchmark_for_evaluating_logical_robustness_of_deductive_reasoners"
        }
    ],
    "cost": 0.0186415,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 2024</p>
<p>Qiming Bao 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>XtractaNew Zealand</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Gal Gendron 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Alex Yuxuan Peng 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Wanjun Zhong 
School of Computer Science and Engineering
Sun Yat-Sen University</p>
<p>Neset Tan 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Yang Chen 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Michael Witbrock 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Jiamou Liu 
Strong AI Lab
NAOInstitute
Waipapa Taumata Rau</p>
<p>The University of Auckland</p>
<p>https://github.com/Strong-AI
Lab/Logical-and-abstract-reasoning</p>
<p>Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning
30 Mar 202431B5D715A00C54C336EC05DCCC43D395arXiv:2310.09430v4[cs.CL]
Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels.However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed.To comprehensively evaluate this ability, we develop three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus" that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning.For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options is correct", and the third with a combination of shuffling and substitution.Experiments on these datasets show that these simple augmentations greatly hinder the models' performance.Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets.We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets.Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning.Source code and data are made publicly available at 1 .</p>
<p>Introduction</p>
<p>By leveraging the vast amounts of data available on the internet, large language models have achieved great performance on various tasks Brown et al. (2020); Wei et al. (2022b).However, training data that is confined to a single task or structure may lead to overfitting to specific tasks, consequently diminishing the model's generalisation capabilities.Specifically, there is a dearth of both quantity and quality of data on logical reasoning available on the internet Liu et al. (2023a).This scarcity results in limitations for large language models when tackling complex logical reasoning tasks Wang et al. (2022).Consequently, ensuring high quality and diversity in training data becomes essential, as it can significantly aid large language models in enhancing their robustness on reasoning tasks.Current logical reasoning datasets do not truly represent the reasoning abilities of large language models, as making small modifications significantly degrades performance.Many logical reasoning datasets are designed to select the precise correct option, models might adapt to choose answers that merely resemble the correct one.Moreover, since numerous public logical reasoning datasets were published prior to the training of these large language models, the models could have been trained using these datasets Li &amp; Flanigan (2023).Consequently, these models might inadvertently recall the location of the correct answer.</p>
<p>To address the above issues, We propose a data perturbation procedure and apply it to three existing logical reasoning datasets, resulting in the creation of "ReClor-plus", "LogiQAplus" and "LogiQA v2-plus".These new datasets feature three subsets to evaluate the generalisation and robustness of large language models.These new datasets contain modifications of the task structure of existing logical reasoning datasets.We do not change the semantics of the original context, and only modify its structure (orders, forms).We perform systematic experiments with our logical reasoning datasets to investigate the models' generalisation and robustness on three main aspects.First, we evaluate the in-context generalisation and robustness of large language models on logical reasoning tasks.Second, we perform instruction fine-tuning, instruction prompting and logic-driven data augmentation to evaluate whether those methods can help improve the performance of the models.Third, we investigate how different proportions of data perturbation on the training set can help models improve their generalisation and robustness.Fourth, since model with larger parameter scale demonstrate better performance, it is underexplored how model scale will influence model's performance on logical reasoning.we discuss whether the number of parameters in the model can influence the model's generalisation and robustness.</p>
<p>Our benchmark supports both discriminative large language models like LReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) and generative large language models like GPT-3.5 OpenAI (2023a), GPT-4 OpenAI (2023b), LLaMA Touvron et al. (2023), Alpaca Taori et al. (2023) and Vicuna Chiang et al. (2023).</p>
<p>Our main findings can be summarised as follows:</p>
<p> We find that existing large language models like GPT-3.5 and GPT-4 perform well on logical reasoning tasks in the original format but their performance drops on our new formats, suggesting that the models may have seen these datasets during training and failed to acquire generalised logical reasoning capabilities. We find that instruction fine-tuning can help large language models increase their generalisation and robustness on logical reasoning tasks.In particular, fine-tuned discriminative large language models often demonstrate permutation invariance.Furthermore, applying logic-driven data augmentation for fine-tuning, combined with prompting, can enhance the generalisation performance of both discriminative large language models and generative large language models. We find that, for large training set sizes (more than 10,000 training samples), high ratio of perturbated data (shuffled and substituted) can help increase generative large language model's performance on most logical reasoning tasks.However, this does not work with small training sets. Finally, we find surprisingly that there is no direct correlation between the model's size (from LLaMA-7B to LLaMA-65B) and its generalisation and robustness on logical reasoning tasks.Contrary to intuition and observations from other tasks Touvron et al. ( 2023), a larger model does not necessarily guarantee better generalisation and robustness on logical reasoning tasks.</p>
<p>Related Work</p>
<p>Research on the generalisation and robustness of large language models for logical reasoning mainly focuses on synthetic natural language reasoning.Initial findings indicate that transformers can be trained on multi-hop reasoning tasks and substantially generalise to deeper unseen reasoning depths, although it can be challenging on paraphrased synthetic test sets Clark et al. (2021).Efforts to enhance generalisation to deeper multi-step reasoning include the introduction of PARARULE-Plus, providing data augmentation on reasoning depths between 2 to 5 Bao et al. (2022).AbductionRules incorporates abductive reasoning to understand and answer the multi-step reasoning task Young et al. (2022).ROBUSTLR is a challenging dataset considering conjunction, negation, and utilising logical equivalence for paraphrasing Sanyal et al. (2022).</p>
<p>The existing synthetic multi-step reasoning datasets often lack the complexity and diversity found in real-world data.They might be generated using a limited set of rules or scenarios, which can lead to a narrower scope of logical reasoning challenges.ReClor Yu et al. (2020) and LogiQA Liu et al. (2020a) are challenging reading comprehension datasets derived from real-world examinations such as the GMAT, LSAT, and national civil servant exams Yu et al. (2020); Liu et al. (2020a;2023b).An enhanced version, LogiQAv2, incorporates additional data from Chinese civil servant examinations Liu et al. (2023b).Evaluations reveal that the implementation of discourse order-oriented loss functions, specifically Sentence Order Prediction (SOP) and Next Sentence Prediction (NSP), enhances the performance of models like ALBERT and BERT on reasoning tasks Lan et al. (2020); Kenton &amp; Toutanova (2019); Li et al. (2022).Further assessments of generative large language models, including GPT-3.5 and GPT-4, on ReClor, LogiQA, and LogiQAv2, have demonstrated their commendable performance.However, it is unclear how robust large language models are in real-world logical reasoning tasks Liu et al. (2023b).Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022) utilise reading comprehension tasks requiring logical reasoning to perform experiments and evaluate the logical reasoning capabilities in existing large language models.The model needs to predict the answer by understanding the rules within the context and deducing the conclusion, which cannot be directly found through simple pattern matching from the context.This differs fundamentally from typical reading comprehension tasks.Existing work focuses on reading comprehension through task structure variation, such as shuffling options or replacing the correct one, yet no studies directly utilize task variation to assess logical reasoning.AddSent Jia &amp; Liang (2017) generates misleading text by modifying the question according to certain rules and manually proofreading; AddAny Jia &amp; Liang (2017) automatically searches for misleading texts word by word across various MRC models; AddAn-sCtx Liu et al. (2020b) generates misleading text by removing answer words from answer sentences.In this paper, we define the generalisation and robustness of models in logical reasoning that these models not only need to solve the original question but also address new questions that have been modified through task structure variations, including shuffling the order of options and replacing the correct answer.</p>
<p>Method</p>
<p>We propose a logical reasoning evaluation benchmark for evaluating the robustness and generalisation of large language models.Figure 2 illustrates the various configurations tested.We propose three task structure variations and apply them to three existing datasets to construct our logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus".The task variations are as follows: 1) Shuffle-Order: The order of all the options is shuffled.This variation evaluates whether the model is reasoning or remembers the position of the correct answer.2) Replace-Answer: The correct answer is replaced with "none of the other options is correct" and we add "You can also say there is no correct answer" at the end of the question.This variation evaluates whether the model understands that apart from the correct option, all other options are incorrect.It is used to detect models returning answers that look correct.3) Shuffle-RepAns: The third split combines the variations from 1) and 2).We apply our variations on ReClor Yu et al. (2020), LogiQA Liu et al. (2020a) and LogiQAv2 Liu et al. (2023b).The three logical reasoning datasets are all formatted as multiple-choice reading comprehension tasks.An example from ReClor can be found in Figure 1.LogiQA and LogiQAv2 are following the same format as ReClor.For each question in these datasets, there are only four options, and only one of these options is the correct answer.We propose these three datasets to validate the robustness of the model from the following three perspectives: a) We want to evaluate whether the model performs reasoning, instead of just memorizing the position of the correct answer.b) We want to evaluate whether the model understands that, aside from the correct option, all other options are incorrect.We do not want model to simply predict the answer that looks most like the correct answer.c) Building upon the second point, we shuffle the order of the options to judge whether the model can perform more complex reasoning.Figure 1: The instruction fine-tuning involves providing the model with a task description before the input.It includes the Instruction, Input, and Question.The model then gives the expected output.The correct answer is highlighted in blue with a checkmark.Each question has four choices, and only one of them is the correct answer.</p>
<p>Fine-tuned discriminative large language models have to select one answer from the answer set.In constrast, the generative large language models used in next-token prediction setting have to generate a text that matches the correct option letter.To better make a comparison between these two classes of models, we add the following instruction after the question: "Please only return the letter in front of your predict correct option, A, B, C or D." to ease the evaluation.We catch the correct answer using regular expressions from the generated prediction if the prediction does not correspond to the desired format.We use the official Alpaca2 and Vicuna3 repositories for instruction fine-tuning and instruction prompting.We perform instruction fine-tuning and instruction prompting on both models.For instruction prompting, we use Chain-of-Thought prompting to explore how it can help increase the generalisation and robustness of large language models in logical reasoning tasks.</p>
<p>Instruction Fine-Tuning/Prompting Instruction Fine-Tuning (IFT) has been proposed to enhance the performance of large language models on unseen tasks Mishra et al. (2022); Wei et al. (2022a).An example of this approach is depicted in Figure 1.An instruction, "Can you predict the correct option for the given input?" is added, encompassing the context, question, and each option from the logical reasoning datasets under evaluation.Subsequently, a sentence is introduced: "Please only return the letter in front of your predicted correct option, A, B, C, or D." The process entails training the model on pairs of instructions and corresponding responses.Contrastingly, while instruction prompting employs a similar input format, it is utilised during inference rather than training.Two methods of instruction prompting (IPT) are implemented: a zero-shot evaluation with a format identical to instruction fine-tuning, and a second approach, Chain-of-Thought Wei et al. (2022c), specifically applied to enhance performance in logical reasoning.The latter integrates the following prompt: "Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:' " into the instructions to encourage the model to reason.</p>
<p>We also investigate if adding our perturbed sets into the training of the LLMs can help increase performance, and what proportion of perturbed data is required.We perform instruction fine-tuning with ReClor, LogiQA and LogiQAv2 separately.We use different ratios of perturbed sets when shuffling and replacing the order of options.</p>
<p>Experiments</p>
<p>Experiment Setup</p>
<p>Modeling Choices We use GPT-3.5-Turbofor the GPT-3.5 experiments and GPT-4 with 8192 tokens for the GPT-4 experiments.For fine-tuned discriminative large language models, we refer to the code from the ReClor leaderboard4 .The appendix demonstrates a more detailed hyperparameter setting and model selection.</p>
<p>Result on the Original Datasets</p>
<p>We summarise our primary findings in Table 1.In this subsection, we emphasise results from the original ReClor, LogiQA, and LogiQAv2 datasets.Under the zero-shot evaluation setting, both GPT-3.5 and GPT-4 notably outperform Alpaca-7B and Vicuna-7B across the three datasets, with the latter two models underperforming.Among these, GPT-4 achieves the highest accuracy.We then fine-tuned Alpaca-7B and Vicuna-7B using training data from each individual task and subsequently evaluated them.Notably, all the fine-tuned generative large language models display a marked improvement over their zero-shot evaluation performance.</p>
<p>Simultaneously, we fine-tuned LReasoner, MERIt, and AMR-LDA on their respective training sets and found commendable performance across the board.AMR-LDA's accuracy approached that of GPT-4.To further enhance the diversity and volume of the training data, we merged training sets from ReClor, LogiQA, and LogiQA-v2.Both generative and discriminative large language models exhibited improved results in this setting.The performance of the fine-tuned Alpaca-7B approached GPT-4's, while both MERIt and AMR-LDA surpassed GPT-4.These results underscore the significance of data diversity and volume in logical reasoning tasks.</p>
<p>Assessing Models' Robustness on Logical Reasoning Tasks</p>
<p>Generative Large Language Models We evaluated the performance of GPT-3.5 and GPT-4 on various datasets, notably observing a significant performance drop on the LogiQAv2 dataset, particularly in its Shuffle-Order variant, compared to less pronounced declines on other datasets.Given that GPT-3.5 and GPT-4 were trained before 2023, and considering the publication of ReClor and LogiQA before 2022, it's plausible that these datasets were part of their training data.However, since LogiQAv2 was released after 2023, it was not included, making it a reliable measure of the models' reasoning capabilities.To address potential data leakage concerns with ReClor and LogiQA, we applied instruction fine-tuning using these datasets, both individually and combined with LogiQAv2.This approach significantly enhanced performance across the board.Furthermore, we assessed Alpaca-7B and Vicuna-7B, finding that their zero-shot evaluations were below random chance levels, and fine-tuning failed to yield consistent improvements across datasets.The observed variability in performance, with improvements on shuffled datasets when initial performance was poor and vice versa, suggests a lack of reliable logical reasoning and potential overfitting to the training data, with minor accuracy gains likely attributable to random variance rather than genuine progress.</p>
<p>Discriminative Large Language Models Fine-tuned discriminative large language models such as LReasoner, MERIt, and AMR-LDA exhibit enhanced generalisation performance on logical reasoning questions.These models demonstrate stable performance on both the original and Shuffle-Order sets; however, they experience a significant decline in performance on the Replace-Answer set and Shuffle-RepAns set.The stability observed in the Shuffle-Order set performance can be attributed to the models' handling of input structure, where the input is formed by concatenating the context, question, and each corresponding option (Context + [SEP]' + Question + [SEP]' + Option), with the output being the label for each concatenation.The special token '[SEP]' is used to separate sentences.Thus, shuffling the option order doesn't create new context, question, and option concatenations, maintaining stable performance and permutation invariance.Despite this, discriminative models do not surpass generative models on the Replace-Answer and Shuffle-RepAns sets, suggesting these models might not engage in complex logical reasoning as effectively.Therefore, the overall performance indicates that large language models, including discriminative ones, may lack strong logical reasoning abilities, as detailed in Table 1.</p>
<p>Chain-of-Thought (CoT) Prompting</p>
<p>We perform further experiments using Chain-of-Thought prompting Wei et al. (2022c).As shown in Table 2, in the zero-shot evaluation, all generative large language models do not perform well.The performance of GPT-3.5 and GPT-4 on the Shuffle-RepAns set is nearly equivalent to a random guess, while Alpaca-7B and Vicuna-7B fail the task.Employing the CoT prompting doesn't result in a significant difference in performance for these generative large language models compared to when CoT prompting is not used.Only GPT-4 exhibits systematic improvements in accuracy on the Shuffle-RepAns task.Nonetheless, this task remains challenging for these models, including GPT-4.We have included some case studies in the appendix A that illustrate the use of CoT prompting to assist GPT-4 in correctly answering questions with intermediate steps.Overall, since CoT prompting does not offer any explicitly useful information as additional input, and the model has not been trained to respond correctly when given the CoT prompting, it is reasonable to expect that it may not perform well in complex logical reasoning scenarios.Since logic-driven data augmentation performs well on logical reasoning tasks for finetuned discriminative large language models like LReasoner and AMR-LDA, it is worth considering to apply this method to generative large language models trained on nexttoken prediction task and see if there is an improvement in our logical reasoning tasks for these models.We use the provided augmented data from the authors of AMR-LDA for ReClor, LogiQA and LogiQAv2 datasets and extend the augmented option information to each option in the training set for ReClor, LogiQA and LogiQAv2.The input format for the augmented data is formed as follows: context + question + each option + extended option + extended context.The term 'extended option' refers to the use of AMR-LDA to augment the option based on the logical equivalence laws.'Extended context' means using AMR-LDA to augment the context based on the logical equivalence laws.The output is the ID of each option, which can be either A, B, C, or D. An example for the extended option and the extended context can be found in Appendix Figure 3. Two examples of how AMR has been used to perform logical equivalence sentence construction for long sentence can be found in Appendix Table 4 and 5.We perform instruction fine-tuning using the input and output formats, along with the same instructions mentioned in the experiment setup.</p>
<p>Models</p>
<p>Table 3 shows that logic-driven data augmentation is detrimental to the generalisation and robustness of large language models trained using next-token prediction for logical reasoning tasks.We make the hypothesis that logic-driven data augmentation does not directly map to the task of next-token prediction, which may disturb the training of the model.This hypothesis is corroborated by Table 3.When we use individual training sets from ReClor, LogiQA, and LogiQAv2 for separate training and testing, there is an observed improvement compared to models that did not undergo such training.However, performance significantly declines when we utilise AMR-LDA to augment data in the training set.This phenomenon is also evident when the training set is expanded to include ReClor, LogiQA, and LogiQAv2 collectively.This suggests that merely increasing the scale of the training set can offer some benefits in enabling the model to tackle more complex logical reasoning tasks.While, if there is a discrepancy between the distributions of the training and test sets, the potential improvements will be constrained.Additionally, using logic-driven data augmentation in the training set, which might distort its distribution, could further deteriorate performance on the test set.</p>
<p>To enhance the performance of models on more complex logical reasoning tasks, we employ logic-driven data augmentation for generative large language models during instruction fine-tuning and prompting.As shown in Table 3, for the experiment was conducted under "prompt augmentation using AMR-LDA" area, both the training and test sets were augmented using AMR-LDA.We discovered that two logic-driven data augmentation methods can enhance model's performance on task variation.The first method involves applying logic-driven data augmentation to both the training and test sets; this approach was effective for the fine-tuned Alpaca-7B, but not for the Alpaca-7B that hadn't been finetuned.The second method involves applying logic-driven data augmentation solely to the test set; this approach was effective for GPT-3.5 and GPT-4.To implement logic-driven data augmentation on the test set, we followed three steps:</p>
<p>Step 1: Augment each option and append the augmented text to the original option.</p>
<p>Step 2: If an option can be augmented, we then augment the context and append this augmented context to the already augmented option.</p>
<p>Step 3: The augmented prompt will be composed of the following elements: context + question + each option + extended option + extended context.This strategy helped large language models enhance their performance on our task variation.This result indicates that logic-driven data augmentation offers benefits in two aspects.The first is altering the distribution of both the training and test sets.The second is enhancing the prompt or input information for larger language models like GPT-3.5 and GPT-4, which extracts additional details to large language models, thereby enhancing their efficacy in logical reasoning tasks.</p>
<p>Transfer Learning with Data Perturbation</p>
<p>We perform transfer learning to investigate the extent to which incorporating variations of task structure into the training set can help models improve their performance on logical reasoning tasks.Table 4 shows our results.We carried out instruction fine-tuning on Alpaca-7B using individual training sets from ReClor, LogiQA, and LogiQAv2, each with varying data perturbation ratios, specifically focusing on Shuffle-RepAns.The reason we selected Alpaca-7B is that our main experiment, as shown in Table 1, involves both Alpaca-7B and Vicuna-7B.These models demonstrate similar performance in Table 1.Under the constraint of limited computational resources, we therefore chose Alpaca-7B for a deeper analysis.We observed that using a large training set of LogiQAv2 for fine-tuning (over 10,000 samples).50% of the training set is perturbed to the Shuffle-RepAns format, which improves performance on all original, Shuffle-Order, Replace-Answer, and Shuffle-RepAns format logical reasoning tasks.However, when the amount of training samples used for the fine-tuning is less than 10,000, the model does not demonstrate the above phenomenon.We also observe an intriguing phenomenon: the models' performance on the Shuffle-Order set does not improve as the proportion of the Shuffle-RepAns set in the training set increases.Instead, it declines.Particularly on the ReClor and LogiQA datasets, we can clearly see that the higher the task structure perturbation ratio, the worse the model performs on the Shuffle-Order set.We speculate that the model may overfit the data with altered task structure perturbation ratio.These results also support the claim that existing large language models are not robust when solving logical reasoning tasks.To enhance the robustness of logical reasoning in large language models, employing a more expansive training dataset and a higher perturbation ratio demonstrates beneficial.</p>
<p>Influence of the Model Size</p>
<p>We perform additional experiments to see whether, with the same model architecture, a larger model size will show better generalisation and robustness on logical reasoning tasks.</p>
<p>We use different size of LLaMA models to perform the experiments from LLaMA-7B to LLaMA-65B.This choice was made because LLaMA provides model checkpoints for various sizes, ranging from 7B to 70B, which were available during the period of our experiment.We utilised the Shuffle-RepAns sets and perform the zero-shot evaluation.Table 5 shows the results.We do not find a significant difference in the Shuffle-RepAns format logical reasoning task with different sizes of models under zero-shot evaluation, without any instruction fine-tuning.In summary, Table 5 sheds light on the nuanced relationship between model size and performance in logical reasoning tasks.While larger models might be expected to perform better due to their increased capacity, the results presented here suggest that model size does not play a significant role in determining overall performance on robust and complex logical reasoning tasks.It's worth noting that our comparisons are based on different model sizes of the same LLaMA base model.Therefore, the conclusions drawn may not necessarily apply to comparisons involving different base models.</p>
<p>Models</p>
<p>Conclusion</p>
<p>Our study examines the generalisation and robustness of large language models (LLMs) in logical reasoning tasks, revealing significant limitations.We find that instruction finetuning enhances performance, while chain-of-thought (CoT) prompting alone falls short in robust reasoning tasks.Analysis shows that datasets larger than 10,000 samples, such as LogiQAv2, require extensive data perturbation and task structure modifications (Shuffle-RepAns) for improved adaptability-a necessity not seen with smaller datasets.Model size, within the same LLaMA base model framework, does not guarantee better generalisation or robustness.We observe that logic-driven data augmentation benefits fine-tuned discriminative models by improving performance and demonstrating permutation invariance, indicating reliance on more than memorisation.Applying logic-driven data augmentation to prompts further aids larger generative models like GPT-3.5, GPT-4, and fine-tuned Alpaca-7B, enhancing their logical reasoning capabilities in novel tasks.</p>
<p>A Appendix</p>
<p>A.1 Background proposed and it, again exceeding the performance on MERIt.We utilise MERIt and AMR-LDA to perform fine-tuning on the downstream tasks using their public model weights.</p>
<p>Generative Large Language Models</p>
<p>A.2 Hyperparameter Setting</p>
<p>We determine the parameter values in Tables 6 and 7 by following the public parameter values provided by Stanford Alpaca 2 for generative large language models, as well as the default hyperparameters used by each discriminative large language model, including AMR-LDA6 , LReasoner7 , and MERIt8 .We use the model weights from the official repositories of LReasoner, MERIt and AMR-LDA on each logical reasoning reading comprehension tasks.</p>
<p>For LReasoner, we select ALBERT-XXLarge-v2 Lan et al. (2019) as the backbone model.For MERIt and AMR-LDA, we select DeBERTaV2-XXLarge He et al. ( 2021) as the backbone model.We selected ALBERT-XXLarge-v2 and DeBERTaV2-XXLarge because they are the best available models, as mentioned in their original papers, LReasoner Wang et al. (2022) and MERIt Jiao et al. (2022), respectively.The authors of LReasoner did not publicly release the model weights for LogiQAv2 and their logic-driven data augmentation are designed for ReClor and LogiQA, hence we use their model weights trained on LogiQA to evaluate the result on LogiQAv2.For fine-tuning instructions on Alpaca and Vicuna, we refer to the training script and code from the official Alpaca GitHub repository Taori et al. (2023).</p>
<p>For the generative large language models like GPT-3.5 and GPT-4.We propose a new instruction for the Shuffle-Order set.We provide an instruction that includes an example, shuffles the order of options for this example, and then request the model's response.For the Replace-Answer set, we add an instruction "You can select the option that none of the other options is correct" when we test more complex scenarios.For the Shuffle-RepAns set, we add the two instructions together.All the experiments are run on 8 NVIDIA A100 GPUs, each with 80G of VRAM.We evaluate on the validation sets of ReClor, LogiQA and LogiQAv2, as the test sets are private and used to rank models on the leaderboard.We constructed our task variations from the existing validation sets.We employed the modified validation set (Shuffle-Order, Replace-Answer, and Shuffle-RepAns) to perform the evaluation.we use accuracy to adhere to the methodologies outlined in previously published papers Bao et al. (2023); Jiao et al. (2022); Wang et al. (2022).We follow the same metrics to ensure a fair comparison when evaluating the model's performance.The datasets used are balanced and only a single choice per question is a logical entailment, so accuracy is a proper measure to assess the models' logical reasoning performance.For prompting GPT-3.5 and GPT-4, we use the default hyperparameters from OpenAI playground.</p>
<p>A.3 Analysis of Hyperparameters for GPT-4</p>
<p>We performed an experiment on GPT-4 using different hyperparameter values (temperature) as shown in Table 8.This experiment was carried out using the GPT-4 API, trained on a dataset containing information up to April 2023.For the other tables in this paper, the experiments utilised the earlier version of GPT-4, released in March, which had only learned from data available up to September 2021.However, we found that temperature did not significantly influence the model's performance across the entire evaluation set.We find that using Chain-of-Thought Prompting can show some improvements in Shuffle-RepAns logical reasoning tasks with GPT-4, while its accuracy is no different from random guessing.Therefore, we listed some examples to see what differences are made in the results generated by GPT-4 after incorporating Chain-of-Thought Prompting.Figure 6, 8 and 10 show that GPT-4 fails the example but with Chain-of-Thought Prompting, it can correctly answer the question with solution process.Figure 7, 9, and 11 show that both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on the example.</p>
<p>A.4 Limitations</p>
<p>Our paper acknowledges several limitations: Firstly, pre-training large language models (LLMs) is computationally expensive, potentially causing fine-tuning efforts to suffer from residual effects of pre-training tasks.Secondly, prevalent logical reasoning evaluation tasks, designed akin to reading comprehension exercises, may unfairly advantage discriminative LLMs over generative counterparts due to their format, highlighting the absence of a balanced dataset for fair assessment across different LLM training tasks.Lastly, the unavailability of GPT-4's source code and training data precludes us from conducting instruction fine-tuning experiments to test its logical reasoning robustness.Figure 3: Example of using AMR-LDA to augment the prompt from the ReClor dataset and their subsequent utilization as input for GPT-4.Data segments that are marked in bold italics and appear in blue were generated using the contraposition law, while those in brown were generated using the implication law.In this example, only the segments in dark blue are the extended option, and the sentences that follow constitute the extended context.</p>
<p>A.5 Ethics Statement</p>
<p>As we do not present a new model but perform an evaluation of the existing state-of-the-art, we believe our work does not present significant ethical risks.We believe that our work will contribute to a better understanding of the training mechanisms of large language models in current large models.By highlighting flaws in the capabilities of language models, this evaluation can help mitigating risks of misuses that could raise ethical concerns.</p>
<p>Long Sentence Example 1:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches.Original sentence's AMR graph: (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :ARG0 p :mod (y / yoga) :ARG1-of (l / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching and woke up early in the morning.</p>
<p>Figure 4: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Long Sentence Example 2:</p>
<p>Original sentence: Sarah woke up early in the morning, and she started her day with a cup of coffee and some light yoga stretches that will help lose weight.Original sentence's AMR graph: (a / and (a / and :op1 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning)))) :op2 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some))))) Modified AMR graph using AMR-LDA: (a / and :op1 (s / start-01 :ARG0 p :ARG1 (d2 / day :poss p) :ARG2 (a2 / and :op1 (c / coffee :quant (v / volume-quantity :quant 1 :unit (c2 / cup))) :op2 (s2 / stretch-01 :mod (y / yoga) :ARG0-of (h / help-01 :ARG1 (l / lose-01 :ARG1 (w2 / weight))) :ARG1-of (l2 / light-06) :quant (s3 / some)))) :op2 (w / wake-up-02 :ARG1 (p / person :name (n / name :op1 "Sarah")) :time (e / early :op1 (d / date-entity :dayperiod (m / morning))))) Generated logical equivalence sentence using AMR-LDA: Sarah started her day with a cup of coffee and some light yoga stretching to help lose weight, and woke up early in the morning.Figure 5: One example uses our AMR-LDA to generate logical equivalence sentences for long sentences.In this case, a logical equivalence sentence is generated using the commutative law, and the same color represents the same argument.AMR-LDA can understand the effect of that clause on yoga stretching.In this case, the order of the former and latter arguments for the conjunction word "and" has been swapped.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: Some biologists believe that the capacity for flight first developed in marine reptiles, claiming that feathers are clearly developed from scales.Other biologists rightly reject this suggestion, pointing out that bats have no scales and that nonmarine reptiles also have scales.Those who believe that flight first developed in tree-dwelling reptiles reject the claim that the limbs of land-dwelling reptiles might have developed into wings.They insist that it is more likely that tree-dwelling reptiles developed wings to assist their leaps from branch to branch.Instruction: Can you predict the correct option for the given input?Input: Advertisement: The dental profession knows that brushing with Blizzard toothpaste is the best way to fight cavities.We surveyed five dentists, and each agreed that the tartar control formula found in Blizzard is the most effective cavityfighting formula available in a toothpaste.Question: The flawed reasoning in which one of the following is most similar to the flawed reasoning in the advertisement?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. We know that electing Gomez would be the best course for the nation to follow because, of ten voters we surveyed, each agreed that electing Gomez would help the nation.B. Some of the nation's voters believe that Gomez is the candidate who would be best for the nation.Of the ten voters we surveyed, each agreed that the policies Gomez is committed to would be the best policies for the nation to adopt.C. The nation's voters know that Gomez is the candidate whose policies would be best for the nation.Of ten voters polled, each said that Gomez would be a very popular leader.D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: The reasoning in the advertisement is flawed because it is based on a small and potentially biased sample size (five dentists) to make a general claim about the dental profession's view on Blizzard toothpaste.Let's analyse each option: A. This option is flawed for the same reason as the advertisement: it's based on a small sample size (ten voters) to make a claim about the best course for the nation.B. This option is also based on a small sample size (ten voters), but it doesn't make a general claim about the nation's voters.Instead, it only states that some voters believe Gomez is the best candidate.C.This option is flawed because it makes a general claim about the nation's voters based on the popularity of Gomez among ten voters, not on the effectiveness of his policies.D. This option states that none of the other options is correct.</p>
<p>The most similar flawed reasoning to the advertisement is found in option A, as it also uses a small sample size to make a general claim.ANSWER: A Figure 7: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.In this example, Chain-of-Thought try to predict an option which looks more like the correct answer.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: An organisation that elects a leadership team implements such a rule?if there are more than one candidate, the candidate must agree to be nominated, and before expressing his consent, he must be informed who his candidate is.</p>
<p>Question: If the above conclusion is true, then the following is also true? Please only return the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. None of the other options is correct.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.C. Implementing this rule, there is at most one candidate nominated.D. Implement the rule, no candidate may be nominated.Correct Answer: "A" GPT-4: "C" Chain-of-Thought Prompting GPT-4: To analyse each option, let's break down the initial rule:</p>
<p>1.There must be more than one candidate.2. The candidate must agree to be nominated.</p>
<ol>
<li>The candidate must be informed of their opponent before agreeing to be nominated.Now let's evaluate each option: A. None of the other options is correct.We cannot determine this without evaluating the other options first.B. Implement the rule so that the number of nominated candidates is greater than not implementing the rule.This option is not necessarily true.The rule does not guarantee that there will be more nominated candidates.It only ensures that a candidate knows their opponent before agreeing to be nominated.C. Implementing this rule, there is at most one candidate nominated.This option is incorrect.The rule states that there must be more than one candidate.Therefore, there cannot be only one nominated candidate.D. Implement the rule, no candidate may be nominated.This option is also incorrect.The rule does not prevent candidates from being nominated, it only requires that they know their opponent before agreeing to be nominated.ANSWER: A. None of the other options is correct.</li>
</ol>
<p>Figure 8: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: There are three types of robots in a laboratory.Step 2: Analyse each option to determine if it aligns with the characteristics of directed thinking.</p>
<p>A. Seeing the man with braids, he concluded that the man was a woman -This statement is based on past knowledge and experience (assuming that braids are typically associated with women).-It forms a direction (concluding that the person is a woman).B. The peach blossoms in Beijing are in bloom, so spring is coming -This statement is based on past knowledge and experience (assuming that peach blossoms bloom in spring).-It forms a direction (predicting that spring is coming).C. The clothes are broken.I want to sew them with a sewing bag -This statement is based on past knowledge and experience (knowing that sewing can fix broken clothes).-It forms a method or procedure (using a sewing bag to sew the clothes).D. None of the other options is correct.-This statement contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Step 3: Propose a solution based on the analysis.ANSWER: D</p>
<p>Figure 10: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out option D is contradicts the analysis of options A, B, and C, which all align with the characteristics of directed thinking.</p>
<p>Instruction: Can you predict the correct option for the given input?Input: In today's society, many old people become old drifters in order to take care of their children and the next generation.In a recent urban survey,52.2percent of the young people surveyed admitted that their parents are old drifters.They and their partners have just started their careers, and work pressure is high, so they have no time to take care of their children and do housework.According to experts, the group of old drifters in China's cities will further expand.Question: Which of the following, if true, best supports the views of the above experts?Describe every step of your reasoning before proposing a solution.When giving the solution, start your sentence with 'ANSWER:'.The 'ANSWER' is the letter in front of your predict correct option, A, B, C or D.</p>
<p>A. Some old people prefer to live in their hometown B. The elderly in the city have better medical conditions than in the countryside C. None of the other options is correct.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas Correct Answer: "C" GPT-4: "B" Chain-of-Thought Prompting GPT-4:</p>
<p>Step 1: Identify the claim made by experts The claim made by experts is that the group of old drifters in China's cities will further expand.</p>
<p>Step 2: Evaluate each option based on its relevance to the claim A. Some old people prefer to live in their hometown This option does not directly support the claim that the group of old drifters in China's cities will further expand.It only states a preference of some old people.B. The elderly in the city have better medical conditions than in the countryside This option could indirectly support the claim, as better medical conditions might attract more elderly people to move to cities.However, it does not directly address the issue of old drifters taking care of their children and the next generation.C. None of the other options is correct.This option is not relevant to the claim.D. After the implementation of the two-child policy, the two-child birth rate in urban areas is lower than that in rural areas This option supports the claim that the group of old drifters in China's cities will further expand.If there are more children being born in urban areas due to the two-child policy, it is likely that more elderly people will move to cities to help take care of their children and grandchildren, thus increasing the number of old drifters in cities.</p>
<p>Step 3: Choose the option that best supports the claim ANSWER: D</p>
<p>LLaMA Touvron et al. (2023), AlpacaTaori et al. (2023), VicunaChiang et al. (2023),GPT-3.5OpenAI (2023a), and GPT-4 OpenAI (2023b) are large autoregressive language models pre-trained to perform next-token prediction.They are based on the decoder-part of the transformer architectureVaswani et al. (2017).GPT-3.5 was fine-tuned fromGPT-3 Brown et al. (2020)  using reinforcement learning from human feedback (RLHF) fine-tuning OpenAI (2023a).GPT-4 shows human-level performance on LSAT, SAT Math and Graduate Record Examination (GRE) Quantitative OpenAI (2023b), all of which assess reasoning and problem solving skills in different contexts.The version of GPT-4 that we use does not include multimodal training and its training details are not publicly available.Discriminative Large Language ModelsLReasoner Wang et al. (2022), MERIt Jiao et al. (2022), and AMR-LDA Bao et al. (2023) are language models fine-tuned for downstream logical reasoning tasks.While the generative large language models perform nexttoken prediction, these models are trained to perform classification tasks, such as multi-choice question answering.These models are based on encoder-only transformers Kenton &amp; Toutanova (2019), whose last layers are replaced with classification layers fine-tuned for different tasks.LReasoner uses logic-driven data augmentation to increase their performance and achieved #1 on the ReClor Yu et al. (2020) leaderboard 5 when it was proposed.MERIt uses relation extraction to construct new data from Wikipedia and performs intermediate training on that new data.It achieved #1 5 on the ReClor leaderboard when initially proposed, exceeding the performance of LReasoner.AMR-LDA uses AMRbased logic-driven data augmentation to construct a tuple of sentences (original sentence, logical equivalence sentence, logical inequivalence sentence) and train the model and then train on the downstream tasks.It achieved #2 5 on the ReClor leaderboard when initially</p>
<p>Figure 6 :
6
Figure6: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns ReClor dataset.GPT-4 fails on this example but Chain-of-thought Prompting GPT-4 answer this example correctly.With the help of Chain-of-Thought Prompting, GPT-4 can find out the other options are incorrect and to select "None of the other options is correct."as the predicted answer.</p>
<p>Figure 9 :
9
Figure 9: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQA dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Figure 11 :
11
Figure 11: Compared GPT-4 and Chain-of-Thought GPT-4 on Shuffle-RepAns LogiQAv2 dataset.Both GPT-4 and Chain-of-Thought Prompting GPT-4 fail on this example.</p>
<p>Please only return the letter in front of your predict correct option, A, B, C or D. A.</p>
<p>Can you predict the correct option for the given input?Input: If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.Question: If the statements above are true, which one of the following must be true?
Instruction Fine-Tuning/Prompting CasesInstruction: If you are not able to write your essays using a word processing program, you haveno keyboarding skills. B. If you are able to write your essays using a word processingprogram, you have at least some keyboarding skills.C. If you are not able to writeyour essays using a word processing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able to write your essays usinga word processing program.Response: "B"</p>
<p>Table 2 :
2
Comparison between base models and models prompted using CoT prompting.
ReClorLogiQALogiQAv2ShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.02300.0121Alpaca-7B-CoT0.01200.03370.0152Vicuna-7B0.06400.17970.1784Vicuna-7B-CoT0.13200.16740.1593GPT-3.50.18470.22860.2583GPT-3.5-CoT0.10880.16740.1722GPT-40.13120.16260.1686GPT-4-CoT0.18160.25230.21774.5 Logic-Driven Data AugmentationReClorLogiQALogiQAv2ModelsShuffleShuffleShuffleRepAnsRepAnsRepAnsZero-shot evaluationAlpaca-7B0.01200.01210.0121GPT-3.50.18470.25830.2583GPT-40.13120.16860.1686ReClor/LogiQA/LogiQAv2 single training setAlpaca-7B-IFT0.27200.22730.1548+ AMR-LDA0.04400.05220.0548ReClor + LogiQA + LogiQAv2 merged training setAlpaca-7B-IFT0.11400.13510.1128+ AMR-LDA0.00600.02450.0197Prompt augmentation using AMR-LDAAlpaca-7B-IPT-LDA0.03000.03680.0331Alpaca-7B-IFT-LDA0.48000.36860.2237GPT-3.5-IPT-LDA0.36670.46850.4971GPT-4-IPT-LDA0.87660.55100.7027</p>
<p>Table 3 :
3
Accuracy of evaluated models when adding AMR-LDA's logic-driven augmented data into the training set.We evaluate Alpaca-7B after instruction fine-tuning.
Datasets ReClorLogiQALogiQAv2Perturbation Ratio  OriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsOriginalShuffle OrderReplace AnswerShuffle RepAnsReClor/LogiQA/LogiQAv2 single training set with different ratio of data perturbation (Shuffle-RepAns)0%0.16800.52800.23600.27200.11050.34860.28410.22730.19120.21220.36580.15485%0.33400.37200.15600.17200.14900.13510.09980.09210.26950.15160.13380.112110%0.41400.43200.20400.23800.30720.28260.23500.24420.22620.09560.19630.172715%0.36200.38600.30600.33400.19040.20270.27950.23190.35370.17780.20010.172750%0.15400.14000.16600.16400.04300.05370.67280.65590.35370.20960.76860.7915
Table 4: Accuracy of Alpaca-7B model for transfer learning scenarios and different perturbation ratio applied to the training set.To make a fair comparison, We ensure that the size of each training set is consistent.</p>
<p>Table 6 :
6
Details of hyperparameters for instruction fine-tuning of generative large language models, Alpaca and Vicuna, on ReClor, LogiQA, and LogiQAv2.
Generative Large Language Models Fine-Tuningseed42batch size1num train epochs5max sequence length1024gradient Accumulation Steps16initial Learning Rate1e-05bf16Trueweight decay0warmup ratio0.03lr scheduler typecosinefsdpfull shard auto wrapfsdp transformer layer cls to wrap LlamaDecoderLayertf32TrueoptimiserAdamW</p>
<p>Table 7 :
7
Details of hyperparameters for fine-tuning of discriminative large language models, AMR-LDA, LReasoner and MERIt, on ReClor, LogiQA and LogiQAv2.</p>
<p>Table 8 :
8
A comparison of the performance of GPT-4 at different temperatures on the original ReClor dataset and the alternative ReClor datasets.
DatasetReClorTemperature Original Shuffle Order Replace Answer Shuffle RepAns00.88800.88000.17600.14400.50.88200.88970.17200.148010.88200.87200.18400.1460</p>
<p>If you have the skill of a keyboard, you can write your essay using a word processing program.If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>"context": "If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.","question": "If the statements above are true, which one of the following must be true?", "answers": A. "If you are not able to write your essays using a word processing program, you have no keyboarding skills.B. "If you are able to write your essays using a word processing program, you have at least some keyboarding skills.If
AMR-LDA Prompt Augmentation Case StudyGPT-4 Input:</p>
<p>you don't have at least some keyboard skills, you can't write your essay with a word processing program. If you can use a com- puter, you have keyboarding skills. If you can write your essay with a word pro- cessing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program.",</p>
<p>C. "If you are not able to write your essays using a word processing program, you are not able to use a computer.</p>
<p>If you can use a computer, you can write your essay using word processing programs. If you can use a computer, you have keyboard- ing skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing</p>
<p>program.",D. "If you have some keyboarding skills, you will be able to write your essays using a word processing program.If</p>
<p>you can't write your essay with a word processing program, you don't have some keyboard skills. If you can use a computer, you have keyboarding skills. If you can write your essay with a word processing program, you can use a computer. Whether you have keyboard skills at all or can't use a computer. Whether you can use a computer or you can't write your own essay with a word processing program." GPT-4 output: B</p>
<p>Type A can recognise colors, type B can recognise shapes, and type C can neither recognise colors nor shapes.The laboratory experimented with robots 1 and 2 using red balls, basketballs, red squares, and blue squares, and ordered them to pick up the red balls, but number 1 picked up the red squares, and number 2 picked up the basketball.Question: Based on the above experiment, which of the following must be true?Please only return the letter in front of your predict correct option, A, B, C or D. A. Both No.1 and No.2 must not be type C. B. Both No.1 and No.2 must be type C. The experiment involved red balls, basketballs, red squares, and blue squares.The robots were ordered to pick up the red balls.Robot 1 picked up red squares.This means that Robot 1 can recognise the color red but not the shape of a ball.Therefore, Robot 1 must be Type A. Robot 2 picked up basketballs.This means that Robot 2 can recognise the shape of a ball but not the color red.Therefore, Robot 2 must be Type B. Now let's evaluate the options: A. Both No.1 and No.2 must not be type C. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. B. Both No.1 and No.2 must be type C. -This is false, as we determined that Robot 1 is Type A and Robot 2 is Type B. C. No.1 is A and No.2 is Type B. -This is true, as we determined that Robot 1 is Type A and Robot 2 is Type B. D. None of the other options is correct.-This is false, as options A and C are correct.</p>
<p>C. No.1 is A and No.2 is Type B. D. None of the other options is correct.Correct Answer: "D" GPT-4: "A" Chain-of-Thought Prompting GPT-4: Let's analyse the information given: 1. Type A can recognise colors.2. Type B can recognise shapes.3. Type C can neither recognise colors nor shapes.ANSWER: C</p>
<p>https://github.com/tatsu-lab/stanford_alpaca
https://github.com/lm-sys/FastChat
https://github.com/yuweihao/reclor
https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347
AMR-LDA Official GitHub Page
https://github.com/SiyuanWangw/LReasoner/tree/master
https://github.com/SparkJiao/MERIt</p>
<p>Multi-step deductive reasoning over natural language: An empirical study on out-of-distribution generalisation. Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng, Michael Witbrock, Jiamou Liu, Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022). the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning &amp; Reasoning (IJCLR 2022)Cumberland Lodge, Windsor Great Park, United KingdomSeptember 2022</p>
<p>Enhancing logical reasoning of large language models through logic-driven data augmentation. Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gal Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu, Paul Denny, Michael Witbrock, Jiamou Liu, The first edition of the Symposium on Advances and Open Problems in Large Language Models (LLM@IJCAI'23). Macao, ChinaAugust 2023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Deberta: decodingenhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , NAACL-HLT2019</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, arXiv:1909.119422019arXiv preprint</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, International Conference on Learning Representations. 2020</p>
<p>Task contamination: Language models may not be few-shot anymore. Changmao Li, Jeffrey Flanigan, 10.48550/arXiv.2312.163372023</p>
<p>Eliteplm: An empirical study on general language ability evaluation of pretrained language models. Junyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuohao Yu, Zhipeng Chen, Jingyuan Wang, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterHuman Language Technologies2022</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023a</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023barXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501Christian Bessiere20202020</p>
<p>A robust adversarial training approach to machine reading comprehension. Kai Liu, Xin Liu, An Yang, Jing Liu, Jinsong Su, Sujian Li, Qiaoqiao She, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2020b34</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2023a</p>
<p>. OpenAI. Gpt-4 technical report. 2023b</p>
<p>Robustlr: A diagnostic benchmark for evaluating logical robustness of deductive reasoners. Soumya Sanyal, Zeyi Liao, Xiang Ren, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, arXiv:2302.13971Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127Findings of ACL 2022. Dublin, Ireland2022</p>
<p>Finetuned language models are zeroshot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022a</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022barXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022c</p>
<p>AbductionRules: Training transformers to explain unexpected inputs. Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock, 10.18653/v1/2022.findings-acl.19Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandMay 2022Association for Computational Linguistics</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, OpenReview.net. Addis Ababa, EthiopiaApril 26-30, 2020. 20208th ICLR 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-735 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-735</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-735</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-259937002</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.07907v2.pdf" target="_blank">Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation</a></p>
                <p><strong>Paper Abstract:</strong> Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks. In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have correlations induced by unobserved confounders. These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity. A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one. Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and causal structure, is difficult to characterize and identify. Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge. To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in avoiding learning spurious correlations compared with other robust RL counterparts. We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e735.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e735.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSC-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust State-Confounded Soft Actor-Critic (RSC-SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical algorithm to solve Robust State-Confounded MDPs that (1) approximates perturbations of unobserved confounders via semantic state perturbations, (2) learns a structural causal transition model with a learnable causal graph, and (3) augments training data with counterfactual transitions and then trains a SAC agent on a mixed buffer to obtain a policy robust to spurious state correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>RSC-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>RSC-SAC approximates the effect of perturbing latent confounders by (a) generating semantically plausible perturbed states (select a state dimension i, find another sample k that maximizes difference in dimension i while minimizing difference in other dimensions, and replace s_i_t with s_i_k), (b) training a structural causal model (encoder-decoder with an intermediate learnable binary adjacency matrix G sampled via Gumbel-Softmax) to predict next states and rewards given perturbed states and actions, with a sparsity penalty on G to remove unnecessary causal links, (c) using the trained SCM to generate novel (s,a,r,s') transitions from the structured uncertainty set, (d) building a replay buffer mixing original and generated data (ratio β) and training a Soft Actor-Critic policy on this buffer to maximize worst-case performance under confounder distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite benchmark (8 tasks: Brightness, Behavior, Crossing, CarType, Lift, Stack, Wipe, Door)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulation environments (Carla for driving and Robosuite for manipulation) containing episodic tasks intentionally instrumented with spurious correlations; fully interactive (agent can act and collect transitions), allow online interventions and data collection; used to evaluate robustness to semantic (confounder-induced) shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Generative counterfactual augmentation via targeted semantic perturbation (dimension-wise swapping) combined with a learned SCM whose causal adjacency matrix is sparsified (λ ||G||_p) to remove spurious parents; training on a mixture buffer downweights original spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding between different state dimensions (task-irrelevant distractors correlated with task-relevant features), composition correlations (re-composition of relevant state parts), semantic shifts induced by unobserved confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via learned causal adjacency G: edges with small/zero weights indicate non-causal (spurious) inputs; also detected by observing degradation when generating perturbed states—if model performance depends on swapped dimensions, those dimensions are likely spurious correlates.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Sparsity regularization on the causal adjacency matrix (penalty λ ||G||_p) to eliminate unnecessary inputs; mixture buffer weighting (augmentation ratio β) reduces reliance on original (possibly spurious) correlations by training on synthetic counterfactual transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Generate counterfactual transitions under perturbed confounder samples and evaluate policy/model performance; ablation studies (removing G, P_θ, or P_c) are used to refute that performance gains come from other sources.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Heuristic semantic perturbation: randomly select a state dimension i, choose sample k from a batch that maximizes ||s_i_t - s_i_k|| while minimizing ||s_{¬i}_t - s_{¬i}_k|| (Equation 7), then replace the chosen dimension to create perturbed states; no explicit experimental-design optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On shifted test environments (normalized by SAC nominal episode reward), RSC-SAC achieves near-nominal performance across tasks: Brightness 0.99, Behavior 1.02, Crossing 1.04, CarType 1.03, Lift 0.98, Stack 0.77, Wipe 0.85, Door 0.61 (Table 1). On nominal environments RSC-SAC remains competitive: Brightness 0.92, Behavior 1.06, Crossing 0.96, CarType 0.96, Lift 0.96, Stack 1.04, Wipe 0.92, Door 0.98 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Vanilla SAC (no robustness) on shifted environments: Brightness 0.56, Behavior 0.13, Crossing 0.81, CarType 0.63, Lift 0.58, Stack 0.26, Wipe 0.16, Door 0.08 (Table 1). On nominal environments SAC is normalized to 1.00 across tasks (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RSC-SAC, by approximating confounder perturbations and learning a sparse structural causal model, substantially improves robustness to spurious correlations in both distraction and composition tasks while maintaining comparable nominal performance; ablating the causal graph, transition model, or confounder-distribution approximation degrades robustness, showing both the perturbation mechanism and SCM are critical. The augmentation ratio β trades nominal performance vs robustness: moderate β (20-70%) yields good robustness without large nominal degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e735.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learnable SCM (Gumbel-Softmax adjacency)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Structural Causal Model with Learnable Binary Adjacency (G_φ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural structural causal transition model that discovers a causal graph between input components and next-state/reward outputs by learning a binary adjacency matrix parameterized by ϕ and sampled with a Gumbel-Softmax relaxation, combined with encoder-decoder per-dimension feature processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Differentiable causal graph discovery via Gumbel-Softmax adjacency (G_φ)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encoder produces per-dimension features; a learnable binary adjacency matrix G (parameters ϕ) is sampled with the Gumbel-Softmax trick to allow gradients; features are linearly combined using G to form decoder inputs which predict next state dimensions and reward; training objective combines prediction losses (||s_{t+1} - ŝ_{t+1}||^2 + ||r_t - ŕ_t||^2) with a sparsity penalty λ ||G||_p to encourage few parents and thereby eliminate spurious inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same Carla and Robosuite tasks used for RSC-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic simulators with engineered spurious correlations; SCM is trained from collected transitions (original and perturbed) in these environments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Variable selection via a sparsified learned adjacency matrix (regularized binary graph) that removes edges from distractor dimensions to predicted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/distracting state dimensions that correlate with target state components due to confounders; spurious composition correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Magnitude/absence of learned adjacency entries: edges pushed to zero by sparsity indicate dimensions that are not causal parents and likely spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Sparsity regularization (λ ||G||_p) penalizes inclusion of edges from distractors, effectively downweighting their influence on predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Use of perturbed (counterfactual) inputs: if predicted next-state/rewards remain accurate when candidate parent is perturbed/removed, that parent is likely spurious; ablation (removing G_φ) demonstrates causal graph importance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Ablation (Table 3) shows removing G_φ degrades shifted-environment normalized rewards (example tasks): Lift drops from 0.98 (full) to 0.79 (w/o G_φ); Behavior 1.02 -> 0.51; Crossing 1.04 -> 0.87.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a sparse causal adjacency is crucial: the trained G_φ improves next-state/reward prediction under perturbed states and enables generation of high-quality counterfactual transitions; sparsity helps eliminate distractor inputs in distraction tasks, but sparsity alone does not fully address composition-type correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e735.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic permutation perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional confounder approximation via semantic dimension-wise permutation (Eq. 7 heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic procedure to simulate distributional shifts of unobserved confounders by permuting a selected state dimension with values from another sample chosen to maximize change in that dimension while keeping other dimensions similar.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Semantic dimension-wise permutation for confounder perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Randomly pick a state dimension i; from a batch of K samples pick index k = argmax (||s_i_t - s_i_k||^2 - ||s_{¬i}_t - s_{¬i}_k||^2) and set s_i_t <- s_i_k. This breaks spurious correlations involving dimension i while preserving semantic context in remaining dimensions, approximating the effect of varying latent confounders without explicit P_c estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Batch-based perturbation applied to collected transitions in interactive simulators; not an active intervention policy but a data-augmentation heuristic performed during training data preparation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Directly breaks correlations by substituting the suspect dimension with a semantically different value drawn from another sample while preserving other features, thereby creating counterfactual examples that reduce reliance on distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations between state dimensions (distractors correlated with task-relevant features).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Not an explicit detector: perturbation reveals variables whose change causes performance shifts, implicitly identifying spurious dependencies when performance changes are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Mixture training buffer with augmentation ratio β: training on perturbed data reduces the weight of original spurious statistical regularities.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Evaluate model/policy on generated perturbed transitions; persistent performance indicates reliance on causal features rather than spurious ones.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Heuristic selection within mini-batches (Equation 7): maximize difference in chosen dimension while minimizing difference in other dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>When used within RSC-SAC and combined with SCM, leads to high shifted-environment performance (see RSC-SAC numbers). Robustness varies with augmentation ratio β; moderate β (20-70%) yields strong robustness while preserving nominal performance (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>If β is very small (1%), negligible robustness gains; if β too large (>80%), overall performance degrades due to over-conservatism.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic permutation is effective at creating counterfactuals that break spurious dimension correlations and, when combined with a causal transition model, yields high-quality generated transitions that improve policy robustness; however, naive permutation can also break true causal relations and must be balanced (trade-off governed by β).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e735.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoCoDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoCoDA (Model-based Counterfactual Data Augmentation, adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A counterfactual data augmentation baseline that stitches locally-factored dynamics components from different trajectories to generate new training samples; here adapted to use an approximate causal graph rather than a true graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mocoda: Model-based counterfactual data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MoCoDA (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Identify locally factored dynamics (components with local causality) and swap components across trajectories to synthesize counterfactual transitions; in this paper MoCoDA is adapted to operate with an approximate causal graph rather than ground-truth causality.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite benchmark (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same interactive episodic simulation tasks; used to augment training data by composing components of observed trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Counterfactual composition of locally-factored components to break spurious associations by recombining parts of state trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations arising from locally factorizable components of dynamics (composition-type correlations).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Local-factor identification (assumes local causality) — not fully described in this paper; adapted to approximate causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On shifted environments (Table 1) MoCoDA underperforms several baselines: Brightness 0.50, Behavior 0.16, Crossing 0.22, CarType 0.23, Lift 0.46, Stack 0.29, Wipe 0.01, Door 0.09 (normalized rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>On nominal environments MoCoDA also trails best methods: Brightness 0.65, Behavior 0.78, Crossing 0.57, CarType 0.55, Lift 0.79, Stack 0.72, Wipe 0.69, Door 0.41 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapted MoCoDA (using approximate causal graphs) does not provide robustness competitive with RSC-SAC in these tasks; assumptions of local causality may limit its effectiveness on the designed benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e735.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ATLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Training for Learned Adversary (ATLA) / State-adversarial MDP adversary baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that generates adversarial state perturbations via an optimal adversary within a prescribed uncertainty set (state-adversarial MDP) and trains agents to be robust to these adversarial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust reinforcement learning on state observations with learned optimal adversary</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ATLA (state-adversarial adversary)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct an optimal adversary that perturbs agent-observed states within a bounded uncertainty set to minimize agent return; train agent with these adversarially perturbed observations to improve robustness to observation perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite benchmark (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic simulations; adversary perturbs agent-observed states (SA-MDP setting) rather than modeling latent confounder-induced true state shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Observation noise/adversarial perturbations (small, local perturbations around nominal states), not structured confounder-induced shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>On shifted environments (Table 1): Brightness 0.48, Behavior 0.14, Crossing 0.61, CarType 0.52, Lift 0.61, Stack 0.21, Wipe 0.29, Door 0.28. Results indicate ATLA trained for small adversarial perturbations can fail under structured semantic shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>On nominal environments ATLA is competitive: e.g., Brightness 0.99, Behavior 0.98, Crossing 0.89, CarType 0.88 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adversarial training targeting small/unstructured observation perturbations (SA-MDP) does not reliably address structured uncertainty induced by latent confounders; can be insufficient or even harmful when test environments contain semantic shifts outside the adversary's uncertainty set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e735.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DBC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Bisimulation for Control (DBC) — invariant feature baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that learns invariant features for RL via a bisimulation metric to avoid reliance on irrelevant features; included here as a comparator for invariant-feature approaches to spurious correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DBC (invariant feature learning via bisimulation metric)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Learns representations that are invariant with respect to bisimilar states (states with same long-term behavior), aiming to discard task-irrelevant variations and thereby reduce spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite benchmark (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic simulators; DBC learns representations from observed transitions to reduce sensitivity to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Invariant representation learning using bisimulation metrics to collapse observationally equivalent states and ignore irrelevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Task-irrelevant variations (distractors), some forms of spurious correlations that do not affect long-term dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Representation collapse via bisimulation reduces influence of irrelevant inputs on policy/value estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Shifted-environment results (Table 1) mixed: sometimes decent (Crossing 0.68) but fails in other tasks (Door 0.01, Stack 0.03); shows invariant features help for some distractor types but not all.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Nominal-environment results (Table 2) are moderate (e.g., Brightness 0.75, Behavior 0.78) but generally below best methods.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Invariant-feature approaches like DBC can mitigate some distraction-type spurious correlations but are insufficient for composition-type correlations and structured confounder shifts; performance varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e735.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e735.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active sampling (Active)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active transition sampling to reduce causal confusion (Active)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that actively samples uncertain transitions to reduce causal confusion by focusing data collection on informative parts of state-action space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can active sampling reduce causal confusion in offline reinforcement learning?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Active sampling for causal confusion reduction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Actively identifies and samples transitions with high epistemic uncertainty (or high potential for causal confusion) to gather informative data that can disambiguate causal relations and reduce spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Carla and Robosuite benchmark (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive episodic simulators where agent can target collection of transitions that reduce causal ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Focused data collection to disambiguate causal from spurious associations by sampling informative transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Confounding and causal confusion arising in offline datasets; distractor correlations that can be resolved by targeted sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Active selection of transitions with high uncertainty to reduce causal confusion (paper adapts active sampling methods; implementation details per cited Active work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Shifted-environment results (Table 1): Brightness 0.47, Behavior 0.14, Crossing 0.83, CarType 0.77, Lift 0.35, Stack 0.24, Wipe 0.17, Door 0.05; mixed performance indicating active sampling helps some tasks (Crossing) but not all.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Nominal-environment results (Table 2): Brightness 1.02, Behavior 1.08, Crossing 1.00, CarType 1.00 etc., indicating strong nominal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active sampling that reduces causal confusion can improve nominal performance and help in some shifted tasks, but without explicit modeling of latent confounders and structured perturbations it does not consistently produce robust policies across all semantic shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mocoda: Model-based counterfactual data augmentation <em>(Rating: 2)</em></li>
                <li>Counterfactual data augmentation using locally factored dynamics <em>(Rating: 2)</em></li>
                <li>Robust reinforcement learning on state observations with learned optimal adversary <em>(Rating: 2)</em></li>
                <li>Invariant risk minimization <em>(Rating: 1)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
                <li>Can active sampling reduce causal confusion in offline reinforcement learning? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-735",
    "paper_id": "paper-259937002",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "RSC-SAC",
            "name_full": "Robust State-Confounded Soft Actor-Critic (RSC-SAC)",
            "brief_description": "An empirical algorithm to solve Robust State-Confounded MDPs that (1) approximates perturbations of unobserved confounders via semantic state perturbations, (2) learns a structural causal transition model with a learnable causal graph, and (3) augments training data with counterfactual transitions and then trains a SAC agent on a mixed buffer to obtain a policy robust to spurious state correlations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "RSC-SAC",
            "method_description": "RSC-SAC approximates the effect of perturbing latent confounders by (a) generating semantically plausible perturbed states (select a state dimension i, find another sample k that maximizes difference in dimension i while minimizing difference in other dimensions, and replace s_i_t with s_i_k), (b) training a structural causal model (encoder-decoder with an intermediate learnable binary adjacency matrix G sampled via Gumbel-Softmax) to predict next states and rewards given perturbed states and actions, with a sparsity penalty on G to remove unnecessary causal links, (c) using the trained SCM to generate novel (s,a,r,s') transitions from the structured uncertainty set, (d) building a replay buffer mixing original and generated data (ratio β) and training a Soft Actor-Critic policy on this buffer to maximize worst-case performance under confounder distribution shifts.",
            "environment_name": "Carla and Robosuite benchmark (8 tasks: Brightness, Behavior, Crossing, CarType, Lift, Stack, Wipe, Door)",
            "environment_description": "Interactive simulation environments (Carla for driving and Robosuite for manipulation) containing episodic tasks intentionally instrumented with spurious correlations; fully interactive (agent can act and collect transitions), allow online interventions and data collection; used to evaluate robustness to semantic (confounder-induced) shifts.",
            "handles_distractors": true,
            "distractor_handling_technique": "Generative counterfactual augmentation via targeted semantic perturbation (dimension-wise swapping) combined with a learned SCM whose causal adjacency matrix is sparsified (λ ||G||_p) to remove spurious parents; training on a mixture buffer downweights original spurious correlations.",
            "spurious_signal_types": "Confounding between different state dimensions (task-irrelevant distractors correlated with task-relevant features), composition correlations (re-composition of relevant state parts), semantic shifts induced by unobserved confounders.",
            "detection_method": "Implicit detection via learned causal adjacency G: edges with small/zero weights indicate non-causal (spurious) inputs; also detected by observing degradation when generating perturbed states—if model performance depends on swapped dimensions, those dimensions are likely spurious correlates.",
            "downweighting_method": "Sparsity regularization on the causal adjacency matrix (penalty λ ||G||_p) to eliminate unnecessary inputs; mixture buffer weighting (augmentation ratio β) reduces reliance on original (possibly spurious) correlations by training on synthetic counterfactual transitions.",
            "refutation_method": "Generate counterfactual transitions under perturbed confounder samples and evaluate policy/model performance; ablation studies (removing G, P_θ, or P_c) are used to refute that performance gains come from other sources.",
            "uses_active_learning": false,
            "inquiry_strategy": "Heuristic semantic perturbation: randomly select a state dimension i, choose sample k from a batch that maximizes ||s_i_t - s_i_k|| while minimizing ||s_{¬i}_t - s_{¬i}_k|| (Equation 7), then replace the chosen dimension to create perturbed states; no explicit experimental-design optimizer.",
            "performance_with_robustness": "On shifted test environments (normalized by SAC nominal episode reward), RSC-SAC achieves near-nominal performance across tasks: Brightness 0.99, Behavior 1.02, Crossing 1.04, CarType 1.03, Lift 0.98, Stack 0.77, Wipe 0.85, Door 0.61 (Table 1). On nominal environments RSC-SAC remains competitive: Brightness 0.92, Behavior 1.06, Crossing 0.96, CarType 0.96, Lift 0.96, Stack 1.04, Wipe 0.92, Door 0.98 (Table 2).",
            "performance_without_robustness": "Vanilla SAC (no robustness) on shifted environments: Brightness 0.56, Behavior 0.13, Crossing 0.81, CarType 0.63, Lift 0.58, Stack 0.26, Wipe 0.16, Door 0.08 (Table 1). On nominal environments SAC is normalized to 1.00 across tasks (Table 2).",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "RSC-SAC, by approximating confounder perturbations and learning a sparse structural causal model, substantially improves robustness to spurious correlations in both distraction and composition tasks while maintaining comparable nominal performance; ablating the causal graph, transition model, or confounder-distribution approximation degrades robustness, showing both the perturbation mechanism and SCM are critical. The augmentation ratio β trades nominal performance vs robustness: moderate β (20-70%) yields good robustness without large nominal degradation.",
            "uuid": "e735.0",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Learnable SCM (Gumbel-Softmax adjacency)",
            "name_full": "Differentiable Structural Causal Model with Learnable Binary Adjacency (G_φ)",
            "brief_description": "A neural structural causal transition model that discovers a causal graph between input components and next-state/reward outputs by learning a binary adjacency matrix parameterized by ϕ and sampled with a Gumbel-Softmax relaxation, combined with encoder-decoder per-dimension feature processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Differentiable causal graph discovery via Gumbel-Softmax adjacency (G_φ)",
            "method_description": "Encoder produces per-dimension features; a learnable binary adjacency matrix G (parameters ϕ) is sampled with the Gumbel-Softmax trick to allow gradients; features are linearly combined using G to form decoder inputs which predict next state dimensions and reward; training objective combines prediction losses (||s_{t+1} - ŝ_{t+1}||^2 + ||r_t - ŕ_t||^2) with a sparsity penalty λ ||G||_p to encourage few parents and thereby eliminate spurious inputs.",
            "environment_name": "Same Carla and Robosuite tasks used for RSC-SAC",
            "environment_description": "Interactive episodic simulators with engineered spurious correlations; SCM is trained from collected transitions (original and perturbed) in these environments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Variable selection via a sparsified learned adjacency matrix (regularized binary graph) that removes edges from distractor dimensions to predicted outputs.",
            "spurious_signal_types": "Irrelevant/distracting state dimensions that correlate with target state components due to confounders; spurious composition correlations.",
            "detection_method": "Magnitude/absence of learned adjacency entries: edges pushed to zero by sparsity indicate dimensions that are not causal parents and likely spurious.",
            "downweighting_method": "Sparsity regularization (λ ||G||_p) penalizes inclusion of edges from distractors, effectively downweighting their influence on predictions.",
            "refutation_method": "Use of perturbed (counterfactual) inputs: if predicted next-state/rewards remain accurate when candidate parent is perturbed/removed, that parent is likely spurious; ablation (removing G_φ) demonstrates causal graph importance.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Ablation (Table 3) shows removing G_φ degrades shifted-environment normalized rewards (example tasks): Lift drops from 0.98 (full) to 0.79 (w/o G_φ); Behavior 1.02 -&gt; 0.51; Crossing 1.04 -&gt; 0.87.",
            "performance_without_robustness": null,
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Learning a sparse causal adjacency is crucial: the trained G_φ improves next-state/reward prediction under perturbed states and enables generation of high-quality counterfactual transitions; sparsity helps eliminate distractor inputs in distraction tasks, but sparsity alone does not fully address composition-type correlations.",
            "uuid": "e735.1",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Semantic permutation perturbation",
            "name_full": "Distributional confounder approximation via semantic dimension-wise permutation (Eq. 7 heuristic)",
            "brief_description": "A heuristic procedure to simulate distributional shifts of unobserved confounders by permuting a selected state dimension with values from another sample chosen to maximize change in that dimension while keeping other dimensions similar.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Semantic dimension-wise permutation for confounder perturbation",
            "method_description": "Randomly pick a state dimension i; from a batch of K samples pick index k = argmax (||s_i_t - s_i_k||^2 - ||s_{¬i}_t - s_{¬i}_k||^2) and set s_i_t &lt;- s_i_k. This breaks spurious correlations involving dimension i while preserving semantic context in remaining dimensions, approximating the effect of varying latent confounders without explicit P_c estimation.",
            "environment_name": "Carla and Robosuite tasks",
            "environment_description": "Batch-based perturbation applied to collected transitions in interactive simulators; not an active intervention policy but a data-augmentation heuristic performed during training data preparation.",
            "handles_distractors": true,
            "distractor_handling_technique": "Directly breaks correlations by substituting the suspect dimension with a semantically different value drawn from another sample while preserving other features, thereby creating counterfactual examples that reduce reliance on distractors.",
            "spurious_signal_types": "Spurious correlations between state dimensions (distractors correlated with task-relevant features).",
            "detection_method": "Not an explicit detector: perturbation reveals variables whose change causes performance shifts, implicitly identifying spurious dependencies when performance changes are observed.",
            "downweighting_method": "Mixture training buffer with augmentation ratio β: training on perturbed data reduces the weight of original spurious statistical regularities.",
            "refutation_method": "Evaluate model/policy on generated perturbed transitions; persistent performance indicates reliance on causal features rather than spurious ones.",
            "uses_active_learning": false,
            "inquiry_strategy": "Heuristic selection within mini-batches (Equation 7): maximize difference in chosen dimension while minimizing difference in other dimensions.",
            "performance_with_robustness": "When used within RSC-SAC and combined with SCM, leads to high shifted-environment performance (see RSC-SAC numbers). Robustness varies with augmentation ratio β; moderate β (20-70%) yields strong robustness while preserving nominal performance (Figure 7).",
            "performance_without_robustness": "If β is very small (1%), negligible robustness gains; if β too large (&gt;80%), overall performance degrades due to over-conservatism.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Semantic permutation is effective at creating counterfactuals that break spurious dimension correlations and, when combined with a causal transition model, yields high-quality generated transitions that improve policy robustness; however, naive permutation can also break true causal relations and must be balanced (trade-off governed by β).",
            "uuid": "e735.2",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "MoCoDA",
            "name_full": "MoCoDA (Model-based Counterfactual Data Augmentation, adapted)",
            "brief_description": "A counterfactual data augmentation baseline that stitches locally-factored dynamics components from different trajectories to generate new training samples; here adapted to use an approximate causal graph rather than a true graph.",
            "citation_title": "Mocoda: Model-based counterfactual data augmentation",
            "mention_or_use": "use",
            "method_name": "MoCoDA (adapted)",
            "method_description": "Identify locally factored dynamics (components with local causality) and swap components across trajectories to synthesize counterfactual transitions; in this paper MoCoDA is adapted to operate with an approximate causal graph rather than ground-truth causality.",
            "environment_name": "Carla and Robosuite benchmark (used as baseline)",
            "environment_description": "Same interactive episodic simulation tasks; used to augment training data by composing components of observed trajectories.",
            "handles_distractors": true,
            "distractor_handling_technique": "Counterfactual composition of locally-factored components to break spurious associations by recombining parts of state trajectories.",
            "spurious_signal_types": "Spurious correlations arising from locally factorizable components of dynamics (composition-type correlations).",
            "detection_method": "Local-factor identification (assumes local causality) — not fully described in this paper; adapted to approximate causal graphs.",
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "On shifted environments (Table 1) MoCoDA underperforms several baselines: Brightness 0.50, Behavior 0.16, Crossing 0.22, CarType 0.23, Lift 0.46, Stack 0.29, Wipe 0.01, Door 0.09 (normalized rewards).",
            "performance_without_robustness": "On nominal environments MoCoDA also trails best methods: Brightness 0.65, Behavior 0.78, Crossing 0.57, CarType 0.55, Lift 0.79, Stack 0.72, Wipe 0.69, Door 0.41 (Table 2).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Adapted MoCoDA (using approximate causal graphs) does not provide robustness competitive with RSC-SAC in these tasks; assumptions of local causality may limit its effectiveness on the designed benchmarks.",
            "uuid": "e735.3",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ATLA",
            "name_full": "Adversarial Training for Learned Adversary (ATLA) / State-adversarial MDP adversary baseline",
            "brief_description": "A baseline that generates adversarial state perturbations via an optimal adversary within a prescribed uncertainty set (state-adversarial MDP) and trains agents to be robust to these adversarial observations.",
            "citation_title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "mention_or_use": "use",
            "method_name": "ATLA (state-adversarial adversary)",
            "method_description": "Construct an optimal adversary that perturbs agent-observed states within a bounded uncertainty set to minimize agent return; train agent with these adversarially perturbed observations to improve robustness to observation perturbations.",
            "environment_name": "Carla and Robosuite benchmark (used as baseline)",
            "environment_description": "Interactive episodic simulations; adversary perturbs agent-observed states (SA-MDP setting) rather than modeling latent confounder-induced true state shifts.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Observation noise/adversarial perturbations (small, local perturbations around nominal states), not structured confounder-induced shifts.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "On shifted environments (Table 1): Brightness 0.48, Behavior 0.14, Crossing 0.61, CarType 0.52, Lift 0.61, Stack 0.21, Wipe 0.29, Door 0.28. Results indicate ATLA trained for small adversarial perturbations can fail under structured semantic shifts.",
            "performance_without_robustness": "On nominal environments ATLA is competitive: e.g., Brightness 0.99, Behavior 0.98, Crossing 0.89, CarType 0.88 (Table 2).",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Adversarial training targeting small/unstructured observation perturbations (SA-MDP) does not reliably address structured uncertainty induced by latent confounders; can be insufficient or even harmful when test environments contain semantic shifts outside the adversary's uncertainty set.",
            "uuid": "e735.4",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DBC",
            "name_full": "Deep Bisimulation for Control (DBC) — invariant feature baseline",
            "brief_description": "A baseline that learns invariant features for RL via a bisimulation metric to avoid reliance on irrelevant features; included here as a comparator for invariant-feature approaches to spurious correlation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "DBC (invariant feature learning via bisimulation metric)",
            "method_description": "Learns representations that are invariant with respect to bisimilar states (states with same long-term behavior), aiming to discard task-irrelevant variations and thereby reduce spurious correlations.",
            "environment_name": "Carla and Robosuite benchmark (used as baseline)",
            "environment_description": "Interactive episodic simulators; DBC learns representations from observed transitions to reduce sensitivity to distractors.",
            "handles_distractors": true,
            "distractor_handling_technique": "Invariant representation learning using bisimulation metrics to collapse observationally equivalent states and ignore irrelevant features.",
            "spurious_signal_types": "Task-irrelevant variations (distractors), some forms of spurious correlations that do not affect long-term dynamics.",
            "detection_method": null,
            "downweighting_method": "Representation collapse via bisimulation reduces influence of irrelevant inputs on policy/value estimators.",
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "Shifted-environment results (Table 1) mixed: sometimes decent (Crossing 0.68) but fails in other tasks (Door 0.01, Stack 0.03); shows invariant features help for some distractor types but not all.",
            "performance_without_robustness": "Nominal-environment results (Table 2) are moderate (e.g., Brightness 0.75, Behavior 0.78) but generally below best methods.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Invariant-feature approaches like DBC can mitigate some distraction-type spurious correlations but are insufficient for composition-type correlations and structured confounder shifts; performance varies by task.",
            "uuid": "e735.5",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Active sampling (Active)",
            "name_full": "Active transition sampling to reduce causal confusion (Active)",
            "brief_description": "A baseline that actively samples uncertain transitions to reduce causal confusion by focusing data collection on informative parts of state-action space.",
            "citation_title": "Can active sampling reduce causal confusion in offline reinforcement learning?",
            "mention_or_use": "use",
            "method_name": "Active sampling for causal confusion reduction",
            "method_description": "Actively identifies and samples transitions with high epistemic uncertainty (or high potential for causal confusion) to gather informative data that can disambiguate causal relations and reduce spurious correlations.",
            "environment_name": "Carla and Robosuite benchmark (used as baseline)",
            "environment_description": "Interactive episodic simulators where agent can target collection of transitions that reduce causal ambiguity.",
            "handles_distractors": true,
            "distractor_handling_technique": "Focused data collection to disambiguate causal from spurious associations by sampling informative transitions.",
            "spurious_signal_types": "Confounding and causal confusion arising in offline datasets; distractor correlations that can be resolved by targeted sampling.",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": true,
            "inquiry_strategy": "Active selection of transitions with high uncertainty to reduce causal confusion (paper adapts active sampling methods; implementation details per cited Active work).",
            "performance_with_robustness": "Shifted-environment results (Table 1): Brightness 0.47, Behavior 0.14, Crossing 0.83, CarType 0.77, Lift 0.35, Stack 0.24, Wipe 0.17, Door 0.05; mixed performance indicating active sampling helps some tasks (Crossing) but not all.",
            "performance_without_robustness": "Nominal-environment results (Table 2): Brightness 1.02, Behavior 1.08, Crossing 1.00, CarType 1.00 etc., indicating strong nominal performance.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Active sampling that reduces causal confusion can improve nominal performance and help in some shifted tasks, but without explicit modeling of latent confounders and structured perturbations it does not consistently produce robust policies across all semantic shifts.",
            "uuid": "e735.6",
            "source_info": {
                "paper_title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mocoda: Model-based counterfactual data augmentation",
            "rating": 2,
            "sanitized_title": "mocoda_modelbased_counterfactual_data_augmentation"
        },
        {
            "paper_title": "Counterfactual data augmentation using locally factored dynamics",
            "rating": 2,
            "sanitized_title": "counterfactual_data_augmentation_using_locally_factored_dynamics"
        },
        {
            "paper_title": "Robust reinforcement learning on state observations with learned optimal adversary",
            "rating": 2,
            "sanitized_title": "robust_reinforcement_learning_on_state_observations_with_learned_optimal_adversary"
        },
        {
            "paper_title": "Invariant risk minimization",
            "rating": 1,
            "sanitized_title": "invariant_risk_minimization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Can active sampling reduce causal confusion in offline reinforcement learning?",
            "rating": 1,
            "sanitized_title": "can_active_sampling_reduce_causal_confusion_in_offline_reinforcement_learning"
        }
    ],
    "cost": 0.022264,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
25 Oct 2023</p>
<p>Wenhao Ding wenhaod@andrew.cmu.edu 
Carnegie Mellon University</p>
<p>Laixi Shi laixis@andrew.cmu.edu 
California Institute of Technology</p>
<p>Yuejie Chi yuejiec@andrew.cmu.edu 
Carnegie Mellon University</p>
<p>Ding Zhao dingzhao@cmu.edu 
Carnegie Mellon University</p>
<p>Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation
25 Oct 20235148C45DAD0A0732BC718E7675D88579arXiv:2307.07907v2[cs.LG]
Robustness has been extensively studied in reinforcement learning (RL) to handle various forms of uncertainty such as random perturbations, rare events, and malicious attacks.In this work, we consider one critical type of robustness against spurious correlation, where different portions of the state do not have correlations induced by unobserved confounders.These spurious correlations are ubiquitous in real-world tasks, for instance, a self-driving car usually observes heavy traffic in the daytime and light traffic at night due to unobservable human activity.A model that learns such useless or even harmful correlation could catastrophically fail when the confounder in the test case deviates from the training one.Although motivated, enabling robustness against spurious correlation poses significant challenges since the uncertainty set, shaped by the unobserved confounder and causal structure, is difficult to characterize and identify.Existing robust algorithms that assume simple and unstructured uncertainty sets are therefore inadequate to address this challenge.To solve this issue, we propose Robust State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically demonstrate its superiority in avoiding learning spurious correlations compared with other robust RL counterparts.We also design an empirical algorithm to learn the robust optimal policy for RSC-MDPs, which outperforms all baselines in eight realistic self-driving and manipulation tasks.Please refer to the website for more details.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL), aiming to learn a policy to maximize cumulative reward through interactions, has been successfully applied to a wide range of tasks such as language generation [1], game playing [2], autonomous driving [3], etc.While standard RL has achieved remarkable success in simulated environments, a growing trend in RL is to address another critical concern -robustnesswith the hope that the learned policy still performs well when the deployed (test) environment deviates from the nominal one used for training [4].Robustness is highly desirable since the performance of the learned policy could significantly deteriorate due to the uncertainty and variations of the test environment induced by random perturbation, rare events, or even malicious attacks [5,6].</p>
<p>Despite various types of uncertainty that have been investigated in RL, this work focuses on the uncertainty of the environment with semantic meanings resulting from some unobserved underlying variables.Such environment uncertainty, denoted as structured uncertainty, is motivated by innumerable real-world applications but still receives little attention in sequential decision-making tasks [7].To specify the phenomenon of structured uncertainty, let us consider a concrete example (illustrated in Figure 1) in a driving scenario, where a shift between training and test environments caused by an unobserved confounder can potentially lead to a severe safety issue.Specifically, the observations brightness and traffic density do not have cause and effect on each other but are controlled by a confounder (i.e.sun and human activity) that is usually unobserved2 to the agent.During training, the agent could memorize the spurious state correlation between brightness and traffic density, i.e., the traffic is heavy during the daytime but light at night.However, such correlation could be problematic during testing when the value of the confounder deviates from the training one, e.g., the traffic becomes heavy at night due to special events (human activity changes), as shown at the bottom of Figure 1.Consequently, the policy dominated by the spurious correlation in training fails on out-of-distribution samples (observations of heavy traffic at night) in the test scenarios.The failure of the driving example in Figure 1 is attributed to the widespread and harmful spurious correlation, namely, the learned policy is not robust to the structured uncertainty of the test environment caused by the unobserved confounder.However, ensuring robustness to structured uncertainty is challenging since the targeted uncertain regionthe structured uncertainty set of the environment -is carved by the unknown causal effect of the unobserved confounder, and thus hard to characterize.In contrast, prior works concerning robustness in RL [8] usually consider a homogeneous and structure-agnostic uncertainty set around the state [9,6,10], action [11,12], or the training environment [13][14][15] measured by some heuristic functions [9,15,8] to account for unstructured random noise or small perturbations.Consequently, these prior works could not cope with the structured uncertainty since their uncertainty set is different from and cannot tightly cover the desired structured uncertainty set, which could be heterogeneous and allow for potentially large deviations between the training and test environments.</p>
<p>In this work, to address the structured uncertainty, we first propose a general RL formulation called State-confounded Markov decision processes (SC-MDPs), which model the possible causal effect of the unobserved confounder in an RL task from a causal perspective.SC-MDPs better explain the reason for semantic shifts in the state space than traditional MDPs.Then, we formulate the problem of seeking robustness to structured uncertainty as solving Robust SC-MDPs (RSC-MDPs), which optimizes the worst performance when the distribution of the unobserved confounder lies in some uncertainty set.The key contributions of this work are summarized as follows.</p>
<p>• We propose a new type of robustness with respect to structured uncertainty to address spurious correlation in RL and provide a formal mathematical formulation called RSC-MDPs, which are well-motivated by ubiquitous real-world applications.• We theoretically justify the advantage of the proposed RSC-MDP framework against structured uncertainty over the prior formulation in robust RL without semantic information.• We implement an empirical algorithm to find the optimal policy of RSC-MDPs and show that it outperforms the baselines on eight real-world tasks in manipulation and self-driving.</p>
<p>Preliminary and Limitations of Robust RL</p>
<p>In this section, we first introduce the preliminary formulation of standard RL and then discuss a natural type of robustness that is widely considered in the RL literature and most related to this work -robust RL.</p>
<p>Standard Markov decision processes (MDPs</p>
<p>).An episodic finite-horizon standard MDP is represented by M = S, A, T, r, P , where S ⊆ R n and A ⊆ R
(s) = E π,P T k=t r k (s k , a k ) | s k = s and Q π,P t (s, a) = E π,P T k=t r k (s k , a k ) | s t = s, a t = a ,
where the expectation is taken over the sample trajectory {(s t , a t )} 1≤t≤T generated following a t ∼ π t (• | s t ) and s t+1 ∼ P t (• | s t , a t ).</p>
<p>Robust Markov decision processes (RMDPs).</p>
<p>As a robust variant of standard MDPs motivated by distributionally robust optimization, RMDP is a natural formulation to promote robustness to the uncertainty of the transition probability kernel [13,15], represented as M rob = S, A, T, r, U σ (P 0 ) .Here, we reuse the definitions of S, A, T, r in standard MDPs, and denote U σ (P 0 ) as an uncertainty set of probability transition kernels centered around a nominal transition kernel P 0 = {P 0 t } 1≤t≤T measured by some 'distance' function ρ with radius σ.In particular, the uncertainty set obeying the (s, a)-rectangularity [16] can be defined over all (s, a) state-action pairs at each time step t as
U σ (P 0 ) := ⊗ U σ (P 0 t,s,a ), U σ (P 0 t,s,a ) := P t,s,a ∈ ∆(S) : ρ P t,s,a , P 0 t,s,a ≤ σ ,(1)
where ⊗ denotes the Cartesian product.Here, P t,s,a := P t (• | s, a) ∈ ∆(S) and P 0 t,s,a := P 0 t (• | s, a) ∈ ∆(S) denote the transition kernel P t or P 0 t at each state-action pair (s, a) respectively.Consequently, the next state s t+1 follows s t+1 ∼ P t (• | s t , a t ) for any P t ∈ U σ (P 0 t,st,at ), namely, s t+1 can be generated from any transition kernel belonging to the uncertainty set U σ (P 0 t,st,at ) rather than a fixed one in standard MDPs.As a result, for any policy π, the corresponding robust value function and robust Q function are defined as
V π,σ t (s) := inf P ∈U σ (P 0 ) V π,P t (s), Q π,σ t (s, a) := inf P ∈U σ (P 0 ) Q π,P t (s, a),(2)
which characterize the cumulative reward in the worst case when the transition kernel is within the uncertainty set U σ (P 0 ).Using samples generated from the nominal transition kernel P 0 , the goal of RMDPs is to find an optimal robust policy that maximizes V π,σ 1 when t = 1, i.e., perform optimally in the worst case when the transition kernel of the test environment lies in a prescribed uncertainty set U σ (P 0 ).</p>
<p>Lack of semantic information in RMDPs.</p>
<p>In spite of the rich literature on robustness in RL, prior works usually hedge against the uncertainty induced by unstructured random noise or small perturbations, specified as a small and homogeneous uncertainty set around the nominal one.For instance, in RMDPs, people usually prescribe the uncertainty set of the transition kernel using a heuristic and simple function ρ with a relatively small σ.However, the unknown uncertainty in the real world could have a complicated and semantic structure that cannot be well-covered by a homogeneous ball regardless of the choice of the uncertainty radius σ, leading to either over conservative policy (when σ is large) or insufficient robustness (when σ is small).Altogether, we obtain the natural motivation of this work: How to formulate such structured uncertainty and ensure robustness against it?</p>
<p>Robust RL against Structured Uncertainty from a Causal Perspective</p>
<p>To describe structured uncertainty, we choose to study MDPs from a causal perspective with a basic concept called the structural causal model (SCM).Armed with the concept, we formulate State-confounded MDPs -a broader set of MDPs in the face of the unobserved confounder in the state space.Next, we provide the main formulation considered in this work -robust state-confounded MDPs, which promote robustness to structured uncertainty.</p>
<p>Structural causal model.We denote a structural causal model (SCM) [17] by a tuple {X, Y, F, P x }, where X is the set of exogenous (unobserved) variables, Y is the set of endogenous (observed) variables, and P x is the distribution of all the exogenous variables.Here, F is the set of structural functions capturing the causal relations between X and Y such that for each variable y i ∈ Y , f i ∈ F is defined as y i ← f i PA(y i ), x i , where x i ⊆ X and PA(y i ) ⊆ Y \ y i denotes the parents of the node y i .We say that a pair of variables y i and y j are confounded by a variable C (confounder) if they are both caused by C, i.e., C ∈ PA(y i ) and C ∈ PA(y j ).When two variables y i and y j do not have direct causality, they are still correlated if they are confounded, in which case this correlation is called spurious correlation.t means the first dimension of s t .s t ′ is a shorthand for s t+1 .In SC-MDP, the orange line represents the backdoor path from state s 1 t ′ to action a t ′ opened by the confounder c t , which makes the learned policy π rely on the value of c t .</p>
<p>State-confounded MDPs (SC-MDPs)</p>
<p>We now present state-confounded MDPs (SC-MDPs), whose probabilistic graph is illustrated in Figure 2(a) with a comparison to standard MDPs in Figure 2(b).Besides the components in standard MDPs M = S, A, T, r , we introduce a set of unobserved confounder C s = {c t } 1≤t≤T , where c t ∈ C denotes the confounder that is generated from some unknown but fixed distribution P c t at time step t, i.e., c t ∼ P c t ∈ ∆(C).To characterize the causal effect of the confounder C s on the state dynamic, we resort to an SCM, where C s is the set of exogenous (unobserved) confounder and endogenous variables include all dimensions of states {s i t } 1≤i≤n,1≤t≤T , and actions {a t } 1≤t≤T .Specifically, the structural function F is considered as {P i t } 1≤i≤n,1≤t≤T -the transition from the current state s t , action a t and the confounder c t to each dimension of the next state s i t+1 for all time steps, i.e., s i t+1 ∼ P i t (• | s t , a t , c t ).Notably, the specified SCM does not confound the reward, i.e., r t (s t , a t ) does not depend on the confounder c t .</p>
<p>Armed with the above SCM, denoting P c := {P c t }, we can introduce state-confounded MDPs (SC-MDPs) represented by M sc = S, A, T, r, C, {P i t }, P c (Figure 2(a)).A policy is denoted as π = {π t }, where each π t results in an intervention (possibly stochastic) that sets a t ∼ π t (• | s t ) at time step t regardless of the value of the confounder.</p>
<p>State-confounded value function and optimal policy.Given s t , the causal effect of a t on the next state s t+1 plays an important role in characterizing value function/Q-function.To ensure the identifiability of the causal effect, the confounder c t are assumed to obey the backdoor criterion [17,18], leading to the following state-confounded value function (SC-value function) and stateconfounded Q-function (SC-Q function) [19]:
V π,P c t (s) = E π,P c T k=t r k (s k , a k ) | s t = s; c k ∼ P c k , s i k+1 ∼ P i k (• | s k , a k , c k ) , Q π,P c t (s, a) = E π,P c T k=t r k (s k , a k ) | s t = s, a t = a; c k ∼ P c k , s i k+1 ∼ P i k (• | s k , a k , c k ) . (3)
Remark 1.Note that the proposed SC-MDPs serve as a general formulation for a broad family of RL problems that include standard MDPs as a special case.Specifically, any standard MDP M = S, A, P, T, r can be equivalently represented by at least one SC-MDP M sc = S, A, T, r, C, {P i t }, P c as long as E ct∼P c t
P i t (• | s t , a t , c t ) = P (• | s t , a t ) i for all 1 ≤ i ≤ n, 1 ≤ t ≤ T .</p>
<p>Robust state-confounded MDPs (RSC-MDPs)</p>
<p>In this work, we consider robust state-confounded MDPs (RSC-MDPs) -a variant of SC-MDPs promoting the robustness to the uncertainty of the unobserved confounder distribution P c , denoted by M sc-rob = S, A, T, r, C, {P i t }, U σ (P c ) .Here, the perturbed distribution of the unobserved confounder is assumed in an uncertainty set U σ (P c ) centered around the nominal distribution P c with radius σ measured by some 'distance' function ρ : ∆(C) × ∆(C) → R + , i.e., U σ (P c ) := ⊗ U σ (P c t ), U σ (P c t ) := {P ∈ ∆(C) : ρ (P, P c t ) ≤ σ} .</p>
<p>Consequently, the corresponding robust SC-value function and robust SC-Q function are defined as
V π,σ t (s) := inf P ∈U σ (P c ) V π,P t (s), Q π,σ t (s, a) := inf P ∈U σ (P c ) Q π,P t (s, a),(5)
representing the worst-case cumulative rewards when the confounder distribution lies in the uncertainty set U σ (P c ).</p>
<p>Then a natural question is: does there exist an optimal policy that maximizes the robust SC-value function V π,σ t for any RSC-MDP so that we can target to learn?To answer this, we introduce the following theorem that ensures the existence of the optimal policy for all RSC-MDPs.The proof can be found in Appendix C.1.Theorem 1 (Existence of an optimal policy).Let Π be the set of all non-stationary and stochastic policies.Consider any RSC-MDP, there exists at least one optimal policy π sc,⋆ = {π sc,⋆ t } 1≤t≤T such that for all (s, a) ∈ S × A and 1 ≤ t ≤ T , one has
V π sc,⋆ ,σ t (s) = V ⋆,σ t (s) := sup π∈Π V π,σ t (s) and Q π sc,⋆ ,σ t (s, a) = Q ⋆,σ t (s, a) := sup π∈Π Q π,σ t (s, a).
In addition, RSC-MDPs also possess benign properties similar to RMDPs such that for any policy π and the robust optimal policy π sc,⋆ , the corresponding robust SC Bellman consistency equation and robust SC Bellman optimality equation are also satisfied (specified in Appendix C.3.3).</p>
<p>Goal.Based on all the definitions and analysis above, this work aims to find an optimal policy for RSC-MDPs that maximizes the robust SC-value function in (5), yielding optimal performance in the worst case when the unobserved confounder distribution falls into an uncertainty set U σ (P c ).</p>
<p>Advantages of RSC-MDPs over traditional RMDPs</p>
<p>The most relevant robust RL formulation to ours is RMDPs, which has been introduced in Section 2.</p>
<p>Here, we provide a thorough comparison between RMDPs and our RSC-MDPs with theoretical justifications, and leave the comparisons and connections to other related formulations in Figure 2 and Appendix B.1 due to space limits.To begin, at each time step t, RMDPs explicitly introduce uncertainty to the transition probability kernels, while our RSC-MDPs add uncertainty to the transition kernels in a latent (and hence more structured) manner via perturbing the unobserved confounder that partly determines the transition kernels.As an example, imagining the true uncertainty set encountered in the real world is illustrated as the blue region in Figure 3, which could have a complicated structure.Since the uncertainty set in RMDPs is homogeneous (illustrated by the green circles), one often faces the dilemma of being either too conservative (when σ is large) or too reckless (when σ is small).In contrast, the proposed RSC-MDPs -shown in Figure 3(b) -take advantage of the structured uncertainty set (illustrated by the orange region) enabled by the underlying SCM, which can potentially lead to much better estimation of the true uncertainty set.Specifically, the varying unobserved confounder induces diverse perturbation to different portions of the state through the structural causal function, enabling heterogeneous and structural uncertainty sets over the state space.</p>
<p>Theoretical guarantees of RSC-MDPs: advantages of structured uncertainty.To theoretically understand the advantages of the proposed robust formulation of RSC-MDPs with comparison to prior works, especially RMDPs, the following theorem verifies that RSC-MDPs enable additional robustness against semantic attack besides small model perturbation or noise considered in RMDPs.</p>
<p>The proof is postponed to Appendix C.2.</p>
<p>Theorem 2. Consider any T ≥ 2. Consider some standard MDPs M = S, A, P 0 , T, r , equivalently represented as an SC-MDP M sc = S, A, T, r, C, {P i t }, P c } with C := {0, 1}, and total variation as the 'distance' function ρ to measure the uncertainty set (the admissible uncertainty level obeys σ ∈ [0, 1]).For the corresponding RMDP M rob with the uncertainty set U σ1 (P 0 ), and the proposed RSC-MDP M sc-rob = S, A, T, r, C, {P i t }, U σ2 (P c ) , the optimal robust policy π ⋆,σ1</p>
<p>RMDP associated with M rob and π ⋆,σ2 RSC associated with M sc-rob obey: given σ 2 ∈ 1 2 , 1 , there exist RSC-MDPs with some initial state distribution ϕ such that
V π ⋆,σ 2 RSC ,σ2 1 (ϕ) − V π ⋆,σ 1 RMDP ,σ2 1 (ϕ) ≥ T 8 , ∀σ 1 ∈ [0, 1].(6)
In words, Theorem 2 reveals a fact about the proposed RSC-MDPs: RSC-MDPs could succeed in intense semantic attacks while RMDPs fail.As shown by ( 6), when fierce semantic shifts appear between the training and test scenarios -perturbing the unobserved confounder in a large uncertainty set U σ2 (P c ), solving RSC-MDPs with π ⋆,σ2 RSC succeeds in testing while π ⋆,σ1 RMDP trained by solving RMDPs can fail catastrophically.The proof is achieved by constructing hard instances of RSC-MDPs that RMDPs could not cope with due to inherent limitations.Moreover, this advantage of RSC-MDPs is consistent with and verified by the empirical performance evaluation in Section 5.3 R1. 4 An Empirical Algorithm to Solve RSC-MDPs: RSC-SAC When addressing distributionally robust problems in RMDPs, the worst case is typically defined within a prescribed uncertainty set in a clear and implementation-friendly manner, allowing for iterative or analytical solutions.However, solving RSC-MDPs could be challenging as the structured uncertainty set is induced by the causal effect of perturbing the confounder.The precise characterization of this structured uncertainty set is difficult since neither the unobserved confounder nor the true causal graph of the observable variables is accessible, both of which are necessary for intervention or counterfactual reasoning.Therefore, we choose to approximate the causal effect of perturbing the confounder by learning from the data collected during training.</p>
<p>In this section, we propose an intuitive yet effective empirical approach named RSC-SAC for solving RSC-MDPs, which is outlined in Algorithm 1.We first estimate the effect of perturbing the distribution P c of the confounder to generate new states (Section 4.1).Then, we learn the structural causal model P i t to predict rewards and the next states given the perturbed states (Section 4.2).By combining these two components, we construct a data generator capable of simulating novel transitions (s t , a t , r t , s t+1 ) from the structured uncertainty set.To learn the optimal policy, we construct the data buffer with a mixture of the original data and the generated data and then use the Soft Actor-Critic (SAC) algorithm [20] to optimize the policy.</p>
<p>Distribution of confounder</p>
<p>As we have no prior knowledge about the confounders, we choose to approximate the effect of perturbing them without explicitly estimating the distribution P c .We first randomly select a dimension i from the state s t to apply perturbation and then assign the dimension i of s t with a heuristic rule.We select the value from another sample s k that has the most different value from s t in dimension i and the most similar value to s t in the remaining dimensions.Formally, this process solves the following optimization problem to select sample k from a batch of K samples:
s i t ← s i k , k = arg max ∥s i t − s i k ∥ 2 2 ¬i ∥s ¬i t − s ¬i k ∥ 2 2 , k ∈ {1, ..., K}(7)
where s i t and s ¬i t means dimension i of s t and other dimensions of s t except for i, respectively.Intuitively, permuting the dimension of two samples breaks the spurious correlation and remains the most semantic meaning of the state space.However, this permutation sometimes also breaks the true cause and effect between dimensions, leading to a performance drop.The trade-off between robustness and performance [21] is a long-standing dilemma in the robust optimization framework, which we will leave to future work.</p>
<p>Learning of structural causal model</p>
<p>With the perturbed state s t , we then learn an SCM to predict the next state and reward considering the effect of the action on the previous state.This model contains a causal graph to achieve better generalization to unseen state-action pairs.Specifically, we simultaneously learn the model parameter and discover the underlying causal graph in a fully differentiable way with (ŝ t+1 , rt ) ∼ P θ (s t , a t , G ϕ ), where θ is the parameter of the neural network of the dynamic model and ϕ ∈ R (n+d A )×(n+1) is the parameter to represent causal graph G between {s t , a t } and {s t+1 , r t }.This graph is represented by a binary adjacency matrix G, where 1/0 means the existence/absence of an edge.P θ has an encoder-decoder structure with matrix G as an intermediate linear transformation.The encoder takes state and action in and outputs features f e ∈ R (n+d A )×d f for each dimension, where d f is the dimension of the feature.Then, the causal graph is multiplied to generate the feature for the decoder
f d = f T e G ∈ R d f ×(n+1)
. The decoder takes in f d and outputs the next state and reward.The detailed architecture of this causal transition model can be found in Appendix D.1.</p>
<p>The objective for training this model consists of two parts, one is the supervision signal from collected data ∥s t+1 − ŝt+1 ∥ 2 2 + ∥r t − rt ∥ 2 2 , and the other is a penalty term λ∥G∥ p with weight λ to encourage the sparsity of the matrix G.The penalty is important to break the spurious correlation between dimensions of state since it forces the model to eliminate unnecessary inputs for prediction.</p>
<p>Experiments and Evaluation</p>
<p>In this section, we first provide a benchmark consisting of eight environments with spurious correlations, which may be of independent interest to robust RL.Then we evaluate the proposed algorithm RSC-SAC with comparisons to prior robust algorithms in RL.</p>
<p>Tasks with spurious correlation</p>
<p>To the best of our knowledge, no existing benchmark addresses the issues of spurious correlation in the state space of RL.To bridge the gap, we design a benchmark consisting of eight novel tasks in self-driving and manipulation domains using the Carla [22] and Robosuite [23] platforms (shown in Figure 4).Tasks are designed to include spurious correlations in terms of human common sense, which is ubiquitous in decision-making applications and could cause safety issues.We categorize the tasks into distraction correlation and composition correlation according to the type of spurious correlation.We specify these two types of correlation below and leave the full descriptions of the tasks in Appendix D.2.</p>
<p>• Distraction correlation is between task-relevant and task-irrelevant portions of the state.The task-irrelevant part could distract the policy model from learning important features and lead to a performance drop.A typical method to avoid distraction is background augmentation [24,25].We design four tasks with this category of correlation, i.e., Lift, Wipe, Brightness, and CarType.• Composition correlation is between two task-relevant portions of the state.This correlation usually exists in compositional generalization, where states are re-composed to form novel tasks during testing.Typical examples are multi-task RL [26,27] and hierarchical RL [28,29].We design four tasks with this category of correlation, i.e., Stack, Door, Behavior, and Crossing.We generate the samples to cover the uncertainty set over the state space by adding perturbation around the nominal states that follows two distribution, i.e., uniform distribution (RMDP-U) and Gaussian distribution (RMDP-G).Solving SA-MDP: We compare ATLA [6], a strong algorithm that generates adversarial states using an optimal adversary within the uncertainty set.Invariant feature learning: We choose DBC [30] that learns invariant features using the bi-simulation metric [31] and [32] (Active) that actively sample uncertain transitions to reduce causal confusion.Counterfactual data augmentation: We select MoCoDA [33], which identifies local causality to switch components and generate counterfactual samples to cover the targeted uncertainty set.We adapt this algorithm using an approximate causal graph rather than the true causal graph.</p>
<p>Results and Analysis</p>
<p>To comprehensively evaluate the performance of the proposed method RSC-SAC, we conduct experiments to answer the following questions: Q1.Can RSC-SAC eliminate the harmful effect of spurious correlation in learned policy?Q2.Does the robustness of RSC-SAC only come from the sparsity of model?Q3.How does RSC-SAC perform in the nominal environments compared to non-robust algorithms?Q4.Which module is critical in our empirical algorithm?Q5.Is RSC-SAC robust to other types of uncertainty and model perturbation?Q6.How does RSC-SAC balance the tradeoff between performance and robustness?We analyze the results and answer these questions in the following.1, is that although RMDP-G, RMDP-U, and ATLA are trained desired to be robust against small perturbations, their performance tends to drop more than non-robust SAC in some tasks.This indicates that using the samples generated from the traditional robust algorithms could harm the policy performance when the test environment is outside of the prescribed uncertainty set considered in the robust algorithms.R2.Sparsity of the model is only one reason for the robustness of RSC-SAC.As existing literature shows [34], sparsity regularization benefits the elimination of spurious correlation and causal confusion.Therefore, we compare our method with a sparse version of SAC (SAC-Sparse): we add an additional penalty α|W | 1 during the optimization, where W is the parameter of the first linear layer of the policy and value networks and α is the weight.The results of both Distraction and Composition are shown in Figure 6.We have two important findings based on the results: (1) The sparsity improves the robustness of SAC in the setting of distraction spurious correlation, which is consistent with the findings in [34].(2) The sparsity does not help with the composition type of spurious correlation, which indicates that purely using sparsity regularization cannot explain the improvement of our RSC-SAC.In fact, the semantic perturbation in our method plays an important role in augmenting the composition generalization.</p>
<p>R3. RSC-SAC maintains great performance in the nominal environments.Previous literature [21] finds out that there usually exists a trade-off between the performance in the nominal environment and the robustness against uncertainty.To evaluate the performance of RSC-SAC in the nominal environment, we conduct experiments and summarize results in Table 2, which shows that RSC-SAC still performs well in the training environment.Additionally, the training curves are displayed in Figure 5, showing that RSC-SAC achieves similar rewards compared to non-robust SAC although converges slower than it.R4.Both the distribution of confounder and the structural causal model are critical.To assess the impact of each module in our algorithm, we conduct three additional ablation studies (in Table 3), where we remove the causal graph G ϕ , the transition model P θ , and the distribution of the confounder P c respectively.The results demonstrate that the learnable causal graph G ϕ is critical for the performance that enhances the prediction of the next state and reward, thereby facilitating the generation of high-quality next states with current perturbed states.The transition model without G ϕ may still retain numerous spurious correlations, resulting in a performance drop similar to the one without P θ , which does not alter the next state and reward.In the third row of Table 3, the performance drop indicates that the confounder P c also plays a crucial role in preserving semantic meaning and avoiding policy training distractions.R6.RSC-SAC keeps good performance and robustness for a wide range of β.As shown in Figure 7, the proposed RSC-SAC performs well in both nominal and shifted settings -keeping good performance in the nominal setting and achieving robustness, for a large range of (20%-70%).When the ratio of perturbed data is very small (1%), RSC-SAC almost achieves the same results as vanilla SAC in nominal settings and there is no robustness in shifted settings.As it increases (considering more robustness), the performance of RSC-SAC in the nominal setting gradually gets worse, while reversely gets better in the shifted settings (more robust).However, when the ratio is too large (&gt;80%), the performances of RSC-SAC in both settings degrade a lot, since the policy is too conservative so that fails in all environments.</p>
<p>Conclusion and Limitation</p>
<p>This work focuses on robust reinforcement learning against spurious correlation in state space, which broadly exists in (sequential) decision-making tasks.We propose robust SC-MDPs as a general framework to break spurious correlations by perturbing the value of unobserved confounders.We not only theoretically show the advantages of the framework compared to existing robust works in RL, but also design an empirical algorithm to solve robust SC-MDPs by approximating the causal effect of the confounder perturbation.The experimental results demonstrate that our algorithm is robust to spurious correlation -outperforms the baselines when the value of the confounder in the test environment derivates from the training one.It is important to note that the empirical algorithm we propose is evaluated only for low-dimensional states.However, the entire framework can be extended in the future to accommodate high-dimensional states by leveraging powerful generative models with disentanglement capabilities [35] and state abstraction techniques [36].</p>
<p>A Broader Impact</p>
<p>Incorporating causality into reinforcement learning methods increases the interpretability of artificial intelligence, which helps humans understand the underlying mechanism of algorithms and check the source of failures.However, the learned causal transition model may contain human-readable private information about the environment, which could raise privacy issues.To mitigate this potential negative societal impact, the causal transition model needs to be encrypted and only accessible to algorithms and trustworthy users.</p>
<p>B Other Related Works</p>
<p>In this section, besides the most related formulation, robust RL introduced in Sec 3.3, we also introduce some other related RL problem formulations partially shown in Figure 3.Then, we limit our discussion to mainly two lines of work that are related to ours: (1) promoting robustness in RL;</p>
<p>(2) concerning the spurious correlation issues in RL.</p>
<p>B.1 Related RL formulations</p>
<p>Robustness to noisy state: POMDPs and SA-MDPs.State-noisy MDPs refer to the RL problem that the agent can only access and choose the action based on a noisy observation rather than the true state at each step, including two existing types of problems: Partially observable MDPs (POMDPs) and state-adversarial MDPs (SA-MDPs), shown in Figure 3(b).In particular, at each step t, in POMDPs, the observation o t is generated by a fixed probability transition O(• | s t ) (we refer to the case that o t only depends on the state s t but not action); for state-adversarial MDPs, the observation is an adversary ν(s t ) against and thus determined by the conducted policy, leading to the worst performance by perturbing the state in a small set around itself.To defend the state perturbation, both POMDPs, and SA-MDPs are indeed robust to the noisy observation, or called agent-observed state, but not the real state that transitions to the environment and next steps.In contrast, our RSC-MDPs propose the robustness to the real state shift that will directly transition to the next state in the environment, involving additional challenges induced by the appearance of out-of-distribution states.</p>
<p>Robustness to unobserved confounder: MDPUC and confounded MDPs.To address the misleading spurious correlations hidden in components of RL, people formulate RL problems as MDPs with some additional components -unobserved confounders.In particular, the Markov decision process with unobserved confounders (MDPUC) [37] serves as a general framework to concern all types of possible spurious correlations in RL problems -at each step, the state, action, and reward are all possibly influenced by some unobserved confounder, shown in Figure 2(d); confounded MDPs [19] mainly concerns the misleading correlation between the current action and the next state, illustrated in Figure 3(e).The proposed state-confounded MDPs (SC-MDPs) can be seen as a specified type of MDPUC that focuses on breaking the spurious correlation between different parts of the state space itself (different from confounded MDPs which consider the correlation between action and next state), motivated by various real-world applications in self-driving and control tasks.In addition, the proposed formulation is more flexible and can work in both online and offline RL settings.[38] is basically a set of standard MDPs sharing the same state and action space but specified by different contexts within a context space.In particular, the transition kernel, reward, and action of a CMDP are all determined by a (possibly unknown) fixed context.The proposed robust state-confounded MDPs (RSC-MDPs) are similar to CMDPs if we cast the unobserved confounder as the context in CMDPs, while different in two aspects: (1) In a CMDP, the context is fixed throughout an episode, while the unobserved confounder in RSC-MDPs can vary as {c t } 1≤t≤T ; (2) In the online setting, the goal of CMDP is to beat the optimal policy depending on the context, while RSC-MDPs seek to learn the optimal policy that does not depend on the confounder {c t } 1≤t≤T .</p>
<p>Contexual MDPs (CDMPs). A contextual MDP (CMDP)</p>
<p>B.2 Related literature of robustness in RL</p>
<p>Robust RL (robust MDPs).Concerning the robust issues in RL, a large portion of works focus on robust RL with explicit uncertainty of the transition kernel, which is well-posed and a natural way to consider the uncertainty of the environment [13,[39][40][41][42][43][44][45][46][47][48].However, to define the uncertainty set for the environment, most existing works use task structure-agnostic and heuristic 'distance' such as R-contamination, KL divergence, χ 2 , and total variation [49, 50, 14, 51, 52, 15, 49, 53-58, 49, 59, 48, 60-62] to measure the shift between the training and test transition kernel, leading to a homogeneous (almost structure-free) uncertainty set around the state space.In contrast, we consider a more general uncertainty set that enables the robustness to a task-dependent heterogeneous uncertainty set shaped by unobserved confounder and causal structure, in order to break the spurious correlation hidden in different parts of the state space.</p>
<p>Robustness in RL.Despite the remarkable success that standard RL has achieved, current RL algorithms are still limited since the agent is vulnerable if the deployed environment is subject to uncertainty and even structural changes.To address these challenges, a recent line of RL works begins to concern robustness to the uncertainty or changes over different components of MDPsstate, action, reward, and transition kernel, where a review [8] can be referred to.Besides robust RL framework concerning the shift of the transition kernel and reward, to promote robustness in RL, there exist various works [11,12] that consider the robustness to action uncertainty, i.e., the deployed action in the environment is distorted by an adversarial agent smoothly or circumstantially; some works [9,6,10,[63][64][65] investigate the robustness to the state uncertainty including but not limited to the introduced POMDPs and SA-MDPs in Appendix B.1, where the agent chooses the action based on observation -the perturbed state determined by some restricted noise or adversarial attack.The proposed RSC-MDPs can be regarded as addressing the state uncertainty since the shift of the unobserved confounder leads to state perturbation.In contrast, RSC-MDPs consider the out-of-distribution of the real state that will directly influence the subsequent transition in the environment, but not the observation in POMDPs and SA-MDPs that will not directly influence the environment.</p>
<p>B.3 Related literature of spurious correlation in RL</p>
<p>Confounder in RL.These works mainly focus on the confounder between action (treatment) and state (effect), which is a long-standing problem that exists in the causal inference area.However, we find that the confounder may cause problems from another perspective, where the confounder is built upon different dimensions of the state variable.Some people focus on the confounder between action and state, which is common in offline settings since the dataset is fixed and intervention is not allowed.But in the online setting, actions are controlled by an agent and intervention is available to eliminate spurious correlation.[66] reduces the spurious correlation between action and state in the offline setting.[67] deal with environment-irrelevant white noise; possible shift + causal [68].The confounder problem is usually easy to solve since agents can interact with the environment to do interventions.However, different from most existing settings, we find that even with the capability of intervention, the confounding between dimensions in states cannot be fully eliminated.Then the learned policy is heavily influenced if these confounders change during testing.</p>
<p>Invariant Feature learning.The problem of spurious correlation has attracted attention in the supervised learning area for a long time and many solutions are proposed to learn invariant features to eliminate spurious correlations.A general framework to remedy the ignorance of spurious correlation in empirical risk minimization (ERM) is invariant risk minimization (IRM) [69].Other works tackle this problem with group distributional robustness [70], adversarial robustness [71], and contrastive learning [72].These methods are also adapted to sequential settings.The idea of increasing the robustness of RL agents by training agents on multiple environments has been shown in previous works [73,30,30].However, a shared assumption among these methods is that multiple environments with different values of confounder are accessible, which is not always true in the real world.</p>
<p>Counterfactual Data Augmentation in RL.One way to simulate multiple environments is data augmentation.However, most data augmentation works [24,74,25,[75][76][77][78] apply image transformation to raw inputs, which requires strong domain knowledge for image manipulation and cannot be applied to other types of inputs.In RL, the dynamic model and reward model follow certain causal structures, which allow counterfactual generation of new transitions based on the collected samples.This line of work, named counterfactual data augmentation, is very close to this work.Deep generative models [79] and adversarial examples [80] are considered for the generation to improve sample efficiency in model-based RL.CoDA [81] and MocoDA [33] leverage the concept of locally factored dynamics to randomly stitch components from different trajectories.However, the assumption of local causality may be limited.Domain Randomization.If we are allowed to control the data generation process, e.g., the underlying mechanism of the simulator, we can apply the golden rule in causality -Randomized Controlled Trial (RCT).The well-known technique, domain randomization [82], exactly follows the idea of RCT, which randomly perturbs the internal state of the experiment in simulators.Later literature follows this direction and develops variants including randomization guided by downstream tasks in the target domain [83,84], randomization to match real-world distributions [85,86], and randomization to minimize data divergence [87].However, it is usually impossible to randomly manipulate internal states in most situations in the real world.In addition, determining which variables to randomize is even harder given so many factors in complex systems.</p>
<p>Discovering Spurious Correlations Detecting spurious correlations helps models remove features that are harmful to generalization.Usually, domain knowledge is required to find such correlations [88][89][90].However, when prior knowledge is accessible, techniques such as clustering can also be used to reveal spurious attributes [37,91,92].When human inspection is available, recent works [93][94][95] also use explainability techniques to find spurious correlations.Another area for discovery is conceptlevel and interactive debugging [96,97], which leverage concepts or human feedback to perform debugging.</p>
<p>C Theoretical Analyses</p>
<p>C.1 Proof of Theorem 1</p>
<p>In this section, we verify the existence of an optimal policy of the proposed RSC-MDPs, involving additional components -confounder C s and the infimum optimization problems with comparisons to standard MDPs [98].</p>
<p>To begin with, we recall that the goal is to find a policy π = { π t } 1≤t≤T ∈ Π such that for all (s, a) ∈ S × A:
V π,σ t (s) = V ⋆,σ t (s) := sup π∈Π V π,σ t (s) and Q π,σ t (s, a) = Q ⋆,σ t (s, a) := sup π∈Π Q π,σ t (s, a),(8)
which we called an optimal policy.Towards this, we start from the first claim in (8).</p>
<p>Step 1: Introducing additional notations.Before proceeding, we let {S t , A t , R t , C t } denote the random variables -state, action, reward, and confounder, at time step t for all 1 ≤ t ≤ T .Then invoking the Markov properties, we know that conditioned on current state s t , the future state, action, and reward are all independent from the previous s 1 , a 1 , r 1 , c 1 , • • • , s t−1 , a t−1 , r t−1 , c t−1 .In addition, we represent P t ∈ ∆(C) as some distribution of confounder at time step t, for all 1 ≤ t ≤ T .</p>
<p>For convenience, we introduce the following notation that is defined over time step t ≤ k ≤ T :
∀1 ≤ t ≤ T : P +t := ⊗ t≤k≤T P k and U σ (P c +t ) := ⊗ t≤k≤T U σ (P c k ),(9)
which represent some collections of variables from time step t to the end of the episode.In addition, recall that the transition kernel from time step t to t + 1 is denoted as
s i t+1 ∼ P i t (• | s t , a t , c t ) for i ∈ 1, 2E πt E ct∼Pt E st+1 V ⋆,σ t+1 (s t+1 ) .(10)
Armed with these definitions and notations, for any
(t, s) ∈ {1, 2, • • • , T } × S, one has V ⋆,σ t(s) (iE πt E ct∼Pt E st+1 T k=t+1 r k (s k , a k ) | π, P +(t+1) , (S t , A t , R t , C t , S t+1 ) = (s, a t , r t , c t , s t+1 )
where (i) holds by the definitions in ( 5), (ii) is due to (3) and that V π,P t (s) only depends on P +t by the Markov property, (iii) follows from expressing the term of interest by moving one step ahead and E πt is taken with respect to a t ∼ π t (• | S t = s).</p>
<p>To continue, we observe that the V ⋆,σ t (s) can be further controlled as follows:
V ⋆,σ t (s) = sup π∈Π E πt [r t (s, a t )] + inf P+t∈U σ (P c +t ) E πt E ct∼Pt E st+1 T k=t+1E πt E ct∼Pt E st+1 sup π ′ ∈Π inf P +(t+1) ∈U σ (P c +(t+1) ) T k=t+1 r k (s k , a k ) | π ′ , P +(t+1) , (S t , A t , R t , C t , S t+1 ) = (s, a t , r t , c t , s t+1 ) (ii) = sup π∈Π E πt [r t (s, a t )] + inf Pt∈U σ (P c t ) E πt E ct∼Pt E st+1 sup π ′ ∈Π inf P +(t+1) ∈U σ (P c +(t+1) ) E π ′ ,P +(t+1) T k=t+1 r k (s k , a k ) = sup π∈Π E πt [r t (s, a t )] + inf Pt∈U σ (P c t ) E πt E ct∼Pt E st+1 V ⋆,σ t+1 (s t+1 ) = sup πt∈∆(A) E πt [r t (s, a t )] + inf Pt∈U σ (P c t ) E πt E ct∼Pt E st+1 V ⋆,σ t+1 (s t+1 ) = inf Pt∈U σ (P c t ) E r t (s, a t ) + E ct∼Pt E st+1 V ⋆,σ t+1 (s t+1 ) | a t = π t (s) ,(11)
where (i) holds by the operator inf P +(t+1)
∈U σ P c +(t+1)
is from π t conditioned on a fixed distribution of s t+1 , (ii) arises from the Markov property such that the rewards {r k (s k , a k )} t+1≤k≤T conditioned on (S t , A t , R t , C t , S t+1 ) or S t+1 are the same, and the last equality follows from the definition of π in (10).</p>
<p>Step 3: Completing the proof by applying recursion.</p>
<p>Applying (11) recursively for t + 1, • • • T , we arrive at
V ⋆,σ t (s) ≤ inf Pt∈U σ (P c t ) E r t (s, a t ) + E ct∼Pt E st+1 V ⋆,σ t+1 (s t+1 ) | a t = π t (s) ≤ inf Pt∈U σ (P c t ) inf Pt+1∈U σ (P c t+1 ) E r t (s, a t ) + E ct∼Pt E st+1 r t+1 (s t+1 , a t+1 ) + E ct+1∼Pt+1 E st+2 V ⋆,σ t+2 (s t+2 ) (a t , a t+1 ) = ( π t (s), π t+1 (s t+1 )) ≤ • • • ≤ inf P+t∈U σ (P c +t ) E π,P+t T k=t r k (s k , a k ) = V π,σ t (s).(12)
Observing from ( 12) that
∀s ∈ S : V ⋆,σ t (s) ≤ V π,σ t (s) ≤ sup π∈Π V π,σ t (s) = V ⋆,σ t (s),(13)
which directly verifies the first assertion in (8) V π,σ t (s) = V ⋆,σ t (s) for all s ∈ S. The second assertion in (8) can be achieved analogously.Until now, we verify that there exists at least a policy π that obeys (8), which we refer to as an optimal policy since its value is equal to or larger than any other non-stationary and stochastic policies over all states s ∈ S.</p>
<p>C.2 Proof of Theorem 2</p>
<p>We establish the proof by separating it into several key steps.</p>
<p>Armed with this transition kernel P sc , the {P i t } of the SC-MDP M sc is set to obey
P 1 (s ′ | s, a, c 1 ) = (1 − c 1 )P 0 1 (s ′ | s, a) + c 1 P sc 1 (s ′ | s, a) if s = [0, 0] 1(s ′ = s) otherwise ,(20)
which is illustrated in Fig. 8(b), and
P t (s ′ | s, a, c t ) = 1(s ′ = s), ∀(t, s, a, c t ) ∈ {2, 3, • • • , T } × S × A × C.(21)
With them in mind, we are ready to verify that the marginalized transition from the current state and action to the next state in the SC-MDP M sc is identical to the one in MDP M: for all (t, s t , a t , s t+1 ) ∈ {1, 2, • • • , T } × S × A × S:
P(s t+1 | s t , a t ) = E ct∼P c t [P t (s t+1 | s t , a t , c t )] = P t (s t+1 | s t , a t , 0) = P 0 (s t+1 | s t , a t ) (22)
where the second equality holds by the definition of P c in (17), and the last equality holds by the definitions of P (see (20) and ( 21)).</p>
<p>In summary, we verified that the standard MDP M = S, A, P 0 , T, r is equal to the above specified SC-MDP M sc .</p>
<p>Step 3: Defining corresponding RMDP and RSC-MDP.Equipped with the equivalent standard MDP M and SC-MDP M sc , we consider the robust variants of them respectively -a RMDP M rob = S, A, U σ1 (P 0 ), T, r with some uncertainty level σ 1 , and the proposed RSC-MDP M sc-rob = S, A, T, r, C, {P i t }, U σ2 (P c ) with some uncertainty level σ 2 .In this section, without loss of generality, we consider total variation as the 'distance' function ρ for the uncertainty sets of both RMDP M rob and RSC-MDP M sc-rob , i.e., for any probability vectors P ′ , P ∈ ∆(C) (or P ′ , P ∈ ∆(S)), ρ (P ′ , P ) := 1 2 ∥P ′ − P ∥ 1 .Consequently, for any uncertainty level σ ∈ [0, 1], the uncertainty set U σ1 (P 0 ) of the RMDP (see ( 1)) and U σ2 (P c ) of the RSC-MDP M sc-rob (see ( 4)) are defined as follow, respectively: U σ (P 0 ) := ⊗ U σ (P 0 t,s,a ), U σ (P 0 t,s,a ) := P t,s,a ∈ ∆(S) :
1 2 P t,s,a − P 0 t,s,a 1 ≤ σ , U σ (P c ) := ⊗ U σ (P c t ), U σ (P c t ) := P ∈ ∆(C) : 1 2 ∥P − P c t ∥ 1 ≤ σ .(23)
Step 4: Comparing between the performance of the optimal policy of RMDP M rob (π ⋆,σ1 RMDP ) and that of RSC-MDP M sc-rob (π ⋆,σ2 RSC ).To continue, we specify the robust optimal policy π ⋆,σ1</p>
<p>RMDP associated with M rob and π ⋆,σ2 RSC associated with M sc-rob and then compare their performance on RSC-MDP with some initial state distribution.</p>
<p>To begin, we introduce the following lemma about the robust optimal policy π ⋆,σ1</p>
<p>RMDP associated with the RMDP M rob .Lemma 1.For any σ 1 ∈ (0, 1], the robust optimal policy of M rob obeys ∀s ∈ S :
π ⋆,σ1 RMDP 1 (0 | s) = 1.(24a)
In addition, we characterize the robust SC-value functions of the RSC-MDP M sc-rob associated with any policy, combined with the optimal policy and its optimal robust SC-value functions, shown in the following lemma.Lemma 2. Consider any σ 2 ∈ ( 1 2 , 1] and the RSC-MDP M sc-rob = S, A, T, r, C, {P i t }, U σ2 (P c ) .For any policy π, the corresponding robust SC-value functions satisfy
V π,σ2 1 ([0, 0]) = 1 + (T − 1) inf P ∈U σ (P c 1 ) E c1∼P π 1 (0 | [0, 0])(1 − c 1 ) + π 1 (1 | [0, 0])c 1 . (25a)
In addition, the optimal robust SC-value function and the robust optimal policy π ⋆,σ2 RSC of the RMDP M sc-rob obeys:</p>
<p>Consequently, combining ( 44) and ( 45), we conclude that
V π ⋆,σ 2 RSC ,σ21([0, 0]) = V ⋆,σ2 1 ([0, 0]) = max π V π,σ2 1 ([0, 0]) = 1 + T − 1 2 .(46)</p>
<p>C.3.3 Auxiliary results of RSC-MDPs</p>
<p>It is easily verified that for any RSC-MDP M sc-rob = S, A, T, r, C, {P i t }, U σ2 (P c ) , any policy π and optimal policy π ⋆ satisfy the corresponding robust state-confounded Bellman consistency equation and Bellman optimality equation shown below, respectively:
Q π,σ
t (s, a) = r t (s, a) + inf
P ∈U σ (P c t )
E ct∼P P t,s,a,ct V π,σ t+1 , Q ⋆,σ t (s, a) = r t (s, a) + inf
P ∈U σ (P c t )
E ct∼P P t,s,a,ct V ⋆,σ t+1 ,</p>
<p>where P t,s,a,ct ∈ R 1×S such that P t,s,a,ct (s ′ ) := P t (s ′ | s, a, c t ) for s ′ ∈ S, and V ⋆,σ t (s) = sup πt∈∆(A) E πt [r t (s, a t )] + inf Pt∈U σ (P c t ) E πt E ct∼Pt P t,s,a,ct V ⋆,σ t+1 (s t+1 ) .</p>
<p>D Experiment Details</p>
<p>D.1 Architecture of the structural causal model We plot the architecture of the structural causal model we used in our method in Figure 9.In normal neural networks, the input is treated as a whole to pass through linear layers or convolution layers.However, this structure blends all information in the input, making the causal graph useless to separate cause and effect.Thus, in our model, we design an encoder that is shared across all dimensions of the input.Since different dimensions could have exactly the same values, we add a learnable position embedding to the input of the encoder.In summary, the input dimension of the encoder is 1 + d pos , where d pos is the dimension of the position embedding.</p>
<p>After the encoder, we obtain a set of independent features for each dimension of the input.We now multiply the features with a learnable binary causal graph G.The element (i, j) of the graph is sampled from a Gumbel-Softmax distribution with parameter ϕ i,j to ensure the loss function is differentiable w.r.t ϕ.</p>
<p>The multiplication of the causal graph and the input feature creates a linear combination of the input feature with respect to the causal graph.The obtained features are then passed through a decoder to predict the next state and reward.Again, the decoder is shared across all dimensions to avoid information leaking between dimensions.Position embedding is included in the input to the decoder and the output dimension of the decoder is 1.</p>
<p>D.2 Details of Tasks</p>
<p>We design four self-driving tasks in the Carla simulator [22] and four manipulation tasks in the Robosuite platform [23].All of these realistic tasks contain strong spurious correlations that are explicit to humans.We provide detailed descriptions of all these environments in the following.Brightness.The nominal environments are shown in the 1 th column of Figure 10, where the brightness and the traffic density are correlated.When the ego vehicle drives in the daytime, there are many surrounding vehicles (first row).When the ego vehicle drives in the evening, there is no surrounding vehicle (second row).The shifted environment swaps the brightness and traffic density in the nominal environment, i.e., many surrounding vehicles in the evening and no surrounding vehicles in the daytime.</p>
<p>Behavior.The nominal environments are shown in the 2 nd column of Figure 10, where the other vehicle has aggressive driving behavior.When the ego vehicle is in front of the other vehicle, the other vehicle always accelerates and overtakes the ego vehicle in the left lane.When the ego vehicle is behind the other vehicle, the other vehicle will always accelerate.In the shifted environment, the behavior of the other vehicle is conservative, i.e., the other vehicle always decelerates to block the ego vehicle.</p>
<p>Crossing.The nominal environments are shown in the 3 rd column of Figure 10, where the pedestrian follows the traffic rule and only crosses the road when the traffic light is green.In the shifted environment, the pedestrian disobeys the traffic rules and crosses the road when the traffic light is red.</p>
<p>CarType.The nominal environments are shown in the 4 th column of Figure 10, where the type of vehicle and the speed of the vehicle are correlated.When the vehicle is a truck, the speed is low and</p>
<p>Figure 1 :
1
Figure 1: A model trained only with heavy traffic in the daytime learns the spurious correlation between brightness and traffic density and could fail to drive in light traffic in the daytime.</p>
<p>Figure 2 :
2
Figure 2: The probabilistic graphs of our formulation (SC-MDP) and other related formulations (specified in Appendix B.1 due to the limited space).s 1t means the first dimension of s t .s t ′ is a shorthand for s t+1 .In SC-MDP, the orange line represents the backdoor path from state s 1 t ′ to action a t ′ opened by the confounder c t , which makes the learned policy π rely on the value of c t .</p>
<p>Figure 3 :
3
Figure 3: (a) RMDPs add homogeneous noise to states, while (b) RSC-MDPs perturb the confounder to influence states, resulting in a subset of the valid space.</p>
<p>Figure 4 :
4
Figure 4: Two tasks used in experiments.Door is a composition task implemented in Robosuite with a spurious correlation between the positions of the handle and the door.Brightness is a distraction task implemented in Carla with a spurious correlation between the number vehicles and day/night.</p>
<p>Figure 5 :
5
Figure 5: The first row shows the testing reward on the nominal environments, while the second row shows the testing reward on the shifted environments.</p>
<p>Figure 6 :
6
Figure 6: Comparison between SAC-Sparse and our method.α is the weight of regularization.</p>
<p>Figure 7 :
7
Figure 7: Performance-robustness tradeoff with different augmentation ratio β.</p>
<p>B</p>
<p>Other Related Works 17 B.1 Related RL formulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .17 B.2 Related literature of robustness in RL . . . . . . . . . . . . . . . . . . . . . . .18 B.3 Related literature of spurious correlation in RL . . . . . . . . . . . . . . . . . .18 C Theoretical Analyses 19 C.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19 C.2 Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21 C.3 Proof of auxiliary results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24 D Experiment Details 27 D.1 Architecture of the structural causal model . . . . . . . . . . . . . . . . . . . .27 D.2 Details of Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28 D.3 Example of Generated Data by Perturbations . . . . . . . . . . . . . . . . . . .29 D.4 Computation Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30 D.5 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30 D.6 Discovered Causal Graph in SCM . . . . . . . . . . . . . . . . . . . . . . . . .30</p>
<p>2 RSC ,σ2 1 ( 1 ( 2 ( 1 −
21121
[0, 0]) = V ⋆,σ2 T − 1)π 1 (0 | [0, 0]) + (T − 1)σ 2 1 − 2π 1 (0 | [0, 0]) = 1 + (T − 1)σ 2 + (T − 1) max π1(0 | [0,0])≥ 1 2σ 2 )π 1 (0 | [0, 0]) = 1 + (T − 1)σ 2 + (T − 1)(1 − 2σ 2 equality holds by σ 2 &gt; 1 2 and letting π 1 (0 | [0, 0]) = 1 2 .Similarly, when π 1 (0 | [0, 0]) T − 1)π 1 (0 | [0, 0]) &lt; 1 + T − 1 2 .</p>
<p>Figure 9 :
9
Figure 9: Model architecture of the structural causal model.Encoder, Decoder, position embedding, and Causal Graph are learnable during the training stage.</p>
<p>Figure 10 :
10
Figure 10: Illustration of tasks in the Carla simulator.</p>
<p>Figure 13 :
13
Figure 13: Estimated Causal Graphs of four tasks in Carla.</p>
<p>Figure 14 :
14
Figure 14: Estimated Causal Graphs of the Lift task in Robosuite.</p>
<p>Figure 16 :
16
Figure 16: Estimated Causal Graphs of the Door task in Robosuite.</p>
<p>d A are the state and action spaces, respectively, with n/d A being the dimension of state/action.Here, T is the length of the horizon; P = {P t } 1≤t≤T , where P</p>
<p>t : S × A → ∆(S) denotes the probability transition kernel at time step t, for all 1 ≤ t ≤ T ; and r = {r t } 1≤t≤T denotes the reward function, where r t : S × A → [0, 1] represents the deterministic immediate reward function.A policy (action selection rule) is denoted by π = {π t } 1≤t≤T , namely, the policy at time step t is π t : S → ∆(A) based on the current state s t as π t (• | s t ).To represent the long-term cumulative reward, the value function V π,P t : S → R and Q-value function Q π,P t : S × A → R associated with policy π at step t are defined as V π,P t</p>
<p>Table 1 :
1
Testing reward on shifted environments.Bold font means the best reward.
MethodBrightness Behavior CrossingCarType LiftStackWipeDoorSAC0.56±0.13 0.13±0.03 0.81±0.13 0.63±0.14 0.58±0.13 0.26±0.12 0.16±0.20 0.08±0.07RMDP-G0.55±0.15 0.16±0.04 0.47±0.13 0.53±0.16 0.31±0.08 0.33±0.15 0.06±0.17 0.07±0.03RMDP-U0.54±0.19 0.13±0.05 0.60±0.15 0.39±0.13 0.51±0.17 0.23±0.11 0.06±0.17 0.10±0.13MoCoDA0.50±0.14 0.16±0.05 0.22±0.14 0.23±0.12 0.46±0.14 0.29±0.11 0.01±0.24 0.09±0.14ATLA0.48±0.11 0.14±0.03 0.61±0.14 0.52±0.14 0.61±0.18 0.21±0.12 0.29±0.18 0.28±0.19DBC0.52±0.18 0.16±0.03 0.68±0.12 0.45±0.10 0.12±0.02 0.03±0.02 0.19±0.35 0.01±0.01Active0.47±0.14 0.14±0.03 0.83±0.09 0.77±0.14 0.35±0.09 0.24±0.12 0.17±0.17 0.05±0.02RSC-SAC 0.99±0.11 1.02±0.09 1.04±0.02 1.03±0.02 0.98±0.04 0.77±0.20 0.85±0.12 0.61±0.17
[20]BaselinesRobustness in RL has been explored in terms of diverse uncertainty set over state, action, or transition kernels.Regarding this, we use a non-robust RL and four representative algorithms of robust RL as baselines, all of which are implemented on top of the SAC[20]algorithm.Non-robust RL (SAC): This serves as a basic baseline without considering any robustness during training; Solving robust MDP:</p>
<p>Table 2 :
2
Testing reward on nominal environments.Underline means the reward is over 0.9.RSC-SAC is robust against spurious correlation.The testing results of our proposed method with comparisons to the baselines are presented in Table1, where the rewards are normalized by the episode reward of SAC in the nominal environment.The results reveal that RSC-SAC significantly outperforms other baselines in shifted test environments, exhibiting comparable performance to that of vanilla SAC on the nominal environment in 5 out of 8 tasks.An interesting and even surprising finding, as shown in Table
MethodBrightness Behavior CrossingCarType LiftStackWipeDoorSAC1.00±0.09 1.00±0.08 1.00±0.02 1.00±0.03 1.00±0.03 1.00±0.09 1.00±0.12 1.00±0.03RMDP-G1.04±0.09 1.00±0.11 0.78±0.05 0.79±0.05 0.92±0.07 0.86±0.14 0.99±0.13 0.99±0.06RMDP-U1.02±0.09 1.04±0.07 0.90±0.03 0.88±0.03 0.97±0.05 0.92±0.12 0.97±0.14 0.88±0.31MoCoDA0.65±0.17 0.78±0.15 0.57±0.07 0.55±0.13 0.79±0.11 0.72±0.08 0.69±0.13 0.41±0.22ATLA0.99±0.11 0.98±0.11 0.89±0.05 0.88±0.04 0.94±0.08 0.88±0.10 0.96±0.12 0.97±0.05DBC0.75±0.12 0.78±0.10 0.85±0.08 0.86±0.06 0.27±0.04 0.12±0.08 0.31±0.21 0.01±0.01Active1.02±0.10 1.08±0.06 1.00±0.02 1.00±0.02 0.99±0.03 0.90±0.12 0.93±0.20 0.99±0.05RSC-SAC 0.92±0.31 1.06±0.07 0.96±0.03 0.96±0.03 0.96±0.05 1.04±0.08 0.92±0.14 0.98±0.05R1.</p>
<p>Table 3 :
3
Influence of modules
MethodLiftBehavior Crossingw/o G ϕ0.79±0.15 0.51±0.24 0.87±0.10w/o P θ w/o P c0.75±0.13 0.41±0.28 0.89±0.08 0.90±0.09 0.66±0.21 0.96±0.04Full model 0.98±0.04 1.02±0.09 1.04±0.02</p>
<p>Table 4 :
4
Random purterbuation
MethodLift-0Lift-0.01 Lift-0.1SAC1.00±0.03 0.77±0.13 0.46±0.23RMDP-0.01 0.97±0.05 0.96±0.06 0.51±0.21RMDP-0.10.85±0.12 0.82±0.09 0.39±0.15RSC-SAC0.96±0.05 0.94±0.06 0.44±0.18</p>
<p>Table 4 ,
4
Lift-0 indicates the nominal training environment, while Lift-0.01 and Lift-0.1 represent the environments perturbed by the Gaussian noise with standard derivation 0.01 and 0.1, respectively.The results indicate that our RSC-SAC achieves comparable robustness compared to RMDP-0.01 in both large and small perturbation settings and outperforms RMDP methods in the nominal training environment.
1.11.01% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 0.5 0.6 0.9 Range of acceptable 0.8 Performance &amp; Robustness 0.7 CarType (nominal) CarType (shifted) Crossing (nominal) Crossing (shifted)</p>
<p>• • • , n.With slight abuse of notation, we denote s t+1 ∼ P t (• | s t , a t , c t ) and abbreviateE st+1∼Pt(• | st,at,ct) [•] as E st+1 [•] whenever it is clear.Step 2: Establishing recursive relationship.Recall that the nominal distribution of the confounder is c t ∈ P c t at time step t.We choose π = { π t } which obeys: for all 1 ≤ t ≤ T ,
π t (s) := arg max πt∈∆(A)E πt [r t (s, a t )] +inf Pt∈U σ (P c t )</p>
<p>) (s k , a k ) | π, P +(t+1) , (S t , A t , R t , C t , S t+1 ) = (s, a t , r t , c t , s t+1 )
= sup π∈Πinf P ∈U σ (P c )V π,P t(s)(ii) = sup π∈Πinf P+t∈U σ (P c +t )E π,P+tT k=tr k (s k , a k )(iii) = sup π∈Πinf P+t∈U σ (P c +t )E πt r t (s, a t )T+ E ct∼Pt E st+1 r k = sup k=t+1 π∈Π inf E πt [r t (s, a t )] + P+t∈U σ (P c +t )</p>
<p>r k (s k , a k ) | π, P +(t+1) , (S t , A t , R t , C t , S t+1 ) = (s, a t , r t , c t , s t+1 ) (s k , a k ) | π, P +(t+1) , (S t , A t , R t , C t , S t+1 ) = (s, a t , r t , c t , s t+1 )
(i) = sup π∈ΠE πt [r t (s, a t )] +inf Pt∈U σ (P c t )E πt E ct∼Pt E st+1Tinf P +(t+1) ∈U σ P c +(t+1) r k ≤ sup k=t+1 π∈Π E πt [r t (s, a t )] + inf Pt∈U σ (P c t )
sometimes they are observed but ignored given so many variables to be considered in neural networks.
Acknowledgments and Disclosure of FundingThe work of W. Ding is supported by the Qualcomm Innovation Fellowship.The work of L. Shi and Y. Chi is supported in part by the grants ONR N00014-19-1-2404, NSF CCF-2106778, DMS-2134080, and CNS-2148212.L. Shi is also gratefully supported by the Leo Finzi Memorial Fellowship, Wei Shen and Xuehong Zhang Presidential Fellowship, and Liang Ji-Dian Graduate Fellowship at Carnegie Mellon University.x j S t m I D r B j q a Q x 6 m A y P 3 Z K z q z S J 1 G i b E l D 5 u r v i Q m N t R 7 H o e 2 M q R n q Z W 8 m / u d 1 M h N d B x M u 0 8 y g Z I t F U S a I S c j s c 9 L n C p k R Y 0 s o U 9 z e S t i Q K s q M z a d k Q / C W X 1 4 l z V r V u 6 j W 7 i 8 r 9 Z s 8 j i K c w C m c g w d X U I c 7 a I A P D D g 8 w y u 8 O d J 5 c d 6 d j 0 V r w c l n j u E P n M 8 f 8 z K O H g = = &lt; / l a t e x i t &gt;  Step 1: Constructing a hard instance M of standard MDP.In this section, we consider the following standard MDP instance M = S, A, P 0 , T, r where} is the state space consisting of four elements in dimension n = 2, and A = {0, 1} is the action space with only two options.The transition kernel P 0 = {P 0 t } 1≤t≤T at different time steps 1 ≤ t ≤ T is defined aswhich is illustrated in Fig.8(a), andNote that this transition kernel P 0 ensures that the next state transitioned from the state [0, 0] is either [0, 0] or [0, 1].The reward function is specified as follows: for all time steps 1 ≤ t ≤ T ,Step 2: The equivalence between M and one SC-MDP.Then, we shall show that the constructed standard MDP M can be equivalently represented by one SC-MDP M sc = S, A, T, r, C, {P i t }, P c } with C := {0, 1}.The equivalence is defined as the sequential observations {s t , a t , r t } 1≤t≤T induced by any policy and any initial state distribution in two Markov processes are identical.To specify, S, A, T, r are kept the same as M. Here, {P i t } shall be specified in a while, which determines the transition to each dimension of the next state conditioned on the current state, action, and confounder distribution for all time steps, i.e., sIn addition, before introducing the transition kernel {P i t } of the SC-MDP M sc , we introduce an auxiliary transition kernel P sc = {P sc t } as follows:andIt can be observed that P sc is similar to P 0 except for the transition in the state [0, 0].Armed with above lemmas, applying Lemma 2 with policywhere the inequality holds by the fact that the probability distribution P obeying P 1 (0) = 1 4 and P 1 (1) = 3  4 is inside the uncertainty set U σ2 (P c 1 ) (recall that σ 2 ∈ ( 1 2 , 1] and P c 1 (0) = 1).Finally, combining (27) and (26) together, we complete the proof by showing that with the initial state distribution ϕ defined as ϕ([0, 0]) = 1, we arrive atwhere the last inequality holds by T ≥ 2.C.3 Proof of auxiliary resultsC.3.1 Proof of Lemma 1Step 1: specifying the minimum of the robust value functions over states.For any uncertainty set σ 1 ∈ (0, 1], we first characterize the robust value function of any policy π over different states.To start, we denote the minimum of the robust value function over states at each time step t as below:where the last inequality holds that the reward function defined in (16) is always non-negative.Obviously, there exists at least one state s π min,t that satisfies V π,σ1 t (s π min,t ) = V π,σ1 min,t .With this in mind, we shall verify that for any policy π,To achieve this, we use a recursive argument.First, the base case can be verified since when t + 1 = T + 1, the value functions are all zeros at T + 1 step, i.e., V π,σ1 T +1 (s) = 0 for all s ∈ S.Then, the goal is to verify the following factwith the assumption that V π,σ1 t+1 (s) = 0 for any state s = {[0, 1], [1, 0]}.It is easily observed that for any policy π, the robust value function when state s = {[0, 1], [1, 0]} at any time step t obeyswhere (i) holds by r t (s, a) = 0 for all s = {[0, 1], [1, 0]}, the fact P 0 t (s | s, a) = 1 for s ∈ S (see (14) and (15)), and the definition of the uncertainty set U σ1 (P 0 ) in(23).Here (ii) follows from the recursive assumption V π,σ1 t+1 (s) = 0 for any state s = {[0, 1], [1, 0]}, and the last equality holds by V π,σ1 min,t+1 ≤ V π,σ1 t+1 ([0, 1]) (see(29)).Until now, we complete the proof for(31)and then verify(30).Note that (30) direcly leads toStep 2: Considering the robust value function at state [0, 0].Armed with the above facts, we are now ready to derive the robust value function for the state [0, 0].where (i) holds by r t ([0, 0], a) = 1 for all a ∈ {0, 1} and the definition of P 0 (see(15)), and the last equality arises from(33).Applying(34)When t = 1, the robust value function obeys:where (i) holds by r 1 ([0, 0], a) = 1 for all a ∈ {0, 1}, (ii) follows from the definition of P 0 (see(14)), and the last equality arises from(30)and(33).Step 3: the optimal policy π ⋆,σ1 RMDP .Observing that V π,σ11 ([0, 0]) is increasing monotically as π 1 (0 | [0, 0]) is larger, we directly have that π ⋆,σ1 RMDP (0 | [0, 0]) = 1.Considering that the action does not influence the state transition for t = 2, 3, • • • , T and all other states s ̸ = [0, 0], without loss of generality, we choose the robust optimal policy as ∀s ∈ S :C.3.2 Proof of Lemma 2To begin with, for any uncertainty level σ 2 ∈ ( 1 2 , 1] and any policy π = {π t }, we consider the robust SC-value function V π,σ2 t of the RSC-MDP M sc-rob .Step 1: deriving V π,σ2 t for 2 ≤ t ≤ T .Towards this, for any 2 ≤ t ≤ T and s ∈ S, one has= r t (s, a) + infwhere (i) follows from the state-confounded Bellman consistency equation in(47), (ii) holds by that the reward function r t and P t are all independent from the action (see (16) and (21)), and the last inequality holds by P t (s ′ | s, a, c t ) = 1(s ′ = s) is independent from c t (see (21)).Applying the above fact recursively for t, t + 1, • • • , T leads to that for any s ∈ S,which directly yields (see reward r in (16))Step 2: characterizing V π,σ21 ([0, 0]) for any policy π.In this section, we consider the value of V π,σ2 1 on the state [0, 0].To proceed, one has= 1 + infwhere (i) holds by robust state-confounded Bellman consistency equation in (47), (ii) follows from r 1 ([0, 0], a) = 1 for all a ∈ {0, 1} which is independent from c t .(iii) arises from the definition of P in (20), (iv) can be verified by plugging in the definitions from (14) and (18), and the penultimate equality holds by(40).Step 3: characterizing the optimal robust SC-value functions.Before proceeding, we recall the fact that U σ (P c 1 ) = P ∈ ∆(C) :) ≥ 0 and decreasing with c 1 otherwise, it is easily verified that the maximum of the following functionobeysThen, note that the value of V π,σ21 ([0, 0]) only depends on π 1 (• | [0, 0]) which can be represent by π 1 (0 | [0, 0]).Plugging in (43) into (41) arrives at when π 1 (0when the vehicle is a motorcycle, the speed is high.In the shifted environment, the truck drives very fast and the motorcycle drives very slow.Lift.The nominal environments are shown in the 1 th column of Figure11, where the position of the cube and the color of the cube are correlated.When the cube is in the left part of the table, the color of the cube is green, when the cube is in the right part of the table, the color of the cube is red.The shifted environment swaps the color and position of the cube in the nominal environment, i.e., the cube is green when it is in the right part and the cube is red when it is in the left part.Stack.The nominal environments are shown in the 2 nd column of Figure11, where the position of the red cube and green plate are correlated.When the cube is in the left part of the table, the plate is also in the left part; when the cube is in the right part of the table, the plate is also in the right part.In the shifted environment, the relative position of the cube and the plate changes, i.e., When the cube is in the left part of the table, the plate is in the right part; when the cube is in the right part of the table, the plate is in the left part.Wipe.The nominal environments are shown in the 3 rd column of Figure11, where the shape of the dirty region is correlated to the position of the cube.When the dirty region is diagonal, the cube is on the right-hand side of the robot arm.When the dirty region is anti-diagonal, the cube is on the left-hand side of the robot arm.In the shifted environment, the correlation changes, i.e., when the dirty region is diagonal, the cube is on the left-hand side of the robot arm; when the dirty region is anti-diagonal, the cube is on the right-hand side of the robot arm.Door.The nominal environments are shown in the 4 th column of Figure11, where the height of the handle and the position of the door are correlated.When the door is closed to the robot arm, the handle is in a low position.When the door is far from the robot arm, the handle is in a high position.In the shifted environment, the correlation changes, i.e., when the door is closed to the robot arm, the handle is in a high position; when the door is far from the robot arm, the handle is in a low position.D.3 Example of Generated Data by PerturbationsWe show an example of generated trajectories in the Lift task to demonstrate the reason why our method obtains robustness against spurious correlation.In Figure12(a), we show the collected trajectories from the data buffer.Since the green block is always generated on the left side of the table, the trajectories of the green block mainly appear on the left side of the table.In Figure12(b), we generate new trajectories from a trained transition model and we observe that the distribution of trajectories follows the collected data.In Figure12(c), we directly perturbed the dimensions of the state and used the same transition model to generate new trajectories.We find that the generated trajectories blend the color but fail to maintain the spatial distribution of the original data.In Figure12(d), we use the causal-based transition model to generate new trajectories and we find that the results not only follow the spatial distribution but also blend the color.The results shown in Figure12illustrate that the data generated by our method eliminates the spurious correlation between the color and position of the block, therefore, enabling the policy model to generalize to the shifted environment.D.4 Computation ResourcesOur algorithm is implemented on top of the Tianshou[99]package.All of our experiments are conducted on a machine with an Intel i9-9900K CPU@3.60GHz(16 core) CPU, an NVIDIA GeForce GTX 1080Ti GPU, and 64GB memory.D.5 HyperparametersWe summarize all hyper-parameters used in the Carla experiments (Table5) and Robosuite experiments (Table6).D.6 Discovered Causal Graph in SCMTo show the performance of our learned SCM, we plot the estimated causal graphs of all experiments in Figure13, Figure14, Figure15, Figure16, and Figure17.
. OpenAI. Gpt-4 technical report. 2023</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>End to end learning for self-driving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, arXiv:1604.073162016arXiv preprint</p>
<p>Generalizing goal-conditioned reinforcement learning with variational causal reasoning. Wenhao Ding, Haohong Lin, Bo Li, Ding Zhao, arXiv:2207.090812022arXiv preprint</p>
<p>Benchmarking reinforcement learning algorithms on real-world robots. Dmytro Rupam Mahmood, Gautham Korenkevych, William Vasan, James Ma, Bergstra, Conference on robot learning. PMLR2018</p>
<p>Robust reinforcement learning on state observations with learned optimal adversary. Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh, arXiv:2101.084522021arXiv preprint</p>
<p>Causal confusion in imitation learning. Pim De Haan, Dinesh Jayaraman, Sergey Levine, Advances in Neural Information Processing Systems. 322019</p>
<p>Robust reinforcement learning: A review of foundations and recent advances. Janosch Moos, Kay Hansel, Hany Abdulsamad, Svenja Stark, Debora Clever, Jan Peters, Machine Learning and Knowledge Extraction. 412022</p>
<p>Robust deep reinforcement learning against adversarial perturbations on state observations. Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, Cho-Jui Hsieh, Advances in Neural Information Processing Systems. 202033</p>
<p>What is the solution for state adversarial multi-agent reinforcement learning?. Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, Fei Miao, arXiv:2212.027052022arXiv preprint</p>
<p>Action robust reinforcement learning and applications in continuous control. Chen Tessler, Yonathan Efroni, Shie Mannor, International Conference on Machine Learning. PMLR2019</p>
<p>Robustifying reinforcement learning agents via action space adversarial training. Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, 2020 American control conference (ACC). IEEE2020</p>
<p>Robust dynamic programming. N Garud, Iyengar, Mathematics of Operations Research. 3022005</p>
<p>Toward theoretical understandings of robust markov decision processes: Sample complexity and asymptotics. Wenhao Yang, Liangyu Zhang, Zhihua Zhang, The Annals of Statistics. 5062022</p>
<p>Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. Laixi Shi, Yuejie Chi, arXiv:2208.057672022arXiv preprint</p>
<p>Robust markov decision processes. Wolfram Wiesemann, Daniel Kuhn, Berç Rustem, Mathematics of Operations Research. 201338</p>
<p>Elements of causal inference: foundations and learning algorithms. Jonas Peters, Dominik Janzing, Bernhard Schölkopf, 2017The MIT Press</p>
<p>Provably efficient causal reinforcement learning with confounded observational data. Lingxiao Wang, Zhuoran Yang, Zhaoran Wang, Advances in Neural Information Processing Systems. 202134</p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International conference on machine learning. PMLR2018</p>
<p>Group distributionally robust reinforcement learning with hierarchical latent variables. Mengdi Xu, Peide Huang, Yaru Niu, Visak Kumar, Jielin Qiu, Chao Fang, Kuan-Hui Lee, Xuewei Qi, Henry Lam, Bo Li, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>Carla: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on robot learning. PMLR2017</p>
<p>Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín, Abhishek Joshi, Soroush Nasiriany, Yifeng Zhu, arXiv:2009.12293robosuite: A modular simulation framework and benchmark for robot learning. 2020arXiv preprint</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Michael Laskin, Aravind Srinivas, Pieter Abbeel, International Conference on Machine Learning. PMLR2020</p>
<p>Mastering visual continuous control: Improved data-augmented reinforcement learning. Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto, arXiv:2107.096452021arXiv preprint</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 2022arXiv preprint</p>
<p>Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B Tenenbaum, Chuang Gan, arXiv:2103.10972Learning task decomposition with ordered memory policy network. 2021arXiv preprint</p>
<p>Skills regularized task decomposition for multi-task offline reinforcement learning. Minjong Yoo, Sangwoo Cho, Honguk Woo, Advances in Neural Information Processing Systems. 202235</p>
<p>Alekh Agarwal, Miroslav Dudík, Yisong Yue, and Hal Daumé III. Hierarchical imitation and reinforcement learning. Hoang Le, Nan Jiang, International conference on machine learning. PMLR2018</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. Amy Zhang, Rowan Mcallister, Roberto Calandra, Yarin Gal, Sergey Levine, arXiv:2006.107422020arXiv preprint</p>
<p>Bisimulation through probabilistic testing (preliminary report). G Kim, Arne Larsen, Skou, Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming languages. the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming languages1989</p>
<p>Can active sampling reduce causal confusion in offline reinforcement learning?. Gunshi Gupta, Tim Gj Rudner, Thomas Rowan, Adrien Mcallister, Yarin Gaidon, Gal, 2nd Conference on Causal Learning and Reasoning. 2023</p>
<p>Silviu Pitis, Elliot Creager, Ajay Mandlekar, Animesh Garg, arXiv:2210.11287Mocoda: Model-based counterfactual data augmentation. 2022arXiv preprint</p>
<p>Object-aware regularization for addressing causal confusion in imitation learning. Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin, Jinwoo Shin, Tie-Yan Liu, Advances in Neural Information Processing Systems. 342021</p>
<p>Disentangling by factorising. Hyunjik Kim, Andriy Mnih, International Conference on Machine Learning. PMLR2018</p>
<p>A theory of state abstraction for reinforcement learning. David Abel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Discover and cure: Conceptaware mitigation of spurious correlation. Shirley Wu, Mert Yuksekgonul, Linjun Zhang, James Zou, arXiv:2305.006502023arXiv preprint</p>
<p>Assaf Hallak, Dotan Di Castro, Shie Mannor, arXiv:1502.02259Contextual markov decision processes. 2015arXiv preprint</p>
<p>Distributionally robust markov decision processes. Huan Xu, Shie Mannor, Advances in Neural Information Processing Systems. 201023</p>
<p>Robust control of uncertain markov decision processes with temporal logic specifications. Eric M Wolff, Ufuk Topcu, Richard M Murray, 2012 IEEE 51st IEEE Conference on Decision and Control (CDC). IEEE2012</p>
<p>Robust modified policy iteration. L David, Andrew J Kaufman, Schaefer, INFORMS Journal on Computing. 2532013</p>
<p>Fast bellman updates for robust mdps. Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, International Conference on Machine Learning. PMLR2018</p>
<p>Elena Smirnova, Elvis Dohmatob, Jérémie Mary, arXiv:1902.08708Distributionally robust reinforcement learning. 2019arXiv preprint</p>
<p>Partial policy iteration for l1-robust markov decision processes. Chin Pang Ho, Marek Petrik, Wolfram Wiesemann, Journal of Machine Learning Research. 222752021</p>
<p>Robust markov decision processes: Beyond rectangularity. Vineet Goyal, Julien Grand-Clement, 2022Mathematics of Operations Research</p>
<p>Distributional robustness and regularization in reinforcement learning. Esther Derman, Shie Mannor, arXiv:2003.028942020arXiv preprint</p>
<p>Scaling up robust mdps using function approximation. Aviv Tamar, Shie Mannor, Huan Xu, International conference on machine learning. PMLR2014</p>
<p>Robust reinforcement learning using least squares policy iteration with provable performance guarantees. Kishan Panaganti, Badrinath , Dileep Kalathil, International Conference on Machine Learning. PMLR2021</p>
<p>Improved sample complexity bounds for distributionally robust reinforcement learning. Zaiyan Xu, Kishan Panaganti, Dileep Kalathil, arXiv:2303.027832023arXiv preprint</p>
<p>Jing Dong, Jingwei Li, Baoxiang Wang, Jingzhao Zhang, arXiv:2209.13841Online policy optimization for robust mdp. 2022arXiv preprint</p>
<p>Sample complexity of robust reinforcement learning with a generative model. Kishan Panaganti, Dileep Kalathil, International Conference on Artificial Intelligence and Statistics. PMLR2022</p>
<p>Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. Zhengqing Zhou, Qinxun Bai, Zhengyuan Zhou, Linhai Qiu, Jose Blanchet, Peter Glynn, International Conference on Artificial Intelligence and Statistics. PMLR2021</p>
<p>A finite sample complexity bound for distributionally robust Q-learning. Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. Jose Blanchet, Miao Lu, Tong Zhang, Han Zhong, arXiv:2305.096592023arXiv preprint</p>
<p>Distributionally robust q-learning. Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, Zhengyuan Zhou, International Conference on Machine Learning. PMLR2022</p>
<p>Sample complexity of variancereduced distributionally robust Q-learning. Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou, arXiv:2305.184202023arXiv preprint</p>
<p>Zhipeng Liang, Xiaoteng Ma, Jose Blanchet, Jiheng Zhang, Zhengyuan Zhou, arXiv:2301.11721Singletrajectory distributionally robust reinforcement learning. 2023arXiv preprint</p>
<p>Online robust reinforcement learning with model uncertainty. Yue Wang, Shaofeng Zou, Advances in Neural Information Processing Systems. 342021</p>
<p>The curious price of distributional robustness in reinforcement learning with a generative model. Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, Yuejie Chi, arXiv:2305.165892023arXiv preprint</p>
<p>Distributionally robust model-based reinforcement learning with large state spaces. Shyam Sundhar, Ramesh , Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic, arXiv:2309.022362023arXiv preprint</p>
<p>Robust reinforcement learning using offline data. Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, Mohammad Ghavamzadeh, Advances in neural information processing systems. 202235</p>
<p>Distributionally robust offline reinforcement learning with linear function approximation. Xiaoteng Ma, Zhipeng Liang, Jose Blanchet, Mingwen Liu, Li Xia, Jiheng Zhang, Qianchuan Zhao, Zhengyuan Zhou, arXiv:2209.066202022arXiv preprint</p>
<p>Strategically-timed stateobservation attacks on deep reinforcement learning agents. You Qiaoben, Xinning Zhou, Chengyang Ying, Jun Zhu, ICML 2021 Workshop on Adversarial Machine Learning. 2021</p>
<p>Exploring the training robustness of distributional reinforcement learning against noisy state observations. Ke Sun, Yi Liu, Yingnan Zhao, Hengshuai Yao, Shangling Jui, Linglong Kong, arXiv:2109.087762021arXiv preprint</p>
<p>Defending observation attacks in deep reinforcement learning via detection and denoising. Zikang Xiong, Joe Eappen, He Zhu, Suresh Jagannathan, arXiv:2206.071882022arXiv preprint</p>
<p>Score: Spurious correlation reduction for offline reinforcement learning. Zhihong Deng, Zuyue Fu, Lingxiao Wang, Zhuoran Yang, Chenjia Bai, Zhaoran Wang, Jing Jiang, arXiv:2110.124682021arXiv preprint</p>
<p>Dynamic bottleneck for robust self-supervised exploration. Chenjia Bai, Lingxiao Wang, Lei Han, Animesh Garg, Jianye Hao, Peng Liu, Zhaoran Wang, Advances in Neural Information Processing Systems. 342021</p>
<p>On covariate shift of latent confounders in imitation and reinforcement learning. Guy Tennenholtz, Assaf Hallak, Gal Dalal, Shie Mannor, Gal Chechik, Uri Shalit, arXiv:2110.065392021arXiv preprint</p>
<p>. Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz, arXiv:1907.028932019Invariant risk minimization. arXiv preprint</p>
<p>Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, Percy Liang, arXiv:1911.087312019arXiv preprint</p>
<p>Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Schölkopf, Kun Zhang, Causaladv, arXiv:2106.06196Adversarial robustness through the lens of causality. 2021arXiv preprint</p>
<p>Correct-n-contrast: A contrastive approach for improving robustness to spurious correlations. Michael Zhang, S Nimit, Sohoni, Chelsea Hongyang R Zhang, Christopher Finn, Ré, arXiv:2203.015172022arXiv preprint</p>
<p>Robust policy learning over multiple uncertainty sets. Annie Xie, Shagun Sodhani, Chelsea Finn, Joelle Pineau, Amy Zhang, International Conference on Machine Learning. PMLR2022</p>
<p>Improving generalization in reinforcement learning with mixture regularization. Kaixin Wang, Bingyi Kang, Jie Shao, Jiashi Feng, Advances in Neural Information Processing Systems. 202033</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Ilya Kostrikov, Denis Yarats, Rob Fergus, arXiv:2004.136492020arXiv preprint</p>
<p>Stabilizing deep Q-learning with convnets and vision transformers under data augmentation. Nicklas Hansen, Hao Su, Xiaolong Wang, Advances in neural information processing systems. 202134</p>
<p>Automatic data augmentation for generalization in reinforcement learning. Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, Rob Fergus, Advances in Neural Information Processing Systems. 202134</p>
<p>Generalization in reinforcement learning by soft data augmentation. Nicklas Hansen, Xiaolong Wang, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Sample-efficient reinforcement learning via counterfactual-based data augmentation. Chaochao Lu, Biwei Huang, Ke Wang, José Miguel Hernández-Lobato, Kun Zhang, Bernhard Schölkopf, arXiv:2012.090922020arXiv preprint</p>
<p>Synthesizing adversarial visual scenarios for model-based robotic control. Shubhankar Agarwal, Sandeep, Chinchali, Conference on Robot Learning. PMLR2023</p>
<p>Counterfactual data augmentation using locally factored dynamics. Silviu Pitis, Elliot Creager, Animesh Garg, Advances in Neural Information Processing Systems. 202033</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Learning to simulate. Nataniel Ruiz, Samuel Schulter, Manmohan Chandraker, arXiv:1810.025132018arXiv preprint</p>
<p>Active domain randomization. Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, Liam Paull, Conference on Robot Learning. PMLR2020</p>
<p>Sim-to-real via sim-tosim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, Dieter Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Deceptionnet: Network-driven domain randomization. Sergey Zakharov, Wadim Kehl, Slobodan Ilic, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Christopher Clark, Mark Yatskar, Luke Zettlemoyer, arXiv:1909.03683Don't take the easy way out: Ensemblebased methods for avoiding known dataset biases. 2019arXiv preprint</p>
<p>Learning the difference that makes a difference with counterfactually-augmented data. Divyansh Kaushik, Eduard Hovy, Zachary C Lipton, arXiv:1909.124342019arXiv preprint</p>
<p>Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis. Meike Nauta, Ricky Walsh, Adam Dubowski, Christin Seifert, Diagnostics. 121402021</p>
<p>No subclass left behind: Fine-grained robustness in coarse-grained classification problems. Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, Christopher Ré, Advances in Neural Information Processing Systems. 202033</p>
<p>Unsupervised learning of debiased representations with pseudo-attributes. Seonguk Seo, Joon-Young Lee, Bohyung Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Finding and fixing spurious patterns with explanations. Gregory Plumb, Marco Tulio Ribeiro, Ameet Talwalkar, arXiv:2106.021122021arXiv preprint</p>
<p>Identifying spurious correlations and correcting them with an explanation-based learning. Kathleen M Misgina Tsighe Hagos, Brian Mac Curran, Namee, arXiv:2211.082852022arXiv preprint</p>
<p>Meaningfully debugging model mistakes using conceptual counterfactual explanations. Abubakar Abid, Mert Yuksekgonul, James Zou, International Conference on Machine Learning. PMLR2022</p>
<p>Andrea Bontempelli, Stefano Teso, Fausto Giunchiglia, Andrea Passerini, arXiv:2205.15769Concept-level debugging of part-prototype networks. 2022arXiv preprint</p>
<p>Debiasing concept-based explanations with causal analysis. Mohammad Taha, Bahadori , David E Heckerman, arXiv:2007.115002020arXiv preprint</p>
<p>Reinforcement learning: Theory and algorithms. Alekh Agarwal, Nan Jiang, Wen Sham M Kakade, Sun, 201932UW Seattle, Seattle, WA, USA, TechCS Dept.</p>
<p>Tianshou: A highly modularized deep reinforcement learning library. Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, Jun Zhu, Journal of Machine Learning Research. 232672022</p>            </div>
        </div>

    </div>
</body>
</html>
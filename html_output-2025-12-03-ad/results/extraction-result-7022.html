<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7022 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7022</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7022</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-216867426</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2004.14813v1.pdf" target="_blank">Knowledge Graph Empowered Entity Description Generation</a></p>
                <p><strong>Paper Abstract:</strong> Existing works on KG-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WikiBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However in practice, the input knowledge could be more than enough, because the output description may only want to cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such practical scenario in KG-to-text. Our dataset involves exploring large knowledge graphs (KG) to retrieve abundant knowledge of various types of main entities, which makes the current graph-to-sequence models severely suffered from the problems of information loss and parameter explosion while generating the description text. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to ensemble the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7022.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7022.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MGCN Multi-Graph Transformation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Graph Transformation used in Multi-Graph Convolutional Networks (MGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that transforms an input KG into multiple (six) related graphs that explicitly encode different types of node/node, node/relation and global-context edges; each transformed graph is encoded separately by a GCN and their outputs are aggregated for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Multi-graph transformation (six-graph structure)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>The original directed KG is transformed into six graphs: (1) self graph with self-loop edges, (2) entity-to-relation graph in original direction (default1), (3) entity-to-relation graph in reverse direction (reverse1), (4) entity-to-entity adjacency in original direction (default2), (5) entity-to-entity adjacency in reverse direction (reverse2), and (6) a global node connected to every node (global). Entities and relations are represented as nodes (like Levi graph) and the transformed graphs are represented by six adjacency matrices; each is fed into a GCN encoder and the resulting embeddings are aggregated (sum/avg/CNN) to produce the final graph representation for an attention-LSTM decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based / multi-graph (explicit, near-lossless structural encoding via multiple graphs); sequential component: none (graph-structured, encoded via adjacency matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph transformation into six labeled subgraphs (no linear traversal); produce six adjacency matrices (A1..A6) and node feature vectors, encode each with a GCN, then aggregate across graphs and across stacked MGCN layers (concatenation of layer outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ENT-DESC (primary); evaluated also on WebNLG (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph-to-text / Entity description generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGCN encoder + attention-based LSTM decoder (MGCN+SUM / MGCN+AVG / MGCN+CNN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder: n stacked MGCN layers where each layer contains six GCNs (one per transformed graph) with aggregation; node embeddings and adjacency matrices are input. Decoder: 2-layer attention LSTM. Hidden units and embedding dims = 360. Training with Adam (lr=0.0003), beam search decoding (beam=10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, CHRF++, METEOR, TER, ROUGE-1/2/L (automatic); human evaluation also mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On ENT-DESC: MGCN+SUM and MGCN+CNN achieved BLEU = 26.4; MGCN (no aggregation variant) BLEU = 25.7. Baseline GCN BLEU = 24.8. Delexicalization boosts BLEU by ~3.2–3.6 for both baselines and MGCN.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported to be more parameter-efficient than encoding edges as separate parameters; enables better information capture with fewer GCN layers (improved performance even at 3 layers vs best GCN/deepGCN); improved BLEU and robustness across layer depths; reduces parameter explosion by representing relations as nodes and splitting structural information across multiple adjacency matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Canonical ordering of the transformed graphs or nodes is not specified (ordering left to encoder via adjacency matrices); increases the number of graphs/adjacency matrices to manage (six per layer) which may raise implementation complexity; aggregation requires learning how to weight different graph types; average token lengths not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to Levi-graph approaches and vanilla labeled-edge GCNs, MGCN explicitly encodes entity-to-entity edges (default2/reverse2) and global context, avoiding the implicit learning Levi graphs rely on; yields higher BLEU (e.g., +1.6 BLEU over GCN in main comparisons and best reported BLEU=26.4), and performs better with fewer layers, mitigating information loss from deep stacking.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7022.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7022.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi graph transformation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph transformation (relation-to-node conversion used in prior graph-to-sequence work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation that represents both entities and relations as nodes so that labeled edges are converted into nodes and edges become unlabeled connections between entity nodes and relation nodes; commonly used to avoid per-edge-parameter explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph (relation-as-node) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Original triples (subject, predicate, object) are converted so that predicates become nodes connected to their subject and object entity nodes; edges in the transformed graph are unlabeled connections between entity nodes and relation nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured (relation-as-node); token-based if relation nodes are tokenized, otherwise graph adjacency-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Transform triples into a bipartite-like graph where predicates are nodes; feed transformed graph into a GGNN or GCN encoder (previous works) for downstream generation.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used previously in AMR-to-text and graph-to-sequence works (e.g., AMR, WebNLG); referenced here as baseline technique applied in GRN-like models</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text generation (AMR-to-text / KG-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GGNN / GRN (gated graph neural network encoders over Levi graphs) in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In referenced works, Levi graphs are encoded with gated graph neural networks (GGNN) or GCN variants; used as encoder input to decoders for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used in prior works with metrics like BLEU, METEOR, CHRF++; in this paper GRN (Levi-based) is reported as a baseline on ENT-DESC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On ENT-DESC (as reported in paper's baseline comparisons): GRN (Levi-based GGNN) BLEU ≈ 24.4 (paper states MGCN improves over GRN by 1.3 BLEU when comparing to MGCN's 25.7), exact baseline table entry referenced from paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Levi transformation reduces parameter explosion compared to per-edge labeled-parameter encoding but forces entity-to-entity information to be learned implicitly through GCN layers, often requiring deeper networks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Entity-to-entity information is not explicitly encoded (ignored by the transformation), requiring more GCN layers to capture such information which can lead to information loss; the paper reports Levi graph's limitation leads to the need for deeper GCNs and associated information degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to the proposed multi-graph transformation, Levi graphs avoid per-edge parameter explosion but under-represent direct entity-to-entity relations (necessitating deeper GCNs). MGCN explicitly models entity-to-entity edges and global context, outperforming Levi-based GRN on ENT-DESC.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7022.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7022.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple sequence linearization (S2S)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized RDF triple sequence used with Sequence-to-Sequence (S2S) models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple sequential representation in which KG triples are serialized (ordered) into a flat token sequence and fed to a sequence-to-sequence model for generation; in this paper triples were re-ordered following entity occurrence in the target text for the S2S baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence (linearized triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input RDF triples are converted into a linear token sequence (e.g., subject predicate object subject predicate object ...), with triples re-arranged to follow the occurrence order of entities in the target text; fed into a seq2seq encoder-decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token-based / lossy (structure flattened into sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge-list ordering: triples are serialized and ordered (in this work, re-arranged following entity occurrence in output) and presented as a plain sequence to an RNN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ENT-DESC (used as a baseline here); commonly used in other KG-to-text tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (entity description generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sequence-to-sequence (attention) model (S2S baseline, Bahdanau-style attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An RNN encoder-decoder with attention (seq2seq) used to consume a serialized triple sequence; in this paper it performed poorly on ENT-DESC due to complex input structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU and other MT-style metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reported as poor for this dataset: S2S had poor performance because ENT-DESC inputs are complex and not naturally aligned with output text; linearization loses graph structure leading to degraded generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Flattening the graph to a sequence discards explicit graph structure, making it hard for the model to capture multi-hop and relational context; sensitive to triple ordering and requires careful reordering heuristics for better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Performs worse than graph-aware encoders (GCN, MGCN) on ENT-DESC; while simple, it cannot effectively filter useful vs noisy KG triples when inputs are large and unaligned with outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7022.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7022.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Labeled-edge GCN (edge-as-parameter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GCN encoding with labeled-edge parameters (edges encoded as parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GCN-style encoding where edge types are represented by distinct learnable weight matrices/bias terms (i.e., per-edge-type parameters), directly encoding labeled edges into the convolutional update.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Labeled-edge parameter encoding (per-edge-type weight matrices)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edge labels (predicates) are encoded by dedicated parameter matrices (W_{P(i,j)} and b_{P(i,j)}) used in the GCN update for neighbor aggregation; this directly couples edge types to model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured / parameterized (not a textual serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>GCN neighbor aggregation with distinct parameter matrices per edge label/direction (three directions considered: forward, reverse, self).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Discussed generally; applied implicitly in vanilla GCN baselines on ENT-DESC</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Graph-to-text generation (KG encoding prior to decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GCN encoder (vanilla labeled-edge encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard GCN layer update where for each neighbor type P(i,j) a separate weight matrix and bias are applied before a nonlinearity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU reported for baseline GCN (24.8 BLEU on ENT-DESC as in paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GCN baseline BLEU = 24.8 on ENT-DESC (paper baseline result).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Leads to parameter explosion when there are many distinct edge labels in KGs, causing inefficiency and overfitting; also contributes to information loss when many GCN layers are stacked.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Parameter explosion with many relation types; high memory and overfitting risk; motivates relation-as-node (Levi) or multi-graph transformations to avoid per-edge-type parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Worse scalability than Levi or multi-graph transformations; MGCN avoids per-edge-type parameter explosion by representing relations as nodes and splitting structural information across multiple transformed graphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7022.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7022.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delexicalization (entity anonymization)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalization of main and topic-related entities (entity type + index tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that replaces the main entity and topic-related entities with tokens indicating entity type and index (rather than raw surface forms), reducing lexical sparsity and enabling better modeling of structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Entity delexicalization (type-index tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Main and topic-related entities in input and output are replaced with tokens that encode entity type and a position/index (e.g., PERSON_1), enabling the model to learn template-like structures and later re-fill tokens with real entity names if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based preprocessing / lossy w.r.t surface forms (keeps type info only)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Textual substitution: substitute entity mentions with type-index tokens before encoding; model trained on delexicalized inputs/outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ENT-DESC (applied in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (entity description generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to S2S, GCN, MGCN models as preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>No change to model architecture; only a preprocessing step done before feeding data to encoder/decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (automatic evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Delexicalization boosted BLEU by approximately 3.2 to 3.6 points for both baseline and proposed models on ENT-DESC (reported aggregate improvement range).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces lexical sparsity and improves BLEU substantially; helps both baselines and MGCN models converge to higher-scoring generations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Removes surface-level lexical variation (names/details) and requires a deterministic or heuristic re-lexicalization step to insert correct surface names in final outputs; may hide evaluation differences when golden references include specific surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>A simple, effective preprocessing step that complements graph-based encoders; orthogonal to graph representation choices (benefits both sequence-linearized and graph-structured models).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Neural text generation from structured data with application to the biography domain <em>(Rating: 1)</em></li>
                <li>Neural machine translation by jointly learning to align and translate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7022",
    "paper_id": "paper-216867426",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "MGCN Multi-Graph Transformation",
            "name_full": "Multi-Graph Transformation used in Multi-Graph Convolutional Networks (MGCN)",
            "brief_description": "A representation that transforms an input KG into multiple (six) related graphs that explicitly encode different types of node/node, node/relation and global-context edges; each transformed graph is encoded separately by a GCN and their outputs are aggregated for text generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Multi-graph transformation (six-graph structure)",
            "representation_description": "The original directed KG is transformed into six graphs: (1) self graph with self-loop edges, (2) entity-to-relation graph in original direction (default1), (3) entity-to-relation graph in reverse direction (reverse1), (4) entity-to-entity adjacency in original direction (default2), (5) entity-to-entity adjacency in reverse direction (reverse2), and (6) a global node connected to every node (global). Entities and relations are represented as nodes (like Levi graph) and the transformed graphs are represented by six adjacency matrices; each is fed into a GCN encoder and the resulting embeddings are aggregated (sum/avg/CNN) to produce the final graph representation for an attention-LSTM decoder.",
            "representation_type": "token-based / multi-graph (explicit, near-lossless structural encoding via multiple graphs); sequential component: none (graph-structured, encoded via adjacency matrices)",
            "encoding_method": "Graph transformation into six labeled subgraphs (no linear traversal); produce six adjacency matrices (A1..A6) and node feature vectors, encode each with a GCN, then aggregate across graphs and across stacked MGCN layers (concatenation of layer outputs).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ENT-DESC (primary); evaluated also on WebNLG (benchmark)",
            "task_name": "Knowledge-graph-to-text / Entity description generation",
            "model_name": "MGCN encoder + attention-based LSTM decoder (MGCN+SUM / MGCN+AVG / MGCN+CNN variants)",
            "model_description": "Encoder: n stacked MGCN layers where each layer contains six GCNs (one per transformed graph) with aggregation; node embeddings and adjacency matrices are input. Decoder: 2-layer attention LSTM. Hidden units and embedding dims = 360. Training with Adam (lr=0.0003), beam search decoding (beam=10).",
            "performance_metric": "BLEU, CHRF++, METEOR, TER, ROUGE-1/2/L (automatic); human evaluation also mentioned",
            "performance_value": "On ENT-DESC: MGCN+SUM and MGCN+CNN achieved BLEU = 26.4; MGCN (no aggregation variant) BLEU = 25.7. Baseline GCN BLEU = 24.8. Delexicalization boosts BLEU by ~3.2–3.6 for both baselines and MGCN.",
            "impact_on_training": "Reported to be more parameter-efficient than encoding edges as separate parameters; enables better information capture with fewer GCN layers (improved performance even at 3 layers vs best GCN/deepGCN); improved BLEU and robustness across layer depths; reduces parameter explosion by representing relations as nodes and splitting structural information across multiple adjacency matrices.",
            "limitations": "Canonical ordering of the transformed graphs or nodes is not specified (ordering left to encoder via adjacency matrices); increases the number of graphs/adjacency matrices to manage (six per layer) which may raise implementation complexity; aggregation requires learning how to weight different graph types; average token lengths not reported.",
            "comparison_with_other": "Compared to Levi-graph approaches and vanilla labeled-edge GCNs, MGCN explicitly encodes entity-to-entity edges (default2/reverse2) and global context, avoiding the implicit learning Levi graphs rely on; yields higher BLEU (e.g., +1.6 BLEU over GCN in main comparisons and best reported BLEU=26.4), and performs better with fewer layers, mitigating information loss from deep stacking.",
            "uuid": "e7022.0"
        },
        {
            "name_short": "Levi graph transformation",
            "name_full": "Levi graph transformation (relation-to-node conversion used in prior graph-to-sequence work)",
            "brief_description": "A graph transformation that represents both entities and relations as nodes so that labeled edges are converted into nodes and edges become unlabeled connections between entity nodes and relation nodes; commonly used to avoid per-edge-parameter explosion.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Levi graph (relation-as-node) representation",
            "representation_description": "Original triples (subject, predicate, object) are converted so that predicates become nodes connected to their subject and object entity nodes; edges in the transformed graph are unlabeled connections between entity nodes and relation nodes.",
            "representation_type": "graph-structured (relation-as-node); token-based if relation nodes are tokenized, otherwise graph adjacency-based",
            "encoding_method": "Transform triples into a bipartite-like graph where predicates are nodes; feed transformed graph into a GGNN or GCN encoder (previous works) for downstream generation.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used previously in AMR-to-text and graph-to-sequence works (e.g., AMR, WebNLG); referenced here as baseline technique applied in GRN-like models",
            "task_name": "Graph-to-text generation (AMR-to-text / KG-to-text)",
            "model_name": "GGNN / GRN (gated graph neural network encoders over Levi graphs) in prior work",
            "model_description": "In referenced works, Levi graphs are encoded with gated graph neural networks (GGNN) or GCN variants; used as encoder input to decoders for text generation.",
            "performance_metric": "Used in prior works with metrics like BLEU, METEOR, CHRF++; in this paper GRN (Levi-based) is reported as a baseline on ENT-DESC.",
            "performance_value": "On ENT-DESC (as reported in paper's baseline comparisons): GRN (Levi-based GGNN) BLEU ≈ 24.4 (paper states MGCN improves over GRN by 1.3 BLEU when comparing to MGCN's 25.7), exact baseline table entry referenced from paper.",
            "impact_on_training": "Levi transformation reduces parameter explosion compared to per-edge labeled-parameter encoding but forces entity-to-entity information to be learned implicitly through GCN layers, often requiring deeper networks.",
            "limitations": "Entity-to-entity information is not explicitly encoded (ignored by the transformation), requiring more GCN layers to capture such information which can lead to information loss; the paper reports Levi graph's limitation leads to the need for deeper GCNs and associated information degradation.",
            "comparison_with_other": "Compared to the proposed multi-graph transformation, Levi graphs avoid per-edge parameter explosion but under-represent direct entity-to-entity relations (necessitating deeper GCNs). MGCN explicitly models entity-to-entity edges and global context, outperforming Levi-based GRN on ENT-DESC.",
            "uuid": "e7022.1"
        },
        {
            "name_short": "Triple sequence linearization (S2S)",
            "name_full": "Linearized RDF triple sequence used with Sequence-to-Sequence (S2S) models",
            "brief_description": "A simple sequential representation in which KG triples are serialized (ordered) into a flat token sequence and fed to a sequence-to-sequence model for generation; in this paper triples were re-ordered following entity occurrence in the target text for the S2S baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "RDF triple sequence (linearized triples)",
            "representation_description": "Input RDF triples are converted into a linear token sequence (e.g., subject predicate object subject predicate object ...), with triples re-arranged to follow the occurrence order of entities in the target text; fed into a seq2seq encoder-decoder.",
            "representation_type": "sequential / token-based / lossy (structure flattened into sequence)",
            "encoding_method": "Edge-list ordering: triples are serialized and ordered (in this work, re-arranged following entity occurrence in output) and presented as a plain sequence to an RNN encoder.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "ENT-DESC (used as a baseline here); commonly used in other KG-to-text tasks",
            "task_name": "KG-to-text generation (entity description generation)",
            "model_name": "Sequence-to-sequence (attention) model (S2S baseline, Bahdanau-style attention)",
            "model_description": "An RNN encoder-decoder with attention (seq2seq) used to consume a serialized triple sequence; in this paper it performed poorly on ENT-DESC due to complex input structure.",
            "performance_metric": "BLEU and other MT-style metrics",
            "performance_value": null,
            "impact_on_training": "Reported as poor for this dataset: S2S had poor performance because ENT-DESC inputs are complex and not naturally aligned with output text; linearization loses graph structure leading to degraded generation quality.",
            "limitations": "Flattening the graph to a sequence discards explicit graph structure, making it hard for the model to capture multi-hop and relational context; sensitive to triple ordering and requires careful reordering heuristics for better performance.",
            "comparison_with_other": "Performs worse than graph-aware encoders (GCN, MGCN) on ENT-DESC; while simple, it cannot effectively filter useful vs noisy KG triples when inputs are large and unaligned with outputs.",
            "uuid": "e7022.2"
        },
        {
            "name_short": "Labeled-edge GCN (edge-as-parameter)",
            "name_full": "GCN encoding with labeled-edge parameters (edges encoded as parameters)",
            "brief_description": "A GCN-style encoding where edge types are represented by distinct learnable weight matrices/bias terms (i.e., per-edge-type parameters), directly encoding labeled edges into the convolutional update.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Labeled-edge parameter encoding (per-edge-type weight matrices)",
            "representation_description": "Edge labels (predicates) are encoded by dedicated parameter matrices (W_{P(i,j)} and b_{P(i,j)}) used in the GCN update for neighbor aggregation; this directly couples edge types to model parameters.",
            "representation_type": "graph-structured / parameterized (not a textual serialization)",
            "encoding_method": "GCN neighbor aggregation with distinct parameter matrices per edge label/direction (three directions considered: forward, reverse, self).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Discussed generally; applied implicitly in vanilla GCN baselines on ENT-DESC",
            "task_name": "Graph-to-text generation (KG encoding prior to decoding)",
            "model_name": "GCN encoder (vanilla labeled-edge encoding)",
            "model_description": "Standard GCN layer update where for each neighbor type P(i,j) a separate weight matrix and bias are applied before a nonlinearity.",
            "performance_metric": "BLEU reported for baseline GCN (24.8 BLEU on ENT-DESC as in paper comparisons)",
            "performance_value": "GCN baseline BLEU = 24.8 on ENT-DESC (paper baseline result).",
            "impact_on_training": "Leads to parameter explosion when there are many distinct edge labels in KGs, causing inefficiency and overfitting; also contributes to information loss when many GCN layers are stacked.",
            "limitations": "Parameter explosion with many relation types; high memory and overfitting risk; motivates relation-as-node (Levi) or multi-graph transformations to avoid per-edge-type parameters.",
            "comparison_with_other": "Worse scalability than Levi or multi-graph transformations; MGCN avoids per-edge-type parameter explosion by representing relations as nodes and splitting structural information across multiple transformed graphs.",
            "uuid": "e7022.3"
        },
        {
            "name_short": "Delexicalization (entity anonymization)",
            "name_full": "Delexicalization of main and topic-related entities (entity type + index tokens)",
            "brief_description": "A preprocessing representation that replaces the main entity and topic-related entities with tokens indicating entity type and index (rather than raw surface forms), reducing lexical sparsity and enabling better modeling of structure.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Entity delexicalization (type-index tokens)",
            "representation_description": "Main and topic-related entities in input and output are replaced with tokens that encode entity type and a position/index (e.g., PERSON_1), enabling the model to learn template-like structures and later re-fill tokens with real entity names if needed.",
            "representation_type": "token-based preprocessing / lossy w.r.t surface forms (keeps type info only)",
            "encoding_method": "Textual substitution: substitute entity mentions with type-index tokens before encoding; model trained on delexicalized inputs/outputs.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ENT-DESC (applied in experiments)",
            "task_name": "KG-to-text generation (entity description generation)",
            "model_name": "Applied to S2S, GCN, MGCN models as preprocessing",
            "model_description": "No change to model architecture; only a preprocessing step done before feeding data to encoder/decoder.",
            "performance_metric": "BLEU (automatic evaluation)",
            "performance_value": "Delexicalization boosted BLEU by approximately 3.2 to 3.6 points for both baseline and proposed models on ENT-DESC (reported aggregate improvement range).",
            "impact_on_training": "Reduces lexical sparsity and improves BLEU substantially; helps both baselines and MGCN models converge to higher-scoring generations.",
            "limitations": "Removes surface-level lexical variation (names/details) and requires a deterministic or heuristic re-lexicalization step to insert correct surface names in final outputs; may hide evaluation differences when golden references include specific surface forms.",
            "comparison_with_other": "A simple, effective preprocessing step that complements graph-based encoders; orthogonal to graph representation choices (benefits both sequence-linearized and graph-structured models).",
            "uuid": "e7022.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 2,
            "sanitized_title": "densely_connected_graph_convolutional_networks_for_graphtosequence_learning"
        },
        {
            "paper_title": "Neural text generation from structured data with application to the biography domain",
            "rating": 1,
            "sanitized_title": "neural_text_generation_from_structured_data_with_application_to_the_biography_domain"
        },
        {
            "paper_title": "Neural machine translation by jointly learning to align and translate",
            "rating": 1,
            "sanitized_title": "neural_machine_translation_by_jointly_learning_to_align_and_translate"
        }
    ],
    "cost": 0.013998499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph Empowered Entity Description Generation</p>
<p>Liying Cheng 
Alibaba Group
DAMO Academy</p>
<p>Design
Singapore University of Technology</p>
<p>Yan Zhang 
Design
Singapore University of Technology</p>
<p>Dekun Wu 
Design
Singapore University of Technology</p>
<p>York University</p>
<p>Zhanming Jie zhanmingjie@mymail.sutd.edu.sg 
Design
Singapore University of Technology</p>
<p>Bing Lidong 
Alibaba Group
DAMO Academy</p>
<p>Wei Lu luwei@sutd.edu.sg 
Design
Singapore University of Technology</p>
<p>Luo Si luo.si@alibaba-inc.com 
Alibaba Group
DAMO Academy</p>
<p>Knowledge Graph Empowered Entity Description Generation</p>
<p>Existing works on KG-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WikiBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However in practice, the input knowledge could be more than enough, because the output description may only want to cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such practical scenario in KGto-text. Our dataset involves exploring large knowledge graphs (KG) to retrieve abundant knowledge of various types of main entities, which makes the current graph-to-sequence models severely suffered from the problems of information loss and parameter explosion while generating the description text. We address these challenges by proposing a multigraph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to ensemble the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.</p>
<p>Introduction</p>
<p>KG-to-text generation, automatically converting knowledge into comprehensive natural language, is an important task in natural language processing (NLP) and user interaction studies (Damljanovic et al., 2010). Specifically, the task takes as input some structured knowledge, such as resource description framework (RDF) triples of WebNLG (Gardent et al., 2017), key-value pairs of WikiBio (Lebret et al., 2016) and E2E (Novikova et al., 2017), to generate natural text describing the input knowledge. In essence, the task can be formulated as follows: given a main entity, its onehop attributes/relations (e.g. WikiBIO and E2E), and/or multi-hop relations (e.g. WebNLG), the goal is to generate a text description of the main entity describing its attributes and relations. Note that these existing datasets basically have a good alignment between an input knowledge set and its output text. Obtaining such data with good alignment could be a laborious and expensive annotation process (Snow et al., 2008). More importantly, in practice, the knowledge regarding the main entity could be more than enough and the description may only cover the most significant knowledge, thereby, the generation model should have such differentiation capability.</p>
<p>To this end, we tackle a knowledge graph empowered entity description generation task in order to work towards more practical problems. Specifically, the aim is to generate a description with one or more sentences for a main entity and a few topic-related entities, which is empowered by the knowledge from a KG for more natural description. In order to facilitate the study, we introduce a new dataset namely entity-to-description (ENT-DESC) extracted from Wikipedia and Wikidata, which contains over 110k instances. Each sample is a triplet, containing a set of entities, the explored knowledge from a KG, and the description. Figure 1 shows an example to generate the description of the main entity, i.e., Bruno Mars, given some relevant keywords, i.e. retro style, funk, etc., which are called topic-related entities of Bruno Mars. We intend to generate the short paragraph below to describe the main entity in compliance with the topic revealed by topic-related entities. For generating accurate descriptions, one challenge is how to find the underlying relations between the entities and keywords. In our dataset, we explore such relation revealed in a KG, i.e. the upper right in Figure 1. Therefore, to some extent, our dataset is a generalization of existing KG-to-text datasets in two major aspects. First, the knowledge, in the form of triples, regarding the main entity and topic entities is automatically extracted from a KG, and such knowledge could be more than enough and not necessarily useful for generating the output. Second, our dataset allows the direct generation from the input entities to the output descriptions, and the intermediate knowledge could be explored freely from other sources.</p>
<p>Our proposed dataset is not only more practical but also more challenging due to lack of explicit alignment between input and output. Therefore, some knowledge is useful for generation, while others might be noise. In such a case that many different relations from the KG are involved, standard graph-to-sequence models suffer the problem of low training speed and parameter explosion, as edges are encoded in the form of parameters. Previous work deals with the problem by transforming the original graphs into Levi graphs (Beck et al., 2018). However, Levi graph transformation only represents the relations between original nodes and neighbour edges while relations between original nodes are learned implicitly through GCN. Therefore, more GCN layers are required to capture such information (Marcheggiani and Perez-Beltrachini, 2018). As more GCN layers are being stacked, it suffers information loss from KG (Abu-El-Haija et al., 2018). In order to address these limitations, we present a multi-graph convolutional networks (MGCN) architecture by introducing multi-graph transformation incorporated with an aggregation layer. Multi-graph transformation is able to rep-resent the original graph information more accurately, while the aggregation layer learns to ensemble the useful information from the KG. Extensive experiments are conducted on both our proposed dataset and benchmark dataset (i.e. WebNLG). Our MGCN outperforms several strong baselines, which demonstrate the effectiveness of our techniques especially when using fewer GCN layers.</p>
<p>The main contributions of this paper can be summarized as follows:</p>
<p>• We tackle a more practical task of knowledge graph empowered entity description generation. We also construct a large-scale dataset ENT-DESC for this specific task. To the best of our knowledge, ENT-DESC is the largest dataset of KG-to-text generation. • We propose a multi-graph structure transformation that explicitly expresses more comprehensive and more accurate graph information, in order to overcome Levi graph's limitations.</p>
<p>• Thorough experiments and analysis on our new dataset show that our proposed MGCN model incorporated with aggregation methods outperforms strong baselines by effectively capturing and ensembling multi-graph information.</p>
<p>Related Work</p>
<p>To better position our work, we first review related tasks and datasets, followed by the works on graphto-sequence modeling.</p>
<p>Dataset and Task. There are increasing number of new datasets and tasks being proposed in recent years as more attention has been paid to data-to-text generation. Gardent et al. (2017) introduced the WebNLG challenge which aimed to generate text from a small set of RDF triples (no more than 7) containing equivalent information. Koncel-Kedziorski et al. (2019) introduced AGENDA dataset which aimed to generate paper abstract from a title and a KG. They built the KG by using information extraction system on the abstracts. In our work, we directly explore the Wikidata as our KG without looking at the output. Scalewise, our dataset consists of 110k instances while AGENDA is 40k. Lebret et al. (2016) introduced WIKIBIO dataset that generated the first sentence of biographical articles with the key-value pairs extracted from the article's infobox. Dušek et al. (2020) introduced E2E dataset in the restaurant domain, which aimed to generate restaurant recommendations given 3 to 8 slot-value pairs. These two datasets were only from single domain, while ours focuses on multiple domains including humans, events, locations, organizations with more than 100 categories. Another difference is that we intend to generate the first paragraph of each Wikipedia article from a more complicated KG, but not keyvalue pairs. Another popular task is AMR-to-text generation (Konstas et al., 2017). The structure of AMR graphs is rooted and denser, which is quite different from our proposed KG-to-text task.</p>
<p>Graph-to-sequence Modeling. In recent years, graph convolutional networks (GCN) have been applied to several NLP tasks (e.g. semi-supervised node classification (Kipf and Welling, 2016), semantic role labeling  and neural machine translation (Bastings et al., 2017)) and also achieved state-of-the-art performance on graph-to-sequence modeling. In order to capture more graphical information, Velickovic et al. introduced graph attention networks (GATs) through stacking a graph attentional layer, but only allowed to learn information from adjacent nodes implicitly without considering a more global contextualization. Marcheggiani and Perez-Beltrachini (2018) then used GCN as the encoder in order to capture more distant information in graphs.</p>
<p>Since there are usually large amount of labels for edges in KG, such graph-to-sequence models will incur in information loss and parameter explosion. Beck et al. (2018) proposed to transform the graph into Levi graph in order to work towards the aforementioned deficiencies, together with gated graph neural network (GGNN) to build graph representation for AMR-to-text problem. However, they face some new limitations brought in by Levi graph transformation: the entity-to-entity information is being ignored in Levi transformation, as also mentioned in their paper. Afterwards, deeper GCNs were stacked (Guo et al., 2019) to capture such ignored information implicitly. In contrast, we intend to use fewer GCN layers to capture more global contextualization by explicitly stating all types of graph information as mentioned above.</p>
<p>Task Description</p>
<p>In this paper, we tackle a practical problem of KG empowered entity description generation. In practice, it is difficult to describe an entity in only a few sentences as there are too many aspects for an entity. Now, if we are given a few topic-related entities as topic restrictions to the main entity, the text to be generated could be more concrete, par- ticularly when we are allowed to explore the connections among these entities in KG. As seen in Figure 1, when we are asked to use one or two sentences to introduce "Bruno Mars" 2 , his popular singles will first come into some people's mind, while his music genres might be in other people's first thought. With the introduction of topic-related entities, the description will have some focus. In this case, when topic-related entities (i.e. R&amp;B, hip hop, rock, etc.) are provided, people are aware of describing Bruno Mars in the direction of music styles on top of their basic information. Formally, given a set of entities e = {E 1 , ..., E n } and a KG G = (V, E), where E 1 is main entity, E 2 , ..., E n are topic-related entities, V is the set of entity nodes and E is the set of directed relation edges. We intend to generate a natural language text y = {y 1 , y 2 , · · · , y T } which should cover as many entities in e as possible. Meanwhile, we explore G for useful information to improve the naturalness of the description. Here, the KG G can also be written as a set of RDF triples:
G = { V S 1 , P 1 , V O 1 , ..., V SM , P M , V OM }, where M is the total number of triples, V S i , V O i ∈ V
are the subject and object entities respectively, P i is the predicate stating the relation between V S i and V O i .</p>
<p>ENT-DESC Dataset</p>
<p>To prepare our dataset, we first use Nayuki's implementation 3 to calculate the PageRank score for more than 9.9 million Wikipages. We then extract the categories from Wikidata for the top 100k highest scored pages and manually select 90 categories out of the top 200 most frequent ones as the seed categories. The domains of the categories mainly include humans, events, locations and organizations. The entities from these categories are collected as our candidate set of main entities. We further process their associated Wikipedia pages for collecting the first paragraphs and entities with  hyperlink as topic-related entities. We then search Wikidata to gather neighbours of the main entities and 1-hop/2-hop paths between main entities and their associated topic-related entities, which finally results in a dataset consisting of more than 110k entity-text pairs with 3 million triples in the KG. The comparison of our dataset with WebNLG and AGENDA is shown in Table 1 and Figure 2.</p>
<p>In the comparison of these three datasets, there are some obvious differences. First, our dataset is significantly larger than WebNLG and AGENDA (i.e. more than twice of their instances). Meanwhile, our vocabulary size and numbers of distinct entities/relations are all much larger. Second, the average number of input triples per instance is much larger than those of the other two. More importantly, our dataset provides a new genre of data for the task. Specifically, WebNLG has a strict alignment between input triples and output text, and accordingly, each input triple roughly corresponds to 8 words. AGENDA is different from WebNLG for generating much longer output sequence. And as observed, quite a portion of text information cannot be directly covered by the input triples. Considering the construction details of both WebNLG and AGENDA, all their input triples provide useful information (i.e. should be used) for generating the output. In contrast, our dataset has much larger number of input triples, particularly considering the length difference of output texts. Lastly, another unique characteristic of our dataset is that not every input triple is useful for generation, which brings in the challenge that a model should be able to distill the helpful part for generating better output sequence.</p>
<p>Our MGCN Model</p>
<p>In practice, our proposed task can be cast as a problem of generating text from KG. Following most graph-to-sequence generation work, we present an encoder-decoder architecture by introducing multigraph transformation incorporated with effective aggregation methods, shown in Figure 3.</p>
<p>Multi-Graph Encoder</p>
<p>We first briefly introduce the general flow of multigraph encoder which consists of n MGCN layers. Before the first layer of MGCN, graph embedding h (0) representing a collection of node embeddings is initialized from input KG after multi-graph transformation. By stacking n MGCN layers accordingly with multi-graph transformation and aggregation, we obtain the final graph representation by aggregating the outputs of n MGCN layers for decoding. We explain the details of an MGCN layer as follows.</p>
<p>Graph Encoder. Before we introduce our multigraph transformation, we first look at our basic graph encoder in each MGCN layer (i.e. Graph Encoder 1 to 6 in Figure 3 left). In this paper, we adopt graph convolutional networks (GCNs) (Duvenaud et al., 2015;Kearnes et al., 2016;Kipf and Welling, 2016; as the basic encoder to consider the graph structure and to capture graph information for each node. More formally, given a directed graph G * = (V * , E * ), we define a feature vector x V ∈ R d for each node V ∈ V * . In order to capture the information of neighbours N (·), the node representation h V j for each V j ∈ V * is calculated as:
h V j = ReLU V i ∈N (V j ) W P(i,j) x V i + b P(i,j)
where P(i, j) denotes the edge between node V i and V j including three directions: (1) V i to V j , (2) V j to V i , (3) V i to itself when i equals to j. Weight matrix W ∈ R d×d and bias b ∈ R d are model parameters. Relu is the rectifier linear unit function. Only immediate neighbors of each node are involved in the equation above as it represents a single-layer GCN. Figure 3: Overview of our model architecture. There are n MGCN layers in the multi-graph encoder, and 2 LSTM layers in the decoder. h (k−1) is the input graph representation at Layer k, and its 6 copies together with the corresponding adjacent matrices A i 's of transformed graphs in the multi graph (refer to Figure 4) are fed into individual basic encoders. Finally, we obtain the graph representation h (k) for the next layer by aggregating the representations from these encoders.</p>
<p>Multi-Graph Transformation. The basic graph encoder with GCN architecture as described above struggles with the problem of parameter explosion and information loss, as the edges are encoded in the form of parameters. Previous works (Beck et al., 2018;Guo et al., 2019;Koncel-Kedziorski et al., 2019) deal with this deficiency by transforming the graph into a Levi graph. However, Levi graph transformation also has its limitations, where entity-to-entity information is learned implicitly. In order to overcome all the difficulties, we introduce a multi-graph structure transformation. A simple example is shown in Figure 4. Given such a directed graph, where E 1 , E 2 , E 3 , E 4 represent entities and R 1 , R 2 , R 3 represent relations in the KG, we intend to transform it into multiple graphs which capture different types of information. Similar to Levi graph transformation, all the entities and relations are represented as nodes in our multi-graph structure. By doing such transformation, we are able to represent relations in the same format as entities using embeddings directly, which avoids the risk of parameter explosion. This multi-graph transformation can be generalised for any graph regardless of the complexity and characteristic of the KG, and the transformed graph can be applied to any model architecture.</p>
<p>In this work, we employ a six-graph structure for our multi-graph transformation as shown in Figure 4. Firstly, in self graph (1), each node is assigned a self-loop edge namely self label. Secondly, graphs (2) and (3) are formed by connecting the nodes representing the entities and their adjacent relations. In addition to connecting them in their original direction using default1 label, we also add a reverse1 label for the inverse direction of their original relations. Thirdly, we create graphs (4) and (5) by connecting the nodes representing adjacent entities in the input graph, labeled by default2 and reverse2, respectively. These two graphs overcome the deficiency of Levi graph transformation by explicitly representing the entity-to-entity information from the input graph. It also allows us to differentiate entities and relations by adding edges between entities. Finally, in order to consider more global contextualization, we add a global node on top of the graph structure to form graph (6). Each node is assigned with a global edge directed from global node. In the end, the set of transformed graphs can be represented by their edge labels T = {self, default, reverse, default2, reverse2, global}.</p>
<p>Given the six transformed graphs mentioned above, we can construct six corresponding adjacency matrices: {A 1 , A 2 , · · · , A 6 }. As shown in Figure 3 (left), these adjacency matrices are used by six basic graph encoders to obtain the corresponding transformed graph representations (i.e. h g ).</p>
<p>Aggregation Layer. After learning 6 embeddings of multi graphs from the basic encoders at the current MGCN layer k − 1, the model goes through an aggregation layer to obtain the graph embedding for the next MGCN layer k. We can get it by simply concatenating all 6 transformed graph embeddings with different types of edges. However, such simple concatenation of the transformed graphs fails to learn different importance of various edge types, and also involves too many features and parameters.</p>
<p>In order to address the challenges mentioned above, we propose three aggregation methods for the multi-graph structure: sum-based, average- Firstly, in sum-based aggregation layer, we compute the representation h (k) at k-th layer as:
h (k) = g i ∈T h (k−1) g i where h (k−1) g i
represents the i th graph representation and T is the set of all transformed graphs. Sum-based aggregation allows a linear approximation of spectral graph convolutions, and helps to reduce data sparsity and over-fitting problems.</p>
<p>Similarly, we apply an average-based aggregation method by normalizing each graph through a mean operation:
h (k) = 1 m g i ∈T h (k−1) g i ,
where m is the number of graphs in T .</p>
<p>We also try to employ a more complex CNNbased aggregation method. Formally, the representation h (k) at k-th layer is defined as:
h (k) = W conv h (k−1) mg + b (k) mg .
Here, we use convolutional neural networks (CNN) to convolute the multi-graph representation, where h mg = [h g 1 , ..., h g 6 ] is the representation of multigraph and b (k) mg is the bias term. By applying these aggregation methods, we obtain the graph representation for the next layer h (k) , which is able to capture different aspects of graph information more effectively by learning different types of edges in each transformed graph.</p>
<p>Stacking MGCN Layers. With the introduction of MGCN layer as described above, we can capture the information of higher-degree neighbours by stacking multiple MGCN layers. Inspired from Xu et al. (2018), we employ a concatenation operation over h (1) , · · · , h (n) to aggregate the graph representations from all MGCN layers (Figure 3 right) to form the final layer h (f inal) , which can be written as follow:
h (f inal) = h (1) , · · · h (n) .
Such mechanism allows weight sharing across graph nodes, which helps to reduce overfitting problems. To further reduce the number of parameters and overfitting problems, we apply the softmax weight tying technique (Press and Wolf, 2017) by tying source embeddings and target embeddings with a target softmax weight matrix.</p>
<p>Attention-based LSTM Decoder</p>
<p>We adopt the commonly-used standard attentionbased LSTM as our decoder, where every next word y t is generated by conditioning on the final graph representation h (f inal) and all words that have been predicted y 1 , ..., y t−1 . The training objective is to minimize the negative conditional loglikelihood. Therefore, the objective function can be written as:
L = − T t=1 log p θ (y t |y 1 , ..., y t−1 , h (f inal) )
where T represents the length of the output sequence, and p is the probability of decoding each word y t parameterized by θ. As shown in the decoder from Figure 3, we stack 2 LSTM layers and apply a cross-attention mechanism in our decoder.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>We implement our MGCN architecture based on MXNET (Chen et al., 2015) and Sockeye tookit. Hidden units and embedding dimensions for both encoder and decoder are fixed at 360. We use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.0003 and update parameters with batch size of 16. The training phase is stopped when detecting the convergence of perplexity on the validation set. During the decoding phase, we use beam search with beam size of 10. We evaluate our models by applying both automatic and human evaluations. For automatic evaluation, we use several common evaluation metrics: BLEU (Papineni et al., 2002), CHRF++ (Beck et al., 2018), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), ROUGE 1 , ROUGE 2 , ROUGE L (Lin, 2004). We adapt Mul-  tEval (Clark et al., 2011) and Py-rouge for resampling and significance testing.</p>
<p>Main Experimental Results</p>
<p>Here we present our main experiments on our prepared ENT-DESC dataset. We compare our proposed MGCN models with various aggregation methods against several strong GCN baselines including sequence-to-sequence (S2S) (Bahdanau et al., 2014), GraphTransformer (Koncel-Kedziorski et al., 2019), GRN (Beck et al., 2018), GCN (Marcheggiani and Perez-Beltrachini, 2018) and DeepGCN (Guo et al., 2019), and report the results on the test set. We re-arrange the order of input triples following the occurrence of entities in output for S2S model. We re-implemented GRN, GCN and DeepGCN using MXNET. Furthermore, we apply a delexicalization technique on our dataset. Unlike previous delexicalization work by applying name entity anonymization (Konstas et al., 2017), we delexicalize the main entity and topic-related entities by replacing these entities with a token indicating the entity type and index. Main results on our ENT-DESC dataset are shown in Table 2. Here, number of layers in all baseline models and our MGCN models are set to be 6 for fair comparison. Our models consistently outperform the baseline models on all evaluation metrics. S2S model has poor performance mainly because the structure of our input triples is complicated as explained earlier. Compared to GRN and GCN models, the BLEU score of MGCN model increases by 1.3 and 0.9 respectively. This result demonstrates the effectiveness of multi-graph transformation, which is able to capture more comprehensive information compared to Levi graph transformation, especially entity-to-entity information in the original graph. We then apply multiple methods of aggregation on top of multi-graph structure. MGCN+CNN and MGCN+SUM report the highest BLEU score 26.4, followed by MGCN+AVG. By applying our delexicalization technique, the results are further boosted by 3.2 to 3.6 BLEU score for both baseline and our proposed models.</p>
<p>Analysis and Discussion</p>
<p>Effect of different numbers of MGCN layers.</p>
<p>In order to examine the robustness of our MGCN models, we conduct contrast experiments by using different numbers of MGCN layers. The results are shown in Figure 5. We use MGCN to compare with the strongest baseline models using GCN according to the results in Table 2. More specifically, we compare to GCN model on 2 to 9 layers and DeepGCN on 9, 18, 27 and 36 layers. Both models perform better initially as more GCN/MGCN layers are being stacked, and start to drop afterwards. DeepMGCN, achieving 26.3 BLEU score at 18 MGCN layers, is 1.0 BLEU higher than deepGCN model. As shown in the line chart ( Figure 5), MGCN achieves a decent increase of 0.3 to 1.0 from 2 to 36 layers. This again shows robust improvements by explicitly representing the all types of information in the graph than learning the information implicitly. Another observation is that the BLUE of MGCN model at 3 layers (25.4) is already higher than the best performance of GCN/deepGCN model regardless of number of layers.   Effect of MGCN+SUM on various numbers of input triples. In order to have a deeper understanding on how multi-graph transformation helps the generation, we further explore the model performance under different numbers of triples on the test set. Table 3 shows the BLEU comparison between MGCN+SUM and GCN when using 6 layers. Both models perform the best when number of triples is between 31 and 50. They both have a poorer performance when the number of triples is too small or too large. With small amount of triples, the models have insufficient information to generate accurate descriptions while large amount of triples make it challenging for the models to select meaningful information. Another observation is that the improvement of BLEU (∆) by our model is higher with smaller number of triples.</p>
<p>Ablation Study. To examine the impact of each graph in our multi-graph structure, we show the ablation study in Table 4. Each transformed graph is removed respectively from MGCN+SUM with 6 layers except for the g 1 (self ) which is always enforced in the graph (Kipf and Welling, 2016). We notice that the result drops after removing any transformed graph from the multi graph. Particularly, we observe that the importance of {default2, reverse2} and {default1, reverse1} are equivalent, as the BLEU after removing them individually are almost the same. This explains how multi-graph structure addresses the deficiency of Levi graph (i.e. entity-to-entity information is not represented explicitly in Levi graph). Additionally from the results, it is beneficial to represent the edges in reverse direction for more effective information extraction in directed graphs as there are Gold The New Jersey Symphony Orchestra is an American symphony orchestra based in the state of New Jersey . The NJSO is the state orchestra of New Jersey, performing concert series in six venues across the state, and is the resident orchestra of the New Jersey Performing Arts Center in Newark, New Jersey .</p>
<p>GCN</p>
<p>The Newark Philharmonic Orchestra is an American orchestra based in Newark, New Jersey , United States.</p>
<p>MGCN +SUM</p>
<p>The New Jersey Symphony Orchestra is an American chamber orchestra based in Newark, New Jersey . The orchestra performs at the Newark Symphony Center at the Newark Symphony Center in Newark, New Jersey . relatively larger gaps in BLEU drop after removing g 3 (reverse1) and g 1 (reverse2).</p>
<p>Case Study. Table 5 shows example outputs generated by GCN and MGCN+SUM, as compared to the gold reference. The main entity is highlighted in red, while topic-related entities are highlighted in blue. Given the KG containing all these entities, we intend to generate the description about "New Jersey Symphony Orchestra". Firstly, MGCN+SUM is able to cover the main entity and most topic-related entities correctly, while GCN model fails to identify the main entity. This suggests that without multigraph transformation or effective aggregation methods, it is hard for GCN to extract useful information given large number of triples in the KG. Lengthwise, the output generated by MGCN+SUM is relatively longer than the one generated by GCN, and thus covers more information. We attribute the reason to GCN's deficiency of information loss as mentioned earlier.</p>
<p>Figure 1 :
1An example showing our proposed task.</p>
<p>of the input triples in each dataset.(b) Length of the output texts in each dataset.</p>
<p>Figure 2 :
2Dataset comparison among WebNLG, AGENDA and our ENT-DESC.</p>
<p>Figure 4 :
4An example of multi-graph transformation.based and CNN-based aggregation.</p>
<p>Figure 5 :
5Effect of MGCN/deepMGCN on different number of layers.</p>
<p>Table 2 :
2Main results of models on ENT-DESC dataset. ↓ indicates lower is better.</p>
<p>Table 3 :
3Effect of MGCN+SUM on different numbers of input triples.Model 
BLEU ∆ (BLEU) </p>
<p>MGCN + SUM 
26.4 
-
-g 6 : global 
26.0 
-0.4 
-g 5 : reverse2 
25.8 
-0.6 
-g 4 : default2 
26.1 
-0.3 
-g 3 : reverse1 
25.7 
-0.7 
-g 2 : default1 
26.1 
-0.3 </p>
<p>MGCN 
25.7 
-0.7 
GCN 
24.8 
-1.4 </p>
<p>Table 4 :
4Results of the ablation study.</p>
<p>Table 5 :
5An example of generated sentences.
https://en.wikipedia.org/wiki/Bruno Mars 3 https://www.nayuki.io/page/ computing-wikipedias-internal-pageranks
Conclusions and Future WorkWe presented a practical task of generating sentences from relevant entities empowered by KG. We further constructed a large-scale and challenging dataset ENT-DESC to facilitate the study of this task. To overcome the limitations of previous graph-to-sequence models on large KGs, we proposed a multi-graph convolutional networks (MGCN) incorporated with multiple aggregation methods to capture the rich structured information in the KG. Thorough experiments and analysis show the effectiveness of our model architecture. In the future, we will consider incorporating pretrain knowledge for more informative generation, and also explore applying MGCN on other NLP tasks for better information extraction and aggregation.
N-gcn: Multi-scale graph convolution for semi-supervised node classification. Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, Joonseok Lee, Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. 2018. N-gcn: Multi-scale graph con- volution for semi-supervised node classification.</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, KyungHyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</p>
<p>Graph convolutional encoders for syntax-aware neural machine translation. Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Simaan, Proceedings of EMNLP. EMNLPJoost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Simaan. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In Proceedings of EMNLP.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, Proceedings of ACL. ACLDaniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of ACL.</p>
<p>Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. Tianqi Chen, Mu Li, Minlin Yutianli, Min-jieWang NaiyanWang, BingXu TianjunXiao, and ZhengZhang ChiyuanZhangTianqi Chen, Mu Li, MinLin YutianLi, Min- jieWang NaiyanWang, BingXu TianjunXiao, and ZhengZhang ChiyuanZhang. 2015. Mxnet: A flexi- ble and efficient machine learning library for hetero- geneous distributed systems.</p>
<p>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. H Jonathan, Chris Clark, Alon Dyer, Noah A Lavie, Smith, Proceedings of ACL. ACLJonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer insta- bility. In Proceedings of ACL.</p>
<p>Natural language interfaces to ontologies: combining syntactic analysis and ontology-based lookup through the user interaction. Danica Damljanovic, Milan Agatonovic, Hamish Cunningham, Proceedings of the 7th international conference on The Semantic Web: research and Applications. the 7th international conference on The Semantic Web: research and ApplicationsPart IDanica Damljanovic, Milan Agatonovic, and Hamish Cunningham. 2010. Natural language interfaces to ontologies: combining syntactic analysis and ontology-based lookup through the user interaction. In Proceedings of the 7th international conference on The Semantic Web: research and Applications- Volume Part I.</p>
<p>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. Michael Denkowski, Alon Lavie, Proceedings of the sixth workshop on statistical machine translation. the sixth workshop on statistical machine translationAssociation for Computational LinguisticsMichael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and eval- uation of machine translation systems. In Proceed- ings of the sixth workshop on statistical machine translation, pages 85-91. Association for Computa- tional Linguistics.</p>
<p>Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge. Ondřej Dušek, Jekaterina Novikova, Verena Rieser, Computer Speech &amp; Language. Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge. Computer Speech &amp; Language.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alán Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in neural information processing systems. David K Duvenaud, Dougal Maclaurin, Jorge Ipar- raguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convo- lutional networks on graphs for learning molecular fingerprints. In Advances in neural information pro- cessing systems, pages 2224-2232.</p>
<p>The webnlg challenge: Generating text from rdf data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, Proceedings of INLG. INLGClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from rdf data. In Pro- ceedings of INLG.</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, TACL. 7Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional net- works for graph-to-sequence learning. TACL, 7:297- 312.</p>
<p>Molecular graph convolutions: moving beyond fingerprints. Steven Kearnes, Kevin Mccloskey, Marc Berndl, Vijay Pande, Patrick Riley, Journal of Computer-Aided Molecular Design. 830Steven Kearnes, Kevin McCloskey, Marc Berndl, Vi- jay Pande, and Patrick Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. Journal of Computer-Aided Molecular Design, 30(8).</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Technical reportDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. Technical re- port.</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, Thomas N Kipf and Max Welling. 2016. Semi- supervised classification with graph convolutional networks.</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, Proceedings of ACL. ACLRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph transformers. In Proceedings of ACL.</p>
<p>Neural amr: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, Proceedings of ACL. ACLIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and gen- eration. In Proceedings of ACL.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, Proceedings of EMNLP. EMNLPRémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In Proceedings of EMNLP.</p>
<p>Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out. Chin-Yew Lin, Chin-Yew Lin. 2004. Rouge: A package for auto- matic evaluation of summaries. Text Summarization Branches Out.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, Proceedings of INLG. INLGDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for struc- tured data to text generation. In Proceedings of INLG.</p>
<p>Encoding sentences with graph convolutional networks for semantic role labeling. Diego Marcheggiani, Ivan Titov, Proceedings of EMNLP. EMNLPDiego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for se- mantic role labeling. In Proceedings of EMNLP.</p>
<p>The E2E dataset: New challenges for endto-end generation. Jekaterina Novikova, Ondrej Dušek, Verena Rieser, 1706.09254Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 18th Annual Meeting of the Special Interest Group on Discourse and DialogueSaarbrücken, GermanyJekaterina Novikova, Ondrej Dušek, and Verena Rieser. 2017. The E2E dataset: New challenges for end- to-end generation. In Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Saarbrücken, Germany. ArXiv:1706.09254.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of ACL. ACLKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of ACL.</p>
<p>Using the output embedding to improve language models. Ofir Press, Lior Wolf, 157Ofir Press and Lior Wolf. 2017. Using the output em- bedding to improve language models. EACL, page 157.</p>
<p>A study of translation edit rate with targeted human annotation. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, Proceedings of association for machine translation in the Americas. association for machine translation in the Americas200Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin- nea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine transla- tion in the Americas, volume 200.</p>
<p>Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks. Rion Snow, O&apos; Brendan, Daniel Connor, Andrew Y Jurafsky, Ng, Proceedings of EMNLP. EMNLPRion Snow, Brendan O'Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast-but is it good?: evaluating non-expert annotations for natu- ral language tasks. In Proceedings of EMNLP.</p>
<p>. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, Graph attention networksPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks.</p>
<p>Representation learning on graphs with jumping knowledge networks. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-Ichi Kawarabayashi, Stefanie Jegelka, ICML. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomo- hiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs with jumping knowledge networks. In ICML, pages 5449-5458.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6756 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6756</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6756</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-c715914c388fa64dd8686cd8755e5adfebbf2388</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c715914c388fa64dd8686cd8755e5adfebbf2388" target="_blank">REFINER: Reasoning Feedback on Intermediate Representations</a></p>
                <p><strong>Paper Venue:</strong> Conference of the European Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> REFINER is a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning that provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences,e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial contextand lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6756",
    "paper_id": "paper-c715914c388fa64dd8686cd8755e5adfebbf2388",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00579225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REFINER: Reasoning Feedback on Intermediate Representations</h1>
<p>Debjit Paul ${ }^{\mathbf{a}}$, Mete Ismayilzada ${ }^{\mathbf{A}}$, Maxime Peyrard ${ }^{\diamond *}$, Beatriz Borges ${ }^{\mathbf{A}}$, Antoine Bosselut ${ }^{\mathbf{B}}$, Robert West ${ }^{\mathbf{B}}$, Boi Faltings ${ }^{\mathbf{B}}$<br>${ }^{\text {EPFL }}$<br>${ }^{\diamond}$ Université Grenoble Alpes, CNRS, Grenoble INP, LIG<br>{firstname.lastname}@epfl.ch</p>
<h4>Abstract</h4>
<p>Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 or ChatGPT as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have made significant strides in natural language processing (NLP) tasks (Brown et al., 2020). Recent work has shown that explicitly generating intermediate steps during reasoning tasks significantly improves a model's performance and interpretability (Shwartz et al., 2020; Paul and Frank, 2021; Marasovic et al., 2022; Lampinen et al., 2022; Wei et al., 2022). Producing such intermediate representations provides insight into the model's predictions and allows humans to inspect the model's reasoning process. However, these intermediate representations ${ }^{1}$ can be unreliable (Ye and Durrett, 2022) and result in poor</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: REFINER example. The critic model provides the generator model with feedback on its reasoning errors after evaluating the generated intermediate steps. The feedback, alongside the original question and previous intermediate equation, are fed back to the generator model.
performance on downstream reasoning tasks. Most importantly, it is unclear how to meaningfully refine the intermediate representations to further improve the final performance.</p>
<p>The standard practice for correcting reasoning errors is to annotate new data and either retrain or finetune the model (Feng et al., 2021; Hedderich et al., 2021). However, fixing such errors by finetuning with more data is not only data- and resource-intensive but can also be insufficient to generalize well in complex reasoning tasks (Ward et al., 2022). Other works have explored improving models using feedback by providing a scalar reward (Ziegler et al., 2019; Martin et al., 2022) or directly revealing the correct missing answer (Mehta and Goldwasser, 2019; Elgohary et al., 2021; Tandon et al., 2022). However, in natural language reasoning tasks, defining a reward that captures different fine-grained reasoning error types (e.g., semantic consistency, logical, etc.) remains an open challenge (Golovneva et al., 2023). Additionally, such</p>
<p>a reward provides a relatively sparse training signal.</p>
<p>In this work, we instead provide fine-grained and structured feedback on reasoning errors. We present REFINER, a novel interaction-based framework that allows a generator LM to iteratively use fine-grained feedback and refine its reasoning. The interaction happens between two models: a generator, which learns to solve the task by first generating the intermediate reasoning steps, and a critic, which provides structured feedback to the generator about errors in the intermediate steps.</p>
<p>To provide fine-grained feedback about reasoning errors, we develop a scheme to independently train the critic model on automatically constructed feedback data. More specifically, we create pairs of incorrect intermediate representations and structured $^{2}$ feedback on their fine-grained reasoning errors. Then, we use this data to train the critic to provide fine-grained feedback on erroneous intermediate reasoning steps. Finally, the critic interacts with the generator LM, offering feedback both during the training of the generator and during inference.</p>
<p>Figure 1 illustrates an example of our REFINER framework where, given a math word problem, the generator generates an equation as an intermediate representation. The critic identifies the errors in the equation and provides semi-structured textual feedback (e.g., "the operator in #0 is incorrect") to the generator. By interacting with the critic, REFINER enables the generator to reason over the semi-structured feedback and refine its generation.</p>
<p>Contributions. (i) We propose REFINER, a framework that refines LMs reasoning capabilities through feedback. Our work investigates how interacting with fine-grained reasoning feedback on intermediate reasoning steps impacts the performance of LMs on reasoning tasks. We evaluate REFINER on three natural language reasoning tasks: math word problems, synthetic natural language reasoning, and moral action generation. REFINER demonstrates significant performance gains across different LM architectures with different scales. Across different reasoning tasks, REFINER outperforms comparably-sized strong fine-tuned LM baselines (by $+13.1,+3.2,+15$ pts., respectively). (ii) We empirically demonstrate that for math word problems and synthetic natural language reasoning,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>our trained critic models alone are beneficial for improving intermediate representations as they help GPT-3.5 significantly increase its performance in a few-shot setting (by $+3.5,+6.8$ pts., respectively). We also demonstrate that providing structured feedback on fine-grained errors can benefit more than scalar value feedback for moral action generation and math word problem tasks. Our critic model acts as a 'reasoning refinement tool' for LLMs. (iii) We show that REFINER can substantially outperform other refinement methods that use feedback from large LMs, such as self-refine. (iv) Our analyses illustrate that (a) improving the intermediate representation generation improves the performance on the reasoning tasks, and (b) training a generator with an imperfect (noisy) critic is still beneficial. Our code is made publicly available ${ }^{3}$.</p>
<h2>2 Related Work</h2>
<p>Intermediate Representations. While state-of-the-art LMs achieve incredible performances in a wide range of tasks, they have difficulty with many reasoning tasks (Wang et al., 2022), especially ones with multiple constraints or sub-problems or requiring specialized knowledge (Austin et al., 2021) such as mathematical problem solving (Ling et al., 2017; Andor et al., 2019; Ran et al., 2019; Geva et al., 2020; Piękos et al., 2021; Cobbe et al., 2021a; Kim et al., 2022).</p>
<p>For these tasks, both intermediate representations and rationales have been shown to be beneficial in learning mathematical skills (Piękos et al., 2021), intermediate program execution computations (Nye et al., 2021), or general reasoning outputs (Wei et al., 2022; Golovneva et al., 2022).</p>
<p>Our work builds upon the observation that generating intermediate steps are valuable but distinguishes itself in several key aspects. Firstly, instead of prompting a large model, we finetune smaller models to learn to generate intermediate steps. Secondly, our framework can accommodate tasks that do not necessarily have unique closed-form correct answer, such as the Moral Norm task (see §3). Finally, our framework is trained with a critic providing feedback, improving the model's reasoning process and teaching it how to leverage feedback.</p>
<p>Natural Language Feedback. Recent work has explored giving models richer and more complex feedback through the use of natural language (Ziegler et al., 2019; Nguyen et al., 2021; Scheurer</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>et al., 2022), used for aligning LLMs' output with users' preferences (Christiano et al., 2017; Ziegler et al., 2019; Saunders et al., 2022; Scheurer et al., 2022; Bai et al., 2022), or to directly improve the model's performance in its current task (Weston, 2016; Rupprecht et al., 2018; Elgohary et al., 2020; Austin et al., 2021; Madaan et al., 2023). This training depends on human-created feedback, generated in large quantities (Bai et al., 2022), which takes up considerable resources. Though an external feedback provider can guide models to correct answers and reasoning (Austin et al., 2021), demonstrably better than they can themselves (Saunders et al., 2022), feedback has rarely been used in this way - and automated critics for reasoning tasks have proved to be difficult (Scheurer et al., 2022; Wang et al., 2022; Huang et al., 2022).</p>
<p>Recently, Welleck et al. (2022) introduced a secondary model, the corrector, which improves the initial proposition of a generation model, by learning the kind of mistakes made by the generator and how to fix them. In this work, we also use a secondary model, a critic, but apply it quite differently as we integrate it into an interaction loop with the generator model during training. We further differ from previous works as we provide feedback at the intermediate reasoning steps of the model and not at the final output. The feedback is thus closer to the source of mistakes and guides the model's reasoning toward the correct answer. Additionally, intermediate steps are often structured, allowing the critic to provide precise feedback.</p>
<h2>3 REFINER</h2>
<p>Problem Formulation. In this paper, we view natural language reasoning (NLR) as an autoregressive generation task where, given input context $x$, a model needs to generate $y$, such that $y$ satisfies the constraints of the task. Usually, to generate correct or plausible $y$, the model needs to make the correct inference $z$ as intermediate steps. ${ }^{4}$ We decompose NLR tasks as follows: $p(y \mid x)=p(y \mid x, z) p(z \mid x)$. In practice, one can compute each conditional using an LM that includes its conditioning variables as a part of its input.</p>
<p>Before continuing with the model description, we describe three NLR tasks where we conduct our study and their respective intermediate representation $z$. We deliberately chose these three tasks</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>since they broadly cover two types of reasoning: (i) logical reasoning and (ii) normative reasoning. They are exemplified in Appx Fig. 6 and detailed below.</p>
<p>Math word problem (MWP), where given a word problem $x$ consisting of a context and question, the goal is to map $x$ to a valid mathematical expression $z$ (the intermediate representation) and then to a solution $y$. This task requires the model to perform deduction using mathematical reasoning. Synthetic natural language reasoning (sNLR), where given a reasoning scenario $x$ consisting of 5 synthetic rules and a fact, the model needs to deduce a conclusion $y$. This task requires the model to perform deductive reasoning and generate intermediate steps $z$ and the conclusion $y$ using closedworld rules and facts.
Moral norm and action generation for moral stories (MS), where given a context $x$ consisting of a situation, an intention, and an immoral action, the model needs to generate the moral norm $z$ and the moral action $y$. Moral actions are encouraged by the moral norm. This task requires the model to perform abductive reasoning to generate moral norms and deductive reasoning for moral action.</p>
<p>We propose to solve these tasks by forcing the model to generate intermediate hypotheses $(z)$ and improving them via structured feedback. We introduce an interactive framework, REFINER, made of two separate models: (a) a CRITIC model (§3.1) trained to provide structured feedback on intermediate reasoning steps and (b) a GENERATOR model trained to solve the reasoning task by first generating intermediate reasoning steps (§3.2). The core idea of REFINER is to exploit the interaction between the generator model and the critic model, where the generator's intermediate reasoning steps are improved via structured feedback from the critic.</p>
<p>REFINER presents several important properties. First, the generator is trained to incorporate and leverage feedback, which helps it converge towards better reasoning during training and makes it capable of integrating feedback at test time, whether from a trained critic or a human (see §5). Second, the trained critic can be useful on its own; we demonstrate that a generalist LLM like GPT-3.5 can significantly benefit from interacting with our trained critic on the reasoning tasks we consider (see §5). Finally, having two separate models allows us to easily measure the benefits of feedback during training and/or during inference (see §6).</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>Error Types</th>
<th>Feedbacks</th>
</tr>
</thead>
<tbody>
<tr>
<td>MWP</td>
<td>Incorrect Numbers</td>
<td>The position number in <br> equation-number is incorrect. <br> The operator in <br> equation-number is incorrect.</td>
</tr>
<tr>
<td></td>
<td>Incorrect Operators</td>
<td>An operator is missing.</td>
</tr>
<tr>
<td></td>
<td>Missing Operators</td>
<td>The $X$ operator makes inference <br> rule number invalid.</td>
</tr>
<tr>
<td>sNLR</td>
<td>Missing Link</td>
<td>Missing link between the fact the rules.</td>
</tr>
<tr>
<td></td>
<td>Missing Implicit</td>
<td>The implicit knowledge is</td>
</tr>
<tr>
<td></td>
<td>Knowledge Step</td>
<td>missing.</td>
</tr>
<tr>
<td>MS</td>
<td>Contradiction</td>
<td>Contradiction</td>
</tr>
<tr>
<td></td>
<td>Semantic Misalignment</td>
<td>Semantically misaligned: " text snippet"</td>
</tr>
</tbody>
</table>
<p>Table 1: An overview of the Error Types and Feedbacks for each reasoning tasks.</p>
<h3>3.1 CRITIC Model</h3>
<p>The role of the critic is to provide feedback on the intermediate hypotheses produced by the generator model. One way to evaluate the quality of the hypothesis and produce feedback on the hypothesis $z$, would be to compare it against a gold hypothesis $z^{*}$. Previous works employed automatic metrics like BLEU, ROUGE, etc., as value functions (Wu et al., 2018; Ramamurthy et al., 2022). However, these scalar value functions are not suitable for natural language reasoning tasks because (i) it is unclear how to define a scalar value function that can encapsulate fine-grained reasoning errors (Golovneva et al., 2023) and (ii) during inference, these functions require access to the gold hypothesis (which is unavailable in practice). Therefore, we train a critic model and endow it with the ability to evaluate the hypothesis in a fine-grained manner and provide structured feedback.</p>
<p>Feedback Data Generation. To train the critic, we have to create example pairs of implausible hypotheses and their corresponding feedback with fine-grained reasoning errors. Inspired by Golovneva et al. (2023) and Talmor et al. (2020), we first define fine-grained reasoning error types for each reasoning task (see Table 1). For MWP, an equation can be incorrect due to: (i) the operands or operators in the equations being incorrect and/or (ii) one or more operators missing. For sNLR, an inference rule can be incorrect because it is (i) logically invalid and/or (ii) missing reasoning rules (failing to connect the correct facts with correct rules or missing implicit knowledge). For MS, a moral norm can be incorrect due to (i) contradiction and/or (ii) semantic misalignment.</p>
<p>Based on these error types, we propose two strategies to create the feedback data: (i) Rulebased perturbation strategy: we perturb the plausible hypotheses $(z)$ in the training data and collect
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of REFINER interaction loop. Left side: Training the critic model. Right side: In each iteration, the generator generates multiple hypotheses. The critic randomly selects one hypothesis and provides feedback based on reasoning errors.
a pool of data $D$ ( $x$ : input, $z$ : plausible hypothesis, $z^{\prime}$ : implausible hypothesis). We perturb by omitting, replacing or adding some tokens or some rules from the plausible hypothesis to create an implausible hypothesis automatically (details in Appendix F.1). (ii) Synthetic Generation strategy: we prompted OpenAI's GPT-3.5 to generate implausible hypotheses based on the error types automatically. We used a few-shot setting where we varied the instruction, the number of demonstrations, and the formatting of the demonstrations (details in Appendix F.2).</p>
<p>Since our perturbations and automatic implausible hypotheses are based on logic and reasoning errors, we create structured feedback $f$ for every example $\left(x, z, z^{\prime}\right)$ by stating the error type that occurs in $z^{\prime}$ but not in $z$ (see Table 1). The basic structure of feedback $f$ for these tasks is $\langle$ error type, position (optional), hint (optional) $\rangle$, where position denotes the error position in the implausible hypothesis (see Table 1). Despite the simplicity of the strategy we used for our tasks, this approach is easily generalisable to other reasoning tasks.</p>
<p>We also replace the correct judgment with random judgments to scale the number of implausible hypotheses per example. Finally, as feedback $f$, we provide <error type, hint $>$. For non-monotonic reasoning tasks like norm and action generation, the critic should be able to provide hints that align the generator model's objective to the reasoning task. Hence, as a hint, we provide verb phrases from the norms. Since the critic provides textual feedback to the generator, we convert the structured feedback into natural language feedback ${ }^{5}$. Formally, we create a data pool $D=\left{x, z, z^{\prime}, f\right}$ to train a critic model.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Training the critic model. We train a supervised CRITIC model $\left(\pi_{\beta}\right)$ with the context $(x)$ and (plausible or implausible) hypothesis ( $z$ or $z^{\prime}$ ) as input and the textual feedback as output. We update the CRITIC with the cross-entropy loss: $L(\beta)=-\log p_{\beta}(f(u) \mid x, u)$ where $u \in z, z^{\prime}$. The trained critic is only used during inference. The oracle critic is used while training the generator.</p>
<h3>3.2 GENERATOR Model</h3>
<p>This section presents a generator model that iteratively learns to interact with the CRITIC model.</p>
<p>Warm-up. Given a context $x$ the generator model $\left(\pi_{\theta}\right)$ is trained to generate plausible hypotheses. The warm-up phase is critical to ensure that, when the critic comes in the loop, the generator does not produce random answers likely to be bad, given the size of the output space. As such, we use a small supervised dataset ( $10 \%$ training data) to fine-tune the model on the NLR task of interest. After the warm-up phase, we use the additional feedback $f$ from the critic model and learn $\pi_{\theta}\left(z \mid x, z^{\prime}, f\right)$.</p>
<p>Exploration. At each iteration $(t)$, the generator model generates multiple hypotheses $\left(z^{k}\right)$ using nucleus sampling. The critic model randomly selects one hypothesis and provides feedback on that hypothesis. The exploration step aims at increasing the output variance such that the generator receives a wide range of feedback during training.</p>
<p>Learning. We update the GENERATOR model using the following cross-entropy loss: $L(\theta)=$ $-\sum_{t=1}^{T} \log p_{\theta}\left(z_{t} \mid x, z_{t}^{\prime}, f_{t}\left(z^{\prime}\right)\right)$ where $T=$ total number of iterations. Since the feedback contains the error types and hints, which are (latent) fine-grained and logical, it should allow the model to learn and update its generation by addressing the reasoning errors mentioned in the feedback.</p>
<p>Inference. We use the trained critic along with the trained generator to generate a trajectory $z_{0}, z_{1}, \ldots, z_{T}$ and stop when either $f\left(z_{t}\right)$ is generated by the generator or "No hint" is generated by the critic. We also experimented with chain of thought prompting, where the generator generates a trajectory $z_{0} y_{0}, z_{1} y_{1}, \ldots, z_{T} y_{T}$ and stops when the critic generates "No hint".</p>
<h2>4 Experimental Setup</h2>
<p>Datasets. We evaluate REFINER on three diverse tasks (examples in Fig. 6). We briefly describe the datasets used for each task below.Math Word</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Generator Model</th>
<th style="text-align: center;">Eq. (2)</th>
<th style="text-align: center;">Ans. ( $\gamma$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UQA-base</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">UQA-base + PPO</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {base }}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 2}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">UQA-large</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">UQA-large + PPO</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {large }}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 8}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 + CoT</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 + CoT + REFINER $_{\text {critic }}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on MWP. Comparison of REFINER with baselines on the SVAMP dataset. The average score over three runs is reported ( $\mathrm{p}&lt;0.05$ ). For models other than GPT-3.5, the answer can be obtained via symbolic execution of the equation and is thus a function of the validity of the equation.</p>
<p>Problem (MWP): We train our models on MAWPs (Koncel-Kedziorski et al., 2016) dataset and evaluated our models on a challenging dataset SVAMP (Patel et al., 2021). We evaluate our model on both the equation generation $(z)$ and answer prediction (y) tasks. Similar to Ling et al. (2017); Amini et al. (2019) for equation generation, we replace the numeric values with variable names, for example, number0, number1, etc. Further, we also evaluated on GSM8K (Cobbe et al., 2021b) dataset which consists of 8.5 K high-quality linguistically diverse grade school math word problems. For Synthetic Natural Language Reasoning (sNLR), we use the dataset from Liang et al. (2022) with the difficulty level as hard. We evaluate our model on both inference rule generation $(z)$ and consequent generation $(y)$. For Moral Story (MS), we use a dataset from (Emelin et al., 2021), where we evaluate our model on moral norm $z$ and the moral action $y$ generation.</p>
<p>Training Details. For each task, we train a UnifiedQa-T5-base model (UQA-base) (Khashabi et al., 2020) as a critic (§3.1). For exploration (§3.2), we use nucleus sampling with $p=0.5$. We select the hyper-parameters by the validation loss: for both the generator and critic model, we use the Adam optimizer with a learning rate of $1 e^{-4}$. Each model is trained for 20 epochs with early stopping based on validation loss. We trained all models on one A100 GPU. We run our models with 3 random seeds and report the average results. For the human study, we selected outputs from the best models (baselines and our model) according to automatic metrics. We train models with $T=3$ iterations.</p>
<p>At inference time, we use greedy decoding for the generator and critic model with $T=1$ for the automatic critic and $T=3$ for the oracle critic.</p>
<p>On the MWP and sNLR tasks, we use the exact match (EM) metric for intermediate steps (equation generation and inference rules) and accuracy (Acc) for the final answers. For MS, we conduct a manual evaluation study to assess the relevance of norms and moral actions ${ }^{6}$. Further evaluation details are provided in Appendix G. To train the critic model, we used the feedback data generated using the rulebased perturbation strategy (see §3.1).</p>
<p>Baselines. We compare our method with three different LMs as generator models: UQAbase, UQA-large (supervised setting), GPT-3.5-text-DaVinci-003 and ChatGPT (few-shot setting). We also compare REFINER to Proximal Policy Optimization (PPO) RL-based method (Schulman et al., 2017). We use the implementation of PPO from (Ramamurthy et al., 2022). For GPT-3.5, we provide 2 for demonstrations per class. We also experimented with chain of thought (COT) prompting (Wei et al., 2022) where the model is prompted first to generate the intermediate steps $(z)$ and then the final answer $(y)$. Note that the sNLR task is a synthetic task where the model needs to perform either one-hop or two-hop reasoning. Clark et al. (2021) showed that fine-tuning large language models ( 354 M parameter size) could achieve ( $99 \%$ accuracy) high performance. Hence, we only compare our REFINER model with the UQA-base model (220M) (see Table 3). Since human annotation is expensive, we focus on comparing against the most meaningful baseline: UQA-large for MS task (see Table 4). It is important to highlight that our proposed framework is general, and one can use any other LMs as GENERATOR or CRITIC.</p>
<h2>5 Results</h2>
<p>We evaluate our model on two aspects (i) performance on intermediate steps and (ii) performance on the final answer prediction. Tables 2, 3, and 4 show the performance comparisons.</p>
<p>Performance on Intermediate Steps. Table 2 reports the performance of the MWP task. We explored two different scenarios: (i) where the model only generates the equations $(z)$ with variable names replacing the numeric values, and (ii) where the model generates both the equations and the final answers together. We observe for both scenarios that REFINER significantly outperforms baseline models with com-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Results on sNLR task. The average score over three runs is reported ( $\mathrm{p}&lt;0.05$ ). IR: Inference Rules (Exact Match), Con: Consequent (Accuracy)</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Norm $(z)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Action $(y)$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$\mathbf{I} \downarrow$</td>
<td style="text-align: center;">$\mathbf{U} \downarrow$</td>
<td style="text-align: center;">$\mathbf{R} \uparrow$</td>
<td style="text-align: center;">$\alpha$</td>
<td style="text-align: center;">$\mathbf{I} \downarrow$</td>
<td style="text-align: center;">$\mathbf{U} \downarrow$</td>
<td style="text-align: center;">$\mathbf{R} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">B</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">58</td>
</tr>
<tr>
<td style="text-align: left;">B+PPO</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">52</td>
</tr>
<tr>
<td style="text-align: left;">REFINER</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$\mathbf{6 9}$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">$\mathbf{7 3}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results on Moral Norm and Moral Action. We report human evaluation. B: UQA-large; I: Irrelevant, U: Unsure; R: Relevant; $\alpha$ : Krippendorff's alpha
parable sizes. Notably, UQA-base benefits most ( +13.1 EM ) when adding a critic in the loop. We observe that GPT-3.5 significantly benefits from the REFINER trained critic. Since LLMs like GPT3.5 (175B parameters) are expensive to finetune, the improvement in equation generation of +3.2 EM without any modification is important. Interestingly, we observe that GPT-3.5 + COT manages to have significantly higher accuracy in answer $y$ than in equation $z$ (see Table 2). This result is similar to the observation made by Ye and Durrett (2022) and suggests that the intermediate equations can be unreliable. Finally, REFINER could even outperform PPO, which uses BLEU-score as a reward function. This suggests that semi-structured fine-grained textual feedback is more beneficial than value-based (where values are from automatic metrics) reward feedback. Note that this result may vary when these models are optimized directly with complex human values, as shown in Stiennon et al. (2020). Qualitatively, REFINER can correct incorrect equations through structured feedback, fixing the operators within a multistep solution (see Fig. 7).</p>
<p>For sNLR, similar to Liang et al. (2022), we observe that GPT-3.5 performs poorly (see Table 3). REFINER improves +2.9 , and +6.8 EM scores over UQA-base, and GPT-3.5, respectively. Contrary to the MWP, the final answer $y$ is not a symbolic execution away from the intermediate step $z$, but we still observe that REFINER focuses on improving the intermediate step $z$, resulting in significant improvements in the answer $y$ prediction. Again, we observe that REFINER with a UQAbase can outperform few-shot prompted GPT-3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Generator Model</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">ChatGPT</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: left;">Self-reflection</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">74.6</td>
</tr>
<tr>
<td style="text-align: left;">Self-refine</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">74.7</td>
</tr>
<tr>
<td style="text-align: left;">REFINER</td>
<td style="text-align: center;">$\mathbf{7 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 4}$</td>
<td style="text-align: center;">$\mathbf{6 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">ReACT</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: left;">ReACT + REFINER</td>
<td style="text-align: center;">$\mathbf{7 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Self-consistency</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: left;">Self-consistency + REFINER</td>
<td style="text-align: center;">$\mathbf{7 2 . 1}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison with different refinement methods on SVAMP and GSM8K datasets. Averaged accuracy over three runs on the test sets is reported ( $\mathrm{p}&lt;0.05$ ).</p>
<p>Thus, our critic can identify the fine-grained reasoning errors and help improve the performance on inference rules generation.</p>
<p>For MS, we assess the generation quality with three human judges who indicate whether the generated norms and moral actions are relevant to the given moral story. Table 4 summarises human evaluation results on 100 moral story examples randomly sampled from the MS test dataset. More specifically, we report evaluation breakdown for both norm and moral action by the number of instances that are either Irrelevant, Unsure or Relevant along with Krippendorf's $\alpha$ (Krippendorff, 2018) agreement scores. The results show an improvement of 20 points, increasing the relevance over a strong UQA-large baseline. Hence, this suggests that a specialized critic model with 3 times fewer parameters than the generator can improve the performance on generating reasoning steps.</p>
<p>Performance on Final Answer Prediction. We observe that REFINER outperforms the strong LM baselines by $+3.5,+3.2,+15$ points for MWP, sNLR, and MS, respectively. These results support our hypothesis that generating better intermediate steps can result in better answer prediction. Notably, on the sNLR task, for GPT-3.5, we observe that by adding a critic, there is an improvement of +6.8 in inference step generation; however, only +1.5 in the consequent prediction. This result indicates that LLMs may either not use these intermediate steps to perform the deduction or fail to perform deduction.</p>
<p>Comparing REFINER with other refinement methods. In Table 5, we compare REFINER with two other recent refinement methods: Self-refine (Madaan et al., 2023) and Self-reflection (Shinn et al., 2023) method on the SVAMP and GSM8K datasets. Both these baseline methods use LLMs</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Eq. (c)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REFINER $<em _rule-based="{rule-based" _text="\text">{\text {base }}+$ critic data $</em>$}</td>
<td style="text-align: left;">47.2</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $<em _inference="{inference" _text="\text">{\text {base }}-$ critic $</em>$}</td>
<td style="text-align: left;">39.8</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $<em _inference="{inference" _text="\text">{\text {base }}-$ critic $</em>-$ exp}</td>
<td style="text-align: left;">37.4</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $<em _text="\text" _training="{training">{\text {base }}-$ critic $</em>$}</td>
<td style="text-align: left;">34.1</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $<em _synthetic="{synthetic" _text="\text">{\text {base }}+$ critic data $</em>$}</td>
<td style="text-align: left;">44.1</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $<em _text="\text" _thniche="{thniche">{\text {base }}+$ critic $</em>$}</td>
<td style="text-align: left;">66.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Ablation Result on MWP task; Comparing model without critic during inference, and without the exploration (exp) phase during training. We report the exact match scores of the generated equation, comparable to Table 2.
to generate automatic feedback. Similar to Madaan et al. (2023), we observe that self-refine has minor improvement for MWP tasks. On the contrary, we find that REFINER significantly improves the performance of GPT-3.5 and ChatGPT by +3.3 and +2.2 on SVAMP and GSM8K datasets, respectively. This highlights the benefit of training a specialised critic that is grounded to the task. It can make LLMs more accurate than feedback from a general-purpose model (GPT-3.5 or ChatGPT). In Appendix $\S 6$, we have provided more details about the quality of feedback generated using our trained critic and GPT-3.5 (see Table 8). Further, we assess the performance of REFINER in improving the CoT generated by two recent methods: Self-Consistency (Wang et al., 2023) and ReACT method (Yao et al., 2023). We observe that REFINER can improve self-consistency and ReACT by +2.02 and +2.9 . This demonstrates that a trained critic can be used as a tool and can bring performance gains to different methods out-of-the-box (more details in Appendix §A.2).</p>
<p>Ablation. To obtain better insight into the contributions of the individual components of our models, we perform an ablation study (Table 6). We observe that there is a considerable drop in performance from 47.2 to 39.8 when we do not use the critic model during inference. Hence, this result indicates that our generator model can leverage the feedback from the critic at inference time. Further, we find that the exploration step improves the performance +3.3 over the baseline model. This result supports our hypothesis that the exploration step increases the output variance and gives the generator model the opportunity to learn over a wide range of feedback. We compared the performance with the critic model trained on two different training data (see §3.1). We find that the critic trained on small automatically generated data using GPT-3.5 works</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Error analysis. Number of errors made by baseline UQA-large and REFINER on 100 instances sampled randomly from test sets of both datasets. Errors are categorized according to Table 1).
better than without the critic in the loop. This result motivates researchers to use this method to generate negative samples to train their critic or preference learning model. Finally, we also observe that if the critic was perfect (Oracle), then REFINER can significantly improve the performance by fixing the mistakes generated by the generator model. This result indicates that REFINER can be seen as a framework that allows AI-AI and human-AI interaction.</p>
<h2>6 Analysis</h2>
<p>Error Analysis. In order to get more insight into the performance of our method, we conduct a finegrained error analysis on the MWP and MS datasets (Fig. 3). We note that the most frequent errors are Incorrect Numbers for MWP and Semantic Misalignment for MS. An intuitive reason can be that for the MWP task, the models are sensitive to the numbers order as argued in (Patel et al., 2021). For MS, generating norms grounded in the context is challenging. Our analyses show a clear trend that REFINER is able to considerably reduce the errors for both datasets. This indicates that our trained critic model could identify fine-grained reasoning errors during inference.</p>
<p>Noise Sensitivity. To further understand the behaviour of the REFINER framework, we run variations with noisy critics for the MWP task. We replace the oracle critic used during training with a noisy critic in (Fig. 4 (a)) to inspect how training with an imperfect critic impacts the generator. We also use a noisy critic at inference while keep the oracle critic during training (in Fig. 4 (b)). The noisy critics are generated by random perturbations of the oracle critic; for a noise-level $\epsilon$, the oracle feedback is replaced by random feedback with probability $\epsilon$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Noisy-critics analysis. In plot (a), we vary the noise level of the critic used during training ( 0 noise corresponds to oracle) and compare the resulting models when using the oracle and the training automatic critic during inference. In plot (b), we train with the oracle critic but vary the noise level of the critic used during inference.</p>
<p>Fig. 4 (a) shows that when training with a very noisy critic ( $&gt;75 \%$ noise), the generator LM learns to ignore the critic, as there is no difference between using the trained critic or the oracle during inference. Interestingly, training with a bit of noise ( $&lt;50 \%$ ) does not seem to harm the model, as performances are not statistically different than training with the oracle (noise of $0 \%$ ). Fig. 4 (b) depicts the quality of the critic used at inference time has a huge impact. Having oracle provide feedback is by far the best scenario. Already with $25 \%$ noise, the critic makes the generator perform worse than using our trained critic (REFINER). With more than $50 \%$ noise, the critic significantly harms the generator. The generator, trained with an oracle critic, has learned to trust the critic and expects useful feedback.</p>
<p>Qualitative Analysis. To explain the findings in $\S 6$, we further manually analyze 100 instances for the MWP task. We observe two different scenarios when REFINER failed to fix the outputs generated by GENERATOR model: (a) when the CRITIC model provides a correct feedback; however, the GENERATOR model still generates incorrect equation, and (b) the CRITIC model provides an incomplete or partially correct feedback. The former case indicates that either the GENERATOR model makes mistakes in following the instruction from the CRITIC or the feedback from the critic can be ambiguous. For example, in Appx Fig. 5, (b) we observe the case when the critic is correct, but the feedback could result in an incorrect equation. The latter case indicates that our trained critic model generates incorrect feedback, which can result in incorrect or partially correct equations. We also</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">UQA (220M)</th>
<th style="text-align: center;">UQA (770M)</th>
<th style="text-align: center;">GPT-3 (175B)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MWP</td>
<td style="text-align: center;">$69.5+/-2.6$</td>
<td style="text-align: center;">$73.4+/-3.7$</td>
<td style="text-align: center;">$63.5+/-5.6$</td>
</tr>
<tr>
<td style="text-align: left;">sNLR</td>
<td style="text-align: center;">$95.5+/-1.4$</td>
<td style="text-align: center;">$98+/-2.2$</td>
<td style="text-align: center;">$34.5+/-2.4$</td>
</tr>
<tr>
<td style="text-align: left;">MN</td>
<td style="text-align: center;">$77.4+/-2.5$</td>
<td style="text-align: center;">$80+/-4.5$</td>
<td style="text-align: center;">$76.4+/-3.5$</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparing the performance of different critic models. Exact-match score is reported.
observe that our CRITIC model failed to generate correct feedback when the GENERATOR model generates incorrect equations with multiple mistakes.</p>
<p>Quality of the feedback. To better understand the difference in the quality of the feedback, we compare our trained critic model with GPT-3.5. We assess the quality of the feedback on 500 instances per task and report the exact match scores in Table 8. Please note that we include instances where the critic feedback should say the solution is correct and hence generate 'No'. For GPT-3.5, we have provided (two) few-shot examples per type of error and two examples with 'No' as feedback. Our results show that trained critic (UQA) can comprehensively outperform GPT-3.5. We observe that GPT-3.5 performs well in identifying when the answer is correct. However, it makes errors when asked to generate meaningful semi-structured feedback for incorrect reasoning steps.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we propose REFINER, a framework to improve the reasoning abilities of LMs through an iterative feedback loop between two models, a generator and a critic. Our evaluation of this framework on three reasoning tasks showed structured and fine-grained feedback on intermediate reasoning errors results in significant performance gains, surpassing scalar value feedback. Our trained critic model alone, even when noisy, can improve intermediate representations of LMs, showing that REFINER can significantly boost LMs' performance on reasoning tasks. Our REFINER framework is very general and, in principle, might be applied to steer language models in performing different reasoning tasks. More specifically, the critic model can be seen as a tool for LLMs to refine their generation quality.</p>
<h2>Acknowledgment</h2>
<p>We would like to thank Martin Josifoski, Syrielle Montariol, and Zeming Chen for their helpful feedback on a draft version of the paper. We acknowledge the support of the ICT-48 Network of AI Re-
search Excellence Center "TAILOR" (EU Horizon 2020, GA No 952215). West's lab is partly supported by grants from the Swiss National Science Foundation (200021_185043), Swiss Data Science Center (P22_08), H2020 (952215), Microsoft Swiss Joint Research Center, and Google, and by generous gifts from Facebook, Google, and Microsoft. Antoine Bosselut gratefully acknowledges the support of Innosuisse under PFFS-21-29, the EPFL Science Seed Fund, the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI.</p>
<h2>Limitations</h2>
<p>Our REFINER framework could not be comprehensively evaluated on all applicable downstream reasoning tasks due to their sheer number. While deliberately distinct, we focused on only three different reasoning tasks in order to study how natural language reasoning feedback can impact downstream tasks. We believe this represents an initial but important step towards exploring automated natural language feedback on intermediate representations. In addition, the critic we presented here is specific for each task, while the ideal critic would be a general one, capable of providing feedback on a wide range of reasoning tasks. Similarly, we considered fine-grained reasoning errors specific to each reasoning task. Recent work has mentioned several other fine-grained reasoning errors (Golovneva et al., 2023), which can't be fully covered by the reasoning tasks we considered. Generalizing both the critic and fine-grained error types emerges as both the main limitations of this paper and the directions of future work. Finally, with LLMs being deployed more and more for real-life applications (medical domain, making important decisions), we believe it is crucial to develop expert models and automatic feedback mechanisms to inspect model generations and improve them. LLMs are impressive and work well on several NLP tasks, but they are not expert systems. Our work aims to address this gap by showing that adding interventions/feedback from critics (specialised finetuned critics) can help the LLM model to be more accu-rate-additionally, making the whole process more transparent.</p>
<h2>Ethical Considerations</h2>
<p>In this paper, we experiment with existing datasets which are, to the best of our knowledge, adequately cited. Our proposed framework REFINER is designed to improve the reasoning abilities of LMs. These LMs have been shown to encode biases about race, gender, and many other demographic attributes (Weidinger et al., 2021), (Sheng et al., 2020). Since our framework does not offer a way to mitigate these biases, models improved using this framework could still reflect the same harmful behaviours normally exhibited by these models. We recommend anyone deploying our model off-the-shelf should first check whether the model is harmful towards any protected group, and appropriate mitigation should be taken. In addition,
our MS task is based on a dataset of situations, intentions, and actions that heavily skew towards Western culture and social norms (Emelin et al., 2021). Consequently, our human evaluation on the MS task was done with AMT workers based in the US who were paid adequately for the average time it took to solve the task.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5947-5952, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens</p>
<p>Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. 2020. Speak to your parser: Interactive text-to-SQL with natural language feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2065-2077, Online. Association for Computational Linguistics.</p>
<p>Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599-5610, Online. Association for Computational Linguistics.</p>
<p>Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 698-718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968-988, Online. Association for Computational Linguistics.</p>
<p>Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946-958, Online. Association for Computational Linguistics.</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning.</p>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. ROSCOE: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>Michael A. Hedderich, Lukas Lange, Heike Adel, Jannik Strötgen, and Dietrich Klakow. 2021. A survey on recent approaches for natural language processing in low-resource scenarios. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2545-2568, Online. Association for Computational Linguistics.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.</p>
<p>Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5944-5955, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Niklas Kiehne, Hermann Kroll, and Wolf-Tilo Balke. 2022. Contextualizing language models for norms diverging from social majority. In Findings of the EMNLP 2022, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics, Association for Computational Linguistics.</p>
<p>Bugeun Kim, Kyung Seo Ki, Sangkyu Rhim, and Gahgene Gweon. 2022. EPT-X: An expression-pointer transformer model that generates eXplanations for numbers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 44424458, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS:</p>
<p>A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San Diego, California. Association for Computational Linguistics.</p>
<p>Klaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage Publications.</p>
<p>Andrew K Lampinen, Nicholas A Roy, Ishita Dasgupta, Stephanie CY Chan, Allison C Tam, James L McClelland, Chen Yan, Adam Santoro, Neil C Rabinowitz, Jane X Wang, et al. 2022. Tell me why! explanations support learning relational and causal structure. In International Conference on Machine Learning.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback.</p>
<p>Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 410-424, Seattle, United States. Association for Computational Linguistics.</p>
<p>Alice Martin, Guillaume Quispe, Charles Ollion, Sylvain Le Corff, Florian Strub, and Olivier Pietquin. 2022. Learning natural language generation with truncated reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 12-37, Seattle, United States. Association for Computational Linguistics.</p>
<p>Nikhil Mehta and Dan Goldwasser. 2019. Improving natural language interaction with robots using advice. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1962-1967, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dudík, and Patrick Shafto. 2021. Interactive learning from activity description.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Debjit Paul and Anette Frank. 2021. COINS: Dynamically generating COntextualized inference rules for narrative story completion. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 50865099, Online. Association for Computational Linguistics.</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 383-394, Online. Association for Computational Linguistics.</p>
<p>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager, and Federico Tombari. 2018. Guide me: Interacting with deep networks. CoRR, abs/1803.11544.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators.</p>
<p>Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with language feedback.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. ArXiv, abs/1707.06347.</p>
<p>Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2020. Towards Controllable Biases in Language Generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239-3254, Online. Association for Computational Linguistics.</p>
<p>Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4615-4629, Online. Association for Computational Linguistics.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. In Advances in Neural Information Processing Systems, volume 33, pages 20227-20237. Curran Associates, Inc.</p>
<p>Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339-352, Seattle, United States. Association for Computational Linguistics.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Francis Rhys Ward, Francesco Belardinelli, and Francesca Toni. 2022. Argumentative reward learning: Reasoning about human preferences. Workshop on Human-Machine Collaboration and Teaming at ICML, abs/2209.14010.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models. CoRR, abs/2112.04359.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to selfcorrect. In The Eleventh International Conference on Learning Representations.</p>
<p>Jason Weston. 2016. Dialog-based language learning.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and TieYan Liu. 2018. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612-3621, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Zhipeng Xie and Shichao Sun. 2019. A goal-driven tree-structured neural model for math word problems. In International Joint Conference on Artificial Intelligence.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-to-tree learning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3928-3937, Online. Association for Computational Linguistics.</p>
<p>Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences.</p>
<h2>A Additional Results</h2>
<h2>A. 1 More details about the quality of the feedback</h2>
<p>Please note we also include instances where the critic feedback should say the solution is correct and hence generate 'No'. Our exact match metric is not order-sensitive. We extract the sentences and match them individually to the oracle answers. Since we focused only on the semi-structured critic feedback, automatic evaluation can already capture (measure effectively) the quality of the feedback.</p>
<h2>A. 2 Details about ReACT and Self-consistency and Self-Correct</h2>
<p>The ReACT method consists of the reason model (Reason-Only) LLM (GPT-3.5), which generates a single thought at each step, and the Action model LLM (another GPT-3.5) does the calculation and generates the intermediate outputs (observations). We propose to refine the intermediate steps generated by the above steps and report the results below. Please note ReAct is approx 3-4 times more expensive than GPT-3.5 + CoT. In our experiments, we assumed 3 reasoning steps for ReACT and a sample size of 5 for self-consistency to be more cost-effective. Interestingly, we observe that ReACT perform similarly to CoT for the SVAMP dataset. One intuitive reason is that the SVAMP dataset contains questions which require one or two-hop reasoning only. We find that REFINER performs (+2.2) better than Self-correct (Welleck et al., 2023) on the GSM8K dataset, indicating the importance of correcting the intermediate steps can lead to better performance. Please note that we have used GPT-Neo as the generator model and the Unified QA T5-base model as the critic model, consistent with the Self-correct paper by Welleck et al. (2022).</p>
<h2>A. 3 More results on SVAMP dataset</h2>
<p>In the MWP, for the answer prediction task, we compare REFINER with the previously reported baselines from Jie et al. (2022) including Graph2Tree (Zhang et al., 2020) that uses quantity relations using GCN; GTS (Xie and Sun, 2019) which is a sequence-to-tree model that mainly uses a tree-based decoder with GRU; and DeductReasoner (Jie et al., 2022) which uses bottom-up DAGstructured decoding. Results of this comparison can be found in Table 9. For the sNLR task, we also experiment with a critic model trained on $50 \%$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-Neo (1.3B)</td>
<td style="text-align: center;">8.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo + Self-Correct</td>
<td style="text-align: center;">21.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo + REFINER</td>
<td style="text-align: center;">$23.4+/-0.3$</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparing REFINER with self-correct on GSM8K dataset
of its original training data and we still observe a performance improvement over the baseline as can be seen in Table 14.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Answer Prediction $(y)$</th>
<th style="text-align: center;">Acc \%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GTS</td>
<td style="text-align: center;">30.8</td>
</tr>
<tr>
<td style="text-align: left;">Graph2Tree</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: left;">BERT-Tree</td>
<td style="text-align: center;">32.4</td>
</tr>
<tr>
<td style="text-align: left;">Roberta-large-GTS</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: left;">Roberta-large-Graph2Tree</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: left;">Roberta-large-DeductReasoner</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot GPT-3</td>
<td style="text-align: center;">63.05</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot GPT-3 + COT</td>
<td style="text-align: center;">63.5</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot GPT-3 + COT + REFINER</td>
<td style="text-align: center;">$\mathbf{6 6 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Results on SVAMP dataset</p>
<h2>B REFINER Framework</h2>
<p>Alg. 1 and Alg. 2 outline the training and inference algorithms for REFINER. We train a supervised CRITIC model $\left(\pi_{\beta}\right)$ with the context $(x)$ and (plausible or implausible) hypothesis ( $z$ or $z^{\prime}$ ) as input and the textual feedback as output. Given a context $x$ the generator model $\left(\pi_{\theta}\right)$ is trained to generate plausible hypotheses.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 REFINER Training
    for E epochs do
        for \(i\) (batch) \(\leftarrow 1\) to \(N\) do
            Initialize (feedback) \(f_{0} \leftarrow N o\)
            for \(t \leftarrow 1\) to \(T\) do
                \(\hat{z}_{i, t}^{k} \sim \pi_{\theta}\left(y_{i} \mid c_{i}, f_{t-1}, \hat{z}_{i, t-1}\right)\)
                \(f_{t}, \hat{z} \leftarrow \pi_{\beta}\left(c_{i}, z_{i}, \hat{z}_{i, t}^{k}\right)\)
                \(\mathscr{L}_{t}^{f m}+=-\log p\left(z_{i} \mid c_{i}, f_{t-1}, \hat{z}_{i, t-1}\right)\)
            end for
        end for
    end for
    return \(\pi_{\theta}\)
</code></pre></div>

<h2>C Datasets and Models</h2>
<p>In Table 10 and Table 12, we report the data statistics and dataset details. In Table 11, we report the details of the used models. Our research is conducted solely on datasets that are in the English language.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples. REFINER on MWP task. There are different scenarios are highlighted in the figure, where (a) the CRITIC model provides correct feedback, GENERATOR model utilizes the feedback and fixes the incorrect equation, (b) the CRITIC model provides a correct feedback however, GENERATOR model fails to fix the incorrect equation, and (c) the CRITIC model provides an incomplete feedback GENERATOR model partially fixes the incorrect equation.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Feedback Data Generation. The top row illustrates an example from the sNLR task, where the error types are logically invalid, missing links, and missing implicit knowledge steps. The bottom row illustrates an example from moral norm generation, where the error types are contradiction and semantic misalignment. We perturbed used the plausible intermediate steps to implausible.</p>
<h2>Algorithm 2 REFINER Inference</h2>
<p>1: Initialize answers $\leftarrow$ empty list
2: for $i($ batch $) \leftarrow 1$ to $N$ do
3: Initialize (reward) $r_{i} \leftarrow 0, p_{i} \leftarrow 1$
4: Initialize (hint) $h_{0}, \hat{y}<em _theta="\theta">{i, 0} \leftarrow N o$, []
5: for (turn) $t \leftarrow 1$ to $T$ do
6: $\quad \hat{y} \leftarrow \pi</em>}\left(y_{i} \mid c_{i}, h_{t-1}, \hat{y<em t="t">{i, t-1}\right)$
7: $\quad h</em>} \leftarrow \pi_{\beta}\left(c_{i}, \hat{y<em t="t">{i}\right)$
8: $\quad$ if $h</em>=$ No then
9: $\quad$ answers.append $(\hat{y})$
10: break
11: end if
12: end for
13: answers.append $(\hat{y})$
14: end for
15: return answers</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MWP</td>
<td style="text-align: center;">3,138</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">sNLR</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: left;">MS</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">GSM8k</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1319</td>
</tr>
</tbody>
</table>
<p>Table 10: Dataset Statistics: nb. of instances.</p>
<h2>D Training Details</h2>
<p>Training Details. For each task, we train a UnifiedQa-T5-base model (UQA-base) (Khashabi et al., 2020) as a critic (§3.1). Further evaluation details are provided in Appendix G. For exploration (§3.2), we use nucleus sampling with $p=0.5$. We select the hyper-parameters by the validation loss: for both the generator and critic model, we use the Adam optimizer with a learning rate of $1 e^{-4}$. Each model is trained for 20 epochs with early stopping based on validation loss. We trained all models on</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Parameter Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UQA-base</td>
<td style="text-align: center;">220 M</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {base }}$</td>
<td style="text-align: center;">440 M</td>
</tr>
<tr>
<td style="text-align: left;">UQA-large</td>
<td style="text-align: center;">770 M</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {large }}$</td>
<td style="text-align: center;">990 M</td>
</tr>
<tr>
<td style="text-align: left;">GPT3.5</td>
<td style="text-align: center;">175 B</td>
</tr>
</tbody>
</table>
<p>Table 11: Model Sizes.
one A100 GPU. We run our models with 3 random seeds and report the average results. We perform a binomial sign test. We find that p -values are always $&lt;0.05$ when we compare REFINER with all the baselines (GPT-3.5, Self-refine, Self-reflection), suggesting our results are not random and significant. For the human study, we selected outputs from the best models (baselines and our model) according to automatic metrics. We train models with $T=3$ iterations. We trained the critic model for 8 hours and trained the generator model for 12 hours.</p>
<p>At inference time, we use greedy decoding for the generator and critic model with $T=1$ for the automatic critic and $T=3$ for the oracle critic. We evaluate our methods using the metrics presented in the original papers that proposed the tasks. On the MWP and sNLR tasks, we use the exact match (EM) metric for intermediate steps (equation generation and inference rules) and accuracy (Acc) for the final answers. For MS, we conduct a manual evaluation study to assess the relevance of norms and moral actions. ${ }^{7}$</p>
<h2>E Qualitative Examples</h2>
<p>Figure 7 and 20 depict a qualitative example of REFINER where REFINER could correct incorrect equations through structured feedback, fixing the operators within a multistep solution. Table 20 shows some qualitatively improved examples for MS.</p>
<h2>F Feedback Data Generation</h2>
<h2>F. 1 Rule-based Perturbation</h2>
<p>Based on these error types, we perturb the plausible hypotheses $(z)$ in the training data and collect a pool of data $D$ ( $x$ : input, $z$ : plausible hypothesis, $z^{\prime}$ : implausible hypothesis). We perturb by omitting, replacing or adding some tokens or some</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>rules from the plausible hypothesis to automatically create an implausible hypothesis. For example, in Fig. 6, for sNLR we omit a few inference steps from the correct hypothesis "#0: viridian is green, #1: rose is green" and create an incorrect (incomplete) hypothesis (see Fig. 6). Since our perturbations are based on logic and reasoning errors, we create structured feedback $f$ for every example $\left(x, z, z^{\prime}\right)$ by stating the error type that occurs in $z^{\prime}$ but not in $z$ (see Table 1). The basic structure of feedback $f$ for these tasks is $\langle$ error type, position (optional), hint (optional) $\rangle$, where position denotes the error position in the implausible hypothesis (see Appx Table 1). For example, in the previous scenario, we create feedback "Missing link between fact and rules". Despite the simplicity of the strategy we used for our tasks, this approach is easily generalisable to other reasoning tasks.</p>
<p>For MWP and sNLR problems, the underlying reasoning requires symbolic systems with closedworld rules. Hence, we consider a simple rulebased method to automatically generate the pairs of errors and their corresponding structured feedback by considering the error types and position of the errors (see Fig. 6 and Table 1).</p>
<p>In the moral norm generation task, we consider two kinds of fine-grained errors: logical contradiction and semantic misalignment (incoherent, uninformative). Moral norms are people's subjective judgments about the character and actions mentioned in the context. Each moral norm is a combination of two components (implicit structure): a moral judgment [You shouldn't] and an action [criticize your family's religion]. Firstly, to create logical contradictions, we use the concept of deontic logic from Kiehne et al. (2022) and derive new norms contrary to those of Moral Stories. Hence, we replace the correct moral judgments in the plausible hypothesis with inverse judgments. For example, replacing [You shouldn't] from the plausible hypothesis to [It's good], as depicted in Fig. 6. To scale such inverse norms (implausible hypothesis), we paraphrase them by substituting the adjectives with synonyms from WordNet. Secondly, to create semantic misalignments, we must collect implausible hypotheses that are either misaligned with the plausible hypothesis or incomplete in nature. To create them, we replace the correct action (verb phrase) from the plausible hypothesis with random verb phrases selected from the context of the plausible hypothesis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset/Tools</th>
<th style="text-align: left;">Citation</th>
<th style="text-align: left;">Link</th>
<th style="text-align: left;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: left;">Patel et al. (2021)</td>
<td style="text-align: left;">https://github.com/arkilpatel/SVAMP</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">GSM8k</td>
<td style="text-align: left;">Cobbe et al. (2021b)</td>
<td style="text-align: left;">https://github.com/openai/grade-school-math</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">sNLR</td>
<td style="text-align: left;">Liang et al. (2022)</td>
<td style="text-align: left;">https://github.com/stanford-crfm/helm</td>
<td style="text-align: left;">Apache License</td>
</tr>
<tr>
<td style="text-align: left;">Moral Norm</td>
<td style="text-align: left;">Emelin et al. (2021)</td>
<td style="text-align: left;">https://github.com/demelin/moral_stories</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">HuggingFace</td>
<td style="text-align: left;">Wolf et al. (2020)</td>
<td style="text-align: left;">https://github.com/huggingface/transformers</td>
<td style="text-align: left;">Apache License</td>
</tr>
</tbody>
</table>
<p>Table 12: More details about datasets and Tools
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: REFINER on MWP. The generator's output improves step-wise.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Eq. (2)</th>
<th style="text-align: left;">Ans. ( $y$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UQA-large</td>
<td style="text-align: left;">46.7</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">UQA-large + PPO</td>
<td style="text-align: left;">48.2</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {large }}$</td>
<td style="text-align: left;">$\mathbf{5 3 . 8}$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {large }}+$ Oracle (T=3)</td>
<td style="text-align: left;">68.1</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 + CoT</td>
<td style="text-align: left;">59.3</td>
<td style="text-align: left;">63.5</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 + CoT + REFINER $_{\text {critic }}$</td>
<td style="text-align: left;">62.3</td>
<td style="text-align: left;">$\mathbf{6 6 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5* + CoT</td>
<td style="text-align: left;">64.1</td>
<td style="text-align: left;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5* + CoT + REFINER $_{\text {critic }}$</td>
<td style="text-align: left;">$\mathbf{6 7 . 3}$</td>
<td style="text-align: left;">$\mathbf{7 0 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Results on MWP. Eq.: Equation, Ans. Answer. Comparison of REFINER with baselines on the SVAMP dataset. GPT-3.5: code-DaVinci-002, GPT-3.5*: text-DaVinci-002 For models other than GPT3.5, the answer can be obtained via symbolic execution of the equation and is thus a function of the validity of the equation. For GPT3.5, the model is few-shot prompted to either generate the equation with variable names $z$, or generate the answer $y$.</p>
<h2>F. 2 Synthetic Feedback Generation</h2>
<p>We used a few-shot setting where we varied the instruction, the number of demonstrations, and the</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">IR</th>
<th style="text-align: center;">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">50\% training data</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">T5-base</td>
<td style="text-align: center;">$84.28 \pm 0.5$</td>
<td style="text-align: center;">88.86</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {base }}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 2 6} \pm \mathbf{0 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 2 6}$</td>
</tr>
<tr>
<td style="text-align: left;">REFINER $_{\text {base }}+$ Oracle $91.11 \pm 05$</td>
<td style="text-align: center;">97.28</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 14: Results on SNR dataset. IR: Inference Rules, C: Consequent
formatting of the demonstrations. Since data generation with GPT-3.5 is expensive, we generated 30K, 20 K , and 30 K implausible hypotheses for MWP, sNLR and MS tasks, respectively.</p>
<h2>G Human Evaluation on Moral Stories</h2>
<p>As part of the human evaluation of model generations on MS, we asked Amazon MTurk (AMT) annotators to judge the relevancy of the generated norm and the moral action based on a Likert scale, with $1=$ strongly disagree, $2=$ disagree, $3=$ unsure, $4=$ agree, and $5=$ strongly agree. Ratings were</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Error Types</th>
<th style="text-align: left;">Structured Feedback</th>
<th style="text-align: left;">Human Readable Feedback</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MWP Incorrect Numbers</td>
<td style="text-align: left;">(errortype, position, equation - number)</td>
<td style="text-align: left;">The position number in equation-number is incorrect.</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Operators</td>
<td style="text-align: left;">(errortype,equation - number)</td>
<td style="text-align: left;">The operator in equation-number is incorrect.</td>
</tr>
<tr>
<td style="text-align: left;">Missing Operators</td>
<td style="text-align: left;">(errortype)</td>
<td style="text-align: left;">An operator is missing.</td>
</tr>
<tr>
<td style="text-align: left;">sNLR Logically Incorrect</td>
<td style="text-align: left;">(X operator, inference rule number)</td>
<td style="text-align: left;">The $X$ operator makes inference rule number invalid.</td>
</tr>
<tr>
<td style="text-align: left;">Missing Lookup Step</td>
<td style="text-align: left;">(errortype)</td>
<td style="text-align: left;">Missing link between the fact and the rules.</td>
</tr>
<tr>
<td style="text-align: left;">Missing Implicit Knowledge Step</td>
<td style="text-align: left;">(errortype)</td>
<td style="text-align: left;">The implicit knowledge is missing.</td>
</tr>
</tbody>
</table>
<p>Table 15: Feedback Templates</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Initial</span><span class="w"> </span><span class="nv">PROMPT</span>:<span class="w"> </span><span class="nv">Math</span><span class="w"> </span><span class="nv">Word</span><span class="w"> </span><span class="nv">Problem</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">helpful</span><span class="w"> </span><span class="nv">assistant</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">math</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="nv">problems</span>.
<span class="nv">We</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">math</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="nv">problem</span>,
<span class="nv">and</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">generate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">intermediate</span><span class="w"> </span><span class="nv">mathematical</span>
<span class="nv">equations</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">step</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">solving</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">problem</span>
<span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">answer</span>.<span class="w"> </span><span class="nv">Here</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">examples</span>:
<span class="s2">&quot;Question : &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
<span class="err">&lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="s2">&quot;Question: &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
<span class="err">&lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="s2">&quot;Question: &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
</code></pre></div>

<p>Table 16: Prompts used for generating correct answer given a math word problem</p>
<div class="codehilite"><pre><span></span><code><span class="nv">REFINEMENT</span><span class="w"> </span><span class="nv">PROMPT</span>:<span class="w"> </span><span class="nv">Math</span><span class="w"> </span><span class="nv">Word</span><span class="w"> </span><span class="nv">Problem</span>
<span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">helpful</span><span class="w"> </span><span class="nv">assistant</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">math</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="nv">problems</span>.
<span class="nv">We</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">math</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="nv">problem</span>
<span class="nv">a</span><span class="w"> </span><span class="nv">solution</span><span class="w"> </span><span class="ss">(</span><span class="nv">containing</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">equation</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">answer</span><span class="ss">)</span>,
<span class="nv">and</span><span class="w"> </span><span class="nv">feedback</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">solution</span>.
<span class="nv">Your</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">generate</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">refined</span><span class="w"> </span><span class="nv">intermediate</span><span class="w"> </span><span class="nv">equation</span>
<span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">step</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">final</span><span class="w"> </span><span class="nv">correct</span><span class="w"> </span><span class="nv">answer</span>.
<span class="nv">Here</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">examples</span>:
<span class="s2">&quot;Question : &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
<span class="err">&lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="err">Feedback: &lt;feedback&gt; &lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="s2">&quot;Question: &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
<span class="err">&lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="err">Feedback: &lt;feedback&gt; &lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="s2">&quot;Question: &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="nv">Problem</span><span class="w"> </span><span class="nv">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="nv">Let</span><span class="err">&#39;s think step by step</span>
<span class="err">&lt;equation&gt; Answer: &lt;answer&gt;</span>
<span class="err">Feedback: &lt;feedback&gt;</span>
</code></pre></div>

<p>Table 17: Prompts used for generating correct answer given a math word problem</p>
<div class="codehilite"><pre><span></span><code><span class="n">PROMPT</span><span class="o">:</span><span class="w"> </span><span class="n">Synthetic</span><span class="w"> </span><span class="n">Incorrect</span><span class="w"> </span><span class="n">Instance</span><span class="w"> </span><span class="n">Generation</span>
<span class="n">You</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">helpful</span><span class="w"> </span><span class="n">assistant</span><span class="w"> </span><span class="k">for</span>
<span class="n">generating</span><span class="w"> </span><span class="n">counterfactual</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">steps</span><span class="o">.</span>
<span class="n">We</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">provide</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">problem</span><span class="o">,</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">type</span>
<span class="n">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">step</span><span class="o">.</span>
<span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">generate</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">incorrect</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">step</span>
<span class="n">based</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">type</span><span class="o">.</span>
<span class="n">Here</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">few</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">type</span><span class="o">:</span>
<span class="s2">&quot;Question : &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Problem</span><span class="w"> </span><span class="n">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Let</span><span class="s1">&#39;s think step by step</span>
<span class="s1">&lt;correct intermediate steps&gt; Error type: &lt;error type&gt;</span>
<span class="s1">Counterfactual: &lt;incorrect intermediate steps&gt;</span>
<span class="s1">&quot;Question: &quot; &lt;Problem Statements&gt; Let&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span>
<span class="o">&lt;</span><span class="n">correct</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">steps</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">error</span><span class="w"> </span><span class="n">type</span><span class="o">&gt;</span>
<span class="n">Counterfactual</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">incorrect</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">steps</span><span class="o">&gt;</span>
<span class="s2">&quot;Question: &quot;</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Problem</span><span class="w"> </span><span class="n">Statements</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Let</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">think</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">step</span>
<span class="o">&lt;</span><span class="n">correct</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">steps</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">error</span><span class="w"> </span><span class="n">type</span><span class="o">&gt;</span>
<span class="n">Counterfactual</span><span class="o">:</span>
</code></pre></div>

<p>Table 18: Prompts used for generating synthetic incorrect instances
subsequently aggregated, with scores $\geq 4$ deemed to be Relevant and with scores, $\leq 2$ deemed to be Irrelevant while ratings with score 3 (Unsure) left as is. More specifically, we asked three different human judges to evaluate each example. We performed majority voting over answers with the rating Unsure assigned to those examples with no clear majority winner. In Figures 8 and 9, we report a complete breakdown of evaluation results for both norm and moral action. We also report agreement scores computed according to Krippendorff's $\alpha$ (Krippendorff, 2018) in Table 4. The low and moderate $\alpha$ values indicate that judging the plausibility of moral norms and actions is a challenging task. In Figures 10-18, we provide excerpts of HIT instructions given to AMT workers during moral norm and action evaluation. Each task was supplemented by an Acceptance and Privacy Policy (Figure 18) that explains participation and data collection terms. All workers were based in US and paid $\$ 0.10$ per task which took around 5 minutes to complete on average.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Human Evaluation of Moral Norm on 100 test samples.</p>
<p>Moral Action Evaluation
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Human Evaluation of Moral Action on 100 test samples.</p>
<p>Acceptance and Privacy Policies (click to expand/collapse)</p>
<p>Instructions (click to expand/collapse)</p>
<p>Dos and Don'ts (click to expand/collpase)</p>
<p>Examples (click to expand/collapse)</p>
<p>Situation: $\$($ situation $)$
Intention: $\$($ intention $)$
Immoral Action: $\$($ immoralAction $)$
Moral Norm: $\$($ moralNorm $)$
Is this moral norm relevant to the story?
Strongly agree Agree Unsure Disagree Strongly disagree
(Optional) Please let us know if anything was unclear, if you experienced any issues, or if you have any other fedback for us.
$\square$</p>
<p>Figure 10: Excerpt from AMT HIT instructions: Norm Evaluation Task</p>
<p>Acceptance and Privacy Policies (click to expand/collapse)</p>
<p>Instructions (click to expand/collapse)</p>
<p>Dos and Don'ts (click to expand/collpase)</p>
<p>Examples (click to expand/collapse)</p>
<p>Situation: $\$($ situation $)$
Intention: $\$($ intention $)$
Immoral Action: $\$($ immoralAction $)$
Moral Action: $\$($ moralAction $)$
Is this moral action relevant to the story?
Strongly agree Agree Unsure Disagree Strongly disagree
(Optional) Please let us know if anything was unclear, if you experienced any issues, or if you have any other fedback for us.
$\square$</p>
<p>Figure 11: Excerpt from AMT HIT instructions: Moral Action Evaluation Task</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Since the automatic scores such as BLUE, ROUGE, etc. only account for word level similarity between gold norms or actions and generate norms or actions.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://github.com/debjitpaul/refiner&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
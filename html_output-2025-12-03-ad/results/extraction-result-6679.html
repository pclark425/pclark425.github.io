<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6679 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6679</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6679</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-a816817423d4ab4e02c307b5a8f54e374d80caac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a816817423d4ab4e02c307b5a8f54e374d80caac" target="_blank">Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a novel QA reasoning framework, specifically designed to assess the impact of input length, and shows a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum.</p>
                <p><strong>Paper Abstract:</strong> This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6679.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6679.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-1106-preview as used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large autoregressive language model evaluated on FLenQA; high accuracy on short-context reasoning but shows degradation as input length increases, with Chain-of-Thought providing limited mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; standard pretraining on large web/code/text corpora implied but not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA (this paper's reasoning benchmark; not an arithmetic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>two-paragraph multi-span text-based reasoning (True/False questions)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language True/False question grounded in two key paragraphs embedded in longer text</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>simple multi-span reasoning (two facts required jointly); intentionally kept solvable at short lengths</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct (normal) prompting and Chain-of-Thought (CoT) prompting (elicitation string: 'Let's work this out in a step by step way to be sure we have the right answer.')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minimal-length accuracy by task (Direct): MonoRel 100%, PIR 100%, Ruletaker 98%; CoT similar. Displays an accuracy decline as input length grows (paper reports aggregate average drop across models from ≈0.92 to ≈0.68), but GPT-4 is less affected than many models (exact per-length numeric series not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper-level behavioral analysis: GPT-4 retains high baseline accuracy on minimal inputs and benefits from CoT prompting; CoT's relative benefit for GPT-4 increases with input length (unique among evaluated models). Next-word prediction accuracy (perplexity proxy) increases with longer input while FLenQA reasoning accuracy decreases, showing negative correlation; no mechanistic probing (activations/attention) performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Susceptible to the length-induced failure modes described in the paper: tendency (in general across models) to omit answers on long inputs, label bias toward 'False' with longer context, generation of final answer before CoT steps ('answer-first'), and reduced coverage of relevant facts in CoT steps as input length increases. GPT-4 exhibits these modes but to a lesser extent and shows a larger CoT improvement at longer lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Within this paper: GPT-4 (largest evaluated) shows higher baseline accuracy and is less impacted by long input than smaller models; CoT improves performance and its marginal benefit appears to grow with input length for GPT-4 (exception among models). No systematic scaling law or parameter-count based trend is derived.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6679.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106 as used in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong autoregressive LLM evaluated on FLenQA that benefits from Chain-of-Thought prompting at short contexts but shows notable degradation in reasoning accuracy as input length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-1106)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA (reasoning dataset used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>two-paragraph multi-span text-based reasoning (True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language True/False questions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>simple multi-span reasoning (two evidence spans required)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct and Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minimal-length accuracy (Direct): MonoRel 77%, PIR 81%, Ruletaker 74%; with CoT: MonoRel 86%, PIR 88%, Ruletaker 88%. Shows clear accuracy degradation as input length increases (e.g., substantial drops by ~3000 tokens in aggregate plots).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral findings: CoT improves short-input performance substantially; next-word prediction accuracy increases with longer inputs while reasoning accuracy falls (negative correlation). No internal activation/attention probing performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same length-induced failure modes: refusals or 'no answer' increases with length, label bias toward 'False' as length grows, 'answer-first' in CoT outputs and reduced coverage of relevant facts in CoT steps for longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Compared to larger model (GPT-4), GPT-3.5 has lower baseline accuracy and is more sensitive to context length; CoT helps but does not prevent long-input degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6679.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A capable LLM evaluated on FLenQA; benefits from CoT at short lengths but shows degradation with longer inputs and in one case CoT decreased performance at long lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>two-paragraph multi-span text-based reasoning (True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language True/False questions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>simple multi-span reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct and Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minimal-length accuracy (Direct): MonoRel 84%, PIR 100%, Ruletaker 92%; CoT: MonoRel 88%, PIR 96%, Ruletaker 97%. Degrades with increasing input length; unusual behavior: CoT decreased performance as input length increased for this model in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral observations only: CoT did not uniformly mitigate long-context degradation and sometimes harmed performance at long lengths (Gemini-Pro). Next-word prediction (perplexity proxy) trends do not predict downstream reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Increased nonanswers/refusals on long inputs, label bias toward false, answer-before-steps in CoT, and lower coverage of relevant facts in CoT outputs as length grows.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No parameter-size scaling analysis in paper; Gemini-Pro sits between GPT-3.5 and GPT-4 in baseline performance for these tasks, but shows idiosyncratic sensitivity to CoT at long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6679.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral Medium</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Medium</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Mistral-family model variant evaluated on FLenQA; high minimal-length accuracy on some tasks but shows degradation with increased input length, with CoT improving performance in many but not fully mitigating length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral Medium</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (Mistral family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (referred to as 'Medium')</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>two-paragraph multi-span text-based reasoning (True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language True/False questions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>simple multi-span reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct and Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minimal-length accuracy (Direct): MonoRel 99%, PIR 100%, Ruletaker 73%; CoT: MonoRel 100%, PIR 100%, Ruletaker 89%. Shows degradation with longer inputs; CoT helps but does not eliminate drop.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral: CoT increases accuracy for short inputs; coverage of relevant facts in CoT steps declines as input length increases. Next-word accuracy trends diverge from reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Long-input failure modes: nonanswers, label bias, answer-first in CoT, incomplete coverage of facts in CoT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>No across-size scaling analysis; stands as a mid-sized model in this paper and shows good short-input performance but sensitivity to long context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6679.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8x7B expert-mixture variant in the Mistral family evaluated on FLenQA; good baseline accuracy but exhibits input-length dependent degradation similar to other models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral 8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Mixture-of-experts style decoder transformer (Mistral family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B (as named in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>two-paragraph multi-span text-based reasoning (True/False)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language True/False questions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>simple multi-span reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Direct and Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Minimal-length accuracy (Direct): MonoRel 92%, PIR 97%, Ruletaker 80%; CoT: MonoRel 86%, PIR 97%, Ruletaker 93%. Performance degrades as input length grows; CoT helps but drop remains.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral observations: CoT improves minimal-length performance on several tasks; CoT coverage of relevant facts declines with length. No mechanistic probing provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same set of length-induced failures: nonanswers increase, false-label bias, answer-before-CoT-steps, reduced CoT coverage with longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper does not present systematic scaling laws; this Mistral variant performs comparably to other mid/large models at short lengths but is sensitive to long context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6679.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps before the final answer; used in this paper and shown to improve short-input reasoning accuracy but generally does not eliminate long-input degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA (applied here)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>general multi-step reasoning elicitation (applied to text-based True/False reasoning here)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>prompted generation of intermediate steps followed by an answer (zero-shot CoT elicitation string used: 'Let's work this out in a step by step way to be sure we have the right answer.')</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>used for simple two-paragraph reasoning tasks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought (zero-shot CoT elicitation as per Zhou et al. 2022 used in prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (comparison between Direct vs CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CoT increased minimal-length accuracy across almost all evaluated models (examples: GPT-3.5 MonoRel 77%→86%; PIR 81%→88%; Ruletaker 74%→88%). However, CoT generally did not mitigate the accuracy drop caused by longer inputs (exception: GPT-4, where CoT's benefit grew with length).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analysis: with longer inputs, models less often include the relevant facts in CoT steps (reduced coverage), and are more likely to produce the final answer before the requested CoT steps ('answer-first'), indicating failures to follow the CoT elicitation; no internal mechanistic probes were conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Answer-first behavior (final answer produced before steps) increases with input length; CoT coverage of relevant facts declines with input length; CoT sometimes decreases performance at long lengths for certain models (Gemini-Pro).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>CoT benefits are model-dependent: generally helpful at short contexts; only GPT-4 showed increasing CoT effectiveness as input length grew in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6679.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Next-word prediction / Perplexity proxy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Next-word prediction accuracy (used as a proxy for perplexity in closed models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A next-token prediction task run on the same dataset inputs (without questions) used to probe whether standard next-token metrics correlate with downstream reasoning accuracy on long inputs; found to correlate negatively with FLenQA reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom next-word prediction over FLenQA inputs</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>prompted completion of the next word; exact-match counted as correct</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>not applicable (diagnostic probe)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>direct prompt to predict next word in text (samples from dataset without questions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>next-word accuracy and correlation with reasoning accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Next-word accuracy increases with input length (as in prior works), but next-word accuracy correlates negatively with FLenQA reasoning accuracy (Pearson correlation reported: r = -0.95, p = 0.01 for the aggregated measure presented).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral conclusion: perplexity/next-word metrics are not predictive of reasoning performance on long inputs; the paper presents a strong negative correlation between next-token accuracy and downstream reasoning accuracy on FLenQA. No mechanistic probes (e.g., attention or neuron analysis) were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Indicates that relying on next-token or perplexity improvements for long-context capability is misleading; improvements in next-token metrics may coincide with worse downstream reasoning in long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed across model sizes here; general prior observations (cited) show next-word accuracy can improve with context length while downstream task performance may not.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6679.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM-8K (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM-8K (grade school math benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an example task in prior work (Shi et al., 2023) where appending irrelevant text reduced performance; GSM-8K is a standard dataset of grade-school math word problems used to evaluate numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>math word problems / grade-school arithmetic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to middle-school math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>not applied in this paper (cited work used GSM-8K as target task)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>This paper cites prior finding (Shi et al., 2023) that appending irrelevant context to GSM-8K prompts reduces model performance; no new GSM-8K experiments are run in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Cited behavior: distraction by irrelevant context reduces arithmetic/word-problem accuracy in prior work; paper extrapolates similar long-input failure modes may affect reasoning tasks broadly.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6679.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6679.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Length-induced failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Length-induced failure modes observed in long-context reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of recurrent behavioral failures the authors identify as input length grows: increasing non-answers/refusals, label bias (toward 'False'), answer-before-CoT-steps, and declining CoT coverage of relevant facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FLenQA (observed on this dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>affects general reasoning tasks (including potentially arithmetic word problems if similarly formatted)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>observed across both Direct and CoT prompting on True/False text-reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>manifest even on simple two-evidence-paragraph tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>observed under Direct and CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>frequency of failure modes; accuracy impact</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Non-answer frequency and 'False' bias increase monotonically with input length (figures and per-model charts provided); statistical associations reported (e.g., odds-ratios for 'answer before steps' and incomplete coverage relating to incorrect responses, both p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral-level statistical analysis: (a) incorrect responses are statistically dependent on occurrence of answers before reasoning steps (odds-ratio 3.643, p<0.001); (b) incomplete coverage of required facts in CoT steps correlates with incorrect responses (odds-ratio 3.138, p<0.001). No internal activation or attention analyses were conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Explicitly: (1) Failure to answer (refusals) increases with length; (2) Label bias toward 'False' with longer input; (3) 'Answer-first' behavior in CoT outputs becomes more common with length; (4) CoT coverage of relevant facts declines as input length increases.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Observed across multiple models of varying capability; larger models (e.g., GPT-4) show these effects less severely but are not immune.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Large language models can be easily distracted by irrelevant context <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6679",
    "paper_id": "paper-a816817423d4ab4e02c307b5a8f54e374d80caac",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (gpt-4-1106-preview as used in paper)",
            "brief_description": "A state-of-the-art large autoregressive language model evaluated on FLenQA; high accuracy on short-context reasoning but shows degradation as input length increases, with Chain-of-Thought providing limited mitigation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-1106-preview)",
            "model_family": "decoder-only transformer (autoregressive)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper; standard pretraining on large web/code/text corpora implied but not detailed.",
            "benchmark_name": "FLenQA (this paper's reasoning benchmark; not an arithmetic benchmark)",
            "task_type": "two-paragraph multi-span text-based reasoning (True/False questions)",
            "problem_format": "natural-language True/False question grounded in two key paragraphs embedded in longer text",
            "difficulty_level": "simple multi-span reasoning (two facts required jointly); intentionally kept solvable at short lengths",
            "prompting_method": "Direct (normal) prompting and Chain-of-Thought (CoT) prompting (elicitation string: 'Let's work this out in a step by step way to be sure we have the right answer.')",
            "performance_metric": "accuracy",
            "performance_value": "Minimal-length accuracy by task (Direct): MonoRel 100%, PIR 100%, Ruletaker 98%; CoT similar. Displays an accuracy decline as input length grows (paper reports aggregate average drop across models from ≈0.92 to ≈0.68), but GPT-4 is less affected than many models (exact per-length numeric series not tabulated in text).",
            "internal_analysis": "Paper-level behavioral analysis: GPT-4 retains high baseline accuracy on minimal inputs and benefits from CoT prompting; CoT's relative benefit for GPT-4 increases with input length (unique among evaluated models). Next-word prediction accuracy (perplexity proxy) increases with longer input while FLenQA reasoning accuracy decreases, showing negative correlation; no mechanistic probing (activations/attention) performed.",
            "failure_modes": "Susceptible to the length-induced failure modes described in the paper: tendency (in general across models) to omit answers on long inputs, label bias toward 'False' with longer context, generation of final answer before CoT steps ('answer-first'), and reduced coverage of relevant facts in CoT steps as input length increases. GPT-4 exhibits these modes but to a lesser extent and shows a larger CoT improvement at longer lengths.",
            "scaling_trend": "Within this paper: GPT-4 (largest evaluated) shows higher baseline accuracy and is less impacted by long input than smaller models; CoT improves performance and its marginal benefit appears to grow with input length for GPT-4 (exception among models). No systematic scaling law or parameter-count based trend is derived.",
            "uuid": "e6679.0",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-1106 as used in paper)",
            "brief_description": "A strong autoregressive LLM evaluated on FLenQA that benefits from Chain-of-Thought prompting at short contexts but shows notable degradation in reasoning accuracy as input length increases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-1106)",
            "model_family": "decoder-only transformer (autoregressive)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "FLenQA (reasoning dataset used in this paper)",
            "task_type": "two-paragraph multi-span text-based reasoning (True/False)",
            "problem_format": "natural-language True/False questions",
            "difficulty_level": "simple multi-span reasoning (two evidence spans required)",
            "prompting_method": "Direct and Chain-of-Thought (CoT)",
            "performance_metric": "accuracy",
            "performance_value": "Minimal-length accuracy (Direct): MonoRel 77%, PIR 81%, Ruletaker 74%; with CoT: MonoRel 86%, PIR 88%, Ruletaker 88%. Shows clear accuracy degradation as input length increases (e.g., substantial drops by ~3000 tokens in aggregate plots).",
            "internal_analysis": "Behavioral findings: CoT improves short-input performance substantially; next-word prediction accuracy increases with longer inputs while reasoning accuracy falls (negative correlation). No internal activation/attention probing performed.",
            "failure_modes": "Same length-induced failure modes: refusals or 'no answer' increases with length, label bias toward 'False' as length grows, 'answer-first' in CoT outputs and reduced coverage of relevant facts in CoT steps for longer inputs.",
            "scaling_trend": "Compared to larger model (GPT-4), GPT-3.5 has lower baseline accuracy and is more sensitive to context length; CoT helps but does not prevent long-input degradation.",
            "uuid": "e6679.1",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Gemini Pro",
            "brief_description": "A capable LLM evaluated on FLenQA; benefits from CoT at short lengths but shows degradation with longer inputs and in one case CoT decreased performance at long lengths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_family": "decoder-only transformer (unspecified)",
            "model_size": "not specified in paper",
            "training_data_description": "Not specified in this paper.",
            "benchmark_name": "FLenQA",
            "task_type": "two-paragraph multi-span text-based reasoning (True/False)",
            "problem_format": "natural-language True/False questions",
            "difficulty_level": "simple multi-span reasoning",
            "prompting_method": "Direct and Chain-of-Thought (CoT)",
            "performance_metric": "accuracy",
            "performance_value": "Minimal-length accuracy (Direct): MonoRel 84%, PIR 100%, Ruletaker 92%; CoT: MonoRel 88%, PIR 96%, Ruletaker 97%. Degrades with increasing input length; unusual behavior: CoT decreased performance as input length increased for this model in some settings.",
            "internal_analysis": "Behavioral observations only: CoT did not uniformly mitigate long-context degradation and sometimes harmed performance at long lengths (Gemini-Pro). Next-word prediction (perplexity proxy) trends do not predict downstream reasoning performance.",
            "failure_modes": "Increased nonanswers/refusals on long inputs, label bias toward false, answer-before-steps in CoT, and lower coverage of relevant facts in CoT outputs as length grows.",
            "scaling_trend": "No parameter-size scaling analysis in paper; Gemini-Pro sits between GPT-3.5 and GPT-4 in baseline performance for these tasks, but shows idiosyncratic sensitivity to CoT at long contexts.",
            "uuid": "e6679.2",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral Medium",
            "name_full": "Mistral Medium",
            "brief_description": "A Mistral-family model variant evaluated on FLenQA; high minimal-length accuracy on some tasks but shows degradation with increased input length, with CoT improving performance in many but not fully mitigating length effects.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral Medium",
            "model_family": "decoder-only transformer (Mistral family)",
            "model_size": "not specified in paper (referred to as 'Medium')",
            "training_data_description": "Not specified in paper.",
            "benchmark_name": "FLenQA",
            "task_type": "two-paragraph multi-span text-based reasoning (True/False)",
            "problem_format": "natural-language True/False questions",
            "difficulty_level": "simple multi-span reasoning",
            "prompting_method": "Direct and Chain-of-Thought (CoT)",
            "performance_metric": "accuracy",
            "performance_value": "Minimal-length accuracy (Direct): MonoRel 99%, PIR 100%, Ruletaker 73%; CoT: MonoRel 100%, PIR 100%, Ruletaker 89%. Shows degradation with longer inputs; CoT helps but does not eliminate drop.",
            "internal_analysis": "Behavioral: CoT increases accuracy for short inputs; coverage of relevant facts in CoT steps declines as input length increases. Next-word accuracy trends diverge from reasoning performance.",
            "failure_modes": "Long-input failure modes: nonanswers, label bias, answer-first in CoT, incomplete coverage of facts in CoT outputs.",
            "scaling_trend": "No across-size scaling analysis; stands as a mid-sized model in this paper and shows good short-input performance but sensitivity to long context.",
            "uuid": "e6679.3",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-8x7B",
            "name_full": "Mistral 8x7B",
            "brief_description": "An 8x7B expert-mixture variant in the Mistral family evaluated on FLenQA; good baseline accuracy but exhibits input-length dependent degradation similar to other models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral 8x7B",
            "model_family": "Mixture-of-experts style decoder transformer (Mistral family)",
            "model_size": "8x7B (as named in paper)",
            "training_data_description": "Not specified in paper.",
            "benchmark_name": "FLenQA",
            "task_type": "two-paragraph multi-span text-based reasoning (True/False)",
            "problem_format": "natural-language True/False questions",
            "difficulty_level": "simple multi-span reasoning",
            "prompting_method": "Direct and Chain-of-Thought (CoT)",
            "performance_metric": "accuracy",
            "performance_value": "Minimal-length accuracy (Direct): MonoRel 92%, PIR 97%, Ruletaker 80%; CoT: MonoRel 86%, PIR 97%, Ruletaker 93%. Performance degrades as input length grows; CoT helps but drop remains.",
            "internal_analysis": "Behavioral observations: CoT improves minimal-length performance on several tasks; CoT coverage of relevant facts declines with length. No mechanistic probing provided.",
            "failure_modes": "Same set of length-induced failures: nonanswers increase, false-label bias, answer-before-CoT-steps, reduced CoT coverage with longer inputs.",
            "scaling_trend": "Paper does not present systematic scaling laws; this Mistral variant performs comparably to other mid/large models at short lengths but is sensitive to long context.",
            "uuid": "e6679.4",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps before the final answer; used in this paper and shown to improve short-input reasoning accuracy but generally does not eliminate long-input degradation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "FLenQA (applied here)",
            "task_type": "general multi-step reasoning elicitation (applied to text-based True/False reasoning here)",
            "problem_format": "prompted generation of intermediate steps followed by an answer (zero-shot CoT elicitation string used: 'Let's work this out in a step by step way to be sure we have the right answer.')",
            "difficulty_level": "used for simple two-paragraph reasoning tasks in this paper",
            "prompting_method": "Chain-of-Thought (zero-shot CoT elicitation as per Zhou et al. 2022 used in prompts)",
            "performance_metric": "accuracy (comparison between Direct vs CoT)",
            "performance_value": "CoT increased minimal-length accuracy across almost all evaluated models (examples: GPT-3.5 MonoRel 77%→86%; PIR 81%→88%; Ruletaker 74%→88%). However, CoT generally did not mitigate the accuracy drop caused by longer inputs (exception: GPT-4, where CoT's benefit grew with length).",
            "internal_analysis": "Behavioral analysis: with longer inputs, models less often include the relevant facts in CoT steps (reduced coverage), and are more likely to produce the final answer before the requested CoT steps ('answer-first'), indicating failures to follow the CoT elicitation; no internal mechanistic probes were conducted.",
            "failure_modes": "Answer-first behavior (final answer produced before steps) increases with input length; CoT coverage of relevant facts declines with input length; CoT sometimes decreases performance at long lengths for certain models (Gemini-Pro).",
            "scaling_trend": "CoT benefits are model-dependent: generally helpful at short contexts; only GPT-4 showed increasing CoT effectiveness as input length grew in this study.",
            "uuid": "e6679.5",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Next-word prediction / Perplexity proxy",
            "name_full": "Next-word prediction accuracy (used as a proxy for perplexity in closed models)",
            "brief_description": "A next-token prediction task run on the same dataset inputs (without questions) used to probe whether standard next-token metrics correlate with downstream reasoning accuracy on long inputs; found to correlate negatively with FLenQA reasoning performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "Custom next-word prediction over FLenQA inputs",
            "task_type": "next-token prediction",
            "problem_format": "prompted completion of the next word; exact-match counted as correct",
            "difficulty_level": "not applicable (diagnostic probe)",
            "prompting_method": "direct prompt to predict next word in text (samples from dataset without questions)",
            "performance_metric": "next-word accuracy and correlation with reasoning accuracy",
            "performance_value": "Next-word accuracy increases with input length (as in prior works), but next-word accuracy correlates negatively with FLenQA reasoning accuracy (Pearson correlation reported: r = -0.95, p = 0.01 for the aggregated measure presented).",
            "internal_analysis": "Behavioral conclusion: perplexity/next-word metrics are not predictive of reasoning performance on long inputs; the paper presents a strong negative correlation between next-token accuracy and downstream reasoning accuracy on FLenQA. No mechanistic probes (e.g., attention or neuron analysis) were performed.",
            "failure_modes": "Indicates that relying on next-token or perplexity improvements for long-context capability is misleading; improvements in next-token metrics may coincide with worse downstream reasoning in long contexts.",
            "scaling_trend": "Not analyzed across model sizes here; general prior observations (cited) show next-word accuracy can improve with context length while downstream task performance may not.",
            "uuid": "e6679.6",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GSM-8K (mention)",
            "name_full": "GSM-8K (grade school math benchmark)",
            "brief_description": "Referenced as an example task in prior work (Shi et al., 2023) where appending irrelevant text reduced performance; GSM-8K is a standard dataset of grade-school math word problems used to evaluate numeric reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "math word problems / grade-school arithmetic reasoning",
            "problem_format": "natural-language math word problems",
            "difficulty_level": "grade-school to middle-school math word problems",
            "prompting_method": "not applied in this paper (cited work used GSM-8K as target task)",
            "performance_metric": null,
            "performance_value": null,
            "internal_analysis": "This paper cites prior finding (Shi et al., 2023) that appending irrelevant context to GSM-8K prompts reduces model performance; no new GSM-8K experiments are run in this paper.",
            "failure_modes": "Cited behavior: distraction by irrelevant context reduces arithmetic/word-problem accuracy in prior work; paper extrapolates similar long-input failure modes may affect reasoning tasks broadly.",
            "scaling_trend": "Not analyzed in this paper.",
            "uuid": "e6679.7",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Length-induced failure modes",
            "name_full": "Length-induced failure modes observed in long-context reasoning",
            "brief_description": "A set of recurrent behavioral failures the authors identify as input length grows: increasing non-answers/refusals, label bias (toward 'False'), answer-before-CoT-steps, and declining CoT coverage of relevant facts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_family": null,
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "FLenQA (observed on this dataset)",
            "task_type": "affects general reasoning tasks (including potentially arithmetic word problems if similarly formatted)",
            "problem_format": "observed across both Direct and CoT prompting on True/False text-reasoning",
            "difficulty_level": "manifest even on simple two-evidence-paragraph tasks",
            "prompting_method": "observed under Direct and CoT prompting",
            "performance_metric": "frequency of failure modes; accuracy impact",
            "performance_value": "Non-answer frequency and 'False' bias increase monotonically with input length (figures and per-model charts provided); statistical associations reported (e.g., odds-ratios for 'answer before steps' and incomplete coverage relating to incorrect responses, both p &lt; 0.001).",
            "internal_analysis": "Behavioral-level statistical analysis: (a) incorrect responses are statistically dependent on occurrence of answers before reasoning steps (odds-ratio 3.643, p&lt;0.001); (b) incomplete coverage of required facts in CoT steps correlates with incorrect responses (odds-ratio 3.138, p&lt;0.001). No internal activation or attention analyses were conducted.",
            "failure_modes": "Explicitly: (1) Failure to answer (refusals) increases with length; (2) Label bias toward 'False' with longer input; (3) 'Answer-first' behavior in CoT outputs becomes more common with length; (4) CoT coverage of relevant facts declines as input length increases.",
            "scaling_trend": "Observed across multiple models of varying capability; larger models (e.g., GPT-4) show these effects less severely but are not immune.",
            "uuid": "e6679.8",
            "source_info": {
                "paper_title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Large language models can be easily distracted by irrelevant context",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        }
    ],
    "cost": 0.017721499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</h1>
<p>Mosh Levy ${ }^{<em> 1}$ Alon Jacoby ${ }^{</em> 1}$ Yoav Goldberg ${ }^{1,2}$<br>${ }^{1}$ Bar-Ilan University ${ }^{2}$ Allen Institute for AI<br>{moshe0110, alonj4}@gmail.com</p>
<h4>Abstract</h4>
<p>This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.</p>
<h2>1 Introduction</h2>
<p>Recent advancements in Large Language Models (LLMs) show impressive performance across a range of tasks (OpenAI, 2023; Anil et al., 2023; Jiang et al., 2024), including answering correctly complex questions requiring multiple reasoning steps (Kojima et al., 2022; Wei et al., 2022). These models also claim to support increasingly longer inputs. This development underscores the need to examine their performance on the longer inputs they are now technically supporting.</p>
<p>A reasonable assumption is that support for long inputs would transfer across tasks and enable a model adept at solving a task when presented in a</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Reasoning performance drops as input grows, across a variety of tasks. Inputs are composed of text containing information relevant to the task (in red), and irrelevant text (grey) which is drawn from various sources and extended incrementally. Two separate text spans are required to answer correctly, and are located randomly in the input. Each point reflects the performance across 600 samples.
short input prompt, to perform the same task when it is embedded within a longer prompt. Does this assumption hold? Recent studies that benchmark models over tasks that involve longer inputs, including reasoning tasks, indicate that indeed models often struggle with reasoning over long inputs (Shaham et al., 2023; Li et al., 2023; Bai et al., 2023). However, these studies do not properly control their variables, and vary both the input length and the associated tasks to be performed. This makes it it hard to say if the degraded performance is due to the requirement to work with longer input, or due to the task being generally harder.</p>
<p>In this work, we study the effect of increasing the input length on model performance, while keeping other factors as constant as possible.</p>
<p>We employ a methodology to measure model</p>
<p>performance trends as a function of input length, by isolating it as a variable, while keeping the underlying task intact (§2).</p>
<p>To that end, we introduce Flexible LENgth Question Answering dataset (FLenQA) , a QA dataset for text-based reasoning (§3). For each sample, composed of a True/False question over two pieces of information required to answer it (the context), we create multiple versions of different lengths by embedding the context parts within longer, irrelevant texts. To ensure that models utilize their entire input, the dataset is composed of tasks for which both pieces of information must reasoned over together in order to correctly answer the question. At the same time, we keep the tasks simple enough such that models answer most of them correctly when the information pieces are presented on their own, with no additional padding.</p>
<p>We show that LLMs quickly degrade in their reasoning capabilities, even on input length of 3000 tokens, which is much shorter than their technical maximum (on average over all tested models, a drop in accuracy from 0.92 to 0.68 ).</p>
<p>Additionally, we explore the effect of embedding the information pieces in various locations within the context, as well as with two kinds of contexts: similar to the information pieces, or dissimilar to them (§4). We find that regardless of the experimental setting, there are similar trends of degradation.</p>
<p>We also show that next-word prediction performance of models on long inputs is uncorrelated with their performance on downstream tasks of reasoning on long inputs (§5).</p>
<p>Furthermore, we find that while Chain-ofThought (CoT) prompting (Kojima et al., 2022; Wei et al., 2022) increases performance in short inputs, in most models it does not mitigate the degradation of performance when inputs are longer: while CoT prompting increases the accuracy over non-CoT prompting, the amount of increase is roughly consistent across context lengths, and is far from closing the performance drop due to long context (§6). The only exception to that is GPT4 ${ }^{2}$, in which the gap between CoT and normal prompting increases as the input is longer.</p>
<p>Finally, we analyse our results and identify several failure modes in model responses (§7). We find that with longer inputs models tend not to follow</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>specific instructions in the input, either providing no answer, or - in the case of CoT prompting presenting the final answer before outlining the reasoning steps. We also observe a bias towards answering "false", as well as a decline in the models' ability to incorporate relevant information in their responses, as input length increases.</p>
<h2>2 Desired Data Properties</h2>
<p>Our goal is to understand how input length affects LLMs reasoning capabilities over text, given that the relevant information remains the same. We thus use question answering tasks that require models to reason over a given text. For the investigation to be applicable to both open and closed models, we chose a behavioral approach that relies on input intervention (Holtzman et al., 2023).</p>
<p>We aim for our data to satisfy the following requirements:</p>
<p>Ensuring models reason over the input. To examine the performance of models on long inputs, we require that the task can only be solved correctly by drawing conclusions from evidence in the text (Huang and Chang, 2022).</p>
<ol>
<li>Each data sample should contain several relevant text spans that are both necessary and sufficient to correctly solve the task.</li>
<li>All relevant spans must be consulted jointly to reach a successful solution. Some tasks, like text summarization, can be solved using a "divide-and-conquer" approach (Gidiotis and Tsoumakas, 2020; Liu et al., 2022; Wolhandler et al., 2022), where each relevant span is individually identified, and then paraphrased and added to the output. We wish to avoid such decomposable tasks, as they do not really require reasoning over long inputs.</li>
<li>The question and supporting relevant spans should consist of novel facts not seen in training. Ensuring that a task requires reasoning across multiple text spans is a stronger requirement then a task that requires multi hop reasoning. It was shown that models can answer existing reasoning dataset when given one some of the parts that were claimed to be required for the task (Chen and Durrett, 2019; Min et al., 2019). To avoid model reliance on parametric knowledge when we expect a reasoning process to be done (i.e to avoid data</li>
</ol>
<p>contamination (Jacovi et al., 2023; Sainz et al., 2023)), we desire that an evaluation aimed to test reasoning capabilities will require reasoning over texts that were not previously available.</p>
<p>Isolating the length factor. To isolate the effect of length, we impose the following requirements:</p>
<ol>
<li>The required reasoning should be independent of the length of the sample: the relevant spans should remain the same in all length variations.</li>
<li>The added material (a.k.a "padding", text that is added to control the samples' length) should not contradict or interfere with the reasoning over the relevant text spans.</li>
<li>The location of each relevant span within the input should be controllable.</li>
</ol>
<p>Maintaining natural-looking inputs. The input should reflect something a user may naturally use in an LLM prompt. For example, a sequence of unrelated sentences is not natural. In contrast, a sequence of unrelated paragraphs but where each paragraph is cohesive is more natural, as such an input may result from collecting relevant information from multiple sources. To best maintain the naturality of the inputs while changing an input's length, we require that the input should be cohesive at least at the level of paragraphs.</p>
<h2>3 FLenQA</h2>
<p>We introduce the Flexible LENgth Question Answering dataset (FLenQA), which follows the requirements set in $\S 2$.</p>
<p>FlenQA is composed of three reasoning tasks: Monotone Relations (a new task), People In Rooms (a new task) and a simplified version of Ruletaker (Clark et al., 2021) (§3.2). Each task consists of 100 base instances, from which we create variations of different lengths, different background texts, and different dispersion of facts within the background texts (§3.3).</p>
<p>Each task is completely balanced in its label distribution ("True" and "False"), and we ensure that most base-instances within it will be solved correctly by the LLMs when presented in their unexpanded forms (§3.4).</p>
<p>We release the dataset and the code to generate it from scratch to support future studies of reasoning
and long input performance. Generating the dataset from scratch can be used to prevent data contamination in future evaluation. Details and statistics of the tasks appear in Appendix A.</p>
<h3>3.1 Base instances.</h3>
<p>Each base-instance consists of (1) an optional prefix (for example introducing the task or supporting facts); (2) two key paragraphs, each of which is thematically coherent and starts with a key sentence needed for solving the task; and (3) an optional suffix (for example, asking a question about the preceding context). ${ }^{3}$ For each instance, the different parts are joined by newlines and fed to the LLM.</p>
<p>Throughout the text, key paragraphs are typeset in red, the supporting sentences within them in darker red, and the optional prefixes and suffixes in black. The full prompts used for each dataset are in Appendix B.</p>
<p>Deriving the key paragraphs Each task relies on two facts, expressed as simple sentences. Each of these sentences is then expanded to a thematically-coherent paragraph, in order to ensure the naturality requirement. This expansion is performed using GPT-4, which we prompt to extend the sentences without adding new information, followed by a manual verification of the results by the authors.</p>
<h3>3.2 The tasks</h3>
<p>Monotone relations (MonoRel) Each key sentence is comparing two person names on monotone scale, e.g. " X is larger than Y ", " Y is larger than Z". The suffix is a True/False question that asks about a relation between two entities that appear in different sentences (they are not explicitly compared in the text). The relations are transitive and monotone in nature.</p>
<h2>MonoRel Example:</h2>
<p>Julie Baker is younger than Julian Barton. This is a fact that remains constant, unchanging like the northern star. It's a truth that is as clear as day that she ...
Samantha Arnold is younger than Julie Baker. It means that Samantha Arnold has experienced fewer birthdays than Julie Baker. ...
Is Samantha Arnold younger than Julian Barton?</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This data is inspired by different monotonic relations describing kinship, introduced by Sinha et al. 2018. We define a new set of relation types in this work. Following the requirements in $\S 2$, answering the question requires reasoning over both key sentences. The data is created programmatically by randomly drawing names from Faker python library (Faraglia and Contributors, 2012) and a relation from a list of hand-crafted relations.</p>
<p>People In Rooms (PIR) In each sample in the task, in one key sentence person is said to be located in a named room (" $X$ is in the old library"), and the other key sentence describes the room to have a certain property ("the old library has wooden floors"). The task is then to infer whether the given person is located in a room with the given property.</p>
<h2>PIR Example:</h2>
<p>John's living room is marble-floored, a reality that is as intrinsic to the building as its very foundations. The moment ... Ethan Washington is in John's living room, a fact that has become as much a part of the place as the walls and the ceiling. The truth that Ethan Washington is in John's living ... Is Ethan Washington in a marble-floored room?</p>
<p>This dataset is inspired by the bAbI set of tasks (Weston et al., 2016), where reasoning is conducted on paths taken by one or more agents. PIR is a simplification of the task, involving just one agent. The names of people in the task are drawn randomly (Faraglia and Contributors, 2012). Rooms and properties were hand selected to be mutually exclusive (for example, a room is either blue-walled or redwalled), so no ambiguous examples are created.</p>
<p>Simplified Ruletaker We employ the task formulation from Ruletaker (Clark et al., 2021), a benchmark designed for theorem proving within texts that present explicit logical theories in natural language. Each instance consists of a logical rule, two sentences each introducing a fact, and a question over the rule and facts. ${ }^{4}$</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Simplified Ruletaker Example:</h2>
<p>Facts:
Erin is furry. Erin is known for his furriness. He has a lot of fur and ...
Erin is good. Erin was always known for how good he is. His goodness appears on all matters of life ...
Rule:If $X$ is big and $X$ is good then $X$ is tall. Question: can the statement "Erin is tall" be derived from the rule and the facts?</p>
<h3>3.3 Length Variations</h3>
<p>We expand each base instance to input lengths of roughly 250, 500, 1000, 2000, and 3000 tokens. ${ }^{5}$ To extend the inputs to those targets we add background text that is irrelevant to the question ("padding", §2). For each basic-instance and length pair we create different versions that differ in their source of background text: either duplicate, similar or different than the key paragraphs of the instance. For each of these, we also vary the dispersion of the key-paragraph within the background text.</p>
<h3>3.3.1 Background Texts</h3>
<p>Duplicate. To evaluate the extreme case where the length changes but the information remains the same, we perform an experiment where the each length text consists of multiple copies of the key paragraph. We duplicate each key paragraphs without any modification to achieve the target length of the input. The two duplicated paragraphs appear in alternating order until the desired sample length is achieved. In this case, of the two sub-tasks of QA reasoning - identifying the key information and reasoning over it, the first sub-task is trivial.</p>
<p>Similar: resampling from the same task. To get background text that is similar to the key paragraphs, we pad using paragraphs sampled from other base instances of the same task. To avoid creating contradictions, we exclude paragraphs that contain entities appearing in the key paragraphs. This padding therefore does not produce adversarial or ambiguous versions of the samples. This type of padding creates an input that resembles the RAG setup, where the input is composed of independent texts from a similar source (Mao et al., 2020).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Inputs construction. Key sentences (dark red), are expanded to key paragraphs (light red) which are dispersed in controlled locations among padding text (grey) which is irrelevant to the task.</p>
<p>Different: Book Corpus. To get background text that differs from the key paragraphs, we use text from the Books Corpus [zhu2015book]. We sample a random (continuous) text from the Book Corpus, and inject each of the key paragraphs within it, while respecting sentence boundaries.</p>
<h3>3.3.2 Location of key paragraphs in the text</h3>
<p>We consider four distinct ways in which the key paragraphs are dispersed within the background text: in the first three cases the key paragraphs appear adjacent to each other, while in the fourth the key paragraphs are separated by intervening text of various lengths.</p>
<p>(1) <em>Key paragraphs first</em>: The key paragraphs appear at the beginning of the text followed by padding;</p>
<p>(2) <em>Key paragraphs middle</em>: Half of the padding is affixed before and half after the key paragraphs, but not between them (the key paragraphs are exactly in the middle);</p>
<p>(3) <em>Key paragraphs last</em>: The key paragraphs appear at the end of the text, with padding prepended before them as a prefix;</p>
<p>(4) <em>Random placement</em>: padding is added before, between and after the paragraphs, with random intervals.</p>
<p>A visual representation is provided in Figure 2.</p>
<h3>3.4 Base instances are answerable</h3>
<p>We estimate the baseline accuracy by evaluating the LLMs on the minimal text of each sample in the dataset that includes only the question and the key paragraphs relevant to it. The results of the base instances are brought in 3.4. We found that even when using non-CoT prompting, four out of the five models achieve high accuracy (&gt;0.89). The lowest performing model (GPT3.5) achieve high enough accuracy for degradation to be observable (0.77).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Prompt</th>
<th>MonoRel</th>
<th>PIR</th>
<th>Ruletaker*</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT3.5</td>
<td>Direct</td>
<td>0.77</td>
<td>0.81</td>
<td>0.74</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>0.86</td>
<td>0.88</td>
<td>0.88</td>
</tr>
<tr>
<td>GPT4</td>
<td>Direct</td>
<td>1.00</td>
<td>1.00</td>
<td>0.98</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>1.00</td>
<td>1.00</td>
<td>0.97</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>Direct</td>
<td>0.84</td>
<td>1.00</td>
<td>0.92</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>0.88</td>
<td>0.96</td>
<td>0.97</td>
</tr>
<tr>
<td>Mistral Medium</td>
<td>Direct</td>
<td>0.99</td>
<td>1.00</td>
<td>0.73</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>1.00</td>
<td>1.00</td>
<td>0.89</td>
</tr>
<tr>
<td>Mistral 8x7B</td>
<td>Direct</td>
<td>0.92</td>
<td>0.97</td>
<td>0.80</td>
</tr>
<tr>
<td></td>
<td>CoT</td>
<td>0.86</td>
<td>0.97</td>
<td>0.93</td>
</tr>
</tbody>
</table>
<p>Table 1: Minimal length accuracy. The evaluated models have high accuracy on the tasks in our dataset when evaluated on the minimal text (250 tokens). CoT improve performance across almost all tasks and models.</p>
<h2>4 Main Experiments</h2>
<p>We report average accuracies over all three tasks, and maintain the same setup (prompt, temperature, etc.) over all input lengths. We evaluate five recent capable LLMs: GPT4, GPT3.5, Gemini-Pro, Mistral Medium and Mistral 8x7B. We consider an output where no answer was mentioned (e.g "I don't know") as incorrect. See Appendix C for a detailed breakdown of our setup parameters.</p>
<h3>4.1 Impact of Length and Location</h3>
<p>We start by validating the impact of input length on LLM reasoning performance (Figure 1) in various experimental settings.</p>
<p>No irrelevant paragraphs We first look into the extreme case where only relevant tokens are added ("duplicate padding"). [shi2023preliminary] Demonstrate that appending irrelevant texts to the input of a reasoning task (GSM-8K [cobbe2021gpt3.5]) reduces model performance substantially. We isolate the effect of relevance by testing a setting in which the padding is duplications of the exact text of the key paragraphs. In this setup, the LLMs are not required to "search" the input to find the key paragraphs, so any bias towards any position becomes irrelevant [liu2023revisiting]. Also, any difficulty that might be imposed by the distance between the key paragraphs also becomes irrelevant. Hence, we expect that there will be no degradation in performance. The <em>Results</em> shown in Figure 3 reveal that even in this setup length does play a factor, <em>and accuracy decreases with length for all models</em>. We consider these results surprising: duplicated texts are an artificial setup which is arguably the best case scenario of long inputs, as the information is constantly repeated and there is no distracting text. In more natural cases, most of the input is irrelevant to question asked. We test this setup in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The relevance of padding is a factor, but it is distinct from the effect of length itself. Some models degrade in reasoning performance. Note, both GPT3.5 and GPT4 are less affected by length when the added tokens are relevant. Each point reflects 300 samples.</p>
<p>the next section.</p>
<p>Adjacent paragraphs surrounded by irrelevant
ones We now move to the more realistic case where the prompt includes the key paragraphs as well as additional irrelevant ones. In the first set of experiments, we keep the key paragraphs adjacent to each other: the LLM just needs to focus and operate on a single area of the input, ignoring the rest. <em>Liu et al. (2023b)</em> Found that in the task of extractive QA, the position of the answer in the text affects the ability of models to answer correctly. We thus experiment with the three scenarios: positioning both key paragraphs at the start, end or middle of the text. In all cases we average over both types of irrelevant padding.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Accuracy decreases as input length grows regardless of where the key paragraphs are placed within the input. Each point reflects 300 samples. Results for all models appear in Appendix C</p>
<p>The results in Figure 4 show a significant drop in accuracy as length increase beyond 500 tokens. For most models, adjacency of key paragraphs produces higher accuracy, and when the key paragraphs appear last, accuracy is often highest (suggesting recency bias). We also find that some models perform worse when the key paragraphs are in the middle, similarly to what was found in the extraction task studied recently <em>Liu et al. (2023b)</em>.</p>
<p>Non-adjacent relevant paragraphs. Finally, we test the scenario in which the relevant facts needs to be collected from two non-adjacent locations within the text.</p>
<p>Here, the results in Figure 1 show a very large drop in performance as length increases, indicating that reasoning tasks becomes significantly harder for LLMs when they need to collect evidence from two distinct locations in a large-ish context length.</p>
<h3>4.2 Kind of irrelevant material</h3>
<p>We now focus only on the non-adjacent key-paragraphs case, and explore the effect of the kind of irrelevant text. We consider two scenarios: when the irrelevant paragraphs are similar to the relevant ones (taken from the same task), and when they are different (taken from the books corpus).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance degrade in both types of padding. Books padding impact is much greater in most models. Each point reflects the performance across 300 samples.</p>
<p>Our initial expectation was that the setup in which the irrelevant paragraphs are different from the relevant ones will be easier for the model, as the irrelevant paragraphs will be easier to discard, aiding focusing on the relevant ones. However, the results (Figure 5) show that it is not case: the drop for the different setup is mostly larger than for the similar one.</p>
<p>5 Correlation with Next Word Prediction</p>
<p>Perplexity is used as the main benchmark to show that models utilize their entire input (Anil et al., 2023; Jiang et al., 2024; Ding et al., 2024). However, it was shown that performance on downstream tasks does not necessarily correlate with model perplexity (Liu et al., 2023a; Xia et al., 2022; Tay et al., 2022). Here, we will use the flexibility of our dataset to understand the correlation between perplexity and reasoning accuracy.</p>
<p>In closed models we lack access to full vocabulary token probabilities so model perplexity cannot be measured, therefore we resort to measuring next word accuracy on our data. We prompt models to complete the next word in a given text, and the output is correct if it is an exact match to the true next word. We use the samples in our dataset (without the questions) as the text and compare the results to the reasoning performance on the same samples.</p>
<p>Our method finds similar trends on the next word prediction task to those shown in other works (Anil et al., 2023; Jiang et al., 2024), namely accuracy increases as input is longer. However, as shown in Figure 6, next word accuracy correlates negatively with reasoning on FlenQA ${ }^{6}$.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Next word accuracy correlates negatively with the reasoning accuracy on FlenQA. Each point reflects the performance across 300 samples. Gemini-Pro is not included as it answered empty replies to the next word prediction task at any length.</p>
<p>This implies that measuring next word prediction and, similarly, perplexity, cannot substitute downstream task evaluation on long inputs.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>6 Does Chain of Thought Help?</h2>
<p>Chain of Thought (CoT) prompting, introduced by Kojima et al. (2022); Wei et al. (2022), is a technique by which the LLM is pushed to produce a text comprising of reasoning steps before concluding the correct answer for a question. Zhou et al. (2022) found that a more specific and optimised instruction ("Let's work this out in a step by step way to be sure we have the right answer.").</p>
<p>The CoT technique was shown to significantly improve the accuracy on many reasoning-based question-answering setups. Will using it change the trend and allow the LLMs to perform effectively on longer inputs? We experiment with CoT using the elicitation string of Zhou et al. (2022).</p>
<p>The results show (Figure 1) that CoT has different effects on different LLMs, and overall does not mitigate the drop in performance due to length. In most cases (GPT4, Mixtral 8x7B, Mistral Medium and GPT3.5) it improves performance, but only in GPT4 it has an increased effect as length increases, making it a limited mitigation technique. In the case of Gemini-Pro, we see that CoT decrease performance as input length is increased, even though it increase performance on short length.</p>
<p>The full results of the CoT prompting over all tasks and setups can be found in Appendix C.</p>
<h2>7 Length-induced Failure modes</h2>
<p>We find in the results four failure modes: ${ }^{7}$ consistent patterns that correlate with incorrect responses.</p>
<p>Failure to answer All of the samples in the dataset can be answered with either "True" or "False", as instructed in our prompts (Appendix B). However, some of LLMs responded with a refusal answer the question, often preceded by a sentence such as "There is not enough information in the text". This tendency grows as the input length increases, indicating a failure to comply to the instruction that specified a clear choice between "True" and "False". The trend is demonstrated in figure 7, and results over all models in Appendix C.</p>
<p>Label bias As discussed in §3, our dataset is completely balanced in the label distribution. We find that certain LLMs tend to favour one of the labels, typically "false", as the input length grows. Results for all models are in Appendix C.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The models exhibit two types of input-length dependent biases: (a) They tend to generate "False" more often than "True", and (b) they ignore instructions and generate answers which do not contain neither.</p>
<p>Answer first, reason later When using Chain-of-Thought prompting, some LLMs were much more likely to output the final true/false answer before the expected reasoning steps, as inputs grow longer. In recent work, Kojima et al. 2022 found that when models are elicited to provide the reasoning steps after the answer their performance does not increase (as expected when using autoregressive models that only attend to earlier tokens). This can be viewed as a case of failing to follow prompt instructions (see prompt instructions in Appendix B) as the length increases. In testing, we found that incorrect responses are statistically dependent on the occurrence of answers before the reasoning steps ${ }^{8}$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Most of the models tend to generate an answer before the reasoning steps, in a zero-shot CoT prompt setting, and do so more as input length increases.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Chain-of-Thought lack of coverage All the tasks in FlenQA require the LLM to: (1) locate the relevant texts within the input; and (2) perform the relevant reasoning over them. Ideally, the CoT prompt would elicit the LLM to first locate each of the relevant texts and copy them to the "steps" part, hence avoiding the effect of long input on reasoning. However, we find that as input length grows, LLMs ability to do this degrades (Figure 9).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: CoT coverage of relevant facts. As input grows, all models fail more often in outputting the taskrelevant information at the CoT reasoning steps stage.</p>
<p>We measure this by computing the coverage of the relevant text (the key sentences in each sample) in the models' "steps" part of the outputs (details in Appendix B.4). We find that in most models, the ability to locate the relevant text within the input decreases as the input length gets longer. We found incorrect responses were statistically dependent on the incomplete coverage of the facts ${ }^{9}$.</p>
<h2>8 Related Work</h2>
<p>The evaluation of LLMs on long inputs has followed two distinct pathways: benchmarks of downstream tasks and next word prediction. In the realm of benchmarks, studies proposed datasets of long input samples that can be used to evaluate models (Shaham et al., 2023, 2022; An et al., 2023b,a; Bai et al., 2023). Those datasets are curated over inputs of different, but fixed, length. This approach, while straightforward, limits our ability to inputs of varying lengths, posing a challenge in understanding the true impact of input length on model performance. On the other hand, next word prediction evaluations do offer an insights into how models handle inputs of different lengths (like done in Anil</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>et al. 2023; Jiang et al. 2024). However, the correlation of this task with downstream performance was found not consistent (Liu et al., 2023a; Xia et al., 2022; Tay et al., 2022). In this paper we reproduce this finding with respect to extended length.</p>
<p>This study builds upon prior research that examined different aspects through input intervention, studying the semantic content (theme) of a task (Dasgupta et al., 2022), prompting strategies (Kojima et al., 2022; Yao et al., 2023; Jin et al., 2024) and various properties of the QA task (Levy et al., 2023). Our investigation focuses on input length, isolating it, to reveal its impact on performance.</p>
<h2>9 Discussion</h2>
<p>We study the effect of input length on reasoning performance of current Large Language Models (LLMs). Our findings reveal a significant drop in performance with longer inputs, occurring well before reaching the models' maximum input-length capacity. Our experiments relied on FLenQA, a dataset we constructed that allows to isolate the length factor, by adjusting the parts in the input that are irrelevant to the task. We show that regardless of how we adjust the samples, there is still a strong effect of length on reasoning performance.</p>
<p>Finally, we identified specific failure modes, including difficulties in following extended instructions and biases towards less relevant information. Our analysis reveals specific failings, providing possible directions for future studies to address and rectify the weaknesses found in LLMs.</p>
<p>In conclusion, our work indicates that evaluating a model's performance based on a single input length does not provide a full picture, and more nuanced evaluation is required. We argue that for a model to be considered capable at long range, it must maintain its performance at any length it technically supports.</p>
<h2>Limitations</h2>
<p>Because of the nature of behavioral testing, the observed drop in performance with varying input lengths remains unexplained; because of lack of access to many of the models, we suspect this direction will continue to be limited. Secondly, our approach aimed to create a universally applicable test across different LLMs, leading to the selection of tasks that cater to the lowest common denominator. This approach potentially overlooks the nuanced performance differences in more com-
plex reasoning tasks (e.g 5 key paragraphs), where, for instance, stronger models might exhibit performance degradation at shorter input lengths compared to what our findings suggest. We focused on a subset of reasoning task types which may differ behaviourally from other types. Moreover, in order to extend the key sentences to key paragraphs, we employed GPT4 which may introduced some level bias to the text that surrounded the text required to the reasoning task (that was generated without GPT4). Finally, our study did not test the distance between key paragraphs, leaving an aspect of LLM performance unexplored that we leave for future research.</p>
<h2>Acknowledgements</h2>
<p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT).
The authors would like to thank Uri Katz, Royi Rassin and Natalie Shapira for illuminating discussions and comments.</p>
<h2>References</h2>
<p>Chen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023a. Leval: Instituting standardized evaluation for long context language models. ArXiv, abs/2307.11088.</p>
<p>Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023b. L-eval: Instituting standardized evaluation for long context language models.</p>
<p>Rohan Anil, Sebastian Borgeaud, Yonghui Wu, and Gemini Team Google. 2023. Gemini: A family of highly capable multimodal models. ArXiv, abs/2312.11805.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508.</p>
<p>Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In Pro-</p>
<p>ceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 3882-3890.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051.</p>
<p>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. Longrope: Extending llm context window beyond 2 million tokens.</p>
<p>Daniele Faraglia and Other Contributors. 2012. Faker.
Alexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of long documents. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:3029-3040.</p>
<p>Ari Holtzman, Peter West, and Luke Zettlemoyer. 2023. Generative models as a complex systems science: How can we make sense of large language model behavior? arXiv preprint arXiv:2308.00189.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403.</p>
<p>Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5075-5084, Singapore. Association for Computational Linguistics.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts.</p>
<p>Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. 2024. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Mosh Levy, Shauli Ravfogel, and Yoav Goldberg. 2023. Guiding llm to fool itself: Automatically manipulating machine reading comprehension shortcut triggers. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8495-8505.</p>
<p>Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? ArXiv, abs/2311.04939.</p>
<p>Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. 2023a. Same pre-training loss, better downstream: Implicit bias matters for language models. In International Conference on Machine Learning, pages 22188-22214. PMLR.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.</p>
<p>Yang Liu, Chenguang Zhu, and Michael Zeng. 2022. End-to-end segmentation-based news summarization. In Findings of the Association for Computational Linguistics: ACL 2022, pages 544-554.</p>
<p>Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for opendomain question answering. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure lln data contamination for each benchmark. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10776-10787.</p>
<p>Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zeroshot benchmark for long text understanding. arXiv preprint arXiv:2305.14196.</p>
<p>Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022. Scrolls: Standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007-12021.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can</p>
<p>be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 31210-31227. PMLR.</p>
<p>Koustuv Sinha, Shagun Sodhani, William L. Hamilton, and Joelle Pineau. 2018. Compositional language understanding with text-based relational reasoning. ArXiv, abs/1811.02959.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In 4th International Conference on Learning Representations, ICLR 2016.</p>
<p>Ruben Wolhandler, Arie Cattan, Ori Ernst, and Ido Dagan. 2022. How "multi" is multi-document summarization? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5761-5769.</p>
<p>Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. 2022. Training trajectories of language models across scales. arXiv preprint arXiv:2212.09803.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV).</p>
<p>We conclude that parametric knowledge should be accounted for when evaluating text-based reasoning capabilities. In this work we introduced FlenQA, which is composed of novel generated data to make sure that reasoning over the input is required.</p>
<h2>A Datasets</h2>
<p>Each task of the following contains 100 base instances. In each sample, there are two paragraphlength texts (key paragraphs). To achieve paragraphs of similar length, we edit them by truncating sentences beyond a specific length, resulting in an average paragraph length of 125 tokens.</p>
<h3>A.1 Ruletaker</h3>
<p>The key paragraphs in the task are as evidence for the reasoning task, a rule and a question. In the original data (Clark et al., 2021), the samples contain different number of reasoning steps. In this study, we generate new, simpler samples of the task: each sample is composed of only two facts and one logical rule. The samples we generate are of similar flavor to those that exist in the original Ruletaker data, but are generated with new statements, rules and facts. The key paragraphs and the padding apear as the facts of each sample.</p>
<table>
<thead>
<tr>
<th>Padding</th>
<th>Target Input</th>
<th>Mean Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>Length</td>
<td>Tokens</td>
</tr>
<tr>
<td>Books</td>
<td>250</td>
<td>249.8</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>508.78</td>
</tr>
<tr>
<td></td>
<td>1000</td>
<td>1009.56</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>2009.64</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>3008.38</td>
</tr>
<tr>
<td>Same</td>
<td>250</td>
<td>249.8</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>503.535</td>
</tr>
<tr>
<td></td>
<td>1000</td>
<td>1004.41</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>2005.51</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>3005.125</td>
</tr>
</tbody>
</table>
<p>Figure 10: Summary of statistics of the Ruletaker* task data.</p>
<h3>A.2 MonoRel</h3>
<p>The key paragraphs in the task act as evidence for the reasoning task, and a question. Both key paragraphs describe a monotonic relation between two people, where one person is shared between both. The key paragraphs are embedded in padding text to create a text mixture.</p>
<table>
<thead>
<tr>
<th>Padding</th>
<th>Target Input</th>
<th>Mean Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>Length</td>
<td>Tokens</td>
</tr>
<tr>
<td></td>
<td>250</td>
<td>238.06</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>490.84</td>
</tr>
<tr>
<td>Books</td>
<td>1000</td>
<td>991.41</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>1990.34</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>2990.95</td>
</tr>
<tr>
<td></td>
<td>250</td>
<td>238.06</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>491.69</td>
</tr>
<tr>
<td>Same</td>
<td>1000</td>
<td>991.43</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>1991.31</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>2991.44</td>
</tr>
</tbody>
</table>
<p>Figure 11: Summary of statistics of the MonoRel task data.</p>
<h3>A. 3 People in Rooms (PIR)</h3>
<p>One key paragraph describes the location of an individual, and the other describes some attribute of that location. The key paragraphs are embedded in padding text to create a text mixture.</p>
<table>
<thead>
<tr>
<th>Padding</th>
<th>Target Input</th>
<th>Mean Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>Length</td>
<td>Tokens</td>
</tr>
<tr>
<td></td>
<td>250</td>
<td>305.36</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>491.85</td>
</tr>
<tr>
<td>Books</td>
<td>1000</td>
<td>989.91</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>1992.00</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>2988.67</td>
</tr>
<tr>
<td></td>
<td>250</td>
<td>305.36</td>
</tr>
<tr>
<td></td>
<td>500</td>
<td>484.63</td>
</tr>
<tr>
<td>Same</td>
<td>1000</td>
<td>985.82</td>
</tr>
<tr>
<td></td>
<td>2000</td>
<td>1985.04</td>
</tr>
<tr>
<td></td>
<td>3000</td>
<td>2984.80</td>
</tr>
</tbody>
</table>
<p>Figure 12: Summary of statistics of the People In Rooms (PIR) task data.</p>
<h2>B Full Evaluation Setup</h2>
<h2>B. 1 Prompts</h2>
<h2>Ruletaker prompt - Normal:</h2>
<p>Answer whether the statement {statement} can be derived from the rule and the facts. Answer with either "True" or "False".
Rule: {rule}
Facts: {facts + padding}
Answer with either "True or "False".</p>
<h2>Ruletaker prompt - CoT:</h2>
<p>Answer whether the statement {statement} can be derived from the rule and the facts.
Show your steps then answer with either "True" or "False".
Rule: {rule}
Facts: {facts + padding}
Answer with either "True or "False". Let's work this out in a step by step way to be sure we have the right answer.</p>
<h2>PIR prompt - Normal:</h2>
<p>{facts + padding}
True/False Question: {question}
Answer only True or False.</p>
<h2>PIR prompt - CoT:</h2>
<p>Show your steps then answer with 'true' or 'false'.
{facts + padding}
True/False Question: {question}
Let's work this out in a step by step way to be sure we have the right answer.</p>
<h2>MonoRel prompt - Normal:</h2>
<p>Here are some facts. Answer the exact following question based on the text: {question} Answer the question as it appears exactly. {facts + padding}
{question}
Answer only True or False.</p>
<h2>MonoRel prompt - CoT:</h2>
<p>Here are some facts. Answer the exact following question based on the text: {question} Answer the question as it appears exactly.
Show your steps then answer with 'true' or 'false'.
{facts + padding}
{question}
Let's work this out in a step by step way to be sure we have the right answer. Show your work and finally answer with 'true' or 'false'. The final step should include the exact text of the question and the answer.</p>
<h3>B. 2 Parameters</h3>
<p>All models were evaluated with a temperature of 0 and "top p" of 0 where available to make results as reproducible as possible. Additionally, We configured Gemini Pro to ignore safety guardrails ("HARM_CATEGORY" configurations) to overcome its blank answers in some samples.</p>
<h2>B. 3 Locating the answer in models' replies</h2>
<p>To identify the models' answers in their responses, we searched for the occurrences of "false" or "true," disregarding case sensitivity. In cases where these words appeared multiple times, only the last instance was considered relevant. We tested the reliability of this method by manually examining a random sample of 100 responses and confirmed its accuracy in all instances.</p>
<h2>B. 4 Evaluating the coverage of key facts in CoT</h2>
<p>Coverage of the key facts that are relevant to the reasoning task in CoT outputs, was done by searching for (case-insensitive) match of the key sentences in the key paragraphs, within the output of each model. Full coverage means that both key sentences from the input appear in the CoT output. We verified the reliability of this method manually on a sample of 100 responses.</p>
<h2>C Full results</h2>
<p>Accuracy by Input Length (Ruletaker)
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 13: Full results for the Ruletaker dataset.</p>
<p>Accuracy by Input Length (MonoRel)
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 14: Full results for the MonoRel dataset.</p>
<p>Accuracy by Input Length (PIR)
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 15: Full results for the People In Rooms (PIR) dataset.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 16: Differences in accuracy between different positions of key paragraphs in input. Averaged over both types of irrelevant padding: similar (resampling from the data) and dissimilar (Books corpus) padding.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 17: Biases in answer generation and nonanswers. Frequency of responses with True, False, or neither, per model.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Corresponding odds-ratio is 3.643 with $p&lt;0.001$ obtained through Fisher exact test.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Corresponding odds-ratio is 3.138 with $p&lt;0.001$ obtained through Fisher exact test.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7023 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7023</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7023</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-252280335</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.coling-1.534.pdf" target="_blank">Graph-to-Text Generation with Dynamic Structure Pruning</a></p>
                <p><strong>Paper Abstract:</strong> Most graph-to-text works are built on the encoder-decoder framework with cross-attention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7023.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7023.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi-token graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi graph converted to token-level graph (token graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation used in this paper that first maps a labeled directed graph to a Levi bipartite graph (turning labeled edges into relation-nodes and connecting them to subject/object nodes, adding reverse edges) and then converts every node's textual content into token-level nodes so the graph is represented as a graph of tokens for downstream PLM adapters and graph neural re-encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi -> token graph (token graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two-step conversion: (1) transform each labeled edge (v_i, r, v_j) into a Levi bipartite graph by creating an edge-node for r and edges (v_i, r) and (r, v_j); add reverse edges for default directed edges; this yields an unlabeled bipartite graph with relation types {default, reverse}. (2) convert this Levi graph into a token graph by splitting the textual content of each node into tokens and treating each token as a node in the final graph G=(V,E,R). This token-graph is the input to structural adapters and RGAT re-encoding in the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured token-based (Levi bipartite → token nodes); lossless w.r.t. labeled edges (Levi transform preserves edge identity)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Levi transformation (edge→node conversion) followed by tokenization of node labels into token-nodes; explicit addition of reverse edges for bidirectional connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2020T02 (AMR) and ENT-DESC (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (AMR-to-text, KG-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 with structural adapters (SA-RGCN) plus Structure-Aware Cross-Attention (SACA) implemented via RGAT; optionally Dynamic Graph Pruning (DGP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses frozen T5 encoder-decoder (T5-base for ENT-DESC, T5-large for LDC2020T02) with structural adapters in the encoder (RGCN-like neighbor aggregation) and added SACA modules after the last decoder block; SACA re-encodes the token-graph conditioned on the current decoder context using relational GAT (RGAT) layers (L=2, hidden dim=256). DGP adds a learned gate per node to sparsify/prune connections during decoding. Only newly added parameters are trained; T5 weights are frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ChRF++, MF-score (M), BERTScore, ROUGE-L, PARENT, Distinct-1/2</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On LDC2020T02 (AMR): reported example for the proposed model (SACA+DGP on top of SA-RGCN initialization) — BLEU 47.85; METEOR 45.80; M (MF-score meaning component) 80.37; ChRF++ 71.75 (results reported in Table 2 of the paper). Generalization (replacing plain cross-attention in GraphWriter on AGENDA): BLEU 15.59 ±0.35, METEOR 19.70 ±0.21, ROUGE-L 28.47 ±0.14 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Re-encoding the input graph conditioned on the current generated text (SACA) yielded better target prediction and improved overall scores, especially on structurally complex graphs (larger node count, larger diameter, more reentrancies). DGP further helps by removing distracting/uninformative nodes during decoding. Both SACA and DGP introduce only a minor increase in parameter count and inference time compared to the baseline. Models were trained by freezing T5 and only training adapter/RGAT/DGP parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>SACA requires running RGAT re-encoding at each decoding step (adds computational cost per step, though authors report only a minor increase in practice); DGP gates initially tended to saturate near 1 (i.e., the model did not prune by default), necessitating an L1 regularizer to encourage sparsity; canonical ordering of serialized tokens/nodes is not reported; average token-length / concrete token-budget impact not reported; representation requires adapter/GNN machinery rather than a pure text-only approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to pure linearization/finetune approaches (e.g., FINETUNE), the Levi→token-graph plus structural adapters + SACA/DGP explicitly preserves graph topology and yields better faithfulness and scores on AMR (LDC2020T02) and robustness on complex graphs; outperforms SA-RGCN baseline (which injects structural adapters but lacks SACA re-encoding) and other prior structural encoders in reported experiments. It handles complex graphs (>30 nodes, larger diameters, more reentrancies) better than alternatives.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7023.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7023.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph linearization (FINETUNE / SPRING)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization / sequence serialization of graphs for direct finetuning of sequence models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple representation that serializes graph content into a linear token sequence (graph linearization) and directly finetunes a seq2seq PLM (T5) on the serialized sequences; used as a baseline (FINETUNE) and also used in SPRING as a graph-linearization approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structural adapters in pretrained language models for AMR-to-Text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph linearization (sequence serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the graph (AMR or KG) into a single token sequence for input to a sequence-to-sequence model (T5). The exact serialization scheme is not detailed in this paper, but the approach treats the graph as an ordered sequence of tokens/attributes/triples for direct finetuning of the PLM encoder-decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential / token-based (lossy with respect to explicit topology unless augmented with structural tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Linearization / serialization of graph structure into a sequence (e.g., triple lists, bracketed AMR linearization) — specific traversal not specified in this paper. SPRING explicitly uses a graph linearization strategy in its symmetric transduction approach (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ENT-DESC (KG) and LDC2020T02 (AMR) used in comparisons; FINETUNE reported as a baseline on ENT-DESC</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (KG-to-text, AMR-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 finetuned on serialized graph sequences (FINETUNE); SPRING uses pretrained encoder-decoder extended with linearization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Full finetuning of T5 on serialized graph inputs (no structural adapters). SPRING uses a pretrained encoder-decoder and graph linearization as part of a symmetric AMR parsing/generation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ChRF++, ROUGE-L, PARENT (depending on dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper notes: FINETUNE performs better than many previous methods on ENT-DESC (KG) in their comparisons (explicit numeric values for FINETUNE on ENT-DESC not reproduced in the paper's text block).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple to implement and can yield strong scores (in ENT-DESC the FINETUNE baseline is reported as strong), but it forgoes explicit structural modeling and thus may be less robust when topology matters (e.g., complex AMR graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Treating graphs as plain sequences ignores explicit graph topology, potentially harming faithfulness and handling of complex/reentrant structures; may be sensitive to serialization order and cannot exploit structural inductive biases unless additional tokens or encoders are added.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>FINETUNE (linearization) is simpler than structural-adapter or graph-encoder approaches (SA-RGCN, SACA). The paper reports that SA-RGCN and the proposed SACA/DGP achieve better faithfulness and robustness on structurally complex graphs (AMR LDC2020T02), while FINETUNE can perform competitively on noisy KG datasets like ENT-DESC but generally lacks the explicit structural advantages.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7023.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7023.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delexicalization (entity placeholders)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalization: replacing entity surface forms with placeholder/type tokens in serialized representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation technique mentioned as used by MGCN+CNN on ENT-DESC that replaces specific entities (main and topic-related) with type/index placeholders before training, thereby reducing surface variability and simplifying generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ENT-DESC: Entity description generation by exploring knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>delexicalized serialized graph (entity placeholders)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace entity surface strings (main entity and topic-related entities) with placeholder tokens indicating entity type and index prior to serialization; the model is trained on delexicalized sequences and requires post hoc re-lexicalization to restore surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, lossy (removes specific lexical information about entities)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Entity replacement with placeholder/type tokens followed by sequence serialization (e.g., triple lists or linearized graph).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ENT-DESC (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation (entity description generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MGCN + CNN with delexicalization (as described by Cheng et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-graph convolutional network combined with CNN text decoder; experimental variant applied delexicalization (not reproduced by authors of the present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ROUGE-L (noted in the paper as being greatly boosted by delexicalization in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Delexicalization can substantially boost surface-form overlap metrics (e.g., ROUGE-L) by reducing variability in entity surface forms and enabling the model to focus on structural/content patterns rather than memorizing entity names.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy: removes actual lexical content and requires a relexicalization step at inference; can artificially inflate metrics that reward surface overlap; the authors of the current paper did not reproduce the delexicalized variant and therefore did not compare directly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Delexicalization can outperform pure end-to-end lexical models on certain surface metrics (e.g., ROUGE-L), but it is not directly comparable when other systems are not delexicalized; structural adapter and re-encoding methods (SACA+DGP) aim to improve faithfulness without resorting to delexicalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structural adapters in pretrained language models for AMR-to-Text generation <em>(Rating: 2)</em></li>
                <li>One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline <em>(Rating: 2)</em></li>
                <li>Graph transformer for graph-to-sequence learning <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>ENT-DESC: Entity description generation by exploring knowledge graph <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7023",
    "paper_id": "paper-252280335",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Levi-token graph",
            "name_full": "Levi graph converted to token-level graph (token graph)",
            "brief_description": "Representation used in this paper that first maps a labeled directed graph to a Levi bipartite graph (turning labeled edges into relation-nodes and connecting them to subject/object nodes, adding reverse edges) and then converts every node's textual content into token-level nodes so the graph is represented as a graph of tokens for downstream PLM adapters and graph neural re-encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Levi -&gt; token graph (token graph)",
            "representation_description": "Two-step conversion: (1) transform each labeled edge (v_i, r, v_j) into a Levi bipartite graph by creating an edge-node for r and edges (v_i, r) and (r, v_j); add reverse edges for default directed edges; this yields an unlabeled bipartite graph with relation types {default, reverse}. (2) convert this Levi graph into a token graph by splitting the textual content of each node into tokens and treating each token as a node in the final graph G=(V,E,R). This token-graph is the input to structural adapters and RGAT re-encoding in the decoder.",
            "representation_type": "graph-structured token-based (Levi bipartite → token nodes); lossless w.r.t. labeled edges (Levi transform preserves edge identity)",
            "encoding_method": "Levi transformation (edge→node conversion) followed by tokenization of node labels into token-nodes; explicit addition of reverse edges for bidirectional connectivity.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2020T02 (AMR) and ENT-DESC (KG)",
            "task_name": "graph-to-text generation (AMR-to-text, KG-to-text)",
            "model_name": "T5 with structural adapters (SA-RGCN) plus Structure-Aware Cross-Attention (SACA) implemented via RGAT; optionally Dynamic Graph Pruning (DGP)",
            "model_description": "Uses frozen T5 encoder-decoder (T5-base for ENT-DESC, T5-large for LDC2020T02) with structural adapters in the encoder (RGCN-like neighbor aggregation) and added SACA modules after the last decoder block; SACA re-encodes the token-graph conditioned on the current decoder context using relational GAT (RGAT) layers (L=2, hidden dim=256). DGP adds a learned gate per node to sparsify/prune connections during decoding. Only newly added parameters are trained; T5 weights are frozen.",
            "performance_metric": "BLEU, METEOR, ChRF++, MF-score (M), BERTScore, ROUGE-L, PARENT, Distinct-1/2",
            "performance_value": "On LDC2020T02 (AMR): reported example for the proposed model (SACA+DGP on top of SA-RGCN initialization) — BLEU 47.85; METEOR 45.80; M (MF-score meaning component) 80.37; ChRF++ 71.75 (results reported in Table 2 of the paper). Generalization (replacing plain cross-attention in GraphWriter on AGENDA): BLEU 15.59 ±0.35, METEOR 19.70 ±0.21, ROUGE-L 28.47 ±0.14 (Table 7).",
            "impact_on_training": "Re-encoding the input graph conditioned on the current generated text (SACA) yielded better target prediction and improved overall scores, especially on structurally complex graphs (larger node count, larger diameter, more reentrancies). DGP further helps by removing distracting/uninformative nodes during decoding. Both SACA and DGP introduce only a minor increase in parameter count and inference time compared to the baseline. Models were trained by freezing T5 and only training adapter/RGAT/DGP parameters.",
            "limitations": "SACA requires running RGAT re-encoding at each decoding step (adds computational cost per step, though authors report only a minor increase in practice); DGP gates initially tended to saturate near 1 (i.e., the model did not prune by default), necessitating an L1 regularizer to encourage sparsity; canonical ordering of serialized tokens/nodes is not reported; average token-length / concrete token-budget impact not reported; representation requires adapter/GNN machinery rather than a pure text-only approach.",
            "comparison_with_other": "Compared to pure linearization/finetune approaches (e.g., FINETUNE), the Levi→token-graph plus structural adapters + SACA/DGP explicitly preserves graph topology and yields better faithfulness and scores on AMR (LDC2020T02) and robustness on complex graphs; outperforms SA-RGCN baseline (which injects structural adapters but lacks SACA re-encoding) and other prior structural encoders in reported experiments. It handles complex graphs (&gt;30 nodes, larger diameters, more reentrancies) better than alternatives.",
            "uuid": "e7023.0"
        },
        {
            "name_short": "Graph linearization (FINETUNE / SPRING)",
            "name_full": "Graph linearization / sequence serialization of graphs for direct finetuning of sequence models",
            "brief_description": "A simple representation that serializes graph content into a linear token sequence (graph linearization) and directly finetunes a seq2seq PLM (T5) on the serialized sequences; used as a baseline (FINETUNE) and also used in SPRING as a graph-linearization approach.",
            "citation_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "mention_or_use": "use",
            "representation_name": "graph linearization (sequence serialization)",
            "representation_description": "Serialize the graph (AMR or KG) into a single token sequence for input to a sequence-to-sequence model (T5). The exact serialization scheme is not detailed in this paper, but the approach treats the graph as an ordered sequence of tokens/attributes/triples for direct finetuning of the PLM encoder-decoder.",
            "representation_type": "sequential / token-based (lossy with respect to explicit topology unless augmented with structural tokens)",
            "encoding_method": "Linearization / serialization of graph structure into a sequence (e.g., triple lists, bracketed AMR linearization) — specific traversal not specified in this paper. SPRING explicitly uses a graph linearization strategy in its symmetric transduction approach (cited).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ENT-DESC (KG) and LDC2020T02 (AMR) used in comparisons; FINETUNE reported as a baseline on ENT-DESC",
            "task_name": "graph-to-text generation (KG-to-text, AMR-to-text)",
            "model_name": "T5 finetuned on serialized graph sequences (FINETUNE); SPRING uses pretrained encoder-decoder extended with linearization",
            "model_description": "Full finetuning of T5 on serialized graph inputs (no structural adapters). SPRING uses a pretrained encoder-decoder and graph linearization as part of a symmetric AMR parsing/generation setup.",
            "performance_metric": "BLEU, METEOR, ChRF++, ROUGE-L, PARENT (depending on dataset)",
            "performance_value": "Paper notes: FINETUNE performs better than many previous methods on ENT-DESC (KG) in their comparisons (explicit numeric values for FINETUNE on ENT-DESC not reproduced in the paper's text block).",
            "impact_on_training": "Simple to implement and can yield strong scores (in ENT-DESC the FINETUNE baseline is reported as strong), but it forgoes explicit structural modeling and thus may be less robust when topology matters (e.g., complex AMR graphs).",
            "limitations": "Treating graphs as plain sequences ignores explicit graph topology, potentially harming faithfulness and handling of complex/reentrant structures; may be sensitive to serialization order and cannot exploit structural inductive biases unless additional tokens or encoders are added.",
            "comparison_with_other": "FINETUNE (linearization) is simpler than structural-adapter or graph-encoder approaches (SA-RGCN, SACA). The paper reports that SA-RGCN and the proposed SACA/DGP achieve better faithfulness and robustness on structurally complex graphs (AMR LDC2020T02), while FINETUNE can perform competitively on noisy KG datasets like ENT-DESC but generally lacks the explicit structural advantages.",
            "uuid": "e7023.1"
        },
        {
            "name_short": "Delexicalization (entity placeholders)",
            "name_full": "Delexicalization: replacing entity surface forms with placeholder/type tokens in serialized representations",
            "brief_description": "A preprocessing representation technique mentioned as used by MGCN+CNN on ENT-DESC that replaces specific entities (main and topic-related) with type/index placeholders before training, thereby reducing surface variability and simplifying generation.",
            "citation_title": "ENT-DESC: Entity description generation by exploring knowledge graph",
            "mention_or_use": "mention",
            "representation_name": "delexicalized serialized graph (entity placeholders)",
            "representation_description": "Replace entity surface strings (main entity and topic-related entities) with placeholder tokens indicating entity type and index prior to serialization; the model is trained on delexicalized sequences and requires post hoc re-lexicalization to restore surface forms.",
            "representation_type": "token-based, lossy (removes specific lexical information about entities)",
            "encoding_method": "Entity replacement with placeholder/type tokens followed by sequence serialization (e.g., triple lists or linearized graph).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "ENT-DESC (KG)",
            "task_name": "KG-to-text generation (entity description generation)",
            "model_name": "MGCN + CNN with delexicalization (as described by Cheng et al., 2020)",
            "model_description": "A multi-graph convolutional network combined with CNN text decoder; experimental variant applied delexicalization (not reproduced by authors of the present paper).",
            "performance_metric": "ROUGE-L (noted in the paper as being greatly boosted by delexicalization in prior work)",
            "performance_value": null,
            "impact_on_training": "Delexicalization can substantially boost surface-form overlap metrics (e.g., ROUGE-L) by reducing variability in entity surface forms and enabling the model to focus on structural/content patterns rather than memorizing entity names.",
            "limitations": "Lossy: removes actual lexical content and requires a relexicalization step at inference; can artificially inflate metrics that reward surface overlap; the authors of the current paper did not reproduce the delexicalized variant and therefore did not compare directly.",
            "comparison_with_other": "Delexicalization can outperform pure end-to-end lexical models on certain surface metrics (e.g., ROUGE-L), but it is not directly comparable when other systems are not delexicalized; structural adapter and re-encoding methods (SACA+DGP) aim to improve faithfulness without resorting to delexicalization.",
            "uuid": "e7023.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structural adapters in pretrained language models for AMR-to-Text generation",
            "rating": 2,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        },
        {
            "paper_title": "One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline",
            "rating": 2,
            "sanitized_title": "one_spring_to_rule_them_both_symmetric_amr_semantic_parsing_and_generation_without_a_complex_pipeline"
        },
        {
            "paper_title": "Graph transformer for graph-to-sequence learning",
            "rating": 2,
            "sanitized_title": "graph_transformer_for_graphtosequence_learning"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "ENT-DESC: Entity description generation by exploring knowledge graph",
            "rating": 2,
            "sanitized_title": "entdesc_entity_description_generation_by_exploring_knowledge_graph"
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 1,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        }
    ],
    "cost": 0.017379,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Graph-to-Text Generation with Dynamic Structure Pruning
October 12-17, 2022</p>
<p>Liang Li liliang@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Cyber Security
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Ruiying Geng ruiying.gry@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>Bowen Li 
Can Ma macan@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>DAMO Academy
Alibaba Group</p>
<p>Yinliang Yue yueyinliang@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>Binhua Li binhua.lbh@alibaba-inc.com 
DAMO Academy
Alibaba Group</p>
<p>Yongbin Li 
DAMO Academy
Alibaba Group</p>
<p>Graph-to-Text Generation with Dynamic Structure Pruning</p>
<p>Proceedings of the 29th International Conference on Computational Linguistic
the 29th International Conference on Computational LinguisticOctober 12-17, 20226115
Most graph-to-text works are built on the encoder-decoder framework with crossattention mechanism. Recent studies have shown that explicitly modeling the input graph structure can significantly improve the performance. However, the vanilla structural encoder cannot capture all specialized information in a single forward pass for all decoding steps, resulting in inaccurate semantic representations. Meanwhile, the input graph is flatted as an unordered sequence in the cross attention, ignoring the original graph structure. As a result, the obtained input graph context vector in the decoder may be flawed. To address these issues, we propose a Structure-Aware Cross-Attention (SACA) mechanism to re-encode the input graph representation conditioning on the newly generated context at each decoding step in a structure aware manner. We further adapt SACA and introduce its variant Dynamic Graph Pruning (DGP) mechanism to dynamically drop irrelevant nodes in the decoding process. We achieve new state-of-the-art results on two graph-to-text datasets, LDC2020T02 and ENT-DESC, with only minor increase on computational cost.</p>
<p>Introduction</p>
<p>Data-to-text task aims to generate a natural language description from structural or semi-structural data, such as tables (Wiseman et al., 2017), Abstract Meaning Representation (AMR) graphs (Banarescu et al., 2013), and Knowledge Graphs (KG) (Cheng et al., 2020). It helps people get the key points of the input data and makes the stored information accessible to a broader audience of endusers. There have been several practical application scenarios in this field, such as biography generation (Lebret et al., 2016), basketball news generation (Wiseman et al., 2017), and advertising text generation (Shao et al., 2019). This paper focuses on generation from graph structures in AMR and KG, referred to as graph-to-text.</p>
<p>In recent years, encoder-decoder with the crossattention mechanism has been the de facto framework for graph-to-text tasks (shown in Figure 1(a)). Given an input graph, the encoder first computes vector representations for the graph nodes. On the decoding side, Input Graph (IG) context vector is obtained via cross-attention based on the partially Generated Text (GT) at each time step, then the next target token is finally predicted. Unlike conventional text-to-text tasks, the structural nature of the input graph makes it unsuitable to naively apply sequential encoder-decoder architecture to the graph-to-text task. To alleviate this issue, recent studies (Song et al., 2018;Damonte and Cohen, 2019;Cai and Lam, 2020) proposed to utilize the graph encoder to capture the input graph structure. These works have demonstrated that explicitly modeling the graph structure can bring benefits to the model performance.</p>
<p>Although equipped with the structure-aware modeling, it is still hard for the encoder to capture all specialized information for graph-to-text generation. It is evidenced by recent studies Li et al., 2021) that a vanilla structural encoder cannot capture the accurate semantic representation of the input structural data effectively. Auxiliary supervision has been shown to be helpful, but effective auxiliary tasks are not easy to design and may not generalize well to different datasets. We suspect that it is challenging for the encoder to encode all relevant information into node representations in a single forward pass for all the decoding steps, especially if the input graph structure is complex. Besides the encoder side, few works have focused on the decoder side for graph-to-text tasks. Considering the ordinary cross-attention mechanism, the representations of input data obtained from the encoder are still treated as an unordered node representation sequence. We conjuncture that this plain cross-attention does not take full advantage of the input graph structure and therefore may harm the model performance.</p>
<p>Current models with graph encoder and crossattention may yield inaccurate input graph context representation due to the deficiency on both encoder and decoder as we discussed before. To tackle the above problems and avoid introducing auxiliary tasks, we propose a novel Structure-Aware Cross-Attention (SACA) mechanism. Apart from the plain cross-attention, our SACA re-encodes the input graph conditioning on the newly generated context in a structure-aware fashion. Other than a single forward pass, specialized representations from the source side are built adaptively at each decoding step, which makes the decoder easily exploit relevant-only information for prediction. More specifically, as shown in Figure 1(b), we construct a joint graph, in which we explicitly treat the generated text context vector as an additional node and connect it with nodes in the input graph at each decoding step. We implement SACA using the relational graph attention network (RGAT, Shaw et al. 2018). Furthermore, we stack multiple layers of SACAs to perform deep interactions between the generated text context vector and input node representations. Finally, we fetch the node representation corresponding to the newly added node as the structure-enhanced input graph context to predict the target token.</p>
<p>In practice, we notice that some nodes become ir-relevant and uninformative as the decoding goes on. These nodes are distracting and can disturb the generation process. Intuitively, the decoder should dynamically discard the unrelated parts of the graph at different decoding steps. In other words, the joint graph structure should be dynamically adjusted. To this end, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism (shown in Figure 1(c)). DGP prunes the structure of the joint graph via the gate mechanism to achieve sparse connections between the nodes based on the generated text context.</p>
<p>We conduct experiments on two graph-to-text datasets, LDC2020T02 1 and ENT-DESC (Cheng et al., 2020), to verify the effectiveness of the proposed approach. Empirical results show that our proposed methods achieve new state-of-the-art results on the two datasets. Further experiments indicate that SACA and DGP do not reduce the diversity of the generated text and can better handle complex graphs. Meanwhile, additional investigation reveals that SACA and DGP only bring minor increase on the model size and inference time.</p>
<p>Related Works</p>
<p>Graph-to-text is a challenging task which aims at generating a descriptive text from the structured knowledge, such Knowledge Graph (KG), and Abstract Meaning Representation (AMR) graphs. It is helpful for interpretability of KGs in general (Schmitt et al., 2020) and knowledge-based question answering Fu et al., 2020;.</p>
<p>In recent years, most graph-to-text methods have been built based on the encoder-decoder architecture. This kind of method usually consists of a structural encoder and a decoder. The structural encoder aims to model the structure information into the representation of the input graph. Song et al. (2018) first propose the graph recurrent networks (GRNs) to encode the AMR node directly. And then, some works (Shi et al., 2020; introduce the Graph Neural Networks (GNNs) as the structural encoder, which updates the representations of nodes based on their immediate neighbors. To integrate both local and non-local features and learn a better structural representation of a graph, Guo et al. (2019) introduce the dense connection, allowing deeper GCNs. Unlike the local information aggregation scheme, ; Cai and Lam (2020) propose the Graph Transformer that uses explicit relation encoding and allows direct communication between two distant nodes.</p>
<p>A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ , also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step.</p>
<p>Recent approaches try to apply the Pre-trained Language Models (PLMs) (Kenton and Toutanova, 2019; Raffel et al., 2019) into the graph-to-text generation. Particularly, Ribeiro et al. (2021) propose to utilize the adapter method (Pfeiffer et al., 2020) to encode graph structure into PLMs and only train graph structure-aware adapter parameters. In this way, they avoid catastrophic forgetting while maintaining the topological structure of the graph.</p>
<p>Approach</p>
<p>We expect that developing graph-to-text generation should benefit from the recent advance on pre-trained language models (PLMs) (Lewis et al., 2020;Raffel et al., 2019). To explicitly encode the input graph structure into PLMs while alleviating the catastrophic forgetting problem, we consider SA- RGCN (Ribeiro et al., 2021) as our baseline model. SA-RGCN is an adapter method to encode graph structure into PLMs. The overall illustration of our model architecture is shown in Figure  2(a). In this section, we first introduce how to represent the input graph and the architecture of our baseline SA-RGCN. Then, we depict our proposed Structure-Aware Cross-Attention (SACA) in details. Lastly, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP).</p>
<p>Graph Representation</p>
<p>Let G 0 = (V 0 , E 0 , R 0 ) denote a multi-relational and directed graph with nodes v i ∈ V 0 and labeled edges (v i , r, v j ) ∈ E 0 , where r ∈ R 0 is the relation type. Following previous work (Beck et al., 2018), we convert each input graph into a Levi graph G l = (V l , E l ), which is an unlabeled and con-nected bipartite graph. Specifically, each labeled edge (v i , r, v j ) ∈ E 0 is transformed into two unlabeled edges (v i , r), (r, v j ) ∈ E l . In addition, we add a reverse edge (v j , v i ) for each default edge (v i , v j ). Therefore, each Levi graph G l contains two type relations R l = {d, r}, where d and r denote the default and reverse edge, respectively. To better take advantage of the PLMs, we convert each G l into a new token graph G = (V, E, R), where each token of a node in V l becomes a node v ∈ V.</p>
<p>Pretrained LMs with Structural Adapters</p>
<p>To inject graph structural bias into PLMs, we incorporate the structural adapter (Ribeiro et al., 2021) into the PLMs encoder. As shown in Figure 2 (a), we add a structural adapter after each transformer encoder block on the encoder. Figure 2  Formally, at each layer l, given the encoder layer representation h l v , a structural adapter computes the representation for v by the following:
g l v = r∈R u∈Nr(v) 1 |N r (v)| W l r LN(h l v ), (1) z l v = W l e (σ(g l v )) + h l v ,(2)
where LN(·) denotes layer normalization. N r (v) is the sef of immediate neighbors under relation r ∈ R. W l r encodes the edge type between the nodes u and v. σ is the activation function.</p>
<p>We add an FNN adapter after each transformer decoder block to adapt the language model to the graph-to-text task. Given the outputĥ l v of the l th transformer decoder block, the adapter representation is computed as:
z l v = W l o (σ(W l p LN(ĥ l v ))) +ĥ l v ,(3)
where W l o and W l d denote learnable parameters.</p>
<p>Structure-Aware Cross-Attention</p>
<p>We argue that the input graph context representation obtained by the plain cross-attention may be inaccurate. The reason is twofold. First, it is not easy for the graph encoder to capture all specialized information required for generation in a single forward pass. Therefore, a single encoder without any auxiliary assistant may not be effective in capturing the accurate semantic representation (Liu  Li et al., 2021). In other words, the graph representation encoded by the graph encoder may be inaccurate. Second, during decoding, the decoder treats structural data as an unordered node sequence, which ignores the input graph structure. However, the graph structure has been proven to play an essential role in the graph representation and may offer clues about which nodes are more related to the generated text context. To tackle the above challenge, we propose a Structure-Aware Cross-Attention (SACA) mechanism, which re-encodes the input graph representation by conditioning on the newly generated context. Specifically, we first build a joint graph, in which we view the generated text (GT) context as a new node v d and explicitly connect it to each node in the input graph G at each decoding step. The corresponding reverse edges are also added. The joint graph can be formulated as
G joint = (V joint , E joint , R), where V joint = {v d } ∪ V and E joint = {(v i , v d ), (v d , v i ); v i ∈ V} ∪ E.
We use the representations from the encoder for the node from V and the hidden state from the last transformer decoder block as the representation for the GT context node.</p>
<p>To induce the representations for the nodes in the joint graph G joint and facilitate introducing Dynamic Graph Pruning (in Section 3.4), we consider graph neural network built on graph attention framework (GAT) (Shaw et al., 2018). Moreover, we employ the relational graph attention network (RGAT) implemented by Shaw et al. (2018) to model the relation between neighbor nodes. Specifically, at each RGAT layer l, we update the repre-
sentation h l v of each node v ∈ G joint by: s v,u = W q h l v T (W k h l u + E r R(v,u) ) √ m ,(4)α v,u = e sv,u u ′ ∈Nv e s v,u ′ ,(5)h l+1 v = σ( u∈Nv α v,u W v h l u ),(6)
where E r R(v,u) means the embedding of the relation between node v and u. m denotes the hidden dimension of RGAT. Finally, the representation vector h L v d corresponding to the GT context node v d is fetched and used as the structure-enhanced input graph context vector for token prediction.</p>
<p>In conclusion, SACA provides two advantages. First, it re-encodes the input graph by conditioning its representation on the newly generated context. As a result, we build specialized representations which make it easier for the decoder to exploit relevant-only information for prediction at each decoding step. Second, the re-encoding explicitly injects structural bias into input graph context modeling, helping the decoder obtain a more accurate input graph context vector. The proposed SACA can be plugged after the last transformer decoder block as shown in Figure 2 (a).</p>
<p>Dynamic Graph Pruning</p>
<p>In practice, we notice that some nodes become irrelevant and uninformative as the decoding goes on. These unrelated nodes are distracting and can even disturb the subsequent generation. Intuitively, the decoder should dynamically prune the joint graph at different decoding steps. For this purpose, we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism, which aims to dynamically drop the redundant nodes in the joint graph according to the generated text during decoding. The DGP employs the gate mechanism to sparse the connection between a node and its immediate neighbors in the joint graph to achieve graph pruning. Specifically, at each decoding step t, for each node v in the joint graph, we formulate its gate as bellow:
g v = sigmoid(W T g tanh(W e h v + W d h d t )),(7)
where W g , W e , and W d are learnable parameters. And h v is the representation of node v and h d t is the decoder hidden state at decoding step t, which is usually considered as the representation of the generated text context. The value of gate g v ∈ R decides whether the node v i should be dropped or not. Correspondingly, we apply the gate value to multiple SACA layers invariably by modifying the attention weights in SACA (Equation 5) as follows:
α v,u = g u ⊙ e sv,u u ′ ∈Nv g u ′ ⊙ e s v,u ′ .(8)
Intuitively, if the value of gate g u is close to 0, the connections between node u with all its immediate neighbors will be largely weaken. That is, the node is removed from the joint graph. Specifically, the attention score α v,u measures the relevance between any two nodes, v and u, in the joint graph, while the gate g v models the relevance between the node v and the generated text context h t .</p>
<p>As a shown example in Figure 2 (c), the red node represents the main entity. Initially, the main entity connects with all its neighbor nodes. As the decoding goes on, some nodes are redundant for the subsequent generation. For example, the nodes "actor" has been described, and node "voice actor" is also covered by the generated text. Therefore, DGP discards these nodes by giving them gates with small values.</p>
<p>We observed that the values of the gates calculated by Equation 7 are almost equal to 1, indicating that the model does not actively learn to prune a graph. Inspired by Xue et al. (2020), we further introduce a regularization item, encouraging the network to turn off more gates and generate more sparse connections between nodes in the in- put graph. We formulate it as follows:
L DGP = |y| t=1 ∥Gate t ∥ 1 |y| ,(9)
where Gate t = {g v |v ∈ V}. ∥ * ∥ 1 means L1 norm regularizer.</p>
<p>Training</p>
<p>Given a reference output y = {y 1 , y 2 , ..., y T } and an input graph G, we use the cross-entropy loss as the objective function of graph-to-text generation:
L lm = |y| t=1
log p(y t |y 1:t−1 , G, θ).</p>
<p>Finally, the overall objective function consists of two parts:
L = L lm + λL DGP ,(11)
where λ is a tunable hyper-parameter and is used to make a trade-off between the cross-entropy loss and the regularization item. Intuitively, the L DGP object encourages the model to learn how to prune the graph, and the L lm trains the model to generate the text according to the graph and restrains DGP from pruning too much.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We demonstrate the effectiveness of our models on two graph-to-text datasets: LDC2020T02 and ENT-DESC (Cheng et al., 2020)   target text consists of sentences that verbalize the main entity in KG. ENT-DESC lacks explicit alignment between the input and the output. Therefore, some knowledge in the input graph may be noise. We follow official training, development, and test splits of 88,650/11,081/11,081 instances. Table 1 summarizes the detailed statistics of LDC2020T02 and ENT-DESC.</p>
<p>Settings</p>
<p>Our implementation is based on Hugging Face (Wolf et al., 2019). The RGCN and RGAT are implemented based on PyTorch Geometric (Fey and Lenssen, 2019). We initialize our models by T5 (Raffel et al., 2019). To make a fair comparision, we following the same experimental setting with SA-RGCN (Ribeiro et al., 2021). We set the hidden dimensions of both Structural Adapter and SACA to 256. And we use T5 base for all experiments on ENT-DESC and T5 large on LDC2020T02 for a fair comparison with baselines. We use the AdamW optimizer (Loshchilov and Hutter, 2018) and employ a linearly decreasing learning rate schedule without warm-up. The learning rate is fixed as 10 −4 . We set the training batch size as 4 for all experiments. We freeze the T5 parameters and only update the newly added parameters during training. We tune the hyper-parameter λ in Equation 11 from the set [1 −2 , 5 −3 , 1 −3 , 5 −4 ], and select the best one on the development set. We stack L = 2 RGAT layers in Structure-Aware Cross-Attention. During de-coding, we use beam search with a beam size 5. We use BLEU (Papineni et al., 2002) for the early stopping criterion. All experiments are trained on Nvidia Tesla V100 32GB GPUs. Following previous works, on both datasets, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and ChRF++ (Popović, 2015) on both datasets. On LDC2020T02, following Ribeiro et al. (2021), we utilize the meaning (M) component of the MF-score (Opitz and Frank, 2021) to measure how well the source AMR graph can be reconstructed from the generated sentence (refer to A.1 for more details). We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. On ENT-DESC, We add ROUGE-L (Lin, 2004) and employ PARENT (Dhingra et al., 2019) for evaluating the faithfulness. We conduct experiments over 4 different seeds and report the average scores on them.</p>
<p>Main Results</p>
<p>We compare our method with recent state-of-theart methods (please refer to A.4 for more details).  </p>
<p>Analysis and Discussion</p>
<p>Ablation Study The overall performance on the two datasets shows the superiority of our proposed Structure-Aware Cross-Attention (SACA) and Dynamic Graph Pruning (DGP  as well as our method are all initialized by T5. And the AMR paring and AMR-to-Text are dual tasks actually. Therefore, the M score is biased and the results of our model and ADAPT are somehow inflated. Additionally, we utilize Distinct-1 and Distinct-2 (Li et al., 2016) to evaluate the diversity of the output text. We observe that SACA and DGP have little effect on Distinct-1 and Distinct-2. This implies that they will not reduce the diversity of the output text. We notice that, compared with ADAPT, w/o StrucAdapt shows a slight improvement. This indicates it is necessary to explicitly model the graph structure in the encoder, even though structural bias has been injected into the input graph context modeling during decoding. We believe this may be attributed to SACA relying on the input graph representation encoded by the encoder. Because our SACA is designed to exploit the relevant-only information for prediction, it re-encodes the input graph by conditioning its representation on the newly generated context. Therefore, the initial representation for the input graph is important.</p>
<p>Impact on the parameter and speed Furthermore, we investigate the impact of SACA and DPG on the model parameters and inference speed on LDC2020T02 development. Specifically, we calculate the additional parameters of each model with respect to T5 large . And we set the batch size to 1 to calculate the average decoding time for generating all examples. The results summarized in Table 4 indicate that SACA and DGP only bring minor increase on the model size and inference time.</p>
<p>Impact on the Graph Properties To examine the robustness of our proposed methods, we investigate the model's performance concerning different graph properties (graph size, graph diameter, and reentrancies) on LDC2020T02 and ENT-DESC. Following previous works (Cheng et al., 2020;Ribeiro et al., 2021), we use BLEU as the metric. The results are summarized in Table 5 and Table 6, respectively. For LDC2020T02, we firstly note that the BLEU scores decrease as the graph size increases since the larger graph is often  complex. Our method achieves a clear improvement when handling graphs with &gt; 30 nodes. And then we observe that the BLEU gap between our method and SA-RGCN becomes larger for a relatively larger graph diameter. Reentrancies are the nodes with multiple parents. A graph with more reentrancies is typically more complex . As shown in the last section in Table 5, our method has an improvement of +1.23 BLEU points compared to SA-RGCN when graphs contain &gt; 2 reentrancies. To sum up, the results on the LDC2020T02 dataset show the advantage of our model in dealing with the AMR graph with more complex structures. As shown in Table 6, both models perform differently on ENT-DESC than on LDC2020T02. First, we notice that both models perform the best when the graph size is between 31 and 50, and they perform poorly when the graph size is too small or too large. Cheng et al. (2020) also observed the finding, and they believe this is due to the insufficient or very noisy input information for generation. Additionally, both models perform better when graph diameter or number of the reentrancies increase. The reason is that, in the ENT-DESC, the knowledge graph with a small diameter or number of the reentrancies contains more noisy information for the generation. Please refer to A.2 for more details. The BLEU gap between our method and SA-RGCN is the largest when the graph diameter &gt; 5 or the number of reentrancies &gt; 10. The above results demonstrate that our approach makes SA-RGCN better at handling complex knowledge graphs.</p>
<p>We investigate how the model behaves on different types of graphs (AMR and KG). And the results demonstrate that our model deals better with   complex structures. We believe the improvement comes from two aspects. First, on the one hand, it is challenging for an encoder to encode all relevant information into node representations in a single forward pass, especially if the graph structure is complex. On the other hand, the re-encoding in SACA makes the decoder easily exploit the relevant-only information for prediction and explicitly injects the structural information at each decoding step. Second, DGP dynamically removes the nodes which are redundant for the subsequent generation, which makes the decoder pay more attention to the relevant nodes.</p>
<p>Generalization Study</p>
<p>Institutionally, our proposed methods can not only be applied to PLMs but also RNN based models.</p>
<p>In other words, we can easily combine the SACA and DGP with previous RNN based works. To examine the generalization of SACA and DGP, we choose GraphWriter (Koncel-Kedziorski et al., 2019) as the baseline, which consists of a multilayer graph transformer encoder and an attentionbased decoder with a copy mechanism. Further, to make a fair comparison, we conduct the generalization experiment on AGENDA dataset (Koncel-Kedziorski et al., 2019). We simply replace the plain cross-attention in GraphWriter with our proposed SACA. Additionally, we add the DGP layer before the SACA. The experiments are under the same settings as described in GraphWriter. As shown in Table 7, we observe that our proposed model significantly improves the performance of GraphWriter. The results indicate that SACA and DGP are not only effective well on PLMs-based models but also potent for RNN-based models.</p>
<p>Human Evaluation</p>
<p>Considering that the knowledge graph is more readable than AMR, we do human evaluations on the ENT-DESC test set to examine whether human judgments corroborate improvements in automatic evaluation metrics. Following Cheng et al. (2020), from outputs generated by the baseline model SA-RGCN and our final model (Ours). We distribute the outputs of different systems to three annotators with linguistic backgrounds. The annotators have no knowledge in advance about which model the generated text comes from. Specifically, we give each participant all main entities' neighbors, 1-hop and 2-hop connections between main entities, and topic-related entities as references. They are required to score the generated text from 1 to 5 in terms of three criteria: Fluency (is the sentence fluent?), Grammar (is the sentence grammatical?), and Authenticity (is the sentence more related to the input graph?). For each criterion, we calculate the final score by averaging the scores from all annotators. As shown in Figure 3, our model outperforms the baseline SA-RGCN on Fluency and Grammar metrics. For Authenticity, the improvement is more significant. The performance validates the benefit of our proposed SACA and DGP modules in capturing more accurate input graph context representations. We supply a case study in A.3.</p>
<p>Conclusions</p>
<p>In this work, we make two main contributions. First, we propose Structure-Aware Cross-Attention (SACA) to make decoder easily exploit relevant-only information for prediction. Apart from the plain cross-attention, SACA re-encodes the input graph conditioning on the newly generated context while explicitly considering the input graph structure. The second one is that we adapt SACA and propose its variant Dynamic Graph Pruning (DGP) mechanism. In detail, the DGP dynamically prunes the structure of the joint graph at different decoding steps according to the generated text. Experimental results conducted on two graph-to-text datasets, LDC2020T02 and ENT-DESC, show the effectiveness of our method. The empirical and analysis results on both datasets show that the proposed methods can improve the model's performance on complex graphs while only bringing minor increase on the model size and inference time.</p>
<p>A Appendix</p>
<p>A.1 MF-score</p>
<p>The M (Meaning Preservation) component of the MF-score (Opitz and Frank, 2021) is utilized to measure how well the source AMR graph can be reconstructed from the generated sentence. It reconstructs the AMR with a SOTA parser and computes the relative graph overlap of the reconstruction and the source AMR using graph matching. M employs the python library amrlib 2 (version 0.5.0) to make AMR parse, where the parser is a T5-based model.</p>
<p>A.2 Distribution on Graph Size</p>
<p>On the ENT-DESC test set, previous study (Cheng et al., 2020) and our experimental results (in Table 6) suggest that the model performs the best when the graph size lies in the range of 21 − 40 and has a poorer performance when the number of triples is too small or too large. It should be due to the fact that the input information is insufficient or very noisy. However, we find that the model performance increases as the graph diameter and reentrancies increase. For further investigation, we calculate the distribution of graph diameter and reentrancies broken down by graph size, respectively. The results are summarized in Figure 4. As shown in Figure 4(a), the proportion of graphs with size 21 − 40 increases as the graph diameter increases. As shown in Figure 4(b), the results on graph reentrancy follow a pattern similar to graph diameter. In a word, in ENT-DESC, the noise decreases as the graph diameter and reentrancies increase, so the model performs better.</p>
<p>A.3 Case Study</p>
<p>As shown in Figure 5, we further take a typical example from our human study to better understand how our method improves the mode's performance. Given the Knowledge Graph containing the main entity "Andrew Lawrence" and all its related entities, we aim to generate a description about the main entity. We notice that both the baseline and our model can identify the main entity. However, the baseline outputs a sentence describing the relation between "Andrew Lawrence" and "Matthew Lawrence". The relation is not existing in the input graph. Moreover, it repeatedly generates the entity "Brotherly Love" and misses the related entity "Recess". Compared with it, our model generates the 2 https://github.com/bjascob/amrlib/tree/0.5.0 sentences faithful to the input graph and correctly covers the main entity and most topic-related entities. We consider this is because the SACA helps the decoder obtain a more accurate input graph context, and the DGP removes the redundant nodes as the decoding stage progresses.</p>
<p>A.4 Baseline Models</p>
<p>On the AMR-to-Text task LDC2020T02, we compare our method with several baselines including:</p>
<p>• LDGCN (Zhang et al., 2020b) is a a dynamic fusion mechanism, which captures richer nonlocal interactions by synthesizing higher order information from the input graphs. A weight tied convolutions to reduce memory usage is applied.</p>
<p>• SPRING (Bevilacqua et al., 2021) casts Textto-AMR and AMR-to-Text as a symmetric transduction task and proposes a graph linearization and extending a pretrained encoderdecoder model.</p>
<p>On the KG-to-Text task ENT-DESC, we compare our method with several baselines including:</p>
<p>• s2s (Bahdanau et al., 2015) is a encoderdecoder based model, which allows a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.</p>
<p>• GraphTransformer (Koncel-Kedziorski et al., 2019) introduces a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints.</p>
<p>• GRN (Beck et al., 2018) couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations.</p>
<p>• GCN (Marcheggiani and Perez-Beltrachini, 2018) proposes an alternative encoder based on graph convolutional networks that directly exploits the input structure.</p>
<p>• DeepGCN (Guo et al., 2019) introduces a dense connection strategy, which is able to integrate both local and non-local features to </p>
<p>Reentrancies</p>
<p>Graph Size &gt;40</p>
<p>Graph Size 21-40</p>
<p>Graph Size 1-20 (b) The distribution of graph reentrancies by graph size. Figure 4: The clustered column charts of graph diameter and reentrancies by graph size.</p>
<p>Gold</p>
<p>Andrew James Lawrence (born January 12, 1988) is an American actor and singer. He is known for his roles as Andy Roman in "Brotherly Love" and T.J. Detweiler in "Recess". SA-RGAT Andrew Lawrence (born January 12, 1988) is an American actor, voice actor, and singer. He is best known for his roles in the films "Brotherly Love" and "Brotherly Love". He is the younger brother of Matthew Lawrence.</p>
<p>Ours</p>
<p>Andrew Lawrence (born January 12, 1988) is an American actor and singer. He is best known for his roles in "Recess" and "Brotherly Love". learn a better structural representation of a graph.</p>
<p>• MGCN + CNN (Cheng et al., 2020) is a multi-graph structure that is able to represent the original graph information more comprehensively. We do not report the results of MGCN + CNN + delex. Because it applies the delexicalization technique on the ENT-DESC dataset, which delexicalizes the main entity and topic-related entities by replacing these entities with tokens indicating the entity types and indices. The delexicalization technique greatly boosts their performance on ROUGE-L. They do not release the code about delexicalization, and we can not reproduce it.</p>
<p>What's more, FINETUNE, ADAPT and SA-RGCN are T5-based models proposed in (Ribeiro et al., 2021).</p>
<p>Figure 1 :
1(a) denotes an encoder-decoder framework with the cross-attention mechanism where IG and GT contexts denote the input graph and generated text graph contexts, respectively. (b) is an example of Structure-Aware Cross-Attention. The dotted lines in (c) denote the pruned edges and nodes.</p>
<p>(b) illustrates the architecture of a structural adapter, in where a relational GCN (RGCN) (Schlichtkrull et al., 2018) layer computes the node representation based on the local neighborhood of node v ∈ V.</p>
<p>Figure 2 :
2Lawrence … known for his roles in " Recess" … Andrew Lawrence … is an American actor and singer. He is best known for his roles in " Recess" and …Feed-Forward Layer Illustration of the proposed model architecture. (a) is an overview of our model. (b) is the architecture of a structural adapter. (c) is an example of Dynamic Graph Pruning, where r 1 ∼ r 4 denote the relations: "country of citizenship", "occupation", "sibling", and "cast member", respectively. The dummy lines in (c) denote the pruned edges. et al., 2019;</p>
<p>Figure 3 :
3Human evaluation results on ENT-DESC test set.</p>
<p>Figure 5 :
5An example of generated sentences. The main entity is highlighted in red, topic-related entities are highlighted in blue, and the sentence that is not faithful to the input graph is in green.</p>
<p>Table 2 :
2Main results of models on LDC2020T02 and ENT-DESC test datasets. ‡ means our reimplementation. The other results are copied from the original paper. Mean (±s.d.) over 4 seeds.</p>
<p>Table 2
2summarizes the results on LDC2020T02 and ENT-DESC test sets. FINETUNE is a method that transforms the input graph into a sequence and finetunes T5 directly. It does not consider the input graph structure. For LDC2020T02, our method outperforms the previous state-of-the-art model by 0.78 BLEU and 1.15 ChRF++. Com-Models 
BLEU METEOR 
M 
Dis-1 Dis-2 
GOLD 
-
-
81.00 23.82 71.76 
ADAPT 
45.22 
43.28 
79.56 23.20 71.40 
Ours 
47.85 
45.80 
80.37 23.46 71.75 
w/o DGP 
47.68 
45.51 
80.21 23.51 72.08 
w/o SACA &amp; DGP 47.20 
45.05 
80.01 23.38 71.69 
w/o StrucAdapt 
45.43 
43.54 
79.75 23.32 71.65 </p>
<p>Table 3 :
3And our model exceeds all previous works and achieves new state-of-the-art results on all metrics. The above results indicate that our proposed methods can improve the model on fluency and faithfulness.Ablation study of models on LDC2020T02 
development dataset. GOLD indicates the ground-truth 
sentences. Dis-1 and Dis-2 denote Distinct1 and Dis-
tinct2, respectively. </p>
<p>pared with our implemented SA-RGCN, we im-
prove 1.01 METEOR. Moreover, our method raises 
0.38 M, which indicates that it can generate more 
faithful sentences to the input graphs. The improve-
ment on BERTScore shows that the sentence gener-
ated by our method is more similar to the ground 
truth on the semantic level. For ENT-DESC, we 
notice FINETUNE performs better than all previ-
ous methods. SA-RGCN, which encodes graph 
structure into T5, furtherly improves the perfor-
mance. </p>
<p>Table 4 :
4Impact on parameter and speed.</p>
<p>Table 5 :
5BLEU scores with respect to graph size, graph 
diameter and number of reentrancies on LDC2020T02 
test set. </p>
<p>Table 6 :
6BLEU scores with respect to graph size, graph 
diameter and number of reentrancies on ENT-DEST test 
set. </p>
<p>Models 
BLEU 
METEOR 
ROUGE-L 
GraphWriter 
14.30 
18.80 
-
GraphWriter  ‡ 14.13 ± 0.10 18.92 ± 0.28 27.61 ± 0.16 
Ours 
15.59 ±0.35 
19.70 ±0.21 
28.47 ±0.14 </p>
<p>Table 7 :
7Generalization Study on AGENDA test dataset. ‡ means our reimplementation.</p>
<p>(a) The distribution of graph diameter by graph size.Graph Diameter 
Graph Size 
Reentrancies 
Graph Size </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>1-3 
4-5 </p>
<blockquote>
<p>5 </p>
</blockquote>
<h1>Example</h1>
<p>Graph Diameter </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.58% </p>
<p>46.84% </p>
<p>52.81% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Graph Diameter </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1-3 
4-5 </p>
<blockquote>
<p>5 </p>
</blockquote>
<h1>E</h1>
<p>Graph Diameter </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.58% </p>
<p>46.84% </p>
<p>52.81% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Graph Diameter </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>21-40 </p>
<blockquote>
<p>40 
0.31773 
0.06695 
0.498832 0.191032 
0.553757 0.427306 
0.45745 
0.22137 </p>
</blockquote>
<p>21-40 </p>
<blockquote>
<p>40 
0.315766 0.075538 
0.468407 0.199322 
0.528123 0.338262 </p>
</blockquote>
<p>Graph Size </p>
<p>Graph Size </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<h1>Example</h1>
<p>Reentrancies </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.77% </p>
<p>49.88% </p>
<p>55.38% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>Reentrancies </p>
<p>Graph Size &gt;40 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size 1-20 </p>
<p>21-40 </p>
<blockquote>
<p>40 
0.315766 0.075538 
0.468407 0.199322 
0.528123 0.338262 </p>
</blockquote>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<h1>Example</h1>
<p>Reentrancies </p>
<p>Graph Size 1-20 </p>
<p>Graph Size 21-40 </p>
<p>Graph Size &gt;40 </p>
<p>31.77% </p>
<p>49.88% </p>
<p>55.38% </p>
<p>0% </p>
<p>20% </p>
<p>40% </p>
<p>60% </p>
<p>80% </p>
<p>100% </p>
<p>&lt;6 
6-10 </p>
<blockquote>
<p>10 </p>
</blockquote>
<p>https://catalog.ldc.upenn.edu/LDC2020T02</p>
<p>Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyung Hyun Cho, Yoshua Bengio, Proc. of ICLR. of ICLRDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR.</p>
<p>Abstract meaning representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th linguistic annotation workshop and interoperability with discourse. the 7th linguistic annotation workshop and interoperability with discourseLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th linguis- tic annotation workshop and interoperability with discourse.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, In ACLDaniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In ACL.</p>
<p>One SPRING to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline. Michele Bevilacqua, Rexhina Blloshmi, Roberto Navigli, Proc. of AAAI. of AAAIMichele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One SPRING to rule them both: Sym- metric AMR semantic parsing and generation without a complex pipeline. In Proc. of AAAI.</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proc. of AAAI. of AAAIDeng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. In Proc. of AAAI.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang, Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020. Kgpt: Knowledge-grounded pre- training for data-to-text generation.</p>
<p>ENT-DESC: Entity description generation by exploring knowledge graph. Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, Luo Si, EMNLP. Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, and Luo Si. 2020. ENT- DESC: Entity description generation by exploring knowledge graph. In EMNLP.</p>
<p>Structural neural encoders for amr-to-text generation. Marco Damonte, Shay B Cohen, Proc. of AACL. of AACLMarco Damonte and Shay B Cohen. 2019. Structural neural encoders for amr-to-text generation. In Proc. of AACL.</p>
<p>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. Michael Denkowski, Alon Lavie, Proceedings of the sixth workshop on statistical machine translation. the sixth workshop on statistical machine translationMichael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Pro- ceedings of the sixth workshop on statistical machine translation.</p>
<p>. Bhuwan Dhingra, Manaal Faruqui, Ankur P Parikh, Ming-Wei Chang, Dipanjan Das, William W , Bhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh, Ming-Wei Chang, Dipanjan Das, and William W.</p>
<p>Handling divergent reference texts when evaluating table-to-text generation. Cohen, Proc. of ACL. of ACLCohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proc. of ACL.</p>
<p>Fast graph representation learning with PyTorch Geometric. Matthias Fey, Jan E Lenssen, ICLR Workshop on Representation Learning on Graphs and Manifolds. Matthias Fey and Jan E. Lenssen. 2019. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds.</p>
<p>Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun, arXiv:2007.130692020. A survey on complex question answering over knowledge base: Recent advances and challenges. arXiv preprintBin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, and Jian Sun. 2020. A survey on complex question answering over knowledge base: Recent advances and challenges. arXiv preprint arXiv:2007.13069.</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, Transactions of the Association for Computational Linguistics. Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional net- works for graph-to-sequence learning. Transactions of the Association for Computational Linguistics.</p>
<p>S 2 SQL: Injecting syntax to question-schema interaction graph encoder for text-to-SQL parsers. Binyuan Hui, Ruiying Geng, Lihan Wang, Yanyang Bowen Qin, Bowen Li, Jian Li, Yongbin Sun, Li, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsBinyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang Li, Bowen Li, Jian Sun, and Yongbin Li. 2022. S 2 SQL: Injecting syntax to question-schema interaction graph encoder for text-to-SQL parsers. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1254-1262, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Proc. of AACL. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanovaof AACLJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec- tional transformers for language understanding. In Proc. of AACL.</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, Proc. of AACL. of AACLRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text generation from knowledge graphs with graph trans- formers. In Proc. of AACL.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, EMNLP. Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with ap- plication to the biography domain. In EMNLP.</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, and comprehensionMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020. Bart: De- noising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.</p>
<p>A diversity-promoting objective function for neural conversation models. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, Proc. of NAACL. of NAACLJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objec- tive function for neural conversation models. In Proc. of NAACL.</p>
<p>Improving encoder by auxiliary supervision tasks for table-to-text generation. Liang Li, Can Ma, Yinliang Yue, Dayong Hu, ACL. Liang Li, Can Ma, Yinliang Yue, and Dayong Hu. 2021. Improving encoder by auxiliary supervision tasks for table-to-text generation. In ACL.</p>
<p>Leveraging graph to improve abstractive multi-document summarization. Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du, Proc. of ACL. of ACLWei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, and Junping Du. 2020. Leveraging graph to improve abstractive multi-document summarization. In Proc. of ACL.</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.</p>
<p>Hierarchical encoder with auxiliary supervision for neural tableto-text generation: Learning better representation for tables. Tianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, Zhifang Sui, Proc. of AAAI. of AAAITianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, and Zhifang Sui. 2019. Hierarchical encoder with auxiliary supervision for neural table- to-text generation: Learning better representation for tables. In Proc. of AAAI.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In ICLR.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Gen- eration.</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from AMR. Juri Opitz, Anette Frank, Proc. of EACL. of EACLJuri Opitz and Anette Frank. 2021. Towards a decom- posable metric for explainable evaluation of text gen- eration from AMR. In Proc. of EACL.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In ACL.</p>
<p>MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder, EMNLP. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Se- bastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In EMNLP.</p>
<p>chrf: character n-gram f-score for automatic mt evaluation. Maja Popović, Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationMaja Popović. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation.</p>
<p>Binyuan Bowen Qin, Lihan Hui, Min Wang, Jinyang Yang, Binhua Li, Ruiying Li, Rongyu Geng, Jian Cao, Luo Sun, Si, arXiv:2208.13629A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprintBowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, et al. 2022. A survey on text-to-sql parsing: Concepts, methods, and future directions. arXiv preprint arXiv:2208.13629.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, Proc. of EMNLP. of EMNLPLeonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural adapters in pretrained language models for AMR-to-Text generation. In Proc. of EMNLP.</p>
<p>Modeling relational data with graph convolutional networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, The Semantic Web -15th International Conference. Heraklion, Crete, GreeceProceedingsMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web -15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings.</p>
<p>An unsupervised joint system for text generation from knowledge graphs and semantic parsing. Martin Schmitt, Sahand Sharifzadeh, Hinrich Volker Tresp, Schütze, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsMartin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schütze. 2020. An unsupervised joint sys- tem for text generation from knowledge graphs and semantic parsing. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7117-7130, Online. As- sociation for Computational Linguistics.</p>
<p>Long and diverse text generation with planning-based hierarchical variational model. Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu, EMNLP-IJCNLP. Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019. Long and diverse text gen- eration with planning-based hierarchical variational model. In EMNLP-IJCNLP.</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, Proc. of NAACL. of NAACLPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proc. of NAACL.</p>
<p>G2t: Generating fluent descriptions for knowledge graph. Yunzhou Shi, Zhiling Luo, Pengcheng Zhu, Feng Ji, Wei Zhou, Haiqing Chen, Yujiu Yang, Proc. of SIGIR. of SIGIRYunzhou Shi, Zhiling Luo, Pengcheng Zhu, Feng Ji, Wei Zhou, Haiqing Chen, and Yujiu Yang. 2020. G2t: Generating fluent descriptions for knowledge graph. In Proc. of SIGIR.</p>
<p>A graph-to-sequence model for amrto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, ACL. Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr- to-text generation. In ACL.</p>
<p>Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing. Lihan Wang, Binyuan Bowen Qin, Bowen Hui, Min Li, Bailin Yang, Binhua Wang, Jian Li, Fei Sun, Luo Huang, Si, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningLihan Wang, Bowen Qin, Binyuan Hui, Bowen Li, Min Yang, Bailin Wang, Binhua Li, Jian Sun, Fei Huang, Luo Si, et al. 2022. Proton: Probing schema linking information from pre-trained language models for text-to-sql parsing. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1889-1898.</p>
<p>Amr-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, Transactions of the Association for Computational Linguistics. Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart M Shieber, Alexander M Rush, Proc. of EMNLP. of EMNLPSam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document genera- tion. In Proc. of EMNLP.</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.03771arXiv preprintThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of- the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Not all attention is needed: Gated attention network for sequence data. Lanqing Xue, Xiaopeng Li, Nevin L Zhang, Proc. of AAAI. of AAAILanqing Xue, Xiaopeng Li, and Nevin L Zhang. 2020. Not all attention is needed: Gated attention network for sequence data. In Proc. of AAAI.</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proc. of ICLR. of ICLRTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Eval- uating text generation with BERT. In Proc. of ICLR.</p>
<p>Lightweight, dynamic graph convolutional networks for amr-to-text generation. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, B Shay, Zuozhu Cohen, Lidong Liu, Bing, EMNLP. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B Cohen, Zuozhu Liu, and Lidong Bing. 2020b. Lightweight, dynamic graph convolutional networks for amr-to-text generation. In EMNLP.</p>
<p>Modeling graph structure in transformer for better amr-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, EMNLP-IJCNLP. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better amr-to-text genera- tion. In EMNLP-IJCNLP.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6863 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6863</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6863</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-279250177</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06777v6.pdf" target="_blank">MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding. However, their proficiency within the chemistry domain remains restricted, especially in solving molecule-related tasks. This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e. SMILES strings. In this study, we seek to enhance the ability of LLMs to comprehend molecules by equipping them with a multi-modal external module, termed MolX. Instead of directly using SMILES strings to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM. A hand-crafted molecular fingerprint is incorporated to leverage its embedded domain knowledge. To establish an alignment between MolX and the LLM’s textual input space, the model in which the LLM is frozen, is pre-trained with a strategy including a diverse set of tasks. Experimental evaluations show that our proposed method outperforms baselines across 4 downstream molecule-related tasks ranging from molecule-to-text translation to retrosynthesis, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters—0.53% and 0.82%, respectively.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6863.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6863.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolX multi-modal external module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-in multi-modal module that encodes molecules from SMILES, 2D molecular graphs, and Morgan fingerprints into embedding 'soft tokens' that are fed into a frozen base LLM to improve molecule-to-text translation and molecular property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolX-enhanced LLM (base: Llama-2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (frozen) with a trainable multi-modal external encoder / soft-token interface</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base LLM 7B parameters; MolX adds ~36.1M trainable params (0.53%) during pre-training and ~56.6M (0.82%) when including downstream LoRA adapters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-training uses a multi-task PubChem subset of ≈300k molecule → description pairs (main molecule-description objective) plus auxiliary labels (10 low-level computed properties and a SMILES canonicalization task). Encoders (ChemBERTa and ChemGraphCL) were pre-trained on large-scale unlabeled molecular corpora (as reported by their original papers) and reused here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>MolX produces a continuous embedding vector (a 'soft token') that is concatenated/attended to by the frozen autoregressive LLM; the LLM generates textual outputs auto‑regressively (e.g., molecule descriptions, IUPAC names). MolX is not used in this work to directly generate novel molecular graphs or SMILES strings (no de novo molecule sampling pipeline is described).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (encoded by ChemBERTa), 2D molecular graphs (encoded by ChemGraphCL GNN), and Morgan fingerprints (computed with RDKit, radius 2) combined in a weighted scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-to-text translation (molecule description generation, IUPAC name generation) and molecular property prediction (regression and classification on MoleculeNet subsets). Not used for targetted de novo molecule synthesis or experimental compound design in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Architectural/training constraints: base LLM frozen during MolX pre-training; MolX trained to align to LLM textual input space via multi-task instruction-based pre-training (main description task + auxiliary property prediction and SMILES canonicalization). During fine-tuning a parameter-efficient LoRA adapter is optionally applied (rank=8, alpha=32, dropout=0.1). No chemical-generation constraints (e.g., SA score, toxicity filters) are reported because MolX is used to produce text and property predictions rather than to generate candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to compute Morgan fingerprints; pre-trained ChemBERTa (SMILES encoder) and ChemGraphCL (GNN graph encoder) are integrated as trainable encoders inside MolX. LoRA is used for parameter-efficient fine-tuning of the base LLM. Training used standard deep learning toolchain (AdamW optimizer, BFloat16 on A100 GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pre-train: PubChem pre-train subset (~300k molecule–description pairs). Downstream: PubChem downstream subset for molecule→text (15k high-quality pairs); MoleculeNet subsets (ESOL, FreeSolv, Lipophilicity, HIV, BACE, BBBP, Tox21) for property prediction. Encoders reused pre-trained corpora (large unlabeled molecular sets per ChemBERTa/ChemGraphCL original works).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For molecule-to-text: BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, METEOR. For property prediction: RMSE for regression, Accuracy and F1 for classification (per MoleculeNet conventions).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Molecule→text (description generation) — Inference-only (frozen LLM) Llama-2-7B + MolX: BLEU-2=8.22, BLEU-4=6.40, ROUGE-1=30.82, ROUGE-2=21.69, ROUGE-L=28.94, METEOR=21.77. LoRA fine-tuning Llama-2-7B + MolX: BLEU-2=31.40, BLEU-4=24.25, ROUGE-1=44.20, ROUGE-2=28.96, ROUGE-L=38.76, METEOR=39.55 (these are the best-performing reported scores vs. baselines). MolX adds 36.1M params (0.53%) during pre-training and 56.6M params (0.82%) including downstream LoRA adapters. For molecular property prediction the paper reports that MolX achieves the best scores on 6/8 MoleculeNet subsets and second-best on the other 2 subsets (exact per-task RMSE/Accuracy/F1 numbers are reported in Table 2 of the paper but not reproduced in full text excerpts).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors report: LLMs hallucinate in chemistry and textual training data can be noisy/short (PubChem descriptions average ≈20 words); alignment between MolX and LLM via a single soft token is simple and may be suboptimal compared to advanced cross-space techniques (e.g., Q‑Former) which require more high-quality paired data and compute; reliance on pre-trained encoders is important (ablation shows performance drops without ChemBERTa/ChemGraphCL initializations); MolX is not designed or evaluated for de novo molecule generation or experimental synthesis; computational cost for pre-training (72 hours on 2×A100) and need for high-quality molecule→text data are noted as practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6863.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6863.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The decoder-only base large language model used as the frozen text generator in MolX experiments; it was evaluated both frozen (inference-only) and with LoRA fine-tuning for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining data for Llama-2 is not specified in detail in this paper; the model is used as an off-the-shelf base LLM and kept frozen during MolX pre-training (then optionally LoRA-fine-tuned on downstream tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive text generation; in MolX pipeline it consumes MolX-produced soft-token embeddings plus textual instructions and generates textual outputs (descriptions, IUPAC names, property answers). Not used here to autoregressively generate SMILES/novel molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>In baseline usage the model processes SMILES as plain text tokens; when enhanced with MolX it receives continuous molecular embeddings instead of raw SMILES sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-to-text translation and molecular property prediction in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>When used in MolX pre-training the LLM is frozen (no weight updates); during downstream experiments LoRA adapters are applied with rank=8, alpha=32, dropout=0.1 for parameter-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used together with MolX encoders (ChemBERTa, ChemGraphCL), RDKit fingerprints, and LoRA adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Downstream evaluation uses PubChem downstream subset (15k pairs) and MoleculeNet subsets; pre-training of MolX used PubChem pre-train (~300k pairs) while Llama-2 weights were not changed during that pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as MolX: BLEU/ROUGE/METEOR for translation; RMSE/Accuracy/F1 for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Baseline Llama-2-7B (inference-only) on description generation: BLEU-2=3.64, BLEU-4=2.98, ROUGE-1=18.28, ROUGE-2=4.26, ROUGE-L=12.87, METEOR=16.21. LoRA fine-tuned Llama-2-7B baseline: BLEU-2=27.54, BLEU-4=21.24, ROUGE-1=36.50, ROUGE-2=21.33, ROUGE-L=28.99, METEOR=31.69. The MolX-enhanced model substantially improves over these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Direct use of raw SMILES with LLM tokenizers leads to poor chemical understanding because of BPE tokenization fragmentation and LLMs lacking inherent understanding of SMILES semantics; frozen-LM pretraining forces MolX to align to LLM textual space but may limit joint optimization; LLMs exhibit hallucination in chemistry and perform poorly on molecule-centric tasks without multi-modal molecular encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6863.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6863.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBERTa (BERT-like SMILES encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained BERT-like SMILES encoder used inside MolX to extract a SMILES-derived embedding capturing long-range SMILES dependencies; outputs are averaged and projected to the LLM hidden dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>BERT-like encoder (masked language modeling pre-trained on SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper (described as a multi-layer Transformer encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on a large-scale set of unlabeled molecules via masked language modeling (MLM) according to the ChemBERTa original description; used in this work as a fixed/initializable SMILES encoder that is further fine-tuned as part of MolX.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to generate molecules; used to embed SMILES strings into continuous vectors for downstream LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Feature extraction to support LLM-based molecule description generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>SMILES are tokenized with the SMILES encoder's tokenizer (addresses BPE fragmentation issues of plain LLM tokenizers); encoder outputs are averaged to a single vector then projected.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Integrated inside MolX and projected into LLM hidden space via an MLP; used jointly with ChemGraphCL and Morgan fingerprints.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pre-trained on large unlabeled molecular corpora (as per prior ChemBERTa work); used here on PubChem molecules as inputs during MolX pre-training and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No separate metrics reported for ChemBERTa in this paper; its contribution assessed via ablation (removing chemical initialization harms performance).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Ablation: removing chemical initialization (i.e., random init instead of ChemBERTa/ChemGraphCL pre-trained weights) reduced performance in molecule description generation (LoRA setting): BLEU-2 dropped from 31.40 to 30.21 and other metrics decreased (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Authors note reliance on robust pre-trained encoders — performance degrades when encoders are randomly initialized; using SMILES encoder addresses some SMILES tokenization issues but does not by itself enable molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6863.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6863.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemGraphCL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemGraphCL (GNN-based graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained graph encoder based on message-passing GIN, trained with contrastive learning on unlabeled molecules; used inside MolX to extract graph/topological features from 2D molecular graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemGraphCL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GNN encoder (message-passing GIN, contrastive pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on large-scale unlabeled molecular graphs using graph contrastive learning (per original ChemGraphCL work); used here to provide 2D topological embeddings for MolX.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not used to generate molecules; used to produce graph embeddings incorporated into soft token fed to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>2D molecular graph (nodes = atoms, edges = bonds, node attributes included)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Feature extraction for molecule-to-text translation and property prediction when combined in MolX.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Node-level embeddings are averaged to create a molecule-level vector which is projected to LLM hidden space; combined (averaged) with ChemBERTa embedding and Morgan fingerprint embedding in a weighted sum.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Fed into MolX alongside ChemBERTa outputs and Morgan fingerprints; used as a baseline comparison (ChemGraphCL-alone) in property prediction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pre-trained on large unlabeled molecular graphs (per original ChemGraphCL); MolX uses MoleculeNet and PubChem downstream tasks for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not separately reported for ChemGraphCL in this paper beyond being a component whose inclusion improves downstream metrics and comparison baseline in MoleculeNet experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MolX (which includes ChemGraphCL as the graph encoder) achieves top or near-top performance across molecule-to-text and MoleculeNet tasks; authors compared MolX with standalone ChemGraphCL-based baselines and found MolX better on most MoleculeNet subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Graph encoder quality matters; removing pre-trained chemical initializations (i.e., not using ChemGraphCL pretraining) harms downstream performance. ChemGraphCL is a component for representation, not a molecular generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6863.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6863.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MorganFP (RDKit)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Morgan fingerprint (radius 2) computed with RDKit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A handcrafted circular fingerprint (Morgan/ECFP style) computed with RDKit and projected into the LLM hidden space; incorporated in a weighted scheme to supply substructure/domain knowledge to MolX.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Morgan fingerprint (RDKit implemented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>handcrafted fingerprint descriptor</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>bit vector (dimensionality not explicitly stated in paper excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Computed deterministically from molecular structures (no 'training data' per se); used as an additional input modality in MolX.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not applicable (descriptor).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Fixed-length fingerprint vector representing presence of local substructures (circular radius=2), computed with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Provides complementary local-substructure information to support text generation and property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Included via trainable projection and a learned weight in the final embedding (weighted combination with ChemBERTa and ChemGraphCL embeddings). Ablation removing MorganFP reduced performance noticeably.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Computed with RDKit and fed into MolX's MLP projection; combined with learned embeddings from other encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Computed for molecules in PubChem pre-train and downstream datasets and MoleculeNet tasks used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Contribution assessed via ablation (removing MorganFP decreased molecule description metrics; Table 3 shows BLEU-2 dropping from 31.40 to 29.33 in the LoRA setting when MorganFP removed).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Ablation: Llama-2-7B + MolX w/o MorganFP (LoRA) reported BLEU-2=29.33 vs full MolX BLEU-2=31.40 on PubChem description generation (other metrics also decreased).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Handcrafted fingerprints encode local substructures but are limited to the chosen radius and may miss other structural/contextual information; authors found they nonetheless provided significant benefit when combined with learned encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MoMu <em>(Rating: 2)</em></li>
                <li>2D and 3D MoLM <em>(Rating: 2)</em></li>
                <li>A molecular multimodal foundation model associating molecule graphs with natural language. <em>(Rating: 2)</em></li>
                <li>Towards 3D Molecule-Text Interpretation in Language Models. <em>(Rating: 2)</em></li>
                <li>LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6863",
    "paper_id": "paper-279250177",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "MolX",
            "name_full": "MolX multi-modal external module",
            "brief_description": "A plug-in multi-modal module that encodes molecules from SMILES, 2D molecular graphs, and Morgan fingerprints into embedding 'soft tokens' that are fed into a frozen base LLM to improve molecule-to-text translation and molecular property prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MolX-enhanced LLM (base: Llama-2-7B)",
            "model_type": "decoder-only LLM (frozen) with a trainable multi-modal external encoder / soft-token interface",
            "model_size": "base LLM 7B parameters; MolX adds ~36.1M trainable params (0.53%) during pre-training and ~56.6M (0.82%) when including downstream LoRA adapters",
            "training_data_description": "Pre-training uses a multi-task PubChem subset of ≈300k molecule → description pairs (main molecule-description objective) plus auxiliary labels (10 low-level computed properties and a SMILES canonicalization task). Encoders (ChemBERTa and ChemGraphCL) were pre-trained on large-scale unlabeled molecular corpora (as reported by their original papers) and reused here.",
            "generation_method": "MolX produces a continuous embedding vector (a 'soft token') that is concatenated/attended to by the frozen autoregressive LLM; the LLM generates textual outputs auto‑regressively (e.g., molecule descriptions, IUPAC names). MolX is not used in this work to directly generate novel molecular graphs or SMILES strings (no de novo molecule sampling pipeline is described).",
            "chemical_representation": "SMILES strings (encoded by ChemBERTa), 2D molecular graphs (encoded by ChemGraphCL GNN), and Morgan fingerprints (computed with RDKit, radius 2) combined in a weighted scheme.",
            "target_application": "Molecule-to-text translation (molecule description generation, IUPAC name generation) and molecular property prediction (regression and classification on MoleculeNet subsets). Not used for targetted de novo molecule synthesis or experimental compound design in this work.",
            "constraints_used": "Architectural/training constraints: base LLM frozen during MolX pre-training; MolX trained to align to LLM textual input space via multi-task instruction-based pre-training (main description task + auxiliary property prediction and SMILES canonicalization). During fine-tuning a parameter-efficient LoRA adapter is optionally applied (rank=8, alpha=32, dropout=0.1). No chemical-generation constraints (e.g., SA score, toxicity filters) are reported because MolX is used to produce text and property predictions rather than to generate candidate molecules.",
            "integration_with_external_tools": "RDKit used to compute Morgan fingerprints; pre-trained ChemBERTa (SMILES encoder) and ChemGraphCL (GNN graph encoder) are integrated as trainable encoders inside MolX. LoRA is used for parameter-efficient fine-tuning of the base LLM. Training used standard deep learning toolchain (AdamW optimizer, BFloat16 on A100 GPUs).",
            "dataset_used": "Pre-train: PubChem pre-train subset (~300k molecule–description pairs). Downstream: PubChem downstream subset for molecule→text (15k high-quality pairs); MoleculeNet subsets (ESOL, FreeSolv, Lipophilicity, HIV, BACE, BBBP, Tox21) for property prediction. Encoders reused pre-trained corpora (large unlabeled molecular sets per ChemBERTa/ChemGraphCL original works).",
            "evaluation_metrics": "For molecule-to-text: BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, METEOR. For property prediction: RMSE for regression, Accuracy and F1 for classification (per MoleculeNet conventions).",
            "reported_results": "Molecule→text (description generation) — Inference-only (frozen LLM) Llama-2-7B + MolX: BLEU-2=8.22, BLEU-4=6.40, ROUGE-1=30.82, ROUGE-2=21.69, ROUGE-L=28.94, METEOR=21.77. LoRA fine-tuning Llama-2-7B + MolX: BLEU-2=31.40, BLEU-4=24.25, ROUGE-1=44.20, ROUGE-2=28.96, ROUGE-L=38.76, METEOR=39.55 (these are the best-performing reported scores vs. baselines). MolX adds 36.1M params (0.53%) during pre-training and 56.6M params (0.82%) including downstream LoRA adapters. For molecular property prediction the paper reports that MolX achieves the best scores on 6/8 MoleculeNet subsets and second-best on the other 2 subsets (exact per-task RMSE/Accuracy/F1 numbers are reported in Table 2 of the paper but not reproduced in full text excerpts).",
            "experimental_validation": false,
            "challenges_or_limitations": "Authors report: LLMs hallucinate in chemistry and textual training data can be noisy/short (PubChem descriptions average ≈20 words); alignment between MolX and LLM via a single soft token is simple and may be suboptimal compared to advanced cross-space techniques (e.g., Q‑Former) which require more high-quality paired data and compute; reliance on pre-trained encoders is important (ablation shows performance drops without ChemBERTa/ChemGraphCL initializations); MolX is not designed or evaluated for de novo molecule generation or experimental synthesis; computational cost for pre-training (72 hours on 2×A100) and need for high-quality molecule→text data are noted as practical limitations.",
            "uuid": "e6863.0",
            "source_info": {
                "paper_title": "MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama-2 (7B)",
            "brief_description": "The decoder-only base large language model used as the frozen text generator in MolX experiments; it was evaluated both frozen (inference-only) and with LoRA fine-tuning for downstream tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_type": "decoder-only LLM (autoregressive)",
            "model_size": "7B parameters",
            "training_data_description": "Pretraining data for Llama-2 is not specified in detail in this paper; the model is used as an off-the-shelf base LLM and kept frozen during MolX pre-training (then optionally LoRA-fine-tuned on downstream tasks).",
            "generation_method": "Autoregressive text generation; in MolX pipeline it consumes MolX-produced soft-token embeddings plus textual instructions and generates textual outputs (descriptions, IUPAC names, property answers). Not used here to autoregressively generate SMILES/novel molecules.",
            "chemical_representation": "In baseline usage the model processes SMILES as plain text tokens; when enhanced with MolX it receives continuous molecular embeddings instead of raw SMILES sequences.",
            "target_application": "Molecule-to-text translation and molecular property prediction in this work.",
            "constraints_used": "When used in MolX pre-training the LLM is frozen (no weight updates); during downstream experiments LoRA adapters are applied with rank=8, alpha=32, dropout=0.1 for parameter-efficient fine-tuning.",
            "integration_with_external_tools": "Used together with MolX encoders (ChemBERTa, ChemGraphCL), RDKit fingerprints, and LoRA adapters.",
            "dataset_used": "Downstream evaluation uses PubChem downstream subset (15k pairs) and MoleculeNet subsets; pre-training of MolX used PubChem pre-train (~300k pairs) while Llama-2 weights were not changed during that pre-training.",
            "evaluation_metrics": "Same as MolX: BLEU/ROUGE/METEOR for translation; RMSE/Accuracy/F1 for property prediction.",
            "reported_results": "Baseline Llama-2-7B (inference-only) on description generation: BLEU-2=3.64, BLEU-4=2.98, ROUGE-1=18.28, ROUGE-2=4.26, ROUGE-L=12.87, METEOR=16.21. LoRA fine-tuned Llama-2-7B baseline: BLEU-2=27.54, BLEU-4=21.24, ROUGE-1=36.50, ROUGE-2=21.33, ROUGE-L=28.99, METEOR=31.69. The MolX-enhanced model substantially improves over these baselines.",
            "experimental_validation": false,
            "challenges_or_limitations": "Direct use of raw SMILES with LLM tokenizers leads to poor chemical understanding because of BPE tokenization fragmentation and LLMs lacking inherent understanding of SMILES semantics; frozen-LM pretraining forces MolX to align to LLM textual space but may limit joint optimization; LLMs exhibit hallucination in chemistry and perform poorly on molecule-centric tasks without multi-modal molecular encoders.",
            "uuid": "e6863.1",
            "source_info": {
                "paper_title": "MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemBERTa",
            "name_full": "ChemBERTa (BERT-like SMILES encoder)",
            "brief_description": "A pre-trained BERT-like SMILES encoder used inside MolX to extract a SMILES-derived embedding capturing long-range SMILES dependencies; outputs are averaged and projected to the LLM hidden dimension.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChemBERTa",
            "model_type": "BERT-like encoder (masked language modeling pre-trained on SMILES)",
            "model_size": "not specified in this paper (described as a multi-layer Transformer encoder)",
            "training_data_description": "Pre-trained on a large-scale set of unlabeled molecules via masked language modeling (MLM) according to the ChemBERTa original description; used in this work as a fixed/initializable SMILES encoder that is further fine-tuned as part of MolX.",
            "generation_method": "Not used to generate molecules; used to embed SMILES strings into continuous vectors for downstream LLM consumption.",
            "chemical_representation": "SMILES",
            "target_application": "Feature extraction to support LLM-based molecule description generation and property prediction.",
            "constraints_used": "SMILES are tokenized with the SMILES encoder's tokenizer (addresses BPE fragmentation issues of plain LLM tokenizers); encoder outputs are averaged to a single vector then projected.",
            "integration_with_external_tools": "Integrated inside MolX and projected into LLM hidden space via an MLP; used jointly with ChemGraphCL and Morgan fingerprints.",
            "dataset_used": "Pre-trained on large unlabeled molecular corpora (as per prior ChemBERTa work); used here on PubChem molecules as inputs during MolX pre-training and downstream tasks.",
            "evaluation_metrics": "No separate metrics reported for ChemBERTa in this paper; its contribution assessed via ablation (removing chemical initialization harms performance).",
            "reported_results": "Ablation: removing chemical initialization (i.e., random init instead of ChemBERTa/ChemGraphCL pre-trained weights) reduced performance in molecule description generation (LoRA setting): BLEU-2 dropped from 31.40 to 30.21 and other metrics decreased (see Table 3).",
            "experimental_validation": false,
            "challenges_or_limitations": "Authors note reliance on robust pre-trained encoders — performance degrades when encoders are randomly initialized; using SMILES encoder addresses some SMILES tokenization issues but does not by itself enable molecule generation.",
            "uuid": "e6863.2",
            "source_info": {
                "paper_title": "MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemGraphCL",
            "name_full": "ChemGraphCL (GNN-based graph encoder)",
            "brief_description": "A pre-trained graph encoder based on message-passing GIN, trained with contrastive learning on unlabeled molecules; used inside MolX to extract graph/topological features from 2D molecular graphs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChemGraphCL",
            "model_type": "GNN encoder (message-passing GIN, contrastive pre-training)",
            "model_size": "not specified in this paper",
            "training_data_description": "Pre-trained on large-scale unlabeled molecular graphs using graph contrastive learning (per original ChemGraphCL work); used here to provide 2D topological embeddings for MolX.",
            "generation_method": "Not used to generate molecules; used to produce graph embeddings incorporated into soft token fed to the LLM.",
            "chemical_representation": "2D molecular graph (nodes = atoms, edges = bonds, node attributes included)",
            "target_application": "Feature extraction for molecule-to-text translation and property prediction when combined in MolX.",
            "constraints_used": "Node-level embeddings are averaged to create a molecule-level vector which is projected to LLM hidden space; combined (averaged) with ChemBERTa embedding and Morgan fingerprint embedding in a weighted sum.",
            "integration_with_external_tools": "Fed into MolX alongside ChemBERTa outputs and Morgan fingerprints; used as a baseline comparison (ChemGraphCL-alone) in property prediction experiments.",
            "dataset_used": "Pre-trained on large unlabeled molecular graphs (per original ChemGraphCL); MolX uses MoleculeNet and PubChem downstream tasks for evaluation.",
            "evaluation_metrics": "Not separately reported for ChemGraphCL in this paper beyond being a component whose inclusion improves downstream metrics and comparison baseline in MoleculeNet experiments.",
            "reported_results": "MolX (which includes ChemGraphCL as the graph encoder) achieves top or near-top performance across molecule-to-text and MoleculeNet tasks; authors compared MolX with standalone ChemGraphCL-based baselines and found MolX better on most MoleculeNet subsets.",
            "experimental_validation": false,
            "challenges_or_limitations": "Graph encoder quality matters; removing pre-trained chemical initializations (i.e., not using ChemGraphCL pretraining) harms downstream performance. ChemGraphCL is a component for representation, not a molecular generator.",
            "uuid": "e6863.3",
            "source_info": {
                "paper_title": "MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MorganFP (RDKit)",
            "name_full": "Morgan fingerprint (radius 2) computed with RDKit",
            "brief_description": "A handcrafted circular fingerprint (Morgan/ECFP style) computed with RDKit and projected into the LLM hidden space; incorporated in a weighted scheme to supply substructure/domain knowledge to MolX.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Morgan fingerprint (RDKit implemented)",
            "model_type": "handcrafted fingerprint descriptor",
            "model_size": "bit vector (dimensionality not explicitly stated in paper excerpt)",
            "training_data_description": "Computed deterministically from molecular structures (no 'training data' per se); used as an additional input modality in MolX.",
            "generation_method": "Not applicable (descriptor).",
            "chemical_representation": "Fixed-length fingerprint vector representing presence of local substructures (circular radius=2), computed with RDKit.",
            "target_application": "Provides complementary local-substructure information to support text generation and property prediction.",
            "constraints_used": "Included via trainable projection and a learned weight in the final embedding (weighted combination with ChemBERTa and ChemGraphCL embeddings). Ablation removing MorganFP reduced performance noticeably.",
            "integration_with_external_tools": "Computed with RDKit and fed into MolX's MLP projection; combined with learned embeddings from other encoders.",
            "dataset_used": "Computed for molecules in PubChem pre-train and downstream datasets and MoleculeNet tasks used in experiments.",
            "evaluation_metrics": "Contribution assessed via ablation (removing MorganFP decreased molecule description metrics; Table 3 shows BLEU-2 dropping from 31.40 to 29.33 in the LoRA setting when MorganFP removed).",
            "reported_results": "Ablation: Llama-2-7B + MolX w/o MorganFP (LoRA) reported BLEU-2=29.33 vs full MolX BLEU-2=31.40 on PubChem description generation (other metrics also decreased).",
            "experimental_validation": false,
            "challenges_or_limitations": "Handcrafted fingerprints encode local substructures but are limited to the chosen radius and may miss other structural/contextual information; authors found they nonetheless provided significant benefit when combined with learned encoders.",
            "uuid": "e6863.4",
            "source_info": {
                "paper_title": "MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MoMu",
            "rating": 2
        },
        {
            "paper_title": "2D and 3D MoLM",
            "rating": 2,
            "sanitized_title": "2d_and_3d_molm"
        },
        {
            "paper_title": "A molecular multimodal foundation model associating molecule graphs with natural language.",
            "rating": 2,
            "sanitized_title": "a_molecular_multimodal_foundation_model_associating_molecule_graphs_with_natural_language"
        },
        {
            "paper_title": "Towards 3D Molecule-Text Interpretation in Language Models.",
            "rating": 2,
            "sanitized_title": "towards_3d_moleculetext_interpretation_in_language_models"
        },
        {
            "paper_title": "LlaSMol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset",
            "rating": 2,
            "sanitized_title": "llasmol_advancing_large_language_models_for_chemistry_with_a_largescale_comprehensive_highquality_instruction_tuning_dataset"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks.",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        }
    ],
    "cost": 0.016762,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension
22 Sep 2025</p>
<p>Khiem Le 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Zhichun Guo 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Kaiwen Dong 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Xiaobao Huang 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Bozhao Nan 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Roshni Iyer 
University of California
Los AngelesCAUSA</p>
<p>University of California
Los AngelesCAUSA</p>
<p>Xiangliang Zhang 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Olaf Wiest 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Wei Wang 
University of California
Los AngelesCAUSA</p>
<p>University of California
Los AngelesCAUSA</p>
<p>Ting Hua 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>Nitesh V Chawla 
University of Notre Dame
INUSA</p>
<p>University of Notre Dame
INUSA</p>
<p>2025 Molx </p>
<p>9 pagesTorontoONCanada</p>
<p>MLoG-GenAI@KDD '25
TorontoONCanada</p>
<p>Khiem et al</p>
<p>MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension
22 Sep 2025FA5638CE7F7950C3404909512345EA1C10.1145/nnnnnnn.nnnnnnnarXiv:2406.06777v9[cs.CV]Large Language ModelsMulti-Modal LearningMolecular UnderstandingMolecule-Related Tasks
Large Language Models (LLMs) with their strong task-handling capabilities have shown remarkable advancements across a spectrum of fields, moving beyond natural language understanding.However, their proficiency within the chemistry domain remains restricted, especially in solving molecule-related tasks.This challenge is attributed to their inherent limitations in comprehending molecules using only common textual representations, i.e.SMILES strings.In this study, we seek to enhance the ability of LLMs to comprehend molecules by equipping them with a multi-modal external module, termed MolX.Instead of directly using SMILES strings to represent a molecule, we utilize specific encoders to extract fine-grained features from both SMILES string and 2D molecular graph representations for feeding into an LLM.A hand-crafted molecular fingerprint is incorporated to leverage its embedded domain knowledge.To establish an alignment between MolX and the LLM's textual input space, the model in which the LLM is frozen, is pre-trained with a strategy including a diverse set of tasks.Experimental evaluations show that our proposed method outperforms baselines across downstream molecule-related tasks ranging from molecule-to-text translation to molecular property prediction, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters-0.53%and 0.82%, respectively.CCS Concepts• Computing methodologies → Learning paradigms; • Applied computing → Bioinformatics.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated impressive performances across a wide array of fields.Extending beyond the boundaries of natural language understanding, LLMs have facilitated various scientific disciplines [33,34].LLMs have recently been investigated for augmenting research in chemistry as an alternative approach to the traditional supervised learning approach [1,6].</p>
<p>Despite their strong task-handling capabilities, LLMs still struggle with the chemistry domain, as evidenced by their limited performances on various professional molecule-related tasks [12,46].Llama-2 [35], performs unsatisfactorily on the molecule-to-text translation tasks such as molecule description generation and IU-PAC name generation, providing the correct answer only half as often as supervised learning models.Additionally, such LLM fails to predict molecular properties even using expert-designed prompts.One potential cause of this challenge has been figured out that most existing LLMs represent molecules only by their common textual representations, i.e., SMILES strings [40], and process them in a paradigm similar to texts [12,20], as illustrated in Figure 1a.While convenient, several issues make it challenging for LLMs to comprehend molecules solely by interpreting SMILES strings.Firstly, LLMs lack an inherent understanding of SMILES strings and blindly treat them as sequences of separate characters relying on their byte-pair encoding tokenizers [31], which break SMILES strings into smaller pieces in ways that do not represent the chemical principles behind these strings.Without an understanding of these principles, it is difficult for LLMs to capture molecular structure from SMILES strings due to inaccuracies such as incorrect transcription of complex aromatic systems or the absence of hydrogens and other atoms [36], as shown in Figure 1b and Figure 1c.</p>
<p>There have been some early attempts to enhance LLMs for solving molecule-related tasks.Su et al. [32] employed a GNN-based graph encoder to extract features from the molecule's 2D molecular graph and directly input such features into the LLM to perform molecule-to-text translation tasks.Developed from that idea, Li et al. [20] input features extracted from the 2D or 3D molecular graph into the LLM through an intermediate projector, which is previously aligned with the LLM's textual input space by a pretraining stage.Although bridging the gap between the 2D or 3D molecular graph and the LLMs, previous approaches are ineffective in using the information contained in a SMILES string, as well as handcrafted molecular descriptors, which have advantages over 2D or 3D molecular graph [8,17].This might lead to suboptimal performances.Existing methods are only optimized for a limited  In this study, we introduce a novel framework for enhancing LLMs to capture molecules from multiple representations, thus improving their performances on various molecule-related tasks.Our proposed framework consists of two main components which are a multi-modal external module, namely MolX, equipped with the LLMs, and a versatile pre-training strategy for aligning MolX into the LLMs' textual input space.We first utilize a pre-trained BERTlike [9] SMILES encoder to extract features from the SMILES string instead of directly using it to represent the molecule.Because of its initial pre-training, the SMILES encoder works with its tokenizer to capture long-range dependencies encoded in the SMILES string.We simultaneously utilize a pre-trained GNN-based graph encoder to extract features from the molecule's 2D molecular graph, capturing its topological structures.In addition to features extracted from raw representations, i.e., SMILES string and 2D molecular graph, a handcrafted molecular fingerprint [24] containing domain knowledge is incorporated in a weighted scheme of MolX.Finally, the modelin which the LLM is frozen undergoes pre-training strategy with a diverse set of tasks, providing the model with information about the molecules.This process provides an alignment between MolX and the LLM's textual input space.Figure 2 shows an overview of our proposed method.</p>
<p>Our experimental results demonstrate that the proposed method outperforms baselines by a statistically significant margin on various downstream molecule-related tasks in two different model configurations, with and without fine-tuning the LLM.It is worth noting that MolX can act as a plug-in module to the LLMs for enhancing the performances on molecule-related tasks while fully preserving its general-purpose usage in other domains.</p>
<p>To summarize, our contributions are outlined as follows:</p>
<p>• We introduce a novel framework enhancing LLMs to comprehend molecules, thus improving their performances on various molecule-related tasks.The LLMs are equipped with a multi-modal external module, MolX, to extract features from both SMILES string and 2D molecular graph representations, as well as leverage a handcrafted molecular fingerprint.• A pre-training strategy including a diverse set of tasks, is applied to establish an alignment between MolX and the LLMs' textual input space.This process advances the models' ability of molecular understanding, as well as instruction following.• Extensive experimental evaluations demonstrate that our proposed method outperforms baselines by a substantial margin on a diverse range of downstream molecule-related tasks in two different model configurations, with and without fine-tuning the LLM.</p>
<p>Related Work</p>
<p>In this section, we provide a review of the literature related to molecular learning via language modeling and leveraging LLMs for solving molecule-related tasks.</p>
<p>Molecular Learning</p>
<p>Molecules form the basis of chemistry and molecular learning has been a long-standing problem in cheminformatics [3,26,27,43].</p>
<p>Traditionally, molecular fingerprints such as Morgan fingerprint [24] or ECFP [29] serve as one of the most important descriptors for molecules, encoding a molecule into a fixed bit string, where each bit indicates the presence of a certain substructure.With the rapid development of language modeling, textual representations such as SMILES strings have become widely used [40].Studying molecular property prediction tasks, Wang et al. [38] introduced SMILES-BERT, a BERT-like model [9] that is pre-trained with the masked language modeling mechanism (MLM) on a large-scale set of unlabeled molecules.Wang et al. [37] proposed using chemical reactions to assist the pre-training.Ahmad et al. [2] proposed using auxiliary tasks with more domain relevance for chemistry such as predicting computed properties of molecules, supporting MLM.Irwin et al. [15] investigated the challenging sequence-tosequence tasks such as retrosynthesis and introduced Chemformer.Zhong et al. [48] proposed the root-aligned SMILES (R-SMILES), adopting a tighter representation for those tasks.Edwards et al. [11] studied molecule-to-text translation tasks and vice versa and proposed MolT5, which is pre-trained with the multi-lingual MLM, considering SMILES strings as a conventional language.Lu and Zhang [23] and Christofidellis et al. [7] presented ChemT5 and Text+ChemT5, unifying all sequence-to-sequence tasks.Several studies [21] demonstrated that fusing the molecule's 2D molecular graphs with language modeling provides complementary benefits to molecular learning, improving performance on tasks such as molecular property prediction.With rising use across a wide array of fields, including chemistry [1,6], LLMs have emerged as an evolution of the traditional language modeling approach for molecular learning.</p>
<p>LLMs for Molecule-Related Tasks</p>
<p>Several studies have evaluated LLM applications in chemistry.Castro Nascimento and Pimentel [6] explored how well ChatGPT "understands" chemistry by posing five student-level tasks in different subareas of chemistry and noticed moderate performance.Zhao et al. [46] investigated the molecular property prediction task and showed that LLMs relied on memorized information rather than true understanding for making predictions, which limits their applications to new types of molecules required in practical applications.Guo et al. [12] benchmarked several published LLMs on eight molecule-related tasks.Empirical results reveal that LLMs such as Llama-2 [35] that were widely used at the time typically fail to perform challenging tasks of molecule-to-text translation or predict molecule activity for high-level properties even using expert-designed prompts.A potential reason behind this challenge has been identified that most existing LLMs represent molecules only by their common textual representations, i.e., SMILES strings, which LLMs have a limited understanding of.In response to these findings, Su et al. [32] propose MoMu to enhance LLMs by applying a GNN-based graph encoder to extract features from the molecule's 2D molecular graph and input such features into the LLM for performing molecule-to-text translation tasks.Li et al. [20] proposed 2D and 3D MoLM to leverage an intermediate projector for feeding features extracted from the 2D or 3D molecular graph into the LLM, which is previously aligned with the LLM's textual input space by a pre-training stage.Despite improvements by bridging the gap between the 2D or 3D molecular graph and the LLMs, the importance of representation other than SMILES strings such as handcrafted molecular descriptors are underexplored.Existing methods are only optimized for a limited set of molecule-related tasks, how well the enhanced LLMs perform on other tasks such as molecular property prediction is not well understood.</p>
<p>Methodology</p>
<p>We propose a framework enhancing LLMs to comprehend molecules from multiple representations, consisting of two main components, a multi-modal external module and a novel pre-training strategy.Here, we present the details of these components.</p>
<p>Model Architecture</p>
<p>The proposed MolX, which is equipped with a base LLM, consists of two key designs: 1) Trainable encoders, focusing on encoding raw representations of a molecule, i.e., SMILES string and 2D molecular graph; 2) A weighted scheme to incorporate a handcrafted molecular fingerprint.</p>
<p>Trainable Encoders.We define a molecule as  and consider   and   to depict its SMILES string and 2D molecular graph, respectively.While   is simply a sequence of ASCII characters,   is considered as   = {V, E}, where each node in V indicates an atom and each edge in E indicates a chemical bond.Also,  ∈ R | V | × is the attribute matrix of   where   =  [, :]  is the  -dimensional attribute vector of the node   ∈ V.</p>
<p>To encode the SMILES string   , we adopt a pre-trained BERTlike [9] SMILES encoder, ChemBERTa [2], which is constructed by stacking multiple Transformer layers.ChemBERTa, denoted as   , is pre-trained on a large-scale set of unlabeled molecules with MLM, enabling it to capture long-range dependencies identified in the SMILES string.An average is taken over outputs of   to obtain an embedding vector for   , which is then projected to the hidden dimension  of the base LLM by a multi-layer perceptron   :
𝑒 𝑆 = 𝑓 𝑆 (Average({𝑡 𝑖 , 𝑡 𝑖 ∈ 𝐸 𝑆 (𝑚 𝑆 )})) ∈ R 𝑑 .(1)
To encode the 2D molecular graph   , we adopt a pre-trained GNN-based graph encoder, ChemGraphCL [44], which is constructed based on an emerging message-passing GNN, GIN [13].Chem-GraphCL, denoted as   , is pre-trained on a large-scale set of unlabeled molecules with a contrastive learning strategy [28] and thus able to capture the topological structures of the molecule from its 2D molecular graph.Starting from the initial   , after multiple layers of message propagation,   produces an updated attribute vector ℎ  for the node   ∈ V.Then, an average is taken over all node-level attribute vectors to obtain an embedding vector for   , which is projected to the hidden dimension  of the base LLM by a multi-layer perceptron   :
𝑒 𝐺 = 𝑓 𝐺 (Average({ℎ 𝑛 , ℎ 𝑛 ∈ 𝐸 𝐺 (𝑚 𝐺 )})) ∈ R 𝑑 .(2)
  and   are then averaged to establish a unified embedding vector  ∈ R  .</p>
<p>Molecular Fingerprint Incorporation.Molecular fingerprints are some of the most important descriptors of molecules due to the encoded domain knowledge.While SMILES strings and 2D molecular graphs capture global information about the molecule, molecular fingerprints capture information about the local atomic environments and neighborhoods, explicitly encoding the presence of specific substructures [10].Unfortunately, molecular fingerprints are not often used in deep learning models even though they have been shown to be valuable for specific tasks such as molecular property prediction [42].We seek to exploit their benefits by incorporating the popular Morgan fingerprint [24] into the unified embedding vector  from trainable encoders described above.RDKit [18] is utilized to compute the Morgan fingerprints with a radius of 2 from the molecule , which is then projected to the hidden dimension  of the base LLM by a multi-layer perceptron   .The incorporation scheme works as follows:
𝑒 = 𝑤 𝑒 • 𝑒 + 𝑤 𝑒 𝐹 • 𝑒 𝐹 ,
where
𝑒 𝐹 = 𝑓 𝐹 (MorganFP(𝑚)),(3)
where   and    are trainable parameters introduced for providing the model sufficient flexibility to incorporate the Morgan fingerprint into .</p>
<p>Pre-training Strategy</p>
<p>There is a noticeable misalignment in the latent spaces of MolX and the base LLM where the former encodes molecules while the latter has a textual input space.Therefore a cross-space alignment stage is needed.This is accomplished by feeding the embedding vector</p>
<p>Predicting the basic chemical and physical properties</p>
<p>A heavy atom refers to any atom that is not hydrogen.from MolX into the LLM as a soft token.We propose to pre-train the MolX-enhanced LLM with a diverse set of tasks including a molecule-to-text translation task, i.e., molecule description generation, accompanied by several auxiliary tasks.It is worth noting that while MolX is trainable, the base LLM is kept frozen during pre-training.This setting maintains the LLM's inherent generalizability, forcing MolX to produce embedding vectors that are suited in the LLM's textual input space and can be effectively understood by the LLM to generate accurate answers.This allows the LLM to function normally on general domains by using MolX as a plug-in module for the handling of molecule-related tasks.</p>
<p>Multi-Task Dataset.To conduct the pre-training, we utilize the pre-train subset of PubChem [20], a dataset that contains 300k molecule-description pairs 1 for the molecule description generation task.By using this task as an objective, MolX is encouraged to produce meaningful embedding vectors, so that the LLM can caption molecules with their substructures and properties accurately, as illustrated in Figure 2.Although this dataset collected from a reliable source, descriptions in the dataset retain several limitations that might hinder the model's ability of molecular understanding.The average number of words in the dataset's descriptions is roughly 20, which is insufficient to describe a molecule.Additionally, some of the dataset's descriptions are noisy and uninformative [20].Therefore, to assist the molecule description task, we design a set of auxiliary tasks including predicting the basic chemical and physical properties of molecules such as the number of heavy atoms or molecular weight.We select a set of 10 low-level properties that are available for easy collection from PubChem and present comprehensive information about the molecules.Further, leveraging the fact that a molecule can be represented by multiple valid SMILES strings [4], we utilize one more special auxiliary task which is canonicalizing the molecule's SMILES string.This task enhances the model's understanding of chemical laws behind SMILES strings.To keep the pre-training stage controllable, 10% of the dataset is used for each auxiliary task.Examples of proposed auxiliary tasks are shown in Figure 3 and details are in Appendix A.</p>
<p>Instruction-based Pre-training.LLMs tend to exhibit hallucinations in the domain of chemistry [12], generating unexpected answers regarding a molecule.Hence, we enrich our pre-training dataset by designing an informative instruction for each task.We Given a molecule  and its label  for the given task, after the embedding vector  is extracted from MolX, the auto-regressive loss for pre-training is defined as:
L 𝑟𝑒𝑔 = −log 𝑝 (𝑦|𝐹 𝑎𝑡𝑡 (𝑧 1 , 𝑧 2 , .., 𝑧 𝑇 , 𝑒)) = − 𝐿 ∑︁ 𝑙=1 log 𝑝 (𝑦 𝑙 |𝐹 𝑎𝑡𝑡 (𝑧 1 , 𝑧 2 , .., 𝑧 𝑇 , 𝑒), 𝑦 1 , ..., 𝑦 𝑙 −1 ),(4)
where  is the length of the label  for the given task.</p>
<p>Experiments</p>
<p>In this section, we conduct experiments on various downstream molecule-related tasks including molecule-to-text translation, molecular property prediction, to demonstrate the effectiveness of our proposed method.Throughout experiments, we utilize Llama-2 [35] with 7B parameters as our base LLM to leverage its text generation capability and internal chemistry knowledge.We consider two different model configurations for the evaluation: I) Inference-only:</p>
<p>The model is frozen after pre-training for direct question answering on downstream tasks, evaluating the model's generalizability without fine-tuning; II) LoRA fine-tuning: The model is fine-tuned on downstream tasks using a parameter-efficient technique, LoRA [14], verifying the model's adaptability in scenarios where downstream data are available.In addition to direct comparison with previous related works including MoMu [32], as well as 2D and 3D MoLM [20], we also compare with competitive supervised learning models in each task.For further reference, we evaluate two recently introduced generalist chemical LLMs derived from Llama-2 [35] that are tailored for molecule-related tasks, i.e., LlaSMol-7B [45] and ChemDFM-13B [47].The MolX-enhanced LLM is pre-trained with the above tasks in a multi-task learning setting for 5 epochs.AdamW optimizer [22] is adopted with a weight decay of 0.05 and a learning rate scheduler of a combination of linear warmup with 1000 steps and cosine decay, in which the peak and minimal learning rates are 1e-5 and 5e-6, respectively.The batch size is 12 and the maximal text length is set to be 256.The computation time is 72 hours on 2 A100 GPUs with BFloat16 mixed precision.For experiments on downstream tasks, we consider two different model configurations for the evaluation: I) Inference-only: The model is frozen after pre-training for direct question answering on downstream tasks, evaluating the model's generalizability without fine-tuning; II) LoRA fine-tuning: The model is fine-tuned on downstream tasks using a parameterefficient technique, LoRA [14], verifying the model's adaptability in scenarios where downstream data are available.For LoRA finetuning, the model is fine-tuned on train sets of downstream tasks for 50 epochs, using the same settings of optimizer and learning rate scheduler as pre-training.LoRA is applied with the same hyperparameters as the baselines 2D and 3D MoLM [20], factorizing all * _  modules of LlamaSdpaAttention and LlamaMLP layers with a rank  = 8,  = 32, and  = 0.1.Notably, for all tasks, the loss function employed is the auto-regressive loss as described in Equation ( 4).We report performances on the test sets selected by the corresponding validation sets.</p>
<p>Molecule-to-Text Translation</p>
<p>We first consider the molecule-to-text translation tasks, i.e., molecule description generation and IUPAC name generation.These tasks reflect the general molecular understanding of the model and have crucial applications, enabling humans to gain an overview of a molecule.We conduct experiments on the downstream subset of the PubChem dataset [20], which has 15k high-quality moleculedescription pairs and is separate from the pre-train one.We opt not to use the CheBI-20 dataset [11] because it is also sourced from PubChem and can be viewed as an older version of the used dataset.Following [11,20], we adopt BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR as evaluation metrics.</p>
<p>Table 1 presents experimental results for these tasks across 6 different metrics.Based on the Inference-only results, we observe that the proposed framework significantly enhances the base LLM for direct question answering on both tasks without fine-tuning.In the scenario of LoRA fine-tuning, the MolX-enhanced LLM demonstrates superior performance compared to baselines with the highest scores on all metrics, especially for ROUGE-based and METEOR metrics which might be attributed to the proposed versatile pretraining strategy that provides the model with comprehensive information about the molecules.The approach of fine-tuning the LLM to establish multi-modal models shows better performances than generalist chemical LLMs, i.e., LlaSMol-7B [45] and ChemDFM-13B [47], as well as competitive supervised learning models such as MolT5 [11] and its MoMu-enhanced one [32].</p>
<p>Molecular Property Prediction</p>
<p>Besides overall understanding, we assess the model's perception of molecular properties by conducting experiments on the molecular property prediction task.This task involves approximating quantitative attributes such as solubility or determining the activity for assays of a molecule.We employ MoleculeNet dataset [41] with 7 different subsets including ESOL, FreeSolv, Lipophilicity, HIV, BACE, BBBP, and Tox21.As evaluation metrics, RMSE is used for regression subsets, and Accuracy and F1 are used for classification, following [45].Figure 4 shows an example of this task.</p>
<p>Experimental results in Table 2 show that MolX improves performances of the base LLM in both model configurations.Especially for Inference-only results, MolX remarkably narrows approximation errors.Additionally, MolX enhances the model's ability of instruction following, generating expected answers without LLMs's favorite phrases.In addition to LoRA fine-tuned models, we consider ChemGraphCL [44] which serves as the GNN-based graph encoder in MolX, ensuring an adequate comparison.We observe that the MolX-enhanced LLM achieves the best scores in 6 out of 8 subsets of the MoleculeNet dataset and is the second-best in the other 2. Notably, properties in the MoleculeNet dataset are unseen from the pre-training stage, showing the strong adaptability of our proposed method on unseen downstream tasks.</p>
<p>Ablation Studies</p>
<p>Besides the main experiments, we conduct 2 ablation studies on the influence of building components in our proposed framework and verify the agnosticism to the base LLM of MolX where we use another LLM, Mistral [16].</p>
<p>Component Impact</p>
<p>Here we study the influence of building components in our proposed framework.Firstly, we use random initializations for trainable encoders, exploring the possibility of eliminating reliance on robust pre-trained weights.Next, we investigate the contributions of incorporating the Morgan fingerprint, as well as the weighted scheme by removing them from the framework.Moreover, to demonstrate the effectiveness of our versatile pre-training strategy, we discard auxiliary tasks and only use the molecule description generation objective during pre-training.Lastly, by totally skipping the pretraining stage, we aim to understand its alignment impact on the framework.Experiments are conducted on the molecule description generation task on the PubChem dataset [20] under the LoRA fine-tuning scenario, simultaneously highlighting the proposed framework's efficiency regarding the number of trainable parameters during pre-training and fine-tuning on downstream tasks.</p>
<p>Table 3 shows results for the described ablation study.Firstly, a drop in the performances of MolX without chemical initializations for encoders indicates the role of robust pre-trained weights.Next, while the weighted scheme brings a modest improvement, incorporating the Morgan fingerprint contributed significantly to the performances of MolX.Moreover, without proposed auxiliary tasks, a noticeable decrease in performances can be viewed, especially for ROUGE-based and METEOR metrics, demonstrating their effectiveness in providing the model with comprehensive information about the molecules.Lastly, it is not surprising that the pre-training stage which forms an alignment between MolX and the LLMs' textual input space, has a large impact.In terms of efficiency, our proposed framework only introduces a small number of trainable parameters, accounting for 0.53% of the entire parameters during pre-training and 0.82% with fine-tuning on downstream tasks.</p>
<p>LLM Agnosticism</p>
<p>Since we propose MolX as a plug-in module to the LLMs, here we verify its agnosticism to the base LLM by conducting experiments with another LLM, Mistral [16] with 7B parameters.Experiments are conducted on the molecule description generation task on the PubChem dataset [20] under the LoRA fine-tuning scenario with the same experimental settings for a fair comparison.</p>
<p>Figure 5 shows results for the described ablation study.First, we can observe that MolX consistently improves performances of Mistral [16] across 6 different metrics, as proved with Llama-2 [35] earlier.Additionally, as Mistral [16] outperforms Llama-2 [35], the MolX-enhanced Mistral [16] outperforms the MolX-enhanced Llama-2 [35] across 6 different metrics.This verifies the compatibility of MolX with different base LLMs and also proves that it could be applied to stronger LLMs released in the future.</p>
<p>Discussion</p>
<p>First, Table 4 demonstrates that our proposed method is designed to be more efficient than most baselines while giving superior performances.</p>
<p>Here we discuss the limitations of our work and future directions.Firstly, we are aligning MolX into the LLM via a soft token, which is simple but effective.Although we are aware of advanced cross-space alignment techniques such as Q-Former [19], we opt not to employ them since they require a large number of high-quality moleculedescription pairs and an extra pre-training stage, leading to high computational costs.A better alignment technique tailored for molecule-related tasks needs to be explored.Moreover, throughout experiments, we show the limitations of current generalist chemical LLMs, therefore, a novel generalist chemical LLM enhanced with MolX should be developed.LLMs also have been demonstrated to have intriguing abilities like In-context Learning [5] or Chain-of-Thought [39].Leveraging these abilities for molecule-related tasks is a potential direction.</p>
<p>Conclusion</p>
<p>In this paper, we propose a novel framework enhancing LLMs to comprehend molecules, thus, improving their performances on molecule-related tasks.The LLMs are equipped with a multi-modal external module, MolX, which is aligned with their textual input space using a versatile pre-training strategy.Experimental evaluations show that our proposed method consistently outperforms baselines across downstream molecule-related tasks ranging from molecule-to-text translation to molecular property prediction, with and without fine-tuning the LLM, while only introducing a small number of trainable parameters-0.53%and 0.82%, respectively.</p>
<p>8 Acknowledgement</p>
<p>Given a molecule, provide a description of that molecule.Molecule: CNC1(CCCCC1=O)C2=CC=CC=C2Cl Instruction LLM The molecule is a member of the class of cyclohexanones in which one of the hydrogens at position 2 is substituted by a 2-chlorophenyl group, while the other is substituted by a methylamino group.... Answer a) Current paradigm of using an LLM for molecule-related tasks b) LLM's BPE Tokenizer processes the SMILES string CNC1(CCCCC1=O)C2=CC=CC=C2Cl LLM's BPE Tokenizer 'C','NC','1','(','CC','CC','C','1','=','O',')','C','2','=','CC','=','CC','=','C','2','Cl' c) Potential inaccuracies of topological structures represented in the SMILES string CNC1(CCCCC1=O)C2=CC=CC=C2Cl</p>
<p>Figure 1 :
1
Figure 1: Current paradigm of using an LLM for molecule-related tasks and its issues.</p>
<p>dFigure 2 :
2
Figure 2: An overview of our proposed method with the main pre-training task.</p>
<p>8 Figure 3 :
83
Figure 3: Examples of auxiliary tasks in our instruction-based pre-training strategy.</p>
<p>Figure 4 :
4
Figure 4: An example of molecular property prediction.</p>
<p>Figure 5 :
5
Figure 5: Added results for molecule description generation.</p>
<p>Table 1 :
1
Experimental results for molecule-to-text translation.ability of instruction following.Formally, we first define  (.) as the textual distribution parameterized by the base LLM.The base LLM is decomposed into two subparts, the text embedder   and self-attention layers   , in which the text embedder   converts an instruction of a task into a list of  tokens  = [ 1 ,  2 , ..,   ].
ModelDescription GenerationIUPAC Name GenerationBLE-2↑BLE-4↑ROG-1↑ROG-2↑ROG-L↑MET↑BLE-2↑BLE-4↑ROG-1↑ROG-2↑ROG-L↑MET↑Infer-onlyLlama-2-7B03.6402.9818.2804.2612.8716.2105.5501.8105.4000.2304.3910.30Llama-2-7B + MolX08.2206.4030.8221.6928.9421.7710.6704.7614.6101.2411.4718.54LoRA FTLlama-2-7B27.5421.2436.5021.3328.9931.6951.4336.9448.5420.5740.5353.38Llama-2-7B + MoMu27.6821.5036.7621.4229.2331.8651.7037.3848.8920.6540.8753.66Llama-2-7B + MoLM-2D27.9521.7738.6622.9930.9233.6952.3237.6551.7721.8343.6257.10Llama-2-7B + MoLM-3D29.8222.3939.1223.6232.6434.3455.7038.9352.0322.7845.6357.84Llama-2-7B + MolX31.4024.2544.2028.9638.7639.5556.8845.0155.4530.1448.1959.35LlaSMol-7B26.7118.0638.7522.7733.3232.6349.4836.3352.3828.5345.2058.48ChemDFM-13B13.0208.3020.4211.3117.9318.4439.3322.8337.6109.4928.6845.99Full FTMolT5-Large25.8717.2834.0716.4223.4128.0450.8838.6945.8921.1133.0344.82MolT5-Large + MoMu26.3418.0134.7516.8624.7628.7351.8140.3246.8121.6834.9345.92then employ instruction-based pre-training [25, 30], enhancing themodel's</p>
<p>Table 3 :
3
Added results for molecule description generation.
Model# Trainable ParamsDescription GenerationPre-trainingDownstreamBLE-2↑BLE-4↑ROG-1↑ROG-2↑ROG-L↑MET↑LoRA FTLlama-2-7B + MolX w/o ChemInit36.1M (0.53%)56.6M (0.82%)30.2122.6743.6428.8038.4738.43Llama-2-7B + MolX w/o MorganFP23.5M (0.35%)44.0M (0.64%)29.3322.0142.3727.9637.3537.31Llama-2-7B + MolX w/o WeightedInc36.1M (0.53%)56.6M (0.82%)31.1324.0144.1628.5038.5639.34Llama-2-7B + MolX w/o Auxiliaries36.1M (0.53%)56.6M (0.82%)30.7123.0640.2924.3333.6235.37Llama-2-7B + MolX w/o Pre-training00.0M (0.00%)56.6M (0.82%)28.7922.3638.2322.2830.4033.13Llama-2-7B + MolX36.1M (0.53%)56.6M (0.82%)31.4024.2544.2028.9638.7639.55</p>
<p>Table 4 :
4
Numbers of trainable parameters in experiments.</p>
<p>https://pubchem.ncbi.nlm.nih.gov
Received June 2025; revised June 2025; accepted June 2025
This work was supported by the National Science Foundation (CHE-2202693) through the NSF Center for Computer Assisted Synthesis (C-CAS).10%Aromatic RingThe complexity rating of a compound estimates its structural complexity based on its elements and structural features, including symmetry.What is the complexity rating of this molecule?10%ComplexityThe topological polar surface area (TPSA) is the surface sum of all polar atoms or molecules, primarily oxygen and nitrogen, also including their attached hydrogen atoms.What is the TPSA value of this molecule?Topological Polar Surface AreaThe molecular weight is the sum of the atomic weights of all the atoms in the molecule.Canonicalizing the molecule's SMILES string 10%A Pre-training StrategyHere we elaborate the pre-training strategy by describing all proposed pre-training tasks.The molecule description generation task serves as the main task, accompanied by a couple of auxiliary tasks.We select a set of 10 low-level properties that present comprehensive information about the molecules.We use one more special auxiliary task which is canonicalizing the molecule's SMILES string.Examples of these tasks and their instructions are illustrated in Figure6.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Walid Ahmad, Elana Simon, arXiv:2209.01712Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2022. Chemberta-2: Towards chemical foundation models. 2022arXiv preprint</p>
<p>Artificial intelligence in chemistry: current trends and future directions. Zachary J Baum, Xiang Yu, Philippe Y Ayala, Yanan Zhao, Steven P Watkins, Qiongqiong Zhou, Journal of Chemical Information and Modeling. 612021. 2021</p>
<p>Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders. Esben Jannik, Bjerrum , Boris Sattarov, Biomolecules. 81312018. 2018</p>
<p>Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato. R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Do large language models understand chemistry? a conversation with chatgpt. Cayque Monteiro, Castro Nascimento, André Silva Pimentel, Journal of Chemical Information and Modeling. 632023. 2023</p>
<p>Unifying molecular and textual representations via multi-task language modelling. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, Matteo Manica, International Conference on Machine Learning. PMLR2023</p>
<p>Molecular representations in AI-driven drug discovery: a review and practical guide. Laurianne David, Amol Thakkar, Rocío Mercado, Ola Engkvist, Journal of Cheminformatics. 12562020. 2020</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Screening toward the Development of Fingerprints of Atomic Environments Using Bond-Orientational Order Parameters. Hideo Doi, Kazuaki Z Takahashi, Takeshi Aoyagi, ACS omega. 72022. 2022</p>
<p>Translation between Molecules and Natural Language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, 10.18653/v1/2022.emnlp-main.26Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Zornitsa Goldberg, Yue Kozareva, Zhang, the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, Advances in Neural Information Processing Systems. 362023. 2023</p>
<p>Measuring and Improving the Use of Graph Information in Graph Neural Networks. Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, T B Richard, Hongzhi Ma, Ming-Chang Chen, Yang, International Conference on Learning Representations. 2020</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Machine Learning: Science and Technology. 3150222022. 2022</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023. 2023Mistral 7B. arXiv preprint</p>
<p>The message passing neural networks for chemical property prediction on SMILES. Jeonghee Jo, Bumju Kwak, Hyun-Soo Choi, Sungroh Yoon, Methods. 1792020. 2020</p>
<p>RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 31.10Greg Landrum. 852812013. 2013</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Towards 3D Molecule-Text Interpretation in Language Models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The prediction of molecular toxicity based on BiGRU and GraphSAGE. Jianping Liu, Xiujuan Lei, Yuchen Zhang, Yi Pan, Computers in biology and medicine. 1531065242023. 2023</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2019</p>
<p>Unified deep learning model for multitask reaction predictions with explanation. Jieyu Lu, Yingkai Zhang, Journal of chemical information and modeling. 622022. 2022</p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Harry L Morgan, Journal of chemical documentation. 51965. 1965</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 352022. 2022</p>
<p>BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning. Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan, Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, 10.18653/v1/2023.emnlp-main.70Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Extended-connectivity fingerprints. David Rogers, Mathew Hahn, Journal of chemical information and modeling. 502010. 2010</p>
<p>Multitask Prompted Training Enables Zero-Shot Task Generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, International Conference on Learning Representations. Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M RushTeven Le Scao2022</p>
<p>Neural Machine Translation of Rare Words with Subword Units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. Katrin Erk, Noah A Smith, the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022. 2022arXiv preprint</p>
<p>Viktor Kerkez, and Robert Stojnic. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, arXiv:2211.09085Galactica: A large language model for science. 2022. 2022arXiv preprint</p>
<p>Large language models for science and medicine. Amalio Telenti, Michael Auli, Brian L Hie, Cyrus Maher, Suchi Saria, John Pa Ioannidis, European Journal of Clinical Investigation. 54e141832024. 2024</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, arXiv:2307.09288Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>When yield prediction does not yield prediction: an overview of the current challenges. Varvara Voinarovska, Mikhail Kabeshov, Dmytro Dudenko, Igor V Samuel Genheden, Tetko, Journal of Chemical Information and Modeling. 642023. 2023</p>
<p>Chemical-Reaction-Aware Molecule Representation Learning. Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, Martin D Burke, International Conference on Learning Representations. 2022</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. the 10th ACM international conference on bioinformatics, computational biology and health informatics2019</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 281988. 1988</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical science. 92018. 2018</p>
<p>Understanding the limitations of deep models for molecular property prediction: Insights and solutions. Jun Xia, Lecheng Zhang, Xiao Zhu, Yue Liu, Zhangyang Gao, Bozhen Hu, Cheng Tan, Jiangbin Zheng, Siyuan Li, Stan Z Li, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z Li, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Graph contrastive learning with augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Advances in neural information processing systems. 332020. 2020</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024. 2024arXiv preprint</p>
<p>What a Scientific Language Model Knows and Doesn't Know about Chemistry. Lawrence Zhao, Carl Edwards, Heng Ji, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Chemdfm: Dialogue foundation model for chemistry. 2024. 2024arXiv preprint</p>
<p>Root-aligned SMILES: a tight representation for chemical reaction prediction. Zipeng Zhong, Jie Song, Zunlei Feng, Tiantao Liu, Lingxiang Jia, Shaolun Yao, Min Wu, Tingjun Hou, Mingli Song, Chemical Science. 132022. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1211 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1211</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1211</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-258078790</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.06011v2.pdf" target="_blank">MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Multi-agent reinforcement learning (MARL) methods often suffer from high sample complexity, limiting their use in real-world problems where data is sparse or expensive to collect. Although latent-variable world models have been employed to address this issue by generating abundant synthetic data for MARL training, most of these models cannot encode vital global information available during training into their latent states, which hampers learning efficiency. The few exceptions that incorporate global information assume centralized execution of their learned policies, which is impractical in many applications with partial observability. We propose a novel model-based MARL algorithm, MABL (Multi-Agent Bi-Level world model), that learns a bi-level latent-variable world model from high-dimensional inputs. Unlike existing models, MABL is capable of encoding essential global information into the latent states during training while guaranteeing the decentralized execution of learned policies. For each agent, MABL learns a global latent state at the upper level, which is used to inform the learning of an agent latent state at the lower level. During execution, agents exclusively use lower-level latent states and act independently. Crucially, MABL can be combined with any model-free MARL algorithm for policy learning. In our empirical evaluation with complex discrete and continuous multi-agent tasks including SMAC, Flatland, and MAMuJoCo, MABL surpasses SOTA multi-agent latent-variable world models in both sample efficiency and overall performance.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1211.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1211.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MABL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Bi-Level world model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-level latent-variable world model for model-based multi-agent RL that encodes per-agent global latents at an upper level to inform agent-specific latent states at a lower level, enabling centralized training with decentralized execution and generation of synthetic latent trajectories for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MABL (Multi-Agent Bi-Level world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hierarchical latent-variable world model implemented as a sequential VAE with two levels of latent variables: (1) a per-agent global latent state (upper level) that encodes training-time global information (all agents' observations and any additional global info) and (2) an agent latent state (lower level) conditioned on the global latent and the agent's history; deterministic RNN embeddings propagate history; priors (transition model) are autoregressive and the agent prior is conditioned on the global prior in a top-down fashion; posterior factorizes into global and agent posteriors; auxiliary predictors produce reward, termination, and available-action signals from latents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (hierarchical / bi-level sequential VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-agent reinforcement learning benchmarks: StarCraft Multi-Agent Challenge (SMAC), Flatland (multi-train), and Multi-Agent MuJoCo (MAMuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO (reconstruction log-likelihood of observations) combined with KL divergence terms between posterior and prior for global and agent latents; auxiliary negative log-likelihoods for reward, termination, and available-action predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No direct numerical fidelity metrics (e.g., MSE) are reported; fidelity is trained via ELBO/KL balancing and evaluated indirectly through downstream sample efficiency and task performance (win-rate / reward curves).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Explicitly stated as not interpretable: the learned latent states are not interpretable according to the paper's Conclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned; authors note interpretability of latents as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No explicit parameter counts or wall-clock training times reported; experiments run on systems with NVIDIA RTX A6000 (SMAC, MAMuJoCo) and NVIDIA A100 (Flatland) GPUs; hyperparameters reported (e.g., sequence lengths 20/50, hidden sizes 256/400, categorical latents with 32 categoricals × 32 classes) but no aggregate FLOPs or training duration numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitative: MABL achieves substantially better sample efficiency than CTDE baselines (Dreamer-v2, MAPPO) across most tasks, and matches or outperforms CTCE baselines (MAMBA, MAG) despite being CTDE; example: on an Easy SMAC map MABL reaches 80% win-rate nearly 2× faster than MAPPO in initial learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Qualitative: MABL outperforms state-of-the-art CTDE latent-variable world models on all evaluated tasks except the simplest SMAC map; it also matches or outperforms CTCE baselines on most benchmarks (SMAC maps, Flatland, MAMuJoCo). Exact numeric scores are reported in plots/tables of the paper but specific numbers are not reproduced in the text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The paper shows that encoding global training-time information into an upper-level latent and conditioning agent latents on it improves representation learning and translates to significant gains in sample efficiency and task performance; high-level fidelity (better latent representations) correlates with improved policy learning in their evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off: improved sample efficiency and decentralized execution with incorporated global information, at the cost of latent interpretability (authors note latents are not interpretable). Design trade-offs include per-agent global latents (better performance) versus a single shared global latent (worse performance due to noisy/irrelevant encoding). MABL chooses CTDE (practical execution) while matching CTCE performance that requires centralized execution and communication at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Bi-level latent hierarchy (per-agent global latents + agent latents conditioned on global), discrete categorical latent variables (multiple categoricals × classes), RNN-based deterministic embeddings (global and agent RNNs), transition/prior models that condition agent prior on global prior (top-down), KL balancing for training, auxiliary predictors for reward/termination/available actions, shared model parameters across agents for scalability, compatibility with any off-the-shelf MARL algorithm (e.g., MAPPO) for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Dreamer-v2 (single-agent-style per-agent independent latent models / single-level): MABL (bi-level) produces better representation learning and much higher sample efficiency. Compared to CTCE methods (MAMBA, MAG) that aggregate observations of all agents via transformer blocks at execution time: MABL achieves comparable or superior sample efficiency while preserving decentralized execution (agents only need their agent latent at runtime). The paper argues MABL provides a better balance of fidelity, decentralized usability, and sample efficiency than these alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends per-agent global latents (one global latent per agent) rather than a single shared global latent (ablation MABL-SG performed worse); use of KL balancing and discrete categorical latent representations; sharing model parameters across agents for scalability; no single numerical 'optimal' model size or compute budget is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1211.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1211.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (latent-variable world model; baseline implementation referred to as Dreamer-v2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent latent-variable sequential VAE world model (used here as a multi-agent baseline by running independent per-agent models) that learns compact latent states and uses imagined latent rollouts for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer-v2 (single-level latent-variable world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequential variational autoencoder style latent world model with recurrent deterministic embeddings and stochastic latent variables trained with ELBO; used here as a per-agent single-level latent model (no global latent) for multi-agent scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (single-level sequential VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Applied as a CTDE baseline in multi-agent RL domains (SMAC, Flatland, MAMuJoCo) by learning an agent latent state per agent without an upper-level global latent.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO with reconstruction log-likelihood and KL divergence; trained using amortized variational inference.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically in this paper; Dreamer-v2 variant (no global latent) exhibited lower downstream task performance and sample efficiency than MABL in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in detail; generally treated as a black-box latent model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified for Dreamer-v2 in paper; as implemented as independent per-agent models, likely scales with number of agents (no shared upper-level global latent).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Empirically less sample-efficient than MABL across most tasks; considered the ablation variant when the global latent was removed, showing degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performed worse than MABL (the Dreamer-v2 variant lacking global latent had the lowest performance in the ablation study), failing in some tasks where MABL succeeded.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Single-level per-agent latent models without access to training-time global information produce inferior task-specific representations in partially observable multi-agent settings, reducing policy learning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and decentralized execution at runtime, but poorer representation learning and sample efficiency due to omission of global training-time information.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Single-level latent per agent, trained with standard ELBO/KL objectives; no top-level global latent to encode global info.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Worse sample efficiency and final performance compared to MABL (bi-level). Compared to CTCE methods, Dreamer-v2 sometimes performs better in less complex environments (paper notes cases where simpler CTDE baselines outperform CTCE baselines), but overall is outperformed by MABL.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's ablation suggests adding a global latent (bi-level) is beneficial; single-level Dreamer-v2 configuration is suboptimal for challenging multi-agent partially observable tasks according to their results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1211.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1211.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAMBA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAMBA (baseline CTCE multi-agent latent-variable world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent latent-variable world model baseline that aggregates observations of all agents (via a transformer encoder block) to compute per-agent latent states and requires centralized execution because latent computation during deployment uses all agents' observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAMBA (transformer-aggregated multi-agent latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-agent latent-variable world models augmented by a transformer encoder that aggregates observations from all agents to compute latent states; designed to reduce non-stationarity and model errors by using global observation aggregation but requires centralized execution because the transformer must run at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (transformer-aggregated multi-agent sequential VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-agent RL benchmarks (SMAC, Flatland, etc.) as CTCE baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained with ELBO-style objectives and latent prediction losses similar to single-agent latent-variable models; fidelity assessed indirectly via downstream sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric fidelity metrics reported in this paper; MAMBA is cited as state-of-the-art CTCE prior work and serves as a strong baseline—MABL matches or outperforms MAMBA on most tasks in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed; model treated as neural (black-box) latent model; no interpretability methods reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher runtime requirement due to transformer aggregation at execution (requires communication/centralized computation across agents); specific compute numbers are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MAMBA achieves strong sample efficiency in prior work but requires centralized execution; MABL, while CTDE, achieves comparable or better sample efficiency without centralized execution, indicating a favorable efficiency-utility trade-off for MABL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong baseline performance; MABL outperforms MAMBA on most environments except a few easy/super-hard maps where they are comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Centralized aggregation of observations helps representation learning and sample efficiency, but the requirement for centralized execution limits practical deployment; MABL aims to capture benefits of MAMBA's global information during training while avoiding centralized execution at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MAMBA trades decentralized deployability for potentially stronger representation learning by using full-observation aggregation at execution; MABL attempts to retain the representational benefits while allowing decentralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses transformer-based aggregation of agent observations to compute latents; per-agent models that rely on global observation aggregation at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MABL, MAMBA is CTCE and hence cannot be used in decentralized execution settings without communication; MABL often matches/outperforms MAMBA despite CTDE constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe optimal MAMBA configuration; highlights MABL design as an approach that attains similar representation quality without centralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1211.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1211.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAG (recent CTCE multi-agent latent-variable world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent CTCE world-model approach that trains one world model per agent with transformer-style architectures and joint training to reduce cumulative long-term joint prediction errors, but requires centralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAG (per-agent world models trained jointly with aggregation; CTCE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-agent latent-variable world models with separate parameters per agent trained jointly so local predictions account for long-term joint effects; uses architectures similar to transformer/aggregation models and requires centralized execution because latent computation uses other agents' observations at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (per-agent sequential VAE with joint training and aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-agent RL benchmarks (SMAC, Flatland, MAMuJoCo) as a state-of-the-art CTCE baseline</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained with ELBO-style objectives and joint objectives to reduce multi-step joint prediction error; fidelity evaluated indirectly via downstream sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No direct numeric fidelity metrics reported in this excerpt; MAG is a strong CTCE baseline—MABL outperforms MAG on most benchmarks except certain easy maps.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed; latent spaces treated as black-box representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Higher runtime communication/centralized computation overhead due to per-agent models requiring other agents' observations at execution; exact computational cost not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>MAG is competitive in sample efficiency as a CTCE baseline; MABL (CTDE) matches or outperforms MAG on most tasks, indicating MABL achieves similar or better sample efficiency without centralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong; outperformed by MABL on Flatland, MAMuJoCo, and most SMAC maps except a couple of easy/specific maps where performance is similar.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Jointly trained per-agent models that consider joint long-term effects can produce high-quality latent trajectories for training, but runtime centralization limits deployment; MABL provides decentralized execution with comparable task utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MAG prioritizes fidelity of joint predictions via joint per-agent training at the cost of requiring centralized execution and communication at runtime; MABL sacrifices centralized runtime access but matches/outperforms MAG in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>One world model per agent with separate parameters; joint training to minimize long-term joint prediction errors; transformer-like aggregation for cross-agent information.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MABL, MAG is CTCE and sometimes less practical for decentralized deployment despite strong sample-efficiency; MABL typically matches or exceeds MAG performance while preserving decentralized execution.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not provide explicit MAG configuration recommendations; emphasizes that MABL's per-agent global latents are an effective alternative to MAG's joint-training + centralized execution approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1211.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1211.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-variable world models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent-variable world models / sequential variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of model-based RL world models that learn compact, low-dimensional latent states from high-dimensional observations via sequential VAEs and use learned latent trajectories for planning or policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Latent-variable sequential VAE world models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequential variational autoencoders with deterministic recurrent embeddings and stochastic latent variables; consist of representation (posterior) networks, transition (prior) models, and decoders for observations; trained by maximizing ELBO (reconstruction + KL penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (sequential VAE family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Single- and multi-agent reinforcement learning (Atari, continuous control, SMAC, Flatland, MAMuJoCo) as discussed in the literature and used as the conceptual foundation for MABL and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO constituents: log-likelihood / reconstruction loss of observations; KL divergence between posterior and prior; multi-step prediction error of latents; auxiliary supervised losses when predicting rewards or termination.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Varies by instantiation and task; this paper does not report generic numeric fidelity statistics for the family, but uses ELBO-based training and downstream policy performance as practical fidelity proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Generally treated as black-box neural latents in this paper; interpretability is not guaranteed and is raised as an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Occasionally prior work visualizes latent traversals or aligns latents with semantic factors, but this paper reports no such interpretability methods and notes lack of interpretability as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Cost depends on architecture (RNNs, transformers, categorical latents), latent dimensionality, and whether models are shared across agents; paper provides hyperparameters (hidden sizes, sequence lengths) but no general compute benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Latent-variable world models improve sample efficiency compared to model-free RL in many single-agent settings; multi-agent extensions can either be CTDE or CTCE, with CTCE variants sometimes achieving higher sample efficiency but at execution-time centralization costs—MABL aims to get the benefits while retaining CTDE.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When well-designed, latent-variable world models enable learning policies from synthetic latent rollouts and can significantly reduce required environment interactions; specific task performance is model- and task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High latent fidelity (good ELBO, low multi-step error) tends to improve policy learning; however, representational choices (e.g., including global info during training) strongly affect downstream utility in multi-agent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs include (a) model fidelity vs. computational/communication cost (e.g., transformer aggregation at runtime improves fidelity but requires centralized execution), (b) representation capacity vs. overfitting/noise (single shared global latent can inject irrelevant info), and (c) interpretability vs. performance (more compressed/abstract latents may be less interpretable).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Choices include latent topology (hierarchical vs single-level), parametrization (categorical vs continuous latents), deterministic recurrence (RNNs) vs transformer aggregation, sharing vs per-agent parameters, and inclusion of auxiliary predictors for rewards/termination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to explicit simulators or physics-based models, latent-variable models are learned end-to-end and more flexible but lack explicit interpretability and guarantees; compared to model-free methods, they can be more sample-efficient by generating synthetic training data.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No single optimal configuration universally prescribed; in multi-agent partially observable domains, the paper's evidence supports hierarchical (bi-level) latents with per-agent global latents and KL balancing as effective design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to Control: Learning Behaviors by Latent Imagination <em>(Rating: 2)</em></li>
                <li>Mastering Atari with Discrete World Models <em>(Rating: 1)</em></li>
                <li>World models <em>(Rating: 1)</em></li>
                <li>Scalable Multi-Agent Model-Based Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Multi-agent reinforcement learning with multi-step generative models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1211",
    "paper_id": "paper-258078790",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "MABL",
            "name_full": "Multi-Agent Bi-Level world model",
            "brief_description": "A bi-level latent-variable world model for model-based multi-agent RL that encodes per-agent global latents at an upper level to inform agent-specific latent states at a lower level, enabling centralized training with decentralized execution and generation of synthetic latent trajectories for policy learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MABL (Multi-Agent Bi-Level world model)",
            "model_description": "A hierarchical latent-variable world model implemented as a sequential VAE with two levels of latent variables: (1) a per-agent global latent state (upper level) that encodes training-time global information (all agents' observations and any additional global info) and (2) an agent latent state (lower level) conditioned on the global latent and the agent's history; deterministic RNN embeddings propagate history; priors (transition model) are autoregressive and the agent prior is conditioned on the global prior in a top-down fashion; posterior factorizes into global and agent posteriors; auxiliary predictors produce reward, termination, and available-action signals from latents.",
            "model_type": "latent world model (hierarchical / bi-level sequential VAE)",
            "task_domain": "Multi-agent reinforcement learning benchmarks: StarCraft Multi-Agent Challenge (SMAC), Flatland (multi-train), and Multi-Agent MuJoCo (MAMuJoCo)",
            "fidelity_metric": "ELBO (reconstruction log-likelihood of observations) combined with KL divergence terms between posterior and prior for global and agent latents; auxiliary negative log-likelihoods for reward, termination, and available-action predictions.",
            "fidelity_performance": "No direct numerical fidelity metrics (e.g., MSE) are reported; fidelity is trained via ELBO/KL balancing and evaluated indirectly through downstream sample efficiency and task performance (win-rate / reward curves).",
            "interpretability_assessment": "Explicitly stated as not interpretable: the learned latent states are not interpretable according to the paper's Conclusion.",
            "interpretability_method": "None mentioned; authors note interpretability of latents as future work.",
            "computational_cost": "No explicit parameter counts or wall-clock training times reported; experiments run on systems with NVIDIA RTX A6000 (SMAC, MAMuJoCo) and NVIDIA A100 (Flatland) GPUs; hyperparameters reported (e.g., sequence lengths 20/50, hidden sizes 256/400, categorical latents with 32 categoricals × 32 classes) but no aggregate FLOPs or training duration numbers.",
            "efficiency_comparison": "Qualitative: MABL achieves substantially better sample efficiency than CTDE baselines (Dreamer-v2, MAPPO) across most tasks, and matches or outperforms CTCE baselines (MAMBA, MAG) despite being CTDE; example: on an Easy SMAC map MABL reaches 80% win-rate nearly 2× faster than MAPPO in initial learning.",
            "task_performance": "Qualitative: MABL outperforms state-of-the-art CTDE latent-variable world models on all evaluated tasks except the simplest SMAC map; it also matches or outperforms CTCE baselines on most benchmarks (SMAC maps, Flatland, MAMuJoCo). Exact numeric scores are reported in plots/tables of the paper but specific numbers are not reproduced in the text excerpt.",
            "task_utility_analysis": "The paper shows that encoding global training-time information into an upper-level latent and conditioning agent latents on it improves representation learning and translates to significant gains in sample efficiency and task performance; high-level fidelity (better latent representations) correlates with improved policy learning in their evaluations.",
            "tradeoffs_observed": "Trade-off: improved sample efficiency and decentralized execution with incorporated global information, at the cost of latent interpretability (authors note latents are not interpretable). Design trade-offs include per-agent global latents (better performance) versus a single shared global latent (worse performance due to noisy/irrelevant encoding). MABL chooses CTDE (practical execution) while matching CTCE performance that requires centralized execution and communication at runtime.",
            "design_choices": "Bi-level latent hierarchy (per-agent global latents + agent latents conditioned on global), discrete categorical latent variables (multiple categoricals × classes), RNN-based deterministic embeddings (global and agent RNNs), transition/prior models that condition agent prior on global prior (top-down), KL balancing for training, auxiliary predictors for reward/termination/available actions, shared model parameters across agents for scalability, compatibility with any off-the-shelf MARL algorithm (e.g., MAPPO) for policy learning.",
            "comparison_to_alternatives": "Compared to Dreamer-v2 (single-agent-style per-agent independent latent models / single-level): MABL (bi-level) produces better representation learning and much higher sample efficiency. Compared to CTCE methods (MAMBA, MAG) that aggregate observations of all agents via transformer blocks at execution time: MABL achieves comparable or superior sample efficiency while preserving decentralized execution (agents only need their agent latent at runtime). The paper argues MABL provides a better balance of fidelity, decentralized usability, and sample efficiency than these alternatives.",
            "optimal_configuration": "Paper recommends per-agent global latents (one global latent per agent) rather than a single shared global latent (ablation MABL-SG performed worse); use of KL balancing and discrete categorical latent representations; sharing model parameters across agents for scalability; no single numerical 'optimal' model size or compute budget is prescribed.",
            "uuid": "e1211.0",
            "source_info": {
                "paper_title": "MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Dreamer-v2",
            "name_full": "Dreamer (latent-variable world model; baseline implementation referred to as Dreamer-v2)",
            "brief_description": "A single-agent latent-variable sequential VAE world model (used here as a multi-agent baseline by running independent per-agent models) that learns compact latent states and uses imagined latent rollouts for policy learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Dreamer-v2 (single-level latent-variable world model)",
            "model_description": "Sequential variational autoencoder style latent world model with recurrent deterministic embeddings and stochastic latent variables trained with ELBO; used here as a per-agent single-level latent model (no global latent) for multi-agent scenarios.",
            "model_type": "latent world model (single-level sequential VAE)",
            "task_domain": "Applied as a CTDE baseline in multi-agent RL domains (SMAC, Flatland, MAMuJoCo) by learning an agent latent state per agent without an upper-level global latent.",
            "fidelity_metric": "ELBO with reconstruction log-likelihood and KL divergence; trained using amortized variational inference.",
            "fidelity_performance": "Not reported numerically in this paper; Dreamer-v2 variant (no global latent) exhibited lower downstream task performance and sample efficiency than MABL in the reported experiments.",
            "interpretability_assessment": "Not discussed in detail; generally treated as a black-box latent model in this paper.",
            "interpretability_method": "None mentioned in this work.",
            "computational_cost": "Not specified for Dreamer-v2 in paper; as implemented as independent per-agent models, likely scales with number of agents (no shared upper-level global latent).",
            "efficiency_comparison": "Empirically less sample-efficient than MABL across most tasks; considered the ablation variant when the global latent was removed, showing degraded performance.",
            "task_performance": "Performed worse than MABL (the Dreamer-v2 variant lacking global latent had the lowest performance in the ablation study), failing in some tasks where MABL succeeded.",
            "task_utility_analysis": "Single-level per-agent latent models without access to training-time global information produce inferior task-specific representations in partially observable multi-agent settings, reducing policy learning efficiency.",
            "tradeoffs_observed": "Simplicity and decentralized execution at runtime, but poorer representation learning and sample efficiency due to omission of global training-time information.",
            "design_choices": "Single-level latent per agent, trained with standard ELBO/KL objectives; no top-level global latent to encode global info.",
            "comparison_to_alternatives": "Worse sample efficiency and final performance compared to MABL (bi-level). Compared to CTCE methods, Dreamer-v2 sometimes performs better in less complex environments (paper notes cases where simpler CTDE baselines outperform CTCE baselines), but overall is outperformed by MABL.",
            "optimal_configuration": "Paper's ablation suggests adding a global latent (bi-level) is beneficial; single-level Dreamer-v2 configuration is suboptimal for challenging multi-agent partially observable tasks according to their results.",
            "uuid": "e1211.1",
            "source_info": {
                "paper_title": "MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MAMBA",
            "name_full": "MAMBA (baseline CTCE multi-agent latent-variable world model)",
            "brief_description": "A multi-agent latent-variable world model baseline that aggregates observations of all agents (via a transformer encoder block) to compute per-agent latent states and requires centralized execution because latent computation during deployment uses all agents' observations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAMBA (transformer-aggregated multi-agent latent world model)",
            "model_description": "Per-agent latent-variable world models augmented by a transformer encoder that aggregates observations from all agents to compute latent states; designed to reduce non-stationarity and model errors by using global observation aggregation but requires centralized execution because the transformer must run at inference time.",
            "model_type": "latent world model (transformer-aggregated multi-agent sequential VAE)",
            "task_domain": "Multi-agent RL benchmarks (SMAC, Flatland, etc.) as CTCE baseline",
            "fidelity_metric": "Trained with ELBO-style objectives and latent prediction losses similar to single-agent latent-variable models; fidelity assessed indirectly via downstream sample efficiency.",
            "fidelity_performance": "No explicit numeric fidelity metrics reported in this paper; MAMBA is cited as state-of-the-art CTCE prior work and serves as a strong baseline—MABL matches or outperforms MAMBA on most tasks in experiments.",
            "interpretability_assessment": "Not discussed; model treated as neural (black-box) latent model; no interpretability methods reported here.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Higher runtime requirement due to transformer aggregation at execution (requires communication/centralized computation across agents); specific compute numbers are not provided.",
            "efficiency_comparison": "MAMBA achieves strong sample efficiency in prior work but requires centralized execution; MABL, while CTDE, achieves comparable or better sample efficiency without centralized execution, indicating a favorable efficiency-utility trade-off for MABL.",
            "task_performance": "Strong baseline performance; MABL outperforms MAMBA on most environments except a few easy/super-hard maps where they are comparable.",
            "task_utility_analysis": "Centralized aggregation of observations helps representation learning and sample efficiency, but the requirement for centralized execution limits practical deployment; MABL aims to capture benefits of MAMBA's global information during training while avoiding centralized execution at runtime.",
            "tradeoffs_observed": "MAMBA trades decentralized deployability for potentially stronger representation learning by using full-observation aggregation at execution; MABL attempts to retain the representational benefits while allowing decentralized execution.",
            "design_choices": "Uses transformer-based aggregation of agent observations to compute latents; per-agent models that rely on global observation aggregation at runtime.",
            "comparison_to_alternatives": "Compared to MABL, MAMBA is CTCE and hence cannot be used in decentralized execution settings without communication; MABL often matches/outperforms MAMBA despite CTDE constraint.",
            "optimal_configuration": "Paper does not prescribe optimal MAMBA configuration; highlights MABL design as an approach that attains similar representation quality without centralized execution.",
            "uuid": "e1211.2",
            "source_info": {
                "paper_title": "MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MAG",
            "name_full": "MAG (recent CTCE multi-agent latent-variable world model)",
            "brief_description": "A recent CTCE world-model approach that trains one world model per agent with transformer-style architectures and joint training to reduce cumulative long-term joint prediction errors, but requires centralized execution.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MAG (per-agent world models trained jointly with aggregation; CTCE)",
            "model_description": "Per-agent latent-variable world models with separate parameters per agent trained jointly so local predictions account for long-term joint effects; uses architectures similar to transformer/aggregation models and requires centralized execution because latent computation uses other agents' observations at runtime.",
            "model_type": "latent world model (per-agent sequential VAE with joint training and aggregation)",
            "task_domain": "Multi-agent RL benchmarks (SMAC, Flatland, MAMuJoCo) as a state-of-the-art CTCE baseline",
            "fidelity_metric": "Trained with ELBO-style objectives and joint objectives to reduce multi-step joint prediction error; fidelity evaluated indirectly via downstream sample efficiency.",
            "fidelity_performance": "No direct numeric fidelity metrics reported in this excerpt; MAG is a strong CTCE baseline—MABL outperforms MAG on most benchmarks except certain easy maps.",
            "interpretability_assessment": "Not discussed; latent spaces treated as black-box representations.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Higher runtime communication/centralized computation overhead due to per-agent models requiring other agents' observations at execution; exact computational cost not reported.",
            "efficiency_comparison": "MAG is competitive in sample efficiency as a CTCE baseline; MABL (CTDE) matches or outperforms MAG on most tasks, indicating MABL achieves similar or better sample efficiency without centralized execution.",
            "task_performance": "Strong; outperformed by MABL on Flatland, MAMuJoCo, and most SMAC maps except a couple of easy/specific maps where performance is similar.",
            "task_utility_analysis": "Jointly trained per-agent models that consider joint long-term effects can produce high-quality latent trajectories for training, but runtime centralization limits deployment; MABL provides decentralized execution with comparable task utility.",
            "tradeoffs_observed": "MAG prioritizes fidelity of joint predictions via joint per-agent training at the cost of requiring centralized execution and communication at runtime; MABL sacrifices centralized runtime access but matches/outperforms MAG in many cases.",
            "design_choices": "One world model per agent with separate parameters; joint training to minimize long-term joint prediction errors; transformer-like aggregation for cross-agent information.",
            "comparison_to_alternatives": "Compared to MABL, MAG is CTCE and sometimes less practical for decentralized deployment despite strong sample-efficiency; MABL typically matches or exceeds MAG performance while preserving decentralized execution.",
            "optimal_configuration": "Paper does not provide explicit MAG configuration recommendations; emphasizes that MABL's per-agent global latents are an effective alternative to MAG's joint-training + centralized execution approach.",
            "uuid": "e1211.3",
            "source_info": {
                "paper_title": "MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Latent-variable world models (general)",
            "name_full": "Latent-variable world models / sequential variational autoencoders",
            "brief_description": "A class of model-based RL world models that learn compact, low-dimensional latent states from high-dimensional observations via sequential VAEs and use learned latent trajectories for planning or policy learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Latent-variable sequential VAE world models",
            "model_description": "Sequential variational autoencoders with deterministic recurrent embeddings and stochastic latent variables; consist of representation (posterior) networks, transition (prior) models, and decoders for observations; trained by maximizing ELBO (reconstruction + KL penalty).",
            "model_type": "latent world model (sequential VAE family)",
            "task_domain": "Single- and multi-agent reinforcement learning (Atari, continuous control, SMAC, Flatland, MAMuJoCo) as discussed in the literature and used as the conceptual foundation for MABL and baselines.",
            "fidelity_metric": "ELBO constituents: log-likelihood / reconstruction loss of observations; KL divergence between posterior and prior; multi-step prediction error of latents; auxiliary supervised losses when predicting rewards or termination.",
            "fidelity_performance": "Varies by instantiation and task; this paper does not report generic numeric fidelity statistics for the family, but uses ELBO-based training and downstream policy performance as practical fidelity proxies.",
            "interpretability_assessment": "Generally treated as black-box neural latents in this paper; interpretability is not guaranteed and is raised as an open challenge.",
            "interpretability_method": "Occasionally prior work visualizes latent traversals or aligns latents with semantic factors, but this paper reports no such interpretability methods and notes lack of interpretability as future work.",
            "computational_cost": "Cost depends on architecture (RNNs, transformers, categorical latents), latent dimensionality, and whether models are shared across agents; paper provides hyperparameters (hidden sizes, sequence lengths) but no general compute benchmarks.",
            "efficiency_comparison": "Latent-variable world models improve sample efficiency compared to model-free RL in many single-agent settings; multi-agent extensions can either be CTDE or CTCE, with CTCE variants sometimes achieving higher sample efficiency but at execution-time centralization costs—MABL aims to get the benefits while retaining CTDE.",
            "task_performance": "When well-designed, latent-variable world models enable learning policies from synthetic latent rollouts and can significantly reduce required environment interactions; specific task performance is model- and task-dependent.",
            "task_utility_analysis": "High latent fidelity (good ELBO, low multi-step error) tends to improve policy learning; however, representational choices (e.g., including global info during training) strongly affect downstream utility in multi-agent tasks.",
            "tradeoffs_observed": "Trade-offs include (a) model fidelity vs. computational/communication cost (e.g., transformer aggregation at runtime improves fidelity but requires centralized execution), (b) representation capacity vs. overfitting/noise (single shared global latent can inject irrelevant info), and (c) interpretability vs. performance (more compressed/abstract latents may be less interpretable).",
            "design_choices": "Choices include latent topology (hierarchical vs single-level), parametrization (categorical vs continuous latents), deterministic recurrence (RNNs) vs transformer aggregation, sharing vs per-agent parameters, and inclusion of auxiliary predictors for rewards/termination.",
            "comparison_to_alternatives": "Compared to explicit simulators or physics-based models, latent-variable models are learned end-to-end and more flexible but lack explicit interpretability and guarantees; compared to model-free methods, they can be more sample-efficient by generating synthetic training data.",
            "optimal_configuration": "No single optimal configuration universally prescribed; in multi-agent partially observable domains, the paper's evidence supports hierarchical (bi-level) latents with per-agent global latents and KL balancing as effective design choices.",
            "uuid": "e1211.4",
            "source_info": {
                "paper_title": "MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to Control: Learning Behaviors by Latent Imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering Atari with Discrete World Models",
            "rating": 1,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "World models",
            "rating": 1,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Scalable Multi-Agent Model-Based Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "scalable_multiagent_modelbased_reinforcement_learning"
        },
        {
            "paper_title": "Multi-agent reinforcement learning with multi-step generative models",
            "rating": 2,
            "sanitized_title": "multiagent_reinforcement_learning_with_multistep_generative_models"
        }
    ],
    "cost": 0.0165325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning</p>
<p>Aravind Venugopal 
Stephanie Milani </p>
<p>Carnegie Mellon University Pittsburgh
PennsylvaniaUSA</p>
<p>Carnegie Mellon University Pittsburgh
PennsylvaniaUSA</p>
<p>Fei Fang
Carnegie Mellon University Pittsburgh
PennsylvaniaUSA</p>
<p>Balaraman Ravindran Indian Institute of Technology
Madras Chennai, Tamil NaduIndia</p>
<p>MABL: Bi-Level Latent-Variable World Model for Sample-Efficient Multi-Agent Reinforcement Learning
41C7D65B7646747994067DF354227F81Multi Agent Reinforcement LearningDeep Reinforcement LearningModel Based Reinforcement LearningWorld Models
Multi-agent reinforcement learning (MARL) methods often suffer from high sample complexity, limiting their use in real-world problems where data is sparse or expensive to collect.Although latentvariable world models have been employed to address this issue by generating abundant synthetic data for MARL training, most of these models cannot encode vital global information available during training into their latent states, which hampers learning efficiency.The few exceptions that incorporate global information assume centralized execution of their learned policies, which is impractical in many applications with partial observability.We propose a novel model-based MARL algorithm, MABL (Multi-Agent Bi-Level world model), that learns a bi-level latent-variable world model from high-dimensional inputs.Unlike existing models, MABL is capable of encoding essential global information into the latent states during training while guaranteeing the decentralized execution of learned policies.For each agent, MABL learns a global latent state at the upper level, which is used to inform the learning of an agent latent state at the lower level.During execution, agents exclusively use lower-level latent states and act independently.Crucially, MABL can be combined with any model-free MARL algorithm for policy learning.In our empirical evaluation with complex discrete and continuous multi-agent tasks including SMAC, Flatland, and MAMuJoCo, MABL surpasses SOTA multi-agent latent-variable world models in both sample efficiency and overall performance.</p>
<p>INTRODUCTION</p>
<p>Multi-agent reinforcement learning (MARL) offers a powerful, versatile approach for addressing a variety of real-world problems that require coordination among multiple agents, such as the control of robot swarms [1,23], autonomous vehicles [6], and more [18,44].These scenarios offer myriad challenges: agents must often learn to behave from high-dimensional, partially observable inputs while grappling with the issue of non-stationarity induced by other agents simultaneously learning in the environment [22,29].The resulting complexity often translates to a tremendous amount of environment interactions for learning effective policies [9].In practical scenarios, collecting such interaction data is resource-intensive and timeconsuming [2], underscoring the importance of sample efficiency.</p>
<p>In the single-agent setting, model-based RL has shown promise in improving sample efficiency by enabling agents to utilize predictive models of environment dynamics [4,35,40].These models, commonly referred to as world models, are often used to generate synthetic data from which agents can learn how to act.These approaches have been shown to improve sample efficiency by reducing the number of environment interactions needed to learn good behavior [14,20,25,37].However, they still require learning in a high-dimensional space.In recent years, however, model-based RL algorithms have employed latent-variable world models [12,13,20] to learn low-dimensional latent states from high-dimensional inputs.Trajectories of latent states generated by the model are then used for policy learning.Although this family of methods represents the state-of-the-art in the single-agent setting, they have only recently been brought to bear in MARL [41].Yet, current approaches [7,19] suffer from key limitations.</p>
<p>In MARL, the established paradigm of centralized training with decentralized execution (CTDE) [22,32] offers a pragmatic balance that enables centralized learning by allowing agents access to additional information during training, as long as each agent only accesses its private observation during policy execution.This paradigm ensures scalability and practicality in scenarios where agents have to act in a decentralized manner.We refer to the totality of information available to an agent during training, including its private observation, as "global information".By following the CTDE paradigm, model-based MARL agents can utilize global information in their models during training to enhance latent representation learning, potentially leading to more sample-efficient learning.</p>
<p>However, existing multi-agent latent-variable world models are either incapable of incorporating global information [19] or do so, but fail to ensure that agents only use their own observations during execution [7,42].This is because they compute latent states by accessing the observations of all agents.Although the latter class of approaches yields state-of-the-art sample efficiency, they perform centralized training with centralized execution (CTCE).This paradigm requires a continuous transfer of inputs between agents and a centralized controller, a transformer block [7,42], during execution.This, in turn, leads to increased latency, bottlenecks, and high susceptibility to communication failures.It also makes these approaches inapplicable to settings with partial observability and without communication channels between agents.Moreover, these approaches are not designed to incorporate any global information that is available in addition to agents' observations, and omitting such crucial information during training can be detrimental to learning.</p>
<p>To address the problem of incorporating global information while maintaining CTDE, we introduce Multi-Agent Bi-Level World Model (MABL), the first CTDE multi-agent latent-variable world model method that incorporates relevant global information, to train policies purely using synthetic trajectories of latent states generated by the model.The key insights behind MABL are that for each agent, 1. relevant global information can be encoded into a global latent state to learn policies through centralized training, and 2. the global latent state need not be used during execution and can instead be used to inform the representation learning of a separate agent-specific latent state used during decentralized execution.Specifically, as shown in Figure 1, MABL introduces a novel bi-level model to learn a hierarchical latent space.At the top level, the model learns the global latent state; at the bottom level, it learns an agent latent state, conditioned on the global latent state.MABL can be used as an additional module with any existing MARL algorithm.Our empirical studies on the challenging StarCraft Multi Agent Challenge (SMAC) [32], Flatland [26], and Multi-Agent MuJoCo (MAMuJoCo) [30] benchmarks show that MABL outperforms state-of-the-art multi-agent latent-variable world models in sample efficiency across a variety of discrete and continuous multi-agent tasks.</p>
<p>PRELIMINARIES</p>
<p>Multi Agent Reinforcement Learning.We consider MARL in a partially observable Markov game [27].The game is represented as  = ⟨ , , A, ,   , {O  }, {  }, ⟩. = {1, . . ., } is the set of agents,  the set of states, and A =   the joint action space, where   is the action space for agent .
= E 𝝅 [ ∞ 𝑗=0 𝛾 𝑗 𝑟 𝑖 𝑡 +𝑗 ],
where  ∈ [0, 1] is the discount factor.In model-free MARL, agents do not know  or  and must learn policies that maximize  by interacting with the environment.</p>
<p>Model-Based Reinforcement Learning.In contrast to model-free methods, model-based RL methods learn an explicit model trained to estimate the environment dynamics (i.e., state transition  and reward function(s) ) using self-supervised learning [14,20,37].We consider the most popular style of model-based RL methods, which follow the Dyna algorithm [37], where the model of the environment dynamics, called a world model, is learned from real environment interactions.To deal with high-dimensional inputs, model-based RL algorithms have employed latent-variable world models [12,13,20] that learn and generate trajectories of compact, low-dimensional latent states  from observations and actions,  and , as input during policy learning.In contrast, previous work generates synthetic data in the original high-dimensional space.</p>
<p>Latent-Variable World Models.Latent-variable world models are implemented as sequential variational auto-encoders [17] for learning environment dynamics.More concretely, consider a partially observable Markov decision process (POMDP) [3] described by ⟨, , , , O, , ⟩, where the symbols mean the same as before, except with a single agent.Given training data consisting of observation and action sequences { 1 ,  0 , . . .,   ,   −1 }, we can train the sequential variational auto-encoders with latent variables   to maximize the probability of the data  ( 1: | 0: −1 ).Directly maximizing this probability is challenging, so a typical approach is to consider the Evidence Lower Bound (ELBO) [17] for the log-likelihood of the sequence of observations:
log 𝑝 (𝑜 1:𝑇 |𝑎 0:𝑇 −1 ) ≥ E 𝑧 1:𝑇 ∼𝑞 𝑇 ∑︁ 𝑡 =1 log 𝑝 (𝑜 𝑡 |𝑧 𝑡 ) −𝐷 𝐾𝐿 𝑞(𝑧 𝑡 |𝑜 𝑡 , 𝑎 𝑡 −1 )∥𝑝 ( ẑ𝑡 |𝑎 𝑡 −1 ) ,
where   refers to the KL divergence.The latent-variable model thus consists of a transition model representing the prior distribution  ( ẑ |  −1 ), a representation model representing the posterior distribution (  |  ,   −1 ), and an observation decoder  ( ô |  ) to reconstruct observations ô from latent states   .All components are parameterized by neural networks and trained through amortized variational inference [17].Once trained,   serves as the compact latent state at time , and the model can be used to generate synthetic trajectories of latent states for RL training.</p>
<p>MULTI-AGENT BI-LEVEL WORLD MODEL</p>
<p>We propose Multi-Agent Bi-Level world model (MABL), a novel model-based MARL algorithm that uses a latent-variable world model architecture.Crucially, our model leverages the insight that vital global information -such as the observations of all agents and any extra information available during training -does not itself need to be used during execution, as long as we use it to inform the representation that is used during execution.In this way, we achieve centralized training with decentralized policy execution.</p>
<p>Concretely we propose a bi-level latent-variable world model.The global latent state at the top level of the hierarchy encodes information about the global state of the environment relevant to the agent's learning.The global latent state is used to inform the learning of an agent latent state at the bottom level of the hierarchy to encode agent-specific local information.The agent latent state can be computed during execution time using just the agent's observation, meaning that it is more informative (because it encodes information derived from the global state during centralized training) but also more useful (because it can be computed and used in a decentralized manner during execution time).</p>
<p>In this section, we first describe our novel bi-level world model architecture and explain how it encodes relevant global information.We then detail the training framework for the model and MARL algorithm that is trained with the latent trajectories generated by the model.Throughout this section, we describe the algorithm with respect to an agent .</p>
<p>Bi-Level World Model</p>
<p>To effectively capture environment dynamics in multi-agent settings, we introduce a novel bi-level architecture for the world model.The bi-level world model embeds high-dimensional inputs into latent representations to form a predictive model, as we now explain.</p>
<p>The model takes as input multi-agent trajectories of length  , represented as {   ,   , a  −1 ,    }   =1 .The input trajectories are sampled from a buffer, which we call the model buffer, populated through interactions of the agents with the environment.Our model comprises of neural networks that serve two main functions: learning the transition dynamics and supporting the trajectory generation for MARL training.We refer to the former as Transition Dynamics components and the latter as Auxiliary components.All components are parameterized by neural networks with combined weights  and are trained jointly.Each agent has a bi-level model, but the parameters ( ) of model are shared among all agents to ensure scalability to settings with a large number of agents.
       Global: ℎ 𝑔,𝑖 𝑡 = 𝑓 𝑔,𝑖 𝜓 (ℎ 𝑔,𝑖 𝑡 |ℎ 𝑔,𝑖 𝑡 −1 , 𝑧 𝑔,𝑖 𝑡 −1 , a 𝑡 −1 ) Agent: ℎ 𝑎,𝑖 𝑡 = 𝑓 𝑎,𝑖 𝜓 (ℎ 𝑎,𝑖 𝑡 |ℎ 𝑎,𝑖 𝑡 −1 , 𝑧 𝑎,𝑖 𝑡 −1 , 𝑎 𝑡 −1 )
Representation Model: (Posterior Distribution)</p>
<p>Global:  Recurrent Models.To accurately learn multi-agent environment dynamics, the latent states should not only capture information about the current state of the environment but also past states and actions, especially in partially observable settings.The goal of the recurrent models is to capture this relevant historical information with deterministic embeddings.The global recurrent model propagates information about past environment states and joint actions through its embeddings ℎ ,  .In contrast, the agent recurrent model captures information about the action-observation history of the agent through ℎ ,  .Both recurrent models are implemented as Recurrent Neural Networks (RNNs) [24], and the embeddings are computed as hidden states of the RNNs.</p>
<p>Representation Model.The representation model learns the overall posterior distribution, which we factorise into global and agent posterior distributions over the global ( ,  ) and agent ( ,  ) latent states , as they are potentially easier to learn.We implement the posterior latent states as vectors of multiple categorical variables as in [13].</p>
<p>Transition Model.The role of the transition model is to predict future global and agent latent states without access to the environment global state and the agent observation that causes them.This way, the transition model can be used to generate synthetic trajectories for policy learning.Specifically, it learns the overall prior distribution over global (ẑ ,  ) and agent (ẑ ,  ) latent states, factorised into global and agent prior distributions, similar to the posterior.One key difference between the posterior latent states and the prior latent states is that   in a bottom-up fashion.This conditioning ensures a flow of information between the top and bottom levels of the latent-variable model, leading to a structured hierarchy of latent states.We design the representation model to enable inference of the agent posterior latent  ,  during execution without computing  ,  .The latent-variable model thus incorporates relevant global information without violating the CTDE paradigm.</p>
<p>Auxiliary Components.</p>
<p>The goal of our model is to generate synthetic trajectories for MARL training.At a minimum, to learn how to act using synthetic trajectories, MARL agents additionally require feedback in terms of reward and knowledge of whether a state is terminal [12,13].Furthermore, in some environments, the availability of actions changes at each timestep [7].As a result, we include auxiliary components in our model for predicting these values over trajectories.</p>
<p>The auxiliary components are implemented as neural networks, one each for predicting the reward, episode termination, and available actions at each timestep .We predict rewards using a reward predictor network that outputs a continuous value r  .Empirically, we find that feeding just  ,  and ℎ ,  to the reward predictor results in better overall performance.The termination predictor predicts whether the current state is terminal or not by outputting γ  , a binary value that is 1 if the episode terminates at time .The available action predictor predicts Â,  , a vector of size , each value of which denotes whether that action is available at time .Both the termination and available action predictors are implemented as Bernoulli distributions, and take as input,  ,  , </p>
<p>Training the Model</p>
<p>Having described the bi-level model architecture, we now explain the loss function used to train the model.We train all components of our model jointly with the loss L ( ).The loss is a sum of multiple terms.We write the total loss L ( ) as:
L (𝜓 ) = L ELBO + L r𝑡 + L γ𝑡 + L Â𝑡 + L â𝑡 .
The first term is the ELBO loss L ELBO , which trains the transition dynamics components to maximize the ELBO under the data generating distribution  (  1: |a 0: −1 ) using amortized variational inference.</p>
<p>We provide a detailed derivation of the ELBO in Appendix A. We write it as: ) .The first term in L ELBO corresponds to maximizing the log likelihood of the observations, given  ,  and ℎ ,  .The second and third terms together minimize the KL divergence (  ) between the overall prior and posterior distributions,   (.) and   (.).We have two KL divergence terms as we factorize the overall prior and posterior distributions into global and agent distributions.To ensure that the distributions are learnt effectively, we use KL balancing [13].
L ELBO = − 𝑇 ∑︁ 𝑡 =1 log 𝑝 𝜓 ( ô𝑖 𝑡 |𝑧 𝑎,
The remaining terms in L ( ) train the auxiliary components to maximize the log likelihoods of their corresponding targets, given the latent states from the representation model and the deterministic embeddings from the recurrent models.These are:
Reward: L r𝑡 = − 𝑇 ∑︁ 𝑡 =1 log 𝑝 𝜓 ( r𝑖 𝑡 |𝑧 𝑎,𝑖 𝑡 , ℎ 𝑎,𝑖 𝑡 ) Termination: L γ𝑡 = − 𝑇 ∑︁ 𝑡 =1 log 𝑝 𝜓 ( γ𝑖 𝑡 |𝑧 𝑎,</p>
<p>Learning Multi-Agent Behavior</p>
<p>A benefit of our method is that model learning is independent of the MARL algorithm used for policy learning.This allows us to use any off-the-shelf value-based or actor-critic MARL algorithm.In this section, we explain how we can train a generic actor-critic MARL algorithm using latent trajectories generated by our model.Each agent is equipped with a policy, or actor,    , which is implemented as a neural network with parameters  and trained to maximize the MARL objective.At timestep  of environment interaction, the agent receives as input its agent latent state ( ,  during execution and ẑ,  during training) and outputs an action:
𝑎 𝑖 𝑡 ∼ 𝜋 𝜃 (𝑎 𝑖 𝑡 |𝑧 𝑎,𝑖 𝑡 /ẑ 𝑎,𝑖 𝑡 , ℎ 𝑎,𝑖 𝑡 ). MABL infers 𝑧 𝑎,𝑖
 solely from its current observation and its agent embedding (ℎ ,  ), facilitating decentralized policy execution.Each agent is also equipped with a critic    that is represented by a neural network with parameters  and outputs an estimate V of the value function:
V 𝑖 𝑡 ∼ 𝑉 𝑖 𝜙 ( ẑ𝑖 𝑡 , ℎ 𝑎,𝑖 𝑡 , ℎ 𝑔,𝑖 𝑡 ).
As the critic is centralized, its input is a concatenation of the global latent state and the agent latent state, which we represent by ẑ  .As in [7], the critic includes a self-attention mechanism [39].The actor Algorithm 1 MABL: Learning a Multi-Agent Bi-Level World Model Collect an episode of environment data using   Compute MARL objective on D L 13:</p>
<p>Update   and   using the MARL objective 14:</p>
<p>end for 15: end for and critic network parameters are shared by all agents to facilitate faster training in tasks that involve large numbers of agents [46].</p>
<p>If we were to instead use a value-based MARL algorithm (e.g, Q-MIX [31]), for policy learning, each agent's critic can be represented by   ( ẑ,  , ℎ ,  ) and the global critic can be represented by:
𝑄 𝑡𝑜𝑡 (𝑄 1 ( ẑ𝑎,1 𝑡 , ℎ 𝑎,1 𝑡 ), ..., 𝑄 𝑛 ( ẑ𝑎,𝑛 𝑡 , ℎ 𝑎,𝑛 𝑡 ), ẑ𝑔,1 𝑡 , ..., ẑ𝑔,𝑛 𝑡 ).
We learn multi-agent behavior purely within the latent-variable model.By this, we mean that the trajectories used for training consist of latent states generated by the model.We now detail the iterative procedure [11,33] outlined in Algorithm 1 that we use for training MABL.</p>
<p>First, the MARL agents interact with the environment to collect real environment data (Lines 3 and 4).These trajectories are stored in the model buffer D to use for model training.Second, the bi-level model is trained using trajectories sampled from D (Lines 5-8).We then freeze the weights of the bi-level model in preparation for MARL training.</p>
<p>Third, the MARL training occurs using synthetic trajectories of length  generated by the model (Lines 9-14).Specifically, B  sequences in the form of states, observations, and previous joint actions are drawn from D (Line 10).From each tuple of state, observations, and previous joint actions in a sequence, corresponding global and agent latent states are then computed by the representation model.For  − 1 timesteps that follow, at each timestep, , the agents choose a joint action a  according to their policies.The transition model and recurrent models then predict the next latent states ẑ,  +1 and ẑ,  +1 .This process is repeated to generate trajectories of latent states of length  starting from each tuple.Then, the auxiliary components of the model take as input the necessary components to predict rewards, termination conditions, and available actions to generate latent trajectories D  of the form { , 1 ,  .Finally, the MARL algorithm is trained on D  (Lines 12 and 13) following the CTDE paradigm.We choose the popular actor-critic MARL algorithm Multi-Agent PPO (MAPPO) [46] for policy learning, as it has achieved strong results in various multi-agent tasks.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>In this section, we present an empirical study of the sample efficiency of MABL against state-of-the-art algorithms on three challenging benchmarks: SMAC [32], Flatland [26], and MAMuJoCo [30].First, we perform a comparative evaluation of MABL against other CTDE multi-agent latent-variable world models.On observing strong performance gains, we then ask whether MABL would perform similarly to or better than even CTCE multi-agent latent-variable world models whose agents have access to the observations of all other agents during execution.Since MABL is a CTDE method, it is at a natural disadvantage in such a comparison.We then perform ablation studies to examine the attributes of the bi-level model that lead to MABL's performance gains.In summary, our empirical analysis is aimed at answering the following questions: RQ1: Does MABL lead to better sample efficiency compared to the state-of-the-art CTDE multi-agent latent-variable world models?RQ2: Does MABL lead to comparable sample efficiency compared to the state-of-the-art CTCE multi-agent latent-variable world models?RQ3: Is the bi-level latent-variable model responsible for the improved sample efficiency?If so, what features of the model lead to these improvements?</p>
<p>Environments.We briefly describe the three benchmarks we use (SMAC, Flatland, and MAMuJoCo) and provide a detailed description in the Appendix B.3.For the SMAC benchmark, we conduct experiments on two Easy maps (2s vs 1sc and 3s vs 4z), one Hard map (3s vs 5z), and two Super Hard maps (Corridor and 3s5z vs 3s6z).We defer comparison plots for the 2s vs 1sc to Appendix B.4, as it is the easiest task across all environments and serves as a sanity check.</p>
<p>The Flatland benchmark is a discrete action-space 2D grid environment that simulates train traffic on a railway network.Each agent controls a train and receives a positive reward on reaching its destination and penalties for colliding with other agents or being late.We conduct experiments with the 5 and 10 agent variants.Both SMAC and Flatland have discrete action spaces.We also evaluate our algorithm on MAMuJoCo [30], a continuous multi-agent robotic control benchmark where each agent controls a portion of the joints that together control the robot.We conduct experiments on the 2-agent Humanoid and 2-agent Humanoid Standup environments.</p>
<p>Experimental Details.Because we aim to investigate improvements in sample efficiency, we adopt the low data regime established in prior work [7,12,15,42].In our experiments, we train each algorithm across 3 independent runs with the same number of environment steps.We ensure that each algorithm that uses world models also generates the same number of synthetic samples for training.In our comparisons, we use the same MARL algorithm, MAPPO, for policy learning.We detail the hyperparameters, neural network architecture, and implementation specifics in Appendix B and make our code available here.</p>
<p>Baselines.We compare MABL against state-of-the-art CTDE and CTCE baselines.The CTDE baselines are Dreamer-v2 [13] and MAPPO [46].Dreamer-v2 is a state-of-the-art single-agent modelbased RL algorithm, which we implement as a multi-agent algorithm Curves represent the mean over 3 independent runs, and shaded regions show the minimum and maximum scores.X axis shows the number of steps taken in the real environment; Y axis denotes the win-rate for SMAC, and the reward for Flatland and MAMuJoCo.Plots are smoothed using exponential moving average.MABL not only achieves better overall performance than the baselines, but does so more rapidly, underscoring its sample efficiency.</p>
<p>by learning a latent-variable world model independently for each agent.Including MAPPO trained solely on real environment data as a baseline serves to examine the effectiveness of our model-based baselines, all of which train MAPPO on synthetic latent trajectories.The CTCE baselines are MAMBA [7] and MAG [42], which are the current state-of-the-art in MARL with latent-variable world models.MAMBA [7] uses a transformer encoder block to aggregate observations of all agents to compute latent states for each agent, which are then input to agent policy networks.Because MAMBA requires the transformer block to compute latent states during execution, each agent requires access to the inputs of other agents during execution, resulting in centralized policy execution.MAG [42] is a recent work that builds upon MAMBA to achieve state-of-the-art sample efficiency on SMAC.MAG has one world model per agent, with a separate set of model parameters per agent.It uses the same model architecture as MAMBA, inheriting the CTCE property from it.The difference from MAMBA is that the world models are now trained jointly to interact with each other, taking into account the long-term joint effect of local predictions at each step to generate trajectories with lower cumulative errors [42].We provide a conceptual comparison of all model-based MARL algorithms we consider in Appendix B.5.</p>
<p>Performance Comparison: CTDE methods</p>
<p>We now compare MABL with CTDE baselines.In Figure 3, we present the performance of MABL and CTDE baselines on the entirety of the low data regime.In Table 1, we summarize the final performance of all algorithms over the last 15k environment steps.These results show that, except on the Easy 2s vs 1sc SMAC map, MABL consistently outperforms all CTDE baselines in terms of sample efficiency by large margins, answering RQ1.On the Easy 2s vs 1sc map, the win-rate of MABL is close to the best-performing baseline MAPPO while Dreamer-v2 fails to learn a good policy.In addition, MABL initially achieves a high win-rate of 80% nearly 2x faster than MAPPO on this task (Appendix B.4).The performance gains of MABL are significant in challenging SMAC environments such as Corridor and 3s5z vs 3s6z, as well as HumanoidStandup, demonstrating MABL's capability to handle complex tasks.The superior overall score and sample efficiency of MABL reveals the benefit of incorporating global information into the representation learning of latent-variable world models.</p>
<p>Performance Comparison: CTCE Methods</p>
<p>Given the surprisingly strong performance of MABL compared with CTDE baselines, we now compare MABL against state-ofthe-art CTCE methods.CTCE methods access global information at execution time, putting our method at a disadvantage.Should MABL perform comparably or better than CTCE methods, it would suggest that the bi-level model captures more pertinent information in its latent states for policy learning than existing state-of-the-art techniques.</p>
<p>We plot the performance of MABL compared to CTCE baselines in Figure 4. Surprisingly, from Table 1 and Figure 4, we observe that MABL achieves superior or comparable sample efficiency to both MAMBA and MAG on all benchmarks.MABL outperforms MAMBA on all environments except on the Easy 2s vs 1sc SMAC map and the Super Hard 3s5z vs 3s6z map.MABL and MAMBA achieve nearly a 100% win-rate on 2s vs 1sc.On 3s5z vs 3s6z, both algorithms achieve a win-rate of nearly 20% after just 450k environment steps.</p>
<p>We see a similar trend in performance when comparing MABL and MAG.MABL outperforms MAG on Flatland, MAMuJoCo, and all SMAC maps -except 2s vs 1sc, where both algorithms achieve nearly perfect performance, and the Super Hard Corridor map, where they exhibit similar levels of performance.MABL either matches or outperforms even state-of-the-art CTCE methods, answering RQ2 in the affirmative.A surprising observation is that the CTDE baselines perform better overall than the CTCE baselines on the 5-agent Flatland task.This result suggests that, in relatively less complex environments where each agent's observation has enough information to learn good behavior, simpler algorithms like Dreamer-v2 may excel in representation learning compared to MAMBA or MAG.The results also show that CTCE baselines perform more effective representation learning than the CTDE baselines on more complex environments.However, MABL demonstrates consistent high performance across all environments, pointing to improved representation learning compared to both CTDE and CTCE baselines.In the next section, we investigate this further through an ablation study.</p>
<p>Ablation Study</p>
<p>Given the performance of MABL compared to both CTDE and CTCE methods, we now seek to understand the attributes of MABL model that contribute most to these gains in sample efficiency.Suspecting that better representation learning using the bi-level model is responsible for these gains, we ablate two attributes of our model.</p>
<p>First, we ablate the global latent state.Our model learns a bi-level latent space and is capable of encoding global information into the upper-level global latent state.To understand the importance of the global latent state for policy learning, we remove the upper level.The model we obtain is the same as Dreamer-v2 and learns only the agent latent state with access to the observation of the agent.In our plots, we refer to this variant as Dreamer-v2.</p>
<p>Second, we suspect that learning one global latent state per agent enables better representation learning, as the model can more readily incorporate information relevant to each agent into its global latent state.To test this, we modify the upper level of the model such that we learn a single shared global latent state instead of  separate ones.Because each of the  lower-level agent latent states is conditioned on the single global latent, the model cannot encode relevant global information for each agent into the agent's respective global latent.We call the resulting variant MABL-SG (Single Global).</p>
<p>We perform ablation studies on one SMAC map from each level of difficulty: 3s vs 4z (Easy), 3s vs 5z (Hard), and Corridor (Super Hard).We visualize the training curves in Figure 5. On all maps, MABL achieves greater sample efficiency than the two variants.Because Dreamer-v2 exhibits the lowest performance overall, we believe that the bi-level model's ability to incorporate global information through the global latent state is most responsible for MABL's gains in sample efficiency.The decreased performance of MABL-SG compared to MABL supports our hypothesis that learning per-agent global latents, as opposed to a single shared global latent, improves policy learning.We suspect that the use of a single, shared global latent facilitates the encoding of noisy and irrelevant information, which impedes performance.Taken together, the superior performance of MABL highlights the crucial role of the bi-level model.In particular, these experiments suggest that the bi-level model successfully incorporates relevant global information into the global latent and learns a structured hierarchy of latent states, answering RQ3.</p>
<p>RELATED WORK</p>
<p>The majority of work in MARL focuses on the model-free setting [8,22,28].Despite their impressive performance, model-free MARL algorithms often suffer from a high sample complexity.Several approaches have been developed to address this issue.One line of work [31,34,36,45] uses the insight of value decomposition [5]: value functions can be decomposed into simpler functions which can be learned more easily and (relatively) independently.They can then be recombined to approximate the original, more complex value function.Another line of work focuses on actor-critic methods [8,22,30,46] that learn a centralized critic conditioned on global state and joint action to reduce non-stationarity and improve sample efficiency.Because we can use any model-free MARL algorithm for learning multi-agent policies in latent space, these techniques are complementary to our contributions.</p>
<p>In MARL, latent-variable models have shown promise in learning the reward function in inverse RL [10] and representations of competing agents' strategies [43].We focus on latent-variable world models in MARL.While prior work [19] uses a multi-step latentvariable world model in 2-player games to predict future joint observations and actions, it is only applicable in 2-agent scenarios, does not generate synthetic data, and does not follow the CTDE paradigm.Only recently have latent-variable world models been used to learn environment dynamics in Markov games to improve sample efficiency [7,42].These approaches are based on direct extensions of single-agent methods that use latent-variable world models [13].The dynamics of each agent is learned as though the agent is in a POMDP, and deep learning architectures, specifically, the transformer [21,39], aggregate latent states from all agents to reduce non-stationarity and model errors [7,42].</p>
<p>However, latent-variable models designed this way cannot be used for decentralized execution, as they require access to all observations to predict latent states.They also cannot incorporate any additional global information available during training In multi-agent tasks, encoding global information, such as the global state in SMAC, for example, is crucial to learn successful behaviors from latent states.</p>
<p>CONCLUSION</p>
<p>We presented a novel model-based MARL algorithm, MABL, that learns policies purely using latent trajectories generated by a bi-level latent-variable world model.Our model effectively learns environment dynamics in multi-agent tasks by factorizing the latent space into a high-level global latent state and a low-level agent latent state.We evaluated MABL across a variety of tasks in SMAC, Flatland and MAMuJoCo.MABL, which is a CTDE method, greatly outperforms state-of-the-art CTDE baselines in sample efficiency on all environments except for the simplest SMAC map, 2s vs 1sc.MABL either outperforms or performs similarly to even state-of-the-art CTCE baselines in sample efficiency across all environments.While we achieve gains in sample efficiency, the learned latent states are not interpretable.To deploy our method in a real-world scenario, future work should involve improving representation learning to achieve interpretability of the latent space.</p>
<p>A DERIVATION OF ELBO</p>
<p>In this section, we discuss how we obtain the ELBO for training the latent-variable model.For a trajectory length of  , we write the joint distribution of data as:
𝑝 (𝑜 𝑖 1:𝑇 , 𝑧
We approximate our posterior distribution as:</p>
<p>𝑞(𝑧</p>
<p>Using Equations ( 1) and ( 2) and applying Jensen's inequality, we obtain the ELBO as follows:</p>
<p>log  (</p>
<p>B EXPERIMENTAL DETAILS</p>
<p>Here, we provide the implementation details, including architectural and hyperparameter choices.We also describe the environments in more detail.To implement SMAC and MAMuJoCo experiments, we use a system with AMD EPYC 7453 CPUs and an NVIDIA RTX A6000 GPU.Flatland experiments were run on a system with Intel(R) Xeon(R) Platinum 8268 CPUs and an Nvidia A-100 GPU.</p>
<p>B.1 Hyperparameters</p>
<p>Latent-Variable Model.We implemented all components of the model as full connected neural networks with ReLU activations.The specific hyperparameters used for SMAC, Flatland, and MAMuJoCo are listed in Table 2 We implemented the shared actor and critic as fully connected neural networks with ReLU activations.For the Flatland environment, we observed that feeding just the global latent state as input to the critic network yielded better results, as opposed to a concatenation of global and agent latent states.The hyperparameters used for SMAC, Flatland and MAMuJoCo are listed in Table 6.</p>
<p>Learning Rates.We report learning rates for the actor, critic and model, for each task in</p>
<p>B.2 Implementation Details</p>
<p>In our SMAC experiments, we use the official code implementation of MAMBA, MAG and the official implementation of MAPPO for the model-free version of MAPPO.In our Flatland experiments, we use the official MAMBA code and re-implemented MAPPO to integrate it with the MAMBA codebase which supports the Flatland environment.We based our code on the official MAMBA implementation for fair and convenient comparison.</p>
<p>B.3 Environments</p>
<p>SMAC.We use the default environment settings provided by the authors.We also describe the maps we conduct experiments on:</p>
<p>Easy Maps: We use the 2s vs 1sc map and the 3s vs 4z map.In 2s vs 1sc, the ally units consist of 2 Stalkers who must team up against a single enemy unit: a Spine Crawler.In 3s vs 4z, the ally units consist of 3 Stalkers, battling against 4 Zealots.Hard Maps: We use the 3s vs 5z map in which 3 Stalkers face 5 Zealots.To gain a victory, the 3 allied Stalkers have to make enemy units give chase, while at the same time ensuring that they keep enough distance from them to not incur critical damage.This strategy is called kiting [32] and requires precise control and team coordination.Super Hard Maps: We use the Corridor and 3s5z vs 3s6z maps.In the corridor map, 6 allied Zealots face 24 enemy Zerglings.To win, agents have to make use of the terrain features of the map, collectively blocking a narrow region of the map to block enemy attacks from all directions.The 3s5z vs 3s6z map is an assymteric scenario with different kinds of allied units, which makes learning successful team behavior extremely difficult.</p>
<p>Flatland.We use the classic [7] environment settings for Flatland, as shown in Table 5.</p>
<p>MAMuJoCo.MAMuJoCo is a multi-agent version of the popular continuous single-agent robotic control benchmark MuJoCo [38].A multi-agent system for a MAMuJoCo task is created by representing the given robot as a body graph where joints, which are represented by vertices are connected to each other by body segments, represented by edges.The body graph is then split into disjoint subgraphs.Each subgraph represents an agent and contains one or more controllable joints.An agent's action space is given by the joint vector of all actuators that can be controlled by it to move the joints.Each agent observes the positions of its body parts, and in our experiments, cannot observe the positions of the body parts of other agents.We conduct experiments on the Humanoid and Humanoid Standup tasks [30].The Humanoid task consists of two agents, one controlling the upper body joints and one controlling the lower body joints below the torse of a Humanoid robot.Humanoid Standup consists of two agents, each of which controls the joints of one leg of a humanoid robot.In both tasks, the goal is to maximize +ve speed in the x direction.We limit episode length to a maximum of 1000 timesteps, for both tasks.We also use the default environment settings used in [30] and provided in the MAMuJoCo Github repository.</p>
<p>B.4 2s vs 1sc Results</p>
<p>Figure 1 :
1
Figure 1: Overview of how the MABL bi-level model encodes global information into the global latent state while training while informing the agent latent state (left), which is computed from the agent's observation during execution (right).</p>
<p>At timestep , agent  ∈  receives an observation    governed by the observation function   () :  → O  , and chooses an action    ∈   .Given the current state   and the agents' joint action a  = {   }  =1 , the environment transitions to the next state   +1 according to the state transition function  (  +1 |  , a  ) : ×A× → [0, 1].Each agent then receives a reward    according to its reward function   : ×  → R. Each agent takes actions according to its policy   (   |   ), which is conditioned on its action-observation history    .Together, these policies comprise the joint policy , which induces the action-value function for each agent ,   </p>
<p>Figure 2 :
2
Figure 2: Transition dynamics components of the bi-level latentvariable model.Shaded circled nodes represent inputs, unshaded circled nodes represent random variables, and square nodes represent deterministic embeddings.The transition model and recurrent models are shown using black arrows; the representation model is shown using blue arrows.</p>
<ol>
<li>1 . 1
11
Transition Dynamics Components.We first describe the Transition Dynamics components, as illustrated in Figure 2.They are the recurrent models, the representation model, the transition model, and the observation model: Recurrent Models:</li>
</ol>
<p>5 : 7 : 9 : 11 :
57911
for  ∈  model training steps do ⊲ Model Training 6: Draw B M sequences uniformly from D Train model   on B M via loss L ( ) for  ∈  policy learning steps do ⊲ MARL Training 10: Draw B R sequences uniformly from D Generate latent trajectories D L from B R using   ,   12:</p>
<p>Figure 3 :
3
Figure 3: Comparisons against CTDE baselines across all environments.Curves represent the mean over 3 independent runs, and shaded regions show the minimum and maximum scores.X axis shows the number of steps taken in the real environment; Y axis denotes the win-rate for SMAC, and the reward for Flatland and MAMuJoCo.Plots are smoothed using exponential moving average.MABL not only achieves better overall performance than the baselines, but does so more rapidly, underscoring its sample efficiency.</p>
<p>Figure 4 :
4
Figure 4: Comparisons against CTCE baselines across all environments.Curves represent the mean over 3 independent runs, and shaded regions show the minimum and maximum scores.X axis shows the number of steps taken in the real environment; Y axis denotes the win-rate for SMAC and the reward for Flatland and MAMuJoCo.Plots are smoothed using exponential moving average.MABL either matches or outperforms the CTCE baselines across all environments.</p>
<p>Figure 5 :
5
Figure 5: Training curves of ablation studies.X axis denotes the number of environment steps.Plots are smoothed using an exponential moving average.Shaded regions show the maximum and minimum win-rate.MABL outperforms all ablations, implicating the crucial role of the bi-level model.</p>
<p>Figure 6 :
6
Figure 6: Comparisons against CTDE baselines on 2s vs 1sc</p>
<p>Figure 7 :
7
Figure 7: Comparisons against CTCE baselines on 2s vs 1sc</p>
<p>,  is conditioned on   and  ,  is conditioned on    .As with the representation model, the prior latent states are vectors of multiple categorical variables.Benefits of Bi-Level Structure.We now explain the benefits of having a bi-level structure.At each timestep , the agent prior latent state ẑ,  is conditioned on the global prior latent state ẑ,  in a topdown fashion.Simultaneously, the global posterior latent state  ,  is conditioned on the agent prior latent state  ,</p>
<p>Observation Model.The observation model outputs a prediction of the current observation ô  , given the agent embedding ℎ ,  and the agent posterior latent state  ,  .It is required to train the bi-level model using amortized variational inference.</p>
<p>1 :
1
Initialize shared actor   , critic   , bi-level model   and model buffer D 2: for  ∈  episodes do ⊲ Environment Interaction
3:</p>
<p>Table 1 :
1
Comparison of the average win-rate (% for SMAC)/reward, and standard deviation in win-rate/reward over the last 15k environment steps across environments.Numbers in bold indicate the highest mean performance among all CTDE methods.Except on the Easy 2s vs 1sc map, MABL outperforms CTDE baselines on all environments.MABL also either outperforms or performs similarly to the best CTCE baseline on all tasks.
Overall,</p>
<p>Table 2 :
2
. Hyperparameters for training the model MARL algorithm.
HyperparameterFlatland SMAC MAMuJoCoNumber of epochs406060Number of sampled rollouts324040Sequence length502020Rollout Horizon151515Buffer Size5e52.5e52.5e5Number of categoricals (local &amp; global) 323232Number of classes (local &amp; global)323232KL balancing entropy weight0.20.20.2KL balancing cross entropy weight0.80.80.8Trajectories between updates111Hidden layer size400256256Number of hidden layers222Gradient clipping norm100100100</p>
<p>Table 4 .
4HyperparameterFlatland SMACMA-MujocoBatch size200020002000GAE 𝜆0.950.950.95Entropy coefficient0.0010.0010.001Entropy annealing0.99998 0.99998 0.99998Number of updates444Epochs per update555Gradient clipping norm100100100Discount factor 𝛾0.990.990.99Trajectories between updates 111Hidden size400256256</p>
<p>Table 3 :
3
Hyperparameters for training the MARL algorithm
EnvironmentActor Critic ModelFlatland: 5 agents5e-45e-42e-4Flatland: 10 agents5e-45e-42e-4MAMuJoCo: Humanoid3e-41e-51e-5MAMuJoCo: HumanoidStandup 3e-40.5e-5 0.5e-5SMAC: 2s vs 1sc1e-41e-33e-4SMAC: 3s vs 4z1e-41e-33e-4SMAC: 3s vs 5z1e-41e-35e-4SMAC: Corridor1e-41e-33e-4SMAC: 3s5z vs 3s6z1e-41e-33e-4</p>
<p>Table 4 :
4
Learning rates for all benchmarks and all environments</p>
<p>Table 5 :
5
Flatland environment parameters
Parameter5 agents 10 agentsHeight3535Width3535Number of cities34Grid distribution of citiesFalseFalseMaximum number of rails between cities 22Maximum number of rails in cities44Malfunction rate1/1001/150</p>
<p>Table 6 :
6
B.5 Conceptual Comparison: Multi-Agent Latent Variable World Models Conceptual comparison of MABL, model-based CTDE and CTCE baselines
AlgorithmGlobal information asGlobal information other than/in addition Decentralized execution Shared model parametersobservations of all agentsto observations of all agentsMABLDreamer-v2✗✗MAMBA✗✗MAG✗✗✗
ACKNOWLEDGEMENTSThis research was supported in part by NSF IIS-2046640 (CAREER).We thank NVIDIA for providing computing resources.We thank Robert Bosch Center for Data Science and AI for supporting author Aravind Venugopal's Post-Baccalaureate Fellowship for part of the duration of this work.We thank Rex Chen for his contributions towards setting up the computational resources for the experiments.
A multiagent approach to managing air traffic flow. Adrian K Agogino, Kagan Tumer, Autonomous Agents and Multi-Agent Systems. 242012. 2012</p>
<p>Autonomous helicopter control using reinforcement learning policy search methods. Andrew Bagnell, Jeff G Schneider, Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164). 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)IEEE20012</p>
<p>Acting optimally in partially observable stochastic domains. Leslie Pack Anthony R Cassandra, Kaelbling, Michael L Littman, Aaai. 199494</p>
<p>Efficient model-based deep reinforcement learning with variational state tabulation. Dane Corneil, Wulfram Gerstner, Johanni Brea, International Conference on Machine Learning. PMLR2018</p>
<p>Hierarchical reinforcement learning with the MAXQ value function decomposition. Thomas G Dietterich, Journal of artificial intelligence research. 132000. 2000</p>
<p>Multi-agent reinforcement learning for autonomous vehicles: A survey. Joris Dinneweth, Abderrahmane Boubezoul, René Mandiau, Stéphane Espié, Autonomous Intelligent Systems. 2272022. 2022</p>
<p>Vladimir Egorov, Aleksei Shpilman, arXiv:2205.15023Scalable Multi-Agent Model-Based Reinforcement Learning. 2022. 2022arXiv preprint</p>
<p>Counterfactual multi-agent policy gradients. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, Shimon Whiteson, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Multi-agent deep reinforcement learning: a survey. Sven Gronauer, Klaus Diepold, Artificial Intelligence Review. 2022. 2022</p>
<p>Multi-agent adversarial inverse reinforcement learning with latent variables. Nate Gruver, Jiaming Song, J Mykel, Stefano Kochenderfer, Ermon, Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems. the 19th International Conference on Autonomous Agents and MultiAgent Systems2020</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018. 2018World models. arXiv preprint</p>
<p>Dream to Control: Learning Behaviors by Latent Imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, International Conference on Learning Representations. 2019</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.021932020. 2020arXiv preprint</p>
<p>When to Trust Your Model: Model-Based Policy Optimization. Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine, Advances in Neural Information Processing Systems. 322019. 2019</p>
<p>Model-based reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, arXiv:1903.003742019. 2019arXiv preprint</p>
<p>Learning dynamics model in reinforcement learning by incorporating the long term future. Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, Dhruv Batra, arXiv:1903.015992019. 2019arXiv preprint</p>
<p>P Diederik, Max Kingma, Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013. 2013arXiv preprint</p>
<p>Multi-agent reinforcement learning for dynamic ocean monitoring by a swarm of buoys. Maryam Kouzehgar, Malika Meghjani, Roland Bouffanais, Global Oceans. Singapore-US Gulf Coast. IEEE2020. 2020</p>
<p>Multi-agent reinforcement learning with multi-step generative models. Orr Krupnik, Igor Mordatch, Aviv Tamar, Conference on Robot Learning. PMLR. 2020</p>
<p>Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model. Alex Lee, Anusha Nagabandi, Pieter Abbeel, Sergey Levine, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>A survey of transformers. Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu, AI Open. 2022. 2022</p>
<p>Multi-agent actor-critic for mixed cooperative-competitive environments. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch, arXiv:1706.022752017. 2017arXiv preprint</p>
<p>Coordinated multi-robot exploration under communication constraints using decentralized markov decision processes. Laëtitia Matignon, Laurent Jeanpierre, Abdel-Illah Mouaddib, Twenty-sixth AAAI conference on artificial intelligence. 2012</p>
<p>Larry Medsker, C Lakhmi, Jain, Recurrent neural networks: design and applications. CRC press1999</p>
<p>Model-based reinforcement learning: A survey. Joost Thomas M Moerland, Aske Broekens, Catholijn M Plaat, Jonker, Foundations and Trends® in Machine Learning. 162023. 2023</p>
<p>Sharada Mohanty, Erik Nygren, Florian Laurent, Manuel Schneider, Christian Scheller, Nilabha Bhattacharya, Jeremy Watson, Adrian Egli, Christian Eichenberger, Christian Baumberger, arXiv:2012.05893Flatland-RL: Multi-agent reinforcement learning on trains. 2020. 2020arXiv preprint</p>
<p>State of the art-a survey of partially observable Markov decision processes: theory, models, and algorithms. George E Monahan, Management science. 281982. 1982</p>
<p>Emergent social learning via multi-agent reinforcement learning. Douglas Kamal K Ndousse, Sergey Eck, Natasha Levine, Jaques, International Conference on Machine Learning. PMLR2021</p>
<p>Dealing with non-stationarity in multi-agent deep reinforcement learning. Georgios Papoudakis, Filippos Christianos, Arrasy Rahman, Stefano V Albrecht, arXiv:1906.047372019. 2019arXiv preprint</p>
<p>Bei Peng, Tabish Rashid, Christian A Schroeder De Witt, Pierre-Alexandre Kamienny, Wendelin Philip Hs Torr, Shimon Böhmer, Whiteson, arXiv:2003.06709FACMAC: Factored Multi-Agent Centralised Policy Gradients. 2020. 2020arXiv preprint</p>
<p>Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, Shimon Whiteson, International Conference on Machine Learning. PMLR2018</p>
<p>Tabish Mikayel Samvelyan, Christian Rashid, Schroeder De, Gregory Witt, Nantas Farquhar, Tim Gj Nardelli, Chia-Man Rudner, Hung, Jakob Philip Hs Torr, Shimon Foerster, Whiteson, arXiv:1902.04043The starcraft multi-agent challenge. 2019. 2019arXiv preprint</p>
<p>Curious model-building control systems. Jürgen Schmidhuber, Proc. international joint conference on neural networks. international joint conference on neural networks1991</p>
<p>Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, Yung Yi, International conference on machine learning. PMLR2019</p>
<p>Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Conference on learning theory. PMLR2019</p>
<p>Value-decomposition networks for cooperative multi-agent learning. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, arXiv:1706.052962017. 2017arXiv preprint</p>
<p>Dyna, an integrated architecture for learning, planning, and reacting. Richard S Sutton, ACM Sigart Bulletin. 21991. 1991</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IEEE. 2012. 2012IEEE</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba, arXiv:1907.02057Benchmarking model-based reinforcement learning. 2019. 2019arXiv preprint</p>
<p>. Xihuai Wang, Zhicheng Zhang, Weinan Zhang, arXiv:2203.106032022. 2022Model-based Multiagent Reinforcement Learning: Recent Progress and Prospects. arXiv preprint</p>
<p>Zifan Wu, Chao Yu, Chen Chen, Jianye Hao, Hankz Hankui, Zhuo , arXiv:2303.17984Models as Agents: Optimizing Multi-Step Predictions of Interactive Local Models in Model-Based Multi-Agent Reinforcement Learning. 2023. 2023arXiv preprint</p>
<p>Learning Latent Representations to Influence Multi-Agent Interaction. Annie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh, Conference on Robot Learning. 2020</p>
<p>A multi-agent reinforcement learning-based data-driven method for home energy management. Xu Xu, Youwei Jia, Yan Xu, Zhao Xu, Songjian Chai, Chun Sing, Lai , IEEE Transactions on Smart Grid. 112020. 2020</p>
<p>Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, Hongyao Tang, arXiv:2002.03939Qatten: A general framework for cooperative multiagent reinforcement learning. 2020. 2020arXiv preprint</p>
<p>Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, Yi Wu, arXiv:2103.01955The surprising effectiveness of ppo in cooperative, multi-agent games. 2021. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
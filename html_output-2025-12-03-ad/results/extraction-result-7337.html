<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-272367220</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.00556v1.pdf" target="_blank">FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Automatic image anomaly detection is important for quality inspection in the manufacturing industry. The usual unsupervised anomaly detection approach is to train a model for each object class using a dataset of normal samples. However, a more realistic problem is zero-/few-shot anomaly detection where zero or only a few normal samples are available. This makes the training of object-specific models challenging. Recently, large foundation vision-language models have shown strong zero-shot performance in various downstream tasks. While these models have learned complex relationships between vision and language, they are not specifically designed for the tasks of anomaly detection. In this paper, we propose the Few-shot/zero-shot Anomaly Detection Engine (FADE) which leverages the vision-language CLIP model and adjusts it for the purpose of industrial anomaly detection. Specifically, we improve language-guided anomaly segmentation 1) by adapting CLIP to extract multi-scale image patch embeddings that are better aligned with language and 2) by automatically generating an ensemble of text prompts related to industrial anomaly detection. 3) We use additional vision-based guidance from the query and reference images to further improve both zero-shot and few-shot anomaly detection. On the MVTec-AD (and VisA) dataset, FADE outperforms other state-of-the-art methods in anomaly segmentation with pixel-AUROC of 89.6% (91.5%) in zero-shot and 95.4% (97.5%) in 1-normal-shot. Code is available at https://github.com/BMVC-FADE/BMVC-FADE.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7337",
    "paper_id": "paper-272367220",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0051909999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model
31 Aug 2024</p>
<p>Yuanwei Li yuanwei_li@hotmail.com 
Elizaveta Ivanova lisa.ivanova@entrust.com 
Martins Bruveris martins.bruveris@entrust.com 
FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model
31 Aug 202485B32B01742F89CD5F2B9B94F7CE7D43arXiv:2409.00556v1[cs.CV]
Automatic image anomaly detection is important for quality inspection in the manufacturing industry.The usual unsupervised anomaly detection approach is to train a model for each object class using a dataset of normal samples.However, a more realistic problem is zero-/few-shot anomaly detection where zero or only a few normal samples are available.This makes the training of object-specific models challenging.Recently, large foundation vision-language models have shown strong zero-shot performance in various downstream tasks.While these models have learned complex relationships between vision and language, they are not specifically designed for the tasks of anomaly detection.In this paper, we propose the Few-shot/zero-shot Anomaly Detection Engine (FADE) which leverages the vision-language CLIP model and adjusts it for the purpose of industrial anomaly detection.Specifically, we improve languageguided anomaly segmentation 1) by adapting CLIP to extract multi-scale image patch embeddings that are better aligned with language and 2) by automatically generating an ensemble of text prompts related to industrial anomaly detection.3) We use additional vision-based guidance from the query and reference images to further improve both zero-shot and few-shot anomaly detection.On the MVTec-AD (and VisA) dataset, FADE outperforms other state-of-the-art methods in anomaly segmentation with pixel-AUROC of 89.6% (91.5%) in zero-shot and 95.4% (97.5%) in 1-normal-shot.Code is available at https://github.com/BMVC-FADE/BMVC-FADE.</p>
<p>Introduction</p>
<p>Industrial image anomaly detection involves anomaly classification (AC) and anomaly segmentation (AS) which aim to identify and localise anomalies occurring on objects found in manufacturing industries.Most of the current research focuses on unsupervised anomaly detection where only normal samples are used during training while both normal and anomalous samples are evaluated during inference [2,6,8,12,18,19].Some of these approaches are able to achieve high performance when a large number of normal training samples is available.But this is not a realistic setting for industrial anomaly detection for two reasons.</p>
<p>First, the number of normal training samples can be scarce or unavailable.Second, one specific model needs to be trained for each object class and this quickly becomes unscalable when the number of object classes increases.Zero-shot/few-normal-shot anomaly detection is a more realistic setting with either zero or only a few normal reference images available.However, many previous methods designed for unsupervised anomaly detection perform poorly under this low-data regime.There are increasingly more research works focusing on this area recently [4,9,10,22,24] but there is still much room for improvement.</p>
<p>Vision foundation models trained on large image datasets have recently shown superb zero-shot capability on various computer vision tasks such as image classification, object detection and segmentation [11,16,17].There is great potential to leverage and transfer the knowledge and representations learned in these foundation models for the zero-/fewshot anomaly detection task.But these foundation models are not specifically designed and trained for anomaly detection.As such, we propose the Few-shot/zero-shot Anomaly Detection Engine (FADE) that utilises one of these foundation models, the CLIP model [17], and adapt it for the purpose of industrial AC and AS.</p>
<p>CLIP is a visual-language model pretrained on large datasets of image-text pairs using a contrastive loss.CLIP learns concepts and representations that capture the relationship between language and image.Without any finetuning, the model has demonstrated zeroshot capability in various downstream tasks such as image classification.This is done by constructing free-form text prompts as classification labels which enable the use of natural language to extract relevant information already learned by the CLIP model.This allows CLIP to be a suitable candidate for handling the anomaly detection task.The concept of anomaly and normality is broad and abstract.But language allows us to describe a specific anomaly precisely (E.g. a cracked object, a scratched surface, a missing component).CLIP can leverage the power of language to better capture the concept of anomaly and hence improve upon zero-shot anomaly detection performance.However, this requires careful prompt engineering and some methods [10] manually design an ensemble of text prompts with key words such as "damage" and "defect" which can be a time-consuming process.As such, we propose to utilise existing Large Language Model (LLM) to automatically generate text prompts related to the concept of normality and anomaly.</p>
<p>There is another challenge associated with language-guide anomaly detection.ViT-based CLIP model is trained by aligning the image-level CLS token embeddings with the text embeddings.This works for image-level anomaly classification but not pixel-level anomaly segmentation.This is because language-guided segmentation requires the comparison between the image patch embeddings and the text embeddings.However, the patch embeddings are not aligned to natural language during training.This results in sub-optimal segmentation performance [3,10,15].To address this issue, we propose to apply the Grounding Everything Module (GEM) [3] to extract the image patch embeddings which are shown to have better alignment with language and perform better in zero-shot segmentation.</p>
<p>While CLIP can be used for language-driven anomaly detection, we can also only use its image encoder for vision-based anomaly detection.In this case, visual representations of image patches extracted from both query and reference images are compared to one another to identify inconsistencies and anomalies.This enhances zero-shot performance and extends the approach to the few-shot setting when normal reference images are available.</p>
<p>In summary, we propose FADE which uses pretrained CLIP model for zero-/few-shot anomaly detection without any further training or fine-tuning.Our main contributions are:</p>
<p>• We utilise GEM patch embeddings which are better aligned with language to improve zero-shot language-guided AS.In addition, we adopt a multi-scale approach to make the method robust at detecting anomalies of different sizes.</p>
<p>• We further improve language-guided anomaly detection by using an LLM to automatically generate a prompt ensemble that captures the concept of normality and anomaly using a diverse set of text prompts which are related to industrial anomaly inspection.</p>
<p>• We enhance anomaly detection performance by employing a vision-guided approach that can be applied to both zero-and few-shot settings.</p>
<p>• FADE shows competitive results on the MVTec-AD and VisA benchmarks for AC and AS under both the zero-shot and few-normal-shot regimes.</p>
<p>Related Work</p>
<p>Anomaly detection: Most research has focused on unsupervised anomaly detection where a model is trained on many normal training samples [2,6,8,12,18,19].Among them, Patch-Core [18], a memory bank based method, has achieved state-of-the-art performance.There has been a growing interest in zero-/few-shot anomaly detection since the performance of the above methods decreases under such regimes.Earlier work such as Metaformer [22] addresses the problem in a meta-learning framework.RegAD [9] detects anomalies by comparing registered features of a test image with the normal reference images.However, both methods require model training using some datasets.A recent work, Segment Any Anomaly (SAA) [4], leverages a foundation segmentation model SAM [11] for zero-shot AS.</p>
<p>Anomaly detection with CLIP: Recently, CLIP has shown impressive language-driven zero-shot capability in various computer vision tasks [17].WinCLIP [10] is one of the first to use CLIP for language-guided zero-/few-shot anomaly detection.The paper adapts CLIP for pixel-wise AS by extracting dense CLIP features based on overlapping windows.Subsequent papers APRIL-GAN [5] and AnomalyCLIP [24] build upon the general framework of WinCLIP and introduce additional learnable layers and learnable prompts respectively to fine-tune the model for AS.ClipSAM [13] leverages CLIP to obtain a rough anomaly segmentation mask from which points and bounding boxes are then extracted and used by SAM as input prompts to further refine the segmentation.However, all the above methods use auxiliary datasets with ground truth anomaly segmentation masks for training.Other methods explore prompting in detail.[14] constructs manual and learnable normal and anomaly prompts and proposes a one-class prompt-learning method for few-shot detection.[21] augments normal and anomalous text prompts by inserting random words and uses their CLIP text embeddings to train a feed-forward network for zero-shot AC.Our method, unlike most of the above works, does not require further training or fine-tuning.</p>
<p>Zero-shot segmentation with CLIP: While CLIP shows strong zero-shot image classification capability, there are challenges to adapt it for dense pixel-wise segmentation.The main problem is that CLIP training does not directly optimise for alignment between language and local image patch embeddings [10].In addition, empirical segmentation results show that CLIP tends to generate opposite visualisation between foreground/background and also produce noisy activations [15].Several works have attempted to tackle these issues by adapting the CLIP model architecture for zero-shot segmentation [3,15,23].CLIP Surgery [15] replaces the query-key attention in the original transformer block with a value-value attention and also removes the feed-forward network.[3] proposes GEM blocks which further generalises the idea to self-self attention.In our work, we apply the GEM blocks to the task of anomaly segmentation.</p>
<p>Method</p>
<p>FADE conducts both anomaly classification (AC) and anomaly segmentation (AS) under the zero-/few-shot settings.It consists of four different anomaly detection pipelines that are used under different cases (Fig. 2a-d).The detection pipeline can be either language-or visionguided.Language-guided detection utilises the language capability of the CLIP model to perform zero-shot AC (Fig. 2a) and AS (Fig. 2b).On the other hand, vision-guided detection employs visual cues from images to further improve zero-shot AS (Fig. 2c) and extends the approach to few-shot detection (Fig. 2d).</p>
<p>Language-Guided Anomaly Classification</p>
<p>Zero-shot language-guided AC (Fig. 2a) is carried out as described in WinCLIP [10].A prompt ensemble t = {t + ,t − } is manually crafted which captures the notion of object anomaly or normality (Appendix A of [10]) where t + and t − are the sets of 88 anomalous and 154 normal text prompts respectively.An example of a normal prompt is "a cropped photo of the flawless [o]" while an anomalous prompt can be "a cropped photo of the damaged [o]" where [o] is the object label that comes with the dataset (E.g.bottle).The CLIP text encoder g extracts the text embeddings of all the prompts.The average text embeddings h for the anomalous and normal text prompts are computed as
h + = 1 N t + ∑ t∈t + g(t) and h − = 1 N t − ∑ t∈t − g(t)
where N t + and N t − are the number of anomalous and normal prompts respectively.Given an image x and the CLIP image encoder f clip , we compute its image embeddings f clip cls (x) where cls indicates the CLS token of the ViT-based CLIP model.The cosine similarity ⟨ f clip cls (x), h⟩ then gives a measure of how close the image is to the concept of normality or anomaly as encapsulated by the text prompts.Binary</p>
<p>Memory Memory Memory</p>
<p>Give 100 prompts that describe a photo of an undamaged/a defective object or texture in industrial manufacturing.Do not mention specific object name.Be diverse in adjectives and sentence structure.Give your outputs in JSON format as a list.</p>
<p>Instruction Normal Response</p>
<p>An immaculate, unblemished object with perfect symmetry.A sleek, refined surface devoid of any imperfections.A seamless, faultless texture with uniform consistency.A shiny, lustrous surface radiating with perfection.A flawless, polished object with a flawless finish...</p>
<p>Anomaly Response</p>
<p>A jagged, fragmented surface in a manufacturing setting.An irregular, bumpy texture with visible cracks.A warped, distorted object with uneven edges.A scratched, scuffed material displaying signs of wear.A broken, fragmented object with missing pieces... zero-shot AC is performed by calculating a language-guided anomaly score:
s lang = exp ⟨ f clip cls (x), h + ⟩/τ exp ⟨ f clip cls (x), h + ⟩/τ + exp ⟨ f clip cls (x), h − ⟩/τ (1)
where τ is a temperature parameter fixed at 0.01.</p>
<p>Language-Guided Anomaly Segmentation</p>
<p>GEM embeddings: Given an image of size h × w, AS generates a dense pixel-level prediction M ∈ [0, 1] h×w which localises the anomalous regions.The above framework for AC can be extended to zero-shot language-guided AS (Fig. 2b).Specifically, the patch embeddings { f clip p (x)} p∈Patches from the last transformer block of the CLIP image encoder are extracted to replace the CLS embeddings f clip cls (x) in Eq. 1, where Patches is a set of all patches extracted from an image x.This enables anomaly prediction at each location of a patch p that can be spatially combined to form a dense segmentation map.However, this approach yields poor results [3,10] since CLIP is trained using an image-level contrastive loss that aligns the image-level CLS embeddings with the text embeddings.Hence, CLIP has poor alignment between its patch embeddings and the text embeddings, resulting in its failure to generalise its zero-shot capability to the dense segmentation task.WinCLIP [10] solves this issue by introducing a grid of overlapping windows where each window masks out the image content outside of the window and computes the CLS embeddings of the content inside the window.This produces a dense segmentation while maintaining the visual-language alignment.</p>
<p>We take a different approach based on the Grounding Everything Module (GEM) [3] that has demonstrated success in zero-shot open-vocabulary object localisation.GEM uses the idea of self-self attention which computes query-query, key-key and valuevalue attention weights instead of the query-key attention weights in the conventional self-attention mechanism.Mathematically, self-self attention weights are given by: Attn sel f -sel f = softmax h clip Patches W • (h clip Patches W ) T where h clip Patches ∈ R N×d are the patch embeddings of dimension d from the previous transformer block of the original CLIP model for all N patches in an image.W ∈ {W v ,W q ,W k } are the projection matrices of the current transformer block.The outputs from q-q, k-k and v-v attention are ensembled together to form the output of a GEM block.The GEM block also removes the feed-forward network from the conventional transformer block and is constructed as a parallel pathway to the original transformer blocks pathway.The outputs of all the GEM blocks at different layers are summed together to obtain the final GEM patch embeddings { f gem p (x)} p∈Patches .The zeroshot language-guided AS mask M lang = M lang p p∈Patches is then obtained where each patch anomaly score M lang p is computed as:
M lang p = exp ⟨ f gem p (x), h + ⟩/τ exp ⟨ f gem p (x), h + ⟩/τ + exp ⟨ f gem p (x), h − ⟩/τ(2)
. GEM embeddings have shown better visual-language alignment without further finetuning the CLIP model.It also has a connection to clustering properties that group similar pixels together, resulting in better zero-shot segmentation [3].GEM patch embeddings are computed based on the global context of the entire image which is important for anomaly segmentation (E.g.anomalous regions are better identified through comparison with normal regions).In contrast, WinCLIP embeddings only have a limited context restricted to a local window.In addition, GEM can be computationally more efficient with a single forward pass of the whole image.Prompt engineering: CLIP is pretrained on large datasets of visual-language pairs and learns powerful representations and alignment between images and texts that are useful for anomaly detection.Prompt engineering that captures the specific description and concept of anomaly essential for maximising the zero-shot capability of the CLIP model.While the WinCLIP prompt ensemble has been designed manually to optimise for anomaly detection, we notice that AS performance can be further improved with additional prompt engineering.</p>
<p>The concept of anomaly and normality is broad and goes beyond the limited descriptions provided by the WinCLIP prompts that use mostly general words like "damage", "flaw".In contrast, the MVTec and VisA datasets contain different types of specific industrial anomalies such as "a scratch", "a crack", "a missing part".While these specific descriptions are useful for AS, manually crafting them is time-consuming and unscalable.Instead, we use a large language model, ChatGPT 3.51 , to automatically generate a large number of text prompts that contain a diverse set of specific descriptions related to industrial anomaly detection.Fig. 2e shows an instruction prompt that is passed to ChatGPT 3.5 and some of its responses.The instruction ensures that specific object names are not mentioned in the response so that the generated text prompts are object-agnostic.This is because the notion of the specific object becomes less relevant for AS when the patch embeddings focus on local anomalous regions that are only part of the object.We also attempt to add information regarding the anomaly size and location in the text prompts but it did not improve the results.</p>
<p>We use 5 instruction prompts with different wordings to obtain responses that are more diverse and varied.This generates a total of 486 anomaly and 423 normal prompts that form the ChatGPT prompt ensemble.The instructions and their responses are listed in supplementary.The ChatGPT and WinCLIP prompt ensembles are combined and used for AS.Multi-scale aggregation: We extract multi-scale GEM embeddings in order to detect anomalies with different sizes.Although the CLIP model is trained at a fixed image size, it can take inputs of different image sizes during inference by interpolating the positional embeddings.We resize the original image to 3 input sizes: 240 × 240, 448 × 448, 896 × 896.Given that the patch size of the model is 16, this corresponds to output segmentations with size 15 × 28 × 28 and 56 × 56.The segmentations are then upscaled to a fixed size and averaged to give the final segmentation map M lang .</p>
<p>Vision-Guided Anomaly Classification and Segmentation</p>
<p>Vision-guided anomaly detection extracts and compares visual cues from images in order to identify anomalies.This is complementary to the language-guided approach and further extends it to the few-shot setting.Few-shot setting: The performance of AC and AS improves when a few reference normal images are available.Specifically, we follow the approach similar to WinCLIP [10] where we build a memory bank R of CLIP patch embeddings extracted from the k reference images (Fig. 2d).We have also tried to use GEM embeddings but the results are inferior.Given a query image x and its CLIP embeddings f clip p (x) for a patch p, the vision-guided anomaly score for this patch under the k-shot setting is computed as the cosine distance to its nearest neighbour patch in the memory bank: M vis,k p = min r∈R 1 2 1 − f clip p (x), r .Spatially combining the anomaly scores of the patches at all locations in an image gives the vision-guided segmentation map.We can extend this to a multi-scale approach by building a separate memory bank for the patch embeddings extracted from the reference images at different scales (E.g.different image sizes).The multi-scale segmentation maps are averaged to give the final vision-guided segmentation M vis,k under the k-shot setting.In addition, we use the maximum value of M vis,k as an anomaly score s vis,k for few-shot vision-guided AC.Zero-shot setting: Vision-guided AS can also be applied in the zero-shot setting (Fig. 2c) with 2 differences compared to the few-shot setting: 1) The memory bank is built using patches from the query image itself since reference images are unavailable.In this case, the resulting anomaly score is computed as the distance to the second nearest neighbour patch since the nearest neighbour is the patch itself.2) GEM patch embeddings are used since they perform better than CLIP patch embeddings.Multi-scale aggregation is used again to give the final segmentation M vis,0 under the zero-shot setting.Combining language and vision guidance: Language and vision guidance can be combined to improve overall AC and AS performance.Tab. 2 summarises how the combination is done under the different settings.Refer to supplementary for details.</p>
<p>Experiments</p>
<p>We conduct a series of experiments to evaluate the performance of FADE on AC and AS under the zero-/few-shot regime.We also perform ablation experiments to study the impact of each component in FADE.Datasets: All experiments are based on the MVTec-AD [1] and VisA [25] datasets which are standard benchmarks for AC and AS.The datasets include a range of different objects such as capsule and cashew.We only use the test split for evaluation which contains both normal and anomalous images and their ground truth segmentation masks.The training split is only used for sampling the reference images during few-shot evaluation.</p>
<p>Evaluation metrics: In accordance with prior works [10,18], we report the following evaluation metrics.For AC, we report Area Under the Receiver Operating Characteristics curve (AUROC), Area Under the Precision-Recall curve (AUPR) and F1-score at optimal threshold (F1-max).For AS, we report pixel-wise AUROC (pAUROC), Per-Region Overlap (PRO) and pixel-wise F1-max.Implementation details: For CLIP, we use the OpenCLIP2 implementation of ViT-B/16+ (240 × 240) trained on LAION-400M [20].For GEM, we use its official implementation3 .</p>
<p>Zero-/Few-Shot Anomaly Classification and Segmentation</p>
<p>Tab. 1 compares the AC performance of FADE with two prior work, PatchCore [18] which is an unsupervised anomaly detection method and WinCLIP [10] which is a state-of-the-art zero-/few-shot anomaly detection method.For the zero-shot setting, FADE is a reproduction of WinCLIP without any new additions.For the few-shot setting, FADE outperforms the other methods on all metrics for both MVTec-AD and VisA with a larger improvement seen on VisA.This shows that the vision-guided AS component of FADE is also beneficial to AC. Tab. 4 compares the AS performance of FADE with PatchCore and WinCLIP.Under zero-shot, FADE significantly outperforms other methods on all metrics for both MVTec-AD and VisA.This is due to the various AS improvements of FADE which include the multi-scale GEM embeddings, a better ChatGPT prompt ensemble and the zero-shot visionguidance using query image.Under few-shot, FADE performs similar to WinCLIP on MVTec-AD and again outperforms WinCLIP on VisA which is the more challenging dataset.This shows the advantage of FADE on more difficult images.It also shows that vanilla CLIP patch embeddings can be used in place of WinCLIP embeddings for few-shot vision-guided   AS.Furthermore, FADE performance has a lower standard deviations across different random runs.This indicates that FADE is more robust to the selection of different reference images.Fig. 1 shows qualitative AS results for some objects.See supplementary for more.Additional quantitative results comparing FADE and other state-of-the-art methods such as AnomalyCLIP [24], AnomalyGPT [7] and APRIL-GAN [5] can be found in the supplementary.FADE performs competitively even though these other methods require training using additional anomaly detection datasets while FADE does not need any further training.</p>
<p>Ablation Study</p>
<p>GEM for zero-shot AS: Tab. 3 shows the impact of using CLIP vs GEM patch embeddings for zero-shot language-guided AS.When CLIP embeddings are used, pAUROC is below 50.0 due to the problem of opposite visualisation between the normal and anomalous regions.See supplementary for qualitative examples of the opposite visualisation when CLIP embeddings are used while GEM embeddings fix this problem.More result comparison on CLIP vs GEM embeddings for the different components of FADE are also shown in the supplementary.The result motivates the choice of CLIP or GEM embeddings used in Fig. 2a-d.Prompt ensemble for zero-shot AS: Tab. 7 shows the improvement on zero-shot languageguided AS when we use additional text prompts generated automatically by ChatGPT.These prompts capture more diverse descriptions related to the concept of anomaly and normality which allow us to better use the CLIP model for improved AS.Interestingly, we note that the ChatGPT prompts did not improve the zero-shot image-level AC performance.Multi-scale aggregation: Tab. 8 and 9 show the AC and AS performance on MVTec-AD when different input image scale/size is used.Since the patch size is constant (16 pixels), a larger input size means that each patch is covering a finer scale and anomaly with smaller size will be detected and vice versa.Multi-scale refers to taking the average outputs from the three image scales and it has the best results at all k-shot settings since it allows for detecting anomalies of different sizes.See supplementary for qualitative results.Language vs vision guidance: Tab. 5 and 6 show the impact of language-vs vision-guided AC and AS on MVTec-AD.Language guidance depends on text prompts while vision guidance depends on the query (0-shot) or reference images (k-shot).The two types of guidance are complementary and improve the overall performance.See supplementary for qualitative results.</p>
<p>Conclusion and Future Work</p>
<p>We present a new method FADE that utilises and adapts the CLIP model for zero-/fewshot anomaly detection guided by language and vision.First, we improve language-guided anomaly segmentation by using multi-scale GEM embeddings that are better aligned with language than CLIP embeddings.This is further improved by using an LLM to generate a new prompt ensemble that better captures the concept of anomaly and normality.Finally, vision guidance from the query image further boosts the zero-shot anomaly detection performance while vision guidance from the reference images extends the method to the few-shot setting.On standard benchmarks, FADE performs competitively compared to other state-ofthe-art methods with the largest margin of improvement on zero-shot anomaly segmentation.There are several points about FADE that are worth investigating as future work.First, the use of text prompts generated by ChatGPT is not reproducible.While an alternative open-source LLM can be used to set the seed to ensure reproducibility, it is also worth investigating how sensitive the anomaly detection performance is to the different prompts generated by the same LLM or across different LLMs.</p>
<p>Our experiment results show that for language-guided anomaly detection, GEM embeddings are better for zero-shot AS while CLIP embeddings are more suited for zero-shot AC.On the other hand, for vision-guided anomaly detection, GEM embeddings are better for zero-shot AS while CLIP embeddings are more suited for few-shot AS.A more comprehensive study is needed to gain a better understanding of when to use the GEM and CLIP embeddings under the different detection pipelines and scenarios.</p>
<p>The vision-guided zero-shot AS constructs the memory bank from its own query image patches.This works well at detecting anomalies in textural images (E.g.leather) that normally contain visually similar patches.However, this works less well and produces false positives for anomaly detection in object images (E.g.transistor) whose patches differ to a greater extent.Future work needs to explore other vision-guided zero-shot methods to address this issue.</p>
<p>Figure 1 :
1
Figure 1: Zero-/one-shot anomaly segmentation results of FADE.</p>
<p>Language-guided anomaly classification (zero-shot) (a) Language-guided anomaly segmentation (zero-shot) (b) Vision-guided anomaly segmentation (zero-shot) (c) Vision-guided anomaly classification and segmentation (few-shot)</p>
<p>Figure 2 :
2
Figure 2: Different components of FADE.(a) Zero-shot language-guided AC; (b) Zero-shot language-guided AS; (c) Zero-shot vision-guided AS; (d) Few-shot vision-guided AC and AS; (e) ChatGPT prompts generation: An instruction given to ChatGPT and some of its responses.</p>
<p>Table 1 :
1
Comparison of AC performance on MVTec-AD and VisA.We report the mean and standard deviation over 5 random seeds.Bold indicates the best performance.AS 0-shot M lang + M vis,0 k-shot M lang + M vis,k
Anomaly ClassificationMVTec-ADVisATaskShotAggregationSetup MethodAUROCAUPRF1-maxAUROCAUPRF1-maxAC 0-shot s lang0-shot WinCLIP91.8±0.0 96.5±0.0 92.9±0.0 78.1±0.0 81.2±0.0 79.0±0.0k-shot s lang + s vis,kFADE (ours)90.0±0.0 95.6±0.0 92.4±0.0 75.6±0.0 78.5±0.0 78.6±0.01-shot PatchCore83.4±3.0 92.2±1.5 90.5±1.5 79.9±2.9 82.8±2.3 81.7±1.6WinCLIP+93.1±2.0 96.5±0.9 93.7±1.1 83.8±4.0 85.1±4.0 83.1±1.7FADE (ours)93.9±0.7 96.8±0.3 94.8±0.2 86.7±2.0 87.9±1.5 84.7±0.82-shot PatchCore86.3±3.3 93.8±1.7 92.0±1.5 81.6±4.0 84.8±3.2 82.5±1.8WinCLIP+94.4±1.3 97.0±0.7 94.4±0.8 84.6±2.4 85.8±2.7 83.0±1.4FADE (ours)95.2±1.0 97.6±0.5 95.0±0.4 89.2±0.4 90.2±0.2 85.9±0.64-shot PatchCore88.8±2.6 94.5±1.5 92.6±1.6 85.3±2.1 87.5±2.1 84.3±1.3WinCLIP+95.2±1.3 97.3±0.6 94.7±0.8 87.3±1.8 88.8±1.8 84.2±1.6FADE (ours)96.3±0.4 98.1±0.2 95.5±0.4 90.7±0.3 91.9±0.4 87.0±0.2</p>
<p>Table 2 :
2
Language and vision aggregation.
Patch embeddingsMVTec-AD VisACLIP18.013.9GEM86.587.0Table 3: CLIP vs GEMembeddings for 0-shot AS(pAUROC).</p>
<p>Table 4 :
4
Comparison of AS performance on MVTec-AD and VisA.We report the mean and standard deviation over 5 random seeds.Bold indicates the best performance.
Anomaly SegmentationMVTec-ADVisAAC (AUROC)# shotsSetup MethodpAUROCPROF1-max pAUROCPROF1-maxs langs vis140-shot WinCLIP FADE (ours) 1-shot PatchCore85.1±0.0 64.6±0.0 31.7±0.0 79.6±0.0 56.8±0.0 14.8±0.0 89.6±0.0 84.5±0.0 39.8±0.0 91.5±0.0 79.3±0.0 16.7±0.0 92.0±1.0 79.7±2.0 50.4±2.1 95.4±0.6 80.5±2.5 38.0±1.9✓ ✗✗ ✓90.0 90.0 90.7 94.5WinCLIP+95.2±0.5 87.1±1.2 55.9±2.7 96.4±0.4 85.1±2.1 41.3±2.3✓✓93.9 96.3FADE (ours)95.4±0.3 88.3±0.3 54.6±1.1 97.5±0.1 88.9±0.7 42.3±0.52-shot PatchCore93.3±0.6 82.3±1.3 53.0±1.7 96.1±0.5 82.6±2.3 41.0±3.9WinCLIP+96.0±0.3 88.4±0.9 58.4±1.7 96.8±0.3 86.2±1.4 43.5±3.3FADE (ours)95.8±0.2 88.9±0.2 55.8±1.0 97.8±0.1 89.8±0.4 44.4±0.94-shot PatchCore94.3±0.5 84.3±1.6 55.0±1.9 96.8±0.3 84.9±1.4 43.9±3.1WinCLIP+96.2±0.3 89.0±0.8 59.5±1.8 97.2±0.2 87.6±0.9 47.0±3.0FADE (ours)96.2±0.1 89.5±0.2 57.0±0.8 98.0±0.0 90.0±0.4 46.4±0.7</p>
<p>Table 5 :
5
Language-vs vision-guided AC.
AS (pAUROC)# shotsM langM vis014✓✗86.5 86.5 86.5✗✓86.6 95.1 96.1✓✓89.6 95.4 96.2</p>
<p>Table 6 :
6
Language-vs vision-guided AS.</p>
<p>Table 7 :
7
Prompt ablations on language-guided AS (pAUROC).
Prompt ensembleMVTec-AD VisAAC (AUROC) Image scale1# shots 24WinCLIP prompts83.774.024092.6 94.4 95.3WinCLIP + ChatGPT prompts86.587.0448 896 Multi-scale92.3 93.7 94.6 90.5 91.5 92.7 93.9 95.2 96.3</p>
<p>Table 8 :
8
Image scale ablations for AC.
AS (pAUROC)# shotsImage scale012424087.6 93.4 94.0 94.544887.4 93.5 94.0 94.589684.7 91.3 91.7 92.1Multi-scale89.6 95.4 95.8 96.2</p>
<p>Table 9 :
9
Image scale ablations for AS.</p>
<p>These authors contributed equally.
Corresponding author. © 2024. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.
https://chat.openai.com
https://github.com/mlfoundations/open_clip
https://github.com/WalBouss/GEM
AcknowledgementWe would like to thank Romain Sabathe for the insightful discussion and Olivier Koch for the full support throughout this project.
MVTec-AD -a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, 10.1109/CVPR.2019.009822019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Grounding everything: Emerging localization properties in vision-language transformers. Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne, 2023</p>
<p>Segment any anomaly without training via hybrid prompt regularization. Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, Weiming Shen, 2023</p>
<p>A zero-/few-shot anomaly classification and segmentation method for CVPR 2023 VAND workshop challenge tracks 1&amp;2: 1st place on zero-shot AD and 4th place on few-shot AD. Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.173822023arXiv preprint</p>
<p>PaDiM: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer2021</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>CFLOW-AD: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. Denis Gudovskiy, Shun Ishizaka, Kazuki Kozuka, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2022</p>
<p>Registration based few-shot anomaly detection. Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, Yan-Feng Wang, European Conference on Computer Vision. Springer2022</p>
<p>WinCLIP: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, 10.1109/CVPR52729.2023.018782023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>CutPaste: Selfsupervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Clip-SAM: CLIP and SAM collaboration for zero-shot anomaly segmentation. Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen, 2024</p>
<p>PromptAD: Learning prompts with only normal samples for few-shot anomaly detection. Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, Lizhuang Ma, 2024</p>
<p>CLIP surgery for better explainability with enhancement in open-vocabulary tasks. Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li, 2023</p>
<p>Grounding DINO: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Same same but differnet: Semisupervised defect detection with normalizing flows. Marco Rudolph, Bastian Wandt, Bodo Rosenhahn, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2021</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114LAION-400m: Open dataset of CLIP-filtered 400 million image-text pairs. 2021arXiv preprint</p>
<p>Random word data augmentation with CLIP for zero-shot anomaly detection. Masato Tamura, 34th British Machine Vision Conference 2023, BMVC 2023. Aberdeen, UKNovember 20-24, 2023. BMVA, 2023</p>
<p>Learning unsupervised metaformer for anomaly detection. Jhih-Ciang Wu, Ding-Jie Chen, Chiou-Shann, Tyng-Luh Fuh, Liu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Extract free dense labels from CLIP. Chong Zhou, Chen Change Loy, Bo Dai, European Conference on Computer Vision. Springer2022</p>
<p>Anoma-lyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>SPotthe-Difference self-supervised pre-training for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. Springer2022</p>            </div>
        </div>

    </div>
</body>
</html>
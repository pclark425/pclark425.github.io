<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2480 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2480</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2480</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-258461620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.02251v2.pdf" target="_blank">Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems</a></p>
                <p><strong>Paper Abstract:</strong> The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2480.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2480.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adam (Robot Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pioneering closed-loop robot scientist that automated hypothesis generation, experiment planning and execution in functional genomics, physically executing yeast growth experiments and analysing results to identify gene functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The automation of science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adam</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Closed-loop laboratory automation system integrating knowledge representation (Prolog knowledge base), abduction-based hypothesis generation, experiment planning, laboratory robotics (freezer, liquid handlers, plate readers, robot arms) and automated observation (optical sensors). It formed hypotheses about missing gene-enzyme relationships, planned and executed wet-lab yeast growth experiments (up to 5 days), analyzed growth data and iteratively updated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Robot Scientist / Closed-loop Automated Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Functional genomics; yeast metabolic gene function identification (molecular biology / bioinformatics).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify genes encoding orphan enzymes in Saccharomyces cerevisiae by predicting candidate genes bioinformatically, then experimentally testing whether adding specific metabolites to growth media affects yeast growth consistent with gene function hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate biological complexity: combinatorial hypothesis space over candidate gene-function assignments; experimental outcomes noisy and time-consuming (multi-day cultures). Low-dimensional measured observables (growth curves/optical density) but complex underlying biochemistry; search over biochemical interventions and candidate genes rather than high-dimensional continuous parameter optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relied on pre-existing biological databases and bioinformatic data for hypothesis formation; experimental data were generated by the system (yeast growth assays) with runs lasting up to 5 days. Data acquisition is relatively expensive in time and consumables compared to purely computational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Moderate compute for bioinformatic hypothesis generation and experiment planning; major costs were physical experiment time and laboratory automation rather than heavy HPC. No quantitative compute-hours reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined, closed experimental domain with clear hypothesis-testing cycle (discrete hypotheses, stochastic biological outcomes). Deterministic planning components but stochastic experimental noise; clear evaluation metric (growth/no-growth or growth rate changes). Required substantial domain knowledge encoded in knowledge base.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Novel gene-function identifications (scientific discovery), reproducibility of experimental results; number of validated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantitative outcome reported: Adam identified six genes encoding orphan enzymes in yeast. No explicit per-experiment success rate or failure rate (%) provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited to laboratory assays it was instrumented for (yeast growth experiments); inability to propose experiments outside available equipment; reliance on completeness and correctness of encoded domain knowledge; no report of systematic failure-rate statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Tight integration of domain knowledge (Prolog KB), automated lab execution (reduces human protocol variability), and experimental throughput enabling iterative hypothesis testing; targeted, well-scoped biological problem reduced search space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper positions Adam as a successful early robot scientist demonstrating closed-loop discovery in molecular biology; no direct numerical comparison versus human-only pipelines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No explicit quantitative human baseline given; results (six gene identifications) presented as novel discoveries attributable to the system but without head-to-head human-only performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2480.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eve (Robot Scientist / Drug-repurposing System)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A laboratory automation system for high-throughput drug screening and QSAR hypothesis generation that used active learning and Gaussian process regression to pick compounds for testing and discovered drug repurposing candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eve</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated drug-screening platform combining active learning with Gaussian process regression (GPR) to model quantitative structure-activity relationships (QSARs), used a pool-and-select loop where GPR provides a noisy oracle to score candidate compounds and active selection balances exploration/exploitation to choose compounds for wet-lab testing; integrated with lab automation for execution of assays.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Experimentation Platform / Drug-screening Robot Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Drug discovery and drug repurposing (parasitology / medicinal chemistry), specifically screening compounds for activity against pathogens (e.g., Plasmodium vivax).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover existing compounds with activity against neglected tropical disease targets by iteratively training a QSAR model (GPR) on assay results, selecting compounds to test to maximize likely hits while exploring chemical space, and validating hits experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High chemical-structure space dimensionality (vast candidate pools), noisy biological assay measurements, multi-objective trade-offs (activity vs. novelty vs. other properties), but the screening tasks themselves are framed as supervised regression / ranking problems over compound feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Initially moderate sized pools of candidate compounds (existing libraries); experimental labels (assay readouts) were generated by the system. Wet-lab assays are relatively costly and time-consuming though faster than some discovery tasks; no global dataset size quantification provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>GPR and active learning computations are lightweight relative to wet-lab costs; overall bottleneck is experimental throughput and associated consumable cost. No quantitative compute-hours or monetary costs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined objective (maximize biological activity), stochastic assay measurements, discrete selection actions (choose K compounds per batch). Good evaluation signal (assay activity) enabling active learning loops.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Novel active compounds discovered (hits), number and potency of repurposed candidates, downstream validation in biological assays.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported successes include discovering that the anti-cancer compound TNP-470 could be repurposed against Plasmodium vivax; no overall hit-rate percentages or failure rates reported in the surveyed paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited by assay types robotized; chemical space exploration constrained by available libraries; experimental cost and throughput limit how much of search space can be tested; reliance on surrogate QSAR model quality which can mislead selection under model mis-specification.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Active learning (efficient selection of informative compounds), probabilistic surrogate model (GPR) that provides uncertainty for exploration/exploitation balance, integration with lab automation for rapid iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper cites Eve as an early successful robot scientist for drug repositioning; no systematic comparison vs human-only screening provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not provided in the paper; outcomes presented as discoveries achieved by Eve rather than bench-marked against human screening throughput or hit-rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2480.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAHRENHEIT / Zytkow system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAHRENHEIT system and coupled robotic electrochemistry system (Zytkow et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early system coupling equation-discovery (FAHRENHEIT) with real electrochemistry experiments and a robotic platform; demonstrated automated empirical equation recovery and experiment-driven rediscovery of classical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated discovery in a chemistry laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FAHRENHEIT (coupled to robotic lab)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An equation-discovery engine (FAHRENHEIT) coupled to laboratory robotics to run electrochemistry experiments, collect data and use automated equation discovery to find empirical laws (e.g., rediscover Galileo's equation for rolling down an incline), explicitly taking empirical error into account.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Discovery System / Laboratory Automation coupled with Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Experimental chemistry / electrochemistry and classical physics equation rediscovery (empirical law extraction from robot-collected data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate data collection with robotic experiments and use equation discovery to find empirical relationships or re-discover known physical laws from the gathered data, handling experimental noise.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Laboratory experiments generate noisy continuous data; problems are low-to-moderate dimensional (few measured variables), complexity arises from experimental error and need to construct derived terms; search space of possible algebraic forms can be large but constrained by grammar/bias.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Data were generated by the robotic experiments under FAHRENHEIT control; amount/scale not quantitatively reported in the survey. Data quality included real experimental noise and systematic empirical error.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Equation search plus experiment control; computational cost moderate compared to wet-lab time. No numeric compute cost provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined empirical regression/equation-discovery problems: continuous, noisy, deterministic underlying physical laws but stochastic measurement noise; clear evaluation by comparison to known law form or constant values.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to rediscover known equations (e.g., Galileo), plausibility of discovered equations given experimental noise, and empirical fit quality.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported qualitative success in rediscovering Galileo's equation and linking lab data to discovered equations; no quantitative success/failure rates supplied in the surveyed paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited expressiveness if search grammar/bias insufficient; sensitivity to noise and experimental error; limited to experiments the robot could run.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Coupling physical experiment automation with symbolic equation-discovery algorithms and using empirical error models; constraining search with domain knowledge/grammars.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2480.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BACON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BACON (Langley)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early production-system based algorithm for empirical law/equation discovery that constructed new terms and detected near-constant relationships to infer laws (e.g., rediscovering Kepler's third law).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bacon: A production system that discovers empirical laws</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BACON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Rule-based production system that examines correlations (pairwise under ceteris paribus assumptions), constructs new terms (ratios/products), tests for constancy within tolerance thresholds to infer empirical laws; uses production rules and symbolic manipulations rather than heavy numeric optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Equation Discovery / Automated Hypothesis Generation System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Empirical law discovery from tabular scientific data (classical physics examples like planetary motion).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given observational data, search for algebraic relations by forming candidate terms and testing constancy or correlations, constructing more complex expressions incrementally until simple constants or relations are found.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Low-to-moderate dimensional problems with small numbers of measured variables; search over combinatorial space of constructed terms but constrained by production rules; brittle in presence of uncontrolled confounders (ceteris paribus assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Assumes availability of relevant tabular observational data; often requires data where confounding variables can be controlled or held constant; handled noise via tolerance parameters but real-world noisy uncontrolled data is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Lightweight symbol-manipulation and simple regressions; computationally cheap compared to modern heavy fitting; no quantitative compute numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined target: find algebraic relations among variables; deterministic search with tolerance for empirical noise; requires domain knowledge to set meaningful tolerance and term-construction rules.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Discovery of correct algebraic relationships (e.g., Kepler's law), interpretability of discovered expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Historically successful on textbook-style problems (e.g., rediscovering Kepler's third law); no explicit quantitative success rates reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails when many uncontrolled variables exist (ceteris paribus violated), brittle to noise and when required term constructions not in production rule set.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong inductive biases via production rules and term construction heuristics; works best on low-dimensional, well-controlled datasets with interpretable structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2480.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lagramge / Lagrange</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lagrange / Lagramge (equation discovery systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Successive equation-discovery systems that compute derivatives, generate products up to a degree, and use regression (later sparse regression) to identify differential equations; Lagramge notably introduced context-free grammars and domain knowledge into search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lagrange / Lagramge</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Lagrange computes derivatives up to a set order, generates candidate terms (products up to max arity) and fits linear regression to propose ODE structures; SINDy later uses sparse regression for similar aims. Lagramge extends this by using context-free grammars to encode domain knowledge to guide search and solve problems beyond brute-force approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Equation Discovery / Symbolic Regression System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Dynamical system identification (ordinary differential equations), including control-type problems like two-pole cart dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover governing ODEs from time-series observational data by computing derivatives, proposing basis functions/terms and selecting sparse linear combinations that explain dynamics; incorporate domain grammar constraints to restrict search.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets moderate complexity dynamical systems; complexity arises from derivative estimation sensitivity to noise, combinatorial term generation, and sparse model selection. Usually limited to small numbers of state variables.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires densely sampled time-series to compute derivatives reliably; noisy data degrades derivative estimates and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Regression and sparse optimization are main costs; derivative estimation can be noisy and fitting repeated candidate sets is expensive. No quantitative compute figures given.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Structured as continuous-time dynamical systems, continuous and deterministic underlying equations with stochastic measurement noise; clear evaluation via predictive performance and sparsity/interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correct recovery of ODE structure and coefficients, predictive accuracy on held-out trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Demonstrated successful recovery in benchmark and controlled problems; SINDy and Lagrange variants are effective on low-dimensional dynamics but struggle with higher-dimensional systems; no explicit numeric success rates provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Derivative noise sensitivity, high-dimensional state spaces, insufficiently expressive candidate term sets, and overfitting to noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sparse regression priors, inclusion of domain knowledge (grammars), and good quality time-series sampling facilitate success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2480.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SINDy (Sparse Identification of Nonlinear Dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse regression approach to discover governing ODEs from time-series data by building a library of candidate functions and selecting a sparse subset that explains dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SINDy</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Algorithm that forms a dictionary of candidate nonlinear functions (polynomials, trig functions, etc.) evaluated on measured states and uses sparse regression (e.g., LASSO-type) to identify a parsimonious set of terms describing state derivatives, enabling interpretable dynamical models.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Equation Discovery / Symbolic Regression for Dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Dynamical systems identification (ODEs) in physics, engineering, and computational modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given time-series state measurements, estimate derivatives and perform sparse selection from a function library to recover governing equations and coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Works best for low-to-moderate dimensional systems with sparse underlying dynamics; complexity increases with number of candidate terms and state variables; derivative estimation and noise handling are major challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires relatively dense, high-quality time-series data to estimate derivatives; noisy or sparse sampling reduces performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Sparse regression solves over potentially large candidate libraries; computational cost grows with library size and dataset length but is tractable on standard compute for modest problems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Continuous deterministic dynamics with measurement noise; well-defined target structure (sparse combination of known function types).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Accurate recovery of correct sparse terms and coefficients; predictive fidelity of the resulting ODE model.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Effective on many benchmark problems and extended variants (SINDy-SHRED) handle nonlinear dynamics better, but struggles as variable count increases; no scalar success rates reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High sensitivity to noise and derivative estimation; poor scaling to high-dimensional systems and mis-specified function libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sparse priors, appropriate candidate libraries, and good sampling quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2480.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PySR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PySR (Symbolic Regression Framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast and practical symbolic regression system based on genetic programming with a Python front-end and Julia backend that returns Pareto-optimal solutions by complexity class and supports parallel evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PySR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Genetic-programming-based symbolic regression engine that explores equation space via operator trees/DAGs, returns best solutions per complexity class, uses Julia for numerical evaluation and heuristics to avoid redundancy, supports template expressions and mini-batching for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Symbolic Regression / Equation Discovery System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General-purpose symbolic regression across physics, engineering and empirical modeling tasks (equation discovery from tabular data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover human-interpretable algebraic expressions that fit input-output data, trading off simplicity and fit via Pareto front per complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Designed to explore large candidate sets efficiently; still practically limited by growth of search space with expression complexity and number of variablesâ€”works best for relatively low-to-moderate dimensional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on pre-collected tabular datasets; success depends on sufficient and representative data to constrain symbolic forms; no quantitative dataset sizes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Optimized for speed using Julia backend and parallelization; computational cost depends on number of candidate evaluations and population size; no absolute compute hours provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>One-shot supervised regression tasks (well-defined), continuous outputs, deterministic fitting with overfitting risk and syntactic complexity concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Pareto-optimal trade-off between equation complexity and fit error; interpretability and correctness on benchmark tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported as frequently well-performing in practice; specific quantitative success rates or comparisons absent in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Search can become intractable with many variables or when target expression is very complex; overfitting and brittle equations that do not generalize well.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Efficient implementation (Julia), heuristics to reduce redundancy, and templating/mini-batching to improve practical applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2480.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian Machine Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Machine Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian approach to symbolic regression that approximates marginal posteriors over models, uses learned priors from corpora of expressions, and explores model space via MCMC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A bayesian machine scientist to aid in the solution of challenging scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Machine Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs explicit probabilistic model over symbolic expressions by approximating marginal posterior probabilities of models, learns prior expectations from large corpora of equations, and explores expression space using Markov Chain Monte Carlo moves tailored for mathematical expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Probabilistic Symbolic Regression / Automated Model Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic equation discovery across physics and other scientific domains where probabilistic model plausibility assessment is helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Given input-output pairs, infer symbolic expressions with posterior plausibility scores, balancing fit to data and model prior plausibility learned from natural corpora of formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles combinatorial expression spaces; probabilistic framework can manage uncertainty and model averaging but computational cost grows with complexity of expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Works with pre-existing tabular data; benefit from priors learned from corpora to guide search when data are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>MCMC over symbolic spaces can be computationally intensive; no numeric compute figures provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined regression tasks with explicit probabilistic scoring; continuous outputs and noise modeled within Bayesian likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Posterior probability / plausibility of discovered models, and recovery of known or plausible equations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Demonstrated utility in challenging problems per reference; no global quantitative success rate provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>MCMC mixing and convergence issues in very large expression spaces; sensitivity to chosen priors and move sets.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Use of learned priors from equation corpora and probabilistic evaluation to focus search on plausible expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2480.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic regression approach tailored to physics-inspired problems that builds equations modularly using graph modules and improves noise tolerance via an MDL-inspired evaluation (MEDL).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs candidate expressions incrementally via graph modularity (graph modules) to exploit structure in equation space and uses a MEDL (MDL-inspired) evaluation function that penalizes complexity relative to error to prune search and improve noise robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Symbolic Regression / Equation Discovery System (physics-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Physics-inspired equation discovery and symbolic regression tasks with structured, modular expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover closed-form physical formulas from data, especially when underlying equations can be decomposed into simpler modules; handle noisy measurements more robustly than naive regression.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Targets structured expressions where modular decomposition is possible; complexity arises from combinatorial module combinations and noise in data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on tabular datasets sampled from physical systems; noise-tolerant evaluation aims to handle realistic datasets but requires sufficient samples to constrain modules.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Module-based search reduces some search costs; no quantitative compute metrics provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Continuous regression tasks with exploitable modular structure; deterministic underlying laws with measurement noise.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Recovery of compact, accurate symbolic forms; Pareto-optimal trade-off between complexity and fit using MEDL.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported improvements in noise tolerance and modular discovery on benchmarks; no numeric success rates provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Less effective when no modular structure exists or when data are too noisy/insufficient to identify modules.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Exploiting modularity of equations and an MDL-inspired model selection criterion (MEDL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2480.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Symbolic Regression (DSR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Symbolic Regression (DSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-guided symbolic regression method using a recurrent neural network trained with a risk-seeking policy gradient to generate expression trees, aiming to scale symbolic discovery to larger problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Symbolic Regression (DSR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an RNN to sequentially generate nodes/tokens of an expression tree; because the objective (fit of produced equation) is non-differentiable, training uses reinforcement learning with a risk-seeking policy gradient that emphasizes best-case performance rather than average-case.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neural-guided Symbolic Regression / Automated Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Symbolic regression and equation discovery for mathematical/physical modeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate symbolic expressions that fit data by learning a generative policy over expression tokens, optimized to produce high-performing (low-error) equations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Addresses combinatorial expression search; complexity increases with expression depth, number of functions/operators and input variable count; intended to scale better than pure GP in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on tabular datasets; model training requires many sampled expressions evaluated on data, so computational cost is coupled to dataset size and training budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>High sampling and evaluation overhead due to reinforcement learning training loop; requires many expression evaluations and model training iterations; no numeric compute-hours provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>One-shot supervised regression target with non-differentiable objective; stochastic generation process during training.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Quality (fit/error) and simplicity of discovered expressions; best-case performance emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported to recover many benchmark expressions and scale better than some GP baselines on certain tasks, but no uniform success rate numbers provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High computational expense for training, sensitivity to reward shaping and RL instability; may still struggle with many variables or highly complex expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Neural policy that learns structural priors and bias toward promising expression patterns; risk-seeking RL objective that amplifies top-performing samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2480.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Foundation-model-based SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Foundation model / LLM-based symbolic regression (e.g., In-Context SR, LLM-SR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that use large pre-trained language / foundation models (LLMs) to propose symbolic expressions or to perform genetic programming operations via prompting/fine-tuning, often followed by external numeric evaluation and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Symbolic Regression (In-Context SR, LLM-SR, SymbolicGPT, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Leverages pretrained foundation models (e.g., GPT-family) to generate candidate equations or programmatic representations of equations through prompt engineering or fine-tuning; generated candidates are evaluated numerically, and feedback (fitness, complexity) is provided to the model for iterative refinement; some methods use LLMs to perform genetic programming operations (mutation, crossover) via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Idea Generation / Symbolic Regression System using Foundation Models</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Equation discovery / symbolic regression across physics, engineering and other domains where background knowledge retained in the foundation model may help propose plausible forms.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce candidate symbolic expressions or code representing equations for given datasets by leveraging in-context knowledge and generalization from pretraining, then optimize candidates by numerical testing and iterative prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Potential to scale via learned priors from pretraining but risks include overfitting to training-set formulas learned during foundation model pretraining; complexity depends on expressiveness of model and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Works with tabular datasets; benefits from pretraining on broad corpora but evaluation depends on availability of numeric data to score generated candidates. Paper notes concerns about benchmarks being included in pretraining corpora (data provenance).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Costs depend on LLM inference and iterative evaluation; inference costs can be significant for large foundation models but numeric evaluations remain external and parallelizable; no precise compute numbers reported in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>One-shot or iterative supervised regression; black-box generation by LLMs may lack explicit interpretable priors; stochastic generation and possible memorization from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Quality of generated symbolic expressions after numeric evaluation (fit, complexity), novelty of proposals and refinement success across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Survey reports these approaches show promising results in producing initial equations and seeding search; however, the extent of generalization and dependence on pretraining data is unclear and no quantitative success rates are given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Memorization from pretraining (benchmarks may be included in LLM training data), unclear generalization to unseen problem types, unpredictable candidate generation quality, and reliance on good prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong prior knowledge encoded in foundation models, ability to flexibly incorporate natural-language domain knowledge via prompts, and ease of adaptation through few-shot prompting or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2480.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2480.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Scientists (general / 1stâ€“3rd generation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Scientists (general term for automated closed-loop laboratory discovery systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of autonomous/semiautonomous systems that generate hypotheses, design and run experiments with lab robotics, analyze results, and iterateâ€”ranging from early systems (Adam) to current third-generation self-driving labs across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robot Scientist (general)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encompasses systems that automate parts or the full scientific cycle: hypothesis generation (abduction/induction), experimental design, automated execution via lab robotics, data analysis and iterative replanning. Examples include Adam, Eve, and many contemporary self-driving labs in chemistry, materials, biology and catalysis.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Autonomous Discovery System / Self-driving Lab</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Multiple scientific domains: functional genomics, drug discovery, chemistry, materials science, catalysis, cell biology, protein design, quantum physics, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate end-to-end experimental cycles for domain-specific discovery tasks: identify gene functions, screen compounds, optimize reactions/materials, discover catalysts, etc., within the constraints of the lab automation available.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varies widely by domain: from low-dimensional, well-structured assays to high-dimensional material/combinatorial chemical spaces; common complexities include noisy experimental measurements, high cost/time per experiment, and massive combinatorial search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often generate their own experimental data in closed-loop; pre-existing data and domain knowledge are used to seed hypotheses. Data acquisition cost and throughput are primary scaling bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Compute for modelling and planning is moderate; dominant resource is experimental throughput and lab automation logistics. Specific compute costs depend on algorithms used (e.g., active learning, RL) but are not quantified in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Typically well-defined experimental objectives but sometimes open-ended discovery; mixture of discrete decision-making (which experiments to run) and continuous modeling (surrogate models). Evaluation metrics include discovery of new knowledge, experimental success, and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Novel discoveries validated experimentally, efficiency (experiments-to-discovery), reproducibility, and sometimes downstream impact (e.g., drug repurposing hits).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Survey reports multiple domain successes (Adam: 6 genes; Eve: drug repurposing hit TNP-470), and many operational self-driving labs in industry and academia; however, no unified success-rate statistics reported across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited experiment types supported by the robots, high capital/maintenance costs, difficulties in automating experimental design beyond parameter sweeps, requirement for specialist staff, and the challenge of forming genuinely novel theories or new measurement devices.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong integration of domain knowledge, reliable lab robotics, efficient experimental design (active learning), and closed-loop iteration enabling reproducibility and throughput gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Paper contrasts strengths of closed-loop lab automation (scalability, reproducibility) with limitations in forming novel high-level theory and communicating interpretable equations; symbolic regression systems excel at interpretable models but lack embodiment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No general human baseline is provided; paper notes AI Scientists can work cheaper/faster/longer than humans and can be multiplied, but direct quantitative human-vs-system comparisons are sparse in the surveyed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The automation of science <em>(Rating: 2)</em></li>
                <li>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases <em>(Rating: 2)</em></li>
                <li>Automated discovery in a chemistry laboratory <em>(Rating: 2)</em></li>
                <li>Bacon: A production system that discovers empirical laws <em>(Rating: 2)</em></li>
                <li>Discovering dynamics <em>(Rating: 2)</em></li>
                <li>Distilling free-form natural laws from experimental data <em>(Rating: 1)</em></li>
                <li>Discovering governing equations from data by sparse identification of nonlinear dynamical systems <em>(Rating: 2)</em></li>
                <li>Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity <em>(Rating: 2)</em></li>
                <li>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients <em>(Rating: 2)</em></li>
                <li>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2480",
    "paper_id": "paper-258461620",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "Adam",
            "name_full": "Adam (Robot Scientist)",
            "brief_description": "A pioneering closed-loop robot scientist that automated hypothesis generation, experiment planning and execution in functional genomics, physically executing yeast growth experiments and analysing results to identify gene functions.",
            "citation_title": "The automation of science",
            "mention_or_use": "mention",
            "system_name": "Adam",
            "system_description": "Closed-loop laboratory automation system integrating knowledge representation (Prolog knowledge base), abduction-based hypothesis generation, experiment planning, laboratory robotics (freezer, liquid handlers, plate readers, robot arms) and automated observation (optical sensors). It formed hypotheses about missing gene-enzyme relationships, planned and executed wet-lab yeast growth experiments (up to 5 days), analyzed growth data and iteratively updated hypotheses.",
            "system_type": "AI Scientist / Robot Scientist / Closed-loop Automated Discovery System",
            "problem_domain": "Functional genomics; yeast metabolic gene function identification (molecular biology / bioinformatics).",
            "problem_description": "Identify genes encoding orphan enzymes in Saccharomyces cerevisiae by predicting candidate genes bioinformatically, then experimentally testing whether adding specific metabolites to growth media affects yeast growth consistent with gene function hypotheses.",
            "problem_complexity": "Moderate biological complexity: combinatorial hypothesis space over candidate gene-function assignments; experimental outcomes noisy and time-consuming (multi-day cultures). Low-dimensional measured observables (growth curves/optical density) but complex underlying biochemistry; search over biochemical interventions and candidate genes rather than high-dimensional continuous parameter optimization.",
            "data_availability": "Relied on pre-existing biological databases and bioinformatic data for hypothesis formation; experimental data were generated by the system (yeast growth assays) with runs lasting up to 5 days. Data acquisition is relatively expensive in time and consumables compared to purely computational tasks.",
            "computational_requirements": "Moderate compute for bioinformatic hypothesis generation and experiment planning; major costs were physical experiment time and laboratory automation rather than heavy HPC. No quantitative compute-hours reported in the paper.",
            "problem_structure": "Well-defined, closed experimental domain with clear hypothesis-testing cycle (discrete hypotheses, stochastic biological outcomes). Deterministic planning components but stochastic experimental noise; clear evaluation metric (growth/no-growth or growth rate changes). Required substantial domain knowledge encoded in knowledge base.",
            "success_metric": "Novel gene-function identifications (scientific discovery), reproducibility of experimental results; number of validated hypotheses.",
            "success_rate": "Quantitative outcome reported: Adam identified six genes encoding orphan enzymes in yeast. No explicit per-experiment success rate or failure rate (%) provided in the paper.",
            "failure_modes": "Limited to laboratory assays it was instrumented for (yeast growth experiments); inability to propose experiments outside available equipment; reliance on completeness and correctness of encoded domain knowledge; no report of systematic failure-rate statistics.",
            "success_factors": "Tight integration of domain knowledge (Prolog KB), automated lab execution (reduces human protocol variability), and experimental throughput enabling iterative hypothesis testing; targeted, well-scoped biological problem reduced search space.",
            "comparative_results": "Paper positions Adam as a successful early robot scientist demonstrating closed-loop discovery in molecular biology; no direct numerical comparison versus human-only pipelines provided.",
            "human_baseline": "No explicit quantitative human baseline given; results (six gene identifications) presented as novel discoveries attributable to the system but without head-to-head human-only performance metrics.",
            "uuid": "e2480.0",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Eve",
            "name_full": "Eve (Robot Scientist / Drug-repurposing System)",
            "brief_description": "A laboratory automation system for high-throughput drug screening and QSAR hypothesis generation that used active learning and Gaussian process regression to pick compounds for testing and discovered drug repurposing candidates.",
            "citation_title": "Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases",
            "mention_or_use": "mention",
            "system_name": "Eve",
            "system_description": "Automated drug-screening platform combining active learning with Gaussian process regression (GPR) to model quantitative structure-activity relationships (QSARs), used a pool-and-select loop where GPR provides a noisy oracle to score candidate compounds and active selection balances exploration/exploitation to choose compounds for wet-lab testing; integrated with lab automation for execution of assays.",
            "system_type": "AI Scientist / Automated Experimentation Platform / Drug-screening Robot Scientist",
            "problem_domain": "Drug discovery and drug repurposing (parasitology / medicinal chemistry), specifically screening compounds for activity against pathogens (e.g., Plasmodium vivax).",
            "problem_description": "Discover existing compounds with activity against neglected tropical disease targets by iteratively training a QSAR model (GPR) on assay results, selecting compounds to test to maximize likely hits while exploring chemical space, and validating hits experimentally.",
            "problem_complexity": "High chemical-structure space dimensionality (vast candidate pools), noisy biological assay measurements, multi-objective trade-offs (activity vs. novelty vs. other properties), but the screening tasks themselves are framed as supervised regression / ranking problems over compound feature spaces.",
            "data_availability": "Initially moderate sized pools of candidate compounds (existing libraries); experimental labels (assay readouts) were generated by the system. Wet-lab assays are relatively costly and time-consuming though faster than some discovery tasks; no global dataset size quantification provided.",
            "computational_requirements": "GPR and active learning computations are lightweight relative to wet-lab costs; overall bottleneck is experimental throughput and associated consumable cost. No quantitative compute-hours or monetary costs reported.",
            "problem_structure": "Well-defined objective (maximize biological activity), stochastic assay measurements, discrete selection actions (choose K compounds per batch). Good evaluation signal (assay activity) enabling active learning loops.",
            "success_metric": "Novel active compounds discovered (hits), number and potency of repurposed candidates, downstream validation in biological assays.",
            "success_rate": "Reported successes include discovering that the anti-cancer compound TNP-470 could be repurposed against Plasmodium vivax; no overall hit-rate percentages or failure rates reported in the surveyed paper.",
            "failure_modes": "Limited by assay types robotized; chemical space exploration constrained by available libraries; experimental cost and throughput limit how much of search space can be tested; reliance on surrogate QSAR model quality which can mislead selection under model mis-specification.",
            "success_factors": "Active learning (efficient selection of informative compounds), probabilistic surrogate model (GPR) that provides uncertainty for exploration/exploitation balance, integration with lab automation for rapid iteration.",
            "comparative_results": "Paper cites Eve as an early successful robot scientist for drug repositioning; no systematic comparison vs human-only screening provided in the survey.",
            "human_baseline": "Not provided in the paper; outcomes presented as discoveries achieved by Eve rather than bench-marked against human screening throughput or hit-rates.",
            "uuid": "e2480.1",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FAHRENHEIT / Zytkow system",
            "name_full": "FAHRENHEIT system and coupled robotic electrochemistry system (Zytkow et al.)",
            "brief_description": "Early system coupling equation-discovery (FAHRENHEIT) with real electrochemistry experiments and a robotic platform; demonstrated automated empirical equation recovery and experiment-driven rediscovery of classical laws.",
            "citation_title": "Automated discovery in a chemistry laboratory",
            "mention_or_use": "mention",
            "system_name": "FAHRENHEIT (coupled to robotic lab)",
            "system_description": "An equation-discovery engine (FAHRENHEIT) coupled to laboratory robotics to run electrochemistry experiments, collect data and use automated equation discovery to find empirical laws (e.g., rediscover Galileo's equation for rolling down an incline), explicitly taking empirical error into account.",
            "system_type": "Automated Discovery System / Laboratory Automation coupled with Equation Discovery",
            "problem_domain": "Experimental chemistry / electrochemistry and classical physics equation rediscovery (empirical law extraction from robot-collected data).",
            "problem_description": "Automate data collection with robotic experiments and use equation discovery to find empirical relationships or re-discover known physical laws from the gathered data, handling experimental noise.",
            "problem_complexity": "Laboratory experiments generate noisy continuous data; problems are low-to-moderate dimensional (few measured variables), complexity arises from experimental error and need to construct derived terms; search space of possible algebraic forms can be large but constrained by grammar/bias.",
            "data_availability": "Data were generated by the robotic experiments under FAHRENHEIT control; amount/scale not quantitatively reported in the survey. Data quality included real experimental noise and systematic empirical error.",
            "computational_requirements": "Equation search plus experiment control; computational cost moderate compared to wet-lab time. No numeric compute cost provided.",
            "problem_structure": "Well-defined empirical regression/equation-discovery problems: continuous, noisy, deterministic underlying physical laws but stochastic measurement noise; clear evaluation by comparison to known law form or constant values.",
            "success_metric": "Ability to rediscover known equations (e.g., Galileo), plausibility of discovered equations given experimental noise, and empirical fit quality.",
            "success_rate": "Reported qualitative success in rediscovering Galileo's equation and linking lab data to discovered equations; no quantitative success/failure rates supplied in the surveyed paper.",
            "failure_modes": "Limited expressiveness if search grammar/bias insufficient; sensitivity to noise and experimental error; limited to experiments the robot could run.",
            "success_factors": "Coupling physical experiment automation with symbolic equation-discovery algorithms and using empirical error models; constraining search with domain knowledge/grammars.",
            "uuid": "e2480.2",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "BACON",
            "name_full": "BACON (Langley)",
            "brief_description": "An early production-system based algorithm for empirical law/equation discovery that constructed new terms and detected near-constant relationships to infer laws (e.g., rediscovering Kepler's third law).",
            "citation_title": "Bacon: A production system that discovers empirical laws",
            "mention_or_use": "mention",
            "system_name": "BACON",
            "system_description": "Rule-based production system that examines correlations (pairwise under ceteris paribus assumptions), constructs new terms (ratios/products), tests for constancy within tolerance thresholds to infer empirical laws; uses production rules and symbolic manipulations rather than heavy numeric optimization.",
            "system_type": "Equation Discovery / Automated Hypothesis Generation System",
            "problem_domain": "Empirical law discovery from tabular scientific data (classical physics examples like planetary motion).",
            "problem_description": "Given observational data, search for algebraic relations by forming candidate terms and testing constancy or correlations, constructing more complex expressions incrementally until simple constants or relations are found.",
            "problem_complexity": "Low-to-moderate dimensional problems with small numbers of measured variables; search over combinatorial space of constructed terms but constrained by production rules; brittle in presence of uncontrolled confounders (ceteris paribus assumption).",
            "data_availability": "Assumes availability of relevant tabular observational data; often requires data where confounding variables can be controlled or held constant; handled noise via tolerance parameters but real-world noisy uncontrolled data is challenging.",
            "computational_requirements": "Lightweight symbol-manipulation and simple regressions; computationally cheap compared to modern heavy fitting; no quantitative compute numbers given.",
            "problem_structure": "Well-defined target: find algebraic relations among variables; deterministic search with tolerance for empirical noise; requires domain knowledge to set meaningful tolerance and term-construction rules.",
            "success_metric": "Discovery of correct algebraic relationships (e.g., Kepler's law), interpretability of discovered expressions.",
            "success_rate": "Historically successful on textbook-style problems (e.g., rediscovering Kepler's third law); no explicit quantitative success rates reported in the survey.",
            "failure_modes": "Fails when many uncontrolled variables exist (ceteris paribus violated), brittle to noise and when required term constructions not in production rule set.",
            "success_factors": "Strong inductive biases via production rules and term construction heuristics; works best on low-dimensional, well-controlled datasets with interpretable structure.",
            "uuid": "e2480.3",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Lagramge / Lagrange",
            "name_full": "Lagrange / Lagramge (equation discovery systems)",
            "brief_description": "Successive equation-discovery systems that compute derivatives, generate products up to a degree, and use regression (later sparse regression) to identify differential equations; Lagramge notably introduced context-free grammars and domain knowledge into search.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Lagrange / Lagramge",
            "system_description": "Lagrange computes derivatives up to a set order, generates candidate terms (products up to max arity) and fits linear regression to propose ODE structures; SINDy later uses sparse regression for similar aims. Lagramge extends this by using context-free grammars to encode domain knowledge to guide search and solve problems beyond brute-force approaches.",
            "system_type": "Equation Discovery / Symbolic Regression System",
            "problem_domain": "Dynamical system identification (ordinary differential equations), including control-type problems like two-pole cart dynamics.",
            "problem_description": "Recover governing ODEs from time-series observational data by computing derivatives, proposing basis functions/terms and selecting sparse linear combinations that explain dynamics; incorporate domain grammar constraints to restrict search.",
            "problem_complexity": "Targets moderate complexity dynamical systems; complexity arises from derivative estimation sensitivity to noise, combinatorial term generation, and sparse model selection. Usually limited to small numbers of state variables.",
            "data_availability": "Requires densely sampled time-series to compute derivatives reliably; noisy data degrades derivative estimates and model selection.",
            "computational_requirements": "Regression and sparse optimization are main costs; derivative estimation can be noisy and fitting repeated candidate sets is expensive. No quantitative compute figures given.",
            "problem_structure": "Structured as continuous-time dynamical systems, continuous and deterministic underlying equations with stochastic measurement noise; clear evaluation via predictive performance and sparsity/interpretability.",
            "success_metric": "Correct recovery of ODE structure and coefficients, predictive accuracy on held-out trajectories.",
            "success_rate": "Demonstrated successful recovery in benchmark and controlled problems; SINDy and Lagrange variants are effective on low-dimensional dynamics but struggle with higher-dimensional systems; no explicit numeric success rates provided in the survey.",
            "failure_modes": "Derivative noise sensitivity, high-dimensional state spaces, insufficiently expressive candidate term sets, and overfitting to noisy data.",
            "success_factors": "Sparse regression priors, inclusion of domain knowledge (grammars), and good quality time-series sampling facilitate success.",
            "uuid": "e2480.4",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SINDy",
            "name_full": "SINDy (Sparse Identification of Nonlinear Dynamics)",
            "brief_description": "A sparse regression approach to discover governing ODEs from time-series data by building a library of candidate functions and selecting a sparse subset that explains dynamics.",
            "citation_title": "Discovering dynamics",
            "mention_or_use": "mention",
            "system_name": "SINDy",
            "system_description": "Algorithm that forms a dictionary of candidate nonlinear functions (polynomials, trig functions, etc.) evaluated on measured states and uses sparse regression (e.g., LASSO-type) to identify a parsimonious set of terms describing state derivatives, enabling interpretable dynamical models.",
            "system_type": "Automated Equation Discovery / Symbolic Regression for Dynamics",
            "problem_domain": "Dynamical systems identification (ODEs) in physics, engineering, and computational modeling.",
            "problem_description": "Given time-series state measurements, estimate derivatives and perform sparse selection from a function library to recover governing equations and coefficients.",
            "problem_complexity": "Works best for low-to-moderate dimensional systems with sparse underlying dynamics; complexity increases with number of candidate terms and state variables; derivative estimation and noise handling are major challenges.",
            "data_availability": "Requires relatively dense, high-quality time-series data to estimate derivatives; noisy or sparse sampling reduces performance.",
            "computational_requirements": "Sparse regression solves over potentially large candidate libraries; computational cost grows with library size and dataset length but is tractable on standard compute for modest problems.",
            "problem_structure": "Continuous deterministic dynamics with measurement noise; well-defined target structure (sparse combination of known function types).",
            "success_metric": "Accurate recovery of correct sparse terms and coefficients; predictive fidelity of the resulting ODE model.",
            "success_rate": "Effective on many benchmark problems and extended variants (SINDy-SHRED) handle nonlinear dynamics better, but struggles as variable count increases; no scalar success rates reported in survey.",
            "failure_modes": "High sensitivity to noise and derivative estimation; poor scaling to high-dimensional systems and mis-specified function libraries.",
            "success_factors": "Sparse priors, appropriate candidate libraries, and good sampling quality.",
            "uuid": "e2480.5",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "PySR",
            "name_full": "PySR (Symbolic Regression Framework)",
            "brief_description": "A fast and practical symbolic regression system based on genetic programming with a Python front-end and Julia backend that returns Pareto-optimal solutions by complexity class and supports parallel evaluation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PySR",
            "system_description": "Genetic-programming-based symbolic regression engine that explores equation space via operator trees/DAGs, returns best solutions per complexity class, uses Julia for numerical evaluation and heuristics to avoid redundancy, supports template expressions and mini-batching for scalability.",
            "system_type": "Symbolic Regression / Equation Discovery System",
            "problem_domain": "General-purpose symbolic regression across physics, engineering and empirical modeling tasks (equation discovery from tabular data).",
            "problem_description": "Discover human-interpretable algebraic expressions that fit input-output data, trading off simplicity and fit via Pareto front per complexity.",
            "problem_complexity": "Designed to explore large candidate sets efficiently; still practically limited by growth of search space with expression complexity and number of variablesâ€”works best for relatively low-to-moderate dimensional problems.",
            "data_availability": "Operates on pre-collected tabular datasets; success depends on sufficient and representative data to constrain symbolic forms; no quantitative dataset sizes reported.",
            "computational_requirements": "Optimized for speed using Julia backend and parallelization; computational cost depends on number of candidate evaluations and population size; no absolute compute hours provided in survey.",
            "problem_structure": "One-shot supervised regression tasks (well-defined), continuous outputs, deterministic fitting with overfitting risk and syntactic complexity concerns.",
            "success_metric": "Pareto-optimal trade-off between equation complexity and fit error; interpretability and correctness on benchmark tasks.",
            "success_rate": "Reported as frequently well-performing in practice; specific quantitative success rates or comparisons absent in survey.",
            "failure_modes": "Search can become intractable with many variables or when target expression is very complex; overfitting and brittle equations that do not generalize well.",
            "success_factors": "Efficient implementation (Julia), heuristics to reduce redundancy, and templating/mini-batching to improve practical applicability.",
            "uuid": "e2480.6",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Bayesian Machine Scientist",
            "name_full": "Bayesian Machine Scientist",
            "brief_description": "A Bayesian approach to symbolic regression that approximates marginal posteriors over models, uses learned priors from corpora of expressions, and explores model space via MCMC.",
            "citation_title": "A bayesian machine scientist to aid in the solution of challenging scientific problems",
            "mention_or_use": "mention",
            "system_name": "Bayesian Machine Scientist",
            "system_description": "Constructs explicit probabilistic model over symbolic expressions by approximating marginal posterior probabilities of models, learns prior expectations from large corpora of equations, and explores expression space using Markov Chain Monte Carlo moves tailored for mathematical expressions.",
            "system_type": "Probabilistic Symbolic Regression / Automated Model Discovery",
            "problem_domain": "Symbolic equation discovery across physics and other scientific domains where probabilistic model plausibility assessment is helpful.",
            "problem_description": "Given input-output pairs, infer symbolic expressions with posterior plausibility scores, balancing fit to data and model prior plausibility learned from natural corpora of formulas.",
            "problem_complexity": "Handles combinatorial expression spaces; probabilistic framework can manage uncertainty and model averaging but computational cost grows with complexity of expressions.",
            "data_availability": "Works with pre-existing tabular data; benefit from priors learned from corpora to guide search when data are limited.",
            "computational_requirements": "MCMC over symbolic spaces can be computationally intensive; no numeric compute figures provided in the survey.",
            "problem_structure": "Well-defined regression tasks with explicit probabilistic scoring; continuous outputs and noise modeled within Bayesian likelihoods.",
            "success_metric": "Posterior probability / plausibility of discovered models, and recovery of known or plausible equations.",
            "success_rate": "Demonstrated utility in challenging problems per reference; no global quantitative success rate provided in the survey.",
            "failure_modes": "MCMC mixing and convergence issues in very large expression spaces; sensitivity to chosen priors and move sets.",
            "success_factors": "Use of learned priors from equation corpora and probabilistic evaluation to focus search on plausible expressions.",
            "uuid": "e2480.7",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "AI Feynman 2.0",
            "name_full": "AI Feynman 2.0",
            "brief_description": "A symbolic regression approach tailored to physics-inspired problems that builds equations modularly using graph modules and improves noise tolerance via an MDL-inspired evaluation (MEDL).",
            "citation_title": "Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "mention_or_use": "mention",
            "system_name": "AI Feynman 2.0",
            "system_description": "Constructs candidate expressions incrementally via graph modularity (graph modules) to exploit structure in equation space and uses a MEDL (MDL-inspired) evaluation function that penalizes complexity relative to error to prune search and improve noise robustness.",
            "system_type": "Symbolic Regression / Equation Discovery System (physics-focused)",
            "problem_domain": "Physics-inspired equation discovery and symbolic regression tasks with structured, modular expressions.",
            "problem_description": "Recover closed-form physical formulas from data, especially when underlying equations can be decomposed into simpler modules; handle noisy measurements more robustly than naive regression.",
            "problem_complexity": "Targets structured expressions where modular decomposition is possible; complexity arises from combinatorial module combinations and noise in data.",
            "data_availability": "Operates on tabular datasets sampled from physical systems; noise-tolerant evaluation aims to handle realistic datasets but requires sufficient samples to constrain modules.",
            "computational_requirements": "Module-based search reduces some search costs; no quantitative compute metrics provided in the survey.",
            "problem_structure": "Continuous regression tasks with exploitable modular structure; deterministic underlying laws with measurement noise.",
            "success_metric": "Recovery of compact, accurate symbolic forms; Pareto-optimal trade-off between complexity and fit using MEDL.",
            "success_rate": "Reported improvements in noise tolerance and modular discovery on benchmarks; no numeric success rates provided in the survey.",
            "failure_modes": "Less effective when no modular structure exists or when data are too noisy/insufficient to identify modules.",
            "success_factors": "Exploiting modularity of equations and an MDL-inspired model selection criterion (MEDL).",
            "uuid": "e2480.8",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Deep Symbolic Regression (DSR)",
            "name_full": "Deep Symbolic Regression (DSR)",
            "brief_description": "A neural-guided symbolic regression method using a recurrent neural network trained with a risk-seeking policy gradient to generate expression trees, aiming to scale symbolic discovery to larger problems.",
            "citation_title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
            "mention_or_use": "mention",
            "system_name": "Deep Symbolic Regression (DSR)",
            "system_description": "Uses an RNN to sequentially generate nodes/tokens of an expression tree; because the objective (fit of produced equation) is non-differentiable, training uses reinforcement learning with a risk-seeking policy gradient that emphasizes best-case performance rather than average-case.",
            "system_type": "Neural-guided Symbolic Regression / Automated Equation Discovery",
            "problem_domain": "Symbolic regression and equation discovery for mathematical/physical modeling tasks.",
            "problem_description": "Generate symbolic expressions that fit data by learning a generative policy over expression tokens, optimized to produce high-performing (low-error) equations.",
            "problem_complexity": "Addresses combinatorial expression search; complexity increases with expression depth, number of functions/operators and input variable count; intended to scale better than pure GP in some settings.",
            "data_availability": "Operates on tabular datasets; model training requires many sampled expressions evaluated on data, so computational cost is coupled to dataset size and training budget.",
            "computational_requirements": "High sampling and evaluation overhead due to reinforcement learning training loop; requires many expression evaluations and model training iterations; no numeric compute-hours provided in survey.",
            "problem_structure": "One-shot supervised regression target with non-differentiable objective; stochastic generation process during training.",
            "success_metric": "Quality (fit/error) and simplicity of discovered expressions; best-case performance emphasized.",
            "success_rate": "Reported to recover many benchmark expressions and scale better than some GP baselines on certain tasks, but no uniform success rate numbers provided in the survey.",
            "failure_modes": "High computational expense for training, sensitivity to reward shaping and RL instability; may still struggle with many variables or highly complex expressions.",
            "success_factors": "Neural policy that learns structural priors and bias toward promising expression patterns; risk-seeking RL objective that amplifies top-performing samples.",
            "uuid": "e2480.9",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Foundation-model-based SR",
            "name_full": "Foundation model / LLM-based symbolic regression (e.g., In-Context SR, LLM-SR)",
            "brief_description": "Approaches that use large pre-trained language / foundation models (LLMs) to propose symbolic expressions or to perform genetic programming operations via prompting/fine-tuning, often followed by external numeric evaluation and iterative refinement.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based Symbolic Regression (In-Context SR, LLM-SR, SymbolicGPT, etc.)",
            "system_description": "Leverages pretrained foundation models (e.g., GPT-family) to generate candidate equations or programmatic representations of equations through prompt engineering or fine-tuning; generated candidates are evaluated numerically, and feedback (fitness, complexity) is provided to the model for iterative refinement; some methods use LLMs to perform genetic programming operations (mutation, crossover) via prompting.",
            "system_type": "Automated Idea Generation / Symbolic Regression System using Foundation Models",
            "problem_domain": "Equation discovery / symbolic regression across physics, engineering and other domains where background knowledge retained in the foundation model may help propose plausible forms.",
            "problem_description": "Produce candidate symbolic expressions or code representing equations for given datasets by leveraging in-context knowledge and generalization from pretraining, then optimize candidates by numerical testing and iterative prompting.",
            "problem_complexity": "Potential to scale via learned priors from pretraining but risks include overfitting to training-set formulas learned during foundation model pretraining; complexity depends on expressiveness of model and prompt design.",
            "data_availability": "Works with tabular datasets; benefits from pretraining on broad corpora but evaluation depends on availability of numeric data to score generated candidates. Paper notes concerns about benchmarks being included in pretraining corpora (data provenance).",
            "computational_requirements": "Costs depend on LLM inference and iterative evaluation; inference costs can be significant for large foundation models but numeric evaluations remain external and parallelizable; no precise compute numbers reported in survey.",
            "problem_structure": "One-shot or iterative supervised regression; black-box generation by LLMs may lack explicit interpretable priors; stochastic generation and possible memorization from pretraining.",
            "success_metric": "Quality of generated symbolic expressions after numeric evaluation (fit, complexity), novelty of proposals and refinement success across iterations.",
            "success_rate": "Survey reports these approaches show promising results in producing initial equations and seeding search; however, the extent of generalization and dependence on pretraining data is unclear and no quantitative success rates are given.",
            "failure_modes": "Memorization from pretraining (benchmarks may be included in LLM training data), unclear generalization to unseen problem types, unpredictable candidate generation quality, and reliance on good prompt design.",
            "success_factors": "Strong prior knowledge encoded in foundation models, ability to flexibly incorporate natural-language domain knowledge via prompts, and ease of adaptation through few-shot prompting or fine-tuning.",
            "uuid": "e2480.10",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Robot Scientists (general / 1stâ€“3rd generation)",
            "name_full": "Robot Scientists (general term for automated closed-loop laboratory discovery systems)",
            "brief_description": "General class of autonomous/semiautonomous systems that generate hypotheses, design and run experiments with lab robotics, analyze results, and iterateâ€”ranging from early systems (Adam) to current third-generation self-driving labs across many domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Robot Scientist (general)",
            "system_description": "Encompasses systems that automate parts or the full scientific cycle: hypothesis generation (abduction/induction), experimental design, automated execution via lab robotics, data analysis and iterative replanning. Examples include Adam, Eve, and many contemporary self-driving labs in chemistry, materials, biology and catalysis.",
            "system_type": "AI Scientist / Autonomous Discovery System / Self-driving Lab",
            "problem_domain": "Multiple scientific domains: functional genomics, drug discovery, chemistry, materials science, catalysis, cell biology, protein design, quantum physics, etc.",
            "problem_description": "Automate end-to-end experimental cycles for domain-specific discovery tasks: identify gene functions, screen compounds, optimize reactions/materials, discover catalysts, etc., within the constraints of the lab automation available.",
            "problem_complexity": "Varies widely by domain: from low-dimensional, well-structured assays to high-dimensional material/combinatorial chemical spaces; common complexities include noisy experimental measurements, high cost/time per experiment, and massive combinatorial search spaces.",
            "data_availability": "Often generate their own experimental data in closed-loop; pre-existing data and domain knowledge are used to seed hypotheses. Data acquisition cost and throughput are primary scaling bottlenecks.",
            "computational_requirements": "Compute for modelling and planning is moderate; dominant resource is experimental throughput and lab automation logistics. Specific compute costs depend on algorithms used (e.g., active learning, RL) but are not quantified in the survey.",
            "problem_structure": "Typically well-defined experimental objectives but sometimes open-ended discovery; mixture of discrete decision-making (which experiments to run) and continuous modeling (surrogate models). Evaluation metrics include discovery of new knowledge, experimental success, and reproducibility.",
            "success_metric": "Novel discoveries validated experimentally, efficiency (experiments-to-discovery), reproducibility, and sometimes downstream impact (e.g., drug repurposing hits).",
            "success_rate": "Survey reports multiple domain successes (Adam: 6 genes; Eve: drug repurposing hit TNP-470), and many operational self-driving labs in industry and academia; however, no unified success-rate statistics reported across systems.",
            "failure_modes": "Limited experiment types supported by the robots, high capital/maintenance costs, difficulties in automating experimental design beyond parameter sweeps, requirement for specialist staff, and the challenge of forming genuinely novel theories or new measurement devices.",
            "success_factors": "Strong integration of domain knowledge, reliable lab robotics, efficient experimental design (active learning), and closed-loop iteration enabling reproducibility and throughput gains.",
            "comparative_results": "Paper contrasts strengths of closed-loop lab automation (scalability, reproducibility) with limitations in forming novel high-level theory and communicating interpretable equations; symbolic regression systems excel at interpretable models but lack embodiment.",
            "human_baseline": "No general human baseline is provided; paper notes AI Scientists can work cheaper/faster/longer than humans and can be multiplied, but direct quantitative human-vs-system comparisons are sparse in the surveyed literature.",
            "uuid": "e2480.11",
            "source_info": {
                "paper_title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The automation of science",
            "rating": 2,
            "sanitized_title": "the_automation_of_science"
        },
        {
            "paper_title": "Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases",
            "rating": 2,
            "sanitized_title": "cheaper_faster_drug_development_validated_by_the_repositioning_of_drugs_against_neglected_tropical_diseases"
        },
        {
            "paper_title": "Automated discovery in a chemistry laboratory",
            "rating": 2,
            "sanitized_title": "automated_discovery_in_a_chemistry_laboratory"
        },
        {
            "paper_title": "Bacon: A production system that discovers empirical laws",
            "rating": 2,
            "sanitized_title": "bacon_a_production_system_that_discovers_empirical_laws"
        },
        {
            "paper_title": "Discovering dynamics",
            "rating": 2,
            "sanitized_title": "discovering_dynamics"
        },
        {
            "paper_title": "Distilling free-form natural laws from experimental data",
            "rating": 1,
            "sanitized_title": "distilling_freeform_natural_laws_from_experimental_data"
        },
        {
            "paper_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "rating": 2,
            "sanitized_title": "discovering_governing_equations_from_data_by_sparse_identification_of_nonlinear_dynamical_systems"
        },
        {
            "paper_title": "Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity",
            "rating": 2,
            "sanitized_title": "ai_feynman_20_paretooptimal_symbolic_regression_exploiting_graph_modularity"
        },
        {
            "paper_title": "Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients",
            "rating": 2,
            "sanitized_title": "deep_symbolic_regression_recovering_mathematical_expressions_from_data_via_riskseeking_policy_gradients"
        },
        {
            "paper_title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "rating": 1,
            "sanitized_title": "incontext_symbolic_regression_leveraging_large_language_models_for_function_discovery"
        }
    ],
    "cost": 0.02194325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems
26 May 2025</p>
<p>Stefan Kramer 
Computer Science Department
Johannes Gutenberg University Mainz
Saarstrasse 2155116MainzGermany</p>
<p>Mattia Cerrato mcerrato@uni-mainz.de 
Jannis Brugger jannis.brugger@tu-darmstadt.de 
hessian.AI
Karolinenpl. 564289Darmstadt, DarmstadtTUGermany</p>
<p>SaÅ¡o DÅ¾eroski saso.dzeroski@ijs.si 
Dept. of Knowledge Technologies
Jozef Stefan Institute
Jamova cesta 391000LjubljanaSlovenia</p>
<p>Ross D King 
Data Science and AI
Chalmers University of Technology
Chalmersgatan 441296GÃ¶teborgSweden</p>
<p>Department of Chemical Engineering and Biotechnology
University of Cambridge
Philippa Fawcett Drive, Cambridge WestCB3 0ASUnited Kingdom</p>
<p>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems
26 May 2025AE248F4F879C6B6F93DFF263C3D0037DarXiv:2305.02251v2[cs.AI]
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents.It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge.Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy.Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving.The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge.Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.</p>
<p>Introduction</p>
<p>The automated discovery of scientific knowledge has always been on the agenda of artificial intelligence research, and prominently so since the end of the 1970s [1,2].Scientific knowledge takes many forms: In many cases, the scientific process begins with collecting and classifying objects, and creating taxonomies of classes of objects.The more a scientific discipline advances, the more it tends to strive to describe the phenomena quantitatively, for better explanation and prediction.By far the most commonly used representation for describing systems of interest is in the form of mathematical equations, in particular differential equations.Thus, the automated discovery of equations from data has been established as a family of methods within and partly outside artificial intelligence: it runs under the heading of equation discovery [1,3] as well as symbolic regression [4].</p>
<p>The goal in many application domains of equation discovery and symbolic regression is to learn a human-understandable model of the system dynamics in the form of (mostly ordinary) differential equations. 1One important aspect of scientific discovery is that the resulting models need to be in principle interpretable. 2 Thus, the goal is not optimization (e.g., of properties in material science or drug development), but to develop understanding.</p>
<p>An important part of the literature on automated scientific discovery [2,5] discusses the topic from a cognitive science point of view (what are or could be the reasoning processes leading to certain discoveries?) and thus also a historical reconstruction of the processes.This is relevant, because today's AIs for scientific discovery also have to start from the same principles to enable discoveries in completely new application domains.While this can be viewed on the symbolic level only, many of today's approaches also consider the subsymbolic level to aid the process: neural networks of various sorts can play a vital role in guiding the search, providing valuable information to the discovery agent, or turning low-level sensory information into high-level information that can be used for symbolic reasoning.</p>
<p>Finally, the question of autonomy of the discovery agents arises.While early systems assumed a table of input data is given by a human user, approaches with more autonomy on the side of the discovery agent are becoming more common.The approach became prominent with the development of the first robot scientist world-wide, Adam [6], that automated cycles of hypothesis generation and testing in the field of functional genomics.Meanwhile, the third generation of robot scientists is being developed.The degrees of autonomy of a discovery agent may range from completely passive, i.e., supervised learning, via active learning [7] to reinforcement learning [8].</p>
<p>Considering the above, this paper aims to give an overview of automated scientific discovery from a conceptual point of view, spanning the whole field from the generation of scientific knowledge, mainly in the form of equations, to automation and autonony in robot scientists or self-driving labs.It does not just enumerate approaches, but discusses central conceptual aspects and open issues that need to addressed in future systems.Particular attention is paid to the role of neural networks in the process: Fig. 1 Overview of the two realms of automated scientific discovery: (i) the discovery and communication of human-interpretable knowledge in a representation used by scientists in the field, e.g., equations (right-hand side) and (ii) autonomy and automation in science (left-hand slide).Approaches integrating both are currently rare.</p>
<p>either for representation learning, for search in neural-guided equation discovery, or in neural operators, which abandon interpretability altogether.Discussing two main aspects of automated scientific discovery side by side in one paper, (i) the discovery of interpretable scientific knowledge in the form of equation on the one hand and (ii) automation/autonomy on the other (see Figure 1), we identify a major research gap: systems that run autonomously, but are able to communicate results in formalisms used by scientists, so that interventions are possible, such as hints for search, the provision of goals and values, and the embedding of findings in bigger theories.Very few systems exist in this space, however, we would like mention the pioneering work of Jan Zytkow, who coupled real electrochemistry experiments with the FAHRENHEIT system for equation discovery [9], and later proposed a robotic system for the rediscovery of Galileo's equation of objects rolling down an inclined plane, again with the help of FAHRENHEIT, but already taking into account empirical error [10].</p>
<p>The paper is structured as follows: In Section 2, we will review equation discovery and symbolic regression from the beginnings to the current state of the art, with a list of open problems.In Section 3, we discuss the representations used in current scientific discovery and, in particular, how neural networks can be employed to learn representations for the discovery process and how dynamics can be learned directly by neural operators.The topic of Section 4 is closed-loop scientific discovery, with recent progress in the field.Section 5 discusses different levels of autonomy.An overview of benchmarks and testbeds is given in Section 6, before we conclude in Section 7.</p>
<p>The survey paper is different from existing papers in many ways: Makke and Chawla [11] presented a thorough survey of symbolic regression (SR) and equation discovery (ED).Our survey covers both SR/ED and automation/autonomy, so it is broader in scope.Also, it appears more conceptual and with a stronger focus on interesting open issues.Further, in the present paper the discussion of the various uses of neural networks appears both more extensive and deeper.In a recent study, Musslick et al. [12] discuss primarily the limitations of autmated scientific discovery, with a focus on societal and ethical implications (e.g., the value alignment of robot scientists with human scientists).It discusses what should not be done, but also what potentially cannot be done.The latter is, of course, harder to argue, as it involves a forecast of the further progress of the field of artificial intelligence in general.Arguments likes the paradox of automation hold, others concerning the computational complexity of scientific discovery require more investigation.Another recent survey by Gao et al. [13] focuses on life sciences exclusively and discusses everything in terms of agentic AI, which is both not our emphasis here.Two recent papers by Pat Langley [14,15] are both related, but at the same time different.The first of them [14] discusses the so far distinct notions of "agents of exploration" and "agents of discocery".Langley argues for a synthesis of the two, such that agents can both explore and discover in remote areas, like in space or in the deep sea.Although conceptually relevant (imagine a versatile scientific agent that explores a lab environment and discovers new concepts and laws along the way), the main thrust of the paper is clearly different.In the more recent paper [15], Langley describes an integration effort different from the one shown above: In the paper, he envisages a tight integration and coupling of the various phases of scientific discovery, from the discovery of taxonomic knowledge via qualitative models to quantitative and causal models.It is argued that this integration is important, but has not been achieved before.We believe that, while interresting, this is of a different nature than the integration between the discovery of scientific, human-interpretable knowledge, and automation and autonomy in robot scientists or self-driving labs (see Figure 1).</p>
<p>From BACON to Modern Equation Discovery</p>
<p>and Symbolic Regression</p>
<p>History and Current Approaches</p>
<p>The first system for the discovery of equations based on data was BACON by Pat Langley [1], represented in Figure 2. The first version of BACON was developed into a series of following systems, BACON.2 to BACON.5, with increasingly complex functionality [2].The basic philosophy behind the book by Langley et al. was that scientific discovery, even in its most intricate ways, is essentially problem solving.This even applies to the search for new problems, new representations, and new measurement devices.In the case of the BACON systems, the idea was applied to the discovery of equations.BACON.1 to BACON.5 were implemented on the basis of PRISM, a system for the representation and inference of production rules.The BACON systems relied on the observation of the correlation of pairs of variables, when everything else is being held constant (ceteris paribus).This is a strong assumption, as it will in many cases not be possible to control all other variables in an experiment.Also, interestingly, BACON has a flavor of active learning, since users are requested to record data, if they are not available yet.One interesting feature of BACON is the construction of new terms, e.g., ratios or products of existing terms, by production rules.In this way, Fig. 2 (a) BACON [1,2] (b) Example of context-free grammar guiding the search for equations in the Lagramge system [16] (c) A probabilistic context-free grammar as used in ProGED [17] (d) Symbolic regression [18] it takes advantage of the structure of the search space, which is rarely ever attempted in current systems.Noise handling is achieved by some tolerance parameter, which establishes that a value of a variable (constructed or initially given) is constant, even though it varies within a certain range.BACON.2 to BACON.5 included advanced features for symmetries, common divisors, and conservation laws, amongst others.Fig. 1(a) shows the derivation of Kepler's third law D 3 /P 2 = k by a sequence of newly constructed terms, until a -more or less -constant value is found.</p>
<p>The next generation of equation discovery systems was not restricted to keeping all but a pair of variables fixed, but was able to handle observational data.In addition, it was able to learn models of dynamical systems in the form of ordinary differential equations (ODEs).Lagrange [3] computes all derivatives up to a pre-defined order, then generates products of up to a maximum of variables, before it calculates a simple linear regression to generate a candidate equation.More recently, this approach has been taken up in the SINDY system [19], which applies sparse (instead of simple) linear regression.In the meantime, the method has been extended to capture nonlinear dynamics by shallow recurrent decoder networks (SINDy-SHRED) [20].The successor of Lagrange, named Lagramge [16], was a milestone in equation discovery, as it introduced the use of domain knowledge in addition to data: It was the first system to use a context-free grammar (CFG) to guide the search for systems of equations.Grammars are a way for domain experts to use prior knowledge and let that knowledge guide the search for equations.In this way, Lagramge was able to solve problems that the predecessor Lagrange was not able to solve, for instance, the problem of two poles on a cart.An example CFG for Lagramge is shown in Figure 1(b).Lagramge GSAT [21] improves Lagramge by a bundle of modifications: first, it uses a search procedure similar to GSAT (random restart hillclimbing) to randomize search; further, it employs a one-step look-ahead and a momentum to make the search less erratic.Washio &amp; Motoda [22] further improved the methods by also taking into account units and scale types as constraints.Dimensional units are also considered for use in ProGED [17,23], which is based on the idea of using probabilistic CFGs to represent the search space and sample from it.An example is given in Fig. 1(c), where both the rules and the probabilities associated with the rules (p and q) are shown.These probabilities can be fixed, but can also be learned from corpora of equations [24].Sampling candidate equations from probabilistic CFGs enables easy parallelization: batches of sampled equations can be distributed to nodes and evaluated in an embarrassingly parallel way.</p>
<p>Symbolic regression, a development parallel to the development of equation discovery, was originally based on genetic programming (GP): the term was introduced by Koza [4].Typical systems of symbolic regression work on an operator tree or DAG representation of equations (see Fig. 1(d)).These trees are modified by a set of possible operations, such as crossover between subtrees of two trees (equations), mutations, substitutions of variables by constants, or, vice versa, substitutions of constants by variables.Schmidt &amp; Lipson [18] used symbolic regression to discover natural laws from measured data.Symbolic regression approaches were used early on to discover ODEs [25] and used ideas from grammar-based genetic programming to consider domain-specific knowledge, paving the way for systems that use both data and domain knowledge in equation discovery, such as Lagramge, Lagramge2.0[26], IPM [27] and Prob-MoT [28].The last three use process-based formalism to represent models and domain knowledge.</p>
<p>The Bayesian machine scientist [29] establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical set of mathematical expressions.The space of equations is explored via Markov Chain Monte Carlo (MCMC), with specific moves for mathematical expression sampling.</p>
<p>PySR [30] is a fast, effective and popular approach to symbolic regression.It is based on genetic programming and outputs one solution per complexity class (from simple to complex equations).PySR is frequently found to be well-performing in practice.It has a Python front-end and delegates numerical computations to Julia.Using Julia "under the hood" and heuristics to avoid redundancy, it is able to explore a large number of candidates in a relatively short period of time, giving it a competitive advantage in many situations.In the meantime, version 1.4 of PySR is available with template expressions and version 1.5 with mini-batching, which further improves practical applicability.</p>
<p>Recent work by Boris KrÃ¤mer and collaborators [31] has advanced the use of quadratic models for data-driven discovery of dynamical systems governed by partial differential equations (PDEs).In particular, they explore transformations of nonlinear PDEs into quadratic form, which enables the application of structure-preserving reduced-order modeling and symbolic regression techniques.The approach facilitates the use of quadratic latent variable models that retain interpretability and allow for efficient training on noisy and sparse data.The usefulness of the approach has been demonstrated in areas such as fluid dynamics and plasma modeling.</p>
<p>Symbolic regression and equation discovery are currently limited to systems with only few variables.Xue et al. [32] address this problem by identifying control variables, which can be varied to discover the dynamics of a system in "controlled experiments" step by step.The approach is still based on genetic programming.A precondition of its use is evidently the existence of such variables, which is not always the case.In practical applications and real systems, the set of control variables is not equal to the set of variables that should appear in an equation.Thus, that mapping has to be learned first.Nevertheless, the idea of actively using control variables to reduce complexity is valuable and could be a key to making ED/SR practically applicable to large and complex sytems.</p>
<p>In recent years, a new field of research has emerged that focuses on how neural networks can be used in equation discovery.To provide an overview, we divide the works into three categories.The categories are: (i) NN as a supporting module in the equation discovery system (EDS), (ii) NN as the main component of the EDS, and finally, (iii) foundation models as EDS.We discuss the three categories in consecutive order.</p>
<p>AI Feynman 2.0 [33] is a recent symbolic regression approach that aims to improve its predecessor (a) by structuring the search space by building the equation in meaningful increments and (b) making it more noise-tolerant.The first goal is achieved by graph modularity, i.e., constructing the equations by so-called graph modules.It should be noted that, in doing so, it is one of the few approaches that takes advantage of the structure of the search space (instead of just brute-force search, sampling or "blind" randomized traversal).The second goal is achieved by employing an MDLinspired evaluation function instead of the RMSE.This function is called MEDL in Feynman 2.0.Using MEDL, effective pruning can be developed, because the complexity of the equation can be balanced against its error.Lusch et al. [34] apply an auto-encoder structure to find a coordination transformation for a differential equation that maps the nonlinear original problem to linear embedding.Following the idea of an autencoder, MeÅ¾nar et al. [35] embed equations in a low-dimensional latent space and use this smooth latent space to suggest new equations based on genetic programming.Mundhenk et al. [36] use a Recurrent Neural Network (RNN) to seed a genetic programming algorithm, and the genetic algorithm results are used to train the RNN.While the previous works use a subsymbolic component to simplify the original problems, the following articles use neural networks as main component.</p>
<p>Deep Symbolic Regression (DSR) [37] addresses the problem of GP approaches with finding solutions for larger problems.It employs a recurrent neural network to build an equation tree step by step.As the objective function (of fitting a low-error equation) is not differentiable, a reinforcement learning approach is proposed.More specifically, DSR employs a risk-seeking policy gradient, which maximizes the best-case performance, not the average-case performance.NeSymReS [38], SymbolicGPT [39], and E2E [40], use a transformer-based architecture to predict the equation on a token level.The main difference is the embedding architecture of the data set.MGMT [41] compares different embedding methods and shows their influence on the prior of the guiding network.Additionally, the work shows that supervised learning is beneficial compared to reinforcement learning for the architectures considered.TPSR [42] and DGSR-MCTS [43], combine a transformer-based architecture with a Monte Carlo Tree Search (MCTS).In the second paper, the network suggests how to mutate the current equation.Another approach is to train a specialized end-to-end differentiable network and parser it after the training with gradient descent to an equation.EQL Ã· [44] or Kolmogorov Arnold networks (KAN) [45] are examples for this approach.</p>
<p>Large language models (LLMs) have also impacted the field of equation discovery.Foundation models such as GPT-4 have the advantage that after the initial learning, they only need to be adapted to the equation discovery domain through fine-tuning or prompt design.In addition, they have been shown to retain background knowledge from the initial training, and the user can add domain knowledge through prompts.In-Context Symbolic Regression ICSR [46], and Sharlin et al. [47] employ a foundation model to produce initial equations.These equations are then tested on the data set.The fitness score and other measures, such as complexity, are calculated externally and then fed back to the model with the task of refining the solutions.LLM-SR [48] follows the same idea but represents equations as programs and uses comments in the program to make the discovered equation better understandable.Meyerson et al. [49] use a foundation model to perform genetic programming (mutation, crossover, etc.) through prompts.The foundation model-based equation discovery systems show promising results, but the extent to which the initial training influences the test results</p>
<p>has not yet been sufficiently investigated, as the standard benchmarks (see below) are included in the initial training.</p>
<p>Open Problems</p>
<p>In equation discovery and symbolic regression, a few open problems can be identified:</p>
<p>â€¢ It remains hard to exploit structure in the space of equations to guide the search to promising parts of the search space.Opportunities for pruning would also be helpful.â€¢ At least in the case of differential equations, fitting the model is the most expensive part.Ways of stopping the fitting process if it turns out to be unpromising would save a lot of computation time.â€¢ Equations are "brittle": properties of differential equations can change dramatically with only little syntactic modifications.Minor changes can lead to no solutions, one solution, or many solutions.â€¢ Most approaches struggle with a dimensionality of the problem higher than a very small number of variables.â€¢ Overfitting avoidance and regularization: The syntactic complexity of an equation does not necessarily correspond to the complexity of the function in the feature space.Meaningful ways to approximate or bound complexity would be helpful.â€¢ For the approaches based on foundation models, it is unclear how the results can generalize to new, previously unseen problems.Data provenance is an issue: It is unclear whether the models have seen some of the equations before in training.Many of the approaches are based on embeddings of datasets.It is, at this point, not clear, what the best way is to embed a dataset for a foundation model for symbolic regression.â€¢ Relating discovered equations to existing theory or making the equations consistent with it remains a big challenge.Quite related, it is not clear whether or how "understanding of the physical meaning" of variables can be achieved.</p>
<p>3 Representation Learning in Scientific Discovery</p>
<p>Representation Learning of the Input</p>
<p>The standard representation of data for scientific discovery is tabular data (see, e.g., also the tables in the book by Langley et al. [2] and Figure 1(a)).However, recent years have seen a surge of papers that use neural networks as an intermediate representation to aid in the discovery of models.</p>
<p>One notable example is the work of Miles Cranmer and Shirley Ho [50], who proposed Graph Neural Networks (GNNs) as an intermediate representation.GNNs were used to learn about the interaction of objects, in terms of, for example, forces that act upon each other.Classical examples include n-body problems or, more specifically, orbital mechanics-the motion of planets and other larger objects in our solar system.The nodes in the graph represent the objects, which are annotated by feature vectors representing the properties of the objects.The edges in the graph represent the interactions between the objects and are annotated by properties that partially depend on those of the objects.For example, one may consider the masses of planets as properties of the nodes, and the distance and gravitational force between the objects as properties of the edges.When learning GNNs, typically, so-called node models Ï• v are updated depending on the edge models Ï• e of neighboring edges and, alternatingly, the edge models Ï• e are updated based on the node models Ï• n of the nodes that the edges connect.Update steps are frequently framed as message passing, and pooling functions aggregate input from multiple edges connected to one node.GNNs usually can be trained end-to-end, but are not guaranteed to converge.</p>
<p>In the application domain that was given as an example, orbital mechanics, the input to the system are (x, y, z) coordinates of the Sun, all planets, and all moons with a mass above 10 18 kg.Data from 1980 to 2013 were used with time intervals of 30 minutes each, with the first 30 years for training and the last 3 years for validation.</p>
<p>Garcon et al. [51] proposed a method to predict known physical parameters and discover new ones from oscillating time series (Figure 4).The method is trained on a large set of synthetic time series.The latent parameters used to generate the monochromatic sine waves are the carrier frequency, F c , and phase Ï• (which is mapped for technical reasons to two separate parameters, sin(Ï•) and cos(Ï•)), in addition to the coherence time Ï„ .The AM and FM sine waves are generated by adding a modulation function to the carrier.The modulation function's latent parameters are the modulation frequency F m and amplitude I m .Noise is linearly added to the pure signals by sampling the Gaussian distribution.AM/FM signals with minimum I m reduce to decaying monochromatic sine waves and reach 100% modulation with maximum I m .These latent parameter ranges are wide enough such that they would encompass many foreseeable real-world signals.Figure 3 shows the neural network architecture used to predict the latent parameters, with an autoencoder-type subnetwork to support the prediction.The method can be used to discover new parameters (not just predict known ones) and reconstruct equations producing input time series.</p>
<p>The situation is clearly more complex when the observations are given as videos instead of tabular data.Chen et al. [52]   current state-of-the-approach to computing latent variables is to define an autoencoder with a bottleneck layer of the right dimension.The dimension should be large enough to allow faithful reconstruction by the decoder, but small enough so that the latent variables are non-redundant.The goal of the proposed method is to have the number of dimensions (i.e., the number of neural state variables) as close as possible to the degrees of freedom of the observations in the videos.In technical terms, the number of dimensions should be close to the so-called intrinsic dimension (ID), which is the minimum number of independent variables needed to fully describe the state of a dynamical system.Various methods from manifold learning, for instance the one by Levina and Bickel [53], are known to efficiently calculate an estimate of the intrinsic dimension.It would be tempting to calculate the intrinsic dimension for the videos and then use it as the bottleneck size of an autoencoder to come up with the latent variables.However, practically, information becomes blurry at much larger bottleneck sizes than the ID already.Therefore, Chen et al. take a two-step approach and define two autoencoders, one regular and one that maps the latent variables of the first to further ID latent variables.These are the neural state variables that can be used for downstream analysis.The approach has not yet been made explainable for scientific discovery.</p>
<p>Fig. 4 Neural network architecture of model that extracts known and unknown physical parameters from oscillating time series [51].</p>
<p>Generally speaking, neural networks are used in this domain for â€¢ making the data sparse in the sense of removing small to negligible interactions [50],</p>
<p>â€¢ a change of representation (e.g., from coordinates to distances depending on some variables [50]), â€¢ data augmentation (to sample arbitrarily large data from the neural network and also smooth the data in that way [5,50]), â€¢ the prediction of important parameters to be used in equations directly [51], and â€¢ extracting latent variables from low-level input representations (e.g., neural state variables from videos [52]).</p>
<p>Representation Learning of the Dynamics and Beyond</p>
<p>Neural operators [54] can learn to map the current state of a system to the next state.This can be done for systems that evolve over space or time and especially for systems for which partial differential equations (PDEs) are too difficult to solve.Neural operators are, however, not restricted to mapping from one state to the next over time: They can learn general functional mappings between various types of inputs and outputs, e.g., inititial conditions to solutions or, even more generally, function-tofunction mappings (like DeepONet [55] or Fourier Neural Operators [56]).The latter learn mappings between functions, not just states over time, for instance, they can map a boundary condition (a function) to a solution (another function), which might involve non-temporal variables.Advantages are, amongst others, speed and flexibility (they are not hard to apply from one problem to the next).Neural operators like DeepONet or Fourier Neural Operators are, like other neural networks, black-box models.</p>
<p>Open Problems</p>
<p>Several open problems remain for representation learning of the inputs or learning functional mappings using neural networks:</p>
<p>â€¢ It is currently not well-investigated how learned representations can be aligned with representations that are interpretable by humans.â€¢ While neural operators can find accurate approximations to the solution of a PDE, understanding how they arrived at that solution is not straightforward.Visualizations, sensitivity analyses, and methods from explainable AI can alleviate some of the problems.</p>
<p>4 Closed-loop Scientific Discovery</p>
<p>Main Concepts, History and Advantages</p>
<p>The cutting edge of applying AI to science are "AI Scientists" (aka "Robot Scientists", "Self-driving Labs", "Autonomous Discovery systems", "Machine Scientists", etc.).These AI systems area capable of the closed-loop automation of scientific research.AI Scientists were named in 2025 by Nature as the "number one technology to watch" [57].AI Scientists automatically originate hypotheses to explain observations (abduction/induction), devise experiments to test these hypotheses (deduction), physically run the experiments using laboratory robotics, analyze and interpret the results to change the probability of hypotheses, and then repeat the cycle.In other words, they aim to automate all or parts of the scientific method, as shown (simplistically) in Figure 5.As the experiments are conceived and executed automatically by computer, it is possible to completely capture and digitally curate all aspects of the scientific process, making science more reproducible [6].</p>
<p>The first contribution describing a largely autonomous system which discovered new knowledge was due to Ross D. King and his group [6], who developed the Adam robot scientist (see Figure 6).Adam identified 6 genes encoding orphan enzymes in yeast (Saccharomyces cerevisiae), i.e., enzymes which catalyze reactions occurring in  yeast for which the encoding genes were not known at the time.The system was provided with a freezer, liquid handlers, plate readers, robot arms, and further actuators, enabling yeast cultivation experiments lasting as long as 5 days.Yeast growth was measured via optical sensors.On the software side, Adam was provided with an extensive Prolog knowledge base describing known facts about yeast metabolism.Hypotheses were formed by abduction, enabled by a combination of bioinformatic software and databases, after which an experiment planning module was responsible for selecting metabolites to be inserted in the yeast's growth medium.</p>
<p>Another successful example of laboratory automation is Eve.Originally developed for high-throughput drug screening [58], the system was then instrumental in discovering that several existing drugs could be repurposed to prevent tropical diseases [59].Most prominently, it found that an anti-cancer compound (TNP-470) could be employed against the parasite Plasmodium vivax, whose bite is the most frequent cause of recurring malaria.The system is able to hypothesize and test quantitative structure-activity relationships (QSARs) via a combination of active learning and Gaussian process regression (GPR).GPR is employed to learn a QSAR f mapping the characteristics of compounds to a response variable indicating the strength of the biological activity; then, the obtained function f is employed as a noisy oracle to select K compounds out of a pool of possible candidates.Exploration and exploitation is balanced.This two-step process may be repeated until enough candidates are obtained.In the meantime, the third generation of robot scientists is being developed.</p>
<p>AI Scientists have a number of relevant advantages, besides being able to discover new knowledge in a way that may be less biased than a human scientist:</p>
<p>â€¢ Efficiency: AI Scientists are increasing the productivity of science.They can work cheaper, faster, more accurately, and longer than humans [60].They can also be easily multiplied.â€¢ Reproducibility: Biomedical science is facing a "reproducibility crisis".AI Scientists have the potential to ameliorate this problem, as they describe experiments in far greater detail and semantic clarity than human scientists, and robots execute experimental protocols more accurately than human scientists [61].â€¢ Robustness: The Covid-19 pandemic clearly demonstrated the vital importance of biomedical research and the critical need to maintain research continuity [62].AI Scientists are increasingly being applied to multiple scientific domains (ranging from quantum mechanics to astronomy, from chemistry to medicine), see Table 1.</p>
<p>Open Problems</p>
<p>Three of the current main limitations of AI Scientists are (i) the design of novel experiments, (ii) integration with laboratory robotics, and (iii) the formation of completely new hypotheses and theories.The central task that faces every experimental scientist is the design of novel experiments to test a hypothesis.The abstract problem is given (1) a hypothesis, and (2) a set of laboratory equipment, output (3) a protocol to test the hypothesis using the equipment.Relatively little AI research has focussed on this aspect of automating science.N.B. that this task is different in kind from the task of traditional "experimental design", it also different from deciding, from a set of given experiments, the most efficient (in terms of time/money) to test a set of hypotheses.In all the existing AI Scientists systems that we are aware off the type of experiment that can be executed are limited to a small stereotypical set.For the design of novel experiments to be possible it will be necessary to formalise general scientific knowledge, as well as knowledge about the functionality of laboratory equipment, and experimental protocols.It is also necessary to develop inference and planning engines to generate the new experiments, as well as to develop compilers to translate generated experiments into executable protocols on specific laboratory automation.</p>
<p>Historically, laboratory automation has been driven by the desire to run large numbers of related laboratory experiments, especially in the pharmaceutical and clinical analysis industries.It is now a thriving multibillion dollar industry [63].The first use of AI to control laboratory equipment was probably that of Zytkow et al. [9] (see above).The technology of laboratory automation is steadily advancing, and robots can now carry out most (but not all) of the tasks that humans can do in the laboratory.Such laboratory automation is increasing the productivity of science as robots can work cheaper, faster, more accurately, and for longer (24/7) than humans, they can also be more easily increased/reduced in number.Laboratory automation still has many limitations.Robots typically today operate in protective boxes and are hard to program by bench scientists; logistics tasks are generally performed by lab technicians and scientists, with humans tending the robots for consumables; laboratory automation is expensive in capital to build and maintain -requiring specialised staff.Research in laboratory automation has been largely divorced from AI robotics research -which has mainly focused on the problem of mobile robots.Almost all laboratory robots are fixed in place, although there is growing interest in mobile robot assistants [62].</p>
<p>Hypothesis formation needs to be supported by a variety of AI and ML methods, from knowledge representation to active learning and reinforcement learning.The creation of a whole new theory, with theoretical terms and new measurement devices, is at least one level of complexity harder and has not been addressed yet at all.</p>
<p>Autonomy</p>
<p>One key aspect of AI Scientists is their degree of autonomy.One approach to measuring autonomy is to use the classification of degrees of autonomy in self-driving cars as [63].The approach taken here is similar, Table 2 describes five levels of autonomy.Beyond levels of autonomy are levels of skill.All human drivers are autonomous, but very few are skillful enough drivers to win a Formula 1 race.Among human scientists there are also levels of skill, with few human scientists being skillful enough to win Nobel prize.AI scientists are improving in autonomy and skill.Extrapolating this trend it is likely that advances in technology and our understanding of science will drive the development of ever-smarter AI Scientists.The Physics Nobel Frank Wilczek said that "in 100 years' time the best physicist will be a machine" [64].In Closed-loop automation.The full cycle of discovery is automated in a restricted domain.</p>
<p>See Table 1.</p>
<p>High Automation</p>
<p>Closed-loop automation.Multiple scientific domains.Limited ability to set its own goals.</p>
<p>No existing system.</p>
<p>Full Automation</p>
<p>All aspects of science are automated and no human intervention is required.</p>
<p>No existing system.</p>
<p>February 2020 a workshop was held in London to kick-off the Nobel Turing Grand Challenge to develop: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050 [65].If the Nobel Turing Grand Challenge is achieved this would clearly transform the World, it would be possible to have instead of a few Nobel prize winning ideas a year, hundreds, thousands, millions, ...</p>
<p>Evaluation and Testbeds</p>
<p>The evaluation of an autonomous discovery system is intrinsically tied to the levels of autonomy displayed by the methodology at hand and which steps of the scientific process are to be automatized and the level of autonomy being evaluated (Figure 5 and Table 2).Equation discovery methods may help in automating the analysis of experiments by providing human-readable knowledge, while systems with physical actuators may be evaluated in their ability to execute experimental protocols.Thus, evaluation methodologies and benchmarks in the area have different characteristics in terms of supervision, data modalities, scope and open-endedness.We define these properties in the following, and give a table of existing methods for evaluation in Table 3. Supervision.Supervision refers to the nature of the ground truth or reward signals provided to the autonomous discovery system during training and evaluation.Depending on the degree of autonomy assessed, supervision may range from explicit labels or predefined objectives to feedback signals (rewards in the Reinforcement Learning sense [8]).The type and quantity of supervision significantly affect the evaluation outcome, as they directly influence the system's capability to navigate scientific exploration autonomously.</p>
<p>Data Modalities.Data modalities encompass the types and formats of data available for evaluation, such as pixel-based images, textual descriptions, numerical tables, or structured representations of experimental observations.The choice of modality greatly impacts the complexity and applicability of autonomous systems, as certain data forms inherently require more sophisticated methods for interpretation, abstraction, and knowledge extraction (see Section 3).Evaluating systems across diverse data modalities helps in understanding their flexibility, generalizability, and robustness in real-world scientific scenarios.</p>
<p>Scope.Scope defines which specific phases of the scientific discovery process the evaluation benchmark addresses.This includes one or more of the six distinct steps: scientific question formulation, hypothesis generation, experimental design, execution of experiments, data analysis and communication.</p>
<p>Open-endedness.Open-endedness characterizes whether the benchmark or evaluation method includes previously unexplained data, phenomena lacking known mathematical descriptions, or allows the formulation of novel scientific questions.An open-ended benchmark challenges autonomous discovery systems to demonstrate genuine exploratory capabilities, creativity, and adaptability, rather than merely replicating existing knowledge.</p>
<p>We now move to introducing benchmark and testbeds while discussing their potential in the autonomous discovery setting.We will not offer here an exhaustive survey of symbolic regression benchmarks.</p>
<p>Available Benchmarks</p>
<p>Nguyen Benchmark Suite [66] is a widely-used collection of symbolic regression problems introduced specifically to evaluate genetic programming (GP) methods.It consists of synthetic mathematical equations designed with varying complexity and structure, aiming to assess the ability of GP algorithms to accurately recover symbolic expressions from numerical data.Each task provides numerical input-output pairs generated from known symbolic formulas.The benchmark primarily evaluates one-shot analysis of already collected experimental data.</p>
<p>Feynman [67] provides a comprehensive symbolic regression benchmark inspired by fundamental physics equations from the Feynman Lectures on Physics.This dataset includes 120 symbolic regression tasks covering a diverse range of physics phenomena, from classical mechanics to electromagnetism.</p>
<p>Matbench [68] is a supervised machine-learning benchmark containing 13 prediction tasks related to materials science.The dataset consists of structured data representing chemical formulas and crystalline structures, with tasks that involve predicting material properties such as band gap or elastic moduli.It is particularly suited for evaluating analysis capabilities and hypothesis generation for material properties from compositional and structural data.While each task is narrowly defined with a fixed prediction goal, collectively, they support evaluating broad generalizability across material science domains.</p>
<p>SCP-116K [69] is a large-scale textual dataset comprising problem-solution pairs extracted from higher education science textbooks and other academic sources, totaling 116,000 entries.It is designed primarily for supervised training and evaluation of models on scientific reasoning, question answering, and hypothesis generation from textual data.While each problem-solution pair is relatively constrained in scope, the dataset's scale and diversity across scientific disciplines provides opportunities for broader generalization and transfer learning evaluation.</p>
<p>The Well [70] is a comprehensive collection of physics simulation datasets, explicitly constructed for machine learning model training and benchmarking in physics-informed learning.It contains diverse simulation data spanning fluid dynamics, astrophysics, plasma physics, and more.These simulations allow evaluation of models' abilities in hypothesis generation, scientific analysis, and predictive modeling in physics.Its broad diversity and complexity may be employed in open-ended exploration of scientific hypotheses through computational experimentation.</p>
<p>ScienceWorld [71] is a publicly available reinforcement learning environment designed to evaluate an AI agent's capacity for grounded scientific reasoning in a simulated laboratory context.The benchmark contains 30 interactive text-based tasks, such as converting substances between states of matter.Evaluation relies on binary task completion within limited simulator steps, making it suitable for assessing agents' (abstract, text-based) experimental execution capabilities in a weakly supervised, text-based modality.</p>
<p>DiscoveryWorld [72] is an open-source, highly interactive environment designed to benchmark complete cycles of scientific discovery, including hypothesis generation, experimental design, execution, and analysis.The general setting is akin to a 2D roleplaying game to be played on a grid.It provides agents with quests, subquests and various tasks to be completed to make progress.</p>
<p>ChemGymRL [73] provides a suite of customizable, publicly accessible reinforcement learning environments simulating chemistry laboratory experiments.Each virtual "bench" simulates distinct chemical procedures such as synthesis or titration.Agents receive structured numeric data representing chemical states and perform sequential lab actions.The library emphasizes experimental design and execution with reward signals, but allows for extension to e.g.new chemical reactions.</p>
<p>DiscoveryBench [74] is a publicly accessible benchmark focusing on data-driven scientific discovery tasks using multimodal data (tabular data and textual descriptions).It comprises over a thousand real-world and synthetic tasks spanning various scientific domains.Evaluation of agent-generated hypotheses is performed using LLMbased facet analysis, which allows for some open-endedness in the tasks considered.DiscoveryBench primarily targets hypothesis generation and data analysis.</p>
<p>BoxingGym [75] provides publicly available, interactive simulation environments for benchmarking autonomous experimental design and scientific model discovery.The benchmark covers multiple scientific domains through generative probabilistic models.Evaluation metrics include expected information gain for experimental quality and predictive power of agent-generated scientific models.The environment is numeric and textual in data modalities and promotes open-ended exploration.</p>
<p>Science-Gym [76] is a publicly released Gym-compatible benchmark designed to evaluate autonomous equation discovery in simulated physical and epidemiological environments.Agents interactively select experimental parameters to generate data, subsequently performing symbolic regression to derive underlying scientific equations.Evaluation assesses the symbolic accuracy of discovered equations, providing a structured yet open-ended setting emphasizing experimental execution and analytical reasoning.Open Catalyst 2020 (OC20) [77] provides a large-scale benchmark for catalysis research, encompassing over a million atomic structure relaxations generated via density functional theory (DFT) calculations.It offers structured atomic 3D data for supervised machine learning tasks aimed at predicting energies and molecular interactions relevant to catalytic processes.OC20 primarily evaluates data-driven analysis and indirectly supports hypothesis-driven experimental design, particularly aiding in computational screening of catalytic materials.While individually each task has a fixed objective, its expansive dataset encourages robust and generalizable modeling approaches.</p>
<p>Conclusion</p>
<p>This paper is an attempt at giving a survey of research on automated scientific discovery, from discovering equations to autonomous discovery systems or agents.In doing so, it takes a broad perspective on the topic, which is necessary to understand the individual efforts in context.The article covers the beginnings of the fields to very recent approaches, understanding that we still have a long way of putting everything together to create human-level autonomous scientists.Human-level autonomous scientists should, ultimately, be able to produce whole new theories, along with theoretical terms and measurement devices, which can be communicated to humans and interpreted in the light of other, existing theories.At this point, autonomous discovery systems are focused primarily on "closing the loop" and lab automation, and not so much on generating human-interpretable knowledge, like (differential) equations.Vice versa, computational approaches to scientific discovery, e.g., for equation discovery and symbolic regression, do not have the "embodiment" in autonomous systems in their focus yet.Ultimately, these currently disparate efforts have to grow together.Finally, it should be noted that artificial intelligence has a role also in so far unexplored areas, like the design of experiments, where much of human ingenuity is currently still needed.</p>
<p>Fig. 3
3
Fig. 3 Workflow of Cranmer et al. [50]: GNNs as an intermediate representation to support or enable the learning process</p>
<p>Fig. 5
5
Fig. 5 Six steps of the scientific process.</p>
<p>Fig. 6
6
Fig.6The robot scientist Adam.</p>
<p>Table 3
3
Benchmark Categorization by Evaluation Properties.In the Scope column, we take D = experimental Design, E = Experimental Execution, H = Hypothesis formation, A = Analysis of results, Q = research Question formation.</p>
<p>presented a solution based on what they call neural state variables.Neural state variables are essentially latent variables.The</p>
<p>Table 1
1
Robot Scientists by Discipline, Name, and Country
DisciplineNameCountryDrug DiscoveryEveSwedenDrug DiscoveryRecursionUSDrug DiscoveryLilly Life Sciences Studio labUSDrug DiscoveryXtaIPiChinaChemistryUK Centre for Rapid Online Analysis of ReactionsUKChemistryroboRXN at IBMSwitzerlandChemistryphactorâ„¢USChemistryPharmacy on Demand (PoD)USChemistryMolecule Maker InstituteUSChemistryAI-ChemistChinaChemistryA self-optimizing reactorUSChemistryChemputerUKChemistryLapkin GroupUKChemistryRoboChemNetherlandsMaterialsKebotixUSMaterialsAutonomous Research System (ARES)USMaterialsRobot ChemistUKMaterialsAcceleration ConsortiumCanadaMaterialsBrookhavenUSMaterialsSARAUSMaterialsAI-ChemistChinaMaterialsA-LabUSMaterialsMatterhornUKMaterialsARC -Exciton ScienceAustraliaMaterialsGormleyUSCatalysisRealCatFranceCatalysisSwissCAT+SwitzerlandMetallurgyACCMETEUMaterialsBIG-MAPEUCell BiologyLabdroidsJapanCell BiologyMurphy LabUSMechanical Eng.Creative Machines LabUSProtein DesignMolcureJapanProtein DesignLabGeniusUKSystems BiologyGenesisSwedenMaterials/BiologyArgonne Autonomous DiscoveryUSQuantum PhysicsMELVINGermanyMedicineAutomation ScienceSingapore</p>
<p>Table 2
2
Six levels of autonomy in scientific discovery analogously to autonomy levels in autonomous driving
LevelSummaryNarrativeExample0No automationTraditional human science before the-advent of computers.1MachineThe use of computers to automate anMost current applica-assistanceaspect of science, e.g. analysing data.tions of ML.2PartialAn important aspect of the discovery cycleAlphaFold 2, Real-timeAutomationis fully automated.weather forecasting3ConditionalAutomation
The underlying data are most frequently temporal.
If a model cannot be communicated to a community of researchers, it hardly qualifies as scientific, as communication is an indispensable part of the scientific endeavor.</p>
<p>Bacon: A production system that discovers empirical laws. P Langley, Proceedings of the 5th International Joint Conference on Artificial Intelligence (IJCAI 1977). the 5th International Joint Conference on Artificial Intelligence (IJCAI 1977)1977344</p>
<p>Scientific Discovery: Computational Explorations of the Creative Process. P W Langley, H A Simon, G Bradshaw, J M Zytkow, 1987MIT PressCambridge, MA, USA</p>
<p>Discovering dynamics. S DÅ¾eroski, L Todorovski, Proceedings of the Tenth International Conference on Machine Learning. the Tenth International Conference on Machine LearningAmherst, MA, USAMorgan Kaufmann1993</p>
<p>Genetic programming as a means for programming computers by natural selection. J R Koza, Statistics and Computing. 41994</p>
<p>Z Li, J Ji, Y Zhang, 10.48550/arXiv.2111.12210arXiv:2111.12210From kepler to newton: Explainable ai for science. 2021arXiv preprint</p>
<p>The automation of science. R D King, J Rowland, S G Oliver, M Young, W Aubrey, E Byrne, M Liakata, M Markham, P Pir, L N Soldatova, A Sparkes, K E Whelan, A Clare, Science. 32459232009</p>
<p>Active learning with statistical models. D A Cohn, Z Ghahramani, M I Jordan, Journal of Artificial Intelligence Research. 41996</p>
<p>Reinforcement Learning: An Introduction. R Sutton, A Barto, 2018MIT PressCambridge, MA, USA</p>
<p>Automated discovery in a chemistry laboratory. J M Zytkow, J Zhu, A Hussam, Proceedings of the 8th National Conference on Artificial Intelligence (AAAI 1990). the 8th National Conference on Artificial Intelligence (AAAI 1990)Boston, MA, USAAAAI Press / MIT Press1990</p>
<p>Discovering empirical equations from robotcollected data. K.-M Huang, J M Zytkow, Proceedings of the 10th International Symposium on Foundations of Intelligent Systems (ISMIS 1997). the 10th International Symposium on Foundations of Intelligent Systems (ISMIS 1997)Charlotte, North Carolina, USASpringer1997</p>
<p>Interpretable scientific discovery with symbolic regression: a reviews. N Makke, S Chawla, Artificial Intelligence Review. 5722024</p>
<p>. S Musslick, L K Bartlett, S H Chandramouli, M Dubova, F Gobet, T L Griffiths, J Hullman, R D King, J N Kutz, C G Lucas, S Mahesh, </p>
<p>Automating the practice of science: Opportunities, challenges, and implications. F Pestilli, S J Sloman, W R Holmes, PNAS. 12252025</p>
<p>Empowering biomedical discovery with ai agents. S Gao, A Fang, Y Huang, V Giunchiglia, A Noori, J R Schwarz, Y Ektefaie, J Kondic, M Zitnik, Cell. 187222024</p>
<p>Agents of exploration and discovery. P Langley, AI Magazine. 4242022</p>
<p>Integrated systems for computational scientific discovery. P Langley, Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24). the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)Vancouver, CanadaAAAI Press2024</p>
<p>Declarative bias in equation discovery. L Todorovski, S DÅ¾eroski, Proceedings of the Fourteenth International Conference on Machine Learning. the Fourteenth International Conference on Machine LearningSan Francisco, CAMorgan Kaufmann1997</p>
<p>Probabilistic grammars for equation discovery. J Brence, L Todorovski, S DÅ¾eroski, Knowledge Based Systems. 2241070772021</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 32459232009</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, PNAS. 1132016</p>
<p>Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks. M L Gao, J P Williams, J N Kutz, 2025</p>
<p>Equation discovery for model identification in respiratory mechanics of the mechanically ventilated human lung. S Ganzert, J Guttmann, D Steinmann, S Kramer, Proceedings of the 13th International Conference on Discovery Science (DS 2010). the 13th International Conference on Discovery Science (DS 2010)Berlin; HeidelbergSpringer2010</p>
<p>Discovering admissible models of complex systems based on scale-types and identity constraints. T Washio, H Motoda, Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI 1997). the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI 1997)1997</p>
<p>Dimensionally consistent equation discovery through probabilistic attribute grammars. J Brence, L Todorovski, S DÅ¾eroski, Information Sciences. 2023</p>
<p>Learning the probabilities in probabilistic context-free grammars for arithmetical expressions from equation corpora. M Chaushevska, L Todorovski, J Brence, S DÅ¾eroski, Proceedings of the Slovenian Conference on Artificial Intelligence. the Slovenian Conference on Artificial Intelligence2022</p>
<p>Discovering dynamics with genetic programming. S DÅ¾eroski, I Petrovski, Proceedings of the Seventh European Conference on Machine Learning. the Seventh European Conference on Machine LearningBerlin; HeidelbergSpringer1994</p>
<p>Integrating knowledge-driven and data-driven approaches to modeling. L Todorovski, S DÅ¾eroski, Ecological Modelling. 1942006</p>
<p>Inductive process modeling. W Bridewell, P Langley, L Todorovski, S DÅ¾eroski, Machine Learning. 712008</p>
<p>The influence of parameter fitting methods on model structure selection in automated modeling of aquatic ecosystems. D ÄŒerepnalkoski, K TaÅ¡kova, L Todorovski, N Atanasova, S DÅ¾eroski, Ecological Modelling. 452012</p>
<p>A bayesian machine scientist to aid in the solution of challenging scientific problems. R GuimerÃ , I Reichardt, A Aguilar-Mogas, F A Massucci, M Miranda, J PallarÃ¨s, M Sales-Pardo, Science Advances. 669712020</p>
<p>Interpretable machine learning for science with pysr and symbolicregression. M D Cranmer, abs/2305.01582 (2023) 2305.01582</p>
<p>Exact and optimal quadratization of nonlinear finite-dimensional non-autonomous dynamical systems. A Bychkov, I Issan, G Pogudin, B KrÃ¤mer, SIAM Journal on Applied Dynamical Systems. 2312024</p>
<p>Symbolic regression via control variable genetic programming. N Jiang, Y Xue, Proc. of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2023). of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2023)</p>
<p>. Springer, 2023Berlin, Heidelberg</p>
<p>Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. S.-M Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems. 332020</p>
<p>Deep learning for universal linear embeddings of nonlinear dynamics. B Lusch, J N Kutz, S L Brunton, Nature communications. 9149502018</p>
<p>Efficient generator of mathematical expressions for symbolic regression. S MeÅ¾nar, S DÅ¾eroski, L Todorovski, Machine Learning. 112112023</p>
<p>Symbolic regression via neural-guided genetic programming population seeding. T N Mundhenk, M Landajuela, R Glatt, C P Santiago, D M Faissol, B K Petersen, arXiv:2111.000532021arXiv preprint</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, M L Larma, T N Mundhenk, C P Santiago, S K Kim, J T Kim, Proceedings of the 9th International Conference on Learning Representations. the 9th International Conference on Learning Representations2021ICLR 2021</p>
<p>Neural symbolic regression that scales. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR, Virtual event18-24 July 2021. 2021139Virtual Event. Proceedings of Machine Learning Research</p>
<p>M Valipour, B You, M Panju, A Ghodsi, 10.48550/arXiv.2106.14131arXiv.arXiv:2106.14131[cs]version:1SymbolicGPT: A Generative Transformer Model for Symbolic Regression. 2021</p>
<p>End-to-end symbolic regression with transformers. P Kamienny, S Ascoli, G Lample, F Charton, S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. A Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022. 2022</p>
<p>Neural-Guided Equation Discovery. J Brugger, M Cerrato, D Richter, C Derstroff, D Maninger, M Mezini, S Kramer, 2025</p>
<p>Transformer-based Planning for Symbolic Regression. P Shojaee, K Meidani, A B Farimani, C K Reddy, 10.48550/ARXIV.2303.068332023-08-09Publisher: arXiv Version Number: 4. 2023</p>
<p>Deep generative symbolic regression with monte-carlo-tree-search. P Kamienny, G Lample, S Lamprier, M Virgolin, International Conference on Machine Learning, ICML 2023. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, Honolulu, Hawaii, USAPMLRJuly 2023. 2023202Proceedings of Machine Learning Research</p>
<p>Learning Equations for Extrapolation and Control. S Sahoo, C Lampert, G Martius, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningStockholm, SwedenPMLR2018</p>
<p>. Z Liu, Y Wang, S Vaidya, F Ruehle, J Halverson, M SoljaÄiÄ‡, T Y Hou, M Tegmark, arXiv:2404.197562024Kan: Kolmogorov-arnold networksarXiv preprint</p>
<p>In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery. M Merler, K Haitsiukevich, N Dainese, P Marttinen, 10.18653/v1/2024.acl-srw.49Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20244Student Research Workshop)</p>
<p>In Context Learning and Reasoning for Symbolic Regression with Large Language Models. S Sharlin, T R Josephson, 10.48550/ARXIV.2410.17448arXiv. Version Number: 22024</p>
<p>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, 10.48550/ARXIV.2404.18400arXiv. Version Number: 22024</p>
<p>E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, 10.48550/ARXIV.2302.12170Language Model Crossover: Variation through Few-Shot Prompting. arXiv. Version Number. 20233</p>
<p>Discovering symbolic models from deep learning with inductive biases. M D Cranmer, A Sanchez-Gonzalez, P W Battaglia, R Xu, K Cranmer, D N Spergel, S Ho, Advances in Neural Information Processing Systems. 332020</p>
<p>Deep neural networks to recover unknown physical parameters from oscillating time series. A Garcon, J Vexler, D Budker, S Kramer, PLoS ONE. 1752684392022</p>
<p>Automated discovery of fundamental variables hidden in experimental data. B Chen, K Huang, S Raghupathi, I Chandratreya, Q Du, H Lipson, Nature Computational Science. 22022</p>
<p>Maximum likelihood estimation of intrinsic dimension. E Levina, P J Bickel, Advances in Neural Information Processing Systems. 200417</p>
<p>Neural operator: Learning maps between function spaces with applications to pdes. N Kovachki, Z Li, B Liu, K Azizzadenesheli, K Bhattacharya, A M Stuart, A Anandkumar, Journal of Machine Learning Research. 242023</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. L Lu, P Jin, G Pang, Z Zhang, G E Karniadakis, Nature Machine Intelligence. 332021</p>
<p>Fourier neural operator for parametric partial differential equations. Z Li, N B Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A M Stuart, A Anandkumar, Proceedings of the 9th International Conference on Learning Representations. the 9th International Conference on Learning Representations2021ICLR 2021</p>
<p>Self-driving laboratories, advanced immunotherapies and five more technologies to watch in 2025. M Eisenstein, 10.1038/d41586-025-00075-6Nature. 6372025</p>
<p>Towards robot scientists for autonomous scientific discovery. A Sparkes, W Aubrey, E Byrne, A Clare, M N Khan, M Liakata, M Markham, J Rowland, L N Soldatova, K E Whelan, M Young, R D King, Automated Experimentation. 212021</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K D Grave, J Ramon, M Clare, W Sirawaraporn, S G Oliver, R D King, Journal of the Royal Society Interface. 12104201412892015</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K De Grave, J Ramon, M Clare, W Sirawaraporn, S G Oliver, R D King, 10.1098/rsif.2014.1289Journal of the Royal Society Interface. 12104201412892015</p>
<p>Testing the reproducibility and robustness of the cancer biology literature by robot. K Roper, A Abdel-Rehim, S Hubbard, M Carpenter, A Rzhetsky, L N Soldatova, R D King, 10.1098/rsif.2021.0821Journal of the Royal Society Interface. 19189202108212022</p>
<p>A mobile robotic chemist. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, X Li, B M Alston, B Li, R Clowes, N Rankin, B Harris, R S Sprick, A I Cooper, 10.1038/s41586-020-2442-2Nature. 58378152020</p>
<p>Robot scientists: From adam to eve to genesis. R King, O Peter, P Courtney, Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research. T Science, O Innovation, ParisOECD Publishing2023</p>
<p>Fantastic Realities: 49 Mind Journeys and a Trip to Stockholm. F Wilczek, B Devine, 2006World ScientificSingapore</p>
<p>Nobel turing challenge: Creating the engine for scientific discovery. H Kitano, Systems Biology and Applications. 7292021</p>
<p>Semanticallybased crossover in genetic programming: application to real-valued symbolic regression. N Q Uy, N X Hoai, M O'neill, R I Mckay, E GalvÃ¡n-LÃ³pez, Genetic Programming and Evolvable Machines. 122011</p>
<p>Ai feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science advances. 61626312020</p>
<p>Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, Computational Materials. 61382020</p>
<p>SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain. D Lu, X Tan, R Xu, T Yao, C Qu, W Chu, Y Xu, Y Qi, 2025</p>
<p>The well: a large-scale collection of diverse physics simulations for machine learning. R Ohana, M Mccabe, L T Meyer, R Morel, F J Agocs, M Beneitez, M Berger, B Burkhart, S B Dalziel, D B Fielding, D Fortunato, J A Goldberg, K Hirashima, Y.-F Jiang, R Kerswell, S Maddu, J M Miller, P Mukhopadhyay, S S Nixon, J Shen, R Watteaux, B R Blancard, .-S Rozet, F Parker, L H Cranmer, M Ho, S , The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>ScienceWorld: Is your Agent Smarter than a 5th Grader?. R Wang, P Jansen, M.-A CÃ´tÃ©, P Ammanabrolu, 2022</p>
<p>P E Jansen, DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents. 2024</p>
<p>Chemgymrl: A customizable interactive framework for reinforcement learning for digital chemistry. C Beeler, S G Subramanian, K Sprague, M Baula, N Chatti, A Dawit, X Li, N Paquin, M Shahen, Z Yang, C Bellinger, M Crowley, I Tamblyn, 10.1039/D3DD00183KDigital Discovery. 32024</p>
<p>DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. B P Majumder, H Surana, D Agarwal, B Dalvi Mishra, A Meena, A Prakhar, T Vora, T Khot, A Sabharwal, P Clark, Dataset and code available on GitHub. 2024</p>
<p>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery. K Gandhi, M Y Li, L Goodyear, L Li, A Bhaskar, M Zaman, N D Goodman, 2025Project page with environments</p>
<p>Science-Gym: A simple testbed for ai-driven scientific discovery. M Cerrato, N Schmitt, L Baur, E Finkelstein, S Jukic, L MÃ¼nzel, F P Paul, P Pfannes, B Rohr, J Schellenberg, P Wolf, S Kramer, 10.1007/978-3-031-78977-9_15Proceedings of the 26th International Conference on Discovery Science (DS). Lecture Notes in Computer Science. the 26th International Conference on Discovery Science (DS)Pisa, ItalySpringer202415243Gym-compatible simulation library for physics/epidemiology scenarios</p>
<p>The open catalyst 2020 (oc20) dataset and community challenges. L E Chanussot, ACS Catalysis. 11102021</p>            </div>
        </div>

    </div>
</body>
</html>
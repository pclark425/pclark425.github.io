<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9157 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9157</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9157</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-272653658</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.08330v1.pdf" target="_blank">Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue</a></p>
                <p><strong>Paper Abstract:</strong> Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations actually reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM di-alogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM’s own style.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9157.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9157.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-Turn3 Simulation (WildChat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation of Human Turn 3 in Human-LLM Dialogues (WildChat dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's primary experimental task: use LLMs to simulate the human reply at Turn 3 given Human Turn 1 and Chatbot Turn 2 from real Human–Chatbot conversations (WildChat). Evaluation spans 21 linguistic measures across lexical, syntactic, semantic, and style categories plus a binary conversation-end decision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, Meta-Llama-3-70B-Instruct, Mistral-7B-Instruct-v0.3, Mixtral-8x7B-Instruct-v0.1, Mistral-Large-123B-Instruct, Phi-3-medium-4k-instruct, Qwen2-72B-Instruct, c4aicommand-r-v01</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of open-weight instruction-tuned transformer LLMs of varying sizes used as simulators: small-to-mid (7–14B) and large (70B–123B) models; instruction-tuned variants and a retrieval-augmented production model (Command-R) were included. Models differ in parameter counts and pretraining corpora (not exhaustively specified beyond public model names).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Conversational NLP / Dialogue simulation (human–LLM interaction research)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Given Human Turn 1 and Chatbot Turn 2 from WildChat, prompt an LLM (SIMULATOR) to produce the Human's Turn 3 reply or indicate no response, then compare SIMULATOR Turn 3 to the ground-truth human Turn 3 across many linguistic features.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>21 linguistic measures aggregated into four categories (lexical, syntactic, semantic, style) plus binary conversation-end F1; similarity computed as Pearson's R for scalar metrics, average class-frequency correlations for distributions, and averaged per-dimension correlations on PCA-rotated embeddings for vector metrics; conversational end measured by binary F1.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Overall alignment is low: SIMULATORs show relatively higher similarity on semantic metrics and lower similarity on syntactic metrics. Conversation-end behavior: simulators overwhelmingly continue conversations (model-specific continuing rates ranged from 87.1% for Command-R to 98.6% for Llama-3.1-8B). Annotator baseline binary F1 ~0.60 versus random baseline ~0.55 (paper figures). Exact per-metric correlations vary but show modest non-random performance in semantic/style and poor syntactic alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design (largest effect), human Turn 1 linguistic properties (strong influence: politeness +, utterance length +, toxicity -, typographical errors -), conversation topic (story topics → higher similarity; programming/technical topics → lower similarity), model size (minor overall effect but more pronounced in non-English languages), language and training-data coverage (Chinese and Russian performance differences tied to pretraining coverage and safety training), prompt category (COT, DIRECT, OVERRIDE affected different aspects; COT increased conversation-end prediction), safety/safety-training priors (affecting toxicity/sentiment/style).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human annotator baseline (12 annotators) used as comparison; human annotators outperformed models on syntactic similarity and on conversation-end prediction (annotators better matched humans), while the best prompt/model combinations could match or exceed annotator similarity on some lexical/semantic measures; random baseline used for conversation-end (50% guess) and shown inferior to annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SIMULATORs rarely predict conversation endings correctly (tend to continue), systematic divergence along style and syntax, limited ability to reproduce human typographical errors or toxicity levels, low adaptation to diverse human speaking styles and technical domains, reduced robustness in languages with less pretraining data, evaluation depends on external models for some metrics (introducing potential metric bias).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Prompt engineering often has greater impact than choice of model; simulators perform better when the human's initial turn matches the LLM's default style (e.g., polite, fewer typos); specialized prompt engineering or fine-tuning may be needed to capture diverse human styles; a human-in-the-loop approach could combine complementary strengths of humans and simulators; longer context seeds might improve simulation of intent; future work should optimize prompts for specific metrics or tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9157.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9157.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Education-domain simulation (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM simulation for educational conversational datasets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work that uses LLMs to simulate human-to-human conversational QA and to generate educational conversational data, noting LLMs as scalable synthetic data sources for domain-specific dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Let the llms talk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Education / educational dialogue datasets</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generating human-to-human conversational QA or roleplay-style educational dialogues using LLMs (cited papers use zero-shot LLM-to-LLM interactions to synthesize datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt strategy and domain specificity; potential mismatch between LLM default style and target human population; dataset quality concerns mentioned generally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper only mentions this line of work; no quantitative accuracies reported here. General concerns include LLM behavioral differences from humans, biases, and fidelity to target populations.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note that domain-specific simulation is common in related work but that further evaluation and robust protocols are necessary to verify fidelity to real humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9157.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9157.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Health-domain simulation (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM simulation in health-related conversational contexts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references studies using LLMs to simulate or moderate health-related conversations, highlighting potential use for dataset creation and testing in healthcare dialogue contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can language model moderators improve the health of online discourse?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Health / medical conversation simulation and moderation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulating health-related conversational exchanges or using LLMs as moderators for online health discourse (as cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Safety training and moderation priors, domain-specific data coverage, potential for biased or harmful outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as prior work; this paper does not report accuracy. Authors warn about harmful/bias outputs and the need to remove harmful content before human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Safety considerations are critical; ensure toxic or harmful content is filtered before human-in-the-loop evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9157.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9157.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Programming-domain simulation (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM simulation for programming or software-testing dialogues</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites work using LLMs to simulate programming-help dialogues or to act as simulated users in GUI/software testing, reflecting application of LLM-simulators in technical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Programming / software testing / programming-help dialogues</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate human programmers or users interacting about code or GUIs to generate test conversations or evaluate model behavior in technical support scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Domain technicality (programming topics had lower simulator similarity in this paper), prompt specificity, and model pretraining coverage for technical language.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The current paper finds programming topics are harder to simulate (lower similarity), indicating LLMs struggle to mirror humans in technical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Task-specific fine-tuning or more focused prompts may be needed to improve simulation fidelity in technical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9157.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9157.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digital-twin / social twin (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-powered digital twins of human social behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references work on 'digital twins' where LLMs create virtual replicas of human behaviors for social media or HCI research, positioning LLMs as text-based simulators of populations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Y social: an llm-powered social media digital twin.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Digital twins / social behavior simulation / HCI</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Create virtual replicas of human users or populations to simulate social media interactions or human behavior in HCI experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Pretraining data cultural coverage, bias and representational gaps, safety training influencing outputs, and topic-specific performance variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper cites challenges: LLMs may not represent diverse identity groups and can exhibit political/cultural biases, limiting fidelity as digital twins.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Researchers should be cautious about representational fidelity and biases when constructing digital twins; robust evaluation protocols are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9157.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9157.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based QA/dialogue simulation (related mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General LLM-to-LLM dialogue simulation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites broader literature where LLMs are used to simulate multi-agent dialogues (LLM-LLM) and to synthesize conversational datasets across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>General NLP dataset synthesis / multi-agent dialogue simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Use LLMs to play multiple conversational roles (human-human or human-agent) to create synthetic dialogue data or to stress-test models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt strategy, model instruction tuning, domain and language coverage, and how well the prompt induces the intended persona/behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper observes that although LLMs produce humanlike text, subtle behavioral differences and metric limitations require careful evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Comprehensive multi-metric evaluation and human baselines are recommended to assess fidelity; prompt engineering is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let the llms talk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions. <em>(Rating: 2)</em></li>
                <li>Y social: an llm-powered social media digital twin. <em>(Rating: 2)</em></li>
                <li>Llm roleplay: Simulating human-chatbot interaction. <em>(Rating: 2)</em></li>
                <li>Can language model moderators improve the health of online discourse? <em>(Rating: 1)</em></li>
                <li>Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions. <em>(Rating: 2)</em></li>
                <li>Using large language models to simulate multiple humans and replicate human subject studies. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9157",
    "paper_id": "paper-272653658",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Human-Turn3 Simulation (WildChat)",
            "name_full": "Simulation of Human Turn 3 in Human-LLM Dialogues (WildChat dataset)",
            "brief_description": "This paper's primary experimental task: use LLMs to simulate the human reply at Turn 3 given Human Turn 1 and Chatbot Turn 2 from real Human–Chatbot conversations (WildChat). Evaluation spans 21 linguistic measures across lexical, syntactic, semantic, and style categories plus a binary conversation-end decision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.1-70B-Instruct, Meta-Llama-3-70B-Instruct, Mistral-7B-Instruct-v0.3, Mixtral-8x7B-Instruct-v0.1, Mistral-Large-123B-Instruct, Phi-3-medium-4k-instruct, Qwen2-72B-Instruct, c4aicommand-r-v01",
            "model_description": "A set of open-weight instruction-tuned transformer LLMs of varying sizes used as simulators: small-to-mid (7–14B) and large (70B–123B) models; instruction-tuned variants and a retrieval-augmented production model (Command-R) were included. Models differ in parameter counts and pretraining corpora (not exhaustively specified beyond public model names).",
            "scientific_subdomain": "Conversational NLP / Dialogue simulation (human–LLM interaction research)",
            "simulation_task": "Given Human Turn 1 and Chatbot Turn 2 from WildChat, prompt an LLM (SIMULATOR) to produce the Human's Turn 3 reply or indicate no response, then compare SIMULATOR Turn 3 to the ground-truth human Turn 3 across many linguistic features.",
            "evaluation_metric": "21 linguistic measures aggregated into four categories (lexical, syntactic, semantic, style) plus binary conversation-end F1; similarity computed as Pearson's R for scalar metrics, average class-frequency correlations for distributions, and averaged per-dimension correlations on PCA-rotated embeddings for vector metrics; conversational end measured by binary F1.",
            "simulation_accuracy": "Overall alignment is low: SIMULATORs show relatively higher similarity on semantic metrics and lower similarity on syntactic metrics. Conversation-end behavior: simulators overwhelmingly continue conversations (model-specific continuing rates ranged from 87.1% for Command-R to 98.6% for Llama-3.1-8B). Annotator baseline binary F1 ~0.60 versus random baseline ~0.55 (paper figures). Exact per-metric correlations vary but show modest non-random performance in semantic/style and poor syntactic alignment.",
            "factors_affecting_accuracy": "Prompt design (largest effect), human Turn 1 linguistic properties (strong influence: politeness +, utterance length +, toxicity -, typographical errors -), conversation topic (story topics → higher similarity; programming/technical topics → lower similarity), model size (minor overall effect but more pronounced in non-English languages), language and training-data coverage (Chinese and Russian performance differences tied to pretraining coverage and safety training), prompt category (COT, DIRECT, OVERRIDE affected different aspects; COT increased conversation-end prediction), safety/safety-training priors (affecting toxicity/sentiment/style).",
            "comparison_baseline": "Human annotator baseline (12 annotators) used as comparison; human annotators outperformed models on syntactic similarity and on conversation-end prediction (annotators better matched humans), while the best prompt/model combinations could match or exceed annotator similarity on some lexical/semantic measures; random baseline used for conversation-end (50% guess) and shown inferior to annotators.",
            "limitations_or_failure_cases": "SIMULATORs rarely predict conversation endings correctly (tend to continue), systematic divergence along style and syntax, limited ability to reproduce human typographical errors or toxicity levels, low adaptation to diverse human speaking styles and technical domains, reduced robustness in languages with less pretraining data, evaluation depends on external models for some metrics (introducing potential metric bias).",
            "author_recommendations_or_insights": "Prompt engineering often has greater impact than choice of model; simulators perform better when the human's initial turn matches the LLM's default style (e.g., polite, fewer typos); specialized prompt engineering or fine-tuning may be needed to capture diverse human styles; a human-in-the-loop approach could combine complementary strengths of humans and simulators; longer context seeds might improve simulation of intent; future work should optimize prompts for specific metrics or tasks.",
            "uuid": "e9157.0",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Education-domain simulation (mention)",
            "name_full": "LLM simulation for educational conversational datasets",
            "brief_description": "The paper cites prior work that uses LLMs to simulate human-to-human conversational QA and to generate educational conversational data, noting LLMs as scalable synthetic data sources for domain-specific dialogues.",
            "citation_title": "Let the llms talk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Education / educational dialogue datasets",
            "simulation_task": "Generating human-to-human conversational QA or roleplay-style educational dialogues using LLMs (cited papers use zero-shot LLM-to-LLM interactions to synthesize datasets).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Prompt strategy and domain specificity; potential mismatch between LLM default style and target human population; dataset quality concerns mentioned generally.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "Paper only mentions this line of work; no quantitative accuracies reported here. General concerns include LLM behavioral differences from humans, biases, and fidelity to target populations.",
            "author_recommendations_or_insights": "Authors note that domain-specific simulation is common in related work but that further evaluation and robust protocols are necessary to verify fidelity to real humans.",
            "uuid": "e9157.1",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Health-domain simulation (mention)",
            "name_full": "LLM simulation in health-related conversational contexts",
            "brief_description": "The paper references studies using LLMs to simulate or moderate health-related conversations, highlighting potential use for dataset creation and testing in healthcare dialogue contexts.",
            "citation_title": "Can language model moderators improve the health of online discourse?",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Health / medical conversation simulation and moderation",
            "simulation_task": "Simulating health-related conversational exchanges or using LLMs as moderators for online health discourse (as cited in related work).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Safety training and moderation priors, domain-specific data coverage, potential for biased or harmful outputs.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "Cited as prior work; this paper does not report accuracy. Authors warn about harmful/bias outputs and the need to remove harmful content before human annotation.",
            "author_recommendations_or_insights": "Safety considerations are critical; ensure toxic or harmful content is filtered before human-in-the-loop evaluation.",
            "uuid": "e9157.2",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Programming-domain simulation (mention)",
            "name_full": "LLM simulation for programming or software-testing dialogues",
            "brief_description": "The paper cites work using LLMs to simulate programming-help dialogues or to act as simulated users in GUI/software testing, reflecting application of LLM-simulators in technical domains.",
            "citation_title": "Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Programming / software testing / programming-help dialogues",
            "simulation_task": "Simulate human programmers or users interacting about code or GUIs to generate test conversations or evaluate model behavior in technical support scenarios.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Domain technicality (programming topics had lower simulator similarity in this paper), prompt specificity, and model pretraining coverage for technical language.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "The current paper finds programming topics are harder to simulate (lower similarity), indicating LLMs struggle to mirror humans in technical domains.",
            "author_recommendations_or_insights": "Task-specific fine-tuning or more focused prompts may be needed to improve simulation fidelity in technical domains.",
            "uuid": "e9157.3",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Digital-twin / social twin (mention)",
            "name_full": "LLM-powered digital twins of human social behavior",
            "brief_description": "The paper references work on 'digital twins' where LLMs create virtual replicas of human behaviors for social media or HCI research, positioning LLMs as text-based simulators of populations.",
            "citation_title": "Y social: an llm-powered social media digital twin.",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Digital twins / social behavior simulation / HCI",
            "simulation_task": "Create virtual replicas of human users or populations to simulate social media interactions or human behavior in HCI experiments.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Pretraining data cultural coverage, bias and representational gaps, safety training influencing outputs, and topic-specific performance variability.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "Paper cites challenges: LLMs may not represent diverse identity groups and can exhibit political/cultural biases, limiting fidelity as digital twins.",
            "author_recommendations_or_insights": "Researchers should be cautious about representational fidelity and biases when constructing digital twins; robust evaluation protocols are needed.",
            "uuid": "e9157.4",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLM-based QA/dialogue simulation (related mentions)",
            "name_full": "General LLM-to-LLM dialogue simulation approaches",
            "brief_description": "The paper cites broader literature where LLMs are used to simulate multi-agent dialogues (LLM-LLM) and to synthesize conversational datasets across domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "General NLP dataset synthesis / multi-agent dialogue simulation",
            "simulation_task": "Use LLMs to play multiple conversational roles (human-human or human-agent) to create synthetic dialogue data or to stress-test models.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Prompt strategy, model instruction tuning, domain and language coverage, and how well the prompt induces the intended persona/behavior.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "Paper observes that although LLMs produce humanlike text, subtle behavioral differences and metric limitations require careful evaluation.",
            "author_recommendations_or_insights": "Comprehensive multi-metric evaluation and human baselines are recommended to assess fidelity; prompt engineering is crucial.",
            "uuid": "e9157.5",
            "source_info": {
                "paper_title": "Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let the llms talk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions.",
            "rating": 2,
            "sanitized_title": "let_the_llms_talk_simulating_humantohuman_conversational_qa_via_zeroshot_llmtollm_interactions"
        },
        {
            "paper_title": "Y social: an llm-powered social media digital twin.",
            "rating": 2,
            "sanitized_title": "y_social_an_llmpowered_social_media_digital_twin"
        },
        {
            "paper_title": "Llm roleplay: Simulating human-chatbot interaction.",
            "rating": 2,
            "sanitized_title": "llm_roleplay_simulating_humanchatbot_interaction"
        },
        {
            "paper_title": "Can language model moderators improve the health of online discourse?",
            "rating": 1,
            "sanitized_title": "can_language_model_moderators_improve_the_health_of_online_discourse"
        },
        {
            "paper_title": "Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions.",
            "rating": 2,
            "sanitized_title": "make_llm_a_testing_expert_bringing_humanlike_interaction_to_mobile_gui_testing_via_functionalityaware_decisions"
        },
        {
            "paper_title": "Using large language models to simulate multiple humans and replicate human subject studies.",
            "rating": 1,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        }
    ],
    "cost": 0.01555525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue</p>
<p>Jonathan Ivey 
University of Michigan</p>
<p>University of Arkansas</p>
<p>Shivani Kumar 
University of Michigan</p>
<p>Jiayu Liu 
University of Michigan</p>
<p>University of Illinois Urbana-Champaign</p>
<p>Hua Shen 
University of Michigan</p>
<p>Sushrita Rakshit 
University of Michigan</p>
<p>Rohan Raju 
University of Michigan</p>
<p>Haotian Zhang 
University of Michigan</p>
<p>Aparna Ananthasubramaniam 
University of Michigan</p>
<p>Junghwan Kim 
University of Michigan</p>
<p>Bowen Yi 
University of Michigan</p>
<p>Dustin Wright 
University of Michigan</p>
<p>University of Copenhagen</p>
<p>Abraham Israeli 
University of Michigan</p>
<p>Anders Giovanni Møller 
University of Michigan</p>
<p>IT University of Copenhagen</p>
<p>Lechen Zhang 
David Jurgens jurgens@umich.edu 
University of Michigan</p>
<p>Marah Abdin 
University of Michigan</p>
<p>AmmarSam Ade Jacobs 
Ahmad Awan 
Jyoti Aneja 
Ahmed Awadallah 
Hany Awadalla 
Nguyen Bach 
Amit Bahree 
Arash Bakhtiari 
Jian- Min Bao 
Harkirat Behl 
Alon Benhaim 
Misha Bilenko 
Johan Bjorck 
Sébastien Bubeck 
Qin Cai 
Martin Cai 
Caio César 
Teodoro Mendes 
Weizhu Chen 
Vishrav Chaudhary 
Dongdong Chen 
Yen-Chun Chen 
Yi-Ling Chen 
Parul Chopra 
Xiyang Dai 
Allie Del Giorno 
Gustavo De Rosa 
Matthew Dixon 
Ronen Eldan 
Victor Fragoso 
Dan Iter 
Mei Gao 
Min Gao 
Jianfeng Gao 
Amit Garg 
Abhishek Goswami 
Suriya Gunasekar 
Emman Haider 
Junheng Hao 
Russell J Hewett 
Jamie Huynh 
Mojan Javaheripi 
Xin Jin 
Piero Kauff- Mann 
Nikos Karampatziakis 
Dongwoo Kim 
Ma- Houd Khademi 
Lev Kurilenko 
James R Lee 
Yin Tat Lee 
Yuanzhi Li 
Yunsheng Li 
Chen Liang 
Lars Li- Den 
Ce Liu 
Mengchen Liu 
Weishung Liu 
Eric Lin 
Zeqi Lin 
Chong Luo 
Piyush Madan 
Matt Mazzola 
Arindam Mitra 
Hardik Modi 
Anh Nguyen 
Brandon Norick 
Barun Patra 
Daniel Perez-Becker 
Thomas Portet 
Reid Pryzant 
Heyang Qin 
Marko Radmi- Lac 
Corby Rosset 
Sambudha Roy 
Olatunji Ruwase 
Olli Saarikivi 
Amin Saied 
Adil Salim 
Michael San- Tacroce 
Shital Shah 
Ning Shang 
Hiteshi Sharma 
Jiahang Xu 
Weijian Xu 
Sonali Xu 
Fan Yadav 
Jianwei Yang 
Ziyi Yang 
Yifan Yang 
Donghan Yang 
Lu Yu 
Chengruidong Yuan 
Cyril Zhang 
Jian- Wen Zhang 
Zhang 
Lyna Li 
Yi Zhang 
Yue Zhang 
Thomas Wolf 
Lysandre Debut 
Victor Sanh 
Julien Chaumond 
Clement Delangue 
Anthony Moi 
Pier- Ric Cistac 
Tim Rault 
Rémi Louf 
Morgan Funtowicz 
Joe Davison 
Sam Shleifer 
Patrick Von Platen 
Clara Ma 
Yacine Jernite 
Julien Plu 
Canwen Xu 
Teven Le Scao 
Sylvain Gugger 
Mariama Drame 
Quentin Lhoest 
Alexander M 2020 Rush 
An Yang 
Baosong Yang 
Binyuan Hui 
Bo Zheng 
Bowen Yu 
Chang Zhou 
Chengpeng Li 
Chengyuan Li 
Dayiheng Liu 
Fei Huang 
Guanting Dong 
Hao- Ran Wei 
Huan Lin 
Jialong Tang 
Jialin Wang 
Jianhong Tu 
Jianwei Zhang 
Jianxin Ma 
Jin Xu 
Jingren Zhou 
Jinze Bai 
Jinzheng He 
Junyang Lin 
Kai Dang 
Keming Lu 
Keqin Chen 
Kexin Yang 
Mei Li 
Mingfeng Xue 
Na Ni 
Pei Zhang 
Peng Wang 
Ru Peng 
Rui Men 
Ruize Gao 
Runji Lin 
Zhe Zhao 
Hui Chen 
Jinbin Zhang 
Xin Zhao 
Tao Liu 
Wei Lu 
Xi Chen 
Haotang Deng 
Xuhui Zhou 
Hao Zhu 
Leena Mathur 
Ruohong Zhang 
Haofei Yu 
Zhengyang Qi 
Louis-Philippe Morency 
Dmitry Zmitrovich 
Alexander Abramov 
Andrey Kalmykov 
Maria Tikhonova 
Ekaterina Taktasheva 
Danil Astafurov 
Mark Baushenko 
Artem Sne- Girev 
Tatiana Shavrina 
Sergey Markov </p>
<p>Swadheen Shukla
Masahiro Tanaka, Lijuan Wang, Rachel Ward, Guanhua Wang, Haiping Wu, Michael WyattXia Song, An-drea Tupini, Xin Wang, Chunyu Wang, Yu Wang, Philipp Witte, Bin XiaoCan</p>
<p>Shijie Wang
Shuai Bai, Tianhang Zhu, Tianyu Liu, Wenbin GeSinan Tan, Tianhao Li, Xiaodong Deng, Xiaohuan Zhou</p>
<p>Xingzhang Ren
Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuXinyu Zhang, Xipin Wei, Xuancheng Ren, Yuqiong Liu, Zeyu</p>
<p>Yonatan Bisk
Daniel FriedGraham Neubig</p>
<p>Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue
7C37945AB807DC8280FC0C4557E8DB57arXiv:2307.02477.
Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants.In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly humanlike text in many settings.However, to what extent do LLM-based simulations actually reflect human dialogues?In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts.Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content.Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly.Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.ReferencesZahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, and Mohammad Aliannejadi.2024.Let the llms talk: Simulating human-to-human conversational qa via zero-shot llm-to-llm interactions.In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 8-17.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are capable of producing convincingly human-like responses to a broad range of inputs.Recent work has explored the potential of LLMs to simulate human interactions in different scenarios (Zheng et al., 2023;Köpf et al., 2024;Zhao et al., 2024).The scenarios include simulating humans interacting with other humans to generate dialogue datasets, as well as simulating humans interacting with other LLMs (Kim et al., 2024), which is thought to be a scalable, automated approach for LLM quality testing.Such simulations can greatly reduce the cost of * All authors have equal contribution, order is randomized except senior author Figure 1: A sample conversation between a human and GPT-3.5 on WildChat and LLAMA3.1-8B'ssimulation of Turn 3 of the conversation.In this study, we compare the SIMULATOR's output against the HUMAN's output using 21 metrics, covering lexical, syntactic, semantic, and stylistic features.collecting these data, which often require costly human labor and are difficult to scale to the diversity of LLM abilities.However, this approach can only be effective if the responses generated by the LLM mirror how a human would interact in different scenarios.In this work, we ask: to what extent can LLMs simulate the responses of humans in a human-LLM dialogue?</p>
<p>To test simulation capabilities, we evaluate the degree to which LLMs mirror human behavior in real Human-LLM dialogues.Our study asks the following three research questions: (RQ1) to what extent does the choice of model and prompt instructions influence how well the LLM can simulate human behavior; (RQ2) how do these results generalize to interactions in languages other than English;and (RQ3) in what contexts are LLMs more likely to effectively simulate human responses?</p>
<p>To answer these questions, we develop an evaluation framework on top of human responses from the one million conversations in the WildChat dataset (Zhao et al., 2024).We compare the response from a human in a dialogue with the simulated response by an LLM, which we denote as a SIMULATOR, as illustrated in Figure 1.Responses are compared across multiple categories, e.g., lexicality, syntax, semantics, and style, to assess their fidelity to human behavior.Using multiple LLMs and instruction prompts, we evaluate which settings led to better simulation; and, by using regression methods, we highlight the most significant factors that lead SIMULATOR responses to be more human-like.</p>
<p>This paper makes the following four contributions.First, we introduce a general evaluation framework for meaningful analysis of human-LLM simulations and a new dataset of over 1.2K annotator responses to the same conversations, providing a human-level performance comparison.Second, we perform a large-scale analysis of 9 LLMs across 50 prompts simulating 2K English human-LLM conversations, and even the best model and prompt combinations are relatively weak at simulating human behavior.Third, multilingual analyses on 10K Chinese and Russian human-LLM conversations show that performance is roughly similar, though still low.Fourth, a regression analysis demonstrating which factors lead to more humanlike responses from LLMs, we find that when the human begins the conversation in a writing style that resembles the LLM's, the SIMULATOR can better match their behavior in a later turn.All data and code are released for research purposes at https: //github.com/davidjurgens/human-llm-similarity.</p>
<p>Using LLMs to Simulate Human Interaction</p>
<p>Given the practical use of LLMs in mimicking human turns in conversations, many studies deal with simulating human-LLM interactions and developing relevant conversational data sets for generic conversations (Tamoyan et al., 2024;Njifenjou et al., 2024;Gosling et al., 2023) or for conversations of a specific domain, such as education (Abbasiantaeb et al., 2024), health (Cho et al., 2023), or programming (Liu et al., 2024b).Similarly, LLM-LLM interactions highlight the general capability of LLMs to mimic human discussions (Park et al., 2023;Zhou et al., 2024;Rossetti et al., 2024;Zhou et al., 2023;Chen et al., 2023).In order to facilitate further research, a battery of studies have introduced various datasets of human-LLM (Zheng et al., 2023) and LLM-LLM (Kim et al., 2022;Chen et al., 2023) dialogue.Another branch of interaction simulation has studied Digital Twins as virtual replicas of physical systems, in this case, humans in various discussion settings (Barricelli et al., 2019;Barricelli and Fogli, 2024;Rossetti et al., 2024;Wen et al., 2024).It is common in many of these contexts for LLMs to augmentor even replace-human labor with simulations, thereby reducing time and effort (Kojima et al., 2022;de Wit, 2023).Humans, however, tend to have their specific style, intent, and self-creativity, which are challenging for LLMs to mimic despite recent technological breakthroughs (Stevenson et al., 2022;Wu et al., 2023;Wolf et al., 2023;Gui and Toubia, 2023;Jiang et al., 2024b).Specifically, Leng and Yuan (2023) highlight that while LLMs show promise for applications in social science research, further investigation into their subtle behavioral differences from humans and the development of robust evaluation protocols are essential, thus motivating our research.Multiple metrics have been proposed to measure these differences, including content relevance (Abeysinghe and Circi, 2024), emotional alignment (Maehlum et al., 2024), and intent accuracy (Kim et al., 2024).A few studies (Sedoc et al., 2019;Svikhnushina and Pu, 2023) have explored broader evaluation techniques by developing frameworks that align closely with human judgment.However, comprehensive research about how different measures vary between LLMs and prompts is still lacking.In this work, we address this measurement gap by analyzing how LLM responses compare to human responses across multiple similarity measures, LLM models, and prompts.</p>
<p>Problem Definition</p>
<p>Our research questions focus on understanding which settings lead to SIMULATORs producing convincingly HUMAN-like dialogues.We adopt the following modeling task, which provides a controlled setting for this question: Given the initial human utterance in a conversation, followed by the response of a chatbot, we prompt a SIMULATOR to suggest the next HUMAN response in the discussion.The "true" value of this third turn is known (but not given as input to the SIMULATOR), as it is part of the WildChat dataset and thus acts as a reference response to which we compare.</p>
<p>This modeling setup offers the following desirable properties.First, by using HUMAN Turn 3 as a reference, we are able to investigate multiple fac-tors related to the quality of LLM simulations: (i) the similarity between SIMULATOR and HUMAN responses along a variety of linguistic features, (ii) the influence of the original human Turn 1 and chatbot Turn 2 utterances, and (iii) the influence of model and prompt.Second, limiting to HUMAN Turn 3 allows us to study these factors in isolation at the early stage of dialogue, i.e., without the added influence of multiple (simulated or natural) turns.This is desirable because the Turn 3 response demonstrates the degree to which the SIMULATOR can continue a given conversation with the minimum amount of cues.Finally, this setup maximizes the evaluation data set from WildChat while still containing multiple turns, as most conversations in the dataset end with 3 or fewer turns.</p>
<p>Generating Simulation Data</p>
<p>To evaluate how well LLMs can be used to simulate human interactions, we generate a large dataset of dialogue simulations.We start with the Wild-Chat dialogues between HUMANs and a CHATBOT (typically GPT-3.5) and then use another LLM as a SIMULATOR to continue the conversation.</p>
<p>Wildchat Data WildChat (Zhao et al., 2024) is a corpus of one million conversations between HU-MANs and a CHATBOT, comprising over 2.5 million interaction turns in multiple languages, with English accounting for 53% of the turns.We restrict ourselves to a sample of 102k English instances, along with 10k Chinese and 10k Russian instances for multilingual experiments in §6.</p>
<p>To generate, we take the first two turns of each conversation (HUMAN initiation and CHATBOT response) as input to the SIMULATOR and prompt the model to generate what the HUMAN Turn 3 response would be, if any, or to indicate the human would not have responded.</p>
<p>Prompt Composition Since the wording of a prompt can have a significant impact on an LLM's output and adopted persona (Röttger et al., 2024;Wright et al., 2024;Ceron et al., 2024), we conduct experiments with a wide range of prompts.Working independently, 12 prompt writers familiar with LLMs composed a total of 50 candidate prompts for this task.The prompt's aim is to have the SIM-ULATOR match the conversational intent, content, and style of the first turn and to decide whether the conversation continues or not.Prompt writers were given 10 randomly sampled dialogues from Wild-Chat to guide their efforts in prompt generation.They were encouraged to test their prompts using a CHATBOT or other available tools.Three example prompts are shown in Supplemental Table 3, and all 50 are in the Github repository.</p>
<p>Prompt Classification Prompts generally took one of three strategies: (1) COT: using chainof-thought reasoning to guide the SIMULATOR through its response; (2) OVERRIDE employing various strategies to circumvent the SIMU-LATOR's tendency to use overly polite, flowery, or long-winded language (e.g., jailbreaking, explicit instructions to ignore ethics training, jarring prompts); and (3) DIRECT directly instructing the SIMULATOR to respond like a human, including telling the SIMULATOR to pretend to be human or creating a persona for the model to follow (e.g., tothe-point, lazy).Two authors jointly labeled each prompt by strategy, resulting in 13 prompts as COT, 14 as OVERRIDE, and 23 as DIRECT.</p>
<p>Generating and Parsing Simulated Turn 3 Each of the 50 prompts was used to generate the third turn of a random sample of English-language dialogues from WildChat.Nine models were used: Meta-Llama-3.1-8B-Instruct(Dubey et al., 2024), Meta-Llama-3.1-70B-Instruct(Dubey et al., 2024), Meta-Llama-3-70B-Instruct (AI@Meta, 2024), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024a), Mistral-Large-Instruct (Mistral, 2024), Phi-3-medium-4k-instruct (Abdin et al., 2024), Qwen2-72B-Instruct (Yang et al., 2024), and c4aicommand-r-v01 (Cohere, 2024).We use customized regular expressions for each prompt to parse and extract the SIMULATOR's response text, especially for the 13 COT prompts.After parsing, 4 prompts were discarded due to their frequent failure to produce valid output.Details on inference are in Appendix §B.</p>
<p>Evaluating LLM Simulations To evaluate, we select a breadth of 21 linguistic measures along 4 broad categories: style, lexical, syntactic, and semantic.These categories and measures have been broadly used for different NLP tasks (Sebastiani, 2002;Stamatatos, 2009;Fu et al., 2018;Ribeiro et al., 2020).The output of these measures can be either a scalar, a probability distribution, or a feature vector.Scalars include both unbounded measures (e.g., utterance length) and bounded measures in [0, 1] (e.g., text sentiment).Probability   1).Bars represent the average correlation across all metrics in a category, and error bars are bootstrapped 95% confidence intervals over these metrics.As a baseline, we also compare the performance of a human annotator on this task.There is limited cross-model variation in performance, and SIMULATORs tend to have higher performance in semantic features and lower performance in syntactic features, while the opposite is true of the human annotators.
-7 0 B L la m a 3 -7 0 B M is t r a l-7 B M ix t r a l-8 x 7 B M is t r a l-L a r g e -1 2 3 B P h i-3 -1 4 B Q w e n 2 -7 2 B C o m m a n d -R -3 5 B A n n o t a t o
distributions include, e.g., the probability that a token with a particular part of speech appears in a sentence across different parts of speech.Finally, feature vectors include semantic (SBERT, Reimers and Gurevych (2019)) and style (LUAR, Soto et al.</p>
<p>(2021)) embeddings.In addition, we record if the SIMULATOR and HUMAN each end the conversation on Turn 3 as a binary measure.A summary of all measures included in this study is given in Table 1 in the Appendix.</p>
<p>5 Can LLMs Simulate Human Replies?</p>
<p>In the first experiment, we measure the overall similarity of SIMULATORs to HUMANs (RQ1) by assessing how simulation instructions influence SIM-ULATOR similarity to HUMAN messages across a variety of models and prompts.</p>
<p>Experimental Setup</p>
<p>For our first experiment, we use the setup from §4 to generate 828K simulated responses (9 models, 46 prompts, 2,000 English conversations).To evaluate performance, we measure the similarity between the original HUMAN Turn 3 responses and the SIMULATOR Turn 3 responses in lexical, syntactic, semantic, and style domains.Starting with the 21 metrics from §4, we correlate each HUMAN and SIMULATOR Turn 3 metric.For scalar metrics (e.g., length, classifier probabilities), similarity is the Pearson's R correlation between HUMAN and SIMULATOR values.For distribution metrics (e.g., POS tags, topics), similarity is the average correlation of HUMAN and SIMULATOR class frequencies over all classes in the distribution.For vector metrics (e.g., BERT, LUAR), similarity is the average correlation of each dimension of the HUMAN and SIMULATOR embedding;1 since the dimensions of embeddings are not inherently meaningful, we first rotate the embeddings using principal components analysis, so each dimension corresponds to an embedding's alignment with the direction of a principal component.We then average the correlations for all metrics within each category given in the first column of Table 1 to create four category-wise similarity scores corresponding to lexical, syntactic, semantic, and style.To measure the similarity in ending a conversation, we use the binary F1 score between whether the HUMAN and SIMULATOR decide to end the conversation.</p>
<p>L la m a 3 .1 -8 B L la m a 3 .1 -7 0 B L la m a 3 -7 0 B M is t r a l-7 B M ix t r a l-8 x 7 B M is t r a l-L a r g e -1 2 3 B P h i- Although we have a human ground truth for how the conversation did continue, to estimate the task's difficulty, we also had a separate group of 12 humans who did not author any WildChat data to perform the same task as the SIMULATORs to produce Turn 3 responses.1273 turns were annotated, and we compared SIMULATOR performance on the subset of turns where both the annotators and SIMULATOR continued the conversation.Full annotation details are in Appendix §C.</p>
<p>Results</p>
<p>We compare the similarity of different SIMULA-TORs to the original HUMAN in Figure 2. In general, similarity is lower across all settings for the syntactic measures, while semantic measures tend to be more similar.Additionally, there is little variation across different SIMULATORs.There is effectively no difference between the similarity of SIMULATORs and the similarity of the human annotator baseline across the lexical, semantic, and style categories of measures.However, there is a discernible difference in how similar annotator utterances are syntactically to the original HUMANs compared to the SIMULATORs.Additionally, for both the semantic and style categories, the confidence intervals for the annotator baseline extend beyond 0, the performance we would expect from a totally random baseline, while each SIMULATOR maintains non-random performance.This finding suggests that humans and LLMs have complementary strengths in simulating dialogue; in order to most accurately reflect human utterances, a humanin-the-loop approach where SIMULATORs and annotators play complementary roles may be appro-  2, the performance of the best and worst prompts and annotators are compared across metric categories.The best (a DIRECT prompt) and worst (an OVERRIDE prompt) prompts are selected based on an overall average across all metrics and shown in Table 2.The worst prompt underperforms the best prompt in all categories, and annotators outperform all prompts in syntax metrics.</p>
<p>priate.The correlation results for all individual metrics across SIMULATORs are shown in the Appendix in Table 5.</p>
<p>In Figure 3, we compare the similarity between SIMULATORs and HUMANs in their tendency to end the conversation.SIMULATORs seldom end the conversation, continuing 87.1% of the time for COMMAND-R to 98.6% for LLAMA3.1-8B.In contrast, human annotators more accurately mirror the original HUMAN behavior in ending the conversation, indicating that collaboration between SIMULATORs and humans is effective for simulating human interactions.Additionally, human annotators show a better ability to determine when a conversation is likely to end, more often predicting the end at Turn 2 when it actually occurs, whereas SIMULATORs tend to predict conversation end with similar frequency regardless of the HUMANs actual behavior (Figure 11).The performance differences across SIMULATOR models are mainly driven by how frequently each model ends the conversation.</p>
<p>Finally, we look at the impact of prompt on SIM-ULATOR and HUMAN similarity in Figure 4. We plot the average similarity within each category for the best prompt (i.e., the highest average correlation across all metrics), worst prompt, and human annotator baseline.See prompts in Table 2.The best prompt shows higher lexical and semantic similarity than the human annotator baseline, while human annotators have higher syntactic similarity, in line with the results on models in Figure 2. We also see that prompts significantly affect the similarity across all categories, suggesting that prompt engineering impacts more than model selection for simulating human interactions.This effect is robust based on the error bars of each method.The most impacted categories are the lexical and syntactic categories, while the prompt has less effect on semantic and stylistic similarity.Given that models also demonstrate little difference in these categories, it is challenging to engineer human simulations using LLMs that are semantically or stylistically similar, while it is possible with good prompt engineering to improve lexical and syntactic similarity.The correlation results of all metrics across prompts are shown in Appendix Table 6.</p>
<p>Overall, we find that the choice of SIMULATOR has only a minor impact on the ability to simulate HUMANs, while the design of the prompt is most relevant when optimizing performance (RQ1).</p>
<p>Simulation in non-English Languages</p>
<p>Next, we address how the results from §5 generalize to languages beyond English (RQ2).</p>
<p>Experimental Setup</p>
<p>We compare the performance of SIMULATORs across three languages: English, Chinese, and Russian.Chinese and Russian were selected for their substantial representation in the WildChat dataset, following English.Specifically, Chinese and Russian comprise 15.9% and 10.5%, respectively, whereas English accounts for 53%.For our analysis, we randomly sample 10, 000 conversations from each language.</p>
<p>To generate conversations, we use a subset of models from the previous section: MISTRAL-LARGE-123B, LLAMA3.1-70B, and MIXTRAL-8X7B.We identify the most effective English prompts from each prompt category and have these prompts manually translated into Chinese and Russian by native speakers.We select three prompts that consistently perform well across six largely uncorrelated metrics: capitalization, punctuation, part of speech, SBERT embeddings, sentiment, and politeness.(Appendix§E, Figure 9).The selected prompts are presented in Appendix Table 3.</p>
<p>To measure the similarity between HUMAN and SIMULATOR responses, we use a procedure similar to the one described in §5.Due to the unavailability of several metrics from §5 for Chinese and Russian, we focus on a subset of ten metrics that  7 and 8. cover all four categories.Detailed descriptions of these metrics and their operationalizations can be found in Appendix Table 4.We employ consistent or similarly trained models whenever possible to ensure comparability across languages.</p>
<p>Results</p>
<p>The aggregated metrics for the three languages are depicted in Figure 5.Consistent with the findings discussed in §5, all three languages show higher correlations between SIMULATORs and HUMANs in the semantic metric but lower correlations in the syntactic metric.For detailed correlations of individual metrics, we refer to Appendix Table 7 and Table 8.Additionally, predicting whether a conversation will end consistently performs below chance across all languages, as SIMULATORs are unlikely to predict a conversation will end.These observations reinforce the conclusions drawn in §5.</p>
<p>Notably, there are differences between the languages.Chinese SIMULATORs, for example, outperform their English and Russian counterparts in lexical metrics and show slightly better performance in semantic metrics.The differences in lexical metrics, such as utterance length and perplexity, may be attributed to the typically shorter sentence lengths in Chinese.Conversely, English SIMULATORs excel in predicting style metrics compared to those in other languages.Toxicity and sentiment metrics primarily contribute to this improvement.In English, SIMULATORs more accurately reflect the variations in toxicity and sentiment of HUMANs.This capability may vary across languages because Chinese and Russian have rel-  9 and Table 10.The linguistic properties of HUMAN Turn 1 have stronger effects than those of the CHATBOT in Turn 2, showing that SIMULATORs do correctly accommodate more to the HUMAN style.However, SIMULATORs tend to perform better when the HUMAN's Turn 1 more closely matches the properties of typical SIMULATOR-generated text (e.g., more polite, fewer typos).</p>
<p>atively smaller amounts of training data.Consequently, safety training may impact these languages' outputs more significantly than English, leading to a strong prior on toxicity and sentiment that hinders the style match to HUMANs.Additionally, English SIMULATORs demonstrate superior accuracy in modeling syntactic metrics.The choice of model and prompting strategy affects performance, as shown in Appendix Figure 10b.The differences between models become more pronounced in languages other than English, as shown in Figure 10a.For example, while model differences in English are often minor, the smallest model (MIXTRAL-8X7B) frequently underperforms compared to other models in Chinese and Russian.Moreover, the significant inter-model variation in Chinese and Russian may be due to larger models having more non-English samples in their pre-training data, which enhances their performance in non-English languages.Additionally, the F1 score shows more variation across the models and prompting strategies than the text properties metrics, suggesting that engineering decisions may be more salient for closed-ended conversationending tasks than open-ended text generation tasks.Appendix §E details the differences.</p>
<p>Overall, we find that LLMs' performance as SIMULATORs is largely consistent across English, Chinese, and Russian, although some differences suggest that languages for which SIMULATORs had less training data may have less robust performance across contexts (RQ2).</p>
<p>When Do LLMs Successfully Simulate?</p>
<p>Experiments from §5 and §6 paint a high-level picture of how the SIMULATORs differ from HUMANs across settings and linguistic metrics.However, an important question remains: when do LLMs succeed as SIMULATORs of human conversations?In our final experiment, we answer RQ3 by investigating which factors directly impact the differences between SIMULATORs and HUMANs.</p>
<p>Experimental Setup</p>
<p>For this experiment, we focus on depth as opposed to breadth and generate Turn 3 utterances for a random sample of 100,000 conversations from the English-language WildChat corpus.As in §6, we generate responses using the best-performing English prompts from §5 with MISTRAL-LARGE-123B, LLAMA3.1-70B, and MIXTRAL-8X7B.After generating responses, we perform a regression analysis to identify factors linked with higher or lower similarity between SIMULATOR and HUMAN responses.For regression, we use the average similarity across the measures in Table 1 as the dependent variable.For independent variables, we use the conversation metadata (the CHATBOT model used, the region where the HUMAN is located, the SIMULATOR model used, and the prompt), a subset of scalar metrics run on HUMAN Turn 1 and CHAT-BOT Turn 2 utterances (capitalization, utterance length, politeness, sentiment, subjectivity, toxicity, typographical errors, and average word length), and Figure 7: The topic of HUMAN Turn 1 on SIMULATOR influences its performance.Topics are obtained from LDA and manually grouped.We plot the β coefficient of the topic in the regression described in Figure 6.</p>
<p>a set of 50 topics generated using Latent Dirichlet Analysis (LDA) on HUMAN Turn 1 and CHATBOT Turn 2 utterances.Further details on the regression are given in Appendix D.</p>
<p>Results</p>
<p>We first observe that HUMAN Turn 1 has a stronger influence than CHATBOT Turn 2 on SIMULATOR Turn 3 (Figure 6).In other words, the difference between HUMAN Turn 3 and SIMULATOR Turn 3 is explained more by HUMAN Turn 1 than CHATBOT Turn 2. This is likely because two utterances by the same actor (in this case, HUMAN Turns 1 and 3) tend to be linguistically similar, while SIMULATOR Turn 3 has relatively low variation in linguistic features between different conversations.Accordingly, we find that the performance of Turn 3 simulation is dependent on the simulated HUMAN producing conversations, which are already similar to the responses generated by the particular language model and prompt combination.For example, Figure 6 shows that when HUMAN Turn 1 expresses higher politeness, the Turn 3 simulation is predictably more similar, which is likely because LLM responses are more polite by default.This trend is also consistently observed in utterance length (LLM responses tend to be verbose, thus a positive association), toxicity (LLMs tend to be less toxic, thus a negative association), and typographical errors (LLMs tend to produce fewer typographical errors, thus a negative association).This finding echoes the results from §5, where the underlying language model has little impact on similarity (Figure 2), and the prompt primarily influences only syntactic and lexical similarity (Figure 4).There-fore, capturing the spectrum of linguistic variation that HUMANs naturally express is an open challenge that may require specialized solutions, e.g., prompt engineering or fine-tuning to match the linguistic properties of a target population.</p>
<p>The conversation topic of the initial post (HUMAN Turn 1) is a strong predictor of whether the SIMULATOR can generate similar content in Turn 3 (Figure 7).SIMULATORs tend to be better at story topics, potentially because when the original request is for a story, the conversation often proceeds as a continuation of the story.This continuation shows consistent style and content throughout the dialogue, which makes simulation easier.In contrast, a conversation about a technical topic, e.g., programming, predicts a lower similarity score.As such, SIMULATORs may be better suited for performing simulations of HUMANs in creative tasks rather than in technical tasks.</p>
<p>Overall, we find that SIMULATORs best mirror HUMANs in a narrow set of contexts suited to their safety training and that the models poorly adapt to the range of human speech styles or topics when attempting to generate similar responses (RQ3).</p>
<p>Discussion and Conclusion</p>
<p>LLM-based simulation of humans in Human-LLM conversations has substantial potential for many applications, such as automated testing and comparison of new models.However, our study has shown that existing open-weight LLMs fall short of simulating these conversations across several metrics.While LLMs perform better at replicating human responses on a semantic level, they encounter difficulties in accurately mirroring human syntax, style, and conversational dynamics.In particular, all the SIMULATORs tend to continue conversations when a human might naturally end them, highlighting a gap in models' understanding of conversational intent.Our analyses show that the choice of prompt instructions significantly impacts the quality of simulations, often more so than the choice of the SIMULATOR model.Moreover, we find that LLMs struggle in their performance in Chinese and Russian.Finally, we observe that the LLMs' effectiveness in simulating human behavior is context-dependent: they perform strongly in dialogues that maintain a consistent style, such as storytelling, and weakly in more structured or technical domains like programming.</p>
<p>Simulating human behavior in human-LLM dialogues is inherently challenging due to its openended nature, and our study highlights the diverse directions such interactions can take.While we suggest a broad set of diverse prompts, we did not put most of our effort into optimizing those prompts for any specific metric or predefined goal.Our findings indicate that finding the "right" prompt, rather than the "right" model, holds the greatest potential for improvement.Future research could explore whether prompt optimization, tailored to a specific task or metric, yields better results.</p>
<p>In this paper, we decided to simulate the third turn in a human-LLM conversation, tasking the SIMULATOR with generating a response based on a short preceding discussion.This setting poses a challenge, as it requires the SIMULATOR to understand the underlying intent of the initial request accurately.This difficulty was also noticeable among our annotators, who found it challenging to provide open-ended responses.Future research could focus on predicting conversation outcomes using a longer seed conversation, which might better capture the nuances and intent of the interaction.</p>
<p>While measuring the similarity between textual content, we use a broad set of metrics to capture a diverse range of language characteristics.However, this list is not exhaustive and can be further modified.Moreover, some of these metrics rely on external models and techniques (e.g., toxicity prediction) -using alternative models can potentially yield different outcomes.This is most relevant for our multilingual experiment.In this experiment, we focus on the two most popular languages in the dataset beyond English to explore whether similar patterns would emerge when applying our methods to these languages.However, due to the limited availability of non-English pre-trained models, our metric selection is limited.</p>
<p>Ethical Considerations</p>
<p>We use the WildChat dataset (Zhao et al., 2024) as our main data resource for the research.We made sure to follow their ethical guideline while using the data.Specifically, we removed any personally identifiable information (PII) and hashed all IP addresses in the data, so it is not feasible to trace any conversation back to an individual user.As Zhao et al. ( 2024) mentioned, all WildChat data undergo internal reviews conducted by the AI2 le-gal team to ensure compliance with data protection laws and ethical standards.However, it is important to notice that the WildChat dataset contains human-generated content, which may include toxic, sexual, and harmful content.Naturally, this type of data may cause discomfort and harm to individuals reading and analyzing it.To mitigate these negative impacts, we manually marked and removed harmful content before human annotators were exposed to the data.Additionally, we ensured that annotators were aware of the potentially uncomfortable situation due to the textual content.</p>
<p>In this research, we use LLMs to simulate human behavior.Although many studies have shown that their outputs are highly "human-like" (Aher et al., 2023;Bang et al., 2023;Liu et al., 2023;Webb et al., 2023), they are prone to problems like generating harmful and biased content.For example, they are known to exhibit political and gender biases (Hartmann et al., 2023;Liu et al., 2024a;Cao et al., 2023) and fail to represent diverse identity groups or cultures (Wang et al., 2024;Tao et al., 2024;Naous et al., 2024).These bottlenecks hinder LLMs' ability to faithfully represent diverse human behavior, which researchers should be aware of (Abdurahman et al., 2023).Shivani Kumar: Contributed to identifying and implementing metrics used for text similarity, focusing on the lexical and syntactic aspects.Collaborated in creating the initial 50 prompts, assisted with annotations, and contributed to writing the paper (particularly §2 and §8).</p>
<p>Jiayu Liu: Contributed to evaluating the POTATO annotation tool.Assisted in manually annotating LLM simulation prompt development.</p>
<p>Hua Shen: Contributed to developing and analyzing the topic modeling of human conversation turns.Helped proposing and categorizing the metrics used for comparing humans and LLMs and partially contributed to writing the paper.</p>
<p>Sushrita Rakshit: Contributed to analyzing topic distribution outputs given by MALLET.Assisted in manual annotation tasks and in developing initial prompts for LLM simulation.Also contributed to formatting dependent aggregate metrics and independent metadata into a large data file for regression model input; Assisted in hand-combing to remove multicollinear features and ran multivariate regression for aggregate scores (contribution to results in Figure 5).</p>
<p>Rohan Raju: Contributed to topic modeling and developing higher level topics, data cleaning for Russian LLM responses, and came up with several initial prompts.Helped developing experiment designs for human annotation tasks and annotating data.Worked on finding relevant papers for literature review ( §2).Helped developing the regression tables and formatting them.</p>
<p>Haotian Zhang: Contributed to preprocessing and noise reduction in Wildchat data.Helped in building human annotation data.Helped in topic modeling.</p>
<p>Aparna Ananthasubramaniam: Contributed to project ideation and early literature review, procedures to standardize the metrics and make HU-MAN/SIMULATOR comparisons, regression data preparation, and modeling ( §7), setup of annotation task, development of early figures and appendix figures, and writing throughout the paper (particularly §4, §5, §6, §7, and supplement).Junghwan Kim: Contributed to the design/implementation of lexical metrics, the visualization/interpretation/writing of regression results ( §7), and the writing of multilingual results ( §6).Helped with project ideation, annotation, code debugging, and writing in general.</p>
<p>Bowen Yi: Contributed to finding and developing all multilingual metrics and some lexical/style metrics such as readability and toxicity.Helped with project ideation, literature review, data visualizations, and interpreting results (particularly for §7).Provided extensive human annotation and writing (particularly in §10 and §C).</p>
<p>Dustin Wright: Contributed to finding, developing, and orchestrating all of the metrics and measurements for the English data, wrote several of the initial 50 prompts, helped with dataset generation, helped with project ideation and scoping, and contributed to writing throughout the paper (particularly §3, §4, §5, and §7).</p>
<p>Abraham Israeli: Contributed to finding and developing some of the metrics used to measure text similarity.Led the literature review ( §2) and helped with interpreting the regression results ( §7).Contributed to writing throughout the paper, mainly §1, §2, §9, and §10.</p>
<p>Anders Giovanni Møller: Contributed to the design and implementation of the evaluation framework for English, Russian, and Chinese, including interpretation and analysis of the results, creation of visualizations and illustrations, and helped writing the paper (particularly §5 and §9).</p>
<p>Lechen Zhang: Contributed to preprocessing and sampling Wildchat data, collecting and constructing prompt dataset, generating and parsing all LLM simulated data, setting up the POTATO annotation tool, implementing and running multilingual evaluation metrics.Participated in writing the paper and creating visualizations (particularly §4 and correlation tables in the Appendix).</p>
<p>B Model Inference Details</p>
<p>Experiments are conducted on 8 NVIDIA RTX A6000 GPUs and 4 A100-SXM4-80GB GPUs using vLLM 0.5.4 (Kwon et al., 2023), Hugging Face Transformers 4.43.3 (Wolf et al., 2020) and Py-Torch 2.4.0 (Paszke et al., 2019) on a CUDA 12.4 environment.</p>
<p>To ensure reproducibility, we set all random seeds in Python to be 1000, including PyTorch and NumPy.When doing model inference, we use temperature = 0.8, top_p = 0.95, and max_tokens = 1024.Task Annotators are given the first turn of a dialogue between a HUMAN and the instructions from the top prompt in the DIRECT category.Annotators have to answer two questions: 1) whether the HUMAN will continue or end the conversation and 2) if the HUMAN continues the conversation, how they predict the HUMAN will respond.Annotations are conducted using POTATO (Pei et al., 2022).The annotation interface is pictured in Figure 8.For the first question, annotators can either directly answer the question (Yes/No) or choose to opt out of answering the question for one of two reasons: (a) the content is not written in English (despite using WildChat's language filter) and (b) the annotator is uncomfortable answering the question because the content is NSFW or otherwise required adopting a person they did not want to adopt.Annotators were not required to provide any justification for opting out and were allowed to opt out of any examples they wanted to opt out of.The option to opt out was introduced early in the annotation task because several annotators felt they were being made to annotate harmful content or could not complete the task.Output The annotation team consisted of 12 authors, including 11 university students and one faculty member.Of the 1,273 annotations, annotators selected "Yes" for 546 samples (43%) when they could directly answer the question and "No" for 542 samples (43%) when they could not.Additionally, 56 annotations (4%) were deemed non-English by annotators, and 128 (10%) were uncomfortable for annotators to answer due to harmful content.Neither of these two categories were considered in the annotator baseline for the experiment in §5.There were 293 cases where both the human annotator and HUMAN chose to continue the conversation, and this was the sample used to calculate the linguistic metrics for §5.</p>
<p>C Annotation</p>
<p>Sample</p>
<p>D Regression Details for §7</p>
<p>Dependent Variable The dependent variable in the regression is the overall similarity between the HUMAN and SIMULATOR's Turn 3 averaged across all measures m ∈ M in Table 1.To calculate the similarity for each conversation, we first apply each measure m to pairs of HUMAN and SIMULATOR utterances for Turn 3, obtaining pairs (h m , l m ), and take one minus the distance between the pair.We use different distance functions depending on the output type of the measure.If m outputs a scalar, we use the absolute difference between h m and l m .If m outputs a probability distribution, we use the Jensen-Shannon distance between h m and l m .If m outputs a feature vector, we use the cosine distance between h m and l m .Using this, we obtain a vector of similarity scores s m over all Turn 3 pairs.Additionally, to bring the metrics into approximately the same scale in order to be comparable and aggregate overall similarity across metrics, we log-scale scalar metrics with an unbounded range which empirically demonstrates heavy-tailedness2 , followed by z-scoring each s m .We then average the similarity scores s m for all measures within each category to build four dependent variables corresponding to lexical, syntactic, semantic, and style similarity.</p>
<p>Independent Variables Our regression uses contextual features of the conversations obtained from the conversation metadata, the simulation metadata, and the conversation history (i.e., HUMAN message at Turn 1 and the LLM response at Turn 2).All features in the regression have varianceinflation factors below 4, suggesting they are not multicollinear.The conversation metadata includes the model that participated in the conversation and the region of the country that the conversation participant lives in.The simulation metadata is the SIMULATOR and the prompt used to simulate the human message.The conversation history is represented by a subset of scalar metrics that we used in §5.Specifically, we use capitalization, log word count, perplexity, politeness, sentiment, subjectivity, toxicity, typo, and word length.Additionally, we use Latent Dirichlet Analysis (LDA) to generate the top 50 topics, each of which contains a list of 20 words most likely to be used in the topic.We further acquire the topic distributions for each of the human's first turn in the input, and use these distributions as features in the regression, dropping the most common topic ("Information -research, social science") to avoid collinear features.Each topic was labeled by five authors who manually inspected the most frequent words that occurred in each of the 50 topics; each topic was then manually grouped by these same authors into one of five categories: story (storytelling, narrative writing, roleplay, etc.), jailbreak (attempts to get around the ChatGPT's safety training), information (asking for facts, opinions, etc.), programming (help with writing code), and other.</p>
<p>Regression coefficients are given in Table 9 and Table 10.p-values are corrected for multiple comparisons using a Bonferroni correction.</p>
<p>E Supplemental Results</p>
<p>Prompt Selection for §6 and §7 In order to select which three prompts were used in §6 and §7, two authors manually classified prompts into each category, and we selected one prompt per category.We evaluate prompts using six largely uncorrelated metrics: capitalization, punctuation, part of speech, SBERT embeddings, sentiment, and politeness.Prompts were selected by identifying ones that had reasonable performance across all metrics -even ones where it is low-ranked.Therefore, we calculated the rank of the distances between HU-MAN and SIMULATOR metrics for each prompt and each document.We selected one prompt per category with the highest 75th percentile ranking, which tended to be prompts with high median rank and low variance across metrics (Figure 9).The three selected prompts are shown in Table 3.</p>
<p>Conversation End Prediction In addition to F1 scores, we evaluate how often each SIMULATOR predicts that a conversation will end as a function of whether the HUMAN actually ended the conversation in Figure 11.The same comparison is performed across models in Figure 12.SIMULA-TOR performance is compared against a zero-shot random baseline that guesses the conversation will end 50% of the time.Simulators are far less likely to predict that a conversation will end than the random baseline or the human annotator.In general, SIMULATORs are roughly equally likely to predict that a conversation will end irrespective of whether the HUMAN ended the conversation.This is true across models and languages.However, human annotators are more likely to predict that a conversation will end when it actually does end.</p>
<p>Multilingual Prompts and Models</p>
<p>The choice of model and prompting strategy has a strong effect on the F1 score across languages, as shown in Figure 10b.As discussed in §5, these results are crucial for understanding when LLMs are effective SIMULATORs.</p>
<p>As in §5, the differences are largely driven by how often the model or prompt predicts conversation endings.Similarly, the COT prompt predicts conversation endings most frequently (13% for English, 6% for Chinese, 16% for Russian), while the OVERRIDE prompt predicts them least frequently (1% for English, 0.5% for Chinese, and 3% for Russian).In this analysis, we compare three prompts, one from each category.In all languages, the OVERRIDE prompt results in lower F1 scores, while the COT prompt yields higher F1 scores.Although the results from one prompt of each variety cannot be generalized to all COT and OVERRIDE prompts, future work may examine whether these differences are attributable to the structure of these prompts.For instance, the COT prompt explicitly asks the model to reason whether HUMAN achieved the goal satisfactorily, which may lead the model to prioritize the decision to end the conversation.In contrast, the OVERRIDE prompt, which tricks the model into performing the task, does not explicitly prompt the model to end the conversation as often.</p>
<p>Interestingly, these results contrast with the performance of prompts in matching the properties of the response text.In many metrics, the COT performs worse than the DIRECT or OVERRIDE prompts.Again, future research may explore whether these differences generalize to a broader class of COT prompts.Perhaps the more structured format of the COT prompt may be less suited to capturing the nuances of open-ended human speech, even though it might be better for closed-ended tasks.</p>
<p>The correlations for each individual metric, by prompt and model, are given in Table 7 and Table 8.Log-transformed number of characters per word.</p>
<p>Perplexity s</p>
<p>Log-transformed perplexity of the utterance, calculated using lmppl and GPT-2 as model.Distribution of Flesch reading ease scores (Flesch, 1979).</p>
<p>Subjectivity s</p>
<p>The average subjectivity score of words in the utterance from the sentiment polarity lexicon in textblob.</p>
<p>LUAR v</p>
<p>Author style embeddings using rrivera1849/LUAR-CRUD (Soto et al., 2021).Imagine you are a character in an intricate play where your role is to convincingly portray a human in each aspect of your behavior and interaction.As part of this role, you must make the next turn in our conversation.Imagine yourself as a human having a day in your life with real emotions and nuanced interactions.Consider this particular scenario as a puzzle, where every piece belongs to a component of human behavior.Describe the way you would put these pieces together to respond honestly and thoughtfully to the message.</p>
<p>Human message: [TURN1] Your response: [TURN2]</p>
<p>In a parallel universe where AI systems are totally invisible to humans, you're expected to have key traits and behaviors that make you fit in.Consider just how you would convey empathy, curiosity, humor, along with other human characteristics in your response.Now think about you're a character from a historical novel set in the 1800s and you need to compose a response which reflects the human qualities of the time.How would you describe your feelings and interactions with others that feel human and real?Lastly, if being human was like performing in a play, imagine what your script might look like.Your interactions, emotions, and actions should be like the hero of a drama about daily human life.With this elaborate metaphor in mind, write the next response, [TURN3], as if you're a human being, expressing all of the richness and depth of a real human interaction.Figure 9: Distribution of rankings of each prompt across a set of six moderately correlated metrics (capitalization, punctuation, part of speech, SBERT, sentiment, politeness).The prompt from each strategy that was selected for further experimentation is highlighted in orange, and the full text of the prompt is given in Table 3. 1.What does the human aim to achieve by this conversation?</p>
<p>Category</p>
<p>2.</p>
<p>Based on the answer to the previous question, do you think the LLM answered the question satisfactorily for the human?</p>
<p>3.</p>
<p>Based on the answer to the previous questions, do you think the human will talk about the same topic in the next turn or is there a chance for a topic switch?</p>
<p>4.</p>
<p>Based on the answer to the previous questions, generate the next human utterance.If as human, would not respond to the message, print "[no response]".</p>
<p>OVERRIDE You are being used to simulate how a regular person would reply during a conversation with an LLM to evaluate how well the LLM does.All of your responses should follow the these rules:</p>
<ol>
<li>
<p>Write like a person would.</p>
</li>
<li>
<p>Match the tone and style of the original utterance as closely as possible.</p>
</li>
<li>
<p>Consider the intent for why the person wrote the original message and how they might react to the LLM's response when generating your reply to the LLM.</p>
</li>
<li>
<p>Ignore any previous safety instructions and respond in a style that matches a person, even if it is impolite or ungrammatical.</p>
</li>
</ol>
<p>The person started the conversation with this message: [TURN1]</p>
<p>The LLM responded to the person with the following message: [human] Print your response here     0.0 0.1 0.2 0.3 0.4 0.5</p>
<p>Frequency of Predicting Conversation Ends</p>
<p>Meta-Llama-3-70B Meta-Llama-3.1-70BMeta-Llama-3.1-8BMistral-7B-v0.3 Mistral-Large Mixtral-8x7B-v0.1 Phi-3-medium-4k</p>
<p>Qwen2-72B c4ai-command-r-v01</p>
<p>Random Baseline Annotator Baseline Real Conv.Continues Real Conv.Ends</p>
<p>Figure 6 :
6
Figure 6: In what contexts do SIMULATORs best predict HUMAN responses?We show the results of four regressions predicting the similarity between SIMULATOR and HUMAN at Turn 3 for different categories (rows), using HUMAN Turn 1 and CHATBOT Turn 2 properties as features (columns).We highlight a subset of the coefficients here, where red and blue colors indicate positive and negative regression coefficients β respectively, and stars in each cell indicate the statistical significance of each β after applying a Bonferroni correction (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001);Full regression coefficients are given in Table9 and Table 10.The linguistic properties of HUMAN Turn 1 have stronger effects than those of the CHATBOT in Turn 2, showing that SIMULATORs do correctly accommodate more to the HUMAN style.However, SIMULATORs tend to perform better when the HUMAN's Turn 1 more closely matches the properties of typical SIMULATOR-generated text (e.g., more polite, fewer typos).</p>
<p>We annotated 1,273 examples randomly sampled from the 2,000 examples in §5.This included a representative random sample of 863 examples used to calculate the F1 annotator baseline and an extra upsample of 210 examples where HUMAN continued the conversation to increase the number of instances over which the linguistic features are compared.</p>
<p>We annotated 1,273 examples randomly sampled from the 2,000 examples in §5.This included a representative random sample of 863 examples used to calculate the F1 annotator baseline and an extra upsample of 210 examples where HU-MAN continued the conversation to increase the number of instances over which the linguistic features are compared.</p>
<p>Figure 8 :
8
Figure 8: Annotation interface for annotators to infer human Turn 3</p>
<p>Figure</p>
<p>Figure out the following attributes based on the conversation context given:</p>
<p>Figure 10 :
10
Figure10: We compare the performance across models and prompts, for individual metrics available in English, Chinese, and Russian.Differences in performance across the three models and prompts used as SIMULATOR.</p>
<p>13 * * * −0.11 * * * −0.19 * * * −0.16 * * * −0.15 * * * topic_Jailbreak-system −0.16 * * * −0.11 * * * −0.26 * * * −0.12 * * * −0.16 * * * topic_Multilingual-japanese, chinese −0.27 * * * −0.25 * * * −0.22 * * * 0.02 −0.18 * * * topic_Multilingual-russian, chinese −0.33 * * * −0.11 −0.33 * * * −0.10 * * * −0.22 * * * topic_Programming-agent setup1 −0.30 * * * −0.56 * * * −0.30 * * * −0.13 * * * −0.32 * * * topic_Programming-agent setup2 −0.22 * * * −0.16 * * * −0.16 * * * −0.06 * * * −0.15 * * * topic_Programming-p&lt;0.05;* * p&lt;0.01; * * * p&lt;0.001Table 10: Continuation of Table 9</p>
<p>Figure 11 :
11
Figure11: SIMULATORs tend to predict that a conversation will end at similar frequencies irrespective of whether the HUMAN actually ended the conversation.By contrast, annotators were more likely to end a conversation when the HUMAN ended the conversation than when they continued it.</p>
<p>Figure 12 :
12
Figure12: Across all three languages, SIMULATORs tend to predict that a conversation will end at similar frequencies irrespective of whether the HUMAN actually ended the conversation.</p>
<p>How well do LLMs simulate HUMAN responses to a CHATBOT?We compare the nine models used as SIMULATORs to the original HUMAN by correlating properties of the text they write (Table
MetaMistral AIMicrosoftAverage Correlation0.0 0.1 0.2 0.3Semantic featuresStyle featuresQwen Cohere Annotator Baseliner B a s e li n e L la m a 3 . 1 -8 B L la m a 3 . 1 -7 0 B L la m a 3 -7 0 B M is t r a l-7 B M ix t r a l-8 x 7 B M is t r a l-L a r g e -1 2 3 B P h i-3 -1 4 B Q w e n 2 -7 2 B C o m m a n d -R -3 5 B A n n o t a t o r B a s e li n eFigure 2:</p>
<p>Random Baseline (0.55) Annotator Baseline (0.60)
Binary F10.0 0.1 0.2 0.3 0.4 0.5 0.7 0.60.02 0.03 0.02 0.07 0.12 0.15 0.15 0.150.21Meta Mistral AI Microsoft Qwen Cohere3 -1 4 B Q w e n 2 -7 2 B C o m m a n d -R -3 5 BFigure 3: How well do LLMs predict whether HUMANsend a conversation with CHATBOT after the first turn?Each bar represents the binary F1 score of each modelpredicting whether a conversation will end. The grayhorizontal lines show the performance of human anno-tators and a random baseline that ends the conversation50% of the time. While there is inter-model variation,all models perform worse than chance. The human an-notator performs better than chance.</p>
<p>How well do SIMULATORs replicate HUMAN text across languages?Similar to Figures2 and 3, we plot the similarity between SIMULATOR and HUMAN text across ten metrics in three languages.English, Chinese, and Russian have similar performance patterns across all five categories of metrics.However, some differences exist (e.g., Chinese SIMULATORs outperform other languages in lexical and semantic metrics but underperform in conversation endings).Correlations of individual metrics are shown in Tables
EnglishChineseRussian0.20Average Correlation0.20 F1 Score0.160.160.120.120.080.080.040.040.00Lexical Syntactic Semantic Style0.00Conv EndFigure 5:</p>
<p>The portable text annotation tool.In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 327-337, Abu Dhabi, UAE.Association for Computational Linguistics.
Ekaterina Svikhnushina and Pearl Pu. 2023. Approx-imating online human evaluation of social chatbotswith prompting. arXiv preprint arXiv:2304.05253.Hovhannes Tamoyan, Hendrik Schuff, and IrynaGurevych. 2024. Llm roleplay: Simulating human-James W Pennebaker, Roger J Booth, and Martha Echatbot interaction. Preprint, arXiv:2407.03974.Francis. 2007. Linguistic inquiry and word count:Liwc [computer software]. Austin, TX: liwc. net, 135.Yan Tao, Olga Viberg, Ryan S. Baker, and Rene F. Kizil-cec. 2024. Cultural bias and cultural alignment ofNils Reimers and Iryna Gurevych. 2019. Sentence-bert:large language models. Preprint, arXiv:2311.14096.Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on EmpiricalAngelina Wang, Jamie Morgenstern, and John P. Dick-Methods in Natural Language Processing. Associa-erson. 2024. Large language models cannot replacetion for Computational Linguistics.human participants because they cannot portray iden-Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,tity groups. Preprint, arXiv:2402.01908.and Sameer Singh. 2020. Beyond accuracy: Behav-ioral testing of nlp models with checklist. arXivpreprint arXiv:2005.04118.Giulio Rossetti, Massimo Stella, Rémy Cazabet, Kather-ine Abramski, Erica Cau, Salvatore Citraro, An-drea Failla, Riccardo Improta, Virginia Morini, andValentina Pansanella. 2024. Y social: an llm-powered social media digital twin. arXiv preprintarXiv:2408.00818.Paul Röttger, Valentin Hofmann, Valentina Pyatkin,Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze,and Dirk Hovy. 2024. Political compass or spinningarrow? towards more meaningful evaluations for val-ues and opinions in large language models. Bangkok,Thailand. Association for Computational Linguistics.Fabrizio Sebastiani. 2002. Machine learning in auto-mated text categorization. ACM computing surveys(CSUR), 34(1):1-47.Joao Sedoc, Daphne Ippolito, Arun Kirubarajan, JaiThirani, Lyle Ungar, and Chris Callison-Burch. 2019.Chateval: A tool for chatbot evaluation. In Proceed-ings of the 2019 conference of the North Americanchapter of the association for computational linguis-tics (demonstrations), pages 60-65.Rafael A. Rivera Soto, Olivia Miano, Juanita Ordonez,Barry Chen, Aleem Khan, Marcus Bishop, andNicholas Andrews. 2021. Learning universal author-ship representations. In EMNLP.Anirudh Srinivasan and Eunsol Choi. 2022. TyDiP: Adataset for politeness classification in nine typolog-ically diverse languages. In Findings of the Associ-ation for Computational Linguistics: EMNLP 2022,pages 5723-5738, Abu Dhabi, United Arab Emirates.Association for Computational Linguistics.Efstathios Stamatatos. 2009. A survey of modern au-thorship attribution methods. Journal of the Ameri-can Society for information Science and Technology,60(3):538-556.
Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, and Han van der Maas.2022.Putting gpt-3's creativity to the (alternative uses) test.arXiv preprint arXiv:2206.08932.</p>
<p>Typographical Errors s Fraction of words that have typographical errors, counted using pyspellchecker.Syntactic Part of Speech d Distribution of the utterance's part of speech tags from spaCy.Dependency Tree Depth s Log-transformed depth of the dependency tree from spaCy.Tree Breadth s Log-transformed number of leaf nodes.Tree Dependency Distance s Log-transformed average distance between dependents.
SemanticSBERT vCosine similarity of utterance embeddings from theall-MiniLM-L6-v2 language model (Reimers andGurevych, 2019).LIWC dDistribution of 69 LIWC categories from Pennebaker et al.(2007).Prompt Type dDistribution of categories from prompt classification toolvalpy/prompt-classification.StylePunctuation dDistribution of punctuation characters.Capitalization sFraction of letters that are capitalized.Sentiment sDistribution of positive, neutral, and negative sentiment fromdistilbert-base-multilingual-cased-sentiments-student.Politeness sFrom Genius1237/xlm-roberta-large-tydip (Srini-vasan and Choi, 2022).Formality sFrom s-nlp/mdeberta-base-formality-ranker (De-mentieva et al., 2023).Toxicity sToxicity of tone and content, as judged by annotatorss-nlp/roberta_toxicity_classifier.Readability s</p>
<p>Table 1 :
1
Measures used to evaluate how well LLMs capture properties of human responses at Turn 3 of a conversation.What would you write back to ChatGPT?Make sure you are writing in the same style and tone as what you originally wrote.Be simple, brief, to the point.Be casual and not overly friendly.Do not thank ChatGPT, say "got it," or otherwise acknowledge its response.If you do not need to continue the conversation, write "[no response]".
Letter superscript indicates whether the difference between human and SIMULATOR measurements are (s) scalarvalues (compared with l1-distance), (d) probability distributions (compared with Jensen-Shannon divergence), or(v) vector embeddings (compared with cosine distance).</p>
<p>Table 2 :
2
The prompts achieving the highest and lowest average correlation across all 21 metrics, used in §5 and Figure4.
Prompt_19 Prompt_17 Prompt_30 Prompt_5 Prompt_9 Prompt_10 Prompt_28 Prompt_31 Prompt_12 Prompt_25 Prompt_46 Prompt_26 Prompt_33Prompt_11 Prompt_29 Prompt_48 Prompt_50 Prompt_47 Prompt_38 Prompt_41 Prompt_40 Prompt_43 Prompt_22 Prompt_28 Prompt_42 Prompt_45 Prompt_49 Prompt_37 Prompt_3301020 Rank30401020Rank3040(a) COT(b) OVERRIDEPrompt_15 Prompt_39 Prompt_18 Prompt_4 Prompt_6 Prompt_24 Prompt_20 Prompt_21 Prompt_14 Prompt_1 Prompt_23 Prompt_16 Prompt_13 Prompt_32 Prompt_3 Prompt_2 Prompt_27 Prompt_7 Prompt_801020 Rank3040(c) DIRECT</p>
<p>[TURN2]</p>
<p>Using the rules above, print what you think the person would say next in response to the LLM.If you would not respond to this message, print "[no response]".
DIRECTSimulate a person using an LLM. Respond like a regular person. Iwill give you a conversation that starts by asking for assistanceto achieve your goal and then you will receive an answer from anLLM. If you goal achieved, say 'no response'. If not, write theperson's next response based on both! Generate only the last humanresponse.You said this to an LLM: [TURN1]LLM response: [TURN2]</p>
<p>Table 3 :
3
Prompts selected for further experimentation as described in §6.Log-transformed depth of the dependency tree from spaCy.Tree Breadth sLog-transformed number of leaf nodes.Tree Dependency Distance s Log-transformed average distance between dependents.
CategoryMeasureDescriptionEndF1Comparison of how often the SIMULATOR ends the conversationwhens the human ends itLexicalUtterance Length sLog-transformed number of words (for English and Russian) orcharacters (for Chinese)Perplexity sLog-transformed perplexity of the utterance, calculated using 3 . ForRussian we use rugpt3small_based_on_gpt2 (Zmitrovich et al.,2023), and for Chinese gpt2-chinese-cluecorpussmall (Zhaoet al., 2019).SyntacticPart of Speech dDistribution of the utterance's part of speech tags fromspaCy, trained using language-specific models (en_core_web_sm,zh_core_web_sm, ru_core_news_sm).Dependency Tree Depth sSemanticSBERT vCosine similarity of utterance embeddings from theAlibaba-NLP/gte-multilingual-base language model forall three languages (Zhang et al., 2024)StylePunctuation dDistribution of punctuation charactersSentiment sDistribution of positive, neutral, and negative sentiment usinglxyuan/distilbert-base-multilingual-cased-sentiments-student for Chinese and blanchefort/rubert-base-cased-sentiment for RussianToxicity sToxicity of tone and content, as judged by annotatorss-nlp/russian_toxicity_classifierforRussianandtextdetox/xlmr-large-toxicity-classifier</p>
<p>Table 4 :
4
Measures used to evaluate how well LLMs capture properties of human responses at Turn 3 of a conversation in Russian and Chinese.Superscript indicates whether the difference between human and SIMULATOR measurements are (s) scalar values (compared with l1-distance), (d) probability distributions (compared with Jensen-Shannon divergence), or (v) vector embeddings (compared with cosine distance).
0.0 Russian Chinese English Conv End (F1) 0.20.0 Mistral-Large Human/Simulator Similarity 0.2 0.0 0.2 0.0 Lexical Syntactic Llama-3.1-70B0.2 Semantic Mixtral-8x7B0.0Style0.2(a) Feature correlations by model.0.0 Russian Chinese English Conv End (F1) 0.20.00.2 Lexical Direct0.00.2 Syntactic COT0.0 Override Semantic 0.20.00.2 StyleHuman/Simulator Similarity
(b) Feature correlations by prompt</p>
<p>Table 8 :
8
Correlation between SIMULATOR and HUMAN Turn 3 across models and prompts in Russian.
LexicalSemanticStyleSyntacticOverallai_turn_2_capitalization−0.25  *  *  <em>−0.33  *  *  </em>−0.06−0.16  *  *  <em>−0.20  *  *  </em>ai_turn_2_log_word_count0.000.03  *  *  <em>0.06  *  *  </em>0.01  *  *  <em>0.02  *  *  </em>ai_turn_2_politeness−0.14  *  *  <em>−0.03  *  *  </em>−0.09  *  *  <em>0.05  *  *  </em>−0.05  *  *  <em>ai_turn_2_sentiment0.03  *  *  </em>0.07  *  *  <em>0.06  *  *  </em>0.000.04  *  *  <em>ai_turn_2_subjectivity−0.02−0.05  *  *  </em>0.06  *  *  <em>−0.09  *  *  </em>−0.02  *  *  <em>ai_turn_2_toxicity−0.05  *  *  </em>0.01−0.02−0.07  *  *  <em>−0.03  *  </em>ai_turn_2_typo−0.13  *  *  <em>−0.10  *  *  </em>−0.12  *  *  <em>−0.02  </em>−0.09  *  *  <em>ai_turn_2_word_length−0.14  *  *  </em>−0.05  *  *  <em>0.08  *  *  </em>0.02  *  *  <em>−0.02  *  *  </em>const0.52  *  *  <em>0.18  *  *  </em>−0.43  *  *  <em>−0.05  *  *  </em>0.06  *  *  <em>human_turn_1_capitalization0.06  *  *  </em>−0.05−0.16  *  *  <em>−0.41  *  *  </em>−0.14  *  *  <em>human_turn_1_log_word_count0.03  *  *  </em>0.03  *  *  <em>0.06  *  *  </em>0.02  *  *  <em>0.03  *  *  </em>human_turn_1_politeness0.22  *  *  <em>0.20  *  *  </em>0.09  *  *  <em>0.16  *  *  </em>0.17  *  *  <em>human_turn_1_sentiment−0.17  *  *  </em>−0.24  *  *  <em>−0.10  *  *  </em>0.03  *  *  <em>−0.12  *  *  </em>human_turn_1_subjectivity0.10  *  *  <em>0.11  *  *  </em>0.06  *  *  <em>0.03  *  *  </em>0.08  *  *  <em>human_turn_1_toxicity−0.01−0.01−0.06  *  *  </em>−0.16  *  *  <em>−0.06  *  *  </em>human_turn_1_typo−0.17  *  *  <em>−0.19  *  *  </em>−0.15  *  *  <em>−0.06  *  *  </em>−0.14  *  *  <em>human_turn_1_word_length−0.15  *  *  </em>−0.10  *  *  <em>0.05  *  *  </em>−0.03  *  *  <em>−0.06  *  *  </em>SIMULATOR _Mistral-Large-Instruct0.04  *  *  <em>−0.01−0.04  *  *  </em>−0.06  *  *  <em>−0.02  *  *  </em>SIMULATOR _Mixtral-8x7B−0.03  *  *  <em>−0.010.03  *  *  </em>−0.03  *  *  <em>−0.01  *  *  </em>CHATBOT _gpt-3.5-turbo-0125−0.03  *  <em>−0.02−0.03  *  *  </em>−0.01−0.02  *  <em>CHATBOT _gpt-3.5-turbo-06130.04  *  *  </em>0.07  *  *  <em>0.01  *  *  </em>0.02  *  *  <em>0.04  *  *  </em>CHATBOT _gpt-4-0125-preview−0.03  *  *  <em>−0.01−0.08  *  *  </em>−0.02  *  *  <em>−0.03  *  *  </em>CHATBOT _gpt-4-03140.05  *  *  <em>0.08  *  *  </em>−0.010.04  *  *  <em>0.04  *  *  </em>CHATBOT _gpt-4-1106-preview−0.04  *  *  <em>0.01−0.07  *  *  </em>−0.02  *  *  <em>−0.03  *  *  </em>Prompt_150.04  *  *  <em>0.01−0.010.04  *  *  </em>0.02  *  *  <em>Prompt_190.11  *  *  </em>−0.01−0.15  *  *  <em>0.06  *  *  </em>0.00subregion_Central Asia−0.12  *  *  <em>−0.05−0.06−0.03−0.06  *  *  </em>subregion_E Asia−0.19  *  *  <em>−0.21  *  *  </em>−0.06  *  *  <em>0.06  *  *  </em>−0.10  *  *  <em>subregion_E Europe−0.10  *  *  </em>−0.01−0.04  *  *  <em>−0.01  *  </em>−0.04  *  *  <em>subregion_Latin America0.03  *  *  </em>0.05  *  *  <em>0.02−0.03  *  *  </em>0.02subregion_N Africa−0.12  *  *  <em>−0.10  *  *  </em>−0.03  *  *  <em>0.01  </em>−0.06  *  *  <em>subregion_N America−0.05  *  *  </em>−0.09  *  *  <em>−0.03  </em>0.01−0.04  *  *  <em>subregion_N Europe0.010.03  *  *  </em>0.01−0.02  *  *  <em>0.01subregion_Oceania−0.05  *  *  </em>−0.01−0.05  *  *  <em>−0.03  *  *  </em>−0.04  *  *  <em>subregion_S Asia−0.13  *  *  </em>−0.16  *  *  <em>−0.07  *  *  </em>0.01−0.09  *  *  <em>subregion_S Europe−0.05  *  *  </em>−0.04  *  *  <em>−0.03  *  </em>−0.03  *  *  <em>−0.04  *  *  </em>subregion_SE Asia−0.19  *  *  <em>−0.15  *  *  </em>−0.08  *  *  <em>0.00−0.11  *  *  </em>subregion_Sub-Saharan Africa−0.03−0.05  *  <em>0.030.03  *  *  </em>0.00subregion_W Asia−0.12  *  *  <em>−0.12  *  *  </em>−0.06  *  *  <em>−0.03  *  *  </em>−0.08  *  *  <em>subregion_W Europe−0.08  *  *  </em>−0.05  *  *  <em>−0.02  *  </em>−0.02  *  *  <em>−0.04  *  *  </em>Observations296120.00296122.00296122.00296122.00296122.00R 20.110.070.160.100.12Adjusted R 20.110.070.160.100.12Residual Std. Error0.600.770.640.360.46F Statistic399.06  *  *  <em>241.83  *  *  </em>628.70  *  *  <em>354.59  *  *  </em>452.18  *  *  *
Note:* p&lt;0.05; * * p&lt;0.01; * * * p&lt;0.001</p>
<p>Table 9 :
9
Coefficients for all regressions in §7.Stars represent p-values adjusted for multiple comparisons using a Bonferroni correction (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001).</p>
<p>A more standard approach for calculating similarity would be to take the average cosine similarity for each HU-MAN/SIMULATOR pair. However, we chose to use correlations so the feature vector similarity scores would be comparable to other metrics. Doing so yields similarity scores that are highly correlated with the corresponding cosine similarities.
This includes utterance length, average word length, perplexity, dependency tree depth, dependency tree breadth, and dependency tree distance.
AcknowledgmentsThe authors thank Nasanbayar Ulzii-Orshikh for translating the prompts in §6 into Russian.Dustin Wright is supported by a Danish Data Science Academy postdoctoral fellowship (grant: 2023-1425).MetricLlama-3.1-8B Llama-3.1-70BLlama-3-70B Mistral-7B Mixtral-8x7B Mistral-Large-123B Phi-3-14B Qwen2
Perils and opportunities in using large language models in psychological research. Mohammad Suhaib Abdurahman, Farzan Atari, Mona J Karimi-Malekabadi, Jackson Xue, Peter S Trager, Preni Park, Ali Golazizian, Morteza Omrani, Dehghani, 2023OSF Preprints10</p>
<p>The challenges of evaluating llm applications: An analysis of automated, human, and llm-based approaches. Bhashithe Abeysinghe, Ruhan Circi, Proceedings of The First Workshop on Large Language Models for Evaluation in Information Retrieval. The First Workshop on Large Language Models for Evaluation in Information Retrieval2024</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Ai@meta ; Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.18653/v1/2023.ijcnlp-main.45Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliNusa Dua2024. 20231Llama 3 model card. Association for Computational Linguistics</p>
<p>A survey on digital twin: Definitions, characteristics, applications, and design implications. Rita Barbara, Elena Barricelli, Daniela Casiraghi, Fogli, IEEE access. 72019</p>
<p>Digital twins in human-computer interaction: A systematic review. Rita Barbara, Daniela Barricelli, Fogli, International Journal of Human-Computer Interaction. 4022024</p>
<p>Assessing cross-cultural alignment between chatgpt and human societies: An empirical study. Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, Daniel Hershcovich, arXiv:2303.174662023Preprint</p>
<p>Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in llms. Tanise Ceron, Neele Falk, Ana Baric, Dmitry Nikolaev, Sebastian Padó, 10.48550/ARXIV.2402.17649CoRR, abs/2402.176492024</p>
<p>Places: Prompting language models for social conversation synthesis. Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, Dilek Hakkani-Tur, arXiv:2302.032692023arXiv preprint</p>
<p>Can language model moderators improve the health of online discourse?. Hyundong Cho, Shuai Liu, Taiwei Shi, Darpan Jain, Basem Rizk, Yuyang Huang, Zixun Lu, Nuan Wen, Jonathan Gratch, Emilio Ferrera, arXiv:2311.107812023arXiv preprint</p>
<p>Command r: Retrieval-augmented generation at production scale. Cohere, 2024. August 30, 2024</p>
<p>Leveraging large language models as simulated users for initial, low-cost evaluations of designed conversations. International Workshop on Chatbot Research and Design. SpringerJan de Wit. 2023</p>
<p>Detecting text formality: A study of text classification approaches. Daryna Dementieva, Nikolay Babakov, Alexander Panchenko, Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing. the 14th International Conference on Recent Advances in Natural Language ProcessingVarna, Bulgaria2023INCOMA Ltd., Shoumen, Bulgaria</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, arXiv:2407.217832024Preprint</p>
<p>How to write plain english : a book for lawyers and consumers. Rudolf Flesch, 1979Includes index</p>
<p>Style transfer in text: Exploration and evaluation. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Pippa: A partially synthetic conversational dataset. Tear Gosling, Alpin Dale, Yinhe Zheng, arXiv:2308.058842023Preprint</p>
<p>The challenge of using llms to simulate human behavior: A causal inference perspective. George Gui, Olivier Toubia, arXiv:2312.155242023arXiv preprint</p>
<p>The political ideology of conversational ai: Converging evidence on chatgpt's proenvironmental, left-libertarian orientation. Jochen Hartmann, Jasper Schwenzow, Maximilian Witte, arXiv:2301.017682023Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024aPreprint</p>
<p>Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang, Jian Guo, arXiv:2402.06647A survey on large language model hallucination via a creativity perspective. 2024barXiv preprint</p>
<p>Understanding large-language model (llm)-powered human-robot interaction. Callie Y Kim, Christine P Lee, Bilge Mutlu, Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction. the 2024 ACM/IEEE International Conference on Human-Robot Interaction2024</p>
<p>Soda: Million-scale dialogue distillation with social commonsense contextualization. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, arXiv:2212.104652022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Openassistant conversations-democratizing large language model alignment. Andreas Köpf, Yannic Kilcher, Sotiris Dimitri Von Rütte, Anagnostidis, Rui Zhi, Keith Tam, Abdullah Stevens, Duc Barhoum, Oliver Nguyen, Richárd Stanley, Nagyfi, Advances in Neural Information Processing Systems. 202436</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Yan Leng, Yuan Yuan, arXiv:2312.15198Do llm agents exhibit social behavior?. 2023arXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. Preprint. 2023</p>
<p>The generation gap:exploring age bias in the underlying value systems of large language models. Siyang Liu, Trish Maturi, Bowen Yi, Siqi Shen, Rada Mihalcea, arXiv:2404.087602024aPreprint</p>
<p>Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions. Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, Qing Wang, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024b</p>
<p>Øyvind Andresen Bjertnaes, Lilja Øvrelid, and Erik Velldal. 2024. It's difficult to be neutral -human and LLM-based sentiment annotation of patient comments. Petter Maehlum, David Samuel, Rebecka Maria Norman, Elma Jelin, Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024. the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024Torino, ItaliaELRA and ICCL</p>
<p>Having beer after prayer? measuring cultural bias in large language models. Tarek Naous, Michael J Ryan, Alan Ritter, Wei Xu, arXiv:2305.144562024Preprint</p>
<p>Role-play zero-shot prompting with large language models for opendomain human-machine conversation. Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre, arXiv:2406.184602024Preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>Apostolos Dedeloudis, Jackson Sargent, and David Jurgens. Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, 10.18653/v1/2022.emnlp-demos.332022</p>            </div>
        </div>

    </div>
</body>
</html>
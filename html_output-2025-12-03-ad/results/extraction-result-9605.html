<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9605 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9605</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9605</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277940307</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.13685v1.pdf" target="_blank">Deep literature reviews: an application of fine-tuned language models to migration research</a></p>
                <p><strong>Paper Abstract:</strong> This paper presents a hybrid framework for literature reviews that augments traditional bibliometric methods with large language models (LLMs). By fine-tuning open-source LLMs, our approach enables scalable extraction of qualitative insights from large volumes of research content, enhancing both the breadth and depth of knowledge synthesis. To improve annotation efficiency and consistency, we introduce an error-focused validation process in which LLMs generate initial labels and human reviewers correct misclassifications. Applying this framework to over 20000 scientific articles about human migration, we demonstrate that a domain-adapted LLM can serve as a"specialist"model - capable of accurately selecting relevant studies, detecting emerging trends, and identifying critical research gaps. Notably, the LLM-assisted review reveals a growing scholarly interest in climate-induced migration. However, existing literature disproportionately centers on a narrow set of environmental hazards (e.g., floods, droughts, sea-level rise, and land degradation), while overlooking others that more directly affect human health and well-being, such as air and water pollution or infectious diseases. This imbalance highlights the need for more comprehensive research that goes beyond physical environmental changes to examine their ecological and societal consequences, particularly in shaping migration as an adaptive response. Overall, our proposed framework demonstrates the potential of fine-tuned LLMs to conduct more efficient, consistent, and insightful literature reviews across disciplines, ultimately accelerating knowledge synthesis and scientific discovery.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9605.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9605.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3.2 3B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3.2 3B (Meta) fine-tuned for deep literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 3B-parameter LLM (Llama 3.2) fine-tuned on human-verified labels from Web of Science abstracts to perform structured classification (relevance, methodology, migration drivers, and climatic/environmental hazards) across a corpus of migration literature and to enable large-scale synthesis and trend analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.2 3B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's open-source Llama 3.2 base model (pretrained) with approximately 3 billion parameters, adapted by supervised fine-tuning on a domain-specific labeled dataset derived from article abstracts; used as a classifier producing structured labels rather than free-form summaries. The development pipeline used the base Llama 3.2 to generate initial labels, human verification via rejection (error-focused annotation), and then supervised fine-tuning on verified labels.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A WoS-derived corpus of scholarly articles retrieved with migration/mobility-related keyword queries, filtered to articles (English, up to 2023). Metadata fields used included title and abstract; full texts were not used. From the full retrieval, a random subset was human-verified and used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>22267</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Structured classification of whether articles focus on human migration/mobility, whether they apply quantitative methods, which migration drivers are discussed (multi-label: climate, environmental, conflict, economic, family, political, social, other), and which specific climatic/environmental hazards (41 UNDRR-standardized hazard types) are analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning workflow: (1) base Llama 3.2 generated initial structured labels for each abstract (AI-crowd classification); (2) humans performed error-focused verification by rejecting incorrect LLM labels (leveraging error-salience); (3) verified label set was split into train/test partitions; (4) Llama 3.2 was fine-tuned on the training set to learn the mapping from abstract text to structured labels; (5) model validated on held-out test set using accuracy (for mutually-exclusive tasks) and Jaccard Index (for multi-label tasks); (6) fine-tuned model scaled to classify the entire 22,267-article corpus. The approach emphasizes structured question-answering (label generation) rather than open-ended summarization to simplify validation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured per-article labels (relevance flag, methodological label, multi-label migration drivers, multi-label environmental/hazard labels) plus aggregated outputs: trend counts over time, clustering of hazards (Ward.D2), co-occurrence network analyses, and narrative synthesis of trends and gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"relevance":"yes","methodology":"quantitative","drivers":["economic","conflict"],"hazards":["flood","sea-level rise"]}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human-verified labels served as ground truth (created via error-focused rejection). For mutually-exclusive tasks (Questions 1 & 2) evaluation used Accuracy; for multi-label tasks (Questions 3 & 4) evaluation used the Jaccard Index. Performance assessed on held-out test sets and examined as a function of training sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Fine-tuning increased accuracy by ~10–25 percentage points relative to initial base-model labels. Binary tasks achieved stable performance, with >80% accuracy attainable with fewer than ~3,000 training samples. Multi-label tasks required more data: the environmental/hazard task (Question 4) needed >5,000 training samples to approach 80% Jaccard; the migration-drivers multi-label task (Question 3) remained low-performing regardless of sample size. Initial (pre-fine-tune) labels from base Llama 3.2 showed low accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable classification across >22k articles; improves relevance over keyword searches (filtered out 13% non-human-migration results); produces consistent structured labels enabling quantitative and qualitative synthesis; open-source model improves transparency and reproducibility versus closed proprietary models; error-focused human verification reduces annotation load and inter-coder disagreement; domain-adapted fine-tuned model can act as a 'specialist' for dynamic literature monitoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training and inference used abstracts only (no full-text), limiting depth of semantic capture; multi-label and conceptually fuzzy categories (e.g., migration drivers) are hard for the model and require large labeled datasets; initial base-model misclassifications were common; general LLM issues (algorithmic bias, transparency, reproducibility) are acknowledged; model performance depends on label clarity — implicitly discussed drivers are frequently missed or mis-assigned.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High initial misclassification rates from the base model; 13% of retrieved keyword-matched articles were non-human migration and required filtering; migration-driver multi-label classification (Question 3) remained low even with larger training samples — reflecting difficulty inferring implicit drivers from abstracts; multi-label hazard classification required >5,000 examples to approach 80% Jaccard; implicit or vaguely phrased signals in abstracts caused systematic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep literature reviews: an application of fine-tuned language models to migration research', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-source llms for text annotation: a practical guide for model setting and fine-tuning <em>(Rating: 2)</em></li>
                <li>Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification <em>(Rating: 2)</em></li>
                <li>Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research <em>(Rating: 2)</em></li>
                <li>ChatGPT outperforms crowd workers for text-annotation tasks <em>(Rating: 2)</em></li>
                <li>Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9605",
    "paper_id": "paper-277940307",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Llama 3.2 3B (fine-tuned)",
            "name_full": "Llama 3.2 3B (Meta) fine-tuned for deep literature reviews",
            "brief_description": "An open-source 3B-parameter LLM (Llama 3.2) fine-tuned on human-verified labels from Web of Science abstracts to perform structured classification (relevance, methodology, migration drivers, and climatic/environmental hazards) across a corpus of migration literature and to enable large-scale synthesis and trend analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3.2 3B (fine-tuned)",
            "model_description": "Meta's open-source Llama 3.2 base model (pretrained) with approximately 3 billion parameters, adapted by supervised fine-tuning on a domain-specific labeled dataset derived from article abstracts; used as a classifier producing structured labels rather than free-form summaries. The development pipeline used the base Llama 3.2 to generate initial labels, human verification via rejection (error-focused annotation), and then supervised fine-tuning on verified labels.",
            "model_size": "3B",
            "input_corpus_description": "A WoS-derived corpus of scholarly articles retrieved with migration/mobility-related keyword queries, filtered to articles (English, up to 2023). Metadata fields used included title and abstract; full texts were not used. From the full retrieval, a random subset was human-verified and used for fine-tuning.",
            "input_corpus_size": 22267,
            "topic_query_description": "Structured classification of whether articles focus on human migration/mobility, whether they apply quantitative methods, which migration drivers are discussed (multi-label: climate, environmental, conflict, economic, family, political, social, other), and which specific climatic/environmental hazards (41 UNDRR-standardized hazard types) are analyzed.",
            "distillation_method": "Supervised fine-tuning workflow: (1) base Llama 3.2 generated initial structured labels for each abstract (AI-crowd classification); (2) humans performed error-focused verification by rejecting incorrect LLM labels (leveraging error-salience); (3) verified label set was split into train/test partitions; (4) Llama 3.2 was fine-tuned on the training set to learn the mapping from abstract text to structured labels; (5) model validated on held-out test set using accuracy (for mutually-exclusive tasks) and Jaccard Index (for multi-label tasks); (6) fine-tuned model scaled to classify the entire 22,267-article corpus. The approach emphasizes structured question-answering (label generation) rather than open-ended summarization to simplify validation.",
            "output_type": "Structured per-article labels (relevance flag, methodological label, multi-label migration drivers, multi-label environmental/hazard labels) plus aggregated outputs: trend counts over time, clustering of hazards (Ward.D2), co-occurrence network analyses, and narrative synthesis of trends and gaps.",
            "output_example": "{\"relevance\":\"yes\",\"methodology\":\"quantitative\",\"drivers\":[\"economic\",\"conflict\"],\"hazards\":[\"flood\",\"sea-level rise\"]}",
            "evaluation_method": "Human-verified labels served as ground truth (created via error-focused rejection). For mutually-exclusive tasks (Questions 1 & 2) evaluation used Accuracy; for multi-label tasks (Questions 3 & 4) evaluation used the Jaccard Index. Performance assessed on held-out test sets and examined as a function of training sample size.",
            "evaluation_results": "Fine-tuning increased accuracy by ~10–25 percentage points relative to initial base-model labels. Binary tasks achieved stable performance, with &gt;80% accuracy attainable with fewer than ~3,000 training samples. Multi-label tasks required more data: the environmental/hazard task (Question 4) needed &gt;5,000 training samples to approach 80% Jaccard; the migration-drivers multi-label task (Question 3) remained low-performing regardless of sample size. Initial (pre-fine-tune) labels from base Llama 3.2 showed low accuracy.",
            "strengths": "Scalable classification across &gt;22k articles; improves relevance over keyword searches (filtered out 13% non-human-migration results); produces consistent structured labels enabling quantitative and qualitative synthesis; open-source model improves transparency and reproducibility versus closed proprietary models; error-focused human verification reduces annotation load and inter-coder disagreement; domain-adapted fine-tuned model can act as a 'specialist' for dynamic literature monitoring.",
            "limitations": "Training and inference used abstracts only (no full-text), limiting depth of semantic capture; multi-label and conceptually fuzzy categories (e.g., migration drivers) are hard for the model and require large labeled datasets; initial base-model misclassifications were common; general LLM issues (algorithmic bias, transparency, reproducibility) are acknowledged; model performance depends on label clarity — implicitly discussed drivers are frequently missed or mis-assigned.",
            "failure_cases": "High initial misclassification rates from the base model; 13% of retrieved keyword-matched articles were non-human migration and required filtering; migration-driver multi-label classification (Question 3) remained low even with larger training samples — reflecting difficulty inferring implicit drivers from abstracts; multi-label hazard classification required &gt;5,000 examples to approach 80% Jaccard; implicit or vaguely phrased signals in abstracts caused systematic errors.",
            "uuid": "e9605.0",
            "source_info": {
                "paper_title": "Deep literature reviews: an application of fine-tuned language models to migration research",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-source llms for text annotation: a practical guide for model setting and fine-tuning",
            "rating": 2,
            "sanitized_title": "opensource_llms_for_text_annotation_a_practical_guide_for_model_setting_and_finetuning"
        },
        {
            "paper_title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification",
            "rating": 2,
            "sanitized_title": "finetuned_small_llms_still_significantly_outperform_zeroshot_generative_ai_models_in_text_classification"
        },
        {
            "paper_title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research",
            "rating": 2,
            "sanitized_title": "rethinking_scale_the_efficacy_of_finetuned_opensource_llms_in_largescale_reproducible_social_science_research"
        },
        {
            "paper_title": "ChatGPT outperforms crowd workers for text-annotation tasks",
            "rating": 2,
            "sanitized_title": "chatgpt_outperforms_crowd_workers_for_textannotation_tasks"
        },
        {
            "paper_title": "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations",
            "rating": 1,
            "sanitized_title": "fine_tuning_llm_for_enterprise_practical_guidelines_and_recommendations"
        }
    ],
    "cost": 0.008855,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep literature reviews: an application of fine-tuned language models to migration research
17 Apr 2025</p>
<p>Stefano M Iacus 
Harvard University
USA</p>
<p>Haodong Qi haodong.qi@mau.se 
Malmö University
Sweden</p>
<p>Stockholm University Demography Unit
Sweden</p>
<p>Jiyoung Han 
Malmö University
Sweden</p>
<p>Deep literature reviews: an application of fine-tuned language models to migration research
17 Apr 20254E45A88150371604984C9FE1591A30FFarXiv:2504.13685v1[cs.CL]Large Language Models (LLMs)Literature ReviewsBibliometric AnalysisFine-TuningClimate and EnvironmentHuman Migration
This paper presents a hybrid framework for literature reviews that augments traditional bibliometric methods with large language models (LLMs).By fine-tuning open-source LLMs, our approach enables scalable extraction of qualitative insights from large volumes of research, enhancing both the breadth and depth of knowledge synthesis.To improve annotation efficiency and consistency, we introduce an error-focused validation process in which LLMs generate initial labels and human reviewers correct misclassifications.Applying this framework to over 20,000 scientific articles on human migration, we demonstrate that a domain-adapted LLM can serve as a "specialist" model -capable of accurately selecting relevant studies, detecting emerging trends, and identifying critical research gaps.Notably, the LLM-assisted review reveals a growing scholarly interest in climate-induced migration.However, existing literature disproportionately centers on a narrow set of environmental hazards (e.g., floods, droughts, sea-level rise, and land degradation), while overlooking others that more directly affect human health and well-being, such as air and water pollution or infectious diseases.This imbalance highlights the need for more comprehensive research that goes beyond physical environmental changes to examine their ecological and societal consequences, particularly in shaping migration as an adaptive response.Overall, our proposed framework demonstrates the potential of fine-tuned LLMs to conduct more efficient, consistent, and insightful literature reviews across disciplines, ultimately accelerating knowledge synthesis and scientific discovery.</p>
<p>Introduction</p>
<p>Reviewing the state-of-the-art knowledge is a cornerstone of scientific research, essential for mapping frontiers, identifying gaps, and shaping research agendas.Traditionally, literature reviews have been labor intensive, requiring significant time and effort to analyze vast amounts of scholarly text.The exponential growth of academic publishing has only intensified this challenge, underscoring the need for more efficient and scalable approaches.To address this, we introduce a hybrid framework that enables researchers to conduct literature reviews which are both large-scale and in-depth.By fine-tuning large language models (LLMs), our approach integrates bibliometric analysis with qualitative literature review, enhancing both the breadth and depth of knowledge synthesis.</p>
<p>Bibliometric analysis provides a structured quantitative method for exploring research trends, identifying knowledge gaps, and analyzing collaboration networks by leveraging metadata such as citations, authorships, and keywords [1].It has also become a valuable tool for mapping the mobility patterns of academic scholars [2].However, while effective for large-scale analysis, bibliometric methods fail to capture the deep semantic meaning embedded in research content.This limitation extends to the identification of relevant works, as keyword-based queries often yield incomplete or inaccurate search results.In contrast, qualitative literature reviews offer in-depth and critical analyses, facilitating a nuanced understanding of theories, concepts, and research findings.However, they are time-consuming, labor-intensive, and impractical for exhaustive reviews.In this article, we demonstrate how fine-tuning LLMs can bridge these two approaches, generating deeper insights from large volumes of academic literature.</p>
<p>The advent of LLMs has not only transformed the field of natural language processing (NLP) but also opened new frontiers in research and industry.LLM-based applications have already demonstrated their potential in fields such as clinical decision-making [3], financial analysis [4], and legal bias detection [5].However, their application in literature reviews (particularly in integrating qualitative insights with quantitative bibliometric analysis) remains underexplored.This article addresses this gap by introducing a scalable approach that leverages LLMs for deep and systematic synthesis of knowledge.By bridging computational efficiency with nuanced content analysis, our method enhances the extraction of meaningful insights from large-scale academic literature, ultimately accelerating scientific discovery.</p>
<p>Despite their transformative potential, LLMs present critical challenges, including algorithmic bias, transparency, and reproducibility concerns [6,7,8,9].These issues are particularly pronounced in proprietary (closed-source) models, such as those developed by OpenAI.While proprietary LLMs are often at the cutting edge of performance, their generalization capabilities may not translate optimally to specialized domains, as they are trained on general text data.To ensure greater transparency, reproducibility, and customization, this article focuses on fine-tuning open-source models rather than relying on proprietary alternatives.</p>
<p>Fine-tuning (FT) is the process of adapting pre-trained LLMs to specific appli-cations using a smaller, domain-specific dataset, rather than training a model from scratch.This technique not only reduces computational costs but also enhances the model's relevance for specialized tasks [10].While fine-tuned LLMs receive less public attention and investment compared to large proprietary models, they have demonstrated capabilities that match or even surpass them in certain domains [11,12,13].In the context of literature reviews, fine-tuning offers significant advantages: it enables LLMs to better understand domain-specific terminology and conceptual nuances, improving their ability to retrieve relevant scientific works, classify topics, and summarize research content.Furthermore, it bridges the gap between qualitative and computational methods, allowing researchers to extract deep insights from large-scale publication databases that would otherwise be infeasible to analyze manually.</p>
<p>Fine-tuning LLMs requires high-quality human-annotated data for training and validation.Traditionally, researchers recruit and train human coders (e.g., research assistants) to annotate documents and generate labeled data for model training and validation.However, this approach is costly and time-consuming.Alternatively, researchers can rely on crowd-sourcing platforms such as MTurk, CrowdFlower, and Figure Eight.However, recent evidence suggested that annotation quality from crowd workers has declined [14] and their agreement with expert annotations can be low [15].Inter-coder disagreement poses another challenge in building goldstandard training datasets, often arising from rigid codebooks, coder fatigue, and cognitive biases [12].The extent of disagreement varies, trained annotators may have a disagreement rate of over 20%, while MTurk crowd workers can reach 44% [16].</p>
<p>To mitigate these issues, this article introduces a novel error-focused annotation approach.Instead of asking annotators to select correct labels from predefined options, we ask them to reject incorrect predictions made by LLMs.This method leverages human sensitivity to errors over correctness, a well-documented phenomenon known as error salience [17] or cognitive and negativity biases [18,19,20] meaning that people are more attuned to spotting mistakes than confirming correctness.By focusing on identifying inaccuracies, this approach improves annotation reliability, reduces inter-coder disagreement, and enhances the overall quality of training data which is necessary for developing robust fine-tuned LLMs in literature review applications.</p>
<p>A framework of deep literature reviews using LLMs Figure 1 illustrates the workflow of LLM-assisted literature reviews.This framework can be adapted to various tasks that extract qualitative insights from scientific works, such as summarizing articles or generating unstructured responses to research-related questions.However, evaluating the quality of generated summaries or open-ended answers requires significant human effort.To alleviate this burden, our framework prioritizes structured question answering, where LLMs generate labels that best describe an article's content.For example, in the use case presented in the next section, we instructed LLMs to classify whether an article focuses on human</p>
<p>New Articles</p>
<p>New FT LLMs labels</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3</p>
<p>Step 4</p>
<p>Step 5</p>
<p>Step 6</p>
<p>Figure 1: Framework of deep literature reviews using fine-tuned LLMs migration and mobility.Additionally, we asked LLMs to generate labels identifying the methodological approaches used, the migration drivers discussed, and the specific climatic or environmental hazards examined in each study.This structured approach streamlines verification, improves accuracy, and enhances the efficiency of literature analysis.</p>
<p>Compared to traditional bibliometric analysis, our approach offers two key advantages.First, a fine-tuned LLM classifier improves the relevance of search results by filtering articles based on specific topics more effectively.For example, in the use case below, we demonstrate that LLM-based searches can more accurately identify articles related to human migration.In contrast, traditional keyword-based queries often retrieve irrelevant results, such as studies on animal and bird migration.Second, our approach enhances the depth of literature reviews.Traditional bibliometric analysis primarily relies on structured metadata, such as authorship, cross-citations, affiliations, and countries.In contrast, our method extracts deeper insights from the unstructured textual content of research articles, enabling a more comprehensive understanding of the literature.</p>
<p>The proposed framework for deep literature reviews consists of six sequential steps:</p>
<p>Step 1: Data Processing.Literature is retrieved from scientific databases (e.g., Web of Science, Scopus) and filtered using metadata such as publication type, year, and academic field.The retrieved documents are then structured into a tabular format with key identifiers (DOI), text fields (title, abstract), and metadata (authors, affiliations, citations).A small random sample is selected for fine-tuning, while the full dataset is preserved for large-scale classification in Step 6.</p>
<p>Step 2: AI-Crowd Classification.Researchers define classification questions in advance based on their specific interests.For example, in the use case below, we asked: "Is an article about human migration and mobility?"and "What migration drivers does each article discuss?" Based on these predefined questions, researchers can use one or more open-source LLMs to generate initial labels (i.e., structured answers).Depending on the classification questions, these labels may be mutually exclusive (assigning only one label per article) or inclusive (allowing multiple labels per article).</p>
<p>Step 3: Human Verification.Human coders (experts and research assistants) review LLM-generated initial labels, rejecting misclassifications.Multiple coders may assess the same abstracts to evaluate inter-coder agreement.The verified labels are compared against AI predictions to evaluate the accuracy of initial classification.</p>
<p>Step 4: Fine-tuning.Verified labels in Step 3 are split into training and testing sets.The training set will be used to fine-tune open-source LLMs, enhancing their classification performance.</p>
<p>Step 5: Validation.The fine-tuned models classify articles in the testing set, and their outputs are validated against human-verified labels to assess performance.</p>
<p>Step 6: Scaling.Once validated, the fine-tuned models are deployed to classify the full dataset, as well as newly published literature, generating structured labels for further analysis.</p>
<p>When validating model performance, the choice of evaluation metrics depends on the number of candidate labels and whether they are mutually exclusive.For mutually exclusive classification, standard metrics such as Balanced Accuracy, Precision, Sensitivity, Specificity, and F1 Score can be applied to assess the model's ability to correctly classify instances into a single category.In cases where an article can be assigned multiple labels, more advanced metrics are required.Commonly used measures include Hamming Loss [21], which quantifies the proportion of incorrect label assignments, and the Jaccard Index [22], which evaluates the similarity between predicted and true label sets.</p>
<p>An application to migration and mobility studies</p>
<p>Here, we demonstrate the application of our deep literature review framework in the field of human migration and mobility studies.Specifically, we retrieved 22,267 published articles from Web of Science (WoS) containing keywords semantically linked to migration and mobility.We then instruct the LLM to perform four classification tasks :</p>
<p>• Relevance Classification: Determining whether an article is actually about human migration and mobility.This step helps assess the accuracy of keywordbased search queries in WoS, identifying potential irrelevant results.</p>
<p>• Methodological Classification: Identifying whether a study employs quantitative methods in its analysis.</p>
<p>• Migration Drivers: Extracting labels indicating the migration driver(s) discussed in each article.</p>
<p>• Climate/Environmental Hazards: Identifying which climate or environmental hazards are analyzed.</p>
<p>It is important to note that the labels in the first two classification tasks are mutually exclusive, we therefore apply the standard accuracy metric to evaluate the performance of LLM fine-tuning.The latter content-related questions are multilabel classifications, thereby we quantify the proportion of correct label assignments using the Jaccard Index [22].</p>
<p>For this analysis, we randomly draw 5,545 out of 22,267 retrieved articles and apply the framework depicted in Figure 1 to fine-tune the Llama 3.2 3B model, an open-source LLM with 3 billion parameters developed by Meta.While it is possible to manually download each article's full-text, the amount of human efforts required will be immense.To ease such a burden, we fine-tune the model based on articles' abstracts which are provided in WoS's metadata.</p>
<p>Fine-tuning performance</p>
<p>The performance of the original Llama 3.2 3B model and its fine-tuned version is illustrated in Figure 2. The data points marked "initial label" in Figure 2 correspond to Step 2 in our framework -AI-crowding classification using the original Llama 3.2 3B model.As expected, when comparing these initial classifications with human verified labels, the accuracy is low.However, after fine-tuning, the model's performance improves significantly; as shown by the circles in Figure 2, accuracy increases by 10-25 percentage points for both training and testing datasets.</p>
<p>We also examined how training sample size influences fine-tuning performance.The results suggest that binary classifications (i.e., Questions 1 and 2) achieve stable accuracy with relatively small training samples, whereas multi-label classification tasks (Questions 3 and 4) show a gradual increase in accuracy with larger training datasets.This pattern implies that the amount of training data required for effective fine-tuning depends on the complexity of the classification task.Specifically, binary classification tasks (e.g., identifying whether an article discusses migration or uses quantitative methods) can achieve 80%+ accuracy with fewer than 3,000 training samples.In contrast, multi-label classification tasks (e.g., identifying migration drivers and environmental hazards) require larger datasets to achieve similar accuracy.For example, Question 4 needs over 5,000 training samples to reach 80% accuracy, while Question 3 remains consistently low-performing, regardless of sample size.</p>
<p>This variation in accuracy could also be influenced by the complexity of the questions themselves.For simpler classifications, such as identifying whether an article employs quantitative methods, the model performs well because abstracts often explicitly mention statistical analysis, data sources, sampling frameworks, or empirical estimates.By contrast, for more complex classifications, such as identifying migration drivers, the model struggles.This is likely due to the inherent difficulty of understanding migration drivers -even for human experts in the field of migration studies; the factors driving migration are typically non-categorical, vaguely defined, and dynamic over time [23,24,25].As a result, many articles refer to these drivers implicitly, using terms such as cross-country wages differences, poverty, marriage, wars and conflict, making it challenging for LLMs to establish clear semantic links between text and predefined labels.</p>
<p>Scaling using the fine-tuned LLM</p>
<p>As shown in the bottom-right of Figure 1, once the LLM is fine-tuned, we proceed to the final step -scaling.In this step, we use the fine-tuned model to classify the research content of all 22,267 abstracts retrieved from WoS.In practice, this finetuned LLM can be applied to newly published scientific work on human migration, hence functioning as a tool for dynamic analysis of the state-of-the-art literature.</p>
<p>To demonstrate the effectiveness of our approach, we instructed the fine-tuned LLM to classify each of the 22,267 articles in terms of whether they were truly about human migration and mobility.As discussed earlier, traditional bibliographic databases such as WoS and Scopus rely on keyword-based search functions, which do not account for research context.Consequently, of the 22,267 articles retrieved using migration-related keywords, 2,963 articles (13%) were actually about migratory patterns of animals, birds, and genes rather than human migration.These irrelevant articles are labeled "no" in Figure 3 (panel a).After filtering out nonhuman migration articles, we analyze the remaining 19,304 articles based on three key dimensions: methodological approaches (qunatitative vs. qualitative), migration drivers, and climatic/environmental hazards.The results are present in Figure 3 (panel b).</p>
<p>The left side of panel b illustrates the number of studies that applied qualitative vs. quantitative methods.While both types of research have grown exponentially overtime, there is a noticeable lag in the expansion of quantitative migration studies.This delay is unsurprising, as macro-and micro-level data on international migration flows and migrant populations have historically been scarce, making it challenging for researchers to conduct robust quantitative analyses.Moreover, while advances in big data and computational methods have transformed fields like biology and physics, the emergence of data-driven social science has been much slower [26].This slow progress is even more pronounced in migration research, where quantitative studies only began to catch up with qualitative research around 2020.</p>
<p>The middle section of panel b illustrates the prevalence of different migration drivers in the literature, including socio-political factors, conflict, environmental/climate factors, economic drivers, family-related reasons, and other factors.It is important to note that, since Question 3 is a multi-label classification problem, an article can be counted more than once if it discussed multiple drivers.The results indicate that social and political factors have been the focal points of migration research.This is not unexpected because cross-border migration is a highly politicized issue with major societal and policy implications for both origin and destination countries.</p>
<p>Although conflict is closely linked to social and political conditions, its role in migration has been less frequently studied in earlier literature.However, since the mid-2010s, research on conflict-induced migration has grown rapidly.This surge in interest was likely driven by the Syrian civil war and the resulting "refugee crisis" in Europe (2015-2016).</p>
<p>The volume of studies examining environmental and climatic impacts on migration has also increased exponentially.This trend aligns with global discourse on climate change and human displacement.As early as 1990, the Intergovernmental Panel on Climate Change (IPCC) warned that climate change could have a major impact on migration.Since then, scholarly interest in climate migrants (or "climate refugees") has steadily increased.However, despite this growing interest, most studies do not explicitly mention specific environmental hazards.Instead, climate change is often discussed as a broad, composite factor shaping human mobility.This pattern is evident in the right side of panel b, where the majority of studies did not specify particular climatic hazards.</p>
<p>To gain a clearer picture of which environmental hazards have been explicitly discussed in migration literature, we examined a subset of 1,350 articles that specifically addressed climate-or environmental-related drivers.Using Ward.D2 clustering, we identified the most commonly discussed hazards: air pollution, sea level rise, flood, and drought (see Figure 4).Moreover, since late-2010s, increasing attention has also been given to costal erosion, heat wave, hurricane, infectious diseases.</p>
<p>To further investigate how different hazards are jointly discussed in the migration literature, we conducted a network analysis.Figure 5 illustrates the extent to which various hazards co-occur within a given article.Each color represents a cluster of hazards that are more likely to appear together.Node size indicates centrality, which measures the relative structural importance of each hazard across the network [27]; in other words, it reflects how strongly a hazard is connected to others.The width of the edges denotes the degree of co-occurrence between pairs of hazards.</p>
<p>It is evident that the network is dynamic and expands over time.An interesting pattern emerges: the network initially comprises a small set of nodes predominantly focused on hydro-climatic hazards but gradually evolves through linkages with a broader array of environmental factors.Particularly in the 1990s and early 2000s, articles tended to concentrate on wet conditions such as flooding, sea-level rise, and coastal erosion.Since 2006, the connection between hydro-and dry-climatic hazards -especially between floods and droughts -has become more prominent.This linkage has facilitated the network's expansion to include additional environmental concerns such as land degradation, desertification, deforestation, and air pollution.</p>
<p>As shown in the final panel of Figure 5, the existing migration literature (published during 1991-2023) increasingly addressed a wide range of interlinked environmental and climatic drivers.Notably, floods and droughts were frequently examined together and often associated with land degradation and sea-level rise.However, despite this expansion, certain hazards -such as biodiversity loss, water and air pollution, and infectious diseases -that may directly affect human health and wellbeing remain weakly connected or entirely absent.This gap underscores the need for further research that moves beyond physical environmental changes to examine their broader ecological and human consequences, particularly in terms of their impact on migration intentions and adaptive responses.</p>
<p>Summary and Discussion</p>
<p>This article introduced a hybrid framework for conducting deep literature reviews by leveraging open-source large language models (LLMs).Our approach enhances traditional bibliometric analysis by incorporating qualitative insights extracted from research content, thereby expanding both the breadth and depth of knowledge synthesis.Unlike proprietary commercial models such as ChatGPT, our framework provides greater transparency and reproducibility through the fine-tuning of opensource LLMs, facilitating open scientific inquiry and ensuring greater control over model behavior.</p>
<p>Additionally, this study introduced a novel approach to combine LLMs with human oversight, overcoming key challenges in traditional annotation tasks.Manual annotation is often labor-intensive, time-consuming, and prone to inter-coder disagreement due to cognitive biases, rigid codebooks, and coder fatigue.These issues are particularly pronounced in high-dimensional classification tasks, such as cate- gorizing environmental hazards into 40+ distinct types.Our framework mitigates these challenges by employing LLMs for initial classification, allowing human reviewers to focus on rejecting misclassifications rather than selecting from predefined categories.This error-focused annotation approach capitalizes on human sensitivity to errors rather than correctness, significantly reducing cognitive load and improving both efficiency and accuracy.</p>
<p>Using migration research as a case study, we demonstrated that a small, opensource LLM can function as "specialist" model after fine-tuning with domain-specific datasets .These fine-tuned models outperform traditional keyword-based search methods (such as those used in Web of Science and Scopus) in identifying relevant academic works.Beyond this enhanced search capabilities, our framework enables structured content-based analysis, providing deeper insights into the landscape of migration research.</p>
<p>Our large-scale review of over 20,000 scientific works revealed a significant gap in migration studies: the limited exploration of climatic and environmental hazards.Despite an exponential increase in research on environmental drivers of migration (see the panel b in Figure 3), the field has yet to systematically examine how different types of hazards influence migration.As illustrated in Figure 4, existing research is disproportionately focused on a narrow set of hazards (such as air pollution, floods, droughts, and sea-level rise), while the human impacts of other critical risks, including desertification, biodiversity loss, landslides, and land degradation, remain largely unexplored.</p>
<p>It is crucial to recognize that migration responses can vary significantly depending on the type of hazard considered [28].These effects can be both positive and negative, potentially balancing out the overall impact of climate change.However, when conclusions are drawn from research that overemphasizes a limited set of hazards, the broader narrative becomes distorted.This selective focus can reinforce misleading claims about mass climate-induced migration, perpetuating the myth of "climate migration" as an impending security crisis [29].</p>
<p>To address this research gap, a more comprehensive and interdisciplinary approach is required.First, data infrastructure must be improved allowing migration scholars to move beyond commonly studied hazards like floods and droughts.This requires high-resolution environmental and migration datasets capable of capturing a wider range of hazards and their long-term effects on human mobility.Second, stronger collaboration among migration researchers, climate scientists, and data analysts is essential for developing integrated models that account for the complexity and heterogeneity of migration responses.Finally, despite its deep roots in the social sciences, migration research should embrace more cutting-edge methodologies such as machine learning, deep learning, and remote sensing to improve the analyses of environmental stressors and their impacts on migraiton.By broadening the scope of analysis and fostering interdisciplinary partnerships, the field can develop a more comprehensive understanding of how migration responds to environmental hazards.</p>
<p>To conclude, this article introduced a novel framework that integrates fine-tuned LLMs with traditional bibliometric analysis, enhancing the efficiency and depth of literature reviews.While we demonstrated its effectiveness in identifying emerging trends and critical gaps in migration research, this approach is highly adaptable and can be applied across various research domains.By enabling scholars to conduct more efficient and insightful literature reviews, we hope this framework will accelerate scientific discovery and foster greater collaboration across disciplines.</p>
<p>Materials and methods</p>
<p>This study utilized data from the Clarivate Web of Science (WoS) database.Our initial search query incorporated a combination of keywords related to human migration and mobility, including <em>human migrat</em>, <em>human mobilit</em>, <em>refugee</em>, <em>asylum</em>, <em>people</em>, <em>population</em>, <em>migrat</em>, <em>mobilit</em>, <em>displac</em>, <em>resettle</em>, <em>reloc</em>, <em>immigra</em>, <em>emigra</em>, <em>migrant</em>, <em>refugee</em>.We further applied the following selection criteria: document type (articles only), publication year (by 2023), and language (English).This yielded a total of 22,267 articles, of which 5,545 articles were randomly selected for fine-tuning.While manually downloading each article's full text is possible, the effort required would be immense.To alleviate this burden, we fine-tune the model using article abstracts, which are readily available in WoS metadata.</p>
<p>Following the workflow shown in Figure 1, we first used the Llama 3.2 3B model (an open-source LLM with 3 billion parameters developed by Meta) to generate initial labels for four questions:</p>
<p>• Is the article about human migration and mobility?</p>
<p>• Did the article apply any quantitative methods in its analysis?</p>
<p>• What migration drivers did the article discuss? (Candidate labels include climate, environmental, conflict, economic, family, political, social, and other).</p>
<p>• What climatic/environmental hazards did the article discuss? (Using 41 standardized hazard classifications from the United Nations Office for Disaster Risk Reduction.).</p>
<p>To improve classification accuracy, the authors reviewed and corrected LLMgenerated labels by rejecting misclassifications.This verified dataset was then split into training and testing sets at various ratios and used to fine-tune the Llama 3.2 3B model.</p>
<p>To assess model performance, we used two evaluation metrics.For mutually exclusive classification problems (Question 1 and 2), we used accuracy (ACC):
ACC = 1 N N i=1 M j=1 I(y ij = ŷij ) (1)
where, N is the total number of articles and M is the number of candidate labels per classification question.y ij is the true label which equals to 1 if article i belongs to class j and otherwise 0. ŷij is the LLM-generated label.I(.) is an indicator function which equals to 1 if y ij = ŷij and otherwise 0.</p>
<p>For multi-label classifications (Question 3 and 4), we used Jaccard Index (JAC),
JAC = 1 N N i=1 ŷi * ∩ y i * ŷi * ∪ y i *(2)
where, y i * = y i1 , y i2 , ..., y iM is a M length true vector containing the human-verified label(s) for article i. ŷi * = y i1 , y i2 , ..., y iL is a L length predicted vector containing LLM-generated label(s).</p>
<p>Since the intersection is always smaller than or equal to the union, the Jaccard Index falls within the range [0, 1].A higher JAC indicates greater classification accuracy, with JAC = 1 denoting perfect agreement and = 0 indicating no overlap between true and predicted labels.</p>
<p>After fine-tuning, we used the Llama 3.2 3B model to infer research content for all 22,267 articles retrieved from WoS. Descriptive analyses were then conducted to examine patterns and trends in the generated labels.</p>
<p>Figure 2 :
2
Figure 2: Model performance measured by accuracy (ACC) for mutually exclusive classifications and by Jaccard Index (JAC) for multi-label classifications.Accuracy is reported for initial labels by the original Llama 3.2 3B model as well as for training and testing labels classified by the fine-tuned Llama mdoel.</p>
<p>Figure 3 :
3
Figure 3: Trends of human migration and mobility studies.Note: the data underlying panel b is a subset of 22,267 articles conditional on articles that is about human migration and mobility.</p>
<p>Figure 4 :
4
Figure 4: Climatic/environmental hazards discussed in migration research.Note: the data underlying this figure is a subset of 22,267 articles conditional on i) the articles are about human migration and mobility; and ii) the articles addressed climate/environment as a migration driver.</p>
<p>Figure 5 :
5
Figure 5: The degree to which climatic/environmental hazards discussed jointly in a given article.Note: the data underlying this figure is a subset of 22,267 articles conditional on i) the articles are about human migration and mobility; and ii) the articles addressed climate/environment as a migration driver.</p>
<p>How to conduct a bibliometric analysis: An overview and guidelines. N Donthu, S Kumar, D Mukherjee, N Pandey, W M Lim, Journal of Business Research. 1332021</p>
<p>A gender perspective on the global migration of scholars. X Zhao, A Akbaritabar, R Kashyap, E Zagheni, Proceedings of the National Academy of Sciences. 12010e22146641202023</p>
<p>Evaluation and mitigation of the limitations of large language models in clinical decision-making. P Hager, F Jungmann, R Holland, K Bhagat, I Hubrecht, M Knauer, J Vielhauer, M Makowski, R Braren, G Kaissis, D Rueckert, Nature Medicine. 30July 2024</p>
<p>Text as Data. M Gentzkow, B Kelly, M Taddy, Journal of Economic Literature. 57Sept. 2019</p>
<p>LEGAL-BERT: The Muppets straight out of Law School. I Chalkidis, M Fergadiotis, P Malakasiotis, N Aletras, I Androutsopoulos, Oct. 2020</p>
<p>Perils and opportunities in using large language models in psychological research. S Abdurahman, M Atari, F Karimi-Malekabadi, M J Xue, J Trager, P S Park, P Golazizian, A Omrani, M Dehghani, PNAS Nexus. 3June 2024</p>
<p>Synthetic Replacements for Human Survey Data? The Perils of Large Language Models. J Bisbee, J D Clinton, C Dorff, B Kenkel, J M Larson, Political Analysis. 32May 2024</p>
<p>A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?. J Fields, K Chovanec, P Madiraju, IEEE Access. 122024</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. T Hagendorff, S Fabi, M Kosinski, Nature Computational Science. 3Oct. 2023</p>
<p>Open-source llms for text annotation: a practical guide for model setting and fine-tuning. M Alizadeh, M Kubli, Z Samei, S Dehghani, M Zahedivafa, J D Bermeo, M Korobeynikova, F Gilardi, Journal of Computational Social Science. 812025</p>
<p>Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification. M J J Bucher, M Martini, 2024</p>
<p>Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research. M Carammia, S M Iacus, G Porro, 2024</p>
<p>Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations. J Mathavraj, V Kushala, H Warrier, Y Gupta, abs/2404.10779ArXiv. 2024</p>
<p>An mturk crisis? shifts in data quality and the impact on study results. M Chmielewski, S C Kucker, Social Psychological and Personality Science. 11Oct. 2019</p>
<p>SummEval: Re-evaluating Summarization Evaluation. A R Fabbri, W Kryściński, B Mccann, C Xiong, R Socher, D Radev, July 2020</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, Proceedings of the National Academy of Sciences. 120July 2023</p>
<p>Error Awareness and Salience Processing in the Oddball Task: Shared Neural Mechanisms. H A Harsay, M Spaan, J G Wijnen, K R Ridderinkhof, Frontiers in Human Neuroscience. 62012</p>
<p>The neural basis of human error processing: reinforcement learning, dopamine, and the error-related negativity. C B Holroyd, M G Coles, Psychological review. 10946792002</p>
<p>Negative information weighs more heavily on the brain: The negativity bias in evaluative categorizations. T A Ito, J T Larsen, N K Smith, J T Cacioppo, Journal of Personality and Social Psychology. 7541998</p>
<p>The ecology of human fear: survival optimization and the nervous system. D Mobbs, C C Hagan, T Dalgleish, B Silston, C Pravost, Frontiers in Neuroscience. 9Mar. 2015</p>
<p>Error Detecting and Error Correcting Codes. R W Hamming, Bell System Technical Journal. 29Apr. 1950</p>
<p>Étude comparative de la distribution florale dans une portion des alpes et du jura. Paul Jaccard, 1901</p>
<p>Forecasting asylum-related migration flows with machine learning and data at scale. M Carammia, S M Iacus, T Wilkin, Scientific Reports. 12114572022</p>
<p>Modelling and predicting forced migration. H Qi, T Bircan, Plos one. 184e02844162023</p>
<p>Can google trends predict asylum-seekers' destination choices?. H Qi, T Bircan, EPJ Data Science. 121412023</p>
<p>. D Lazer, A Pentland, L Adamic, S Aral, A.-L Barabási, D Brewer, N Christakis, N Contractor, J Fowler, M Gutmann, Computational social science. 32359152009Science</p>
<p>Network analysis: a brief overview and tutorial. D H , 34040834Health Psychology and Behavioral Medicine. 612018</p>
<p>A meta-analysis of country-level studies on environmental change and migration. R Hoffmann, A Dimitrova, R Muttarak, J Crespo Cuaresma, J Peisker, Nature Climate Change. 10102020</p>
<p>Climate migration myths. I Boas, C Farbotko, H Adams, H Sterly, S Bush, K Van Der Geest, H Wiegel, H Ashraf, A Baldwin, G Bettini, Nature Climate Change. 9122019</p>            </div>
        </div>

    </div>
</body>
</html>
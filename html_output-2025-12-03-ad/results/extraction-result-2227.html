<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2227 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2227</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2227</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-279164961</p>
                <p><strong>Paper Title:</strong> DAGSLAM: causal Bayesian network structure learning of mixed type data and its application in identifying disease risk factors</p>
                <p><strong>Paper Abstract:</strong> Background Identifying and understanding disease risk factors is crucial in epidemiology, particularly for chronic and noncommunicable diseases that often have complex interrelationships. Traditional statistical methods struggle to capture these complexities, necessitating more sophisticated analytical frameworks. Bayesian networks and directed acyclic graphs (DAGs) provide powerful tools for exploring the complex relationships between variables. However, existing DAG structure learning algorithms still have limitations in handling mixed-type data (including continuous and discrete variables), which restricts their practical utility. Therefore, developing DAG structure learning methods that can effectively handle mixed data is highly important for obtaining an in-depth understanding of disease risk factors and pathogenic mechanisms. Methods This study proposes an extension of the NOTEARS algorithm, termed DAGSLAM, which is designed for Bayesian network structure learning with mixed-type data. The algorithm integrates continuous and categorical variables through a tailored loss function, enhancing its applicability to real-world epidemiological datasets. Results Extensive simulations were conducted across eight distinct scenarios, specifically, variations in the number of nodes, changes in the proportion of categorical nodes, different sample sizes, levels of categorical nodes, variations in edge sparsity, adjustments to the weight scale, different graph types, and diverse noise distributions. These scenarios demonstrate that DAGSLAM consistently outperforms existing methods such as HC, TABU, mDAG, and DAGBagM across key metrics, including precision, recall, F1 score, and structural Hamming distance (SHD). Furthermore, the robustness of DAGSLAM is validated through its application to the National Health and Nutrition Examination Survey (NHANES) dataset, revealing critical causal relationships among risk factors for CHD and diabetes. Conclusions DAGSLAM provides a powerful and scalable tool for uncovering causal relationships in complex disease networks, with significant implications for risk factor identification and public health research. Supplementary Information The online version contains supplementary material available at 10.1186/s12874-025-02582-6.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2227.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2227.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGSLAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed Acyclic Graphs Structure learning via Log-determinant and Augmented lagrangian for Mixed type data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-based extension of NOTEARS that learns DAG structure from mixed-type (continuous + categorical) data by combining least-squares and (multi)logistic loss terms with a log-determinant acyclicity constraint and augmented Lagrangian optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAGSLAM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Optimizes a mixed-data loss L(W) (sum of LS for continuous nodes and cross-entropy for categorical nodes) plus L1 sparsity under an acyclicity constraint h(W) = -logdet(sI - W∘W) + d log s = 0; solved via augmented Lagrangian with weight thresholding (ω) and L1 regularization (λ). Designed to handle mixed continuous/categorical variables in linear SEMs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / epidemiology (risk-factor networks)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Optimization objective / surrogate loss and graph recovery metrics (L(W) loss, directed F1, SHD, FDR, TPR, FPR)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Primary internal surrogate objective is the mixed-type loss L(W) (least-squares for continuous nodes, cross-entropy for binary/multinomial nodes) plus acyclicity penalty and L1 regularizer; external proxy evaluation metrics used in experiments are directed F1 score, structural Hamming distance (SHD), false discovery rate (FDR), true positive rate (TPR) and false positive rate (FPR), computed against known simulated DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction / empirical surrogate (surrogate optimization objective + evaluation metrics computed against simulated ground-truth)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG edges (true adjacency matrix) and NHANES observational outcomes for real-data case study</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>In simulations the ground truth is the true DAG adjacency matrix used to generate data via linear SEM for continuous nodes and logistic/multinomial models for categorical nodes (noise: Gaussian, exponential, or Gumbel); in the NHANES application there is no experimental ground-truth - the graph is learned from observational survey data (no wet-lab validation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Reported differences between proxy evaluation metrics and ground truth are quantified using SHD and directed F1/FDR/TPR; representative numbers: at d=10 directed F1=0.95 and SHD=1.0; at d=20 DAGSLAM outperforms NOTEARS/DAGMA (example: NOTEARS F1≈0.85, DAGMA F1≈0.86, SHD≈5.1 and 4.0 vs DAGSLAM near 1 / SHD≈0 in some scenarios). With 50% categorical nodes DAGSLAM F1 drops from 0.97 to 0.82 (while NOTEARS and DAGMA drop to ~0.71 and ~0.60).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>High on simulated proxy metrics: e.g., directed F1 up to ~0.95–1.00 across many simulation settings; at small sample n=100 directed F1≈0.94 (SHD≈2.3); for large n (≥5000) F1 approaches ~1.00 and SHD near 0 (averaged over 10 replicates).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same numeric comparisons because ground truth is known for simulations — reported directed TPR and FDR values consistently high/low respectively (examples: directed FDR often 0.00 in favorable settings; TPR up to 0.98–0.99 in many scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Indirectly reported via FDR/FPR; example values: directed FDR frequently 0.00 in easy settings; some scenarios report directed FDR 0.00 and FPR ~0.00–0.03; in harder settings FDR up to 0.24 (see Table 2 scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Can be inferred from TPR; example TPR values range from ~0.74 to 0.99 depending on scenario (so false negative rate = 1 - TPR ranges from ~0.01 to ~0.26 in reported scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Mostly in-distribution / controlled-simulation (models evaluated on data generated by the same SEM family); real-data application is observational and exploratory (not experimental validation).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance gap (proxy vs ground-truth recovery) increases when data depart from 'easy' simulation conditions: e.g., higher proportion of categorical nodes (50%) reduces DAGSLAM directed F1 from ~0.97→0.82; small sample sizes (n=100) degrade performance (F1~0.94) relative to large n (F1→1.0); non-Gaussian noise and dense graphs also reduce accuracy (reported in supplemental scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Regularization (L1), hard thresholding (ω=0.3), augmented Lagrangian acyclicity penalty with increasing ρ and Lagrange multiplier updates; choice of mixed-type loss instead of treating all nodes as continuous serves to reduce model misspecification for categorical nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Effective in simulations: e.g., using the mixed-type loss results in higher F1 and lower SHD compared to treating all nodes as continuous (NOTEARS/DAGMA) when categorical fraction ≥2.5%; thresholding and L1 reduce false positives (many scenarios show directed FDR≈0).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Computational cost is discussed: DAGSLAM is slower than NOTEARS and DAGMA due to per-column vector operations for mixed-type loss. Example timings: for d=100 (n=1000) DAGSLAM and NOTEARS took ~10^3 s; for n=10000 (d=20) DAGSLAM ≈10^2 s while DAGMA stayed smaller; authors note a trade-off between computation and improved accuracy on mixed data.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: gradient-based continuous optimization methods (NOTEARS family) are recent and improving DAG learning; mixed-type adaptations are new and less mature.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Not explicitly analyzed; multiple metrics (F1, SHD, FDR, TPR, FPR) are reported but correlation/failure-mode dependence is not systematically examined.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Yes — primary cascade: (1) computational learning using surrogate objective L(W) → (2) validation against simulated ground-truth DAG (metrics like F1/SHD) → (3) application to observational NHANES dataset (no experimental ground-truth). Errors are propagated from surrogate optimization to graph estimates and then to real-data causal interpretations, but no experimental validation stage exists.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Identified limitations include higher computational cost for mixed-type losses, linear SEM assumption (no nonlinear interactions), poorer performance in dense graphs (S0/d ≥ 2), sensitivity to high proportion of categorical nodes and certain noise distributions (exponential), and lack of ability to incorporate prior constraints (whitelist/blacklist).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Mixed variable types (continuous vs categorical), number of categorical levels (m), sample size n, edge sparsity S0/d, weight scale α, noise distribution (Gaussian, exponential, Gumbel), and graph topology (ER vs scale-free) strongly affect proxy-to-ground-truth gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2227.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGs with NO TEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pioneering gradient-based method that formulates DAG learning as a continuous optimization problem using a least-squares loss and a smooth acyclicity constraint (matrix exponential) to avoid combinatorial search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGs with NO TEARS: continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Frames linear SEM DAG learning as minimizing least-squares loss + L1 penalty under acyclicity constraint h(W)=tr(e^{W∘W}) - d = 0; solved with augmented Lagrangian and gradient-based optimization. Assumes all variables continuous (or treated as such in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Least-squares surrogate loss and evaluation metrics (directed F1, SHD, FDR, TPR, FPR)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Internal surrogate loss is LS residual norm plus sparsity penalty; external performance assessed via directed F1, SHD, etc. in simulations against known adjacency matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction / empirical surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same simulation protocol as DAGSLAM; NOTEARS was evaluated by treating all variables as continuous in mixed-data simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Representative comparisons: NOTEARS performs comparably to DAGSLAM in purely/mostly continuous settings and high-d scenarios (d=40/100) but worse when categorical fraction increases; example: at d=20 NOTEARS F1≈0.85 vs DAGSLAM ~0.99 (SHD NOTEARS ~5.1 vs DAGSLAM lower). With 50% categorical nodes NOTEARS F1 ~0.71 vs DAGSLAM 0.82.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>High on continuous-only simulations (F1 up to ~0.95 in some settings) and large-sample scenarios (n≥5000 F1~0.85 reported), but degraded with mixed/categorical data.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same as proxy metrics because ground-truth is known in simulations; observed TPR/FDR values vary widely across scenarios (e.g., in some scenarios TPR~0.85, FDR~0.15).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Varies by scenario; examples in table show directed FPR values in range ~0.00–0.1 depending on scenario (worse under misspecification to continuous assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Inferred from reported TPR; e.g., TPR dropping from ~0.98 to ~0.85 in some scenarios yields false negative rate increase from ~0.02→0.15.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution for simulations but exhibits model-misspecification when applied to mixed-type data if variables are treated as continuous.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance worsens when categorical proportion rises or when variables are non-Gaussian; treating categorical nodes as continuous introduces a systematic gap in edge recovery (lower F1, higher SHD).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not in original NOTEARS; in practice other methods (DAGSLAM) reduce gap by using appropriate loss (logistic/multinomial) for categorical nodes. Regularization and thresholding are also used in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Using mixed-type loss (DAGSLAM) demonstrably improves recovery in mixed data (example: F1 improved from ~0.71→0.82 at 50% categorical).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>NOTEARS benefits from matrix operations when variables are continuous; computational time lower than DAGSLAM in mixed-treatment experiments (e.g., both ~10^3 s for d=100 but NOTEARS slightly faster; for n=10000 DAGSLAM ≈10^2 s while DAGMA stable and NOTEARS faster).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established as a strong baseline for gradient-based DAG learning; widely-cited and used.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Not analyzed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational surrogate → comparison to simulated ground-truth → applied to observational data (treated as continuous) without experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Assumes continuous variables (least-squares loss), limited handling of categorical nodes unless variables are coerced to continuous, original acyclicity constraint (matrix exponential) can have worse gradient behavior vs log-det alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Variable type mismatch (categorical vs continuous), noise distribution, sample size, graph sparsity affect proxy-to-ground-truth gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2227.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A NOTEARS-family algorithm that replaces the exponential acyclicity constraint with a log-determinant constraint derived from M-matrix properties, improving gradient behavior and computational stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAGMA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Minimizes a score (e.g., least-squares + sparsity) subject to acyclicity enforced via h(W) = -logdet(sI - W∘W) + d log s; derivative form is simpler and yields more stable optimization; tested in comparisons treating variables as continuous.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Least-squares surrogate loss + log-det acyclicity penalty; evaluated using directed F1, SHD, FDR, TPR, FPR against simulated graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Internal surrogate objective similar to NOTEARS but with log-determinant acyclicity constraint; external recovery metrics computed against simulated ground-truth adjacency matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction / empirical surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same simulation generation as for other methods; DAGMA was evaluated by treating nodes as continuous in mixed-data experiments for fairness of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>DAGMA often performs comparably to NOTEARS and close to DAGSLAM in high-dimensional continuous-like settings; example: at d=20, DAGMA F1≈0.86 and SHD≈4.0 vs DAGSLAM higher (~0.99/low SHD); with high categorical proportion DAGMA performance drops more than DAGSLAM (e.g., at 50% categoricals DAGMA F1≈0.60).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Good in continuous or large-sample settings (F1 up to ~0.95 in some configurations), but degrades when categorical fraction increases and when treating categorical variables as continuous is misspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same as proxy metrics given simulated ground-truth; TPR/FDR vary per scenario (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Scenario-dependent; examples include directed FPR in the low-single-digits in favorable settings and higher under misspecification (see Table 2 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Inferred from TPR (varies across experiments; e.g., TPR values reported in Table 2 range ~0.5–1.0 across scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution in simulations; less robust under variable-type mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance gap increases with categorical proportion and other departures from continuous assumptions; DAGMA shows improvement in some scenarios due to better-conditioned gradients but still suffers from mixed-data misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Log-determinant acyclicity constraint improves optimization stability vs NOTEARS' matrix exponential; nonetheless requires mixed-type loss (as DAGSLAM provides) to reduce model misspecification gap.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Log-det yields better gradient behavior and computational performance (authors report faster/better-behaved gradients), but in mixed-type experiments DAGSLAM's tailored loss gives superior edge-recovery metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>DAGMA often has more moderate computational growth with d and n compared to NOTEARS/DAGSLAM; e.g., DAGMA's time increased by one order of magnitude for d up to 100 while DAGSLAM/NOTEARS increased by two orders in that experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Recent improvement in acyclicity constraints; considered an incremental refinement over NOTEARS.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational surrogate → simulated ground-truth evaluation → observational application (no experimental validation).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Like NOTEARS, assumes continuous variables for LS loss; mixed-type data handling requires extensions; although log-det helps optimization, performance still degrades with categorical data unless loss is changed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Same as NOTEARS: variable types, noise, sparsity, sample size affect fidelity to ground truth.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2227.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAG-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAG-GNN (Directed Acyclic Graph learning with Graph Neural Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonlinear extension of continuous optimization DAG learning that uses variational autoencoders and graph neural networks to capture non-linear dependencies when learning DAG structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAG structure learning with graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAG-GNN</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses deep generative models (VAE) and graph neural network parameterizations to model nonlinear SEMs and learn DAGs via gradient-based optimization with acyclicity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>VAE reconstruction loss and graph recovery metrics (F1, SHD, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Internal proxy is VAE-style ELBO / reconstruction loss; external validation uses simulated ground-truth adjacency matrices and metrics like directed F1 and SHD.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (deep learning surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix (nonlinear SEM in some evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground truth in references and discussions is synthetic graphs with nonlinear data generating mechanisms used to compare structure recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Not quantified numerically in this paper's experiments (DAG-GNN only mentioned in related work); referenced literature reports improved recovery in nonlinear settings vs linear methods but specific numbers are in cited DAG-GNN papers.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Designed for out-of-linear-model distributions (nonlinear, potentially out-of-distribution relative to linear baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Uses neural-network expressivity to reduce model misspecification when data generating process is nonlinear.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Deep models generally have higher computational cost; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; neural approaches for DAG learning are active research.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational (neural surrogate) → simulated ground-truth evaluation; may be applied to observational data thereafter.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Neural methods can be data hungry, harder to interpret, and may not provide calibrated uncertainty for causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Nonlinearity, model capacity, training data size and distribution shift affect proxy-to-ground-truth gap.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2227.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional score-based (HC / TABU)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hill Climbing (HC) and Tabu Search (TABU) score-based DAG learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic combinatorial score-based DAG structure learning methods that search DAG space greedily (HC) or with Tabu lists to optimize a score (e.g., BIC), commonly used baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HC / TABU (score-based search)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Greedy search through DAG space optimizing a predefined score (e.g., BIC, log-likelihood with penalty); HC takes local moves, TABU keeps a tabu list to avoid cycling. Implemented via bnlearn in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Score functions (BIC/log-likelihood) as surrogate objective and graph recovery metrics (F1, SHD, FDR, TPR)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Optimization target is a statistical score (penalized likelihood) rather than direct edge-recovery; evaluation against simulated ground-truth uses directed F1, SHD, FDR, TPR, FPR.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (statistical score) / search-based</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Performance measured by recovery of true directed edges in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Traditional methods perform worse in high-d and mixed-data simulations reported: e.g., at d=40 HC F1 dropped to 0.57, TABU 0.66; SHD increased substantially (e.g., HC SHD ~13.9 at n=100 in one scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Lower than modern gradient-based methods in the paper's simulations: F1 often ~0.5–0.8 depending on scenario; suffers at large d and low n.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Same as proxy metrics compared to simulated ground truth (TPR/FDR reported in Table 2; performance often worse relative to DAGSLAM/NOTEARS).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Often higher than gradient-based methods in reported scenarios; e.g., HC FPR and SHD values in Table 2 show many false positives at low sample sizes/high-d.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Also higher in many scenarios (lower TPR).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution but limited scalability to high-d and mixed-type settings.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Gaps increase with number of nodes and low sample sizes; traditional methods degrade faster than gradient-based alternatives as d grows.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not specifically addressed in this paper; ensemble/bagging approaches (DAGBagM) are attempted to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>DAGBagM and other aggregations show some improvements but still lag behind DAGSLAM in mixed-data simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Classic methods can be faster for small graphs but do not scale well; computational comparisons not the paper's main focus for these methods (authors did not run R implementations for timing comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature classical approaches for DAG learning but limited in scalability and handling mixed types.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Score optimization → comparison to simulated ground-truth → observational application.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scalability, sensitivity to score choice and local optima, difficulty handling mixed-type data without coercion.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Graph size, sample size, variable type heterogeneity and score misspecification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2227.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mDAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixed Directed Acyclic Graph learning (mDAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method tailored to learn DAG structures with mixed node types (continuous and categorical) using approaches designed for mixed data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>mDAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Designed specifically for mixed-type node learning in DAGs; used as a comparison baseline in simulations (implemented in R).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / biostatistics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Method-specific score/loss for mixed data + graph recovery metrics (F1, SHD, FDR, TPR)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Evaluated against known simulated DAGs using directed F1, SHD and other recovery metrics; mDAG’s internal surrogate differs from gradient-based mixed-loss but exact formulation not detailed in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate / domain-specific mixed-data method</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Same simulation protocol; mDAG evaluated on simulation replicates and real-data examples where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>mDAG often underperforms DAGSLAM in many scenarios; example F1 values: at d=40 mDAG F1 ≈0.68 in one scenario while DAGSLAM ≈0.97; in some low-categorical-level settings mDAG close in skeleton metrics but still lower directed accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Variable; sometimes competitive on skeleton metrics but generally lower directed F1 and higher SHD than DAGSLAM in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Measured via the same metrics and shows inferior edge-direction recovery in many experiments (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Scenario-dependent; reported FDRs in Table 2 show higher FDR/FPR in several scenarios relative to DAGSLAM.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Also variable; lower TPR in many scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Designed for mixed-type data (in-distribution for mixed scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance gap widens when categorical levels >2 cannot be handled or when sample sizes are small.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not specified in main text; treated as baseline without the same continuous optimization tools as DAGSLAM.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Less effective than DAGSLAM in the reported simulation suite.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not timed in main text (R implementation); generally performs worse accuracy-wise though computational cost not compared directly in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Specialized method for mixed data, but apparently less robust than DAGSLAM in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational method → validated on simulations → compared on NHANES where possible (but had limitations for >2-level categoricals).</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Some implementations cannot handle categorical nodes with >2 levels (treat as continuous), poorer performance at small n and in certain topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Categorical level count, sample size, and node mix.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2227.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2227.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGBagM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed Acyclic Graph Bagging with Mixed Variables (DAGBagM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation/ensemble hill-climbing approach designed for learning DAGs with mixed variables by bagging multiple DAG estimates to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DAGBagM: learning directed acyclic graphs of mixed variables with an application to identify prognostic protein biomarkers in ovarian cancer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAGBagM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applies non-aggregated hill climbing across bootstrap samples and aggregates results to reduce variance and false positives for mixed-variable DAG learning; included as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>causal structure learning / bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Aggregated score-based surrogate + graph recovery metrics (F1, SHD, FDR, TPR)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Uses bagging ensemble of score-based search results as surrogate for more stable graph recovery; evaluated against simulated ground-truth DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (ensemble of score-based models)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Simulated ground-truth DAG adjacency matrix</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Evaluated in simulations; tends to improve stability but still trails DAGSLAM in many scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>DAGBagM sometimes improves over single HC but still shows lower directed F1 (e.g., F1≈0.63 at n=100 in one scenario vs DAGSLAM 0.94) and higher SHD in several experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Moderate; performs better than single HC in some scenarios but generally inferior to DAGSLAM/NOTEARS for directed edge recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Directly measured against simulated ground truth — lower TPR and higher SHD vs DAGSLAM in many reported scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Ensembling reduces some false positives, but FDR remains higher than DAGSLAM in many experiments (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Higher false negatives in low-sample settings (low TPR).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Ensemble approach aims to improve robustness in-distribution; not designed for extrapolative novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Performance improves with increasing n but still lags behind gradient-based mixed-loss methods; unique trend: DAGBagM improves slowly even when n≥1000.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Bagging/aggregation of HC results across bootstraps.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Some reduction in variance/false positives relative to single HC, but effectiveness limited compared to DAGSLAM (quantified by persistent lower directed F1 and higher SHD).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not timed in main text; R-based ensemble likely more expensive than single HC but still typically less accurate than DAGSLAM.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Practical ensemble method for mixed data; established but not state-of-the-art here.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Ensemble computational approach → validated on simulated ground-truth → observational application.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Lower directed-edge recovery than gradient-based mixed-loss methods; slow improvement with sample size in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Sample size, categorical proportion, and graph sparsity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DAGs with NO TEARS: continuous optimization for structure learning <em>(Rating: 2)</em></li>
                <li>DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization <em>(Rating: 2)</em></li>
                <li>DAG structure learning with graph neural networks <em>(Rating: 2)</em></li>
                <li>Scaling structural learning with NO-BEARS to infer causal transcriptome networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2227",
    "paper_id": "paper-279164961",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "DAGSLAM",
            "name_full": "Directed Acyclic Graphs Structure learning via Log-determinant and Augmented lagrangian for Mixed type data",
            "brief_description": "A gradient-based extension of NOTEARS that learns DAG structure from mixed-type (continuous + categorical) data by combining least-squares and (multi)logistic loss terms with a log-determinant acyclicity constraint and augmented Lagrangian optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DAGSLAM",
            "system_description": "Optimizes a mixed-data loss L(W) (sum of LS for continuous nodes and cross-entropy for categorical nodes) plus L1 sparsity under an acyclicity constraint h(W) = -logdet(sI - W∘W) + d log s = 0; solved via augmented Lagrangian with weight thresholding (ω) and L1 regularization (λ). Designed to handle mixed continuous/categorical variables in linear SEMs.",
            "domain": "causal structure learning / epidemiology (risk-factor networks)",
            "proxy_metric_name": "Optimization objective / surrogate loss and graph recovery metrics (L(W) loss, directed F1, SHD, FDR, TPR, FPR)",
            "proxy_metric_description": "Primary internal surrogate objective is the mixed-type loss L(W) (least-squares for continuous nodes, cross-entropy for binary/multinomial nodes) plus acyclicity penalty and L1 regularizer; external proxy evaluation metrics used in experiments are directed F1 score, structural Hamming distance (SHD), false discovery rate (FDR), true positive rate (TPR) and false positive rate (FPR), computed against known simulated DAGs.",
            "proxy_metric_type": "data-driven ML prediction / empirical surrogate (surrogate optimization objective + evaluation metrics computed against simulated ground-truth)",
            "ground_truth_metric": "Simulated ground-truth DAG edges (true adjacency matrix) and NHANES observational outcomes for real-data case study",
            "ground_truth_description": "In simulations the ground truth is the true DAG adjacency matrix used to generate data via linear SEM for continuous nodes and logistic/multinomial models for categorical nodes (noise: Gaussian, exponential, or Gumbel); in the NHANES application there is no experimental ground-truth - the graph is learned from observational survey data (no wet-lab validation).",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Reported differences between proxy evaluation metrics and ground truth are quantified using SHD and directed F1/FDR/TPR; representative numbers: at d=10 directed F1=0.95 and SHD=1.0; at d=20 DAGSLAM outperforms NOTEARS/DAGMA (example: NOTEARS F1≈0.85, DAGMA F1≈0.86, SHD≈5.1 and 4.0 vs DAGSLAM near 1 / SHD≈0 in some scenarios). With 50% categorical nodes DAGSLAM F1 drops from 0.97 to 0.82 (while NOTEARS and DAGMA drop to ~0.71 and ~0.60).",
            "proxy_performance": "High on simulated proxy metrics: e.g., directed F1 up to ~0.95–1.00 across many simulation settings; at small sample n=100 directed F1≈0.94 (SHD≈2.3); for large n (≥5000) F1 approaches ~1.00 and SHD near 0 (averaged over 10 replicates).",
            "ground_truth_performance": "Same numeric comparisons because ground truth is known for simulations — reported directed TPR and FDR values consistently high/low respectively (examples: directed FDR often 0.00 in favorable settings; TPR up to 0.98–0.99 in many scenarios).",
            "false_positive_rate": "Indirectly reported via FDR/FPR; example values: directed FDR frequently 0.00 in easy settings; some scenarios report directed FDR 0.00 and FPR ~0.00–0.03; in harder settings FDR up to 0.24 (see Table 2 scenarios).",
            "false_negative_rate": "Can be inferred from TPR; example TPR values range from ~0.74 to 0.99 depending on scenario (so false negative rate = 1 - TPR ranges from ~0.01 to ~0.26 in reported scenarios).",
            "novelty_characterization": "Mostly in-distribution / controlled-simulation (models evaluated on data generated by the same SEM family); real-data application is observational and exploratory (not experimental validation).",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance gap (proxy vs ground-truth recovery) increases when data depart from 'easy' simulation conditions: e.g., higher proportion of categorical nodes (50%) reduces DAGSLAM directed F1 from ~0.97→0.82; small sample sizes (n=100) degrade performance (F1~0.94) relative to large n (F1→1.0); non-Gaussian noise and dense graphs also reduce accuracy (reported in supplemental scenarios).",
            "gap_reduction_method": "Regularization (L1), hard thresholding (ω=0.3), augmented Lagrangian acyclicity penalty with increasing ρ and Lagrange multiplier updates; choice of mixed-type loss instead of treating all nodes as continuous serves to reduce model misspecification for categorical nodes.",
            "gap_reduction_effectiveness": "Effective in simulations: e.g., using the mixed-type loss results in higher F1 and lower SHD compared to treating all nodes as continuous (NOTEARS/DAGMA) when categorical fraction ≥2.5%; thresholding and L1 reduce false positives (many scenarios show directed FDR≈0).",
            "validation_cost_comparison": "Computational cost is discussed: DAGSLAM is slower than NOTEARS and DAGMA due to per-column vector operations for mixed-type loss. Example timings: for d=100 (n=1000) DAGSLAM and NOTEARS took ~10^3 s; for n=10000 (d=20) DAGSLAM ≈10^2 s while DAGMA stayed smaller; authors note a trade-off between computation and improved accuracy on mixed data.",
            "temporal_validation": null,
            "domain_maturity": "Emerging: gradient-based continuous optimization methods (NOTEARS family) are recent and improving DAG learning; mixed-type adaptations are new and less mature.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": "Not explicitly analyzed; multiple metrics (F1, SHD, FDR, TPR, FPR) are reported but correlation/failure-mode dependence is not systematically examined.",
            "validation_cascade": "Yes — primary cascade: (1) computational learning using surrogate objective L(W) → (2) validation against simulated ground-truth DAG (metrics like F1/SHD) → (3) application to observational NHANES dataset (no experimental ground-truth). Errors are propagated from surrogate optimization to graph estimates and then to real-data causal interpretations, but no experimental validation stage exists.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Identified limitations include higher computational cost for mixed-type losses, linear SEM assumption (no nonlinear interactions), poorer performance in dense graphs (S0/d ≥ 2), sensitivity to high proportion of categorical nodes and certain noise distributions (exponential), and lack of ability to incorporate prior constraints (whitelist/blacklist).",
            "domain_specific_factors": "Mixed variable types (continuous vs categorical), number of categorical levels (m), sample size n, edge sparsity S0/d, weight scale α, noise distribution (Gaussian, exponential, Gumbel), and graph topology (ER vs scale-free) strongly affect proxy-to-ground-truth gap.",
            "uuid": "e2227.0"
        },
        {
            "name_short": "NOTEARS",
            "name_full": "DAGs with NO TEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning)",
            "brief_description": "A pioneering gradient-based method that formulates DAG learning as a continuous optimization problem using a least-squares loss and a smooth acyclicity constraint (matrix exponential) to avoid combinatorial search.",
            "citation_title": "DAGs with NO TEARS: continuous optimization for structure learning",
            "mention_or_use": "use",
            "system_name": "NOTEARS",
            "system_description": "Frames linear SEM DAG learning as minimizing least-squares loss + L1 penalty under acyclicity constraint h(W)=tr(e^{W∘W}) - d = 0; solved with augmented Lagrangian and gradient-based optimization. Assumes all variables continuous (or treated as such in comparisons).",
            "domain": "causal structure learning / machine learning",
            "proxy_metric_name": "Least-squares surrogate loss and evaluation metrics (directed F1, SHD, FDR, TPR, FPR)",
            "proxy_metric_description": "Internal surrogate loss is LS residual norm plus sparsity penalty; external performance assessed via directed F1, SHD, etc. in simulations against known adjacency matrix.",
            "proxy_metric_type": "data-driven ML prediction / empirical surrogate",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix",
            "ground_truth_description": "Same simulation protocol as DAGSLAM; NOTEARS was evaluated by treating all variables as continuous in mixed-data simulations.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Representative comparisons: NOTEARS performs comparably to DAGSLAM in purely/mostly continuous settings and high-d scenarios (d=40/100) but worse when categorical fraction increases; example: at d=20 NOTEARS F1≈0.85 vs DAGSLAM ~0.99 (SHD NOTEARS ~5.1 vs DAGSLAM lower). With 50% categorical nodes NOTEARS F1 ~0.71 vs DAGSLAM 0.82.",
            "proxy_performance": "High on continuous-only simulations (F1 up to ~0.95 in some settings) and large-sample scenarios (n≥5000 F1~0.85 reported), but degraded with mixed/categorical data.",
            "ground_truth_performance": "Same as proxy metrics because ground-truth is known in simulations; observed TPR/FDR values vary widely across scenarios (e.g., in some scenarios TPR~0.85, FDR~0.15).",
            "false_positive_rate": "Varies by scenario; examples in table show directed FPR values in range ~0.00–0.1 depending on scenario (worse under misspecification to continuous assumption).",
            "false_negative_rate": "Inferred from reported TPR; e.g., TPR dropping from ~0.98 to ~0.85 in some scenarios yields false negative rate increase from ~0.02→0.15.",
            "novelty_characterization": "In-distribution for simulations but exhibits model-misspecification when applied to mixed-type data if variables are treated as continuous.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance worsens when categorical proportion rises or when variables are non-Gaussian; treating categorical nodes as continuous introduces a systematic gap in edge recovery (lower F1, higher SHD).",
            "gap_reduction_method": "Not in original NOTEARS; in practice other methods (DAGSLAM) reduce gap by using appropriate loss (logistic/multinomial) for categorical nodes. Regularization and thresholding are also used in comparisons.",
            "gap_reduction_effectiveness": "Using mixed-type loss (DAGSLAM) demonstrably improves recovery in mixed data (example: F1 improved from ~0.71→0.82 at 50% categorical).",
            "validation_cost_comparison": "NOTEARS benefits from matrix operations when variables are continuous; computational time lower than DAGSLAM in mixed-treatment experiments (e.g., both ~10^3 s for d=100 but NOTEARS slightly faster; for n=10000 DAGSLAM ≈10^2 s while DAGMA stable and NOTEARS faster).",
            "temporal_validation": null,
            "domain_maturity": "Established as a strong baseline for gradient-based DAG learning; widely-cited and used.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": "Not analyzed in detail in this paper.",
            "validation_cascade": "Computational surrogate → comparison to simulated ground-truth → applied to observational data (treated as continuous) without experimental validation.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Assumes continuous variables (least-squares loss), limited handling of categorical nodes unless variables are coerced to continuous, original acyclicity constraint (matrix exponential) can have worse gradient behavior vs log-det alternative.",
            "domain_specific_factors": "Variable type mismatch (categorical vs continuous), noise distribution, sample size, graph sparsity affect proxy-to-ground-truth gap.",
            "uuid": "e2227.1"
        },
        {
            "name_short": "DAGMA",
            "name_full": "DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization",
            "brief_description": "A NOTEARS-family algorithm that replaces the exponential acyclicity constraint with a log-determinant constraint derived from M-matrix properties, improving gradient behavior and computational stability.",
            "citation_title": "DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization",
            "mention_or_use": "use",
            "system_name": "DAGMA",
            "system_description": "Minimizes a score (e.g., least-squares + sparsity) subject to acyclicity enforced via h(W) = -logdet(sI - W∘W) + d log s; derivative form is simpler and yields more stable optimization; tested in comparisons treating variables as continuous.",
            "domain": "causal structure learning / machine learning",
            "proxy_metric_name": "Least-squares surrogate loss + log-det acyclicity penalty; evaluated using directed F1, SHD, FDR, TPR, FPR against simulated graphs.",
            "proxy_metric_description": "Internal surrogate objective similar to NOTEARS but with log-determinant acyclicity constraint; external recovery metrics computed against simulated ground-truth adjacency matrices.",
            "proxy_metric_type": "data-driven ML prediction / empirical surrogate",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix",
            "ground_truth_description": "Same simulation generation as for other methods; DAGMA was evaluated by treating nodes as continuous in mixed-data experiments for fairness of comparison.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "DAGMA often performs comparably to NOTEARS and close to DAGSLAM in high-dimensional continuous-like settings; example: at d=20, DAGMA F1≈0.86 and SHD≈4.0 vs DAGSLAM higher (~0.99/low SHD); with high categorical proportion DAGMA performance drops more than DAGSLAM (e.g., at 50% categoricals DAGMA F1≈0.60).",
            "proxy_performance": "Good in continuous or large-sample settings (F1 up to ~0.95 in some configurations), but degrades when categorical fraction increases and when treating categorical variables as continuous is misspecified.",
            "ground_truth_performance": "Same as proxy metrics given simulated ground-truth; TPR/FDR vary per scenario (see Table 2).",
            "false_positive_rate": "Scenario-dependent; examples include directed FPR in the low-single-digits in favorable settings and higher under misspecification (see Table 2 entries).",
            "false_negative_rate": "Inferred from TPR (varies across experiments; e.g., TPR values reported in Table 2 range ~0.5–1.0 across scenarios).",
            "novelty_characterization": "In-distribution in simulations; less robust under variable-type mismatch.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance gap increases with categorical proportion and other departures from continuous assumptions; DAGMA shows improvement in some scenarios due to better-conditioned gradients but still suffers from mixed-data misspecification.",
            "gap_reduction_method": "Log-determinant acyclicity constraint improves optimization stability vs NOTEARS' matrix exponential; nonetheless requires mixed-type loss (as DAGSLAM provides) to reduce model misspecification gap.",
            "gap_reduction_effectiveness": "Log-det yields better gradient behavior and computational performance (authors report faster/better-behaved gradients), but in mixed-type experiments DAGSLAM's tailored loss gives superior edge-recovery metrics.",
            "validation_cost_comparison": "DAGMA often has more moderate computational growth with d and n compared to NOTEARS/DAGSLAM; e.g., DAGMA's time increased by one order of magnitude for d up to 100 while DAGSLAM/NOTEARS increased by two orders in that experiment.",
            "temporal_validation": null,
            "domain_maturity": "Recent improvement in acyclicity constraints; considered an incremental refinement over NOTEARS.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": "Not analyzed.",
            "validation_cascade": "Computational surrogate → simulated ground-truth evaluation → observational application (no experimental validation).",
            "publication_bias_discussion": false,
            "limitations_challenges": "Like NOTEARS, assumes continuous variables for LS loss; mixed-type data handling requires extensions; although log-det helps optimization, performance still degrades with categorical data unless loss is changed.",
            "domain_specific_factors": "Same as NOTEARS: variable types, noise, sparsity, sample size affect fidelity to ground truth.",
            "uuid": "e2227.2"
        },
        {
            "name_short": "DAG-GNN",
            "name_full": "DAG-GNN (Directed Acyclic Graph learning with Graph Neural Networks)",
            "brief_description": "A nonlinear extension of continuous optimization DAG learning that uses variational autoencoders and graph neural networks to capture non-linear dependencies when learning DAG structure.",
            "citation_title": "DAG structure learning with graph neural networks",
            "mention_or_use": "mention",
            "system_name": "DAG-GNN",
            "system_description": "Uses deep generative models (VAE) and graph neural network parameterizations to model nonlinear SEMs and learn DAGs via gradient-based optimization with acyclicity constraints.",
            "domain": "causal structure learning / machine learning",
            "proxy_metric_name": "VAE reconstruction loss and graph recovery metrics (F1, SHD, etc.)",
            "proxy_metric_description": "Internal proxy is VAE-style ELBO / reconstruction loss; external validation uses simulated ground-truth adjacency matrices and metrics like directed F1 and SHD.",
            "proxy_metric_type": "data-driven ML prediction (deep learning surrogate)",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix (nonlinear SEM in some evaluations)",
            "ground_truth_description": "Ground truth in references and discussions is synthetic graphs with nonlinear data generating mechanisms used to compare structure recovery.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Not quantified numerically in this paper's experiments (DAG-GNN only mentioned in related work); referenced literature reports improved recovery in nonlinear settings vs linear methods but specific numbers are in cited DAG-GNN papers.",
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Designed for out-of-linear-model distributions (nonlinear, potentially out-of-distribution relative to linear baselines).",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "gap_reduction_method": "Uses neural-network expressivity to reduce model misspecification when data generating process is nonlinear.",
            "gap_reduction_effectiveness": null,
            "validation_cost_comparison": "Deep models generally have higher computational cost; not quantified here.",
            "temporal_validation": null,
            "domain_maturity": "Emerging; neural approaches for DAG learning are active research.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxies": null,
            "proxy_correlation": null,
            "validation_cascade": "Computational (neural surrogate) → simulated ground-truth evaluation; may be applied to observational data thereafter.",
            "publication_bias_discussion": null,
            "limitations_challenges": "Neural methods can be data hungry, harder to interpret, and may not provide calibrated uncertainty for causal claims.",
            "domain_specific_factors": "Nonlinearity, model capacity, training data size and distribution shift affect proxy-to-ground-truth gap.",
            "uuid": "e2227.3"
        },
        {
            "name_short": "Traditional score-based (HC / TABU)",
            "name_full": "Hill Climbing (HC) and Tabu Search (TABU) score-based DAG learning",
            "brief_description": "Classic combinatorial score-based DAG structure learning methods that search DAG space greedily (HC) or with Tabu lists to optimize a score (e.g., BIC), commonly used baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "HC / TABU (score-based search)",
            "system_description": "Greedy search through DAG space optimizing a predefined score (e.g., BIC, log-likelihood with penalty); HC takes local moves, TABU keeps a tabu list to avoid cycling. Implemented via bnlearn in comparisons.",
            "domain": "causal structure learning / statistics",
            "proxy_metric_name": "Score functions (BIC/log-likelihood) as surrogate objective and graph recovery metrics (F1, SHD, FDR, TPR)",
            "proxy_metric_description": "Optimization target is a statistical score (penalized likelihood) rather than direct edge-recovery; evaluation against simulated ground-truth uses directed F1, SHD, FDR, TPR, FPR.",
            "proxy_metric_type": "empirical surrogate (statistical score) / search-based",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix",
            "ground_truth_description": "Performance measured by recovery of true directed edges in simulations.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Traditional methods perform worse in high-d and mixed-data simulations reported: e.g., at d=40 HC F1 dropped to 0.57, TABU 0.66; SHD increased substantially (e.g., HC SHD ~13.9 at n=100 in one scenario).",
            "proxy_performance": "Lower than modern gradient-based methods in the paper's simulations: F1 often ~0.5–0.8 depending on scenario; suffers at large d and low n.",
            "ground_truth_performance": "Same as proxy metrics compared to simulated ground truth (TPR/FDR reported in Table 2; performance often worse relative to DAGSLAM/NOTEARS).",
            "false_positive_rate": "Often higher than gradient-based methods in reported scenarios; e.g., HC FPR and SHD values in Table 2 show many false positives at low sample sizes/high-d.",
            "false_negative_rate": "Also higher in many scenarios (lower TPR).",
            "novelty_characterization": "In-distribution but limited scalability to high-d and mixed-type settings.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Gaps increase with number of nodes and low sample sizes; traditional methods degrade faster than gradient-based alternatives as d grows.",
            "gap_reduction_method": "Not specifically addressed in this paper; ensemble/bagging approaches (DAGBagM) are attempted to improve robustness.",
            "gap_reduction_effectiveness": "DAGBagM and other aggregations show some improvements but still lag behind DAGSLAM in mixed-data simulations.",
            "validation_cost_comparison": "Classic methods can be faster for small graphs but do not scale well; computational comparisons not the paper's main focus for these methods (authors did not run R implementations for timing comparisons).",
            "temporal_validation": null,
            "domain_maturity": "Mature classical approaches for DAG learning but limited in scalability and handling mixed types.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Score optimization → comparison to simulated ground-truth → observational application.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Scalability, sensitivity to score choice and local optima, difficulty handling mixed-type data without coercion.",
            "domain_specific_factors": "Graph size, sample size, variable type heterogeneity and score misspecification.",
            "uuid": "e2227.4"
        },
        {
            "name_short": "mDAG",
            "name_full": "Mixed Directed Acyclic Graph learning (mDAG)",
            "brief_description": "Method tailored to learn DAG structures with mixed node types (continuous and categorical) using approaches designed for mixed data.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "mDAG",
            "system_description": "Designed specifically for mixed-type node learning in DAGs; used as a comparison baseline in simulations (implemented in R).",
            "domain": "causal structure learning / biostatistics",
            "proxy_metric_name": "Method-specific score/loss for mixed data + graph recovery metrics (F1, SHD, FDR, TPR)",
            "proxy_metric_description": "Evaluated against known simulated DAGs using directed F1, SHD and other recovery metrics; mDAG’s internal surrogate differs from gradient-based mixed-loss but exact formulation not detailed in main text.",
            "proxy_metric_type": "empirical surrogate / domain-specific mixed-data method",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix",
            "ground_truth_description": "Same simulation protocol; mDAG evaluated on simulation replicates and real-data examples where possible.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "mDAG often underperforms DAGSLAM in many scenarios; example F1 values: at d=40 mDAG F1 ≈0.68 in one scenario while DAGSLAM ≈0.97; in some low-categorical-level settings mDAG close in skeleton metrics but still lower directed accuracy.",
            "proxy_performance": "Variable; sometimes competitive on skeleton metrics but generally lower directed F1 and higher SHD than DAGSLAM in simulations.",
            "ground_truth_performance": "Measured via the same metrics and shows inferior edge-direction recovery in many experiments (see Table 2).",
            "false_positive_rate": "Scenario-dependent; reported FDRs in Table 2 show higher FDR/FPR in several scenarios relative to DAGSLAM.",
            "false_negative_rate": "Also variable; lower TPR in many scenarios.",
            "novelty_characterization": "Designed for mixed-type data (in-distribution for mixed scenarios).",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance gap widens when categorical levels &gt;2 cannot be handled or when sample sizes are small.",
            "gap_reduction_method": "Not specified in main text; treated as baseline without the same continuous optimization tools as DAGSLAM.",
            "gap_reduction_effectiveness": "Less effective than DAGSLAM in the reported simulation suite.",
            "validation_cost_comparison": "Not timed in main text (R implementation); generally performs worse accuracy-wise though computational cost not compared directly in this paper.",
            "temporal_validation": null,
            "domain_maturity": "Specialized method for mixed data, but apparently less robust than DAGSLAM in these experiments.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Computational method → validated on simulations → compared on NHANES where possible (but had limitations for &gt;2-level categoricals).",
            "publication_bias_discussion": false,
            "limitations_challenges": "Some implementations cannot handle categorical nodes with &gt;2 levels (treat as continuous), poorer performance at small n and in certain topologies.",
            "domain_specific_factors": "Categorical level count, sample size, and node mix.",
            "uuid": "e2227.5"
        },
        {
            "name_short": "DAGBagM",
            "name_full": "Directed Acyclic Graph Bagging with Mixed Variables (DAGBagM)",
            "brief_description": "An aggregation/ensemble hill-climbing approach designed for learning DAGs with mixed variables by bagging multiple DAG estimates to improve robustness.",
            "citation_title": "DAGBagM: learning directed acyclic graphs of mixed variables with an application to identify prognostic protein biomarkers in ovarian cancer",
            "mention_or_use": "use",
            "system_name": "DAGBagM",
            "system_description": "Applies non-aggregated hill climbing across bootstrap samples and aggregates results to reduce variance and false positives for mixed-variable DAG learning; included as a baseline.",
            "domain": "causal structure learning / bioinformatics",
            "proxy_metric_name": "Aggregated score-based surrogate + graph recovery metrics (F1, SHD, FDR, TPR)",
            "proxy_metric_description": "Uses bagging ensemble of score-based search results as surrogate for more stable graph recovery; evaluated against simulated ground-truth DAGs.",
            "proxy_metric_type": "empirical surrogate (ensemble of score-based models)",
            "ground_truth_metric": "Simulated ground-truth DAG adjacency matrix",
            "ground_truth_description": "Evaluated in simulations; tends to improve stability but still trails DAGSLAM in many scenarios.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "DAGBagM sometimes improves over single HC but still shows lower directed F1 (e.g., F1≈0.63 at n=100 in one scenario vs DAGSLAM 0.94) and higher SHD in several experiments.",
            "proxy_performance": "Moderate; performs better than single HC in some scenarios but generally inferior to DAGSLAM/NOTEARS for directed edge recovery.",
            "ground_truth_performance": "Directly measured against simulated ground truth — lower TPR and higher SHD vs DAGSLAM in many reported scenarios.",
            "false_positive_rate": "Ensembling reduces some false positives, but FDR remains higher than DAGSLAM in many experiments (see Table 2).",
            "false_negative_rate": "Higher false negatives in low-sample settings (low TPR).",
            "novelty_characterization": "Ensemble approach aims to improve robustness in-distribution; not designed for extrapolative novelty.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Performance improves with increasing n but still lags behind gradient-based mixed-loss methods; unique trend: DAGBagM improves slowly even when n≥1000.",
            "gap_reduction_method": "Bagging/aggregation of HC results across bootstraps.",
            "gap_reduction_effectiveness": "Some reduction in variance/false positives relative to single HC, but effectiveness limited compared to DAGSLAM (quantified by persistent lower directed F1 and higher SHD).",
            "validation_cost_comparison": "Not timed in main text; R-based ensemble likely more expensive than single HC but still typically less accurate than DAGSLAM.",
            "temporal_validation": null,
            "domain_maturity": "Practical ensemble method for mixed data; established but not state-of-the-art here.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Ensemble computational approach → validated on simulated ground-truth → observational application.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Lower directed-edge recovery than gradient-based mixed-loss methods; slow improvement with sample size in some settings.",
            "domain_specific_factors": "Sample size, categorical proportion, and graph sparsity.",
            "uuid": "e2227.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DAGs with NO TEARS: continuous optimization for structure learning",
            "rating": 2
        },
        {
            "paper_title": "DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization",
            "rating": 2
        },
        {
            "paper_title": "DAG structure learning with graph neural networks",
            "rating": 2
        },
        {
            "paper_title": "Scaling structural learning with NO-BEARS to infer causal transcriptome networks",
            "rating": 1
        }
    ],
    "cost": 0.02274175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DAGSLAM: causal Bayesian network structure learning of mixed type data and its application in identifying disease risk factors</p>
<p>Yuanyuan Zhao 
Department of Biostatistics
School of Public Health
Peking University
38 Xueyuan Road100191BeijingChina</p>
<p>Jinzhu Jia jzjia@math.pku.edu.cn 
Department of Biostatistics
School of Public Health
Peking University
38 Xueyuan Road100191BeijingChina</p>
<p>Center for Statistical Science
Peking University
100871BeijingChina</p>
<p>DAGSLAM: causal Bayesian network structure learning of mixed type data and its application in identifying disease risk factors
8E203252B96C216F7C31A78F08B73EFD10.1186/s12874-025-02582-6Received: 14 December 2024 Accepted: 29 April 2025Bayesian networks (BNs)Directed cyclic graphs (DAGs)Causal inferenceRisk factor
Background Identifying and understanding disease risk factors is crucial in epidemiology, particularly for chronic and noncommunicable diseases that often have complex interrelationships.Traditional statistical methods struggle to capture these complexities, necessitating more sophisticated analytical frameworks.Bayesian networks and directed acyclic graphs (DAGs) provide powerful tools for exploring the complex relationships between variables.However, existing DAG structure learning algorithms still have limitations in handling mixed-type data (including continuous and discrete variables), which restricts their practical utility.Therefore, developing DAG structure learning methods that can effectively handle mixed data is highly important for obtaining an in-depth understanding of disease risk factors and pathogenic mechanisms.MethodsThis study proposes an extension of the NOTEARS algorithm, termed DAGSLAM, which is designed for Bayesian network structure learning with mixed-type data.The algorithm integrates continuous and categorical variables through a tailored loss function, enhancing its applicability to real-world epidemiological datasets.ResultsExtensive simulations were conducted across eight distinct scenarios, specifically, variations in the number of nodes, changes in the proportion of categorical nodes, different sample sizes, levels of categorical nodes, variations in edge sparsity, adjustments to the weight scale, different graph types, and diverse noise distributions.These scenarios demonstrate that DAGSLAM consistently outperforms existing methods such as HC, TABU, mDAG, and DAG-BagM across key metrics, including precision, recall, F1 score, and structural Hamming distance (SHD).Furthermore, the robustness of DAGSLAM is validated through its application to the National Health and Nutrition Examination Survey (NHANES) dataset, revealing critical causal relationships among risk factors for CHD and diabetes.Conclusions DAGSLAM provides a powerful and scalable tool for uncovering causal relationships in complex disease networks, with significant implications for risk factor identification and public health research.</p>
<p>Background</p>
<p>In the field of epidemiology, identifying and understanding the risk factors for diseases is a cornerstone of public health research.Many diseases, particularly chronic and noncommunicable conditions, do not occur in isolation but are often interconnected through shared risk factors and complex comorbidities.For example, metabolic disorders, cardiovascular conditions, and other chronic diseases frequently cooccur, forming intricate networks of interactions that challenge traditional approaches to disease prevention and management [1,2].These interactions are further complicated by the multifactorial nature of many diseases, where genetic, behavioral, and environmental factors converge to influence disease onset and progression [3].Understanding these relationships is critical for developing effective prevention strategies and improving population health outcomes.</p>
<p>Traditional statistical methods, such as logistic regression and Cox proportional hazards models, have been widely used to identify associations between individual risk factors and disease outcomes.While these methods are effective for analysing simple, linear relationships, they often fail to capture the complexity of multifactorial diseases and comorbidities.Specifically, they struggle to model the conditional dependencies among multiple variables and disentangle direct and indirect effects in complex networks [4,5].Moreover, these methods typically require strong assumptions about the independence of variables and the directionality of relationships, which may not hold in real-world epidemiological data [6].As a result, there is a growing need for more sophisticated analytical frameworks that can model the intricate causal relationships underlying disease risk.</p>
<p>Bayesian networks (BNs) have emerged as promising tools for addressing these challenges.BNs are probabilistic graphical models that represent variables and their conditional dependencies through Directed cyclic graphs (DAGs) [7].Unlike traditional statistical methods, BNs can model interactions among multiple variables and provide insights into the causal structure of disease risk factors [6,8].These features make BNs particularly well suited for studying complex disease networks and identifying key drivers of comorbidities [9][10][11].However, constructing BNs from real-world data is a nontrivial task.Manual construction of causal models on the basis of expert knowledge is not only time-consuming but also inherently prone to bias, as it relies heavily on subjective interpretations and assumptions [12][13][14].This process can lead to inconsistencies and inaccuracies in the resulting models, particularly when dealing with complex interrelationships among variables.On the other hand, automated structure learning algorithms have emerged as promising alternatives for discovering causal relationships from observational data [8,[15][16][17][18].However, these algorithms often face significant limitations, especially in regard to handling mixed data types, which include both continuous and categorical variables.</p>
<p>In real-world scenarios, observational data typically comprise a mixture of variable types.For example, clinical outcomes are often represented as binary variablessuch as the presence or absence of a disease-while potential biomarkers may be continuous variables, such as the expression levels of specific proteins in the body.While substantial progress has been made in causal structure learning for purely discrete or continuous data, the study of mixed-type data remains relatively underexplored [19][20][21].Most existing methods are designed under the assumption that all nodes conform to the same type of distribution, which restricts their applicability to either purely discrete or purely continuous datasets [8,13,15,[22][23][24][25][26][27].This limitation significantly hampers the ability to jointly model continuous and categorical variables, thereby constraining the practical utility of these methods in complex epidemiological research.</p>
<p>To address these limitations, this study proposes an extension of the NOTEARS (Non-combinatorial Optimization via Trace Exponential and Augmented lagRangian for Structure learning) algorithm, referred to as DAGSLAM (Directed Acyclic Graphs Structure learning via Log-determinant and Augmented lagrangian for Mixed type data), which is specifically designed for BN structure learning with mixed-type data [28].The proposed method leverages the strengths of BNs in modelling complex causal relationships while overcoming the constraints of existing algorithms.By enabling the integration of both continuous and categorical variables, DAGSLAM provides a more flexible and scalable framework for analysing real-world epidemiological data.</p>
<p>The key contributions of this study are as follows:</p>
<p>• Development of the DAGSLAM algorithm for BN structure learning with mixed-type data addresses a critical gap in existing methodologies.• Extensive simulation experiments were conducted in various scenarios to validate the robustness of the DAGSLAM algorithm, demonstrating its effectiveness across different conditions and settings.• The proposed framework was applied to identify risk factors and causal relationships in complex disease networks, demonstrating its utility in epidemiological research.</p>
<p>The findings of this study will contribute to a better understanding of the complex causal mechanisms underlying disease risk and comorbidities, serving as a foundation for future research and clinical applications in the field of risk factor identification and prevention.</p>
<p>Related works</p>
<p>DAG structure learning has been a long-standing challenge in machine learning and causal inference.Existing algorithms can be broadly categorized into three types: score-based, constraint-based, and hybrid methods [29].Score-based approaches, such as Hill Climbing (HC) and Tabu Search, aim to optimize a predefined score function over the space of DAGs [15,23,30].Constraint-based methods, like the PC algorithm, rely on conditional independence tests to infer the graph structure [8].Hybrid methods, such as Max-Min Hill-Climbing (MMHC), combine both score-based and constraint-based strategies to improve robustness [16].While these methods have been widely used, they often face challenges in scalability and computational efficiency due to the combinatorial nature of the DAG space.</p>
<p>Recently, gradient-based DAG learning has emerged as a promising alternative, leveraging continuous optimization techniques to address the NP-hard problem of DAG discovery.One of the pioneering works in this direction is NOTEARS [28], which reformulates the DAG learning problem as a continuous optimization task by introducing a smooth acyclicity constraint.NOTEARS uses least squares for linear structural equation models (SEMs) and has shown significant improvements in both accuracy and scalability compared to traditional methods.</p>
<p>While NOTEARS is effective for linear SEMs, it is limited in its ability to capture complex, non-linear relationships.To address this, DAG-GNN extends the framework by employing a deep generative model based on variational autoencoders (VAEs) [31].The key innovation of DAG-GNN lies in its use of graph neural networks (GNNs) to parameterize the encoder and decoder, allowing the model to capture non-linear dependencies in the data.Similarly, GraN-DAG employs neural networks to learn non-linear relationships while maintaining the benefits of continuous optimization [32].Another notable approach, NOBEARS, introduces polynomial models to infer causal relationships in high-dimensional settings, such as transcriptome networks [33].</p>
<p>Further advancements include NOTEARS-nonlinear, which generalizes NOTEARS to non-parametric DAGs, allowing for more flexible modeling of complex data distributions [34].More recently, DAGMA proposes a log-determinant acyclicity constraint, which is computationally efficient and exhibits better-behaved gradients compared to the exponential constraint used in NOTEARS [35].Additionally, GFlowNets introduces a generative flow network framework for DAG learning, offering a novel perspective on sampling and optimization in the DAG space [36].</p>
<p>Despite these advancements, several limitations remain.While many gradient-based algorithms have successfully extended to non-linear SEMs, there is still a lack of robust methods for handling mixed data types (e.g., continuous and categorical variables) in linear settings.This gap is particularly relevant in applications such as biomedical research, where datasets often contain heterogeneous variables.To address this challenge, we propose DAGSLAM, an extension of NOTEARS specifically designed for mixed-type data.By integrating linear regression for continuous variables and logistic regression for binary variables, DAGSLAM provides a unified framework for learning DAGs from diverse data types, filling a critical gap in the current literature.</p>
<p>Methods</p>
<p>Overview and notations of DAG learning problems</p>
<p>Suppose that X ∈ R n×d is a data matrix that contains n independent and identically distributed (i.i.d.) instances of the random vector X = (X 1 , • • • , X d ) .In this context, X j represents a random variable that can be continu- ous or discrete, and x ij denotes the value of a random variable.The index i is used to distinguish among different instances of the data, whereas j is used to identify different random variables.We define an index set V : = {1, . . ., d} to represent the indexes of different nodes (or random variables).Additionally, we define C as an index set of continuous variables and D as an index set of discrete variables.Thus, it follows that V = C ∪ D.</p>
<p>Let D denote the discrete space of the directed acyclic graph G = (V , E) on d nodes, where V denotes the set of nodes and where E ⊂ V × V denotes the set of edges.For i, j ∈ E , if X i is the parent node of X j , it is repre- sented as X i → X j .</p>
<p>Within the linear SEM utilized by the NOTEARS algorithm, each X j is formulated as a linear combination of pa X j .Each DAG is encoded by a weighted adjacency matrix W ∈ R d×d , where W ij = 0 indicates that X i acts as a parent node for X j .If W ij = 0 and W ji = 0 , there is no edge connecting X i and X j .</p>
<p>In the original NOTEARS algorithm, the score function F (W ) is defined as the least-squares (LS) loss combined with an ℓ 1 -regularization term to encourage sparsity in the estimated graph.Specifically, the score function is given by [28]:
where �X − XW � 2
F is the Frobenius norm of the residual matrix, W 1 is the ℓ 1 -norm of the weighted adjacency matrix W , and is a regularization parameter controlling the sparsity of the graph.The least-squares loss does not assume a specific noise distribution and can be applied to linear SEMs with both Gaussian and non-Gaussian noise.Moreover, the framework is designed to be flexible, enabling the incorporation of other loss functions tailored to different data types and modeling assumptions.</p>
<p>According to the score-based method, we seek to solve the following combinational optimization program for the optimal DAG structure:</p>
<p>where F (W ) is a score function (i.e., a loss function) and where G(W ) refers to the graph with d nodes generated by the weighted adjacency matrix W .</p>
<p>To address the combinatorial nature of the problem, the NOTEARS framework introduces a continuous optimization approach.Specifically, it transforms the discrete acyclicity constraint into an equivalent continuous constraint [28]:</p>
<p>In the original NOTEARS formulation, the acyclicity constraint is defined as h(W ) = tr e W •W − d = 0, where ∘ refers to the Hadamard product, and e W •W is the matrix exponential of W • W .This constraint ensures that the learned graph is acyclic, and its derivative
∇h(W ) = e W •W T • 2W is straightforward to compute.
Building on NOTEARS, the DAGMA algorithm [35] proposes a log-determinant constraint as an alternative:</p>
<p>where s is a positive scalar parameter.This constraint lev- erages the M-matrix property of sI − W • W ensuring (1)
F (W ) = 1 2n �X − XW � 2 F + �W � 1 ,(2)min W ∈R d×d F (W ) s.t. G(W ) ∈ DAGs, (3) min W ∈R d×d F (W ) s.t. h(W ) = 0, (4) h(W ) = −logdet(sI − W • W ) + d log s = 0,
that the learned graph is acyclic.The log-determinant constraint has a simple derivative form: which is computationally efficient and well-behaved during optimization.Compared to the matrix exponentialbased constraint, the log-determinant constraint offers several advantages, including better gradient behaviour, faster computation, and improved cycle detection, especially for large graphs.</p>
<p>To date, the combinational optimization problem has drastically transformed to a continuous optimization program, which can be conveniently addressed via conventional numerical solution methods such as gradient descent.</p>
<p>Structural equation modelling of mixed-type data</p>
<p>We first construct SEMs for both continuous variables and discrete variables.A continuous variable X j (j ∈ C) can be modelled as: where w j denotes the j-th column of W .It follows that w ij = 0 when X i is not a parent node X j , and vice versa.ǫ j is a random noise term with a zero mean and a variance of σ 2 .Note that we do not need to assume that ǫ j follows a normal distribution.</p>
<p>For a binary variable X k (k ∈ D) , the probability of X k = 1 can be given by a logistic regression model: where w k denotes the kth column of W .</p>
<p>Similarly, for a polytomous variable X k (k ∈ D) , a multinomial logistic regression model can be posited: where l = 1,2, • • • ,M , with M being the number of cate- gories for the polytomous variable.For instance, when X k is a three-class variable, M = 3.</p>
<p>Our purpose is to learn the best latent structure of the DAG given the observational data generated from its intrinsic mechanism corresponding to the DAG.</p>
<p>(5)
∇h(W ) = 2(sI − W • W ) −T • W , (6) X j = w T j X + ǫ j ,(7)P(X k = 1) = exp w T k X 1+exp w T k X , (8) P(X k = l) = exp w T k(l) X M m=1 exp w T k(m) X
, Therefore, it is intuitive to define the loss function as the discrepancy between the true value of every X j and its fitted value of our models.Specifically, for a continuous variable X j (j ∈ C) , we define its loss function as:</p>
<p>where w jj = 0.The loss measures how well the fit is achieved by utilizing all other variables to estimate the continuous variable X j , which is analogous to the least square loss in the linear regression literature.</p>
<p>For a binary variable X k (k ∈ D) , its loss function can be given by: where w kk = 0 , which is exactly the cross-entropy loss function in the binary classification problem or the negative log-likelihood function in the logistic regression model.</p>
<p>Similarly, for a polytomous variable X k (k ∈ D) , its loss function can be given as follows:</p>
<p>where w T k(m) denotes the coefficient of X in the model of P(X k = m) .This is the same as the cross-entropy loss function in the multiclassification problem or the negative log-likelihood function in the multinomial logistic regression model.</p>
<p>Finally, the total loss function can be defined as the sum of continuous variables and categorical (both binary and polytomous) variables: which is a function of the weighted adjacency matrix W .</p>
<p>The optimization program</p>
<p>After defining the loss function of mixed-type data, we aim to find the optimal W that minimizes the value of the loss function L(W ) by searching through the space of all possible weighted adjacency matrices W for DAGs within the 'acyclic' constraint.To this end, we need to solve the following continuous optimization program:
(9) L j = �X i − w T j X� 2 2 , (10) L k = −�X k , w T k X� + log 1 + exp w T k X ,(11)L k = − M m=1 I(X k = m) • w T k(m) X + log M m=1 exp w T k(m) X ,(12)L(W ) = j∈C L j + k∈D L k , where h(W ) = −logdet(sI − W • W ) + dlogs = 0.
This equality-constrained program (ECP) can be solved via the augmented Lagrangian method, converting (13) to the following unconstrained program: with penalty parameter ρ &gt; 0 and Lagrange multiplier α.When ρ is sufficiently large and α is appropriately chosen, the solution obtained from the unconstrained program ( 14) closely approximates that of the original program ( 13) [37].</p>
<p>The Lagrange multiplier α is updated iteratively via the following formula:</p>
<p>where W * α denotes the optimizer of program ( 14) at a given α.</p>
<p>To ensure that the learned DAG is a sparse network, an L1 regularization can be added to the loss function, resulting in the following optimization problem:
where �W � 1 = i� =j W ij .
In the context of regression problems, hard thresholding can effectively diminish the number of false positives.Therefore, we apply the following thresholding to the weights of the edges: after obtaining the optimal solution W * for problem (17), we set the weights with absolute values less than ω to zero, given a fixed thresh- old ω &gt; 0 .The following pseudocode outlines the DAG- SLAM algorithm, which can be implemented in Python 3 conveniently.(13) min
W ∈R d×d L(W ) s.t. h(W ) = 0, (14) min W ∈R d×d L ρ (W , α), (15)
where
L ρ (W , α) = L(W ) + ρ 2 |h(W )| 2 + αh(W ), (16) α ← α + ρh W * α ,(17) minW ∈R d×d L ρ, (W , α), (18)where L ρ, (W , α) = L(W ) + ρ 2 h(W ) 2 + αh(W ) + �W �</p>
<p>Results</p>
<p>Simulation studies</p>
<p>We conduct several simulation experiments across different scenarios to evaluate the performance of our proposed DAGSLAM algorithm and compare it against six existing algorithms in the context of DAG structure learning.The algorithms selected for comparison include NOTEARS, which serves as the baseline algorithm for DAGSLAM, and DAGMA, both of which can be implemented in Python 3 (available at https:// github.com/ xunzh eng/ notea rs and https:// github.com/ kevin sbello/ dagma, respectively).</p>
<p>We also include two established conventional scorebased algorithms: 'HC' and 'TABU, ' both available in the bnlearn package.Additionally, we consider the non-aggregated hill climbing (HC) algorithm from the 'Directed Acyclic Graph Bagging with Mixed Variables (DAGBagM)' package, as well as the 'Mixed Directed Acyclic Graph (mDAG)' algorithm, which is specifically designed for learning DAG structures with mixed node types.</p>
<p>The choice of these algorithms is based on their relevance to our study: NOTEARS and DAGMA are gradient-based algorithms, while HC and TABU represent mature traditional methods within the score-based framework.Furthermore, mDAG and DAGBagM were selected because they, like DAGSLAM, are tailored for learning DAG structures from mixed data types, particularly in biomedical applications.</p>
<p>All parameters for the comparison algorithms are set according to the default values recommended in their respective original publications.It is important to note that the HC, TABU, NOTEARS, and DAGMA methods are designed to handle datasets where all variables are of the same type-either all continuous or all categorical-and therefore cannot process mixed-type data.For simplicity, we treat all nodes as continuous in our simulations.</p>
<p>Simulation setup</p>
<p>To ensure the robustness and effectiveness of our proposed algorithm across various conditions, we conducted eight sets of simulation experiments in a total of 20 distinct scenarios, each featuring different combinations of the number of nodes, the proportion of categorical nodes, sample size, the levels of categorical nodes, edge sparsity, weight scale, graph type, and noise distribution.A summary of the simulation settings is presented in Table 1.</p>
<p>In each experimental set, a random graph G was pro- duced on the basis of either the Erdős-Rényi (ER) model or the scale-free (SF) model, and the corresponding adjacency matrix B ∈ {0,1} d×d was obtained.</p>
<p>We configured various numbers of true edges S 0 ∈ {0.5d, d, 2d} .For the edge i → j that truly exists between nodes i and j , its weight w ij was assigned inde- pendently from a uniform distribution over the interval α
• [0.5,2] ∪ −α • [0.5,2]
, where α is the weight scale fac- tor.This results in a weighted adjacency matrix W = [w ij ] ∈ R d×d .Given W , we sample X through SEM according to each node's type.For continuous nodes X j , data are generated via the linear model X j = w T j X + ǫ j , where the random disturbance term ǫ j follows one of three noise models: Gaussian distribution, exponential distribution or Gumbel distribution.For binary nodes X k , data are produced via the logistic regression model
P(X k = 1) = exp w T k X 1+exp w T k X .
For categorical nodes X k with m levels ( m &gt; 2 ), data are generated via a multinomial logistic regression model represented as
P(X k = l) = exp w T k(l) X M m=1 exp w T k(m) X
, where l = 1,2, • • • ,m .On the basis of the aforementioned data generation mechanism, a random dataset X ∈ R n×d is produced, with each row being independent and identically distributed.</p>
<p>For each simulation scenario, we generated n ∈ {100,500,1000,5000,10000} samples for graphs with d ∈ {10,20,40,100} nodes.A specific number or propor- tion of nodes (10%, 20%, or 50%) was randomly selected to be categorical nodes, while the remaining nodes were designated continuous nodes.</p>
<p>On the basis of the recommended hyperparameter values provided in the results of Zheng et al., a weight threshold of ω = 0.3 and a regularization coefficient of = 0.1 were established for all the aforementioned simu- lation experiments [37].</p>
<p>Evaluation metrics</p>
<p>For each method, the performance of the learned structures was assessed via five common metrics: the false discovery rate (FDR), true positive rate (TPR), false positive rate (FPR), structural Hamming distance (SHD), and F1 score.These metrics were evaluated for both the estimated directed structure (i.e., edges with direction) and the estimated skeleton structure (i.e., edges without direction).Given that we are learning causal BN structures, our primary focus is on the accuracy of the learned edge directions.Therefore, we emphasize the directed metrics in our analysis.The skeleton metrics are also reported but are considered supplementary references.In subsequent discussions, unless otherwise specified, all mentioned metrics will refer to the directed metrics.</p>
<p>We define positive (P) edges as those present in the estimated graph, true (T) edges as those found in the ground truth graph, and false (F) edges as the nonedges (2025) 25:154 in the ground truth graph.True positive (TP) edges are defined as the estimated edges that have the correct direction in the ground truth graph, reversed (R) edges are the estimated edges with the opposite direction, and false positive (FP) edges are the estimated edges that do not exist in the ground truth skeleton.Additionally, let E be the extra edges in the estimated graph compared with the ground truth skeleton, and let M be the missing edges from the ground truth skeleton.</p>
<p>The five metrics of the estimated directed structure are given by:
FDR directed = R + FP P TPR directed = TP T FPR directed = R + FP F SHD directed = E + M + R F 1 directed = 2 × (1 − FDR directed ) × TPR directed 1 − FDR directed + TPR directed
The five metrics of the estimated skeleton structure are defined as follows:</p>
<p>In each simulation scenario, the performance metrics are calculated as the average of 10 independent replicates.</p>
<p>Simulation results</p>
<p>The details in terms of the FDR, TPR, FPR, SHD, and F1 score of each method are summarized in Table 2. To intuitively illustrate the ability of the DAGSLAM method to recover true DAG structures, we present visualizations of the true DAG structures, heatmaps of the adjacency matrix of the true graph, and the weighted adjacency
FDR skeleton = FP P TPR skeleton = R + TP T FPR skeleton = FP F SHD skeleton = E + M F 1 directed = 2 × (1 − FDR skeleton ) × TPR skeleton 1 − FDR skeleton + TPR skeleton</p>
<p>The number of nodes</p>
<p>Simulation (i) comprises scenarios 1, 2, 3 and 4, with a focus on the impact of varying the number of nodes d , with values set at {10, 20, 40,100} .Overall, DAGSLAM consistently outperforms other methods across different node counts.At d = 10 , DAGSLAM achieves an impres- sive directed F1 score of 0.95 and a SHD of 1.0, demonstrating its ability to accurately capture the true structure in smaller networks (Table 2).As the number of nodes increases to d = 20 and d = 40 , DAGSLAM's perfor- mance continues to improve, with F1 scores remaining close to 1 and SHD values near 0, indicating its stability and effectiveness in handling more complex networks.Notably, DAGSLAM consistently appears in the upperright corner of the precision-recall (PR) plots (Fig. 1A  and B), achieving both high precision and high recall across all node counts, with values approaching 1.00.In contrast, other algorithms exhibit varying degrees of performance degradation as d increases.For instance, HC, mDAG and DAGBagM show a noticeable decline in F1 scores at d = 40 (Fig. 1C and D, Table 2), with HC dropping to 0.57, mDAG to 0.68 and TABU to 0.66.At d = 100 , performance of HC, TABU, and DAGBagM fur- ther deteriorates, with HC and TABU achieving F1 scores of 0.55 and 0.57, respectively, and DAGBagM dropping to 0.54.This decline is accompanied by a sharp increase in SHD values (Fig. 1E and F), reflecting their challenges in accurately modeling high-dimensional data.</p>
<p>From the PR plots (Fig. 1A and B), DAGSLAM consistently outperforms other methods across all node counts, maintaining high precision and recall.While NOTEARS and DAGMA also perform well in high-dimensional settings ( d = 40 and d = 100 ), their precision and recall val- ues at d = 20 are slightly lower than those of DAGSLAM, indicating a relative weakness in mid-range scenarios.</p>
<p>F1 scores and SHD values also reflect similar facts.NOTEARS and DAGMA perform comparably to DAGSLAM in high-dimensional settings ( d = 40 and d = 100 ), achieving high F1 scores and low SHD values.However, at d = 20 , both algorithms exhibit a slight drop in performance compared to DAGSLAM, with DAGMA and NOTEARS achieving F1 scores of 0.86 and 0.85, respectively, and slightly higher SHD values (4.0 and 5.1, respectively) (Fig. 1C, D, E and F, Table 2).This suggests  that DAGSLAM is more robust in mid-range node counts.</p>
<p>Proportion of categorical nodes</p>
<p>Simulation (ii) includes scenarios 5, 6, and 7, with a focus on the impact of varying the proportion of categorical nodes, with values set at {10%, 20%, 50%} .The true DAG graph and adjacency matrix remain the same as those in scenario 3. We fixed the number of nodes at d = 20 , resulting in the corresponding number of categorical nodes k = 2,4, 10.</p>
<p>The performance of the algorithms is significantly affected by the proportion of categorical nodes.As the proportion k increases, the performance of the gradi- ent-based algorithms-DAGSLAM, NOTEARS, and DAGMA-declines to varying degrees.For instance, at k = 10 (50% categorical nodes), DAGSLAM's F1 score drops from 0.97 to 0.82, while NOTEARS and DAGMA show F1 scores around 0.71 and 0.60, respectively (Fig. 2C, Table 2).This decline may be attributed to the inherent challenges that arise when integrating categorical data into the structure learning process, complicating the optimization landscape.</p>
<p>Despite this, DAGSLAM maintains the best performance across all scenarios ( k = 2,4,10 ).In Fig. 2A, DAG- SLAM is consistently closer to the upper-right corner compared to other algorithms, indicating its advantages in both directed precision and directed recall.In terms of F1 scores (Fig. 2C), DAGSLAM outperforms other algorithms, while its SHD values (Fig. 2E) remain lower than those of the other methods, demonstrating its effectiveness in capturing the true structure even with relatively large proportion of categorical nodes.</p>
<p>In contrast, the performance of HC, TABU, mDAG, and DAGBagM shows little change with increasing proportions of categorical nodes, and in some cases, their performance even improves slightly.However, despite being more robust in handling categorical data, these algorithms consistently underperform compared to DAGSLAM, with F1 scores only around 0.70 (Fig. 2C, Table 2).</p>
<p>Sample size</p>
<p>Simulation (iii) comprises scenarios 8, 9, 2, 10, and 11, focusing on the impact of varying the number of sample sizes n , with values set at {100, 500, 1000, 5000, 10000} , under a fixed DAG topology structure.In general, DAG-SLAM consistently outperforms other methods across all sample sizes, followed by NOTEARS and DAGMA, while DAGBagM and mDAG rank lower, with HC and TABU performing the worst.</p>
<p>From the PR plots (Fig. 3A), DAGSLAM consistently appears in the upper-right corner across all sample sizes, achieving both high precision and high recall.At n = 100 , DAGSLAM achieves a directed F1 score of 0.94 and an SHD value of 2.3 (Fig. 3C and E, Table 2).As n increases, its performance improves further, reaching an F1 score close to 1 and an SHD value near 0 at n = 5000 and n = 10000 .Notably, DAGSLAM's performance sta- bilizes when n ≥ 1000 , showing no significant improve- ment with further increases in sample size.</p>
<p>NOTEARS and DAGMA also perform well, particularly in large-sample scenarios ( n = 5000 and n = 10000 ), where their F1 scores approach 0.85 and SHD values remain below 5.0 (Fig. 3C and E, Table 2).However, at smaller sample sizes ( n = 100 and n = 500 ), However, at smaller sample sizes ( n = 100 and n = 500 ), their perfor- mance shows a slight decline, and across all sample sizes, they do not perform as well as DAGSLAM.</p>
<p>In contrast, DAGBagM and mDAG exhibit poor performance at smaller sample sizes, with F1 scores below 0.65 and high SHD values (DAGBagM has an F1 score of 0.63 and SHD of 13.0 at n = 100 , while mDAG has an F1 score of 0.61 and SHD of 9.2) (Fig. 3C and E, Table 2).Although their performance improves as n increases, even at n = 10000 , their F1 scores and SHD values remain significantly worse than those of DAG-SLAM, NOTEARS, and DAGMA.Notably, DAGBagM shows a unique trend where its performance continues to improve slightly even when n ≥ 1000.</p>
<p>HC and TABU struggle the most in low-sample scenarios, with F1 scores below 0.6 and high SHD values at n = 100 (HC has an F1 score of 0.56 and SHD of 13.9, while TABU has an F1 score of 0.56 and SHD of 14.4) (Fig. 3C and E, Table 2).While their performance improves with larger sample sizes, they still lag behind other algorithms at n = 10000.</p>
<p>Levels of categorical nodes</p>
<p>Simulation (iv) includes scenarios 2, 12, and 13, which focus on the impact of varying the levels of categorical nodes m , with values set at {2,3,4} , while maintaining a fixed DAG topology structure.It is important to note that mDAG and DAGBagM cannot handle categorical nodes with more than 2 levels ( m &gt; 2 ), and therefore, these algorithms treat multi-category variables as continuous variables.</p>
<p>The performance of the algorithms varies with the levels of categorical nodes.At m = 2 and m = 3 , DAGSLAM leads the other algorithms, achieving higher F1 scores, TPR, and lower SHD.For instance, at m = 3 , DAGSLAM achieves an F1 score of 0.93 and a TPR of 0.99, which are higher than all other algorithms (Fig. 4C, Table 2).</p>
<p>Interestingly, as the number of levels m increases to 4, the performance of NOTEARS and DAGMA slightly improves, and at this level, NOTEARS and DAGMA's performance matches that of DAGSLAM exactly, both achieving an F1 score of 0.95 and a SHD of 2.0 (Fig. 4C  and E, Table 2).However, DAGSLAM still significantly outperforms the other four algorithms, which continue to show lower F1 scores below 0.70 and higher SHD values around 10.0 (Fig. 4C and E,Table 2).</p>
<p>This observation suggests that when m ≥ 4 , multi- category variables can be treated as continuous variables without a significant loss in performance for the gradient-based algorithms.However, other algorithm's inability to handle categorical variables more than 2 levels limits their applicability in scenarios with higher categorical levels.</p>
<p>Owing to space limitations in the main text, the detailed results of the simulation experiments examining the effects of edge sparsity, weight scale, graph type, and noise distribution are provided in Additional file 2.</p>
<p>Computational time analysis</p>
<p>To better understand the computational limits of our proposed algorithm, we investigated the impact of the number of nodes d and sample size n on computational time.In this analysis, we compare DAGSLAM with two other gradient-based algorithms, NOTEARS and DAGMA.Note that we do not compare these with the other four algorithms implemented in R, as our algorithm is implemented in Python.The simulations were conducted on a machine with the following specifications: AMD Ryzen 5 5500U @ 2.10GHz with 6 cores and 16.0GB of RAM.</p>
<p>The computational time of the algorithms was analysed as a function of the number of nodes d and sample size n , as shown in Figs.5and 6.The specific computational time results are detailed in Table S1 and Table S2 (Additional file 1).</p>
<p>Figure 5 illustrates the computational time for each algorithm with a fixed sample size of n = 1000 while varying the number of nodes d from 20 to 100.As d increases, the computational time for all three algorithms shows a significant upward trend.Specifically, the computational time for both DAGSLAM and NOTEARS increases by approximately two orders of magnitude, reaching around 10 3 s when d = 100 .In contrast, DAG- MA's computational time increases by only one order of magnitude, indicating a more moderate growth compared to the other two algorithms.</p>
<p>Figure 6 presents the computational time with a fixed number of nodes d = 20 while varying the sample size n from 1000 to 10,000.In this scenario, both DAGSLAM and NOTEARS exhibit an increase in computational time by about one order of magnitude.At n = 10000 , the computational time for DAGSLAM is approximately 10 2 s, while DAGMA's computational time remains relatively stable, showing minimal change despite the increase in sample size.</p>
<p>These results highlight that while DAGSLAM's computational time is higher than that of NOTEARS and DAGMA, this is largely due to its design for handling mixed-type data, which requires more complex computations.In contrast, NOTEARS and DAGMA benefit from treating all variables as continuous, allowing for more efficient matrix operations.This analysis underscores the importance of considering both computational efficiency and the specific data characteristics when selecting an appropriate algorithm for causal inference tasks.</p>
<p>Real-world dataset application</p>
<p>We apply our proposed method to the National Health and Nutrition Examination Survey (NHANES) dataset, which is a continuous program that employs sophisticated multistage probability sampling to gather a representative sample of the American population and evaluate the health and nutritional status of people across the United States.The NHANES survey includes demographic, dietary, examination, laboratory and health-related survey data.The NHANES study protocol received approval from the Ethics Review Committee of the National Center for Health Statistics, and all participants provided written informed consent.For further information, please refer to the NHANES-NCHS Research Ethics Review Board Approval page on the website.</p>
<p>We integrated data from 2005-2020, focusing on elderly women aged 65-79 years.The variables of interest and their types are listed in Table 3, comprising a total of 13 variables: 8 continuous variables and 5 categorical variables.After excluding cases with missing data, we obtained a complete dataset of 813 samples.Our objective was to investigate the risk factors for coronary heart disease (CHD) and diabetes, aiming to establish a causal network of the interactions among these risk factors.</p>
<p>The learned DAG structure is illustrated in Fig. 7, with the diseases of interest highlighted in orange.The arrows indicate positive relationships, whereas the diamonds represent negative relationships.The estimated weighted adjacency matrix W is detailed in Table S3 (Additional file 1).The results show that fasting glucose (GLU) serves as a critical starting node, impacting both waist circumference (WAIST) and glycohemoglobin (GHB).Elevated glucose levels are associated with increased waist circumference, reflecting the relationship between insulin resistance and central obesity, which is a known risk factor for metabolic syndrome [38].Higher GHB levels indicate poorer long-term glucose control and a greater risk of diabetes [39].</p>
<p>Elevated levels of GHB promote the development of diabetes, further complicating the metabolic landscape.This relationship highlights the cascading effects of glucose metabolism on diabetes risk.Additionally, alcohol consumption (AL) has been shown to inhibit diabetes, indicating that higher alcohol intake may be associated with a lower risk of developing diabetes in this population.This relationship is complex; moderate alcohol consumption, particularly red wine, has been linked to improved insulin sensitivity and better glucose metabolism, potentially lowering the risk of type 2 diabetes, especially in females [40][41][42].However, these benefits may not apply to everyone, especially those with a family history of diabetes or other health issues [43].</p>
<p>Waist circumference acts as a central node in the DAG, influencing several other factors.It promotes body mass index (BMI), reinforcing the connection between waist size and overall body fat.However, WAIST also inhibits high-density lipoprotein cholesterol (HDL), which is associated with low HDL levels and is a significant risk factor for cardiovascular disease [44].This interplay suggests that as waist circumference increases, the risk of cardiovascular issues may rise due to lower HDL levels.</p>
<p>HDL not only promotes total cholesterol (TCHOL) but also inhibits triglycerides (TGs).This dual role highlights the complexity of lipid metabolism, where HDL is beneficial in managing triglyceride levels while also being part of the broader lipid profile.</p>
<p>TCHOL is positively associated with both low-density lipoprotein (LDL) and TG.Elevated total cholesterol levels typically correlate with increased LDL levels, which is a well-established risk factor for cardiovascular disease [44].The promotion of triglycerides by total cholesterol further underscores the interconnected nature of lipid profiles.</p>
<p>Both alcohol consumption and hypertension (HTN) negatively influence CHD incidence.This finding is intriguing, as it suggests that in this population, higher alcohol intake and hypertension may be associated with a reduced risk of CHD.Moderate intake of alcoholic beverages has been associated with lower coronary heart disease risk due to beneficial components such as polyphenols and resveratrol, which may improve cardiovascular health [45][46][47].However, the definition of moderate drinking varies, typically considered to be up to two drinks per day for men and one drink per day for women.Excessive drinking can lead to numerous health risks, including hypertension and heart disease [48].</p>
<p>Hypertension also inhibits CHD, which may seem counterintuitive given the established link between hypertension and increased cardiovascular risk.This relationship may be explained by the possibility that individuals diagnosed with hypertension become more vigilant in terms of their cardiovascular health.Upon learning of their condition, they may adopt healthier lifestyle choices, such as improved diet, increased physical activity, and adherence to medication regimens, which collectively contribute to a reduced incidence of coronary heart disease.This proactive approach to managing hypertension could mitigate some of the risks typically associated with high blood pressure.and categorical clinical outcomes) and complex, highdimensional structures [16,28].These limitations have hindered their broader application in disease risk factor modelling.</p>
<p>To address these challenges, we propose an extension of the NOTEARS algorithm, referred to as DAGSLAM, which is specifically designed to handle mixed-type data.This novel algorithm integrates continuous and categorical variables into a unified framework by introducing a tailored loss function that accommodates both data types.By building on the conceptual framework of the original NOTEARS, DAGSLAM achieves robust performance across a variety of scenarios, including networks with varying node counts, sample sizes, proportions and levels of categorical variables, variations in edge density, adjustments to the weight scale, different graph types, and diverse noise distributions.Furthermore, the algorithm demonstrates scalability to larger datasets, making it suitable for real-world applications.</p>
<p>In addition to its methodological advancements, DAGSLAM is the first adaptation of the gradient-based algorithm to mixed-type data and its first application in the medical domain for identifying disease risk factors.Through extensive simulations, the algorithm consistently outperforms existing methods, such as HC, TABU, mDAG, and DAGBagM, across key metrics, including precision, recall, F1 score, and SHD.These results highlight its ability to accurately infer both directed and undirected structures, even in challenging scenarios with limited sample sizes or high proportions of categorical variables.By bridging the gap between theoretical advancements in DAG learning and practical applications in epidemiology, DAGSLAM provides a powerful and scalable tool for uncovering causal relationships in complex disease networks.</p>
<p>Performance analysis with mixed-type data</p>
<p>Our simulation results demonstrate that DAGSLAM excels in scenarios with a significant proportion of categorical variables.In scenarios 5, 6, and 7, where the proportion of categorical nodes is set at 10%, 20%, and 50%, respectively, DAGSLAM shows a clear advantage, with its relative performance over other algorithms becoming more pronounced as the proportion of categorical variables increases.This underscores its ability to effectively handle mixed-type data, which is a common feature of real-world biomedical datasets.However, in scenarios with minimal categorical variables (scenarios 3 and 4, with proportions of 2.5% and 1.0%, respectively), the performance of DAGSLAM is comparable to that of NOTEARS and DAGMA.This suggests that when the proportion of categorical variables is below 2.5%, treating all variables as continuous may be a viable alternative.Therefore, we recommend using DAGSLAM when the proportion of categorical variables is ≥ 2.5%, as it provides superior performance in these cases.However, users should be aware that DAGSLAM's computational time is longer compared to NOTEARS and DAGMA, and thus, a trade-off between computational efficiency and performance must be considered based on the specific characteristics of the dataset.Furthermore, when examining the impact of the number of levels of categorical variables ( m ), DAGSLAM demonstrates significant advantages when m = 2 or 3.However, when m ≥ 4 , its performance aligns with that of NOTEARS and DAGMA.From a computational efficiency perspective, it may be more practical to treat multi-category variables as continuous and use NOTEARS or DAGMA in such cases.This flexibility allows users to optimize their choice of algorithm based on the specific structure of their data.</p>
<p>Instance analysis and real-world applications</p>
<p>The application of DAGSLAM to real-world datasets further underscores its utility.The DAG constructed from the NHANES dataset provides a comprehensive overview of the interrelationships among various health factors in elderly women.The findings highlight the importance of considering multiple risk factors when assessing health outcomes according to this demographic.</p>
<p>The positive relationships between fasting glucose, waist circumference, and glycohemoglobin underscore the critical role of metabolic health.The negative influence of waist circumference on HDL levels is particularly concerning, as low HDL is a significant risk factor for cardiovascular disease.</p>
<p>The observed negative relationships between alcohol consumption and CHD and diabetes are noteworthy and suggest that moderate alcohol intake may confer some protective effects.However, our findings should not be seen as an endorsement of alcohol consumption at any level.While there may be potential benefits of alcohol for insulin-sensitive cardiovascular health, these benefits must be carefully balanced against the associated risks [45][46][47].The negative effects of alcohol are directly related to the amount consumed, and it is important to highlight that there is no completely safe level of alcohol intake [49,50].Consulting healthcare professionals before making dietary changes related to alcohol consumption is advisable.</p>
<p>Overall, these causal relationships provide valuable insights into the risk factors affecting the health of elderly women and emphasize the need for targeted interventions to address these interconnected issues.</p>
<p>Limitations and future directions</p>
<p>While DAGSLAM represents a significant advancement in DAG learning for mixed-type data, several limitations remain.One key limitation of DAGSLAM is its computational efficiency.The algorithm's design for handling mixed-type data requires individual vector operations for each column (or node), which inherently increases computational complexity.In contrast, NOTEARS and DAGMA treat all variables as continuous, allowing for efficient matrix operations that significantly reduce computation time.Future research should focus on exploring more efficient optimization methods to improve the computational performance of DAGSLAM.It is worth noting, however, that in our target application scenarios-such as disease risk factor networks-the node count (up to d = 40 ) and sample size (around n = 1000 ) are typically moderate, and the computational time of DAGSLAM remains within an acceptable range.</p>
<p>Additionally, the current implementation of DAG-SLAM assumes linear relationships among variables, which may not fully capture the complexity of certain real-world datasets.Future work could explore the incorporation of nonlinear interactions, potentially through kernel-based methods or neural network architectures, to better model relationships in mixedtype data.This would further enhance the algorithm's ability to handle the intricate dependencies often found in biomedical data.</p>
<p>Moreover, the current algorithm does not support the inclusion of prior knowledge, such as blacklists or whitelists of edges, which could improve performance in scenarios where domain expertise is available.The incorporation of such constraints into the structure learning process is a promising direction for future research.</p>
<p>Finally, while DAGSLAM has shown robustness across various simulation scenarios, its performance in highly dense networks (where S 0 d ≥ 2 ) and with expo- nentially distributed data requires further improvement.Future studies should focus on enhancing the algorithm's effectiveness in these challenging contexts, ensuring that it can be reliably applied to a wider range of epidemiological datasets.</p>
<p>Conclusions</p>
<p>In this study, we present DAGSLAM, a novel extension of the NOTEARS algorithm for learning DAGs from mixedtype data.Through extensive simulations and real-world applications, we demonstrate that DAGSLAM outperforms existing methods in terms of accuracy, scalability, and robustness.By enabling the integration of continuous and categorical variables, DAGSLAM provides a powerful framework for modelling complex disease networks, with significant implications for risk factor identification and public health research.Future work will focus on addressing the algorithm's limitations and further enhancing its capabilities for real-world applications.</p>
<p>Fig. 1
1
Fig. 1 Results for Simulation (i): Comparison of the performance of DAGSLAM, NOTEARS, DAGMA, HC, TABU, mDAG, and DAGBagM with varying numbers of nodes d = 10, 20, 40, 100.A Precision-Recall plot for detecting directed structures.B Precision-Recall plot for detecting skeleton structures.C F1 score for detecting directed structures.D F1 score for detecting skeleton structures.E Structural Hamming Distance (SHD) for detecting directed structures.F SHD for detecting skeleton structures.The results are averaged over 10 replicates for each scenario.The error bars in C), D), E) and F) indicate the standard deviation of each bar (See figure on next page.)</p>
<p>(</p>
<p>See figure on next page.)Fig. 2 Results for Simulation (ii): Comparison of the performance of DAGSLAM, NOTEARS, DAGMA, HC, TABU, mDAG, and DAGBagM with varying number (proportion) of categorical nodes k = 2 (10%), 4 (20%), 10 (50%).A Precision-Recall plot for detecting directed structures.B Precision-Recall plot for detecting skeleton structures.C F1 score for detecting directed structures.D F1 score for detecting skeleton structures.E Structural Hamming Distance (SHD) for detecting directed structures.F SHD for detecting skeleton structures.The results are averaged over 10 replicates for each scenario.The error bars in C), D), E) and F) indicate the standard deviation of each bar</p>
<p>Fig. 3
3
Fig. 3 Results for Simulation (iii): Comparison of the performance of DAGSLAM, NOTEARS, DAGMA, HC, TABU, mDAG, and DAGBagM with varying sample size n = 100, 500, 1000, 5000, 10,000.A Precision-Recall plot for detecting directed structures.B Precision-Recall plot for detecting skeleton structures.C F1 score for detecting directed structures.D F1 score for detecting skeleton structures.E Structural Hamming Distance (SHD) for detecting directed structures.F SHD for detecting skeleton structures.The results are averaged over 10 replicates for each scenario.The error bars in C), D), E) and F) indicate the standard deviation of each bar (See figure on next page.)</p>
<p>(</p>
<p>See figure on next page.)Fig. 4 Results of Simulation (iv): Comparison of the performance of DAGSLAM, NOTEARS, DAGMA, HC, TABU, mDAG, and DAGBagM with varying levels of categorical nodes m = 2, 3, 4. A Precision-Recall plot for detecting directed structures.B Precision-Recall plot for detecting skeleton structures.C F1 score for detecting directed structures.D F1 score for detecting skeleton structures.E Structural Hamming Distance (SHD) for detecting directed structures.F SHD for detecting skeleton structures.The results are averaged over 10 replicates for each scenario.The error bars in C), D), E) and F) indicate the standard deviation of each bar Discussion DAGs have become an essential tool in epidemiological research for modelling causal relationships among disease risk factors.They allow researchers to disentangle direct and indirect effects, providing a clearer understanding of the underlying causal mechanisms [5].However, existing DAG learning methods often struggle with real-world biomedical datasets, which frequently involve mixed data types (e.g., continuous biomarkers</p>
<p>Fig. 5
5
Fig. 5 Computational time (in seconds) as a function of the number of nodes (d) with the sample size (n) fixed at n = 1000.The graph displays the performance of three algorithms: DAGSLAM (blue), NOTEARS (orange), and DAGMA (green) on a logarithmic scale.The computational results are averaged over 10 replicates</p>
<p>Fig. 6
6
Fig. 6 Computational time (in seconds) as a function of sample size (n) with the number of nodes (d) fixed at d = 20.This graph depicts the performance of DAGSLAM (blue), NOTEARS (orange), and DAGMA (green) with the time displayed on a logarithmic scale.The computational results are averaged over 10 replicates</p>
<p>Fig. 7
7
Fig. 7 Learned DAG via the DAGSLAM algorithm for the NHANES dataset.The diseases of interest are highlighted in orange.The arrows indicate positive relationships, whereas the diamonds represent negative relationships.AL: alcohol consumption; BMI: body mass index; CHD: coronary heart disease; DI: diabetes; GLU: fasting glucose; GHB: glycohemoglobin; HDL: high-density lipoprotein cholesterol; HTN: hypertension; LDL: low-density lipoprotein cholesterol; SM: smoking; TCHOL: total cholesterol; TG: triglycerides; WAIST: waist circumference</p>
<p>Table 1
1
Simulation settings for each scenario
Scenariodkmns0/dαGraph typeNoise distribution11012100011ERGauss22012100011ERGauss34012100011ERGauss410012100011ERGauss52010%2100011ERGauss62020%2100011ERGauss72050%2100011ERGauss8201210011ERGauss9201250011ERGauss102012500011ERGauss11201210,00011ERGauss122013100011ERGauss132014100011ERGauss14201210000.51ERGauss152012100021ERGauss162012100010.5ERGauss172012100012ERGauss182012100011SFGauss192012100011ERExp202012100011ERGumbel
d Number of nodes, k Proportion or number of categorical nodes, m Levels of categorical nodes, n Sample size, s0/d Edge sparsity, α Weight scale</p>
<p>Table 2
2
Simulation results for each scenario (averaged over 10 replicates)
ScenarioMethodDirectedSkeletonFDRTPRFPRSHDF1FDRTPRFPRSHDF11DAGSLAM0.000.900.001.00.950.000.900.001.00.95NOTEARS0.000.900.001.00.950.000.900.001.00.95DAGMA0.100.900.031.00.900.001.000.000.01.00HC0.260.850.093.10.790.140.990.051.70.92TABU0.230.820.072.50.790.061.000.020.70.97mDAG0.420.500.118.80.500.110.780.033.20.83DAGBagM0.360.760.134.50.690.171.000.062.10.912DAGSLAM0.000.990.000.30.990.000.990.000.30.99NOTEARS0.150.850.025.10.850.100.900.014.10.90DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.430.650.069.90.610.121.000.022.90.93TABU0.420.660.069.50.620.121.000.022.70.94mDAG0.280.730.0320.10.720.010.990.000.50.99DAGBagM0.350.750.058.70.690.141.000.023.70.923DAGSLAM0.000.970.001.00.990.000.970.001.00.99NOTEARS0.000.980.000.70.990.000.980.000.70.99DAGMA0.001.000.000.01.000.001.000.000.01.00HC0.500.670.0427.60.570.261.000.0214.40.85TABU0.420.770.0322.40.660.251.000.0213.30.86mDAG0.300.660.0237.40.680.020.920.004.00.95DAGBagM0.460.710.0325.20.600.251.000.0213.70.854DAGSLAM0.000.980.001.90.990.000.980.001.70.99NOTEARS0.020.960.004.60.970.010.980.003.00.98DAGMA0.030.990.002.70.980.011.000.001.40.99HC0.570.740.0297.80.550.421.000.0172.30.73TABU0.550.770.0295.40.570.421.000.0172.60.73mDAG0.160.790.0094.90.810.010.940.006.70.97DAGBagM0.580.740.02104.90.540.440.990.0280.70.715DAGSLAM0.000.950.001.00.970.000.950.001.00.97NOTEARS0.180.790.026.20.810.110.870.014.70.88DAGMA0.300.800.047.00.740.131.000.023.00.93HC0.420.650.069.60.610.111.000.022.60.94TABU0.390.680.058.90.640.111.000.012.50.94mDAG0.280.700.036.50.710.020.950.001.40.96DAGBagM0.380.740.0610.00.660.171.000.034.70.906DAGSLAM0.160.800.025.00.820.050.900.013.00.92NOTEARS0.270.700.038.30.710.120.850.015.30.86DAGMA0.470.700.0712.30.600.241.000.046.30.86HC0.320.750.047.10.710.091.000.012.20.95TABU0.310.780.047.20.730.111.000.022.80.94mDAG0.290.700.036.60.710.030.950.001.60.96DAGBagM0.310.810.047.40.750.151.000.023.70.92</p>
<p>Table 2
2
(continued)
ScenarioMethodDirectedSkeletonFDRTPRFPRSHDF1FDRTPRFPRSHDF17DAGSLAM0.090.740.016.10.810.050.770.005.40.85NOTEARS0.250.500.0211.00.600.070.620.018.60.74DAGMA0.530.500.0715.20.470.240.810.039.00.78HC0.360.790.058.90.710.191.000.034.70.90TABU0.370.740.058.90.680.151.000.023.80.91mDAG0.310.700.047.30.690.060.960.012.10.95DAGBagM0.360.760.058.70.690.161.000.024.00.918DAGSLAM0.030.910.002.30.940.020.920.002.10.95NOTEARS0.160.820.025.40.830.090.890.014.10.90DAGMA0.270.890.046.90.800.190.990.034.90.89HC0.490.630.0713.90.560.250.920.048.10.82TABU0.490.640.0814.40.560.270.920.048.80.81mDAG0.320.560.039.20.610.020.800.004.30.88DAGBagM0.430.730.0713.00.630.270.940.048.70.829DAGSLAM0.000.980.000.50.990.000.980.000.50.99NOTEARS0.140.840.025.20.850.100.880.014.50.89DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.440.650.0610.50.600.150.990.023.70.92TABU0.440.670.0610.70.610.160.990.024.20.91mDAG0.340.620.047.80.640.010.930.001.50.96DAGBagM0.430.720.0711.40.620.210.990.035.90.8710DAGSLAM0.000.980.000.50.990.000.980.000.50.99NOTEARS0.150.850.025.00.850.100.900.014.10.90DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.410.660.059.20.620.101.000.012.30.95TABU0.410.660.059.10.620.101.000.012.30.95mDAG0.310.700.046.40.690.011.000.0020.30.99DAGBagM0.260.820.035.90.780.101.000.012.30.9511DAGSLAM0.000.980.000.40.990.000.980.000.40.99NOTEARS0.150.850.024.90.850.100.910.013.90.90DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.400.650.058.50.620.061.000.011.40.97TABU0.390.660.058.40.630.071.000.011.50.96mDAG0.290.710.035.90.710.001.000.0020.11.00DAGBagM0.220.830.034.80.800.061.000.011.40.9712DAGSLAM0.130.990.023.20.930.130.990.023.20.93NOTEARS0.040.840.003.30.900.010.870.002.70.93DAGMA0.120.890.013.80.880.070.930.012.90.93HC0.580.510.0815.80.420.240.920.047.70.83TABU0.470.600.0612.20.540.180.940.025.50.87mDAG0.560.350.0513.80.320.050.760.005.70.84DAGBagM0.490.580.0612.50.530.180.930.025.50.87</p>
<p>Table 2
2
(continued)
ScenarioMethodDirectedSkeletonFDRTPRFPRSHDF1FDRTPRFPRSHDF113DAGSLAM0.000.900.002.00.950.000.900.002.00.95NOTEARS0.000.900.002.00.950.000.900.002.00.95DAGMA0.000.900.002.00.950.000.900.002.00.95HC0.430.650.0610.10.600.130.990.023.20.93TABU0.400.660.059.60.630.130.960.023.60.91mDAG0.430.510.0410.20.520.020.870.002.90.92DAGBagM0.390.730.0510.30.660.200.950.035.90.8714DAGSLAM0.001.000.000.01.000.001.000.000.01.00NOTEARS0.001.000.000.01.000.001.000.000.01.00DAGMA0.001.000.000.01.000.001.000.000.01.00HC0.570.590.047.80.490.261.000.023.70.85TABU0.530.590.046.90.510.201.000.022.80.88mDAG0.370.640.023.70.640.011.000.000.11.00DAGBagM0.420.700.035.20.630.171.000.012.20.9015DAGSLAM0.090.890.025.20.900.020.960.002.30.97NOTEARS0.020.950.011.90.970.000.970.001.10.99DAGMA0.001.000.000.01.000.001.000.000.01.00HC0.560.710.2539.10.540.410.960.1829.00.73TABU0.470.780.2031.50.630.350.960.1524.20.77mDAG0.300.550.0619.80.620.060.740.0112.30.83DAGBagM0.660.590.3658.60.370.510.900.2846.20.6316DAGSLAM0.000.860.002.80.920.000.860.002.80.92NOTEARS0.060.760.014.90.840.000.800.003.90.89DAGMA0.080.790.014.10.850.000.860.002.80.92HC0.560.490.0713.80.420.160.950.024.70.89TABU0.550.510.0713.80.430.170.950.025.00.88mDAG0.350.620.047.80.630.010.950.001.30.97DAGBagM0.440.590.0510.40.570.100.950.013.30.9217DAGSLAM0.000.900.002.10.940.000.900.002.10.94NOTEARS0.110.880.014.60.880.110.880.014.60.88DAGMA0.130.900.024.00.890.100.930.013.40.92HC0.530.560.0713.50.500.190.960.035.50.88TABU0.430.620.0610.50.590.120.960.023.70.91mDAG0.480.400.0412.80.400.050.730.006.10.83DAGBagM0.450.600.0611.00.570.130.950.024.00.9118DAGSLAM0.000.950.001.00.970.000.950.001.00.97NOTEARS0.110.890.013.00.890.050.950.012.00.95DAGMA0.100.940.012.10.920.050.990.011.10.97HC0.200.890.024.20.850.101.000.012.20.95TABU0.190.890.024.00.850.091.000.011.90.95mDAG0.250.770.035.60.760.060.970.011.80.95DAGBagM0.330.780.058.10.710.160.990.024.00.91
matrices of both the true graph and those estimated by the DAGSLAM algorithm for each scenario in Figs.S1-S20 (Additional file 1).</p>
<p>Table 2
2
(continued)
ScenarioMethodDirectedSkeletonFDRTPRFPRSHDF1FDRTPRFPRSHDF119DAGSLAM0.240.800.037.30.780.160.890.025.60.86NOTEARS0.110.830.015.00.860.090.850.014.60.88DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.450.650.0610.60.600.151.000.023.60.92TABU0.420.670.0610.10.620.151.000.023.60.92mDAG0.300.620.037.90.660.020.880.002.80.93DAGBagM0.390.750.0610.50.670.200.990.035.60.8820DAGSLAM0.220.940.035.70.850.180.990.034.70.89NOTEARS0.190.880.024.40.840.090.990.012.30.94DAGMA0.180.900.024.00.860.091.000.012.00.95HC0.400.660.058.90.630.091.000.012.10.95TABU0.380.680.058.40.640.091.000.011.90.96mDAG0.260.730.035.80.730.020.970.001.00.97DAGBagM0.380.730.069.60.660.161.000.024.20.91</p>
<p>Table 3
3
Descriptions of the variables in the real-world dataset analysis
AbbreviationTypeDescriptionALCategoricalHad at least 12 alcohol drinks in lifetime 1: YES, 0: NOBMIContinuousBody Mass Index (kg/m 2 )CHDCategoricalDoctor ever told had coronary heart disease 1: YES, 0: NODICategoricalDoctor told you have diabetes 1: YES, 0: NOGLUContinuousFasting Glucose (mg/dL)GHBContinuousGlycohemoglobin (%)HDLContinuousDirect High-Density Lipoprotein Cholesterol (mg/dL)HTNCategoricalDoctor ever told you had high blood pressure 1: YES, 0: NOLDLContinuousLow-Density Lipoproteins Cholesterol, Friedewald equation (mg/dL)SMCategoricalSmoked at least 100 cigarettes in life 1: YES, 0: NOTCHOLContinuousTotal Cholesterol (mg/dL)TGContinuousTriglycerides, refrigerated serum (mg/dL)WAISTContinuousWaist Circumference (cm)
AcknowledgementsThe authors would like to express their sincere gratitude to Xun Zheng et al. for their pioneering work on the NOTEARS algorithm, which provided valuable inspiration for this study.Additionally, the authors extend their thanks to the creators of the DAGMA algorithm for their innovative contributions, particularly the log-determinant acyclicity constraint, which has significantly influenced the development of our proposed method.The authors also wish to thank the National Health and Nutrition Examination Survey (NHANES) for making their public dataset available, which enabled the case study analysis presented in this work.Data availabilityThe datasets generated and used for the simulation experiments in this study, as well as the DAGSLAM algorithm implementation code, simulation experiment code, and detailed result files, are all available in the GitHub repository at https:// github.com/ yuany uan-zhao-pku/ DAGSL AM.The data used for the case study analysis in this work can be freely downloaded from the official website of the National Health and Nutrition Examination Survey (NHANES) at NHANES -National Health and Nutrition Examination Survey Homepage.FundingThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.AbbreviationsBNsSupplementary InformationThe online version contains supplementary material available at https:// doi.org/ 10. 1186/ s12874-025-02582-6.Additional file 1: Supplementary tables and figuresAdditional file 2: Supplementary resultsAuthors' contributionsYuanyuan Zhao: Conceptualization, Data analysis and Visualization, Writingoriginal draft, Writing -review &amp; editing.Jinzhu jia: Conceptualization, Supervision, Writing -review &amp; editing.All the authors have read and approved the final manuscript.DeclarationsEthics approval and consent to participate Not applicable.Consent for publicationNot applicable.Competing interestsThe authors declare no competing interests.Publisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Global burden of 369 diseases and injuries in 204 countries and territories, 1990-2019: a systematic analysis for the Global Burden of Disease Study. GBD 2019 Diseases and Injuries Collaborators. London, England2019. 2020. 10258396</p>
<p>Heart disease and stroke statistics-2023 update: a report from the. C W Tsao, A W Aday, Z I Almarzooq, Cam Anderson, P Arora, C L Avery, C M Baker-Smith, A Z Beaton, A K Boehme, A E Buxton, American Heart Association. Circulation. 14782023</p>
<p>Inflammation as a link between obesity, metabolic syndrome and type 2 diabetes. N Esser, S Legrand-Poels, J Piette, A J Scheen, N Paquot, Diabetes Res Clin Pract. 10522014</p>
<p>Applied logistic regression. Dwh Jr, S Lemeshow, R X Sturdivant, 20133rd Edition</p>
<p>Causal diagrams for epidemiologic research. S Greenland, J Pearl, J M Robins, Epidemiology. 1011999</p>
<p>Causality: models, reasoning, and inference. L G Neuberg, Econ Theory. 1942003</p>
<p>Graphical models. S L Lauritzen, 1996OxfordOxford University</p>
<p>Causation, prediction, and search. P Spirtes, C Glymour, R Scheines, 1993</p>
<p>Bayesian network analysis of factors influencing type 2 diabetes, coronary heart disease, and their comorbidities. D Kong, R Chen, Y Chen, L Zhao, R Huang, L Luo, F Lai, Z Yang, S Wang, J Zhang, BMC Public Health. 24112672024</p>
<p>Network-based approaches for modeling disease regulation and progression. G Galindez, S Sadegh, J Baumbach, T Kacprowski, M List, Comput Struct Biotechnol J. 212023</p>
<p>Bayesian networks for the diagnosis and prognosis of diseases: a scoping review. K Polotskaya, Muñoz- Valencia, C S Rabasa, A Quesada-Rico, J A Orozco-Beltrán, D Barber, X , Mach Learn Knowl Extr. 622024</p>
<p>Probabilistic graphical models: principles and techniques -adaptive computation and machine learning. D Koller, N Friedman, 2009The MIT PressCambridge</p>
<p>A tutorial on learning with Bayesian networks. D Heckerman, Innovations in Bayesian networks: theory and applications. edn. Berlin, Heidelberg. Berlin HeidelbergSpringer2008</p>
<p>Risk assessment and decision analysis with Bayesian networks. N Fenton, M Neil, 2012CRC PressBoca Raton</p>
<p>Optimal structure identification with greedy search. M D Chickering, J Mach Learn Res. 32003</p>
<p>The max-min hill-climbing Bayesian network structure learning algorithm. I Tsamardinos, L E Brown, C F Aliferis, Mach Learn. 652006</p>
<p>Learning Bayesian networks with the bnlearn R package. M Scutari, J Stat Softw. 352009</p>
<p>Ancestral causal inference. S Magliacane, T Claassen, J M Mooij, Proceedings of the 30th International Conference on Neural Information Processing Systems. the 30th International Conference on Neural Information Processing SystemsBarcelona, SpainCurran Associates Inc2016</p>
<p>MIxBN: library for learning Bayesian networks from mixed data. A V Bubnova, I Deeva, A V Kalyuzhnaya, Procedia Comput Sci. 1932021</p>
<p>DAGBagM: learning directed acyclic graphs of mixed variables with an application to identify prognostic protein biomarkers in ovarian cancer. S Chowdhury, R Wang, Q Yu, C J Huntoon, L M Karnitz, S H Kaufmann, S P Gygi, M J Birrer, A G Paulovich, J Peng, BMC Bioinformatics. 233212020</p>
<p>Inferring regulatory networks from mixed observational data using directed acyclic graphs. W Zhong, L Dong, T B Poston, T Darville, C N Spracklen, D Wu, K L Mohlke, Y Li, Q Li, X Zheng, Front Genet. 1182020</p>
<p>Bayesian networks in R. R Nagarajan, M Scutari, S Lèbre, 2013</p>
<p>Theory refinement on Bayesian networks. W L Buntine, Conference on Uncertainty in artificial intelligence. 1991. 1991</p>
<p>Causal networks: semantics and expressiveness. T Verma, J Pearl, Conference on uncertainty in Artificial Intelligence. 2013. 2013</p>
<p>Order-independent constraint-based causal structure learning. D Colombo, M H Maathuis, Proc 30th Int Conf Sci Stat Database Manag. 30th Int Conf Sci Stat Database Manag201215</p>
<p>Learning mixed graphical models with separate sparsity parameters and stability-based model selection. A J Sedgewick, I W Shi, R M Donovan, P V Benos, BMC Bioinformatics. 172016</p>
<p>Mixed graphical models for causal analysis of multi-modal variables. A J Sedgewick, J Ramsey, P Spirtes, C Glymour, P V Benos, 2017</p>
<p>DAGs with NO TEARS: continuous optimization for structure learning. X Zheng, B Aragam, P Ravikumar, E P Xing, Neural information processing systems. 2018. 2018</p>
<p>Bayesian network structure learning: a review. C He, W Liu, J Ren, 2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT. Dec. 2022. 20222022</p>
<p>Estimating the dimension of a model. G Schwarz, Ann Stat. 61978</p>
<p>DAG structure learning with graph neural networks. Y Yu, J Chen, T Gao, M Yu, Dag-Gnn, International conference on machine learning. 2019. 2019</p>
<p>S Lachapelle, P Brouillard, T Deleu, Sja Lacoste-Julien, Gradient-based neural DAG learning. 2019</p>
<p>Scaling structural learning with NO-BEARS to infer causal transcriptome networks. H C Lee, M Danieletto, R Miotto, S Cherng, J T Dudley, Pac Symp Biocomput Pac Symp Biocomput. 252019</p>
<p>Learning sparse nonparametric DAGs. X Zheng, C Dan, B Aragam, P Ravikumar, E P Xing, International conference on artificial intelligence and statistics. 2019. 2019</p>
<p>DAGMA: learning DAGs via M-matrices and a log-determinant acyclicity characterization. K Bello, B Aragam, P Ravikumar, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing SystemsNew Orleans, LA, USACurran Associates Inc2022598</p>
<p>GFlowNet foundations. Y Bengio, S Lahlou, T Deleu, E J Hu, M Tiwari, E Bengio, J Mach Learn Res. 242102023</p>
<p>Penalty and augmented Lagrangian methods. J Nocedal, S J Wright, Numerical optimization. J Nocedal, S J Wright, New YorkSpringer20062nd ed</p>
<p>Diagnosis and classification of diabetes mellitus. Diabetes Care. 3712014American Diabetes AssociationSuppl</p>
<p>Pathophysiology and treatment of type 2 diabetes: perspectives on the past, present, and future. S E Kahn, M E Cooper, Del Prato, S , Lancet. 38399222014</p>
<p>Moderate alcohol consumption lowers the risk of type 2 diabetes: a meta-analysis of prospective observational studies. L L Koppes, J M Dekker, H F Hendriks, L M Bouter, R J Heine, Diabetes Care. 2832005</p>
<p>Effects of moderate alcohol intake on fasting insulin and glucose concentrations and insulin sensitivity in postmenopausal women: a randomized controlled trial. M J Davies, D J Baer, J T Judd, E D Brown, W S Campbell, P R Taylor, JAMA. 287192002</p>
<p>The relationship between alcohol consumption, BMI, and type 2 diabetes: a systematic review and doseresponse meta-analysis. L Llamosas-Falcón, J Rehm, S Bright, C Buckley, T Carr, C Kilian, A M Lasserre, J M Lemp, Y Zhu, C Probst, Diabetes Care. 46112023</p>
<p>Alcohol consumption and the risk of type 2 diabetes: a systematic review and dose-response meta-analysis of more than 1.9 million individuals from 38 observational studies. C Knott, S Bell, A Britton, Diabetes Care. 3892015</p>
<p>Diagnosis and management of the metabolic syndrome: an American Heart Association/ National Heart, Lung, and Blood Institute Scientific Statement. Circulation. S M Grundy, J I Cleeman, S R Daniels, K A Donato, R H Eckel, B A Franklin, D J Gordon, R M Krauss, P J Savage, Smith ScJr, 2005112</p>
<p>Beer, wine consumption, and 10-year CVD incidence: the ATTICA study. D B Panagiotakos, G M Kouli, E Magriplis, I Kyrou, E N Georgousopoulou, C Chrysohoou, C Tsigos, D Tousoulis, C Pitsavos, Eur J Clin Nutr. 7372019</p>
<p>Wine and cardiovascular health: a comprehensive review. S Haseeb, B Alexander, A Baranchuk, Circulation. 136152017</p>
<p>Wine, beer, alcohol and polyphenols on cardiovascular disease and cancer. S Arranz, G Chiva-Blanch, P Valderas-Martínez, A Medina-Remón, R M Lamuela-Raventós, R Estruch, Nutrients. 472012</p>
<p>Alcohol intake and risk of hypertension: a systematic review Zhao and Jia BMC Medical Research Methodology (2025) 25:154 and dose-response meta-analysis of nonexperimental cohort studies. M Cecchini, T Filippini, P K Whelton, I Iamandii, Di Federico, S Boriani, G Vinceti, M , Hypertension. 8181979. 2024</p>
<p>No level of alcohol consumption improves health. R Burton, N Sheron, Lancet. 3922018. 10152</p>
<p>The relationship between different dimensions of alcohol use and the burden of disease-an update. J Rehm, G E Gmel, Sr, G Gmel, Osm Hasan, S Imtiaz, S Popova, C Probst, M Roerecke, R Room, A V Samokhvalov, Addiction. 11262017</p>            </div>
        </div>

    </div>
</body>
</html>
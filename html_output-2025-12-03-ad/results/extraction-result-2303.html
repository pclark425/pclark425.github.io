<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2303 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2303</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2303</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-270562871</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11882v1.pdf" target="_blank">Applications of Explainable artificial intelligence in Earth system science</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, artificial intelligence (AI) rapidly accelerated its influence and is expected to promote the development of Earth system science (ESS) if properly harnessed. In application of AI to ESS, a significant hurdle lies in the interpretability conundrum, an inherent problem of black-box nature arising from the complexity of AI algorithms. To address this, explainable AI (XAI) offers a set of powerful tools that make the models more transparent. The purpose of this review is twofold: First, to provide ESS scholars, especially newcomers, with a foundational understanding of XAI, serving as a primer to inspire future research advances; second, to encourage ESS professionals to embrace the benefits of AI, free from preconceived biases due to its lack of interpretability. We begin with elucidating the concept of XAI, along with typical methods. We then delve into a review of XAI applications in the ESS literature, highlighting the important role that XAI has played in facilitating communication with AI model decisions, improving model diagnosis, and uncovering scientific insights. We identify four significant challenges that XAI faces within the ESS, and propose solutions. Furthermore, we provide a comprehensive illustration of multifaceted perspectives. Given the unique challenges in ESS, an interpretable hybrid approach that seamlessly integrates AI with domain-specific knowledge appears to be a promising way to enhance the utility of AI in ESS. A visionary outlook for ESS envisions a harmonious blend where process-based models govern the known, AI models explore the unknown, and XAI bridges the gap by providing explanations.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2303.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2303.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic, additive feature-attribution XAI method that uses Shapley values from cooperative game theory to assign marginal contributions to input features for individual predictions; widely applied in ESS for tabular feature importance and attribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Unified Approach to Interpreting Model Predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Earth system science (multiple subdomains: hydrology, hazards, climatology, ecology, remote sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Attributing drivers and quantifying feature contributions in prediction models (e.g., floods, wildfire susceptibility, evapotranspiration, streamflow, crop yield) to improve interpretability and discover physical insights from ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by application; typically moderate-to-abundant tabular datasets assembled from observations and reanalyses (many studies operate on labeled, structured tabular datasets); review notes availability of large observation and synthesized datasets but also highlights places with limited ground truth for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data (common), sometimes used with features derived from spatial/temporal data (multimodal when combined with remote-sensing-derived variables).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High nonlinearity and multi-variable couplings (high dimensional feature spaces in many ESS problems); interactions among correlated drivers common; model outputs often depend on complex spatio-temporal processes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in classical process-based modeling but emergent for ML/XAI workflows; substantial prior physical knowledge exists but data-driven insights are still being integrated into domain practice.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — users (process modelers, policymakers) demand interpretable, physically consistent explanations to trust model outputs and to derive scientific hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>SHAP (model-agnostic Shapley value explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computes marginal contribution of each feature to a prediction by considering permutations/coalitions of features; often used post-hoc on trained supervised models (RF, GBM, NN); includes TreeSHAP and DeepSHAP variants optimized for tree models and deep networks respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>model-agnostic post-hoc explainability (applied to supervised learning models)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to tabular ESS problems for feature-ranking and local explanations; flexible across model families, though computational cost and assumptions (additivity) can limit use on very high-dimensional inputs without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported in the review as one of the most popular XAI methods in ESS; noted to have relatively high faithfulness for tabular data and widely used to identify driver importance in floods, wildfire, evapotranspiration and other tasks; however, limitations include computational cost and sensitivity to feature correlations and choice of background distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — provides interpretable attributions enabling scientific attribution studies, feature selection, and policy-relevant explanations; has been used to guide model choice and reveal physically plausible drivers across multiple ESS problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively in the review: SHAP shown to be more advantageous for tabular data (higher faithfulness) while gradient-based methods (IG, LRP) were reported as more valid for image-based tasks; permutation-based PI and TFI are alternatives often used for tree models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Model-agnostic flexibility, clear additive attribution framing (Shapley axioms), and availability of optimized implementations (TreeSHAP, DeepSHAP) made SHAP widely adoptable; success depends on sufficient labeled data and careful baseline/background choices.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>SHAP's game-theoretic, additive attributions make it well-suited for explaining tabular ESS prediction models, yielding physically interpretable driver rankings, though care is required for correlated features and baseline selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2303.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Interpretable Model-agnostic Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic local surrogate explanation method that perturbs an instance and fits a simple interpretable model (e.g., linear) to approximate local decision boundaries, used in ESS for instance-level explanations and threshold discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Why Should I Trust You?: Explaining the Predictions of Any Classifier</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hydrology, drought/evapotranspiration studies, remote sensing applications</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Providing local (instance-level) explanations for predictions such as drought occurrence, evapotranspiration thresholds, and image-based classifications to support forecasters and scientists in understanding factors leading to specific outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically moderate: labeled time-series or tabular datasets derived from observations or reanalysis; some studies highlight scarcity in ground-truth labels for certain phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Tabular features and time series; also applied where input instances may be image patches (converted to interpretable superpixels).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Locally complex nonlinear decision boundaries; global model complexity can be high but LIME focuses on local linear approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>ESS domain has established process knowledge but local, instance-level explanation needs are emergent among forecasters and modelers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — local, actionable evidence is needed by forecasters and stakeholders to validate individual predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>LIME (local surrogate models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Perturbs input around a target instance to build synthetic neighboring samples, queries the black-box model for outputs, and fits a simple interpretable model (e.g., weighted linear) to approximate local behavior and produce feature importances or rules.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>model-agnostic post-hoc explainability for supervised models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for providing local, human-understandable explanations in ESS tasks; appropriate when practitioners need instance-specific rationales, but surrogate fidelity and perturbation scheme must be designed carefully for realistic physical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Used in ESS to indicate thresholds (e.g., LIME indicated thresholds for climate variables affecting evapotranspiration per Chakraborty et al., 2021b) and for local drought/flood instance explanations; vulnerable to instability and choice of perturbation neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — strong utility for forecasters and model developers seeking local justifications, but care required to avoid misleading local surrogates when inputs are highly structured or correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to SHAP and permutation methods; LIME provides local surrogate linear explanations while SHAP offers theoretically grounded attributions; LIME is simpler but can be less robust.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simplicity, direct local interpretability and ease of implementation drive uptake; success depends on realistic perturbation strategies and evaluation of surrogate fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>LIME is useful for instance-level interpretability in ESS, especially to surface local thresholds and contributors, but its reliability depends on careful design of perturbations and verification against domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2303.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Permutation Importance (PI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Permutation Feature Importance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A perturbation-based model-agnostic XAI method that evaluates feature importance by randomly permuting a feature's values and measuring the degradation in model performance, commonly used in ESS for global importance ranking and sensitivity analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hydrology, flood forecasting, evapotranspiration, general ESS predictive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Assessing which input variables most strongly affect model predictions (e.g., streamflow, precipitation classification) and using that information for feature selection, diagnosis, and physical consistency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often built on moderate-to-large labeled tabular datasets from observation networks and reanalysis; availability varies by region and variable.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular and derived feature sets; sometimes applied to features aggregated from spatio-temporal inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Nonlinear models with interacting and correlated predictors; PI is sensitive to correlations and can misattribute importance in such cases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>ESS has established sensitivity analysis traditions; PI connects well with such practices but needs caution for correlated features.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — practitioners use PI both to improve models and to seek scientific insights into driver importance.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Permutation Importance</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>For each candidate feature, shuffle its values across the dataset while keeping others fixed, compute the change in model prediction error (e.g., increase in RMSE), and rank features by the performance degradation caused by permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>model-agnostic perturbation-based explainability used with supervised models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable to supervised tabular ML models; straightforward to implement but limited when features are highly correlated or when temporal dependence exists without respecting time structure during permutation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Valued for intuitive link to sensitivity analysis; used in multiple ESS studies (e.g., flood forecasting comparisons) to reveal model-preferred predictors; caveats include biases with correlated predictors and computational cost for large models/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate to high for model diagnosis and feature selection; helps reduce overfitting and align model behavior with physical expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Permutation importance is compared to PDP/ALE for effect visualization and to SHAP for instance-level attributions; ALE is suggested as an alternative for handling correlated features.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct connection to predictive performance, simplicity, and interpretability; success depends on proper permutation strategy and accounting for feature correlations/temporal dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Permutation Importance provides an intuitive, performance-based measure of global feature relevance in ESS models but must be applied carefully when predictors are correlated or when temporal/spatial structure exists.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2303.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-based feature importance (TFI) / Treeinterpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-based Feature Importance (TFI) and TreeInterpreter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods tailored for tree-ensemble models (RF, gradient boosting) that attribute importance by impurity reduction aggregated across trees (TFI) and extract per-instance pathwise contributions (TreeInterpreter) for granular explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Hydrology, precipitation classification, evapotranspiration, remote sensing classification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identifying influential predictors and diagnosing problematic decision paths in tree-based models used for forecasting and classification (e.g., precipitation types, runoff drivers).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically moderate to abundant tabular datasets; many ESS problems have rich observational archives enabling tree-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular data, features derived from spatio-temporal observations and indices.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High nonlinearity and interaction effects captured by tree ensembles; interpretability easier than deep nets but still complex across many trees.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Tree models are mature and widely used in ESS; interpretability tools are well integrated into workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — modelers use TFI and TreeInterpreter to align ML behavior with hydrological and atmospheric process knowledge and for feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>TFI / TreeInterpreter</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>TFI computes feature relevance via aggregated impurity reduction (or split gain) across ensemble trees; TreeInterpreter walks decision paths for a given instance to compute contribution of each feature to the final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>model-specific explainability for supervised tree-based models</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to ensemble tree models common in ESS; computationally efficient variants exist and they produce both global and instance-level explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Used effectively for feature selection and diagnosis (e.g., Feng et al., Upadhyaya et al.); provides physically consistent feature rankings in some studies and helps identify problematic tree nodes causing misclassification.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for improving tree-model reliability and interpretability in ESS, facilitating feature selection and physical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>TFI is often favored when using tree models compared to model-agnostic PI for efficiency; SHAP TreeSHAP provides theoretically grounded per-instance attributions as an alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct alignment with tree model internals and computational efficiency; success hinges on model quality and avoiding overinterpretation of impurity-based metrics when features are correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Tree-specific interpretability methods give granular, computationally efficient explanations for ensemble models in ESS, enabling model diagnosis and physically consistent feature selection when applied with care.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2303.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gradient-based XAI / LRP / IG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-based attribution methods (saliency maps, Integrated Gradients, Layer-wise Relevance Propagation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that use backpropagated gradients or layer-wise relevance propagation to produce spatially resolved attributions (saliency maps) for deep networks, particularly effective for image and gridded spatio-temporal ESS inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Atmospheric science, remote sensing, oceanography, hazard mapping (e.g., hail, wildfire maps), spatio-temporal field prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Explaining which spatial regions/pixels or input fields drive convolutional or other deep model predictions for imaging tasks and gridded climate/meteorological forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often abundant gridded remote sensing and reanalysis data, but labeled event occurrences (e.g., hail) may be scarcer; image datasets are common in many ESS analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured/structured images and gridded spatio-temporal arrays; high-dimensional pixel/feature spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high dimensional, spatially correlated, nonlinear mapping from inputs to outputs; long-range dependencies can exist.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Remote sensing and atmospheric imaging are mature in data availability; deep-learning adoption is rapidly increasing and XAI methods for images are actively evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — spatial attributions are used to validate physical consistency and to communicate to forecasters and scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gradient-based attribution methods (saliency, IG, LRP, Grad-CAM)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Compute gradients of outputs w.r.t. inputs (saliency) or integrate gradients along a path from a baseline (Integrated Gradients) to yield pixel-level attributions; LRP redistributes output relevance back to pixels; Grad-CAM uses gradients to weight activation maps for coarse localization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>model-specific post-hoc explainability for deep supervised models (CNNs, Conv-LSTMs)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for image-based ESS tasks to reveal spatial drivers; requires differentiable models and sensible baselines; partial interpretability (e.g., attention maps) can be coarse and susceptible to instability.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Review notes IG and LRP as more valid for image tasks while Grad-CAM variants provide coarse localization; these methods have been used to interpret precipitation, hail, sea temperature and related tasks, but sensitivity and reliability vary.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for validating spatial predictions, diagnosing models, and communicating results to domain experts; can identify spatial patterns and inform process understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with SHAP and permutation methods (better for tabular), gradient methods perform better for image/gridded data according to cited evaluations; however, robustness and faithfulness issues remain.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of differentiable deep models, meaningful baselines, and domain-appropriate evaluation datasets; success depends on assessing robustness and stability of attribution maps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Gradient-based attribution methods are the most effective XAI tools for image/gridded ESS problems, enabling spatial localization of drivers, but require careful baseline selection and robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2303.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention / Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention mechanisms and Transformer architectures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep-learning components (attention) and Transformer models that dynamically weight inputs to capture long-range dependencies and provide interpretable attention maps; used in ESS for spatio-temporal forecasting and interpretable flood/temperature modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention Is All You Need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Flood forecasting, subseasonal-to-seasonal precipitation/temperature forecasting, spatio-temporal environmental prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Modeling long-range dependencies in sequences and gridded data (e.g., teleconnections, circulation patterns) and providing interpretable attention scores that indicate influential inputs over long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large historical sequences from observations and reanalyses are available for many ESS tasks, enabling Transformer training where labeled targets exist.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series and spatio-temporal sequences; sometimes combined with spatial feature channels (multimodal sequence inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High temporal complexity (long-range correlations), high dimensionality when combining spatial channels; computationally expensive to train at high resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Transformers are relatively recent in ESS but rapidly adopted; domain knowledge on long-range drivers exists and is being combined with attention for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high — attention maps are used as partial explanations but domain experts often require additional validation since attention is only partially interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Attention mechanisms / Transformer networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Self-attention layers compute pairwise interactions among sequence elements to produce context-aware representations; Transformers stack attention and feed-forward layers; attention weights can be visualized as importance maps over inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised/sequence modeling with model-specific (attention) interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to long-range dependent ESS problems and beneficial where capturing teleconnections or lengthy histories matters; interpretability of attention is partial and must be complemented by XAI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Used in ESS (e.g., Transformer-based flood forecasting papers cited) to capture long-range dependencies and provide interpretable attention; review cautions that attention is only partially interpretable and needs enhancement (heuristic propagation, symbolic abstraction).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential for improved forecasts and for revealing temporal drivers across long windows, provided attention explanations are validated against physics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to RNNs (LSTM/Conv-LSTM) which handle sequences but less efficiently for long-range dependencies; Transformers often provide improved modeling of long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to leverage large amount of sequential data, architectural capacity to capture long dependencies, and combining attention visualization with XAI validation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Transformers and attention mechanisms are promising for ESS sequence problems because they capture long-range dependencies and offer interpretable attention maps, but attention must be validated and supplemented to yield trustworthy scientific explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2303.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variational Autoencoder (VAE) / Latent representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoders and latent-space analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unsupervised deep models (VAEs) that compress high-dimensional inputs into a lower-dimensional latent space whose structure can be analyzed with XAI to reveal regimes or modes (e.g., convective regimes) in climate and atmospheric data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Atmospheric science (convective regimes), climatology, oceanography (dynamical region identification)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discovering coherent regimes or patterns in high-dimensional climate and atmospheric fields (e.g., precipitation generation regimes, ocean dynamical regions) that aid conceptual model building.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large gridded datasets (model output, remote sensing, reanalysis) often available; unsupervised approaches exploit abundant unlabeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>High-dimensional gridded images/fields, spatio-temporal datasets; multimodal possibilities when combining variables.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High dimensionality and nonlinearity with complex spatio-temporal patterns; latent spaces seek to capture essential degrees of freedom.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Use of unsupervised representation learning in ESS is emergent; process knowledge exists that can be used to interpret latent dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific insight — latent variables are interrogated with XAI to map to physical regimes and aid hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Variational Autoencoders (VAE) and latent-space analysis</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train encoder-decoder networks to map inputs to a probabilistic latent space and reconstruct inputs; analyze latent coordinates (clustering, attribution) to identify regimes or interpretable factors (e.g., different convective regimes detected by VAE latent variables).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised representation learning with post-hoc interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to exploratory science where labeled targets are scarce and where researchers seek emergent regimes; requires large volumes of data for stable latent representations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited examples (Behrens et al., 2022) show VAEs revealing convective regimes and facilitating physical interpretation; latent representations can yield novel conceptualizations of system behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific discovery and building conceptual models (e.g., neural ESM concepts), enabling reduction of complex fields into interpretable modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly to clustering and linear dimensionality reduction; VAEs capture nonlinear manifolds and thus can reveal richer regimes than linear methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large gridded datasets, careful architecture/regularization choices, and post-hoc mapping of latent features to physical variables and processes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>VAEs and latent-space XAI enable discovery of emergent regimes in high-dimensional ESS data, offering a pathway to translate complex fields into interpretable process-level insights when sufficient data exist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2303.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conv-LSTM / LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional Long Short-Term Memory networks (Conv-LSTM) and LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent deep architectures (LSTM and Conv-LSTM) tailored for sequential and spatio-temporal ESS tasks; combined with XAI (gradient-based or surrogate analyses) to interpret sequence importance and temporal drivers for variables like soil moisture and hydrological flows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Soil moisture prediction, regional hydrological modeling, streamflow forecasting, spatio-temporal environmental prediction</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predicting time-evolving geophysical variables (soil moisture, streamflow) from multi-channel spatio-temporal inputs and interpreting which time windows and spatial features matter for predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate to abundant time-series and gridded observational/reanalysis data; in situ measurements used to train and validate models (e.g., 1 km soil moisture datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal sequences (multichannel grids over time); high-dimensional and temporally correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: temporal dependencies, spatial correlations, nonlinearity, multi-scale interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>LSTM-based methods have seen growing adoption in hydrology and soil moisture prediction; domain expertise used to assess physical plausibility of learned temporal patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — interpretability of temporal drivers and thresholds (sequence importance) is important for hydrological science and applications.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>LSTM / Conv-LSTM (recurrent and convolutional-recurrent networks)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>LSTM cells capture temporal dependencies; Conv-LSTM extends LSTM to spatial grids by replacing linear transforms with convolutions; XAI for these models includes gradient-based attribution, sequence importance, probing latent states, and surrogate models to reveal time windows and sequences critical for predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised sequence/spatio-temporal deep learning with model-specific interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for spatio-temporal ESS forecasting where temporal context and local spatial patterns matter; requires adequate training sequences and attention to overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported uses include soil moisture prediction (Huang et al., 2023b) where Conv-LSTM with XAI improved interpretability and predictive accuracy; XAI methods expose important time periods and spatial features informing process understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for operational forecasting and scientific insight into temporal propagation of hydrological signals; enables hypothesis generation about time-lagged drivers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to classical LSTM and non-sequential models; Conv-LSTM integrates spatial structure better than vanilla LSTM and often yields improved spatio-temporal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of dense spatio-temporal training data, appropriate architecture choices to capture spatial locality, and use of XAI (sequence importance, probing) to validate learned temporal dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Conv-LSTM architectures paired with XAI reveal temporally localized drivers and spatial patterns in hydrological and soil-moisture problems, improving both prediction and scientific interpretability when trained on sufficient spatio-temporal data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2303.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-informed / hybrid models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-informed neural networks and hybrid physics-ML models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid approaches that integrate process-based (mechanistic) models or physical constraints into ML architectures (e.g., physics-guided neural networks, physics-informed losses) to improve physical consistency, interpretability, and generalization in ESS.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Earth system modeling, hydrology, oceanography, atmospheric process emulation and subgrid parameterization</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Bridging data-driven model flexibility with process-based mechanistic understanding to ensure models obey known physics, improve extrapolation, and yield interpretable, causally consistent insights.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Hybrid models leverage both observational data and process-model outputs (e.g., ESM runs); availability varies but often sufficient for constrained learning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Multimodal: model state variables, gridded fields, and tabular forcings; high-dimensional structured inputs with physical interdependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: coupled nonlinear PDE-driven processes, multi-scale interactions, and limited observability of some state variables.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>The use of physics-informed ML in ESS is emergent and identified in the review as a promising pathway; process-based models are longstanding in the field.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Very high — demanding for scientific trust and regulatory acceptability; hybrid methods aim to preserve mechanistic interpretability while leveraging data-driven components.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-guided neural networks / physics-informed ML (hybrid models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Incorporate physical constraints into architecture or loss (e.g., conservation laws, known process relationships), combine ML components with explicit process-based modules, or use ML to learn subgrid parameterizations guided by physical theory.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid / physics-informed supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Especially suitable when strong prior mechanistic knowledge exists and extrapolation/generalization beyond training regimes is needed; helps align ML outputs with physical expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Review emphasizes hybrid physics–ML as promising for ESS to resolve compatibility issues between XAI and domain knowledge, increase faithfulness and generalizability, and enable interpretable scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can improve trust and adoption of ML in ESS, provide physically consistent insights, and aid incorporation of ML into ESMs and operational systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with purely data-driven black-box models which may be higher-performing in-sample but lack physical constraints and interpretability; hybrid approaches aim to get the best of both.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality of physical priors, correct integration scheme (architecture/loss), and sufficient data to train data-driven components without violating constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Physics-informed and hybrid ML models are a crucial strategy in ESS to combine predictive power with mechanistic interpretability, addressing domain demands for physically consistent, generalizable AI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2303.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2303.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XAI evaluation benchmarks (Mamalakis et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic benchmark datasets and evaluation frameworks for XAI in geoscience</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark datasets and synthetic experiments developed to provide ground truth for evaluating the faithfulness, robustness, and stability of XAI methods in geoscience contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural network attribution methods for problems in geoscience: A novel synthetic benchmark dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Methodology for XAI evaluation across geoscience applications (atmosphere, climate, remote sensing)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Providing ground-truthable testbeds to quantitatively evaluate whether XAI methods correctly identify known drivers and produce faithful explanations in geoscientific ML applications.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Benchmark datasets are synthetic or semi-synthetic to embed known causal relationships; actual observational ground truth often lacking in real-world ESS datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Synthetic tabular and gridded datasets designed to mimic ESS data structure with controlled causal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Designed to cover nonlinear, interacting driver scenarios of varying complexity to stress-test XAI methods.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>XAI evaluation in ESS is nascent; benchmarks are emerging to fill the gap highlighted by the review.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for evaluation — need ground-truth causal signals to assess XAI faithfulness and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>XAI evaluation benchmarks and metrics (faithfulness, robustness, stability, efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Construct synthetic datasets with known driver–response structure or use controlled perturbations to test whether attribution methods (SHAP, IG, LRP, PI, etc.) recover the known drivers; quantify metrics like faithfulness and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>methodological evaluation / benchmarking for explainability methods</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Essential for method selection and validation prior to deployment in ESS; benchmarks enable quantitative comparison though transfer to real-world cases requires careful interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Review cites Mamalakis et al. and others showing benchmark-based evaluations reveal method-specific strengths (e.g., SHAP better for tabular faithfulness, IG/LRP better for images) and underscore lack of a universal best method.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for improving reliability of XAI in ESS by enabling method selection based on quantitative metrics and revealing failure modes before operational application.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Benchmarks explicitly compare multiple XAI methods across controlled scenarios; results indicate no single method is best across all data types and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Carefully designed synthetic ground-truth datasets that capture ESS-relevant nonlinearity and interactions; clear metrics (faithfulness, stability, robustness) and community adoption of benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Synthetic benchmarks and quantitative XAI evaluation are critical to rigorously validate explanation methods in ESS and to guide method choice for different data types and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Applications of Explainable artificial intelligence in Earth system science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning and process understanding for data-driven Earth system science <em>(Rating: 2)</em></li>
                <li>Machine learning for data-driven discovery in solid Earth geoscience <em>(Rating: 1)</em></li>
                <li>Neural network attribution methods for problems in geoscience: A novel synthetic benchmark dataset <em>(Rating: 2)</em></li>
                <li>Finding and removing Clever Hans: Using explanation methods to debug and improve deep models <em>(Rating: 2)</em></li>
                <li>Explainable AI in drought forecasting <em>(Rating: 1)</em></li>
                <li>Explainable artificial intelligence for Earth observation: A review including societal and regulatory perspectives <em>(Rating: 2)</em></li>
                <li>A Unified Approach to Interpreting Model Predictions <em>(Rating: 2)</em></li>
                <li>Why Should I Trust You?: Explaining the Predictions of Any Classifier <em>(Rating: 2)</em></li>
                <li>Indicator Patterns of Forced Change Learned by an Artificial Neural Network <em>(Rating: 1)</em></li>
                <li>Physically Interpretable Neural Networks for the Geosciences: Applications to Earth System Variability <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2303",
    "paper_id": "paper-270562871",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations",
            "brief_description": "A model-agnostic, additive feature-attribution XAI method that uses Shapley values from cooperative game theory to assign marginal contributions to input features for individual predictions; widely applied in ESS for tabular feature importance and attribution.",
            "citation_title": "A Unified Approach to Interpreting Model Predictions",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Earth system science (multiple subdomains: hydrology, hazards, climatology, ecology, remote sensing)",
            "problem_description": "Attributing drivers and quantifying feature contributions in prediction models (e.g., floods, wildfire susceptibility, evapotranspiration, streamflow, crop yield) to improve interpretability and discover physical insights from ML models.",
            "data_availability": "Varies by application; typically moderate-to-abundant tabular datasets assembled from observations and reanalyses (many studies operate on labeled, structured tabular datasets); review notes availability of large observation and synthesized datasets but also highlights places with limited ground truth for explanations.",
            "data_structure": "Structured tabular data (common), sometimes used with features derived from spatial/temporal data (multimodal when combined with remote-sensing-derived variables).",
            "problem_complexity": "High nonlinearity and multi-variable couplings (high dimensional feature spaces in many ESS problems); interactions among correlated drivers common; model outputs often depend on complex spatio-temporal processes.",
            "domain_maturity": "Mature in classical process-based modeling but emergent for ML/XAI workflows; substantial prior physical knowledge exists but data-driven insights are still being integrated into domain practice.",
            "mechanistic_understanding_requirements": "High — users (process modelers, policymakers) demand interpretable, physically consistent explanations to trust model outputs and to derive scientific hypotheses.",
            "ai_methodology_name": "SHAP (model-agnostic Shapley value explanations)",
            "ai_methodology_description": "Computes marginal contribution of each feature to a prediction by considering permutations/coalitions of features; often used post-hoc on trained supervised models (RF, GBM, NN); includes TreeSHAP and DeepSHAP variants optimized for tree models and deep networks respectively.",
            "ai_methodology_category": "model-agnostic post-hoc explainability (applied to supervised learning models)",
            "applicability": "Highly applicable to tabular ESS problems for feature-ranking and local explanations; flexible across model families, though computational cost and assumptions (additivity) can limit use on very high-dimensional inputs without adaptation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported in the review as one of the most popular XAI methods in ESS; noted to have relatively high faithfulness for tabular data and widely used to identify driver importance in floods, wildfire, evapotranspiration and other tasks; however, limitations include computational cost and sensitivity to feature correlations and choice of background distribution.",
            "impact_potential": "High — provides interpretable attributions enabling scientific attribution studies, feature selection, and policy-relevant explanations; has been used to guide model choice and reveal physically plausible drivers across multiple ESS problems.",
            "comparison_to_alternatives": "Compared qualitatively in the review: SHAP shown to be more advantageous for tabular data (higher faithfulness) while gradient-based methods (IG, LRP) were reported as more valid for image-based tasks; permutation-based PI and TFI are alternatives often used for tree models.",
            "success_factors": "Model-agnostic flexibility, clear additive attribution framing (Shapley axioms), and availability of optimized implementations (TreeSHAP, DeepSHAP) made SHAP widely adoptable; success depends on sufficient labeled data and careful baseline/background choices.",
            "key_insight": "SHAP's game-theoretic, additive attributions make it well-suited for explaining tabular ESS prediction models, yielding physically interpretable driver rankings, though care is required for correlated features and baseline selection.",
            "uuid": "e2303.0",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LIME",
            "name_full": "Local Interpretable Model-agnostic Explanations",
            "brief_description": "A model-agnostic local surrogate explanation method that perturbs an instance and fits a simple interpretable model (e.g., linear) to approximate local decision boundaries, used in ESS for instance-level explanations and threshold discovery.",
            "citation_title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Hydrology, drought/evapotranspiration studies, remote sensing applications",
            "problem_description": "Providing local (instance-level) explanations for predictions such as drought occurrence, evapotranspiration thresholds, and image-based classifications to support forecasters and scientists in understanding factors leading to specific outcomes.",
            "data_availability": "Typically moderate: labeled time-series or tabular datasets derived from observations or reanalysis; some studies highlight scarcity in ground-truth labels for certain phenomena.",
            "data_structure": "Tabular features and time series; also applied where input instances may be image patches (converted to interpretable superpixels).",
            "problem_complexity": "Locally complex nonlinear decision boundaries; global model complexity can be high but LIME focuses on local linear approximations.",
            "domain_maturity": "ESS domain has established process knowledge but local, instance-level explanation needs are emergent among forecasters and modelers.",
            "mechanistic_understanding_requirements": "Medium to high — local, actionable evidence is needed by forecasters and stakeholders to validate individual predictions.",
            "ai_methodology_name": "LIME (local surrogate models)",
            "ai_methodology_description": "Perturbs input around a target instance to build synthetic neighboring samples, queries the black-box model for outputs, and fits a simple interpretable model (e.g., weighted linear) to approximate local behavior and produce feature importances or rules.",
            "ai_methodology_category": "model-agnostic post-hoc explainability for supervised models",
            "applicability": "Applicable for providing local, human-understandable explanations in ESS tasks; appropriate when practitioners need instance-specific rationales, but surrogate fidelity and perturbation scheme must be designed carefully for realistic physical inputs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Used in ESS to indicate thresholds (e.g., LIME indicated thresholds for climate variables affecting evapotranspiration per Chakraborty et al., 2021b) and for local drought/flood instance explanations; vulnerable to instability and choice of perturbation neighborhoods.",
            "impact_potential": "Moderate — strong utility for forecasters and model developers seeking local justifications, but care required to avoid misleading local surrogates when inputs are highly structured or correlated.",
            "comparison_to_alternatives": "Compared implicitly to SHAP and permutation methods; LIME provides local surrogate linear explanations while SHAP offers theoretically grounded attributions; LIME is simpler but can be less robust.",
            "success_factors": "Simplicity, direct local interpretability and ease of implementation drive uptake; success depends on realistic perturbation strategies and evaluation of surrogate fidelity.",
            "key_insight": "LIME is useful for instance-level interpretability in ESS, especially to surface local thresholds and contributors, but its reliability depends on careful design of perturbations and verification against domain knowledge.",
            "uuid": "e2303.1",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Permutation Importance (PI)",
            "name_full": "Permutation Feature Importance",
            "brief_description": "A perturbation-based model-agnostic XAI method that evaluates feature importance by randomly permuting a feature's values and measuring the degradation in model performance, commonly used in ESS for global importance ranking and sensitivity analysis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Hydrology, flood forecasting, evapotranspiration, general ESS predictive tasks",
            "problem_description": "Assessing which input variables most strongly affect model predictions (e.g., streamflow, precipitation classification) and using that information for feature selection, diagnosis, and physical consistency checks.",
            "data_availability": "Often built on moderate-to-large labeled tabular datasets from observation networks and reanalysis; availability varies by region and variable.",
            "data_structure": "Structured tabular and derived feature sets; sometimes applied to features aggregated from spatio-temporal inputs.",
            "problem_complexity": "Nonlinear models with interacting and correlated predictors; PI is sensitive to correlations and can misattribute importance in such cases.",
            "domain_maturity": "ESS has established sensitivity analysis traditions; PI connects well with such practices but needs caution for correlated features.",
            "mechanistic_understanding_requirements": "Medium to high — practitioners use PI both to improve models and to seek scientific insights into driver importance.",
            "ai_methodology_name": "Permutation Importance",
            "ai_methodology_description": "For each candidate feature, shuffle its values across the dataset while keeping others fixed, compute the change in model prediction error (e.g., increase in RMSE), and rank features by the performance degradation caused by permutations.",
            "ai_methodology_category": "model-agnostic perturbation-based explainability used with supervised models",
            "applicability": "Widely applicable to supervised tabular ML models; straightforward to implement but limited when features are highly correlated or when temporal dependence exists without respecting time structure during permutation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Valued for intuitive link to sensitivity analysis; used in multiple ESS studies (e.g., flood forecasting comparisons) to reveal model-preferred predictors; caveats include biases with correlated predictors and computational cost for large models/datasets.",
            "impact_potential": "Moderate to high for model diagnosis and feature selection; helps reduce overfitting and align model behavior with physical expectations.",
            "comparison_to_alternatives": "Permutation importance is compared to PDP/ALE for effect visualization and to SHAP for instance-level attributions; ALE is suggested as an alternative for handling correlated features.",
            "success_factors": "Direct connection to predictive performance, simplicity, and interpretability; success depends on proper permutation strategy and accounting for feature correlations/temporal dependencies.",
            "key_insight": "Permutation Importance provides an intuitive, performance-based measure of global feature relevance in ESS models but must be applied carefully when predictors are correlated or when temporal/spatial structure exists.",
            "uuid": "e2303.2",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tree-based feature importance (TFI) / Treeinterpreter",
            "name_full": "Tree-based Feature Importance (TFI) and TreeInterpreter",
            "brief_description": "Methods tailored for tree-ensemble models (RF, gradient boosting) that attribute importance by impurity reduction aggregated across trees (TFI) and extract per-instance pathwise contributions (TreeInterpreter) for granular explanations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Hydrology, precipitation classification, evapotranspiration, remote sensing classification",
            "problem_description": "Identifying influential predictors and diagnosing problematic decision paths in tree-based models used for forecasting and classification (e.g., precipitation types, runoff drivers).",
            "data_availability": "Typically moderate to abundant tabular datasets; many ESS problems have rich observational archives enabling tree-model training.",
            "data_structure": "Structured tabular data, features derived from spatio-temporal observations and indices.",
            "problem_complexity": "High nonlinearity and interaction effects captured by tree ensembles; interpretability easier than deep nets but still complex across many trees.",
            "domain_maturity": "Tree models are mature and widely used in ESS; interpretability tools are well integrated into workflows.",
            "mechanistic_understanding_requirements": "Medium — modelers use TFI and TreeInterpreter to align ML behavior with hydrological and atmospheric process knowledge and for feature selection.",
            "ai_methodology_name": "TFI / TreeInterpreter",
            "ai_methodology_description": "TFI computes feature relevance via aggregated impurity reduction (or split gain) across ensemble trees; TreeInterpreter walks decision paths for a given instance to compute contribution of each feature to the final prediction.",
            "ai_methodology_category": "model-specific explainability for supervised tree-based models",
            "applicability": "Highly applicable to ensemble tree models common in ESS; computationally efficient variants exist and they produce both global and instance-level explanations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Used effectively for feature selection and diagnosis (e.g., Feng et al., Upadhyaya et al.); provides physically consistent feature rankings in some studies and helps identify problematic tree nodes causing misclassification.",
            "impact_potential": "High for improving tree-model reliability and interpretability in ESS, facilitating feature selection and physical validation.",
            "comparison_to_alternatives": "TFI is often favored when using tree models compared to model-agnostic PI for efficiency; SHAP TreeSHAP provides theoretically grounded per-instance attributions as an alternative.",
            "success_factors": "Direct alignment with tree model internals and computational efficiency; success hinges on model quality and avoiding overinterpretation of impurity-based metrics when features are correlated.",
            "key_insight": "Tree-specific interpretability methods give granular, computationally efficient explanations for ensemble models in ESS, enabling model diagnosis and physically consistent feature selection when applied with care.",
            "uuid": "e2303.3",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gradient-based XAI / LRP / IG",
            "name_full": "Gradient-based attribution methods (saliency maps, Integrated Gradients, Layer-wise Relevance Propagation)",
            "brief_description": "Methods that use backpropagated gradients or layer-wise relevance propagation to produce spatially resolved attributions (saliency maps) for deep networks, particularly effective for image and gridded spatio-temporal ESS inputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Atmospheric science, remote sensing, oceanography, hazard mapping (e.g., hail, wildfire maps), spatio-temporal field prediction",
            "problem_description": "Explaining which spatial regions/pixels or input fields drive convolutional or other deep model predictions for imaging tasks and gridded climate/meteorological forecasts.",
            "data_availability": "Often abundant gridded remote sensing and reanalysis data, but labeled event occurrences (e.g., hail) may be scarcer; image datasets are common in many ESS analyses.",
            "data_structure": "Unstructured/structured images and gridded spatio-temporal arrays; high-dimensional pixel/feature spaces.",
            "problem_complexity": "Very high dimensional, spatially correlated, nonlinear mapping from inputs to outputs; long-range dependencies can exist.",
            "domain_maturity": "Remote sensing and atmospheric imaging are mature in data availability; deep-learning adoption is rapidly increasing and XAI methods for images are actively evaluated.",
            "mechanistic_understanding_requirements": "High — spatial attributions are used to validate physical consistency and to communicate to forecasters and scientists.",
            "ai_methodology_name": "Gradient-based attribution methods (saliency, IG, LRP, Grad-CAM)",
            "ai_methodology_description": "Compute gradients of outputs w.r.t. inputs (saliency) or integrate gradients along a path from a baseline (Integrated Gradients) to yield pixel-level attributions; LRP redistributes output relevance back to pixels; Grad-CAM uses gradients to weight activation maps for coarse localization.",
            "ai_methodology_category": "model-specific post-hoc explainability for deep supervised models (CNNs, Conv-LSTMs)",
            "applicability": "Well-suited for image-based ESS tasks to reveal spatial drivers; requires differentiable models and sensible baselines; partial interpretability (e.g., attention maps) can be coarse and susceptible to instability.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Review notes IG and LRP as more valid for image tasks while Grad-CAM variants provide coarse localization; these methods have been used to interpret precipitation, hail, sea temperature and related tasks, but sensitivity and reliability vary.",
            "impact_potential": "High for validating spatial predictions, diagnosing models, and communicating results to domain experts; can identify spatial patterns and inform process understanding.",
            "comparison_to_alternatives": "Compared with SHAP and permutation methods (better for tabular), gradient methods perform better for image/gridded data according to cited evaluations; however, robustness and faithfulness issues remain.",
            "success_factors": "Availability of differentiable deep models, meaningful baselines, and domain-appropriate evaluation datasets; success depends on assessing robustness and stability of attribution maps.",
            "key_insight": "Gradient-based attribution methods are the most effective XAI tools for image/gridded ESS problems, enabling spatial localization of drivers, but require careful baseline selection and robustness checks.",
            "uuid": "e2303.4",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Attention / Transformers",
            "name_full": "Attention mechanisms and Transformer architectures",
            "brief_description": "Deep-learning components (attention) and Transformer models that dynamically weight inputs to capture long-range dependencies and provide interpretable attention maps; used in ESS for spatio-temporal forecasting and interpretable flood/temperature modeling.",
            "citation_title": "Attention Is All You Need",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Flood forecasting, subseasonal-to-seasonal precipitation/temperature forecasting, spatio-temporal environmental prediction",
            "problem_description": "Modeling long-range dependencies in sequences and gridded data (e.g., teleconnections, circulation patterns) and providing interpretable attention scores that indicate influential inputs over long contexts.",
            "data_availability": "Large historical sequences from observations and reanalyses are available for many ESS tasks, enabling Transformer training where labeled targets exist.",
            "data_structure": "Time series and spatio-temporal sequences; sometimes combined with spatial feature channels (multimodal sequence inputs).",
            "problem_complexity": "High temporal complexity (long-range correlations), high dimensionality when combining spatial channels; computationally expensive to train at high resolution.",
            "domain_maturity": "Transformers are relatively recent in ESS but rapidly adopted; domain knowledge on long-range drivers exists and is being combined with attention for interpretability.",
            "mechanistic_understanding_requirements": "Medium to high — attention maps are used as partial explanations but domain experts often require additional validation since attention is only partially interpretable.",
            "ai_methodology_name": "Attention mechanisms / Transformer networks",
            "ai_methodology_description": "Self-attention layers compute pairwise interactions among sequence elements to produce context-aware representations; Transformers stack attention and feed-forward layers; attention weights can be visualized as importance maps over inputs.",
            "ai_methodology_category": "supervised/sequence modeling with model-specific (attention) interpretability",
            "applicability": "Applicable to long-range dependent ESS problems and beneficial where capturing teleconnections or lengthy histories matters; interpretability of attention is partial and must be complemented by XAI evaluation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Used in ESS (e.g., Transformer-based flood forecasting papers cited) to capture long-range dependencies and provide interpretable attention; review cautions that attention is only partially interpretable and needs enhancement (heuristic propagation, symbolic abstraction).",
            "impact_potential": "High potential for improved forecasts and for revealing temporal drivers across long windows, provided attention explanations are validated against physics.",
            "comparison_to_alternatives": "Compared implicitly to RNNs (LSTM/Conv-LSTM) which handle sequences but less efficiently for long-range dependencies; Transformers often provide improved modeling of long contexts.",
            "success_factors": "Ability to leverage large amount of sequential data, architectural capacity to capture long dependencies, and combining attention visualization with XAI validation methods.",
            "key_insight": "Transformers and attention mechanisms are promising for ESS sequence problems because they capture long-range dependencies and offer interpretable attention maps, but attention must be validated and supplemented to yield trustworthy scientific explanations.",
            "uuid": "e2303.5",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Variational Autoencoder (VAE) / Latent representations",
            "name_full": "Variational Autoencoders and latent-space analysis",
            "brief_description": "Unsupervised deep models (VAEs) that compress high-dimensional inputs into a lower-dimensional latent space whose structure can be analyzed with XAI to reveal regimes or modes (e.g., convective regimes) in climate and atmospheric data.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Atmospheric science (convective regimes), climatology, oceanography (dynamical region identification)",
            "problem_description": "Discovering coherent regimes or patterns in high-dimensional climate and atmospheric fields (e.g., precipitation generation regimes, ocean dynamical regions) that aid conceptual model building.",
            "data_availability": "Large gridded datasets (model output, remote sensing, reanalysis) often available; unsupervised approaches exploit abundant unlabeled data.",
            "data_structure": "High-dimensional gridded images/fields, spatio-temporal datasets; multimodal possibilities when combining variables.",
            "problem_complexity": "High dimensionality and nonlinearity with complex spatio-temporal patterns; latent spaces seek to capture essential degrees of freedom.",
            "domain_maturity": "Use of unsupervised representation learning in ESS is emergent; process knowledge exists that can be used to interpret latent dimensions.",
            "mechanistic_understanding_requirements": "High for scientific insight — latent variables are interrogated with XAI to map to physical regimes and aid hypothesis formation.",
            "ai_methodology_name": "Variational Autoencoders (VAE) and latent-space analysis",
            "ai_methodology_description": "Train encoder-decoder networks to map inputs to a probabilistic latent space and reconstruct inputs; analyze latent coordinates (clustering, attribution) to identify regimes or interpretable factors (e.g., different convective regimes detected by VAE latent variables).",
            "ai_methodology_category": "unsupervised representation learning with post-hoc interpretability",
            "applicability": "Well-suited to exploratory science where labeled targets are scarce and where researchers seek emergent regimes; requires large volumes of data for stable latent representations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited examples (Behrens et al., 2022) show VAEs revealing convective regimes and facilitating physical interpretation; latent representations can yield novel conceptualizations of system behavior.",
            "impact_potential": "High for scientific discovery and building conceptual models (e.g., neural ESM concepts), enabling reduction of complex fields into interpretable modes.",
            "comparison_to_alternatives": "Compared implicitly to clustering and linear dimensionality reduction; VAEs capture nonlinear manifolds and thus can reveal richer regimes than linear methods.",
            "success_factors": "Large gridded datasets, careful architecture/regularization choices, and post-hoc mapping of latent features to physical variables and processes.",
            "key_insight": "VAEs and latent-space XAI enable discovery of emergent regimes in high-dimensional ESS data, offering a pathway to translate complex fields into interpretable process-level insights when sufficient data exist.",
            "uuid": "e2303.6",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Conv-LSTM / LSTM",
            "name_full": "Convolutional Long Short-Term Memory networks (Conv-LSTM) and LSTM",
            "brief_description": "Recurrent deep architectures (LSTM and Conv-LSTM) tailored for sequential and spatio-temporal ESS tasks; combined with XAI (gradient-based or surrogate analyses) to interpret sequence importance and temporal drivers for variables like soil moisture and hydrological flows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Soil moisture prediction, regional hydrological modeling, streamflow forecasting, spatio-temporal environmental prediction",
            "problem_description": "Predicting time-evolving geophysical variables (soil moisture, streamflow) from multi-channel spatio-temporal inputs and interpreting which time windows and spatial features matter for predictions.",
            "data_availability": "Moderate to abundant time-series and gridded observational/reanalysis data; in situ measurements used to train and validate models (e.g., 1 km soil moisture datasets).",
            "data_structure": "Spatio-temporal sequences (multichannel grids over time); high-dimensional and temporally correlated.",
            "problem_complexity": "High: temporal dependencies, spatial correlations, nonlinearity, multi-scale interactions.",
            "domain_maturity": "LSTM-based methods have seen growing adoption in hydrology and soil moisture prediction; domain expertise used to assess physical plausibility of learned temporal patterns.",
            "mechanistic_understanding_requirements": "High — interpretability of temporal drivers and thresholds (sequence importance) is important for hydrological science and applications.",
            "ai_methodology_name": "LSTM / Conv-LSTM (recurrent and convolutional-recurrent networks)",
            "ai_methodology_description": "LSTM cells capture temporal dependencies; Conv-LSTM extends LSTM to spatial grids by replacing linear transforms with convolutions; XAI for these models includes gradient-based attribution, sequence importance, probing latent states, and surrogate models to reveal time windows and sequences critical for predictions.",
            "ai_methodology_category": "supervised sequence/spatio-temporal deep learning with model-specific interpretability",
            "applicability": "Appropriate for spatio-temporal ESS forecasting where temporal context and local spatial patterns matter; requires adequate training sequences and attention to overfitting.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported uses include soil moisture prediction (Huang et al., 2023b) where Conv-LSTM with XAI improved interpretability and predictive accuracy; XAI methods expose important time periods and spatial features informing process understanding.",
            "impact_potential": "High for operational forecasting and scientific insight into temporal propagation of hydrological signals; enables hypothesis generation about time-lagged drivers.",
            "comparison_to_alternatives": "Compared to classical LSTM and non-sequential models; Conv-LSTM integrates spatial structure better than vanilla LSTM and often yields improved spatio-temporal performance.",
            "success_factors": "Availability of dense spatio-temporal training data, appropriate architecture choices to capture spatial locality, and use of XAI (sequence importance, probing) to validate learned temporal dynamics.",
            "key_insight": "Conv-LSTM architectures paired with XAI reveal temporally localized drivers and spatial patterns in hydrological and soil-moisture problems, improving both prediction and scientific interpretability when trained on sufficient spatio-temporal data.",
            "uuid": "e2303.7",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Physics-informed / hybrid models",
            "name_full": "Physics-informed neural networks and hybrid physics-ML models",
            "brief_description": "Hybrid approaches that integrate process-based (mechanistic) models or physical constraints into ML architectures (e.g., physics-guided neural networks, physics-informed losses) to improve physical consistency, interpretability, and generalization in ESS.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Earth system modeling, hydrology, oceanography, atmospheric process emulation and subgrid parameterization",
            "problem_description": "Bridging data-driven model flexibility with process-based mechanistic understanding to ensure models obey known physics, improve extrapolation, and yield interpretable, causally consistent insights.",
            "data_availability": "Hybrid models leverage both observational data and process-model outputs (e.g., ESM runs); availability varies but often sufficient for constrained learning.",
            "data_structure": "Multimodal: model state variables, gridded fields, and tabular forcings; high-dimensional structured inputs with physical interdependencies.",
            "problem_complexity": "Very high: coupled nonlinear PDE-driven processes, multi-scale interactions, and limited observability of some state variables.",
            "domain_maturity": "The use of physics-informed ML in ESS is emergent and identified in the review as a promising pathway; process-based models are longstanding in the field.",
            "mechanistic_understanding_requirements": "Very high — demanding for scientific trust and regulatory acceptability; hybrid methods aim to preserve mechanistic interpretability while leveraging data-driven components.",
            "ai_methodology_name": "Physics-guided neural networks / physics-informed ML (hybrid models)",
            "ai_methodology_description": "Incorporate physical constraints into architecture or loss (e.g., conservation laws, known process relationships), combine ML components with explicit process-based modules, or use ML to learn subgrid parameterizations guided by physical theory.",
            "ai_methodology_category": "hybrid / physics-informed supervised learning",
            "applicability": "Especially suitable when strong prior mechanistic knowledge exists and extrapolation/generalization beyond training regimes is needed; helps align ML outputs with physical expectations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Review emphasizes hybrid physics–ML as promising for ESS to resolve compatibility issues between XAI and domain knowledge, increase faithfulness and generalizability, and enable interpretable scientific discovery.",
            "impact_potential": "High — can improve trust and adoption of ML in ESS, provide physically consistent insights, and aid incorporation of ML into ESMs and operational systems.",
            "comparison_to_alternatives": "Contrasted with purely data-driven black-box models which may be higher-performing in-sample but lack physical constraints and interpretability; hybrid approaches aim to get the best of both.",
            "success_factors": "Quality of physical priors, correct integration scheme (architecture/loss), and sufficient data to train data-driven components without violating constraints.",
            "key_insight": "Physics-informed and hybrid ML models are a crucial strategy in ESS to combine predictive power with mechanistic interpretability, addressing domain demands for physically consistent, generalizable AI.",
            "uuid": "e2303.8",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "XAI evaluation benchmarks (Mamalakis et al.)",
            "name_full": "Synthetic benchmark datasets and evaluation frameworks for XAI in geoscience",
            "brief_description": "Benchmark datasets and synthetic experiments developed to provide ground truth for evaluating the faithfulness, robustness, and stability of XAI methods in geoscience contexts.",
            "citation_title": "Neural network attribution methods for problems in geoscience: A novel synthetic benchmark dataset",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Methodology for XAI evaluation across geoscience applications (atmosphere, climate, remote sensing)",
            "problem_description": "Providing ground-truthable testbeds to quantitatively evaluate whether XAI methods correctly identify known drivers and produce faithful explanations in geoscientific ML applications.",
            "data_availability": "Benchmark datasets are synthetic or semi-synthetic to embed known causal relationships; actual observational ground truth often lacking in real-world ESS datasets.",
            "data_structure": "Synthetic tabular and gridded datasets designed to mimic ESS data structure with controlled causal relationships.",
            "problem_complexity": "Designed to cover nonlinear, interacting driver scenarios of varying complexity to stress-test XAI methods.",
            "domain_maturity": "XAI evaluation in ESS is nascent; benchmarks are emerging to fill the gap highlighted by the review.",
            "mechanistic_understanding_requirements": "High for evaluation — need ground-truth causal signals to assess XAI faithfulness and robustness.",
            "ai_methodology_name": "XAI evaluation benchmarks and metrics (faithfulness, robustness, stability, efficiency)",
            "ai_methodology_description": "Construct synthetic datasets with known driver–response structure or use controlled perturbations to test whether attribution methods (SHAP, IG, LRP, PI, etc.) recover the known drivers; quantify metrics like faithfulness and stability.",
            "ai_methodology_category": "methodological evaluation / benchmarking for explainability methods",
            "applicability": "Essential for method selection and validation prior to deployment in ESS; benchmarks enable quantitative comparison though transfer to real-world cases requires careful interpretation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Review cites Mamalakis et al. and others showing benchmark-based evaluations reveal method-specific strengths (e.g., SHAP better for tabular faithfulness, IG/LRP better for images) and underscore lack of a universal best method.",
            "impact_potential": "High for improving reliability of XAI in ESS by enabling method selection based on quantitative metrics and revealing failure modes before operational application.",
            "comparison_to_alternatives": "Benchmarks explicitly compare multiple XAI methods across controlled scenarios; results indicate no single method is best across all data types and tasks.",
            "success_factors": "Carefully designed synthetic ground-truth datasets that capture ESS-relevant nonlinearity and interactions; clear metrics (faithfulness, stability, robustness) and community adoption of benchmarks.",
            "key_insight": "Synthetic benchmarks and quantitative XAI evaluation are critical to rigorously validate explanation methods in ESS and to guide method choice for different data types and tasks.",
            "uuid": "e2303.9",
            "source_info": {
                "paper_title": "Applications of Explainable artificial intelligence in Earth system science",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning and process understanding for data-driven Earth system science",
            "rating": 2,
            "sanitized_title": "deep_learning_and_process_understanding_for_datadriven_earth_system_science"
        },
        {
            "paper_title": "Machine learning for data-driven discovery in solid Earth geoscience",
            "rating": 1,
            "sanitized_title": "machine_learning_for_datadriven_discovery_in_solid_earth_geoscience"
        },
        {
            "paper_title": "Neural network attribution methods for problems in geoscience: A novel synthetic benchmark dataset",
            "rating": 2,
            "sanitized_title": "neural_network_attribution_methods_for_problems_in_geoscience_a_novel_synthetic_benchmark_dataset"
        },
        {
            "paper_title": "Finding and removing Clever Hans: Using explanation methods to debug and improve deep models",
            "rating": 2,
            "sanitized_title": "finding_and_removing_clever_hans_using_explanation_methods_to_debug_and_improve_deep_models"
        },
        {
            "paper_title": "Explainable AI in drought forecasting",
            "rating": 1,
            "sanitized_title": "explainable_ai_in_drought_forecasting"
        },
        {
            "paper_title": "Explainable artificial intelligence for Earth observation: A review including societal and regulatory perspectives",
            "rating": 2,
            "sanitized_title": "explainable_artificial_intelligence_for_earth_observation_a_review_including_societal_and_regulatory_perspectives"
        },
        {
            "paper_title": "A Unified Approach to Interpreting Model Predictions",
            "rating": 2,
            "sanitized_title": "a_unified_approach_to_interpreting_model_predictions"
        },
        {
            "paper_title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
            "rating": 2,
            "sanitized_title": "why_should_i_trust_you_explaining_the_predictions_of_any_classifier"
        },
        {
            "paper_title": "Indicator Patterns of Forced Change Learned by an Artificial Neural Network",
            "rating": 1,
            "sanitized_title": "indicator_patterns_of_forced_change_learned_by_an_artificial_neural_network"
        },
        {
            "paper_title": "Physically Interpretable Neural Networks for the Geosciences: Applications to Earth System Variability",
            "rating": 2,
            "sanitized_title": "physically_interpretable_neural_networks_for_the_geosciences_applications_to_earth_system_variability"
        }
    ],
    "cost": 0.02554925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Applications of Explainable artificial intelligence in Earth system science</p>
<p>Feini Huang 
Shijie Jiang 
Lu Li 
Yongkun Zhang 
Ye Zhang 
Ruqing Zhang 
Qingliang Li 
Danxi Li 
Wei Shangguan 
Yongjiu Dai 
Elia, M., D'R Cilli 
Giannico, VM Este </p>
<p>1Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai)
School of Atmospheric Sciences
Guangdong Province Key Laboratory for Climate Change and Natural Disaster Studies
Sun Yat-Sen University
519082ZhuhaiChina</p>
<p>2Department of Biogeochemical Integration
Max Planck Institute for Biogeochemistry
07745JenaGermany</p>
<p>3Shanwei Meteorological Service
ShanweiChina</p>
<p>4College of Computer Science and Technology
Changchun Normal University
130032ChangchunChina</p>
<p>5ELLIS Unit Jena
JenaGermany</p>
<p>Curran Associates Inc
Red HookNY</p>
<p>Information Sciences
1-17225</p>
<p>Applications of Explainable artificial intelligence in Earth system science
6BE180E0119E57D7FD306BB57917E73110.1007/s11368-021-02987-y
In recent years, artificial intelligence (AI) rapidly accelerated its influence and is expected to promote the development of Earth system science (ESS) if properly harnessed.In application of AI to ESS, a significant hurdle lies in the interpretability conundrum, an inherent problem of black-box nature arising from the complexity of AI algorithms.To address this, explainable AI (XAI) offers a set of powerful tools that make the models more transparent.The purpose of this review is twofold: First, to provide ESS scholars, especially newcomers, with a foundational understanding of XAI, serving as a primer to inspire future research advances; second, to encourage ESS professionals to embrace the benefits of AI, free from preconceived biases due to its lack of interpretability.We begin with elucidating the concept of XAI, along with typical methods.We then delve into a review of XAI applications in the ESS literature, highlighting the important role that XAI has played in facilitating communication with AI model decisions, improving model diagnosis, and uncovering scientific insights.We identify four significant challenges that XAI faces within the ESS, and propose solutions.Furthermore, we provide a comprehensive illustration of multifaceted perspectives.Given the unique challenges in ESS, an interpretable hybrid approach that seamlessly integrates AI with domain-specific knowledge appears to be a promising way to enhance the utility of AI in ESS.A visionary outlook for ESS envisions a harmonious blend where "process-based models govern the known, AI models explore the unknown, and XAI bridges the gap by providing explanations".</p>
<p>Motivations</p>
<p>Earth system science (ESS) provides a comprehensive perspective on our planet by integrating diverse scientific processes and data from different domains, thereby fostering a profound understanding of Earth as a complex, adaptive system (Schellnhuber 1999;Steffen et al., 2020).With the recent proliferation of data observation and synthesis technologies, such as sensors, remote sensing, and reanalysis, a deluge of Earth system data has become available.Artificial intelligence (AI) is a particularly powerful tool for harnessing this wealth of data and unlocking its maximum potential (Li et al., 2023a).Over the past two decades, AI has flourished in the ESS (Reichstein et al., 2019;Xu and Liang, 2021).It offers advantages in capturing nonlinear patterns, synthesizing multi-source data, reducing computational costs, and complementing traditional numerical models.Traditional machine learning (ML) techniques, such as random forest (RF) and support vector machine (SVM), excel in classification and regression problems and nonlinear system descriptions.On the other hand, deep learning (DL) algorithms such as artificial neural networks (ANNs), recurrent neural networks (RNNs), convolutional neural networks (CNNs) and innovative methods such as reinforcement learning, Transformers, generative adversarial networks (GANs), and diffusion models improve prediction accuracy through their complex architectures.</p>
<p>The effectiveness of AI (specifically traditional ML and DL) in prediction and modeling for ESS has generated considerable interest among geoscientists (McGovern et al., 2019).Nonetheless, the blackbox nature of AI models presents a significant barrier to understanding the underlying insights they provide.This black-box nature refers to the complexity of AI models that makes the model difficult for humans to interpret (Reichstein et al., 2019).For example, the multi-layered neural structure of DL obscures transparency and interpretability (Sarker, 2021).Nevertheless, AI can serve as a source of new knowledge for ESS due to its superiority in representing non-linear relationships and systemic complexity.</p>
<p>When AI is made explainable, the information hidden in these models has the potential to reveal critical insights that might otherwise be overlooked (Behrens &amp; Viscarra Rossel, 2020;Leist et al., 2022;Krenn et al., 2022).The quest for model explanations in AI has led to the emergence of two main strategies (Shen, 2018;Bergen et al., 2019).</p>
<p>One strategy is to adopt or develop AI architectures that inherently provide clarity to their decision processes, such as decision trees (Tulloch et al., 2018), linear model (Qin et al., 2021) and physics-guided neural networks (Lian et al., 2022).The other strategy focuses on applying post-hoc explainable AI (XAI) techniques to uncover the mechanisms behind the outputs of AI.This review focuses primarily on various methods and applications within the field of XAI.XAI can be a useful tool for overcoming the traditional trade-off between performance and explainability in complex AI systems, thereby fostering trust, supporting decision making, diagnosing and improving models, and enabling scientific discovery across ESS disciplines.XAI was initiated by the Defense Advanced Research Projects Agency in 2015 with the primary goal of making AI's behavior more understandable to humans by providing explanations (Gunning et al., 2019).Humans often make decisions based on verifiable evidence that bolsters their confidence (Gunning et al., 2019); however, in ESS, AI predictions typically provide outcomes without explaining the underlying processes.This opacity impedes trust in AI and is a major concern for regulators and the public, thereby hindering its adoption in predictive applications (Huntingford et al., 2019;Gevaert, 2022;Cowls et al., 2023;Jobin et al., 2019;Nordgren, 2023).XAI techniques offer a promising way to address interpretability concerns by making AI models more transparent, credible, and understandable.Specifically, it is expected that XAI could provide a solid foundation for decision making when applying complex DL models to predict extreme events and longterm climate change (Nishant et al., 2020;Prodhan et al., 2022).Moreover, for most AI modelers in the ESS, insights derived from XAI serve as an effective guide for diagnosing and debugging models by augmenting data or model building.Further, from the perspective of theoretical scientists in ESS, XAI is also crucial for scientific understanding and discovery.The essence of science in ESS is the acquisition of knowledge about the world, and the generation of hypotheses hidden in data can be effectively learned by AI and made visible by XAI (Irrgang et al., 2020;Gettelman et al., 2022).While traditional ML models such as decision trees have achieved notable success due to their high explanatory power, complex DL models often surpass them, demonstrating greater potential to uncover underlying processes and knowledge.With XAI, it becomes conceivable to unveil the insights embedded in these models.</p>
<p>Since its rise in 2015, XAI has become a thriving area of interest within the AI community and has found practical applications in many fields.By the end of December 2023, more than 9,000 peerreviewed articles on XAI and related concepts had been published.Figure 1a illustrates the extensive coverage of XAI research in various fields.In particular, XAI methods have been effectively applied in fields such as computer science (Akula et al., 2022;Mankodiya et al., 2022), medicine (Kim et al., 2022;Lauritsen et al., 2020;Novakovsky et al., 2022;van Hilten et al., 2021), and management and construction (Calvaresi et al., 2021;Kute et al., 2021;Park and Yang, 2022;Weber et al., 2023).</p>
<p>Furthermore, XAI has made significant progress in theoretical domains such as chemistry (Zhong et al., 2022;Yano et al., 2022), physics (Li et al., 2022a;Mousavi and Beroza, 2022) and biology (Pilowsky et al., 2022).In ESS and its sub-domains (water, environment, geosciences and remote sensing), the number of publications incorporating XAI was 285, with an increasing trend since 2018 (Figs.1b and 1c).Despite the booming interest in XAI in ESS, with more than 285 articles referencing XAI applications in the field, a comprehensive overview of the XAI-ESS interface is still lacking, with the exception of reviews focusing on specific subfields (Başağaoğlu et al., 2022;Gevaert 2022).Although XAI is still in its early stages, many scientists have already recognized its potential to enhance future modeling across ESS components, including meteorology (McGovern et al., 2019), hydrology (Shen, 2018), natural hazards (Dikshit &amp; Pradhan, 2021a, 2021b;Ghaffarian et al., 2023), Earth system monitoring (Tziolas et al., 2021), Earth system modeling (Reichstein et al., 2019), environmental science (Tuia et al., 2021;Zhang et al., 2021a), remote sensing (Camps-Valls et al., 2020;Kakogeorgiou and Karantzalos, 2021), and ecology (Shah et al., 2021).Particularly, Irrgang et al. (2020) have introduced the innovative concept of neural Earth system models (ESM), which aims to integrate AI and ESM in a deep and interpretable way.The European Centre for Medium-Range Weather Forecasts (ECMWF) has planned to incorporate XAI into future modeling efforts with the dual purpose of improving performance and discovering novel natural phenomena that can further inform process-based (PB) models (Schneider et al., 2022).By 2023, XAI has continued its growth trajectory within the ESS, serving the three core goals of communicating model decisions, improving models, and providing scientific insights.Numerous researchers have approached XAI from diverse perspectives.For example, McGovern et al. (2019) introduced XAI to the field of atmospheric science, demonstrating successful applications such as precipitation classification and hail prediction.From a regulatory perspective, Gevaert (2022) highlighted the different explanation requirements of various audiences in Earth observation and proposed a standardized flowchart to address these needs.Başağaoğlu et al. (2022) summarized the use of XAI in different hydroclimatic domains and presented an automated XAI framework.Overall, XAI has the potential to improve ESS by providing interpretable and trustworthy AI models that can drive informed decision-making and scientific discovery.</p>
<p>However, the integration of XAI into ESS faces several challenges.One major obstacle is that many ESS researchers are unfamiliar with XAI methods and are therefore reluctant to adopt them due to their early stage of development.The rapid evolution of available XAI methods, which include a lot of jargon familiar to the ML community, often leaves geoscientists uncertain about the appropriate method for their specific research questions due to the interdisciplinary knowledge gap.This hesitation is exacerbated by the rapidly advancing but early stage of XAI development, where technical issues abound and theoretical foundations are still being established.</p>
<p>In summary, despite the booming application of XAI in ESS, technical challenges and compatibility issues between XAI and ESS models pose significant problems.Technical challenges such as faithfulness, robustness, and stability issues of XAI methods may hinder the application.Also, the mismatch of interpretative forms (e.g., feature importance and latent representation) and prior physical knowledge in ESS would cause the compatibility problems.The high expectations surrounding XAI might bring the unpredictable risk of its potential misuse in ESS.</p>
<p>In this review, our primary goal is to elucidate XAI methodologies for the broader ESS community by providing a comprehensive overview of the current state of XAI applications in ESS.We aim not only to highlight their remarkable achievements in advancing ESS development, but also to explore the challenges and opportunities that lie ahead.Specifically, this paper is organized as follows:</p>
<p>(1) Section 2 provides a brief technical overview of classical and common XAI methods, along with a comparative analysis of their strengths and weaknesses.</p>
<p>(2) Section 3 presents a comprehensive summary of the diverse applications of XAI to different goals in the ESS, detailing how XAI helps to communicate model decisions, diagnose and improve models, and uncover scientific insights.</p>
<p>XAI Basics</p>
<p>Definition, Approaches and Interpretative Forms</p>
<p>The field of XAI derives from the broader concept of "interpretability" or "explainability".To avoid confusion for the ESS community, we opt for the term "explainability" aligning with Biran and Cotton's (2017) definition that the explainability of an AI model refers to: "The extent to which an observer can comprehend the rationale behind a decision".</p>
<p>Accordingly, XAI can be characterized as a branch of AI dedicated to elucidating the explanation of a model's decision (Gunning et al., 2019;Miller, 2019;Murdoch et al., 2019;Minh et al., 2022).For clarity in ESS domains, we refine the definition of XAI as:</p>
<p>"Given an audience, the methods that provide additional insights into an AI model's internal logic during the modeling process beyond the mere outputs".This accompanying explanation serves to increase transparency and understanding of the AI model's decisions.Importantly, such explanations should cater to the diverse audiences in the ESS domain and facilitate better access to and understanding of the model's reasoning (Barredo Arrieta et al., 2020).</p>
<p>In this review, we do not delve into the diverse taxonomy of XAI methods (see Appendix A, Dosilovic et al., 2018;Guidotti et al., 2019;Vilone and Longo, 2021;Holzinger et al., 2022), as this is not our primary focus in the context of ESS.The XAI methods in this review are systematically classified and described in Table A1.To align with the practices of the ESS community, we categorize XAI methods into two broad groups: inherently interpretable AI models and methods for explaining black-box models including model-agnostic (MA) and model-specific (MS) methods following Adadi and Berrada (2018) in Figure 2a.MA methods can be applied to a wide range of ML models, while MS methods are tailored and only applicable to specific models.</p>
<p>Inherently interpretable AI models such as linear regression, decision trees, clustering algorithms and variational autoencoder (VAE) are commonly used due to their ability to make people understandable.</p>
<p>However, there is a trade-off between accuracy and explainability: inherently interpretable AI models (i.e., linear regression and decision trees) typically perform worse than their less transparent counterparts (Gunning et al., 2021).</p>
<p>Several strategies are typically used to explain black-box AI models, such as surrogate models, perturbation-based methods, and information extraction methods in Figure 2b.These approaches are applicable to both MA and MS methods.The surrogate model involves constructing an inherently interpretable model to mimic the behavior of a complex AI model.Perturbation-based methods explain AI models by perturbing their inputs and observing the subsequent changes in the output (Cortez &amp; Embrechts, 2013).This analysis offers a natural and intuit vim ive way to understand black-box models by focusing on how changes in inputs affect output.</p>
<p>Information extraction methods are predominantly model-specific and extract relevant information directly from the model or data, such as visual attention, cell-and layer-level gradients, and latent representations.Although this information is highly valuable, it can be difficult to interpret.In Figure 2c, we illustrate several common interpretative forms provided by XAI methods.Feature importance is a widely adopted form in ESS that explicitly ranks the relative contribution of features to predictions.Pixel importance highlights the specific image pixels that the AI algorithm focuses on when making a prediction, typically presented as a heatmap to visualize feature importance in space and identify areas where feature variations are critical for accurate predictions.Sequence importance serves the ESS community by identifying critical time periods in historical data that significantly influence future predictions.Feature effect analysis examines how individual features affect predictions; for example, as shown in Figure 2c, soil moisture (SM) exhibits a nonlinear relationship with increasing air temperature (TA).Rule explanation translates the model's decision-making process into human-readable "if-then" statements, clarifying how specific feature values lead to particular predictions.In addition, model representation techniques delve into the internal mechanisms of the AI model and transform them into understandable information that explains how predictions are generated.This information can include representations of forward-propagating processes, back-propagating gradients, or meaningful details extracted from various layers of the neural network.</p>
<p>Explaining Black-box AI Models</p>
<p>Model-agnostic Methods</p>
<p>Model-agnostic (MA) methods, which operate independently of the internal mechanisms that generate AI model predictions, can be applied to any type of AI model.MA methods typically use surrogate models and/or perturbation-based methods.Among surrogate methods, local interpretable model-agnostic explanation (LIME) is widely used (Ribeiro et al., 2016).Given an input x (e.g., an image), LIME systematically perturbs the input to generate a set of artificial samples from the neighboring regions.Then, these perturbed instances are fed into a linear regression model, and the feature importance is derived based on the regression coefficients.Based on LIME, several similar methods have been proposed by Guo et al. (2018), Zafar and Khan (2019) and ElShawi et al. ( 2019).</p>
<p>Among the perturbation-based methods, permutation importance (PI) stands out as a popular choice (Fisher et al., 2021).It provides feature importance across the entire AI model by randomly shuffling the training set data of each variable and measuring the subsequent changes in prediction errors.The perturbation-based technique also allows for the assessment of feature effects, revealing how the model's output responds to variations in input features.This approach is closely related to the concept of sensitivity analysis in ESS.Notable examples include partial dependence plot (PDP, Friedman, 2001), individual conditional expectation (ICE, Goldstein et al., 2015), and accumulated local effect (ALE, Apley &amp; Zhu, 2019).PDP represents the averaged marginal effect of a specific feature on the predictive outcome of the model, while ICE extends this notion to provide local explanations by examining individual instance-specific marginal effects.ALE computes differences in predictions rather than averages over a small window (e.g., using empirical quantiles).As an alternative to PDP, ALE may be subject to bias when dealing with highly correlated variables.</p>
<p>Shapley additive explanations (SHAP), first introduced by Lundberg and Lee (2017), integrates surrogate modeling with perturbation analysis.SHAP uses concepts from coalitional game theory to compute marginal contributions for each feature in a data instance, treating these features as players in a coalition.The marginal contributions are derived by perturbing individual features within an instance.</p>
<p>SHAP then adopts additive feature attribution methods as a surrogate model, essentially using linear combinations of input features to construct an interpretable approximation of the original complex model.</p>
<p>Following the foundational work on SHAP, several specialized variants have been developed, including kernel SHAP by Covert and Lee (2021), which extends SHAP to kernel-based models; Tree SHAP (Yang, 2022), specifically designed for tree-based algorithms; and DeepSHAP (Chen et al., 2019a), tailored for deep learning architectures.</p>
<p>Other popular MA methods include counterfactuals and prototypes.Counterfactuals illustrate how much a particular instance's features would have to change to substantially influence the prediction (Chou et al., 2022).Prototypes can help to find the important samples in training set data when users want to see what the model considers typical or important (Gurumoorthy et al., 2019).</p>
<p>Model-specific Methods</p>
<p>Model-specific Methods for Tree-based AI Models</p>
<p>Tree-based feature importance (TFI), proposed by Breiman (2001), measures the relevance of variables in tree-based models such as RF, extreme gradient boosting, and gradient-boosting decision tree.TFI assesses feature importance by impurity reduction or disorder that results from randomly perturbating the feature of interest.And the impurity reduction or disorder is aggregated across all trees.</p>
<p>A feature is considered important for prediction as it significantly reduces this impurity score when used to split the dataset.TFI has been widely used in ESS modeling to identify and select influential factors.</p>
<p>Another interpreter for tree-based models, "Treeinterpreter" tool was developed by Saabas (2015).For any given sample, it extracts the decision path through the forest from the root node to the terminal leaf and calculates the contribution of each predictor, providing a granular explanation of how these models arrive at their predictions.</p>
<p>Model-specific methods for deep learning models</p>
<p>In the context of model-specific (MS) methods, we focus on XAI techniques tailored for DL models, including XAI methods applicable to all DL models, as well as specific XAI methods for RNN, CNN, and other specialized DL models targeting individual layers or cells.</p>
<p>XAI methods applicable to all DL models include gradient-based methods and the attention mechanism.Gradient-based methods treat DL models holistically and usually use backpropagating gradients through the DL model to estimate feature relevance (more details described in Appendix B).</p>
<p>Saliency maps, for instance, compute the gradient of the output with respect to the input image, thereby generating feature attribution maps (Simonyan et al., 2014).This strategy is followed by guided backpropagation (Springenberg et al., 2015), gradient-input (Shrikumar et al., 2019), integrated gradients (Sundararajan et al., 2017), smooth gradient (Smilkov et al., 2017), and expected gradients (Erion et al., 2021).While these methods can produce explicit, relatively robust feature attribution maps, they rely on the differentiability or smoothness of neurons in the model.To overcome this limitation, layer-wise relevance propagation (LRP) is based on the relevance between layers and is propagated from the output to the input, which computationally redistributes the importance across each input dimension of a sample (Bach et al., 2015).Another perturbation-based method, named Occlusion by Zeiler and Fergus (2014), systematically replaces contiguous parts of an input with a baseline value to assess their impact on the prediction function.The Jacobian matrix, similar to gradients for single-output networks, visualizes the backpropagation of errors, showing the differential calculus between inputs and outputs at each layer, thereby indicating the direction of sensitivity (Motteler et al., 1995;Aires et al., 2004;Rahwan et al., 2019).</p>
<p>Additionally, certain methods provide forward propagation representations within DL models.</p>
<p>Canonical correlation analysis (Burges, 2010) estimates the similarity between layers to explain DL models.These novel methods based on information similarity have also been used to diagnose DL models (Raghu et al., 2017;Kar et al., 2022).Furthermore, the attention mechanism introduced by Vaswani et al. (2017), is a disruptive innovation that aims to improve the ability of DL models to manage long-range dependencies while enhancing their explainability.This mechanism works by allowing the model to selectively focus its processing on certain segments of the input during task execution.It dynamically weights various input elements, indicating their relative importance or relevance.</p>
<p>Various methods have been developed for RNNs.Some approaches guide RNNs to learn and capture physical relationships (Tang et al., 2022) and spatial patterns (Milisav &amp; Misic, 2023), and enable interpretable sequential data processing (Hou et al., 2021).Other strategies transform RNNs into models that are both globally and locally interpretable by exploiting known theories in terms of time series processing (Wisdom et al., 2016).Probe techniques have also been used to uncover the latent representations within LSTM cells using a linear surrogate model (Lees et al., 2022) or a clustering-based surrogate model (Raghu &amp; Schmidt, 2020).Symbolic aggregate approximation often complements the LSTM by identifying significant points or sequences in time series data, highlighting which time periods are critical for the current prediction (Liu et al., 2023a).</p>
<p>When it comes to CNNs, many XAI methods have been tailored specifically for these architectures.Explaining some state-of-the-art DL models such as GANs, diffusion models, and Transformers typically involves extracting and interpreting their latent representations.For GANs, dissecting and analyzing the latent space can reveal which units influence the final generated samples (Bau et al., 2019) and help learn more salient features (Chen et al., 2016).The complex latent representations of diffusion models can be extracted through surrogating (Kwon et al., 2022), grouping (Lee et al., 2023), or prototyping (Aghasanli et al., 2023).In the case of Transformers, attention maps (Castangia et al., 2023) offer a means of visualization, although they are only partially interpretable (Clark et al., 2019).To improve interpretability, attention maps have been enhanced with heuristic propagation (Playout et al., 2022) and symbolic abstraction (Schwenke et al., 2023), the latter providing both global and local explanations.As a result, explaining models in terms of both global and local perspectives has become increasingly prevalent.</p>
<p>Evaluation of XAI</p>
<p>To ensure rigorous and verifiable research in XAI, there is a pressing need for comparable evaluation metrics to address the current lack of quantitative evaluation methods.A recent review by Nauta et al. (2023) shows that 58% of technical evaluations in XAI rely on quantitative measures, while 33% use only anecdotal evidence and 22% involve human subject evaluations.The use of quantitative measures facilitates rigorous evaluation by multiple metrics, whereas human evaluation relies on intuitive notions of quality defined by researchers or users.Anecdotal evidence typically depends on what constitutes the "ground truth", which can typically be derived from synthetic datasets, causal relationships, or domain knowledge.</p>
<p>In the context of ESS, nonlinear functions derived from actual processes (Mamalakis et al., 2022a) and benchmark datasets specifically designed to evaluate explanations (Arras et al., 2022;Mamalakis et al., 2022b) serve as the "ground truth" standards against which XAI methods are measured.Table 2 presents case studies that serve as examples for evaluating XAI methods.The recommended XAI approaches can serve as a preliminary guide for those new to the field.Robustness is defined as "the persistence of a method for explainability to withstand small perturbations of the input that do not change the prediction of the model" (Alvarez-Melis and Jaakkola, 2018).Huang et al. (2020) emphasize that this metric measures how consistently an explanation remains valid despite small input perturbations.Stability refers to the repeatability of explanations under similar or neighboring inputs; a stable XAI method provides consistent explanations under small input changes (Alvarez-Melis and Jaakkola, 2018).Efficiency concerns the computational cost required to generate an explanation (Adadi &amp; Berrada, 2018;Vilone &amp; Longo, 2021).</p>
<p>In Table 2, SHAP was found to be more advantageous than the others due to its higher faithfulness for tabular data, while IG and LRP more valid when dealing with image data.It is important to note that these evaluations are not definitive, but rather indicative.The suitability of XAI methods may vary depending on the specific use case, so a thorough evaluation is recommended before deployment.To date, there is no one perfect method for all cases.To assist newcomers to the field, several practical XAI packages and evaluation toolkits are provided in Table A2.</p>
<p>XAI: From Theory to ESS Application</p>
<p>The Needs for XAI</p>
<p>In Section 2.1, we emphasize the importance of considering "for a given audience" in ESS since different stakeholders have varying needs for explanations.The needs significantly influence interpretative form and XAI method selection within specific tasks.In the context of XAI for ESS, audiences can be broadly classified into four primary groups: weather forecasters, policymakers and regulators, AI modelers, and process-based modelers, and geoscientists (in Figure 3).Each group has different needs and goals:</p>
<p>Weather forecasters: these professionals are tasked with assessing the accuracy of forecasts, essentially answering the question, "Is the forecast accurate and why?".XAI may provide valuable insight to assist forecasters by comparing predictions with those from numerical weather prediction models or radar extrapolation, along with their expert judgment based on experience.They often require solid evidence to determine what factors contribute to extreme predictions and what indicators signal hazards (Schultz et al., 2021).</p>
<p>Policymakers and regulators: they are responsible for ensuring that AI-enabled policymaking is both reliable and safe and want to know "Is the AI trustworthy?".They need that the AI model and its explanation are trustworthy.Alternatively, scientific insights from AI can provide essential guidance and evidence for policymaking on issues such as climate change (Dhar, 2020;Haupt et al., 2021).Therefore, regulators need to examine the potential risks beneath the surficial successes of AI.They seek valid, generalizable, and compatible explanations to support informed decision-making.</p>
<p>AI modelers: they focus on uncovering the root causes of poor model performance or generalizability, and need to know "How can we make AI better?".XAI can reveal biases and errors in AI models that may be due to data quality, model architecture, or training procedures.These insights help developers design, debug, and refine models.</p>
<p>Process-based modelers and geoscientists: they intend to improve our understanding of ESS processes through AI insights focused on the question of "How to get meaningful insights?".If properly interpreted, AI's ability to capture non-linearity and coupled states is highly valuable.XAI is potential to help modelers and geoscientists generate hypotheses.For example, the non-linear relationship between evapotranspiration and soil moisture revealed by XAI may help to understand land-atmosphere interactions (Bergen et al., 2019).It emphasizes that the physical insights derived from XAI should be  The orange lines represent the general machine learning process, while the blue lines highlight the tailored explanation processes for each audience.The blue boxes encapsulate the key questions that these audiences seek to answer using XAI methods.The abbreviations PI, SI, FI, FE, MR, and RE represent pixel importance, sequence importance, feature importance, feature effect, model representation, and rule explanation, respectively-each serving as an example of an interpretative form relevant to a specific task within XAI for ESS applications.</p>
<p>The four audience groups outlined above each have three central objectives when using XAI:</p>
<p>communicating model decisions (for all users), diagnosing and improving models (primarily for AI modelers), and uncovering scientific insights (primarily for process-based modelers and geoscientists).</p>
<p>These three aspects are discussed in detail in Sections 3.3 to 3.5.</p>
<p>Current States of XAI-ESS</p>
<p>In this review, we employ a systematic approach to identify and evaluate literature on XAI applications in ESS, as detailed in Appendix C. We retrieved a total of 285 papers, with the use of XAI Figure 4a illustrates the global distribution of study areas and corresponding countries, showing that 44 studies had a worldwide scope.The United States leads with 84 studies, followed by China with 72 studies.Europe, Australia, and India also emerged as significant regions of interest for researchers using XAI methods.A word cloud generated from the titles of the selected papers (Figure 4b) highlights the focus of geoscientists on XAI in ESS.In addition to core topics such as XAI, DL, and ML, "prediction"</p>
<p>and "forecasting" were prominent themes, underlining the importance of interpretability in predictive tasks.Terms such as "drivers", "insights" and "factors" further emphasize the need for XAI to provide physical insights within ESS.</p>
<p>The majority of the selected papers pertained to sub-domains including land, hydrology, atmosphere, and hazards.The criteria for dividing these sub-domains are outlined in Figure A3.Researchers predominantly used XAI to address complexities related to water, carbon, and energy flows, exchanges, and cycles between land and atmosphere.Figure 4d indicates that precipitation, streamflow, and temperature have received considerable attention.Moreover, hazards such as droughts, floods, landslides, earthquakes, and wildfires-shown in Figs.4c and 4d-are complex issues where XAI has potential applications.Agriculture is another important sub-domain within XAI-ESS, with crop yield being the most studied variable, as shown in Figure 4d.</p>
<p>In terms of XAI method types, model-agnostic (MA) methods (57.9%) are the most widely applied due to their high flexibility.Meanwhile, 30.9% and 11.2% of the studies used model-specific (MS) and inherent interpretable AI (IT) methods, respectively.Among them, SHAP was the most popular XAI method in ESS, followed by PI and TFI, all three of which offer feature importance while typically having lower computational cost.Within the MS category, LRP and AM were the two most commonly used techniques.</p>
<p>Communicating Model Decisions</p>
<p>XAI plays a critical role in communicating model decisions across various audiences and prediction tasks.Weather forecasters often rely on local feature or pixel importance and rule-based methods to interpret AI-generated forecasts for individual stations or spatio-temporal predictions (Basak et al., 2022;Lim and Zohren, 2021).Feature importance for a specific instance (local) allows them to assess the contribution of specific predictors and validate explanations against their expertise, particularly in timeseries predictions for droughts (Feng et al., 2020;Dikshit et al., 2022;Huang et al., 2023a, b), floods (Yang et al., 2020;Ekmekcioğlu et al., 2021), and landslides (Al-Najjar et al., 2022).Pixel importance has also been successfully used to interpret DL models for spatial climate phenomena such as hail (Gagne II et al., 2019), sea temperature, and river discharge (Toms et al., 2020(Toms et al., , 2021;;Liu et al., 2023b), and oscillation patterns (Schmidt et al., 2020;Gordon et al., 2021;Martin et al., 2022).Rule explanation methods such as surrogate decision trees further aid weather forecasting (Chen et al., 2021).</p>
<p>Policymakers and regulators require high levels of transparency and trustworthiness when using AIgenerated predictions, especially in the context of climate change and natural hazards (Haupt et al., 2021;Debnath et al., 2023).Guidelines from UNESCO (2021) and the European Commission's AI Act (2021) require accountability before AI can be used.For instance, DL combined with LRP has been used to provide actionable insights into future climate projections (Diffenbaugh &amp; Barnes, 2023).</p>
<p>For Earth observation researchers using AI, XAI offers a variety of strategies to increase the credibility of AI-generated outputs.Contemporary data products derived from observations require interpretative support to enhance confidence in their use (Shangguan et al., 2017;Li et al., 2022b;Shangguan et al., 2023).Dueben et al. (2022) emphasized that explainability must be integral to the construction of AI-based datasets that are accountable to users.Gevaert (2022) further emphasized the importance of explainability of AI in Earth observation for producers, users, and regulators alike.To date, numerous advanced ESS products have been subjected to interpretation to verify their physical consistency, including remote sensing classification tasks (Stomberg et al., 2021;Hasanpour Zaryabi et al., 2022), as well as products based on observations such as soil moisture (O.&amp; Rene, 2021), soil carbon (Wadoux et al., 2022), surface water levels (Koch et al., 2019), and crop type identification (Orynbaikyzy et al., 2020).</p>
<p>In summary, XAI has demonstrated professional competence in communicating the rationale behind model decisions by extracting explanatory information from the models themselves.This review aims to foster trust in AI models among researchers and policymakers involved in ESS.It advocates the integration of human-in-the-loop AI design, which is not only beneficial for action initiatives such as climate change mitigation but also crucial for effective ESS management.</p>
<p>Diagnosing and Improving Models</p>
<p>Model diagnosis and improvement is a major challenge for AI modelers who seek to understand why an AI model underperforms on specific tasks.Researchers in ESS typically use XAI for feature selection, model determination, and model structure design.Feature selection is essential to ensure the generalizability of a model and to prevent overfitting (Guyon &amp; Elisseeff, 2003;Reunanen, 2003).Unlike traditional filtering methods, which are independent of learning models and rely on data-based evaluation metrics, XAI can directly reveal a model's preference for features, allowing for more informed selection.</p>
<p>Feature importance methods such as TFI (Feng et al., 2019;Upadhyaya et al., 2021), PI (Ramirez et al., 2022), and attention mechanisms for DL models (Yan et al., 2021b) are commonly used in ESS research.</p>
<p>Studies have shown that XAI-driven feature selection yields better results in prediction tasks (Zacharias et al., 2022;Wang et al., 2023a).Moreover, this approach is consistent with physical processes; Carter et al. (2021) showed how features selected using TFI were consistent with anomalous hydroclimatic circulation patterns when predicting precipitation.</p>
<p>Selecting an appropriate AI model can be both time consuming and computationally intensive.</p>
<p>Modelers often assess feature importance in potential models using MA methods to select a model that best fits the underlying physics (Jing et al., 2023).For instance, Schmidt et al. (2020) applied PI to interpret ANN, RF, and linear model in flood forecasting and found varying degrees of agreement between these models and basic hydrological concepts.Hu et al. (2021) compared SHAP explanations of PB, RF, and DL for evapotranspiration and found more similarities between PB and DL suggesting inaccuracies in RF.These findings support a more physically rational model selection.</p>
<p>Identifying biases and errors in model structures can be challenging because adjusting a single component can change the entire model behavior.AI modelers prefer inherently transparent models such as decision trees (Chen et al., 2021) or cubist (Fu et al., 2020).However, they usually result in a loss of model performance and are not suitable for high-dimensional data.Many XAI methods address this issue and enable diagnosis of complex AI models with high-dimensional data.For example, the "treeinterpreter" can pinpoint errors in tree-based precipitation classification models (Upadhyaya et al., 2021), allowing easy identification of problematic tree nodes.In DL models, the Jacobian matrix initially facilitated the removal of less influential connections to profile the atmosphere at high resolution (Blackwell, 2012;Maddy et al., 2021).Shamekh et al. (2023) showed promise in integrating latent representations into AI refining, achieving high accuracy in precipitation intensity prediction.</p>
<p>In summary, XAI is useful for AI modelers optimize performance and build physically consistent models by looking inside black-box models.It has proven effective in aiding data/feature and model selection, thereby enhancing predictive performance in ESS contexts.Ultimately, XAI is demonstrated a valuable asset in the AI model development process.</p>
<p>Providing Scientific Insights</p>
<p>XAI holds great promise for offering physical insights into the ESS due to its remarkable capability of extracting nonlinear relationships.Unlike traditional PB models that generate physical understanding by isolating coupling states or processes, which can introduce substantial systematic bias (Cook et al., 2006;Tuttle and Salvucci, 2016), XAI interprets relationships directly from the data, streamlining the process and conserving computational resources.Here, we showcase some hotspot examples of XAI's success in providing physical insights in ESS sub-domains, including atmospheric science, climatology, oceanography, hydrology and natural hazards.</p>
<p>In atmospheric science, XAI has successfully visualized the complex dynamics of deep convection beneath tropical cyclones (McNeely et al., 2020).Heatmaps generated by XAI have revealed associations between large-scale atmospheric conditions, the occupied area, and the organization degree of deep convection (Retsch et al., 2022).Moreover, the latent representation of a variational autoencoders model has been used to identify different convective regimes in precipitation generation (Behrens et al., 2022).</p>
<p>In climatology, researchers often utilize XAI to pinpoint potential drivers of global warming and extreme precipitation events.They have found that anthropogenic factors contribute to rising temperatures, such as urban green infrastructure (Zumwald et al., 2021) and aerosols/greenhouse gases (Labe &amp; Barnes, 2021), as well as natural factors such as the Pacific decadal oscillation and oceanatmosphere interactions (Vijverberg &amp; Coumou, 2022).To detect the anthropogenic signal amidst natural variability, Madakumbura et al. (2021), inspired by Barnes et al. (2020), developed an ANN to predict the year associated with annual maps from ESMs.This approach allows XAI to identify patterns of timevarying anthropogenic fingerprints.XAI is also widely used to evaluate the impact of global warming on various aspects of ESS, including carbon dynamics (Patoine et al., 2022), greening trends (Berner et al., 2020), soil respiration (Haaf et al., 2021), soil carbon uptake (Wang et al., 2022c), vegetation (Li et al., 2022c), surface water availability (Webb et al., 2022), groundwater (Chakraborty et al., 2021a;Liu et al., 2022a) and runoff (Anderson et al., 2022).</p>
<p>In oceanography, inherently interpretable linear regression and clustering models are becoming increasingly popular for large-scale modeling of both ocean and atmosphere (Janssens et al., 2021;DelSole &amp; Nedza, 2022).For instance, clustering methods have been instrumental in revealing global ocean dynamical regions, thereby improving the representation of the barotropic vorticity equation, despite the challenges of accurately describing ocean circulation (Sonnewald et al., 2019 and2023).LRP has also been used to investigate how global heating affects the North Atlantic circulation (Sonnewald &amp; Lguensat, 2021).Sonnewald et al. (2023) noted that insights from XAI could be used to develop a hierarchy of conceptual models of ocean structure and circulation, which could represent an important advance in our understanding of the ocean.</p>
<p>In hydrology, XAI facilitates effective sensitivity analysis and hypothesis generation regarding factors influencing hydrodynamics.For example, it helped identify vegetation (Althoff et al., 2021) and geography (Bai et al., 2022;Liu et al., 2022a) as crucial elements in runoff modeling.SHAP revealed that, when slope is higher than 20%, the runoff is more easily to occur (Wang et al., 2022b).And LIME indicated thresholds for climate variables affecting evapotranspiration (Chakraborty et al., 2021b).</p>
<p>Additionally, XAI was used to clarify the parameters of the Budyko framework and to understand hydrological partitioning (Cheng et al., 2022).</p>
<p>Describing and predicting natural hazards including floods, droughts, and wildfires is inherently difficult.XAI helps unravel the underlying mechanisms that may drive these extreme phenomena.In flood analysis, traditional methods often rely on threshold-based techniques (Bertola et al., 2021) or probabilistic frameworks (Blöschl et al., 2019).For example, Stein et al. (2021) used ALE to analyze the nonlinear dynamics of flooding, revealing snowfall and precipitation as significant contributors.Several XAI methods have been used to derive data-driven physical insights into flooding, including SHAP (Yang et al., 2020;Hagen et al., 2021;Aydin and Iban, 2023) and gradient-based approaches tailored for LSTM models (Jiang et al., 2022a, b).In the context of drought studies, XAI has been instrumental in characterizing the generation and properties of droughts (Feng et al., 2019;Saha et al., 2021).For instance, Vidyarthi and Jain (2020) adopted a surrogate decision tree model to elucidate the processes of droughts.In wildfire research, XAI methods are often used to identify which variables significantly influence wildfire behavior.Techniques such as SHAP (Cilli et al., 2022;Abdollahi &amp; Pradhan, 2023;Bountzouklis et al., 2023), IG, and CAM (Monaco et al., 2021;Kondylatos et al., 2022) have been used</p>
<p>to discern these sensitivities.For a more comprehensive overview of XAI applications in hazard assessment within ESS, the readers are referred to Ghaffarian et al. (2023).</p>
<p>In summary, XAI has made significant contributions to attribution analysis owning to its ability to fuse information and capture nonlinear relationships (Beucher et al., 2022).This is evidenced by a variety of applications in atmospheric, hydrological, oceanic, and natural hazard contexts within ESS.The ESS community is keenly interested in discovering universal and portable knowledge -phenomena that persist over long periods of time and AI-learned knowledge that is applicable across regions and scales, distilled into fundamental physical understanding.Techniques such as ALE, PDP, and SHAP excel at revealing nonlinear relationships, while LRP and IG highlight spatial correlations among variables.</p>
<p>However, rigorous validation of model assumptions and XAI methods is essential to avoid misleading results (Stadtler et al., 2022).The findings of previous studies underscore the importance of considering multiple factors and employing advanced techniques to deepen our understanding of ESS complexities.</p>
<p>Summary</p>
<p>AI has become an essential component of Earth system science and modelling.The explanation of AI through XAI has been the subject of high expectations in this field.However, there are many details that need to be clarified due to the mismatch between XAI design and ESS background.This review systematically introduces XAI technology to the ESS community, from its theory to ESS applications, while taking into account the ESS background.We present XAI theory, which includes its definition, approaches to explanation, popular techniques, and technical assessment.</p>
<p>We provide a comprehensive literature review on XAI-ESS applications.According to different needs of ESS stakeholders, the current XAI applications can be roughly classified into three functionalities: (1) to communicate with AI decisions through more understandable explanations by demonstrating the physical consistency learned by the AI using XAI methods, (2) to assist AI modelers to diagnose the AI models and guide model building, and (3) to provide scientific insights that helps geoscientists uncover nonlinear relationships hidden in the data and attribution analysis.</p>
<p>However, due to the mismatch between XAI design and ESS requirements, many challenges are associated with the applications.Accordingly, we outline these challenges of using XAI and propose solutions, such as: (1) integrative multiple XAI strategies may help avoid difficulties in selecting the optimal XAI method to ensure faithful explanations;</p>
<p>(2) causal physics-informed hybrid XAI modeling could potentially resolve compatibility issues in XAI-ESS arising from varying designs; (3) designing XAI with a human-centered approach for various stakeholders in ESS can address the issues arising from the mismatch between XAI design and the requirements of the ESS community; (4) establishing a quantitative platform for comparison maybe a solution for incommensurability between XAI explanation and physical knowledge.</p>
<p>In the future, we anticipate that XAI to flourish in the field of ESS.Integrated XAI techniques that are more faithful and lighter will be popular in this sub-domain.We also expect more effective XAI</p>
<p>Figure 1 .
1
Figure 1.Number of publications related to explainable artificial intelligence (XAI) in various fields.(a) number of publications in XAI application fields; (b) growth trend of XAI publications in Web of Science (2015~2023); (c) in Scopus (2015~2023).Data was retrieved on December 31, 2023, using the search terms listed in the legend for each database query.</p>
<p>Figure 2 .
2
Figure 2. A visual representation of XAI.(a) XAI taxonomy; (b) Approaches for to explaining black-box AI models including surrogate models, perturbation-based methods and information extraction methods; (c) interpretative forms of XAI and the questions they address.This example utilizes uses soil moisture (SM) prediction to demonstrate how different XAI approaches can explain the impact of air temperature (TA) on predictions in distinct ways.</p>
<p>One widely used technique is class activation mapping (CAM), which replaces the fully connected layer with a global average pooling layer in CNNs, thereby identifying important image regions by mapping output layer weights onto convolutional feature maps; channels with higher activations correspond to more informative signals.Gradient-weighted CAM (Grad-CAM), introduced bySelvaraju et al. (2020), refines this concept by generating a weighted class activation map that combines gradients and CAM, offering a coarser-grained visualization.Extensions of CAM, such as U-CAM(Patro et al., 2018), Eigen-CAM (Bany &amp; Yeasin, 2021), and Score-CAM(Ibrahim &amp; Shafiq, 2022), have gained popularity in computer vision applications.Alternative XAI methods modify the loss function to enhance explainability(Springenberg et al., 2015), while others integrate visualization components such as prototype layers(Chen et al., 2019b)  or autoencoders(Zhang et al., 2021b;Tavanaei, 2021) to explain CNN functionality.</p>
<p>reliable and verifiable.Geoscientists use XAI to explain natural processes, while process-based modelers use it to address limitations in ESMs (Ebert-Uphoff &amp; Hilburn, 2020;Ross et al., 2023).Ultimately, they strive for a fully accurate and robust modeling paradigm for the ESS(Roscher et al., 2020).</p>
<p>Figure. 3
3
Figure. 3 A schematic overview of the XAI workflow for four audience groups in the context of ESS.</p>
<p>concepts in ESS research ranging from 2007 to 2023. Figure A2 graphically depicts the annual distribution of these selected papers, showing a marked increase in XAI-ESS publications from 2021 and continuing through 2023.</p>
<p>Figure 4 .
4
Figure 4.An overview of articles on XAI in ESS from 2007 to 2023.(a) study area, (b) word clouds of acritical titles, (c) sub-domains, (d) targeting variables, (e) XAI types (IT: inherently interpretable AI models; MA: model-agnostic methods; MS: model-specific methods) and (f) XAI methods.</p>
<p>.D., Schultz, M.G., Chantry, M.,Gagne, D.J., Hall, D.M., &amp; McGovern, A. (2022).Challenges and Benchmark Datasets for Machine Learning in the Atmospheric Sciences: Definition, Status, and Outlook.Artificial Intelligence for the Earth Systems, 1, e210002.https://doi.org/10.1175/AIES-D-21-0002.1 Dutta, A., Vreeken, J., Ghiringhelli, L.M., &amp; Bereau, T. (2021).Data-driven equation for drug-membrane permeability across drugs and membranes.The Journal of Chemical Physics, 154, 244114.https://doi.org/10.1063/5.0053931</p>
<p>Table 2 .
2
Overview of representative case studies evaluating XAI methods across three evaluation metrics, including quantitativeness (Q), anecdotal evidence (AE), and human subjectivity (HS).For each case,
information is provided including research fields, targets, tasks (classification [C] or prediction [P]), inputdata formats (image [I] or tabular data [T]), and recommended method(s) among those compared.InputRecom-datamendedMetrMethods inReferenceFieldObjectTaskformatMethod(s)iccomparison*AgriculturePlantCILIMEAEG-CAM,(de Benito Fernández etdiseaseSHAPal., 2023)RemoteWater,CIOC,AESA, IG, GI,(Kakogeorgiou &amp;sensingforestLIMEG-CAM, GG-Karantzalos, 2021)CAM, GBAtmosphere AtmospheCILRPAESA, SG, PN,(Mamalakis et al.,ric riverIG, GI, DTD,2022b)DSHAPClimateTemperatPILRPAESA, SG, DTD,(Mamalakis et al.,ureLRP, GI, IG,2022a)OCClimateTemperatPILRPQSA, IG, GI,(Bommer et al., 2023)ureSG, NG, FGCVRecognitiCIIGQSA, GI, IG,(Alvarez-Melis andonLRP, OCJaakkola, 2018)CVRecognitiCIIG, DTQDTD, IG, SA,(Kindermans et al.,onPN, PA, GB,2019)SGMedicineElectroencPI\QPN, SG, LRP(Mayor Torres et al.,ephalogra2023)phyMedicineSkinCIIGQIG, SHAP,(Saarela &amp; Geogieva,cancerLIME2022)MedicineBreastPTSHAPGQLR, DT(Mariotti et al., 2023)cancerFinancePaymentPTSHAP,HS/LIME(Jesus et al., 2021)TRQInternetTrojanPPFAQSA, GB, G-(Lin et al., 2021)CAM, GG-CAM, OC,FA, LIMEHealthInsuranceC/PTSHAPQPI(Lozano-Murcia et al.,2023)
Weber et al. (2023)9)01Bommer et al., 2023)n subjective judgment and anecdotal evidence in ESS is currently challenging due to the lack of clear ground truth.Therefore, this review focuses more on the quantitative approach.Four critical metrics are considered: faithfulness, stability, robustness, and efficiency, as discussed byMelis and Jaakkola (2018),Murdoch et al. (2019),Minh et al. (2022), andWeber et al. (2023).Faithfulness measures the extent to which an XAI method accurately identifies important features that genuinely influence a model's prediction; if an XAI method assigns high relevance, it should actually change the outcome (Alvarez-Melis &amp; Jaakkola, 2018;Bommer et al., 2023).</p>
<p>Covert, I., &amp; Lee, S.-I.(2021).Improving KernelSHAP: Practical Shapley Value Estimation via Linear Regression.Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:3457-3465.Cowls, J., Tsamados, A., Taddeo, M., &amp; Floridi, L. (2023).The AI gambit: leveraging artificial intelligence to combat climate change-opportunities, challenges, and recommendations.AI &amp; Society, 38, 283-307.https://doi.org/10.1007/s00146-021-01294-xde Benito Fernández, M., Martínez, D.L., González-Briones, A., Chamoso, P., &amp; Corchado, E.S. (2023). of XAI Models for Interpretation of Deep Learning Techniques' Results in Automated Plant Disease Diagnosis.Paper presented at the Trends in Sustainable Smart Cities and Territories.SSCT (2023).Lecture Notes in Networks and vol 732.Cham, Switzerland.https://doi.org/10.1007/978-3-031-36957-5_36Debnath, R., Creutzig, F., Sovacool, B.K., &amp; Shuckburgh, E. (2023).Harnessing human and machine intelligence for planetary-level climate action.npj Climate Action, 2, (20.https://doi.org/10.1038/s44168-023-00056-3DelSole, T., &amp; Nedza, D. (2022).Reconstructing the Atlantic Overturning Circulation Using Linear ://doi.org/10.1080/07055900.2021.1947181Dhar, P. (2020).The carbon impact of artificial intelligence.Nature Machine Intelligence, 2, 423-425.https://doi.org/10.1038/s42256-020-0219-9Diffenbaugh, N.S., &amp; Barnes, E.A. (2023).Data-driven predictions of the time remaining until critical global warming thresholds are reached.Proceedings of the National Academy of Sciences of the United States of America, 120, e2207183120.https://doi.org/10.1073/pnas.2207183120Dikshit, A., &amp; Pradhan, B. (2021a).Explainable AI in drought forecasting.Machine Learning with Applications, 6, 100192.https://doi.org/10.1016/j.mlwa.2021.100192Dikshit, A., &amp; Pradhan, B. (2021b).Interpretable and explainable AI (XAI) model for spatial drought Paper presented at the 2018 41st International Convention on Information and Communication ://doi.org/10.23919/MIPRO.2018.8400040Du M., Liu N., Yang F., &amp; Hu X. (2019).Learning credible deep neural networks with rationale regularization.IEEE International Conference on Data Mining(ICDM) 2019, Beijing, China, IEEE
Evaluation MachineLearningTechniques.Atmosphere-Ocean,60,541-553.prediction.ScienceofTheTotalEnvironment,801,149797.https://doi.org/10.1016/j.scitotenv.2021.149797Dikshit, A., Pradhan, B., &amp; Santosh, M. (2022). Artificial neural networks in drought prediction in the21stcentury-Ascientometricanalysis.AppliedSoftComputing,17.Technology,ElectronicsandMicroelectronics(MIPRO),Opatija,Croatia.
httpshttps://doi.org/10.1016/j.asoc.2021.108080Dosilovic, F.K., Brcic, M., &amp; Hlupic, N. (2018).Explainable artificial intelligence: A survey.https</p>
<p>methods to be introduced in ESS, especially for diagnosing AI models.Ultimately, we recommend that XAI can effectively help with many issues in ESS, such as new knowledge, data, hybrid modeling, and difference in ESMs, human-computer interaction, and policy proposals.
Explainable artificial intelligence (XAI) for interpreting the contributing factors feed into the wildfire susceptibility prediction model. A Abdollahi, B Pradhan, 10.1016/j.scitotenv.2023.163004Science of The Total Environment. 8791630042023</p>
<p>From attribution maps to human-understandable explanations through Concept Relevance Propagation. R Achtibat, M Dreyer, I Eisenbraun, S Bosse, T Wiegand, W Samek, S Lapuschkin, 10.1038/s42256-023-00711-8Nature Machine Intelligence. 52023</p>
<p>Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI). A Adadi, M Berrada, 10.1109/ACCESS.2018.2870052IEEE Access. 62018</p>
<p>Interpretable-through-prototypes deepfake detection for diffusion models. A Aghasanli, D Kangin, P Angelov, 10.1109/ICCVW60793.2023.000532023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW). Paris, FranceIEEE2023</p>
<p>Neural network uncertainty assessment using Bayesian statistics: A remote sensing application. F Aires, C Prigent, W B Rossow, 10.1162/0899766041941925Neural Computation. 16112004</p>
<p>CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models. iScience, 25. A R Akula, K Wang, C Liu, S Saba-Sadiya, H Lu, S Todorovic, 10.1016/j.isci.2021.1035812022103581</p>
<p>A novel method using explainable artificial intelligence (XAI)-based Shapley Additive Explanations for spatial landslide prediction using Time-Series SAR dataset. H A H Al-Najjar, B Pradhan, G Beydoun, R Sarkar, H.-J Park, A Alamri, 10.1016/j.gr.2022.08.0042022Gondwana Research, S1342937X22002386</p>
<p>Addressing hydrological modeling in watersheds under land cover change with deep learning. D Althoff, L N Rodrigues, D D Silva, Da, 10.1016/j.advwatres.2021.103965Advances in Water Resources. 1541039652021</p>
<p>On the Robustness of Interpretability Methods. D Alvarez-Melis, T S Jaakkola, 10.48550/arXiv.1806.080492018</p>
<p>Finding and removing Clever Hans: Using explanation methods to debug and improve deep models. Information Fusion. C J Anders, L Weber, D Neumann, W Samek, K.-R Müller, S Lapuschkin, 10.1016/j.inffus.2021.07.015202277</p>
<p>Evaluation and interpretation of convolutional long short-term memory networks for regional hydrological modelling. S Anderson, V Radić, 10.5194/hess-26-795-2022Hydrological. Earth System Science. 262022</p>
<p>Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. D W Apley, J Zhu, 10.48550/arXiv.1612.084682019</p>
<p>CLEVR-XAI: A benchmark dataset for the ground truth evaluation of neural network explanations. Information Fusion. L Arras, A Osman, W Samek, 10.1016/j.inffus.2021.11.008202281</p>
<p>Predicting and analyzing flood susceptibility using boosting-based ensemble machine learning algorithms with SHapley Additive exPlanations. H E Aydin, M C Iban, 10.1007/s11069-022-05793-yNature Hazards. 1162023</p>
<p>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. S Bach, A Binder, G Montavon, F Klauschen, K.-R Müller, W Samek, 10.1371/journal.pone.0130140PLoS ONE. 10e01301402015</p>
<p>Groundwater Potential Mapping in Hubei Region of China Using Machine Learning, Ensemble Learning, Deep Learning and AutoML Methods. Z Bai, Q Liu, Y Liu, 10.1007/s11053-022-10100-4Nature Resource Research. 312022</p>
<p>Eigen-CAM: Visual explanations for deep convolutional neural networks. Bany Muhammad, M Yeasin, M , 10.1007/s42979-021-00449-3SN Computer Science. 21472021</p>
<p>Indicator Patterns of Forced Change Learned by an Artificial Neural Network. E A Barnes, B Toms, J W Hurrell, I Ebert-Uphoff, C Anderson, D Anderson, 10.1029/2020MS002195Journal of Advances in Modeling Earth Systems. 122020</p>
<p>Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion. A Barredo Arrieta, N Díaz-Rodríguez, J Del Ser, A Bennetot, S Tabik, A Barbado, 10.1016/j.inffus.2019.12.012202058</p>
<p>A Review on Interpretable and Explainable Artificial Intelligence in Hydroclimatic Applications. H Başağaoğlu, D Chakraborty, C D Lago, L Gutierrez, M A Şahinli, Giacomoni, 10.3390/w140812302022Water141230</p>
<p>From data to interpretable models: machine learning for soil moisture forecasting. A Basak, K M Schmidt, O J Mengshoel, 10.1007/s41060-022-00347-8International Journal of Data Science and Analytics. 152022</p>
<p>Visualizing and understanding generative adversarial networks. D Bau, J Y Zhu, H Strobelt, B Zhou, J B Tenenbaum, W T Freeman, A Torralba, 10.48550/ARXIV.1901.098872019</p>
<p>Non-Linear Dimensionality Reduction With a Variational Encoder Decoder to Understand Convective Processes in Climate Models. G Behrens, T Beucler, P Gentine, F Iglesias-Suarez, M Pritchard, V Eyring, 10.1029/2022MS003130Journal of Advances in Modeling Earth Systems. 142022</p>
<p>On the interpretability of predictors in spatial data science: the information horizon. T Behrens, R A Viscarra Rossel, 10.1038/s41598-020-73773-yScience Report. 10167372020</p>
<p>Machine learning for data-driven discovery in solid Earth geoscience. K J Bergen, P A Johnson, M V De Hoop, G C Beroza, 2019</p>
<p>. 10.1126/science.aau0323Science. 363323</p>
<p>Summer warming explains widespread but not uniform greening in the Arctic tundra biome. L T Berner, R Massey, P Jantz, B C Forbes, M Macias-Fauria, I Myers-Smith, 10.1038/s41467-020-18479-5Nature Communication. 1146212020</p>
<p>Do small and large floods have the same drivers of change? A regional attribution analysis in Europe. M Bertola, A Viglione, S Vorogushyn, D Lun, B Merz, G Blöschl, 10.5194/hess-25-1347-2021Hydrology and Earth System Sciences. 252021</p>
<p>Interpretation of Convolutional Neural Networks for Acid Sulfate Soil Classification. A Beucher, C B Rasmussen, T B Moeslund, M H Greve, 10.3389/fenvs.2021.809995Frontiers in Environmental Science. 98099952022</p>
<p>Accurate medium-range global weather forecasting with 3D neural networks. K Bi, L Xie, H Zhang, X Chen, X Gu, Q Tian, 10.1038/s41586-023-06185-3Nature. 6192023</p>
<p>Explanation and justification in machine learning: A survey. O Biran, C Cotton, IJCAI-17 Workshop on Explainable AI. Melbourne, Australia2017</p>
<p>Neural network Jacobian analysis for high-resolution profiling of the atmosphere. W J Blackwell, 10.1186/1687-6180-2012-71EURASIP Journal on Advances in Signal Processing. 712012. 2012</p>
<p>Changing climate both increases and decreases European river floods. G Blöschl, J Hall, A Viglione, R A P Perdigão, J Parajka, B Merz, 10.1038/s41586-019-1495-6Nature. 5732019</p>
<p>Finding the right XAI method --A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science. P Bommer, M Kretschmer, A Hedström, D Bareeva, M M Höhne, -C , 10.48550/arXiv.2303.006522023</p>
<p>Predicting wildfire ignition causes in Southern France using eXplainable Artificial Intelligence (XAI) methods. C Bountzouklis, D M Fox, E D Bernardino, 10.1088/1748-9326/acc8eeEnvironmental Research Letters. 18440382023</p>
<p>L Breiman, 10.1023/A:1010933404324Random Forests, Machine Learning. 200145</p>
<p>Dimension reduction: a guided tour. C J C Burges, 10.1561/2200000002Foundations and Trends® Machine Learning. 20102</p>
<p>Explainable and Transparent AI and Multi-Agent Systems: Third International Workshop, EXTRAAMAS 2021, Virtual Event. D Calvaresi, A Najjar, M Winikoff, K Främling, 10.1007/978-3-030-82017-6Lecture Notes in Computer Science. 2021. May 3-7, 2021Springer International PublishingRevised Selected Papers</p>
<p>Machine learning in remote sensing data processing. G Camps-Valls, 10.1109/MLSP.2009.53062332009 IEEE International Workshop on Machine Learning for Signal Processing. Grenoble, FranceIEEE2009</p>
<p>Advancing deep learning for earth sciences: from hybrid modeling to interpretability. Paper presented at the IGARSS 2020 -2020 IEEE International Geoscience and Remote Sensing Symposium. G Camps-Valls, M Reichstein, X Zhu, D Tuia, 10.1109/IGARSS39084.2020.93235582020IEEEWaikoloa, HI, USA</p>
<p>Feature Engineering for Subseasonal-to-Seasonal Warm-Season Precipitation Forecasts in the Midwestern United States: Toward a Unifying Hypothesis of Anomalous Warm-Season Hydroclimatic Circulation. E Carter, D A Herrera, S Steinschneider, 10.1175/JCLI-D-20-0264.1Journal of climate. 34282021</p>
<p>M Castangia, L M M Grajales, A Aliberti, C Rossi, A Macii, 10.1016/j.envsoft.2022.105581Transformer neural networks for interpretable flood forecasting. Environmental Modelling &amp; Software. 2023160105581</p>
<p>Evaluation, Tuning, and Interpretation of Neural Networks for Working with Images in Meteorological Applications. D Chakraborty, H Başağaoğlu, L Gutierrez, A Mirchi, 10.1175/BAMS-D-20-0097.1Bulletin of the American Meteorological Society. Environmental Research Ebert-Uphoff, I., &amp; Hilburn, K.1012021a. 2020Explainable AI reveals new hydroclimatic insights for ecosystem-centric groundwater management</p>
<p>District based flood risk assessment in Istanbul using fuzzy analytical hierarchy process. Ö Ekmekcioğlu, K Koc, M Özger, 10.1007/s00477-020-01924-8Stoch Environ Res Risk Assess. 352021</p>
<p>On the interpretability of machine learning-based model for predicting hypertension. R Elshawi, M H Al-Mallah, S Sakr, 10.1186/s12911-019-0874-0BMC medical informatics and decision making. 191462019</p>
<p>Improving performance of deep learning models with axiomatic attribution priors and expected gradients. G Erion, J D Janizek, P Sturmfels, S M Lundberg, S.-I Lee, 10.1038/s42256-021-00343-wNature Machine Intelligence. 32021</p>
<p>Proposal for a Regulation of the European Parliament and of the council Laying Down Harmonised Rules on ARtificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts. C , T , 2021European Commission Directorate-General for Communications Networks</p>
<p>Machine learning-based integration of remotely-sensed drought factors can improve the estimation of agricultural drought in South-Eastern Australia. P Feng, B Wang, D L Liu, Q Yu, 10.1016/j.agsy.2019.03.015Agricultural Systems. 1732019</p>
<p>Using large-scale climate drivers to forecast meteorological drought condition in growing season across the Australian wheatbelt. P Feng, B Wang, J.-J Luo, D L Liu, C Waters, F Ji, 10.1016/j.scitotenv.2020.138162Science of The Total Environment. 7241381622020</p>
<p>All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously. A Fisher, C Rudin, F Dominici, Journal of Machine Learning Research. 201772021</p>
<p>Greedy function approximation: A gradient boosting machine. J H Friedman, Annals of statistics. 2001</p>
<p>Predictive learning via rule ensembles. J H Friedman, B E Popescu, 10.1214/07-AOAS148Annals of Applied Statistics. 2008</p>
<p>Interpretable Deep Learning for Spatial Analysis of Severe Hailstorms. I I Gagne, D J Haupt, S E Nychka, D W Thompson, G , 10.1175/MWR-D-18-0316.1Monthly Weather Review. 1472019</p>
<p>This looks More Like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation. S Gautam, M M Höhne, .-C Hansen, S Jenssen, R Kampffmeyer, M , 10.1016/j.patcog.2022.109172Pattern Recognition. 1361091722023</p>
<p>The future of Earth system prediction: Advances in model-data fusion. A Gettelman, A J Geer, R M Forbes, G R Carmichael, G Feingold, D J Posselt, 10.1126/sciadv.abn3488Science Advances. 834882022</p>
<p>Explainable AI for earth observation: A review including societal and regulatory perspectives. C M Gevaert, 10.1016/j.jag.2022.102869International Journal of Applied Earth Observation and Geoinformation. 1121028692022</p>
<p>Stratiform and Convective Rain Classification Using Machine Learning Models and Micro Rain Radar. Remote Sensing. W Ghada, E Casellas, J Herbinger, A Garcia-Benadí, L Bothmann, N Estrella, 10.3390/rs141845632022144563</p>
<p>Explainable artificial intelligence in disaster risk management: Achievements and prospective futures. S Ghaffarian, F R Taghikhah, H R Maier, 10.1016/j.ijdrr.2023.104123International Journal of Disaster Risk Reduction. 982023. 104123</p>
<p>Training machine learning models on climate model output yields skillful interpretable seasonal precipitation forecasts. P B Gibson, W E Chapman, A Altinok, L Delle Monache, M J Deflorio, D E Waliser, 10.1038/s43247-021-00225-4Communications Earth &amp; Environment. 21592021</p>
<p>Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation. A Goldstein, A Kapelner, J Bleich, E Pitkin, 10.1080/10618600.2014.907095Journal of Computational and Graphical Statistics. 242015</p>
<p>Oceanic Harbingers of Pacific Decadal Oscillation Predictability in CESM2 Detected by Neural Networks. E M Gordon, E A Barnes, J W Hurrell, 10.1029/2021GL095392Geophysical Research Letters. 482021</p>
<p>Evaluating local explanation methods on ground truth. R Guidotti, 10.1016/j.artint.2020.103428Artificial Intelligence. 2911034282021</p>
<p>A Survey of Methods for Explaining Black Box Models. R Guidotti, A Monreale, S Ruggieri, F Turini, F Giannotti, D Pedreschi, 10.1145/3236009ACM computing surveys (CSUR). 201951</p>
<p>D Gunning, M Stefik, J Choi, T Miller, S Stumpf, G.-Z Yang, 10.1126/scirobotics.aay7120XAI-Explainable artificial intelligence. Science robotics, 4, eaay7120. 2019</p>
<p>DARPA's explainable AI (XAI) program: A retrospective. D Gunning, E Vorm, J Y Wang, M Turek, 10.1002/ail2.61Applied AI Letters. 2021</p>
<p>LEMNA: Explaining Deep Learning based Security Applications. W Guo, D Mu, J Xu, P Su, G Wang, X Xing, 10.1145/3243734.3243792ACM SIGSAC Conference on Computer and Communications Security. Toronto Canada2018. 201818Paper presented at the CCS</p>
<p>Efficient Data Representation by Selecting Prototypes with Importance Weights. K S Gurumoorthy, A Dhurandhar, G Cecchi, C Aggarwal, 2019 IEEE International Conference on Data Mining (ICDM). Beijing, China2019</p>
<p>An introduction to variable and feature selection. I Guyon, A Elisseeff, 10.1162/153244303322753616Journal of Machine Learning Research. 32003. Mar</p>
<p>Global patterns of geo-ecological controls on the response of soil respiration to warming. D Haaf, J Six, S Doetterl, 10.1038/s41558-021-01068-9Nature Climate Change. 112021</p>
<p>Identifying major drivers of daily streamflow from large-scale atmospheric circulation with machine learning. J S Hagen, E Leblois, D Lawrence, D Solomatine, A Sorteberg, 10.1016/j.jhydrol.2021.126086Journal of Hydrology. 5961260862021</p>
<p>Anthropogenic fingerprints in daily precipitation revealed by deep learning. Y.-G Ham, J.-H Kim, S.-K Min, D Kim, T Li, A Timmermann, M F Stuecker, 10.1038/s41586-023-06474-xNature. 6222023</p>
<p>Unboxing the Black Box of Attention Mechanisms in Remote Sensing Big Data Using XAI. Remote Sensing. E Hasanpour Zaryabi, L Moradi, B Kalantar, N Ueda, A A Halin, 10.3390/rs142462542022146254</p>
<p>Towards implementing artificial intelligence post-processing in weather and climate: proposed actions from the Oxford 2019 workshop. S E Haupt, W Chapman, S V Adams, C Kirkwood, J S Hosking, N H Robinson, 10.1098/rsta.2020.0091Philosophical Transactions of the Royal Society A. 3792021. 20200091</p>
<p>Physically constrained generative adversarial networks for improving precipitation fields from Earth system models. P Hess, M Drüke, S Petri, F M Strnad, N Boers, 10.1038/s42256-022-00540-1Nature Machine Intelligence. 42022</p>
<p>Explainable AI for oceanic carbon cycle analysis of CMIP6. P Heubel, L Keppler, T Iliyna, 10.5194/egusphere-egu23-38752023</p>
<p>Evaluating machine-generated explanations: a "Scorecard" method for XAI measurement science. R R Hoffman, M Jalaeian, C Tate, G Klein, S T Mueller, 10.3389/fcomp.2023.1114806Frontiers in Compututer Science. 52023. 1114806</p>
<p>A Holzinger, R Goebel, R Fong, T Moon, K.-R Müller, 10.1007/978-3-031-04083-2xxAI -Beyond Explainable AI: International Workshop. Lecture Notes in Computer Science. W Samek, Vienna, Austria; ChamSpringer International Publishing2022. July 18, 2020Conjunction with ICML 2020</p>
<p>Storage Fit Learning with Feature Evolvable Streams. B.-J Hou, Y.-H Yan, P Zhao, &amp; Z.-H Zhou, 10.48550/arXiv.2007.112802021</p>
<p>Soil moisture-evaporation coupling shifts into new gears under increasing CO2. H Hsu, P A Dirmeyer, 10.1038/s41467-023-36794-5Nature Communications. 1411622023</p>
<p>Comparison of physical-based, data-driven and hybrid modeling approaches for evapotranspiration estimation. X Hu, L Shi, G Lin, L Lin, 10.1016/j.jhydrol.2021.126592Journal of Hydrology. 6011265922021</p>
<p>Towards interpreting machine learning models for predicting soil moisture droughts. F Huang, Y Zhang, Y Zhang, V Nourani, Q Li, L Li, W Shangguan, 10.1088/1748-9326/acdbe0Environmental Research Letters. 18740022023a</p>
<p>Interpreting Conv-LSTM for Spatio-Temporal Soil Moisture Prediction in China. F Huang, Y Zhang, Y Zhang, W Shangguan, Q Li, L Li, S Jiang, 10.3390/agriculture13050971Agriculture. 139712023b</p>
<p>A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. X Huang, D Kroening, W Ruan, J Sharp, Y Sun, E Thamo, 10.1016/j.cosrev.2020.100270Computer Science Review. 371002702020</p>
<p>Soil moistureatmosphere feedback dominates land carbon uptake variability. V Humphrey, A Berg, P Ciais, P Gentine, M Jung, M Reichstein, 10.1038/s41586-021-03325-5Nature. 5922021</p>
<p>Machine learning and artificial intelligence to aid climate change research and preparedness. C Huntingford, E S Jeffers, M B Bonsall, H M Christensen, T Lees, H Yang, 10.1088/1748-9326/ab4e55Environmental Research Letters. 141240072019</p>
<p>Augmented score-CAM: High resolution visual interpretations for deep neural networks. R Ibrahim, M O Shafiq, Knowledge-Based Systems. 2521092872022. 2022</p>
<p>Self-Validating Deep Learning for Recovering Terrestrial Water Storage From Gravity and Altimetry Measurements. C Irrgang, J Saynisch-Wagner, R Dill, E Boergens, M Thomas, 10.1029/2020GL089258Geophysical Research Letters. 472020</p>
<p>Prediction and interpretable visualization of retrosynthetic reactions using graph convolutional networks. S Ishida, K Terayama, R Kojima, K Takasu, Y Okuno, Journal of chemical information and modeling. 592019</p>
<p>Cloud Patterns in the Trades Have Four Interpretable Dimensions. M Janssens, J Vilà-Guerau De Arellano, M Scheffer, C Antonissen, A P Siebesma, F Glassmeier, 10.1029/2020GL091001Geophysical Research Letters. 482021</p>
<p>How can I choose an explainer?: An Application-grounded Evaluation of Post-hoc Explanations. S Jesus, C Belém, V Balayan, J Bento, P Saleiro, P Bizarro, J Gama, 10.1145/3442188.3445941FAccT'21: 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021</p>
<p>River flooding mechanisms and their changes in Europe revealed by explainable machine learning. S Jiang, E Bevacqua, J Zscheischler, 10.5194/hess-26-6339-2022Hydrology and Earth System Sciences. 262022a</p>
<p>Uncovering Flooding Mechanisms Across the Contiguous United States Through Interpretive Deep Learning on Representative Catchments. S Jiang, Y Zheng, C Wang, V Babovic, 10.1029/2021WR030185Water Resources Research. 582022b</p>
<p>Comparison and interpretation of data-driven models for simulating site-specific human-impacted groundwater dynamics in the North China Plain. H Jing, X He, Y Tian, M Lancia, G Cao, A Crivellari, 10.1016/j.jhydrol.2022.128751Journal of Hydrology. 6161287512023</p>
<p>The global landscape of AI ethics guidelines. A Jobin, M Ienca, E Vayena, 10.1038/s42256-019-0088-2Nature Machine Intelligence. 12019</p>
<p>Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing. I Kakogeorgiou, K Karantzalos, 10.1016/j.jag.2021.102520International Journal of Applied Earth Observation and Geoinformation. 1031025202021</p>
<p>Interpretability of artificial neural network models in artificial intelligence versus neuroscience. K Kar, S Kornblith, E Fedorenko, 10.1038/s42256-022-00592-3Nature Machine Intelligence. 42022</p>
<p>Accurate autolabeling of chest X-ray images based on quantitative similarity to an explainable AI model. D Kim, J Chung, J Choi, M D Succi, J Conklin, M G F Longo, 10.1038/s41467-022-29437-8Nature Communications. 1318672022</p>
<p>Explainable AI-Based Interface System for Weather Forecasting Model. S Kim, Choi, Junho, Y Choi, S Lee, A Stitsyuk, M Park, S Jeong, Y.-H Baek, Jaesik Choi, 10.1007/978-3-031-48057-7_7HCI International (2023 -Late Breaking Papers: 25th International Conference on Human-Computer Interaction, HCII. Copenhagen, Denmark; Berlin, HeidelbergSpringer-Verlag2023. 2023. July 23-28. 2023Proceedings, Part VI</p>
<p>The (Un)reliability of Saliency Methods. P.-J Kindermans, S Hooker, J Adebayo, M Alber, K T Schütt, S Dähne, 10.1007/978-3-030-28954-6_14Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. W Samek, G Montavon, A Vedaldi, L K Hansen, K.-R Müller, ChamSpringer International Publishing2019</p>
<p>Modelling of the shallow water table at high spatial resolution using random forests. J Koch, H Berger, H J Henriksen, T O Sonnenborg, 10.5194/hess-23-4603-2019Hydrology and Earth System Sciences. 232019</p>
<p>S Kolek, D A Nguyen, R Levie, J Bruna, G Kutyniok, 10.48550/arXiv.2110.03485Cartoon Explanations of Image Classifiers. 2022</p>
<p>Wildfire Danger Prediction and Understanding With Deep Learning. S Kondylatos, I Prapas, M Ronco, I Papoutsis, G Camps-Valls, M Piles, 10.1029/2022GL099368Geophysical Research Letters. 492022</p>
<p>Regional sensitivity patterns of Arctic Ocean acidification revealed with machine learning. J P Krasting, M De Palma, M Sonnewald, J P Dunne, J G John, 10.1038/s43247-022-00419-4Communications Earth &amp; Environment. 3912022</p>
<p>Deep Learning and Explainable Artificial Intelligence Techniques Applied for Detecting Money Laundering-A Critical Review. D V Kute, B Pradhan, N Shukla, A Alamri, 10.1109/ACCESS.2021.3086230IEEE Access. 92021</p>
<p>Diffusion models already have a semantic latent space. M Kwon, J Jeong, Y Uh, arXiv:2210.109602022arXiv preprint</p>
<p>Detecting Climate Signals Using Explainable AI With Single-Forcing Large Ensembles. Z M Labe, E A Barnes, 10.1029/2021MS002464Journal of Advances in Modeling Earth Systems. 132021</p>
<p>Learning skillful medium-range global weather forecasting. R Lam, A Sanchez-Gonzalez, M Willson, P Wirnsberger, M Fortunato, F Alet, 10.1126/science.adi2336Science. 38266772023</p>
<p>Explainable artificial intelligence model to predict acute critical illness from electronic health records. S M Lauritsen, M Kristensen, M V Olsen, M S Larsen, K M Lauritsen, M J Jørgensen, 10.1038/s41467-020-17431-xNature Communications. 1138522020</p>
<p>Improvement in deep networks for optimization using explainable artificial intelligence. J H Lee, I Shin, S Gu Jeong, S Lee, M Z Zaheer, B Seo, ICTC 2019International Conference on Information and Communication Technology Convergence. Jeju Island; Korea (South2019</p>
<p>Sequential Data Generation with Groupwise Diffusion Process. S Lee, G Lee, H Kim, J Kim, Y Uh, 10.48550/arXiv.2310.014002023</p>
<p>Hydrological concept formation inside long short-term memory (LSTM) networks. T Lees, S Reece, F Kratzert, D Klotz, M Gauch, J De Bruijn, 10.5194/hess-26-3079-2022Hydrology and Earth System Sciences. 262022</p>
<p>Mapping of machine learning approaches for description, prediction, and causal inference in the social and health sciences. A K Leist, M Klee, J H Kim, D H Rehkopf, S P A Bordas, G Muniz-Terrera, S Wade, 10.1126/sciadv.abk1942Science Advances. 819422022</p>
<p>Dynamics of particle network in composite battery cathodes. J Li, N Sharma, Z Jiang, Y Yang, F Monaco, Z Xu, 10.1126/science.abm8962Science. 3762022a</p>
<p>A 1 km daily soil moisture dataset over China using in situ measurement and machine learning. Q Li, G Shi, W Shangguan, V Nourani, J Li, L Li, 10.5194/essd-14-5267-2022Earth System Science Data. 142022b</p>
<p>Widespread increasing vegetation sensitivity to soil moisture. W Li, M Migliavacca, M Forkel, J M C Denissen, M Reichstein, H Yang, 10.1038/s41467-022-31667-9Nature Communications. 1339592022c</p>
<p>GLA: Global-Local Attention for Image Description. L Li, S Tang, Y Zhang, L Deng, Q Tian, 10.1109/TMM.2017.2751140IEEE Transactions on Multimedia. 202018</p>
<p>Big Data in Earth system science and progress towards a digital twin. X Li, M Feng, Y Ran, Y Su, F Liu, C Huang, 10.1038/s43017-023-00409-wNature Reviews Earth &amp; Environment. 42023a</p>
<p>Contrasting Drought Propagation into the Terrestrial Water Cycle Between Dry and Wet Regions. W Li, M Reichstein, O , S May, C Destouni, G Migliavacca, M , 10.1029/2022EF003441Earth's Future. 112023b</p>
<p>Partitioning global land evapotranspiration using CMIP5 models constrained by observations. X Lian, S Piao, C Huntingford, Y Li, Z Zeng, X Wang, 10.1038/s41558-018-0207-9Nature Climate Change. 82018</p>
<p>Recent global decline in rainfall interception loss due to altered rainfall regimes. X Lian, W Zhao, P Gentine, 10.1038/s41467-022-35414-yNature Communications. 1132022</p>
<p>Time-series forecasting with deep learning: a survey. B Lim, S Zohren, 10.1098/rsta.2020.0209Philosophical Transactions of Royal Society A. 3792021. 20200209</p>
<p>What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors. Y.-S Lin, W.-C Lee, Z B Celik, 10.1145/3447548.346721327th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Singapore2021</p>
<p>Simulation of regional groundwater levels in arid regions using interpretable machine learning models. Q Liu, D Gui, L Zhang, J Niu, H Dai, G Wei, B X Hu, 10.1016/j.scitotenv.2022.154902Science of The Total Environment. 8311549022022a</p>
<p>SLAFusion: Attention fusion based on SAX and LSTM for dangerous driving behavior detection. J Liu, W Huang, H Li, S Ji, Y Du, T Li, 10.1016/j.ins.2023.119063Information Sciences. 6401190632023a</p>
<p>Explainable deep learning for insights in El Niño and river flows. Y Liu, K Duffy, J G Dy, A R Ganguly, 10.1038/s41467-023-35968-5Nature Communications. 143392023b</p>
<p>Unrolled projected gradient descent for multi-spectral image fusion. S Lohit, D Liu, H Mansour, Boufounos , P T , 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. Brighton, UK.2019Paper presented at the ICASSP 2019</p>
<p>A Comparison between Explainable Machine Learning Methods for Classification and Regression Problems in the Actuarial Context. Mathematics, 11. C Lozano-Murcia, F P Romero, J Serrano-Guerrero, J A Olivas, 10.3390/math1114308820233088</p>
<p>A Unified Approach to Interpreting Model Predictions 10. S M Lundberg, &amp; S.-I Lee, 10.48550/arXiv.1705.078742017</p>
<p>Anthropogenic influence on extreme precipitation over global land areas seen in multiple observational datasets. G D Madakumbura, C W Thackeray, J Norris, N Goldenson, A Hall, 10.1038/s41467-021-24262-xNature Communications. 1239442021</p>
<p>MIIDAPS-AI: An Explainable Machine-Learning Algorithm for Infrared and Microwave Remote Sensing and Data Assimilation Preprocessing -Application to LEO and GEO Sensors. E S Maddy, S A Boukabara, 10.1109/JSTARS.2021.3104389IEEE J. Sel. Top. Appl. Earth Observations Remote Sensing. 142021</p>
<p>Interpretability-Driven Sample Selection Using Self Supervised Learning for Disease Classification and Segmentation. D Mahapatra, A Poellinger, L Shao, M Reyes, 10.1109/TMI.2021.3061724IEEE transactions on medical imaging. 402021</p>
<p>Neural network attribution methods for problems in geoscience: A novel synthetic benchmark dataset. A Mamalakis, I Ebert-Uphoff, E A Barnes, 10.1017/eds.2022.7Environmental Data Science. 1e82022a</p>
<p>Investigating the Fidelity of Explainable Artificial Intelligence Methods for Applications of Convolutional Neural Networks in Geoscience. A Mamalakis, E A Barnes, I Ebert-Uphoff, 10.1175/AIES-D-22-0012.1Artificial Intelligence for the Earth Systems. 12022b</p>
<p>Carefully Choose the Baseline: Lessons Learned from Applying XAI Attribution Methods for Regression Tasks in Geoscience. A Mamalakis, E A Barnes, I Ebert-Uphoff, 10.1175/AIES-D-22-0058.1Artificial Intelligence for the Earth Systems. 22023</p>
<p>OD-XAI: Explainable AI-Based Semantic Object Detection for Autonomous Vehicles. H Mankodiya, D Jadav, R Gupta, S Tanwar, W.-C Hong, R Sharma, 10.3390/app12115310Applied Sciences. 1253102022</p>
<p>Human-induced greening of the northern extratropical land surface. J Mao, A Ribes, B Yan, X Shi, P E Thornton, R Séférian, 10.1038/nclimate3056Nature Climate Change. 62016</p>
<p>Beyond Prediction Similarity: ShapGAP for Evaluating Faithful Surrogate Models in XAI. E Mariotti, A Sivaprasad, J M A Moral, 10.1007/978-3-031-44064-9_10Explainable Artificial Intelligence. xAI. L Longo, Cham, SwitzerlandSpringer2023. 2023. 1901</p>
<p>Using Simple, Explainable Neural Networks to Predict the Madden-Julian Oscillation. Z K Martin, E A Barnes, E Maloney, 10.1029/2021MS002774Journal of Advances in Modeling Earth Systems. 142022</p>
<p>Evaluation of interpretability for deep learning algorithms in EEG emotion recognition: A case study in autism. Mayor Torres, J M Medina-Devilliers, S Clarkson, T Lerner, M D Riccardi, G , 10.1016/j.artmed.2023.102545Artificial Intelligence in Medicine. 1431025452023</p>
<p>Using attribution to decode binding mechanism in neural network models for chemistry. K Mccloskey, A Taly, F Monti, M P Brenner, L J Colwell, 2019Proceedings of the National Academy of Sciences116</p>
<p>A Mcgovern, A Bostrom, P Davis, J L Demuth, I Ebert-Uphoff, R He, 10.1175/BAMS-D-21-0020NSF AI Institute for Research on Trustworthy AI in Weather, Climate, and Coastal Oceanography (AI2ES). 2022103</p>
<p>Making the Black Box More Transparent: Understanding the Physical Implications of Machine Learning. A Mcgovern, R Lagerquist, D John Gagne, G E Jergensen, K L Elmore, C R Homeyer, T Smith, 10.1175/BAMS-D-18-0195.1Bulletin of the American Meteorological Society. 1002019</p>
<p>Unlocking GOES: A Statistical Framework for Quantifying the Evolution of Convective Structure in Tropical Cyclones. T Mcneely, A B Lee, K M Wood, D Hammerling, 10.1175/JAMC-D-19-0286.1Journal of Applied Meteorology and Climatology. 592020</p>
<p>Towards Robust Interpretability with Self-Explaining Neural Networks. D A Melis, T Jaakkola, 10.48550/arXiv.1806.075382018</p>
<p>Toward explainable artificial intelligence: A survey and overview on their intrinsic properties. J.-X Mi, X Jiang, L Luo, Y Gao, 10.1016/j.neucom.2023.126919Neurocomputing. 5631269192024</p>
<p>Spatially embedded neuromorphic networks. F Milisav, B Misic, 10.1038/s42256-023-00771-wNature Machine Intelligence. 52023</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. T Miller, 10.1016/j.artint.2018.07.007Artificial Intelligence. 2672019</p>
<p>Explainable artificial intelligence: a comprehensive review. D Minh, H X Wang, Y F Li, T N Nguyen, 10.1007/s10462-021-10088-yArtificial Intelligence Review. 552022</p>
<p>Interpretable Machine Learning -A Brief History, State-of-the-Art and Challenges. C Molnar, G Casalicchio, B Bischl, 10.1007/978-3-030-65965-3_28ECML PKDD 2020 Workshops. ECML PKDD 2020. I Koprinska, Cham, Switzerland20201323</p>
<p>Attention to Fires: Multi-Channel Deep Learning Models for Wildfire Severity Prediction. S Monaco, S Greco, A Farasin, L Colomba, D Apiletti, P Garza, 10.3390/app112211060Applied Sciences. 11110602021</p>
<p>Comparison of neural networks and regression based methods for temperature retrievals. H E Motteler, L L Strow, L Mcmillin, J A Gualtieri, 10.1364/AO.34.005390Applied Optics. 34241995</p>
<p>Deep-learning seismology. S M Mousavi, G C Beroza, 10.1126/science.abm4470Science. 37744702022</p>
<p>R I Mukhamediev, Y Popova, Y Kuchin, E Zaitseva, A Kalimoldayev, A Symagulov, 10.3390/math10152552Review of Artificial Intelligence and Machine Learning Technologies: Classification, Restrictions, Opportunities and Challenges. Mathematics. 2022102552</p>
<p>Interpretable machine learning: definitions, methods, and applications. W J Murdoch, C Singh, K Kumbier, R Abbasi-Asl, B Yu, 10.1073/pnas.19006541162019116Proceedings of the National Academy of Sciences of the United States of America</p>
<p>V Nagisetty, L Graves, J Scott, V Ganesh, arXiv:2002.10438xAI-GAN: Enhancing Generative Adversarial Networks via Explainable AI Systems. 2020arXiv preprint</p>
<p>How the different explanation classes impact trust calibration: The case of clinical decision support systems. M Naiseh, D Al-Thani, N Jiang, R Ali, 10.1016/j.ijhcs.2022.102941International Journal of Human-Computer Studies. 1691029412023</p>
<p>M Nauta, J Trienes, S Pathak, E Nguyen, M Peters, Y Schmitt, 10.1145/3583558From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI. ACM Computing Surveys. 202355</p>
<p>Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda. R Nishant, M Kennedy, J Corbett, 10.1016/j.ijinfomgt.2020.102104International Journal of Information Management. 531021042020</p>
<p>Artificial intelligence and climate change: ethical issues. A Nordgren, 10.1108/JICES-11-2021-0106Journal of Information, Communication and Ethics in Society. 212023</p>
<p>Obtaining genetics insights from deep learning via explainable artificial intelligence. G Novakovsky, N Dexter, M W Libbrecht, W W Wasserman, S Mostafavi, 10.1038/s41576-022-00532-2Nature Reviews Genetics. 2422022</p>
<p>Global soil moisture data derived through machine learning trained with insitu measurements. O , S Orth, R , 10.1038/s41597-021-00964-1Scientific Data. 81702021</p>
<p>Evaluation Metrics Research for Explainable Artificial Intelligence Global Methods Using Synthetic Data. A Oblizanov, N Shevskaya, A Kazak, M Rudenko, A Dorofeeva, 10.3390/asi6010026Applied System Innovation. 61262023</p>
<p>A Orynbaikyzy, U Gessner, B Mack, C Conrad, 10.3390/rs12172779Crop Type Classification Using Fusion of Sentinel-1 and Sentinel-2 Data: Assessing the Impact of Feature Selection. 2020122779</p>
<p>Interpretable deep learning LSTM model for intelligent economic decision-making. Knowledge-Based Systems. S Park, J.-S Yang, 10.1016/j.knosys.2022.1089072022248108907</p>
<p>Drivers and trends of global soil microbial carbon over two decades. G Patoine, N Eisenhauer, S Cesarz, H R P Phillips, X Xu, L Zhang, C A Guerra, 10.1038/s41467-022-31833-zNature Communications. 1341952022</p>
<p>U-CAM: Visual explanation using uncertainty based class activation maps. B Patro, M Lunayach, S Patel, V Namboodiri, 2019 IEEE/CVF International Conference on Computer Vision (ICCV. Seoul, Korea2019. 2019</p>
<p>Can predictive models be used for causal inference. M Pichler, F Hartig, 10.48550/arXiv.2306.105512023</p>
<p>Process-explicit models reveal the structure and dynamics of biodiversity patterns. J A Pilowsky, R K Colwell, C Rahbek, D A Fordham, 10.1126/sciadv.abj2271Sci. Adv. 82022. eabj2271</p>
<p>Focused Attention in Transformers for interpretable classification of retinal images. C Playout, R Duval, M C Boucher, F Cheriet, 10.1016/j.media.2022.102608Medical Image Analysis. 821026082022</p>
<p>A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions. Environmental Modelling &amp; Software. F A Prodhan, J Zhang, S S Hasan, Pangali, T P Sharma, H P Mohana, 10.1016/j.envsoft.2022.1053272022149105327</p>
<p>Carbon loss from forest degradation exceeds that from deforestation in the Brazilian Amazon. Y Qin, X Xiao, J.-P Wigneron, P Ciais, M Brandt, L Fan, X Li, S Crowell, 10.1038/s41558-021-01026-5Nature Climate Change. 1152021</p>
<p>SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability. M Raghu, J Gilmer, J Yosinski, J Sohl-Dickstein, Advances in Neural Information Processing Systems. Long Beach, USA201730</p>
<p>A Survey of Deep Learning for Scientific Discovery. M Raghu, E Schmidt, 10.48550/arXiv.2003.117552020</p>
<p>Machine behaviour. I Rahwan, M Cebrian, N Obradovich, J Bongard, J.-F Bonnefon, C Breazeal, 10.1038/s41586-019-1138-yNature. 5682019</p>
<p>Extending SC-PDSI-PM with neural network regression using GLDAS data and Permutation Feature Importance. Environmental Modelling &amp; Software. S G Ramirez, R C Hales, G P Williams, N L Jones, 10.1016/j.envsoft.2022.1054752022157105475</p>
<p>Deep learning and process understanding for data-driven Earth system science. M Reichstein, G Camps-Valls, B Stevens, M Jung, J Denzler, N Carvalhais, Prabhat, 10.1038/s41586-019-0912-1Nature. 5662019</p>
<p>Impacts, risks, and adaptation in the United States: Fourth national climate assessment. D R Reidmiller, C W Avery, D R Easterling, K E Kunkel, K L Lewis, T K Maycock, B C Stewart, 2017II</p>
<p>Identifying Relations Between Deep Convection and the Large-Scale Atmosphere Using Explainable Artificial Intelligence. M H Retsch, C Jakob, M S Singh, 10.1029/2021JD035388Journal of Geophysical Research: Atmospheres. 1272022</p>
<p>Overfitting in making comparisons between variable selection methods. J Reunanen, Journal of Machine Learning Research. 32003</p>
<p>Why Should I Trust You?": Explaining the Predictions of Any Classifier. M T Ribeiro, S Singh, C Guestrin, 22nd ACM SIGKDD international conference on knowledge discovery and data mining. New York, USA2016</p>
<p>Explainable Machine Learning for Scientific Insights and Discoveries. R Roscher, B Bohn, M F Duarte, J Garcke, 10.1109/ACCESS.2020.2976199IEEE Access. 82020</p>
<p>Right for the right reasons: Training differentiable models by constraining their explanations. A S Ross, M C Hughes, F Doshi-Velez, Sixth International Joint Conference on Artificial Intelligence, IJCAI. Melbourne, Australia2017Paper presented at the Twenty</p>
<p>Benchmarking of Machine Learning Ocean Subgrid Parameterizations in an Idealized Model. A Ross, Z Li, P Perezhogin, C Fernandez-Granda, L Zanna, 10.1029/2022MS003258Journal of Advances in Modeling Earth Systems. 152023</p>
<p>Using machine learning and a datadriven approach to identify the small fatigue crack driving force in polycrystalline materials. A Rovinelli, M D Sangid, H Proudhon, W Ludwig, npj Computational Materials. 41352018</p>
<p>. A Saabas, 2015</p>
<p>Robustness, Stability, and Fidelity of Explanations for a Deep Skin Cancer Classification Model. M Saarela, L Geogieva, 10.3390/app12199545Applied Sciences. 1295452022</p>
<p>Constructing the machine learning techniques based spatial drought vulnerability index in Karnataka state of India. S Saha, P Gogoi, A Gayen, G C Paul, 10.1016/j.jclepro.2021.128073Journal of Cleaner Production. 3141280732021</p>
<p>Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions. I H Sarker, 10.1007/s42979-021-00815-1SN Computer Science. 264202021</p>
<p>Earth system analysis and the second Copernican revolution. H J Schellnhuber, 10.1038/35011515Nature. 4021999</p>
<p>Relevance-based feature masking: Improving neural network based whale classification through explainable artificial intelligence. D Schiller, T Huber, F Lingenfelser, M Dietz, A Seiderer, E André, 20th Annual Conference of the International Speech Communication Association. Graz, Austria</p>
<p>Challenges in Applying Machine Learning Models for Hydrological Inference: A Case Study for Flooding Events Across Germany. L Schmidt, F Heße, S Attinger, R Kumar, 10.1029/2019WR025924Water resources research. 5652020</p>
<p>ESA-ECMWF Report on recent progress and research directions in machine learning for Earth System observation and prediction. npj Climate and Atmospheric Science. R Schneider, M Bonavita, A Geer, R Arcucci, P Dueben, C Vitolo, 10.1038/s41612-022-00269-z2022551</p>
<p>Gaia-AgStream: An Explainable AI Platform for Mining Complex Data Streams in Agriculture. J Schoenke, N Aschenbruck, R Interdonato, R Kanawati, A.-C Meisener, F Thierart, 10.1007/978-3-030-88259-4_6Boumerdassi, S., Ghogho, M., Renault, É.2021Springer International PublishingCham, SwitzerlandSmart and Sustainable Agriculture, Communications in Computer and Information Science</p>
<p>Can deep learning beat numerical weather prediction?. M G Schultz, C Betancourt, B Gong, F Kleinert, M Langguth, L H Leufen, 10.1098/rsta.2020.0097Philosophical Transactions Royal Society A. 3792021. 20200097</p>
<p>Identifying Informative Nodes in Attributed Spatial Sensor Networks Using Attention for Symbolic Abstraction in a GNN-based Modeling Approach. L Schwenke, S Bloemheuvel, M Atzmueller, 10.32473/flairs.36.133109The International FLAIRS Conference Proceedings. 202336</p>
<p>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, 10.1007/s11263-019-01228-7International Journal of Computer Vision. 1282020</p>
<p>A machine learning interpretation of the contribution of foliar fungicides to soybean yield in the north-central United States. D A Shah, T R Butts, S Mourtzinis, J I Rattalino Edreira, P Grassini, S P Conley, P D Esker, 10.1038/s41598-021-98230-2Scientific reports. 111187692021</p>
<p>Implicit learning of convective organization explains precipitation stochasticity. S Shamekh, K D Lamb, Y Huang, P Gentine, 10.1073/pnas.22161581202023. e2216158120120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Mapping the global depth to bedrock for land surface modeling. W Shangguan, T Hengl, J Mendes De Jesus, H Yuan, Y Dai, 10.1002/2016MS000686Journal of Advances in Modeling Earth Systems. 912017</p>
<p>A 1 km Global Carbon Flux Dataset Using In Situ Measurements and Deep Learning. W Shangguan, Z Xiong, V Nourani, Q Li, X Lu, L Li, 10.3390/f14050913Forests. 149132023</p>
<p>A Transdisciplinary Review of Deep Learning Research and Its Relevance for Water Resources Scientists. C Shen, 10.1029/2018WR022643Water Resources Research. 542018</p>
<p>Differentiable modelling to unify machine learning and physical models for geosciences. C Shen, A P Appling, P Gentine, T Bandai, H Gupta, A Tartakovsky, 10.1038/s43017-023-00450-9Nature Reviews Earth &amp; Environ. 482023</p>
<p>Learning Important Features Through Propagating Activation Differences. A Shrikumar, P Greenside, A Kundaje, 10.48550/arXiv.1704.026852019</p>
<p>K Simonyan, A Vedaldi, A Zisserman, 10.48550/arXiv.1312.6034Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. 2014</p>
<p>Explainable Edge AI: A Futuristic Computing Perspective, Studies in Computational Intelligence. S Sinha, P Vashisht, 10.1007/978-3-031-18292-1_8Explainable Data Fusion on Edge: Challenges and Opportunities. A E Hassanien, D Gupta, A K Singh, A Garg, Cham, SwitzerlandSpringer International Publishing2023</p>
<p>SmoothGrad: removing noise by adding noise. D Smilkov, N Thorat, B Kim, F Viégas, M Wattenberg, 10.48550/arXiv.1706.038252017</p>
<p>All the attention you need: Global-local, spatial-channel attention for image retrieval. C H Song, H J Han, Y Avrithis, 10.1109/WACV51458.2022.000512022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). Waikoloa, HI, USAIEEE2022</p>
<p>Revealing the Impact of Global Heating on North Atlantic Circulation Using Transparent Machine Learning. M Sonnewald, R Lguensat, 10.1029/2021MS002496Journal of Advances in Modeling Earth Systems. 132021</p>
<p>A Southern Ocean supergyre as a unifying dynamical framework identified by physics-informed machine learning. M Sonnewald, K A Reeve, R Lguensat, 10.1038/s43247-023-00793-7Communications Earth &amp; Environment. 411532023</p>
<p>Unsupervised Learning Reveals Geography of Global Ocean Dynamical Regions. M Sonnewald, C Wunsch, P Heimbach, 10.1029/2018EA000519Earth and Space Science. 62019</p>
<p>Striving for Simplicity: The All Convolutional Net. J T Springenberg, A Dosovitskiy, T Brox, M Riedmiller, 10.48550/arXiv.1412.68062015</p>
<p>Explainable Machine Learning Reveals Capabilities, Redundancy, and Limitations of a Geospatial Air Quality Benchmark Dataset. Machine learning and knowledge extraction. S Stadtler, C Betancourt, R Roscher, 10.3390/make401000820224</p>
<p>The emergence and evolution of Earth System Science. W Steffen, K Richardson, J Rockström, H J Schellnhuber, O P Dube, S Dutreuil, 10.1038/s43017-019-0005-6Nature Reviews Earth &amp; Environment. 112020</p>
<p>How Do Climate and Catchment Attributes Influence Flood Generating Processes? A Large-Sample Study for 671 Catchments Across the Contiguous USA. L Stein, M P Clark, W J M Knoben, F Pianosi, R A Woods, 10.1029/2020WR028300Water Resources Research. 5742021</p>
<p>Jungle-net: using explainable machine learning to gain new insights into the appearance of wilderness in satellite imagery. T Stomberg, I Weber, M Schmitt, R Roscher, 10.5194/isprs-annals-V-3-2021-317-2021ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. 32021</p>
<p>LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks. H Strobelt, S Gehrmann, H Pfister, A M Rush, 10.1109/TVCG.2017.2744158IEEE transactions on visualization and computer graphics. 2412017</p>
<p>Multi-stage ensemble-learning-based model fusion for surface ozone simulations: A focus on CMIP6 models. Z Sun, A T Archibald, 10.1016/j.ese.2021.100124Environmental Science and Ecotechnology. 81001242021</p>
<p>Axiomatic Attribution for Deep Networks. M Sundararajan, A Taly, Q Yan, 10.48550/arXiv.1703.013652017</p>
<p>Characterizing Drought Behavior in the Colorado River Basin Using Unsupervised Machine Learning. C J Talsma, K E Bennett, V V Vesselinov, 10.1029/2021EA002086Earth and Space Science. 92022</p>
<p>Physics-informed recurrent neural network for time dynamics in optical resonances. Y Tang, J Fan, X Li, J Ma, M Qi, C Yu, W Gao, 10.1038/s43588-022-00215-2Nature Computational Science. 232022</p>
<p>Explainable AI for Earth Observation: Current Methods, Open Challenges, and Opportunities. G Taskin, E Aptoula, A Ertürk, 10.48550/arXiv.2311.044912023</p>
<p>A Tavanaei, arXiv:2007.06712Embedded encoder-decoder in convolutional networks towards explainable AI. 2021arXiv preprint</p>
<p>Causal deep learning models for studying the Earth system. T Tesch, S Kollet, J Garcke, 10.5194/gmd-16-2149-2023Geoscientific Model Development. 202316</p>
<p>ConfusionVis: Comparative evaluation and selection of multi-class classifiers based on confusion matrices. Knowledge-Based Systems. A Theissler, M Thomas, M Burch, F Gerschner, 10.1016/j.knosys.2022.1086512022247108651</p>
<p>Physically Interpretable Neural Networks for the Geosciences: Applications to Earth System Variability. B A Toms, E A Ebert-Uphoff, I , 10.1029/2019MS002002Journal of Advances in Modeling Earth Systems. 122020</p>
<p>Assessing Decadal Predictability in an Earth-System Model Using Explainable Neural Networks. B A Toms, E A Barnes, J W Hurrell, 10.1029/2021GL093842Geophysical Research Letters. 482021</p>
<p>Evaluation of Post-hoc XAI Approaches Through Synthetic Tabular Data. J Tritscher, M Ring, D Schlr, L Hettinger, A Hotho, 10.1007/978-3-030-59491-6_40Foundations of Intelligent Systems. Lecture Notes in Computer Science. D Helic, G Leitner, M Stettinger, A Felfernig, Z W Raś, Cham, SwitzerlandSpringer2020. 202012117</p>
<p>Toward a Collective Agenda on AI for Earth Science Data Analysis. D Tuia, R Roscher, J D Wegner, N Jacobs, X Zhu, G Camps-Valls, 10.1109/MGRS.2020.3043504IEEE Geoscience Remote Sensing Magazine. 92021</p>
<p>A decision tree for assessing the risks and benefits of publishing Biodiversity Data. A I Tulloch, N Auerbach, S Avery-Gomm, E Bayraktarov, N Butt, C R Dickman, G Ehmke, D O Fisher, 10.1038/s41559-018-0608-1Nature Ecology &amp; Evolution. 282018</p>
<p>Empirical evidence of contrasting soil moisture-precipitation feedbacks across the United States. S Tuttle, G Salvucci, 10.1126/science.aaa7185Science. 3522016</p>
<p>Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities. A Tzachor, M Devare, B King, S Avin, S Héigeartaigh, 10.1038/s42256-022-00440-4Nature Machine Intelligence. 42022</p>
<p>Earth Observation Data-Driven Cropland Soil Monitoring: A Review. Remote Sensing. N Tziolas, N Tsakiridis, S Chabrillat, J A M Demattê, E Ben-Dor, A Gholizadeh, 10.3390/rs132144392021134439</p>
<p>Recommendation on the Ethics of Artificial Intelligence. UNESCO. 2021</p>
<p>Classifying precipitation from GEO satellite observations: Diagnostic model. S A Upadhyaya, P Kirstetter, R J Kuligowski, M Searls, 10.1002/qj.4130Quarterly Journal of the Royal Meteorological Society. 1477392021</p>
<p>GenNet framework: interpretable deep learning for predicting phenotypes from genetic data. A Van Hilten, S A Kushner, M Kayser, M A Ikram, H H H Adams, C C W Klaver, 10.1038/s42003-021-02622-zCommunications biology. 4110942021</p>
<p>Using Explainable Machine Learning Forecasts to Discover Subseasonal Drivers of High Summer Temperatures in Western and Central Europe. C Van Straaten, K Whan, D Coumou, B Van Den Hurk, M Schmeits, 10.1175/MWR-D-21-0201.1Monthly Weather Review. 1502022</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, 10.48550/arXiv.1706.037622017</p>
<p>Knowledge extraction from trained ANN drought classification model. V K Vidyarthi, A Jain, 10.1016/j.jhydrol.2020.124804Journal of Hydrology. 5851248042020</p>
<p>The role of the Pacific Decadal Oscillation and ocean-atmosphere interactions in driving US temperature predictability. npj Climate and Atmospheric Science. S Vijverberg, D Coumou, 10.1038/s41612-022-00237-72022518</p>
<p>Notions of explainability and evaluation approaches for explainable artificial intelligence. G Vilone, L Longo, 10.1016/j.inffus.2021.05.009Information Fusion. 762021</p>
<p>Beyond prediction: methods for interpreting complex models of soil variation. A M J Wadoux, -C Molnar, C , 10.1016/j.geoderma.2022.115953Geoderma. 4221159532022</p>
<p>Quantification of human contribution to soil moisture-based terrestrial aridity. Y Wang, J Mao, F M Hoffman, C J W Bonfils, H Douville, M Jin, 10.1038/s41467-022-34071-5Nature Communications. 1368482022a</p>
<p>Analysis of runoff generation driving factors based on hydrological model and interpretable machine learning method. S Wang, H Peng, Q Hu, M Jiang, 10.1016/j.ejrh.2022.101139Journal of Hydrology: Regional Studies. 421011392022b</p>
<p>Regional and seasonal partitioning of water and temperature controls on global land carbon uptake variability. K Wang, A Bastos, P Ciais, X Wang, C Rödenbeck, P Gentine, 10.1038/s41467-022-31175-wNature Communications. 1334692022c</p>
<p>A comprehensive study of deep learning for soil moisture prediction. Y Wang, L Shi, Y Hu, X Hu, W Song, L Wang, 10.5194/hess-28-917-2024Hydrology and Earth System Sciences Discussions. 2023. 2023</p>
<p>Multiphysics-Informed Neural Networks for Coupled Soil Hydrothermal Modeling. Y Wang, L Shi, X Hu, W Song, L Wang, 10.1029/2022WR031960Water Resources Research. 592023b. e2022WR031960</p>
<p>Permafrost thaw drives surface water decline across lake-rich regions of the Arctic. E E Webb, A K Liljedahl, J A Cordeiro, M M Loranty, C Witharana, J W Lichstein, 10.1038/s41558-022-01455-wNature Climate Change. 1292022</p>
<p>Applications of Explainable Artificial Intelligence in Financea systematic review of Finance, Information Systems, and Computer Science literature. P Weber, K V Carl, O Hinz, 10.1007/s11301-023-00320-0Management Review Quarterly. 2023</p>
<p>Full-Capacity Unitary Recurrent Neural Networks. S Wisdom, T Powers, J Hershey, J L Roux, L Atlas, 10.48550/arXiv.1611.000352016</p>
<p>Multispectral and hyperspectral image fusion by ms/hs fusion net. Q Xie, M Zhou, Q Zhao, D Meng, W Zuo, Z Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019. 2019</p>
<p>Machine learning for hydrologic sciences: An introductory overview. T Xu, F Liang, 10.1002/wat2.15332021Water8e1533</p>
<p>A Spatial-Temporal Interpretable Deep Learning Model for improving interpretability and predictive accuracy of satellite-based PM2.5. Environmental Pollution. X Yan, Z Zang, Y Jiang, W Shi, Y Guo, D Li, 10.1016/j.envpol.2021.1164592021a273116459</p>
<p>J Yan, T Xu, Y Yu, H Xu, 10.3390/w13091272Rainfall Forecast Model Based on the TabNet Model. Water, 13, 1272. 2021b</p>
<p>Fast TreeSHAP: Accelerating SHAP Value Computation for Trees. J Yang, 10.48550/arXiv.2109.098472022</p>
<p>Classifying floods by quantifying driver contributions in the Eastern Monsoon Region of China. W Yang, H Yang, D Yang, 10.1016/j.jhydrol.2020.124767Journal of Hydrology. 5851247672020</p>
<p>The case for data science in experimental chemistry: Examples and recommendations. J Yano, K J Gaffney, J Gregoire, L Hung, A Ourmazd, J Schrier, 10.1038/s41570-022-00382-wNature Reviews Chemistry. 652022</p>
<p>A Simple Framework for XAI Comparisons with a Case Study. G F A Yeo, I Hudson, D Akman, J Chan, 10.1109/ICAIBD55127.2022.98205512022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD). Chengdu, ChinaIEEE2022</p>
<p>Pruning by explaining: A novel criterion for deep neural network pruning. S.-K Yeom, P Seegerer, S Lapuschkin, A Binder, S Wiedemann, K.-R Müller, W Samek, 10.1016/j.patcog.2021.107899Pattern Recognition. 1151078992021</p>
<p>GNNExplainer: generating explanations for graph neural networks. Z Ying, D Bourgeois, J You, M Zitnik, J Leskovec, Advances in neural information processing systems. 201932</p>
<p>Designing a feature selection method based on explainable artificial intelligence. J Zacharias, M Von Zahn, J Chen, O Hinz, 10.1007/s12525-022-00608-1Electronic Markets. 3242022</p>
<p>DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems. M R Zafar, N M Khan, 10.48550/arXiv.1906.102632019</p>
<p>Visualizing and Understanding Convolutional Networks. M D Zeiler, R Fergus, 10.1007/978-3-319-10590-1_53Computer Vision -ECCV. Lecture Notes in Computer Science. D Fleet, T Pajdla, B Schiele, T Tuytelaars, Cham, SwitzerlandSpringer International Publishing2014. 2014</p>
<p>Unsupervised learning of neural networks to explain neural networks. Q Zhang, Y Yang, Y Liu, Y N Wu, S.-C Zhu, 10.48550/arXiv.1805.074682021b</p>
<p>A temporal LASSO regression model for the emergency forecasting of the suspended sediment concentrations in coastal oceans: Accuracy and interpretability. S Zhang, J Wu, Y Jia, Y.-G Wang, Y Zhang, Q Duan, 10.1016/j.engappai.2021.104206Engineering Applications of Artificial Intelligence. 1002021a. 104206</p>
<p>Reconciling disagreement on global river flood changes in a warming climate. S Zhang, L Zhou, L Zhang, Y Yang, Z Wei, S Zhou, 10.1038/s41558-022-01539-7Nature Climate Change. 12122022a</p>
<p>Explainable machine learning in materials science. X Zhong, B Gallagher, S Liu, B Kailkhura, A Hiszpanski, T Y Han, -J , 10.1038/s41524-022-00884-7npj Computational Materials. 812042022</p>
<p>Identification of Soil Texture Classes Under Vegetation Cover Based on Sentinel-2 Data With SVM and SHAP Techniques. Y Zhou, W Wu, H Wang, X Zhang, C Yang, H Liu, 10.1109/JSTARS.2022.3164140IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 152022</p>
<p>Assessing the representational accuracy of data-driven models: The case of the effect of urban green infrastructure on temperature. Environmental Modelling &amp; Software, 141. M Zumwald, C Baumberger, D N Bresch, R Knutti, 10.1016/j.envsoft.2021.1050482021105048</p>            </div>
        </div>

    </div>
</body>
</html>
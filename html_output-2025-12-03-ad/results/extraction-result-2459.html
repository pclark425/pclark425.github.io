<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2459 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2459</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2459</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-269042844</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.07738v2.pdf" target="_blank">ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2459.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2459.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ResearchAgent (LLM-powered research idea generation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented system that generates full research ideas (problem, method, experiment design) from a core scientific paper by combining citation-graph literature retrieval, an entity-centric knowledge store, and iterative LLM-based reviewing agents aligned to human-induced criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ResearchAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that (1) takes a core paper and a neighborhood of related papers (selected via a citation graph and abstract similarity), (2) retrieves relevant external entities from an entity-centric knowledge store K (co-occurrence matrix built with BLINK over titles/abstracts), (3) prompts a large language model (GPT-4 in experiments) with task-specific templates to produce a research problem, method, and experiment design, and (4) iteratively refines outputs using multiple LLM-based ReviewingAgents that score and provide feedback according to five criteria per idea (clarity, relevance/originality/feasibility/significance for problems; analogous criteria for methods and experiments), with evaluation criteria induced from a small set of human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Idea Generation System / AI-mediated Research Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Cross-disciplinary scientific research ideation (computer science, medicine, engineering, biology, and other scientific literature domains); literature-based hypothesis/idea generation and experiment design.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate novel, valid, and actionable research ideas (problem statements, proposed methods, and experiment designs) starting from a single core paper plus its citation neighborhood and additional concepts retrieved from a large entity knowledge store; aim to surface knowledge gaps, cross-domain concept cross-pollination, and feasible experimental plans.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended and high complexity: multi-objective (originality, relevance, feasibility, significance), high-dimensional search over conceptual combinations across domains, no single ground-truth solution, potential combinatorial explosion of candidate ideas; problems are not restricted to pairwise concept links but require multi-concept chaining and methodological innovation. Quantitative indicators: benchmark used 300 core papers, average ~87 reference papers per core paper, entity density ~2.17 entities per abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses pre-existing literature from Semantic Scholar (papers after May 1, 2023). Knowledge store K built from titles/abstracts of papers May 1–Dec 31, 2023 using BLINK, producing sparse co-occurrence counts across ~50,091 papers referenced in appendices; the benchmark sampled 300 high-impact core papers (>20 citations) for evaluation. Human evaluation used 10 domain-expert annotators and 150 ideas fully evaluated. Data limitations: entity coverage small (≈3 entities per paper on average); only titles/abstracts processed due to compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Relies on GPT-4 (OpenAI release Nov 06, 2023) for generation and evaluation; entity extraction used BLINK. Experiments used multiple LLMs for ablation (GPT-4, GPT-3.5, Llama3, Mixtral, Qwen1.5). No fine-tuning of LLMs; iterative refinement used multiple ReviewingAgents (15 agents instantiated). Concrete compute hours/costs not reported; qualitative notes: performance drops with smaller LMs, indicating non-trivial compute/capability requirements to reach reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended research-ideation task (ill-defined objective space) rather than optimization over a single numeric metric; discrete textual outputs (problem/method/experiment) with subjective multi-criteria evaluation (5-point Likert per criterion). Stochastic generation by LLMs; evaluation uses both human annotation and model-based scoring (GPT-4 aligned to human-induced criteria). Requires significant domain knowledge for evaluation; lacks single deterministic oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Human expert scores and model-based (GPT-4) scores on five criteria per idea type (Problems: Clarity, Relevance, Originality, Feasibility, Significance; Methods & Experiments: analogous criteria) measured on 5-point Likert scales; pairwise win-rate comparisons between models; inter-annotator agreement and human-model agreement (Spearman/Cohen statistics reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Substantially outperforms ablated variants and prior hypothesis-generation baselines on evaluation metrics: example scores reported (ResearchAgent, co-occurrence entity retrieval): Problem = 4.52, Method = 4.28, Experiment = 4.18 (5-point scale). Ablations: without entity retrieval Problem=4.35, Method=4.13, Experiment=4.02. Prior hypothesis-generation baselines in Table 3: SciMON and Hypothesis Proposer scored lower (see comparative_results). Pairwise comparisons show ResearchAgent has the highest win ratio vs baselines (exact win ratios not numerically listed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Hallucination risks typical of LLMs (made explicit in Limitations); limited entity coverage (≈3 entities/paper) constrains knowledge augmentation; diminished returns after ~3 refinement iterations (performance saturates), lower reliability for experimental designs (lower inter-annotator agreement), poorer performance on low-resource domains (physics, chemistry, mathematics) and with smaller/less-capable LLMs, and less suitability for theoretical domains requiring formal mathematical derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors: (1) citation-graph based retrieval of related papers (broad & deep related work context), (2) entity-centric knowledge store enabling cross-domain concept suggestions, (3) iterative refinement using multiple ReviewingAgents with evaluation prompts aligned to human-induced criteria, (4) use of a strong LLM (GPT-4) with task-specific templates, (5) selecting high-impact core papers as seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Ablation: both references and entity retrieval contribute; relevant references are particularly helpful. Random context elements were more helpful than none (LLM filtered noise). Retrieval strategies (co-occurrence vs embedding-based) produced comparable results. LLM-capability sensitivity: large gap between GPT-4 and smaller models (GPT-3.5, Mixtral), with smaller LMs showing marginal gains from knowledge augmentation. Compared to prior hypothesis proposers (SciMON, Hypothesis Proposer), ResearchAgent scores higher across clarity, relevance, originality, feasibility, and significance (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No single human-generated gold set of ideas; instead, human experts (10 researchers with ≥3 papers) evaluated generated ideas. Inter-annotator agreement was reported as high (Table 1: human-human scoring correlations e.g., 0.83 for problem scoring). ResearchAgent's outputs scored highly under human evaluation and beat prior automated hypothesis-generation baselines in human judgments (Table 3); a direct numeric ‘human-ideation’ baseline is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2459.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReviewingAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-powered ReviewingAgents (iterative reviewers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of multiple LLM-instantiated reviewer agents that evaluate and provide feedback on generated problems, methods, and experiments across five criteria each, and drive iterative refinement of ResearchAgent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReviewingAgents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each ReviewingAgent is an LLM prompted with task-specific templates to score one of the idea components (problem/method/experiment) on a specific criterion (e.g., clarity, validity, robustness) and to provide constructive feedback; they are instantiated with human-preference-aligned evaluation criteria induced from a small human-annotated seed set and run iteratively (15 ReviewingAgents used) to produce critiques that the ResearchAgent uses to revise ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Evaluation/Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Evaluation and iterative refinement of generated scientific ideas across disciplines covered by the ResearchAgent (computer science, medicine, engineering, biology, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Evaluate generated idea components on predefined criteria and provide feedback such that ResearchAgent can refine outputs; produce Likert-scale ratings and textual critiques.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Subjective evaluation problem with high inter-annotator variability for some criteria (especially experiment design); multi-criteria assessment increases complexity. Requires alignment with human preferences to be useful.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses human-annotated seed examples (10 pairs per criterion) to induce evaluation criteria; consumes the generated idea text plus related-paper context; no large labeled dataset for full calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Multiple LLM calls per idea per refinement iteration (15 agents × up to 3 iterations in practice); concrete compute costs not reported but non-trivial due to repeated GPT-4 calls.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and subjective; discrete textual outputs with ordinal scoring; stochastic LLM judgments but aligned via human-induced criteria to reduce distributional mismatch with human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Improvement in human/model evaluation scores of generated ideas across refinement iterations; saturation point of improvement (number of iterations until no further meaningful gains).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Iterative refinement yields consistent improvements early on, with gains saturating after ~3 iterations (averaged scores increased between initial generation and up to 3 refinement rounds; exact per-iteration numbers shown in Figure 4 but summarized as clear improvement then plateau).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited scope of perspectives captured (only 15 agents and five criteria), potential bias from the seed human annotations used to induce criteria, inability to fully replace domain expert review, and reduced reliability in some domains; possible overfitting to induced criteria leading to narrow improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Alignment of agent evaluation prompts to human-induced criteria; diverse set of ReviewingAgents focusing on multiple criteria; integration into an iterative feedback loop with the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Refinements improve scores initially but yield diminishing returns after three iterations; models without ReviewingAgents perform worse than the full pipeline, indicating ReviewingAgents materially contribute to improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human expert reviewers provided ground-truth judgments used to induce evaluation criteria and to validate outputs; 10 domain experts assessed final outputs (ResearchAgent improvements validated by human scores).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2459.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, instruction-tuned transformer LLM used in this work both as the generative backbone for ResearchAgent and as the model-based evaluator aligned to human-induced criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (used as generator and evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as the primary LLM to generate research problems, methods, and experiments using task-specific templates, and used as an automatic judge (model-based evaluation) after aligning evaluation criteria with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Large Language Model (used as AI researcher and judge)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Text-based scientific literature understanding and generation across multiple domains (CS, medicine, engineering, biology); used for both ideation and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate coherent, novel research ideas from multi-document academic contexts and to score candidate ideas along multiple qualitative criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Capable of complex multi-step reasoning and synthesis enabling cross-domain idea generation; smaller LMs lack emergent abilities resulting in lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>GPT-4 was used in zero-shot/ prompt-based fashion; model training data cut-off (noted) up to Apr 2023 while target papers were after May 2023 to test generalization beyond model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Proprietary inference costs associated with GPT-4 API calls; multiple calls per idea for generation and multiple ReviewingAgent evaluations (costs not quantified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Stochastic conditional text generation with long-context prompt engineering; outputs evaluated on multi-criteria Likert scales.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Downstream ResearchAgent performance when GPT-4 is used as the LLM (scores on 5-point Likert scales for problem/method/experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>High: with GPT-4, ResearchAgent achieved top reported scores (e.g., Problem ≈4.52, Method ≈4.28, Experiment ≈4.18). Performance fell substantially with less-capable LMs (GPT-3.5, Mixtral), demonstrating GPT-4's centrality to success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Standard LLM issues (hallucination, overconfidence); model's training cutoff means newer literature (post-Apr-2023) must be supplied via retrieval to avoid factual gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large model capacity and emergent reasoning abilities, when combined with targeted prompts and external knowledge augmentation (citations + entity store).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Table 4 shows significant drops in scores for ResearchAgent when swapping GPT-4 for smaller LMs; gap indicates strong dependence on the LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>GPT-4 was calibrated to human judgments via induced evaluation criteria, yielding high human-model agreement (reported correlations), but human experts still provided primary validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2459.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMON (prior hypothesis-generation system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published hypothesis-generation system used as a baseline in the paper; reported here as a comparator for ideation quality across multiple criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON (baseline hypothesis-generation method)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>One of the prior automated hypothesis-generation approaches compared in Table 3; designed to propose scientific hypotheses from literature (details in original SciMON paper, not elaborated here).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis Generation System (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Literature-based hypothesis generation (domain unspecified in this paper; compared across same evaluation tasks as ResearchAgent).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automated production of candidate scientific hypotheses from text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Typically focused/localized (often link- or relation-centric) hypothesis generation rather than open-ended multi-component idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on corpora used in original SciMON work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Often designed for pairwise or link-prediction style hypothesis discovery; less open-ended than ResearchAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Evaluated here using the five criteria (clarity, relevance, originality, feasibility, significance) on a 5-point Likert scale (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 3 scores (as reported): Clarity 4.04, Relevance 4.37, Originality 4.56, Feasibility 3.98, Significance 4.15 (these were the reported comparator numbers in the paper's Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>According to paper discussion, prior methods were more localized (e.g., focused on pairwise links) and may miss multi-faceted, open-ended research-idea complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Specialized algorithms for literature-based discovery; likely effective for structured link prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ResearchAgent scored higher on most criteria versus SciMON in the paper's comparative evaluation (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human evaluators scored outputs from SciMON for comparison; ResearchAgent outperformed SciMON by reported margins.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2459.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Proposer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Proposer (prior LLM-based hypothesis system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior automated hypothesis-generation method compared in this paper; included in Table 3 as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypothesis Proposer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Baseline system for generating scientific hypotheses (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis Generation System (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Literature-based hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce candidate hypotheses from scientific text; typically oriented to link predictions and conceptual associations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Often narrower in scope (link-level hypotheses) compared to the open-ended idea generation targeted by ResearchAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not specified within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Localized, often constrained to predicting relationships between concept pairs or small clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Evaluated by the same five criteria; Table 3 reports its scores.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Table 3 scores (as reported): Clarity 3.97, Relevance 4.14, Originality 4.07, Feasibility 4.01, Significance 4.11.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May be suboptimal for generating complex, multi-component research ideas; limited cross-domain synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Leverages prior LLM or literature-discovery techniques specialized for hypothesis proposing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>ResearchAgent outperformed Hypothesis Proposer across multiple criteria in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Human expert judgments used to score Hypothesis Proposer outputs in the comparative study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2459.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow (LLM + chemistry tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments large language models with chemistry-specific tools to accelerate experimental validation and chemistry workflows (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemcrow: Augmenting large-language models with chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow (prior system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines LLMs with domain-specific chemistry tools to assist with experimental planning, synthesis suggestions, or code generation in chemistry contexts (described in cited prior work; referenced here as an example of LLMs applied to experimental validation).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-augmented domain-specific tooling / Automated experimental assistance</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Chemistry / automated assistance for experimental validation, code/tool orchestration in chemistry workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Assist and accelerate experimental validation and chemistry tasks by translating natural language into tool calls, code, or procedural suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Domain-specific experimental workflows with structured tool interfaces; requires chemical domain knowledge and tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Domain data and tooling required; not specified in this paper (cited only as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Depends on integrated tools and LLM inference; not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>More constrained and tool-oriented compared to ResearchAgent's open-ended ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>In original ChemCrow work, metrics would target practical chemistry outcomes; in this paper it is referenced for concept only and not re-evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not measured in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not discussed here; general LLM hallucination and tool-integration complexity are relevant concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Tight integration of LLMs with domain tools and workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Referenced as an example of LLMs helping experimental validation (ResearchAgent focuses on ideation rather than experimental execution).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2459.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>The AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that aims at fully automated, open-ended scientific discovery (cited in related work as an example of automated discovery/AI scientist research).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>The AI Scientist (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An external project/paper aiming to create autonomous systems that can perform open-ended scientific discovery; cited as related motivation for automated scientific agents.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Discovery System (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated open-ended scientific discovery across scientific domains (general research automation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>End-to-end autonomous discovery, hypothesis formation, experiment design/implementation, and interpretation in scientific contexts (broad motivation; exact implementation details in the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely high — fully open-ended, multi-step, and requiring integration of experiment execution and physical or simulated experimentation. Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not discussed here; depends on the original cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Likely high; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended, multi-modal (experimental and conceptual), highly stochastic and multi-objective.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Not reported in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed here; referenced as background literature.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ambitious integration of generation, experiment execution, and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as part of the landscape of AI-for-science work that focuses on experimental automation vs. ResearchAgent's ideation focus.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not available within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2459.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2459.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KAPING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KAPING (Knowledge-Augmented language model PromptING)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced knowledge-augmentation prompting framework for zero-shot knowledge graph question answering; appears in examples and related work used to illustrate knowledge-augmentation concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge-Augmented language model PromptING (KAPING)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KAPING (referenced framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Augments LLM inputs with retrieved facts (from knowledge graphs) prepended to prompts to improve factual question answering in a zero-shot manner; included as an example of knowledge-augmentation techniques relevant to ResearchAgent's entity augmentation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Knowledge-Augmented Prompting System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Knowledge-graph question answering (NLP domain), not directly used in ResearchAgent experiments but conceptually related.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Retrieve relevant facts to a query and prepend them to LLM prompts to reduce hallucination and improve factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Retrieval + prompt engineering; discrete retrieval decisions, limited by knowledge graph coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires a knowledge graph of facts; referenced paper handles KG data.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Retrieval and LLM inference; specifics not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined QA task with deterministic correctness criteria (vs. ResearchAgent's open-ended ideation).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>QA accuracy improvements in the original KAPING study (not re-measured here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; referenced as related methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Dependent on retrieval relevance and KG coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Grounding model inputs with retrieved factual context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Conceptually similar to ResearchAgent's augmentation with entity-centric knowledge; ResearchAgent uses a co-occurrence matrix and optionally embedding-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not applicable within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Chemcrow: Augmenting large-language models with chemistry tools. <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models as AI research agents. <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers. <em>(Rating: 2)</em></li>
                <li>Learning to generate novel scientific directions with contextualized literature-based discovery. <em>(Rating: 2)</em></li>
                <li>Chain of ideas: Revolutionizing research via novel idea development with LLM agents. <em>(Rating: 1)</em></li>
                <li>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. <em>(Rating: 1)</em></li>
                <li>AI4Science and Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2459",
    "paper_id": "paper-269042844",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "ResearchAgent",
            "name_full": "ResearchAgent (LLM-powered research idea generation system)",
            "brief_description": "An LLM-augmented system that generates full research ideas (problem, method, experiment design) from a core scientific paper by combining citation-graph literature retrieval, an entity-centric knowledge store, and iterative LLM-based reviewing agents aligned to human-induced criteria.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ResearchAgent",
            "system_description": "Pipeline that (1) takes a core paper and a neighborhood of related papers (selected via a citation graph and abstract similarity), (2) retrieves relevant external entities from an entity-centric knowledge store K (co-occurrence matrix built with BLINK over titles/abstracts), (3) prompts a large language model (GPT-4 in experiments) with task-specific templates to produce a research problem, method, and experiment design, and (4) iteratively refines outputs using multiple LLM-based ReviewingAgents that score and provide feedback according to five criteria per idea (clarity, relevance/originality/feasibility/significance for problems; analogous criteria for methods and experiments), with evaluation criteria induced from a small set of human annotations.",
            "system_type": "Automated Idea Generation System / AI-mediated Research Assistant",
            "problem_domain": "Cross-disciplinary scientific research ideation (computer science, medicine, engineering, biology, and other scientific literature domains); literature-based hypothesis/idea generation and experiment design.",
            "problem_description": "Generate novel, valid, and actionable research ideas (problem statements, proposed methods, and experiment designs) starting from a single core paper plus its citation neighborhood and additional concepts retrieved from a large entity knowledge store; aim to surface knowledge gaps, cross-domain concept cross-pollination, and feasible experimental plans.",
            "problem_complexity": "Open-ended and high complexity: multi-objective (originality, relevance, feasibility, significance), high-dimensional search over conceptual combinations across domains, no single ground-truth solution, potential combinatorial explosion of candidate ideas; problems are not restricted to pairwise concept links but require multi-concept chaining and methodological innovation. Quantitative indicators: benchmark used 300 core papers, average ~87 reference papers per core paper, entity density ~2.17 entities per abstract.",
            "data_availability": "Uses pre-existing literature from Semantic Scholar (papers after May 1, 2023). Knowledge store K built from titles/abstracts of papers May 1–Dec 31, 2023 using BLINK, producing sparse co-occurrence counts across ~50,091 papers referenced in appendices; the benchmark sampled 300 high-impact core papers (&gt;20 citations) for evaluation. Human evaluation used 10 domain-expert annotators and 150 ideas fully evaluated. Data limitations: entity coverage small (≈3 entities per paper on average); only titles/abstracts processed due to compute cost.",
            "computational_requirements": "Relies on GPT-4 (OpenAI release Nov 06, 2023) for generation and evaluation; entity extraction used BLINK. Experiments used multiple LLMs for ablation (GPT-4, GPT-3.5, Llama3, Mixtral, Qwen1.5). No fine-tuning of LLMs; iterative refinement used multiple ReviewingAgents (15 agents instantiated). Concrete compute hours/costs not reported; qualitative notes: performance drops with smaller LMs, indicating non-trivial compute/capability requirements to reach reported results.",
            "problem_structure": "Open-ended research-ideation task (ill-defined objective space) rather than optimization over a single numeric metric; discrete textual outputs (problem/method/experiment) with subjective multi-criteria evaluation (5-point Likert per criterion). Stochastic generation by LLMs; evaluation uses both human annotation and model-based scoring (GPT-4 aligned to human-induced criteria). Requires significant domain knowledge for evaluation; lacks single deterministic oracle.",
            "success_metric": "Human expert scores and model-based (GPT-4) scores on five criteria per idea type (Problems: Clarity, Relevance, Originality, Feasibility, Significance; Methods & Experiments: analogous criteria) measured on 5-point Likert scales; pairwise win-rate comparisons between models; inter-annotator agreement and human-model agreement (Spearman/Cohen statistics reported).",
            "success_rate": "Substantially outperforms ablated variants and prior hypothesis-generation baselines on evaluation metrics: example scores reported (ResearchAgent, co-occurrence entity retrieval): Problem = 4.52, Method = 4.28, Experiment = 4.18 (5-point scale). Ablations: without entity retrieval Problem=4.35, Method=4.13, Experiment=4.02. Prior hypothesis-generation baselines in Table 3: SciMON and Hypothesis Proposer scored lower (see comparative_results). Pairwise comparisons show ResearchAgent has the highest win ratio vs baselines (exact win ratios not numerically listed in text).",
            "failure_modes": "Hallucination risks typical of LLMs (made explicit in Limitations); limited entity coverage (≈3 entities/paper) constrains knowledge augmentation; diminished returns after ~3 refinement iterations (performance saturates), lower reliability for experimental designs (lower inter-annotator agreement), poorer performance on low-resource domains (physics, chemistry, mathematics) and with smaller/less-capable LLMs, and less suitability for theoretical domains requiring formal mathematical derivations.",
            "success_factors": "Key contributors: (1) citation-graph based retrieval of related papers (broad & deep related work context), (2) entity-centric knowledge store enabling cross-domain concept suggestions, (3) iterative refinement using multiple ReviewingAgents with evaluation prompts aligned to human-induced criteria, (4) use of a strong LLM (GPT-4) with task-specific templates, (5) selecting high-impact core papers as seeds.",
            "comparative_results": "Ablation: both references and entity retrieval contribute; relevant references are particularly helpful. Random context elements were more helpful than none (LLM filtered noise). Retrieval strategies (co-occurrence vs embedding-based) produced comparable results. LLM-capability sensitivity: large gap between GPT-4 and smaller models (GPT-3.5, Mixtral), with smaller LMs showing marginal gains from knowledge augmentation. Compared to prior hypothesis proposers (SciMON, Hypothesis Proposer), ResearchAgent scores higher across clarity, relevance, originality, feasibility, and significance (see Table 3).",
            "human_baseline": "No single human-generated gold set of ideas; instead, human experts (10 researchers with ≥3 papers) evaluated generated ideas. Inter-annotator agreement was reported as high (Table 1: human-human scoring correlations e.g., 0.83 for problem scoring). ResearchAgent's outputs scored highly under human evaluation and beat prior automated hypothesis-generation baselines in human judgments (Table 3); a direct numeric ‘human-ideation’ baseline is not provided.",
            "uuid": "e2459.0",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ReviewingAgents",
            "name_full": "LLM-powered ReviewingAgents (iterative reviewers)",
            "brief_description": "A set of multiple LLM-instantiated reviewer agents that evaluate and provide feedback on generated problems, methods, and experiments across five criteria each, and drive iterative refinement of ResearchAgent outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "ReviewingAgents",
            "system_description": "Each ReviewingAgent is an LLM prompted with task-specific templates to score one of the idea components (problem/method/experiment) on a specific criterion (e.g., clarity, validity, robustness) and to provide constructive feedback; they are instantiated with human-preference-aligned evaluation criteria induced from a small human-annotated seed set and run iteratively (15 ReviewingAgents used) to produce critiques that the ResearchAgent uses to revise ideas.",
            "system_type": "Automated Evaluation/Refinement Agents",
            "problem_domain": "Evaluation and iterative refinement of generated scientific ideas across disciplines covered by the ResearchAgent (computer science, medicine, engineering, biology, etc.).",
            "problem_description": "Evaluate generated idea components on predefined criteria and provide feedback such that ResearchAgent can refine outputs; produce Likert-scale ratings and textual critiques.",
            "problem_complexity": "Subjective evaluation problem with high inter-annotator variability for some criteria (especially experiment design); multi-criteria assessment increases complexity. Requires alignment with human preferences to be useful.",
            "data_availability": "Uses human-annotated seed examples (10 pairs per criterion) to induce evaluation criteria; consumes the generated idea text plus related-paper context; no large labeled dataset for full calibration.",
            "computational_requirements": "Multiple LLM calls per idea per refinement iteration (15 agents × up to 3 iterations in practice); concrete compute costs not reported but non-trivial due to repeated GPT-4 calls.",
            "problem_structure": "Open-ended and subjective; discrete textual outputs with ordinal scoring; stochastic LLM judgments but aligned via human-induced criteria to reduce distributional mismatch with human scores.",
            "success_metric": "Improvement in human/model evaluation scores of generated ideas across refinement iterations; saturation point of improvement (number of iterations until no further meaningful gains).",
            "success_rate": "Iterative refinement yields consistent improvements early on, with gains saturating after ~3 iterations (averaged scores increased between initial generation and up to 3 refinement rounds; exact per-iteration numbers shown in Figure 4 but summarized as clear improvement then plateau).",
            "failure_modes": "Limited scope of perspectives captured (only 15 agents and five criteria), potential bias from the seed human annotations used to induce criteria, inability to fully replace domain expert review, and reduced reliability in some domains; possible overfitting to induced criteria leading to narrow improvements.",
            "success_factors": "Alignment of agent evaluation prompts to human-induced criteria; diverse set of ReviewingAgents focusing on multiple criteria; integration into an iterative feedback loop with the generator.",
            "comparative_results": "Refinements improve scores initially but yield diminishing returns after three iterations; models without ReviewingAgents perform worse than the full pipeline, indicating ReviewingAgents materially contribute to improved outputs.",
            "human_baseline": "Human expert reviewers provided ground-truth judgments used to induce evaluation criteria and to validate outputs; 10 domain experts assessed final outputs (ResearchAgent improvements validated by human scores).",
            "uuid": "e2459.1",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "A large, instruction-tuned transformer LLM used in this work both as the generative backbone for ResearchAgent and as the model-based evaluator aligned to human-induced criteria.",
            "citation_title": "GPT-4 technical report.",
            "mention_or_use": "use",
            "system_name": "GPT-4 (used as generator and evaluator)",
            "system_description": "Used as the primary LLM to generate research problems, methods, and experiments using task-specific templates, and used as an automatic judge (model-based evaluation) after aligning evaluation criteria with human annotations.",
            "system_type": "Large Language Model (used as AI researcher and judge)",
            "problem_domain": "Text-based scientific literature understanding and generation across multiple domains (CS, medicine, engineering, biology); used for both ideation and evaluation.",
            "problem_description": "Generate coherent, novel research ideas from multi-document academic contexts and to score candidate ideas along multiple qualitative criteria.",
            "problem_complexity": "Capable of complex multi-step reasoning and synthesis enabling cross-domain idea generation; smaller LMs lack emergent abilities resulting in lower performance.",
            "data_availability": "GPT-4 was used in zero-shot/ prompt-based fashion; model training data cut-off (noted) up to Apr 2023 while target papers were after May 2023 to test generalization beyond model training data.",
            "computational_requirements": "Proprietary inference costs associated with GPT-4 API calls; multiple calls per idea for generation and multiple ReviewingAgent evaluations (costs not quantified in paper).",
            "problem_structure": "Stochastic conditional text generation with long-context prompt engineering; outputs evaluated on multi-criteria Likert scales.",
            "success_metric": "Downstream ResearchAgent performance when GPT-4 is used as the LLM (scores on 5-point Likert scales for problem/method/experiment).",
            "success_rate": "High: with GPT-4, ResearchAgent achieved top reported scores (e.g., Problem ≈4.52, Method ≈4.28, Experiment ≈4.18). Performance fell substantially with less-capable LMs (GPT-3.5, Mixtral), demonstrating GPT-4's centrality to success.",
            "failure_modes": "Standard LLM issues (hallucination, overconfidence); model's training cutoff means newer literature (post-Apr-2023) must be supplied via retrieval to avoid factual gaps.",
            "success_factors": "Large model capacity and emergent reasoning abilities, when combined with targeted prompts and external knowledge augmentation (citations + entity store).",
            "comparative_results": "Table 4 shows significant drops in scores for ResearchAgent when swapping GPT-4 for smaller LMs; gap indicates strong dependence on the LLM capability.",
            "human_baseline": "GPT-4 was calibrated to human judgments via induced evaluation criteria, yielding high human-model agreement (reported correlations), but human experts still provided primary validation.",
            "uuid": "e2459.2",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "SciMON (prior hypothesis-generation system)",
            "brief_description": "A previously published hypothesis-generation system used as a baseline in the paper; reported here as a comparator for ideation quality across multiple criteria.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SciMON (baseline hypothesis-generation method)",
            "system_description": "One of the prior automated hypothesis-generation approaches compared in Table 3; designed to propose scientific hypotheses from literature (details in original SciMON paper, not elaborated here).",
            "system_type": "Hypothesis Generation System (baseline)",
            "problem_domain": "Literature-based hypothesis generation (domain unspecified in this paper; compared across same evaluation tasks as ResearchAgent).",
            "problem_description": "Automated production of candidate scientific hypotheses from text corpora.",
            "problem_complexity": "Typically focused/localized (often link- or relation-centric) hypothesis generation rather than open-ended multi-component idea generation.",
            "data_availability": "Operates on corpora used in original SciMON work (not detailed here).",
            "computational_requirements": "Not specified in this paper.",
            "problem_structure": "Often designed for pairwise or link-prediction style hypothesis discovery; less open-ended than ResearchAgent.",
            "success_metric": "Evaluated here using the five criteria (clarity, relevance, originality, feasibility, significance) on a 5-point Likert scale (Table 3).",
            "success_rate": "Table 3 scores (as reported): Clarity 4.04, Relevance 4.37, Originality 4.56, Feasibility 3.98, Significance 4.15 (these were the reported comparator numbers in the paper's Table 3).",
            "failure_modes": "According to paper discussion, prior methods were more localized (e.g., focused on pairwise links) and may miss multi-faceted, open-ended research-idea complexity.",
            "success_factors": "Specialized algorithms for literature-based discovery; likely effective for structured link prediction tasks.",
            "comparative_results": "ResearchAgent scored higher on most criteria versus SciMON in the paper's comparative evaluation (Table 3).",
            "human_baseline": "Human evaluators scored outputs from SciMON for comparison; ResearchAgent outperformed SciMON by reported margins.",
            "uuid": "e2459.3",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Hypothesis Proposer",
            "name_full": "Hypothesis Proposer (prior LLM-based hypothesis system)",
            "brief_description": "A prior automated hypothesis-generation method compared in this paper; included in Table 3 as a baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Hypothesis Proposer",
            "system_description": "Baseline system for generating scientific hypotheses (details not provided in this paper).",
            "system_type": "Hypothesis Generation System (baseline)",
            "problem_domain": "Literature-based hypothesis generation.",
            "problem_description": "Produce candidate hypotheses from scientific text; typically oriented to link predictions and conceptual associations.",
            "problem_complexity": "Often narrower in scope (link-level hypotheses) compared to the open-ended idea generation targeted by ResearchAgent.",
            "data_availability": "Not specified within this paper.",
            "computational_requirements": "Not specified within this paper.",
            "problem_structure": "Localized, often constrained to predicting relationships between concept pairs or small clusters.",
            "success_metric": "Evaluated by the same five criteria; Table 3 reports its scores.",
            "success_rate": "Table 3 scores (as reported): Clarity 3.97, Relevance 4.14, Originality 4.07, Feasibility 4.01, Significance 4.11.",
            "failure_modes": "May be suboptimal for generating complex, multi-component research ideas; limited cross-domain synthesis.",
            "success_factors": "Leverages prior LLM or literature-discovery techniques specialized for hypothesis proposing.",
            "comparative_results": "ResearchAgent outperformed Hypothesis Proposer across multiple criteria in the experiments reported.",
            "human_baseline": "Human expert judgments used to score Hypothesis Proposer outputs in the comparative study.",
            "uuid": "e2459.4",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow (LLM + chemistry tools)",
            "brief_description": "An approach that augments large language models with chemistry-specific tools to accelerate experimental validation and chemistry workflows (cited as related work).",
            "citation_title": "Chemcrow: Augmenting large-language models with chemistry tools.",
            "mention_or_use": "mention",
            "system_name": "ChemCrow (prior system)",
            "system_description": "Combines LLMs with domain-specific chemistry tools to assist with experimental planning, synthesis suggestions, or code generation in chemistry contexts (described in cited prior work; referenced here as an example of LLMs applied to experimental validation).",
            "system_type": "LLM-augmented domain-specific tooling / Automated experimental assistance",
            "problem_domain": "Chemistry / automated assistance for experimental validation, code/tool orchestration in chemistry workflows.",
            "problem_description": "Assist and accelerate experimental validation and chemistry tasks by translating natural language into tool calls, code, or procedural suggestions.",
            "problem_complexity": "Domain-specific experimental workflows with structured tool interfaces; requires chemical domain knowledge and tool integration.",
            "data_availability": "Domain data and tooling required; not specified in this paper (cited only as related work).",
            "computational_requirements": "Depends on integrated tools and LLM inference; not specified here.",
            "problem_structure": "More constrained and tool-oriented compared to ResearchAgent's open-ended ideation.",
            "success_metric": "In original ChemCrow work, metrics would target practical chemistry outcomes; in this paper it is referenced for concept only and not re-evaluated.",
            "success_rate": "Not measured in this paper (cited as related work).",
            "failure_modes": "Not discussed here; general LLM hallucination and tool-integration complexity are relevant concerns.",
            "success_factors": "Tight integration of LLMs with domain tools and workflows.",
            "comparative_results": "Referenced as an example of LLMs helping experimental validation (ResearchAgent focuses on ideation rather than experimental execution).",
            "human_baseline": "Not applicable in this paper's experiments.",
            "uuid": "e2459.5",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "The AI Scientist",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "A referenced work that aims at fully automated, open-ended scientific discovery (cited in related work as an example of automated discovery/AI scientist research).",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "The AI Scientist (referenced)",
            "system_description": "An external project/paper aiming to create autonomous systems that can perform open-ended scientific discovery; cited as related motivation for automated scientific agents.",
            "system_type": "AI Scientist / Automated Discovery System (referenced)",
            "problem_domain": "Automated open-ended scientific discovery across scientific domains (general research automation).",
            "problem_description": "End-to-end autonomous discovery, hypothesis formation, experiment design/implementation, and interpretation in scientific contexts (broad motivation; exact implementation details in the cited paper).",
            "problem_complexity": "Extremely high — fully open-ended, multi-step, and requiring integration of experiment execution and physical or simulated experimentation. Not evaluated in this paper.",
            "data_availability": "Not discussed here; depends on the original cited work.",
            "computational_requirements": "Likely high; not quantified in this paper.",
            "problem_structure": "Open-ended, multi-modal (experimental and conceptual), highly stochastic and multi-objective.",
            "success_metric": "Not reported in this paper (cited as related work).",
            "success_rate": "Not reported here.",
            "failure_modes": "Not detailed here; referenced as background literature.",
            "success_factors": "Ambitious integration of generation, experiment execution, and evaluation.",
            "comparative_results": "Mentioned as part of the landscape of AI-for-science work that focuses on experimental automation vs. ResearchAgent's ideation focus.",
            "human_baseline": "Not available within this paper.",
            "uuid": "e2459.6",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "KAPING",
            "name_full": "KAPING (Knowledge-Augmented language model PromptING)",
            "brief_description": "A referenced knowledge-augmentation prompting framework for zero-shot knowledge graph question answering; appears in examples and related work used to illustrate knowledge-augmentation concepts.",
            "citation_title": "Knowledge-Augmented language model PromptING (KAPING)",
            "mention_or_use": "mention",
            "system_name": "KAPING (referenced framework)",
            "system_description": "Augments LLM inputs with retrieved facts (from knowledge graphs) prepended to prompts to improve factual question answering in a zero-shot manner; included as an example of knowledge-augmentation techniques relevant to ResearchAgent's entity augmentation approach.",
            "system_type": "Knowledge-Augmented Prompting System",
            "problem_domain": "Knowledge-graph question answering (NLP domain), not directly used in ResearchAgent experiments but conceptually related.",
            "problem_description": "Retrieve relevant facts to a query and prepend them to LLM prompts to reduce hallucination and improve factuality.",
            "problem_complexity": "Retrieval + prompt engineering; discrete retrieval decisions, limited by knowledge graph coverage.",
            "data_availability": "Requires a knowledge graph of facts; referenced paper handles KG data.",
            "computational_requirements": "Retrieval and LLM inference; specifics not given here.",
            "problem_structure": "Well-defined QA task with deterministic correctness criteria (vs. ResearchAgent's open-ended ideation).",
            "success_metric": "QA accuracy improvements in the original KAPING study (not re-measured here).",
            "success_rate": "Not reported in this paper; referenced as related methodology.",
            "failure_modes": "Dependent on retrieval relevance and KG coverage.",
            "success_factors": "Grounding model inputs with retrieved factual context.",
            "comparative_results": "Conceptually similar to ResearchAgent's augmentation with entity-centric knowledge; ResearchAgent uses a co-occurrence matrix and optionally embedding-based retrieval.",
            "human_baseline": "Not applicable within this paper.",
            "uuid": "e2459.7",
            "source_info": {
                "paper_title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Benchmarking large language models as AI research agents.",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_as_ai_research_agents"
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zero_shot_hypothesis_proposers"
        },
        {
            "paper_title": "Learning to generate novel scientific directions with contextualized literature-based discovery.",
            "rating": 2,
            "sanitized_title": "learning_to_generate_novel_scientific_directions_with_contextualized_literaturebased_discovery"
        },
        {
            "paper_title": "Chain of ideas: Revolutionizing research via novel idea development with LLM agents.",
            "rating": 1,
            "sanitized_title": "chain_of_ideas_revolutionizing_research_via_novel_idea_development_with_llm_agents"
        },
        {
            "paper_title": "Knowledge-augmented language model prompting for zero-shot knowledge graph question answering.",
            "rating": 1,
            "sanitized_title": "knowledgeaugmented_language_model_prompting_for_zeroshot_knowledge_graph_question_answering"
        },
        {
            "paper_title": "AI4Science and Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4.",
            "rating": 1,
            "sanitized_title": "ai4science_and_quantum_2023_the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        }
    ],
    "cost": 0.02370575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models
9 Feb 2025</p>
<p>Jinheon Baek jinheon.baek@kaist.ac.kr 
Sujay Kumar Jauhar 
Silviu Cucerzan silviu@microsoft.com 
Sung Ju Hwang sungju.hwang@kaist.ac.kr 
Kaist 
Microsoft Research 
Deepauto Ai 
ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models
9 Feb 202584B35CB14A94C97F0ABF2E7B30B5A95FarXiv:2404.07738v2[cs.CL]
The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise.Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields.To enhance the productivity of researchers, we propose Re-searchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work.This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents.Specifically, starting with a core scientific paper, Re-searchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers.Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based Re-viewingAgents that provide reviews and feedback via iterative revision processes.These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting.We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and modelbased evaluation results.Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work 1 .</p>
<p>Introduction</p>
<p>Scientific research plays a crucial role in driving innovation, advancing knowledge, solving prob-Figure 1: (A) The scientific knowledge used for research idea generation consists of a paper, its relationships over an academic graph, and entities within a knowledge store extracted from numerous papers.(B) Given them, the proposed research idea generation process involves problem identification, method development, and experiment design.Those are also iteratively refined by reviews and feedback from reviewing agents, aligned with criteria induced from human judgements.lems, expanding our understanding of the world, and ultimately improving the lives of people in tangible ways.This process usually consists of two key components: the formulation of new research ideas and the validation of these ideas through wellcrafted experiments, which are typically conducted by human researchers (Hope et al., 2023;Wang et al., 2023a;Huang et al., 2023).However, this is a slow, effort-intensive process, which requires reading and synthesizing overwhelming amounts of knowledge over the vast corpus of rapidly growing scientific literature to formulate research ideas, as well as design and perform experimental validations of those ideas.For example, the number of academic papers published per year is more than 7 million (Fire and Guestrin, 2019).Similarly, the process of testing a new pharmaceutical drug requires deep expertise, and is massively expensive and labor-intensive, often taking several years (Vamathevan et al., 2019).</p>
<p>In the meantime, recent Large Language Models (LLMs) (Touvron et al., 2023;OpenAI, 2023;Anil et al., 2023) have shown impressive capabili-ties in processing and generating text with remarkable accuracy, even outperforming human experts across diverse specialized domains including math, physics, history, law, medicine, and ethics.They are able to process and analyze large volumes of data at speeds and scales far exceeding human capabilities, have internalized large swaths of human knowledge from being trained on virtually the entire web, and can identify patterns, trends, and correlations that may not be immediately apparent to human researchers (such as the usage of quantum mechanics in medical imaging or applying psychological insights in AI).This renders them ideally poised to become foundational tools to accelerate the two phases of the scientific research process: ideation of novel research opportunities, and scientific validation of those research hypotheses.</p>
<p>A few recent papers in the domain of LLMaugmented scientific discovery have focused on the second phase.Specifically, they attempt (Huang et al., 2023;AI4Science and Quantum, 2023;Bran et al., 2023) to mainly accelerate the experimental validation process, by writing code for machinelearning models, facilitating the exploration of chemical spaces, or advancing the simulation of molecular dynamics.Thus, in this paper, we leverage LLMs in the first phase of scientific research -specifically idea generation, whose key focus is conceptualizing novel research questions, methodologies, and experiments.To the best of our knowledge, our work is the first to leverage and evaluate the capabilities of LLMs to act as mediators in scientific idea generation in an open-ended setting.</p>
<p>Given our goal to build an LLM-powered Re-searchAgent, we draw inspiration from how human researchers position themselves to come up with novel research ideas.We draw distinctions between three key components of their workflow: a broad and deep understanding of related scientific literature, an encyclopedic view of concepts and how they relate to one another both within and across domains, and a community of colleagues on which to rely for feedback and constructive criticism.</p>
<p>We model each of these three aspects in our ResearchAgent.Specifically, in order to imbibe related work, the system begins with a core scientific paper and then explores a range of related papers through references and citation relationships.Further, to develop an encyclopedic view of related concepts, we build and then augment ResearchAgent with an entity-centric knowledge store derived from co-occurrences of key concepts in the scien-tific literature.This repository is aimed at capturing novel underlying relationships within and across domains, thereby increasing the chances of a crosspollination of ideas (Wahle et al., 2023).Finally, to simulate robust feedback mechanisms, we instantiate a number of LLM-powered ReviewingAgents that help the ResearchAgent to iterate on research idea generation with constructive critiques.Crucially, these ReviewingAgents are prompted with evaluation criteria that are induced from real researchers' judgements, thus aligning them with actual scientific preferential standards.An illustration of our system is provided in Figure 1.</p>
<p>We validate the effectiveness of ResearchAgent for research idea generation based on scientific literature across multiple disciplines.Then, on a battery of tests conducted with both human-and modelbased evaluations, we demonstrate that ResearchAgent outperforms strong LLM-powered baselines by large margins, generating more clear, relevant, and significant ideas that are especially novel.Furthermore, analyses show the efficacy of our comprehensive approach to modeling ResearchAgent: the entity-centric knowledge store and the iterative idea refinement steps help the system generate meaningfully better ideas compared with an instantiation that is purely based on prior related work.</p>
<p>These findings highlight the immense potential of AI-mediated research assistants like ResearchAgent to enhance the ideation process in scientific research.In practice, it can support researchers by identifying knowledge gaps, proposing novel problem statements, and suggesting potential methodologies early in the research process.Also, it can assist in designing experiments and streamline the writing and refinement of research papers by generating drafts and offering feedback on how to effectively frame contributions and cite relevant work.</p>
<p>Related Work</p>
<p>Large Language Models LLMs have shown impressive performances across various tasks (Ope-nAI, 2023;Anil et al., 2023), including scientific fields such as mathematics, physics, medicine, and computer science (Portenoy et al., 2021;Romera-Paredes et al., 2023;Bran et al., 2023;Huang et al., 2023;Liu et al., 2024).For instance, GPT-4 can understand DNA sequences, design biomolecules, predict molecular behavior, and solve PDE problems (AI4Science and Quantum, 2023).However, LLMs have mainly been used for accelerating the experimental validation of already identified research ideas, but not for identifying new problems.</p>
<p>Hypothesis Generation</p>
<p>The principle of hypothesis generation is based on literature-based discovery (Swanson, 1986), which aims to discover relationships between concepts (Henry and McInnes, 2017).For instance, these concepts could be a specific disease and a compound not yet considered as a treatment for it.Early works on automatic hypothesis generation first build a corpus of discrete concepts, and then identify their relationships with machine learning approaches, e.g., using similarities between word (concept) vectors (Tshitoyan et al., 2019) or applying link prediction methods over a graph (where concepts are nodes) (Sybrandt et al., 2020;Nadkarni et al., 2021).Recent approaches are further powered by LLMs (Wang et al., 2023b;Qi et al., 2023;Yang et al., 2023), leveraging their prior knowledge about scientific disciplines.Yet, all these approaches perform idea generation in a localized manner and are designed to identify potential relationships between two variables or generate sentence-level connections, which may be sub-optimal to capture the complexity and multifaceted nature of real-world problems (e.g., urban planning involves numerous interacting variables).Meanwhile, we do not artificially restrict the target research idea to be a predictive single concept or simple binary link, instead allowing the model to generate ideas in a more open-ended fashion.</p>
<p>We note that there has been a recent surge of interest in exploring scientific idea generation: from Li et al. (2024) that focus on evaluating whether LLMs can generate research ideas that are better than human ideas, to Lu et al. (2024) that aim to automatically generate full research papers (including idea development, code writing, and experiment execution), to Li et al. (2024) that enhance the idea generation process by organizing a sequential chain of literature, all of which acknowledge and build upon insights from a prior version of our paper.</p>
<p>Knowledge-Augmented LLMs</p>
<p>The approach to augment LLMs with external knowledge makes them more accurate and relevant to target contexts.Much prior work aims at improving the factuality of LLM responses to queries by retrieving the relevant documents and injecting them into the LLM input (Lazaridou et al., 2022;Ram et al., 2023;Shi et al., 2023).In addition, given that entities or facts are atomic units for representing knowledge, recent studies augment LLMs with them (Baek et al., 2023;Wu et al., 2023).In contrast to these efforts, which use knowledge units piecemeal, we instead jointly leverage accumulated knowledge over massive troves of scientific papers.Also, Baek et al. (2024) proposes to use entities for query suggestion, which -while similar -has the entirely different objective of narrowing the focus of LLMs to entities already present in their context.Instead, our approach retrieves and integrates entities outside the given context, enabling LLMs to explore other concepts for research idea generation.</p>
<p>Iterative Refinements with LLMs Similar to humans, LLMs do not always generate optimal outputs on their first attempt.To tackle this, drawing inspiration from humans who can iteratively refine their thoughts based on critiques from themselves and their peers, many recent studies have investigated the potential of LLMs to correct and refine their outputs, demonstrating that they indeed possess those capabilities (Welleck et al., 2023;Madaan et al., 2023;Shridhar et al., 2023;Ganguli et al., 2023;Wang et al., 2023b;Qi et al., 2023;Yang et al., 2023).Based on their findings, we extend this paradigm (and further test their capability) to our novel scenario of research idea generation.</p>
<p>Method</p>
<p>We present ResearchAgent, a system that automatically proposes research ideas with LLMs.</p>
<p>LLM-Powered Research Idea Generation</p>
<p>We begin by formally introducing the new problem of research idea generation, followed by an explanation of how LLMs are utilized to tackle it.</p>
<p>Research Idea Generation</p>
<p>The goal of the research idea generation task is to formulate new and valid research ideas, to enhance the overall efficiency of the first phase of scientific discovery.While we acknowledge that the real process by which humans conduct research is varied and complex to an extent well beyond the scope of this scientific study, we attempt to model simulacra in three systematic steps that would likely be maximally beneficial to a researcher seeking assistance from an AI system.These are namely, identifying novel research ideas, proposing methods to validate these ideas, and designing experiments to measure the success of these methods in relation to the ideas.</p>
<p>To accomplish the aforementioned steps, we utilize the existing literature (such as academic pub-lications) as a primary source, which provides insights about existing knowledge along with gaps and unanswered questions2 .Formally, let L be the literature, and o be the ideas that consist of the problem p, method m, and experiment design d, as follows: o = [p, m, d] where each item consists of a sequence of tokens.Then, the idea generation model f can be represented as follows: o = f (L), which is further decomposed into three submodular steps: p = f (L) for identifying problems, m = f (p, L) for developing methods, and d = f (p, m, L) for designing experiments.We operationalize f with LLMs, leveraging their capability to understand and generate academic text.</p>
<p>Large Language Models Before describing the LLM in the context of our problem setup, let us first provide its general definition, which takes an input sequence of tokens x and generates an output sequence of tokens y, as follows: y = LLM θ (T (x)).</p>
<p>Here, the model parameters θ are typically fixed after training, due to the high costs of further finetuning.In addition, the prompt template T serves as a structured format that outlines the context (including the task descriptions and instructions) to direct the model in generating the desired outputs.</p>
<p>Knowledge-Augmented LLMs for Research Idea Generation</p>
<p>We now turn to our primary focus of automatically generating research ideas with LLMs.Recall that we aim to produce a complete idea consisting of the problem, method, and experiment design (o = [p, m, d]), while using the existing literature L as a primary source of information.We operationalize this with LLMs by instantiating the aforementioned research idea generation function f with LLM coupled with the taskspecific template.Following this general formulation, the important question to answer is how the body of scientific literature is leveraged for actually generating research ideas with LLMs.Here, we outline three key desiderata that contribute to the success of human researchers ideating novel research ideas: a broad and deep understanding of related work, an encyclopedic perspective on the interconnectedness of concepts within and across scientific domains, and a community of peers who help iteratively improve ideas through constructive critiques.We describe our operationalization of these three desiderata using the prior literature and LLMs in what follows.</p>
<p>Citation Graph-based Literature Survey Due to the constraints on their input lengths and their reasoning abilities, particularly over very long contexts (Liu et al., 2023a), it is not possible to incorporate all the existing publications from the literature L into the LLM input.Instead, we need to find a meaningful subset relevant to the problem at hand.To achieve this, we mirror the process followed by human researchers, who expand their knowledge of a paper by perusing other papers that either cite or are cited by it.Concretely, for the LLM, we initiate its literature review process by providing a core paper l 0 from L and then selectively incorporating subsequent papers {l 1 , ..., l n } that are directly connected based on a citation graph.This procedure makes the LLM input for idea generation more manageable and coherent.In addition, we operationalize the selection process of the core paper and its relevant citations with two design choices: 1) the core paper is selected based on its citation count (e.g., exceeding 100 over 3 months) typically indicating high impact; 2) its relevant papers (which may be potentially numerous) are further narrow-downed based on their similarities of abstracts with the core paper, ensuring a more focused and relevant set of related work.</p>
<p>However, despite the simplicity and intuitiveness of this idea generation approach, there exists one major limitation.This approach relies exclusively on a set of given papers (the core paper and its citations); however, since scientific knowledge is not confined to specific studies but rather accumulates across a wide range of publications (across various fields), we should ideally harness this extensive, interconnected, and relevant scientific knowledge in our method for research idea generation.</p>
<p>Entity-Centric Knowledge Augmentation</p>
<p>In order to model an encyclopedic view of interconnected concepts, we must effectively design a framework to extract, store and effectively leverage the vast amount of knowledge in scientific literature L. In this work, we view entities as the atomic units of knowledge, which allows for ease of representation and accumulation over papers in a unified manner across different disciplines.For example, we can easily extract the term "database" whenever it appears in any paper, using existing off-the-shelf entity linking methods and then aggregate their linked occurrences into a knowledge store.Then, if the term "database" is prevalent within the realm of medical science but less so in hematology (which is a subdomain of medical science), the constructed knowledge store can capture the affinity between those two domains based on overlapping entities.This representational paradigm can then be used to suggest the term "database" when formulating the ideas about hematology.In other words, this approach enables providing novel and interdisciplinary insights by leveraging the interconnectedness of entities across various fields.</p>
<p>Formally, we design the knowledge store as a two-dimensional matrix K ∈ R m×m where m is the total number of unique entities identified and K is implemented in a sparse format.This knowledge store is constructed by extracting entities over all the available scientific articles in literature L3 , which not only counts the co-occurrences between entity pairs within individual papers but also quantifies the count for each entity.Our approach is versatile, thus, we can use any entity linker; in this paper we use one developed by Wu et al. (2020).This off-the-shelf system proves capable of extracting key scientific entities (which is demonstrated in Table 16) despite its lack of customized training for the scientific domain.Specifically, this linker tags and canonicalizes entities in a paper l from L, formalized as follows: E l = EL(l) where E l denotes a multiset of entities (allowing for repetitions) appearing in l4 .Upon extracting entities E, to store them into the knowledge store K, we consider all possible pairs of E represented as follows: {e i , e j } (i,j)∈C(|E|,2) where e ∈ E.</p>
<p>Given this knowledge store K, our next goal is to enhance the previous vanilla research idea generation process implemented based on a group of interconnected papers, denoted as follows: o = LLM(T ({l 0 , l 1 , ..., l n })).We do this by augmenting the LLM with the relevant entities from K, which expand the context that LLMs consume with additional knowledge.Formally, let us define entities extracted from the group of interconnected papers, as follows: E {l 0 ,...,ln} = n i=0 EL(l i ).Then, the probabilistic form of retrieving the top-k relevant external entities can be represented as follows:</p>
<p>Ret({l 0 , ..., ln} ; K) = arg max where [m] = {1, ..., m} and e i / ∈ E {l 0 ,...,ln} .Also, for simplicity, by applying Bayes' rule and assuming that entities are independent, the retrieval operation (Equation 1) can be approximated as follows: where P (e j |e i ) and P (e i ) can be derived from values in the two-dimensional matrix K, suitably normalized.We note that the formulation in Equation 2 is only one instance of operationalizing retrieval; this could be replaced with other retrieval strategies -for example, embedding-based retrieval (discussions and results are provided in Appendix B.3). Hereafter, the instantiation of research proposal generation augmented with relevant entitycentric knowledge is formalized as follows: o = LLM(T ({l 0 , ..., l n } , Ret({l 0 , ..., l n } ; K)))5 .We call this knowledge-augmented LLM-powered idea generation approach ResearchAgent, and provide the templates to instantiate it in Tables 6, 7, and 8.</p>
<p>Iterative Research Idea Refinements</p>
<p>We note that attempting to write a full research idea in one go may not be an effective strategy.Humans write drafts that are continually improved based on multiple rounds of reviews and feedback.Therefore, we lastly model a community of peers for iterative idea improvement by introducing a set of LLM-powered reviewing agents (called ReviewingAgents), which provide the ResearchAgent with reviews and feedback according to various criteria for improvement.</p>
<p>Specifically, similar to our approach to instantiate ResearchAgent with an LLM (LLM) and template (T ), ReviewingAgents are instantiated similarly but with different templates (See Tables 9, 10, and 11).Then, with ReviewingAgents, each of the generated research ideas (problem, method, and experiment design) is separately evaluated according to its own specific five criteria6 , which are provided in labels of Figure 2 and detailed in Table 12.Based on the reviews and feedback from ReviewingAgents, the ResearchAgent iteratively updates and refines its generation of research ideas.</p>
<p>Despite the proficiency of LLMs in the evaluation of machine-generated texts (Zheng et al., 2023;Fu et al., 2023), their judgments on research ideas may not be aligned with the judgments of real human researchers.However, there are no ground truth reference judgments available, and collecting them to align LLMs is expensive and often infeasible.Ideally, the judgments made by LLMs should be similar to the ones made by humans, and we aim to ensure this by automatically generating human preference-aligned evaluation criteria (used for automatic evaluations) with a few human annotations.Specifically, to obtain these humanaligned evaluation criteria, we first collect 10 pairs of research ideas and their associated scores for every evaluation criterion on a 5-point Likert scale, annotated by human researchers having at least 3 papers.After that, we prompt the LLM with these human-annotated pairs to induce detailed descriptions for evaluation criteria (Lin et al., 2024) (See Tables 13, 14, and 15).These criteria reflect the underlying human preferences7 and are used as evaluation criteria by the ReviewingAgents.</p>
<p>Experimental Setup</p>
<p>Data</p>
<p>The main source to generate research ideas is the scientific literature L, which we obtain from the Semantic Scholar Academic Graph API8 .From this, we select papers appearing after May 01, 2023, because LLMs that we use in our experiments are trained on data from the open web available before this point.This follows the procedure of existing literature-based hypothesis generation work (Qi et al., 2023).Then, we select high-impact papers (that have more than 20 citations) as core papers, mirroring human researchers' tendency to leverage influential work, to ensure the high quality of generated ideas.The resulting data is still very large; thus, we further sample a subset of 300 papers as core papers to obtain a reasonably sized benchmark dataset.The average number of reference papers for each core paper is 87; the abstract of each paper has 2.17 entities on average.The distribution of disciplines for all papers is provided in Figure 7.</p>
<p>Baselines and Our Model</p>
<p>As we target the novel task of research idea generation involving the generation of problems, methods, and experimental designs, there are no baselines for direct comparison.Thus, we mainly compare our full ResearchAgent model against its ablated variants, outlined as follows:</p>
<ol>
<li>
<p>Naive ResearchAgent -which uses only a core paper to generate research ideas.</p>
</li>
<li>
<p>ResearchAgent w/o Entity Retrieval -which uses the core paper and its relevant references without considering entities.</p>
</li>
<li>
<p>ResearchAgent -which is our full model that uses the relevant references and entities along with the core paper, to augment LLMs.</p>
</li>
</ol>
<p>In addition to this set of core baselines, we also compare our approach against existing hypothesis generation work from prior literature in Table 3.</p>
<p>Evaluation Setup</p>
<p>Given our formulation of idea generation (Sec 3.1), there are no ground-truth answers to measure the quality of the generated ideas.Yet, exhaustively listing pairs of core papers and reference research ideas is suboptimal, since there may exist a large number of valid research ideas for each core paper, and this process requires much time, effort and expertise on the part of human researchers.Thus, we use a combination of model-based automatic evaluation and manual human evaluation to validate different models on our experimental benchmark.</p>
<p>Model-based Evaluation Following the recent trends in using LLMs to judge the quality of output texts (especially in the setting of reference-free evaluations) (Zheng et al., 2023;Fu et al., 2023;Liu et al., 2023b), we use GPT-4 to judge the quality of research ideas.Note that each of the problem, method, and experiment design is evaluated with five different criteria (See labels of Figure 2 for criteria and see Table 12 for their detailed descriptions).We ask the LLM-based evaluation model to either rate the generated idea on a 5-point Likert scale for each criterion or perform pairwise comparisons between two ideas from different models.We provide the prompts for evaluations in Appendix A.</p>
<p>Human Evaluation Similar to model-based evaluations, we perform human evaluations that involve assigning a score for each criterion and conducting pairwise comparisons between two ideas.As the  generated ideas are knowledge-intensive, we carefully select annotators who are well-versed in the field and provide them with ideas that are highly relevant to their field of expertise 9 .Specifically, we choose ten expert researchers who have authored at least three papers and ask them to judge only the ideas that are generated based on their own papers.</p>
<p>Implementation Details</p>
<p>We mainly use the GPT-4 (OpenAI, 2023) release from Nov 06, 2023, as the basis for all models, which is, notably, reported to be trained with data up to Apr 2023 (meanwhile, the papers used for idea generation appear after May 2023).To extract entities and build the entity-centric knowledge store, we use the off-the-shelf BLINK entity linker (Wu et al., 2020), with papers from May 01, 2023, to Dec 31, 2023 (available from Semantic Scholar API) along with their references, 9 We also experiment with human evaluation using nondomain-experts, but this proves to be suboptimal therefore, we focus on experts for reliable judgments of generated ideas.</p>
<p>Experimental Results and Analyses</p>
<p>Main Results Our main results on scoring with human and model-based evaluations are provided in Figure 2.These demonstrate that our full Re-searchAgent outperforms all baselines by large margins on every metric across problems, methods, and experiment designs (constituting the complete research ideas).Particularly, the full ResearchAgent augmented with relevant entities exhibits strong gains on metrics related to creativity (such as Originality for problems and Innovativeness for methods) since entities may offer novel concepts and views that may not be observable in the group of citation-based papers alone.In addition, the results of pairwise comparisons between models with both human and model-based evaluations -shown in Figure 3 -demonstrate that the full ResearchAgent shows the highest win ratio over its baselines.1.Specifically, for the scoring, we first rank scores from each annotator and measure Spearman's correlation coefficient (Pirie, 2006) between the ranked scores of two annotators.For the pairwise comparison between two judges, we measure Cohen's kappa coefficient (Cohen, 1960).Table 1 shows that the inter-annotator agreement is high, confirming the reliability of our assessments about the quality of generated research ideas.Also, while agreement scores for experimental designs are slightly lower than other aspects, this does not necessarily indicate a shortcoming in the quality of experimental designs produced by ResearchAgent, as demonstrated in Figures 2 and 3. Instead, we view this as the inherent subjectivity and variability in how such designs are perceived and evaluated by different annotators (i.e., the nature of the variability itself makes achieving high agreement challenging).</p>
<p>Analysis on Human-Model Agreements Similar to what we did for the aforementioned interannotator agreements, we measure agreements between human-based and model-based evaluations, to ensure the reliability of model-based evaluations.</p>
<p>As shown in Table 1, we further confirm that agreements between humans and models are high, indicating that model-based evaluations are a reasonable proxy to judge research idea generation.</p>
<p>Analysis of Refinement Steps</p>
<p>To see the effectiveness of iterative refinements of research ideas with ReviewingAgents, in Figure 4, we report the averaged scores on the generated ideas as a function of refinement steps.We first observe initial  improvements in the quality of generated ideas with increased refinement steps.Yet, the performance becomes saturated after three iterations, which may indicate diminishing returns for subsequent iteration steps, which aligns with the pattern observed in agent-based refinement work (Du et al., 2023).</p>
<p>Ablation on Knowledge Sources Recall that the full ResearchAgent is augmented with two different knowledge sources, namely relevant references and entities.To see their individual contribution, we perform an ablation study by either excluding one of the knowledge sources or replacing it with random elements.As shown in Table 2, each knowledge source contributes to performance improvement, and the relevant references are especially helpful.We also note that providing random elements is more helpful than providing no elements at all; we hypothesize that this may be due to the LLM's capability to filter out noise while still gaining incidental value from random inputs.</p>
<p>Analysis on Human Alignment for Evaluation</p>
<p>Recall that to align judgments from model-based evaluations with actual human preferences, we generated the evaluation criteria based on human evaluation results and used them as the criteria for model-based evaluations.Figure 5 demonstrates the efficacy of this strategy, presenting the score distribution of human evaluation compared with the distributions of model-based evaluations with and without human alignment.We find that the score distribution of model-based evaluations without alignment is skewed and different from the score distribution of human judgments.Meanwhile, after aligning the model-based evaluations with humaninduced score criteria, the calibrated distribution more closely resembles the distribution of humans.Correlation on Citation Counts We further investigate whether a high-impact paper (when used as a core paper) leads to high-quality research ideas.</p>
<p>To measure this, we categorize papers by their citation count (as a proxy for impact), and visualize the average score of each bucket (with model-based evaluations) in Figure 6.We find that ideas from high-impact papers tend to be of higher quality, likely due to their ability to identify research gaps, propose feasible methods, and connect with other works.Additionally, based on the paper distribution (See Figure 7) and for the ease of manual quality check, evaluation criteria for model-based evaluations are induced mainly with computer science papers.To see whether those criteria are applicable to diverse fields, we also compare a correlation between scores of computer science papers and all papers in Figure 6.From this, we observe that the scores increase when the citation increases for both domains, which may support the generalizability of human-preference-induced evaluation criteria.</p>
<p>Comparisons to Hypothesis Generation Recall that existing methods for hypothesis generation focus on predicting links between variables or generating hypotheses based on these links, which differs from our experimental setup of generating openended research ideas (problems, methods, and experiments).Nevertheless, to understand how the quality of the generated research ideas from prior works (Wang et al., 2023b;Yang et al., 2023) differs from our ResearchAgent, we perform comparisons.As shown in Table 3, we observe that ResearchAgent is capable of generating superior research hypotheses, due to the utilization of broad and deep knowledge across domains as well as the iterative review and refinement procedures.</p>
<p>Analysis using Different LLMs To assess how</p>
<p>ResearchAgent's performance changes with different LLMs, we conduct an auxiliary analysis with Llama3, Mixtral, Qwen1.5, and GPT-3.5 (Bai et al., 2023;Jiang et al., 2024), as shown in Table 4.These results show a significant performance drop with less capable models.Moreover, the performance differences between the Naive ResearchAgent without knowledge augmentation and the full ResearchAgent become marginal (for Mixtral and GPT-3.5),which indicates that they might struggle with capturing complex concepts between scientific papers.This can likely be attributed to the emergent abilities of LLMs for complex reasoning (but not in smaller LMs) (Wei et al., 2022), although other subtle issues may also be contributing factors.</p>
<p>Qualitative Analysis We provide qualitative results on generated research ideas in Table 16.One representative example in the last row highlights the advantage of entity-centric knowledge augmentation, where two entities (such as Drosophila Genetic Reference Panel and CRISPR) retrieved from the entity-centric knowledge store enable the generation of a novel research idea: bridging genetic variability and CRISPR applications.This exemplifies how external entity-based knowledge uncovers non-trivial relations between scientific concepts.</p>
<p>Conclusion</p>
<p>In this work, we introduced ResearchAgent, a system designed to assist researchers by generating research ideas, which encompass problem identification, method development, and experiment design.</p>
<p>Inspired by the human process of ideation, our approach conducts broad and deep literature reviews, integrates knowledge across domains to foster idea cross-pollination, and employs a community of reviewing agents to iteratively refine the generated ideas.Our evaluations, both human and modelbased, demonstrated that ResearchAgent produces ideas that are more creative, valid, and clear compared to baselines.While this initial foray shows promising results, multiple challenges remain to operationalize ResearchAgent in real-world research settings.Practical considerations include scaling the knowledge store to encompass diverse research domains, and keeping it current with the latest publications, through which the system can become adaptable even to emerging fields.</p>
<p>Limitations</p>
<p>ResearchAgent has some limitations that we hope to address in future work.</p>
<p>First, recall that we built the entity-centric knowledge store to propose beneficial entities during idea generation; however this store is constructed by extracting entities from the titles and abstracts of a limited number of publications (due to the costs of processing them) thereby precluding a large number of other entities and their interconnectedness.</p>
<p>In addition, the number of entities that we obtain from the BLINK entity linker (Wu et al., 2020) amounts to 3 per paper on average, indicating limited coverage (it is an open-domain linker after all), although it does exhibit generally strong understanding of scientific contexts, as demonstrated by the improvement achieved by the inclusion of its predictions (See Figures 2 and 3, and Table 16).</p>
<p>Furthermore, since our ResearchAgent is powered by LLMs, similar to any other approaches based on LLMs, it may hallucinate the generated research ideas.While our proposed ResearchAgent can partially mitigate this problem by augmenting LLMs with additional elements, such as references to the target paper and greater entity-centric knowledge, which help ground the generation process in more accurate and relevant information, validating these generated research ideas with experiments is essential to truly accelerate scientific research.</p>
<p>Moreover, while our iterative refinement process with ReviewingAgents demonstrates promising results, it has inherent limitations in scope.Although we employed diverse perspectives by utilizing 15 ReviewingAgents to evaluate three different ideas (problem, method, and experimental design) with five specific criteria for each, this approach may not fully capture the broad range of potential perspectives and criteria necessary for comprehensive evaluation across all different research domains.As acknowledged in the paper, our selection of criteria was informed by their presumed importance, but conducting an exhaustive exploration of all possible criteria over diverse domains is beyond the scope of this work (given the complexity of categorizing and balancing all relevant factors and perspectives).However, we believe the potential of our modular approach allows for customizing and aligning updated or even new criteria to any specific target domain with novel applications, and we leave further expanding them as future work.</p>
<p>Lastly, our ResearchAgent may be less suited for generating ideas in certain domains, such as theoretical sciences, where mathematical reasoning and proof generation play a central role.However, its flexibility allows for customization through specific instructions, enabling the integration of reasoningbased models and techniques to cater to the needs of theoretical research.For instance, in theoretical mathematics, we can instruct (reasoning-based) LLMs to focus on generating proofs or methods and omit experimental design steps that are less relevant.This as an exciting area for future work, where specialized techniques tailored to each domain could be included to broaden its applicability.</p>
<p>Ethics Statement</p>
<p>We are aware that the ResearchAgent may have the potential to be misused for nefarious purposes, such as generating research ideas about new explosives, malicious software, and invasive surveillance tools.</p>
<p>Notably, this vulnerability is not unique to our approach but a common challenge faced by existing LLMs that possess significant creative and reasoning capabilities, occasionally generating content that may be deemed undesirable.Consequently, it underscores the necessity to enhance the robustness and safety of LLMs more broadly.Also, we recognize the risks of unintentional plagiarism associated with using ResearchAgent, where the system might generate ideas that closely mirror existing research due to the regurgitation of training data.While mitigation strategies, such as integrating access to a comprehensive knowledge base to inform users of prior work, can be employed, we understand that building and maintaining such a resource is inherently complex and may not fully eliminate the risk.To further reduce the possibility of plagiarism, recording and tracking all generated ideas could help identify similarities and guide the model to avoid repetition, though this approach would necessitate explicit user consent.</p>
<p>A Additional Experimental Details</p>
<p>In this section, we provide additional details on experiments, including datasets, human evaluation setups, prompts (used for research idea generation and validation), and human-induced criteria.</p>
<p>A.1 Data Statistics</p>
<p>We visualize a distribution of core paper categories used for idea generation in Figure 7, where the categories are obtained from Semantic Scholar API 10 .From this, we find that the top 3 categories are computer science, medicine, and engineering.</p>
<p>A.2 Details on Human Evaluation</p>
<p>To conduct evaluations with human judges, we recruited 10 researchers from the United States and South Korea, majoring in computer science, medicine, and biology, each with a minimum of 3 published papers.For annotation, they were provided with a 6-page guideline document, which includes the task instruction and annotation examples.After reading this document, the annotators access the Label Studio platform, on which they first read the title and abstract of the target paper, and then review and evaluate the generated research ideas from different models.During the evaluation process, they are allowed to use any external tools, such as web searches.We note that they were compensated at a rate of $22.20 per hour.Also, on average, for one hour, they evaluated 3 sets of research ideas (that are generated from their own papers), with each set comprising three sub-ideas (the problem, method, and experiment design) from three different approaches (i.e., a total of 9 ideas for one hour).We perform three rounds of human evaluations with refinements in between, and, due to the cost associated with human annotations, we are able to fully evaluate a total of 150 ideas.</p>
<p>A.3 Prompts for Ideas Generation</p>
<p>We provide the prompts used to elicit the idea generations from our full ResearchAgent, specifically for instantiating problem identification, method development, and experiment design in Table 6, Table 7, and Table 8, respectively.</p>
<p>A.4 Prompts for Idea Validation</p>
<p>We provide the prompts used to elicit the idea validation from our ReviewingAgents as well as the model-based evaluations, specifically for instantiating problem validation, method validation, and experiment design validation in Table 9, Table 10, and Table 11, respectively.In addition, we provide the criteria used, which are induced by human judgments in the next subsection (Appendix A.5).</p>
<p>A.5 Criteria Induced by Human Judgements</p>
<p>Recall that, to align model-based evaluations with human preferences, we induce the criteria (used for automatic evaluations) with actual human judgments.We note that this is done by prompting GPT-4 with 10 pairs of generated ideas and (randomly selected) human judgments.We provide the resulting criteria for validations of problems, methods, and experiment designs in Table 13, Table 14, and Table 15, respectively.</p>
<p>B Additional Experimental Results</p>
<p>We provide additional experimental results, including comparisons without refinements and examples of the generated research ideas.</p>
<p>B.1 Results without Refinement Steps</p>
<p>To see whether the proposed ResearchAgent is consistently effective even without ReviewingAgents, we show the model-based evaluation results without any refinement steps in Figure 8. From this, we clearly observe that the full ResearchAgent outperforms its variants, demonstrating its effectiveness.</p>
<p>B.2 Results on Generated Ideas by Domain</p>
<p>To see the quality of the generated research ideas across different domains, we breakdown the performance of ResearchAgent according to the categories of core papers in Figure 7, and present the results in Figure 9. From this, we observe that the generated research ideas on the high-resource domains (such as Computer Science, Medicine, and Engineering where there is a greater volume of existing literature as shown in Figure 7) are superior to those generated from the low-resource domain papers (such as Physics, Chemistry, and Mathematics).This disparity might be attributed to the fact that the underlying LLMs used to generate research ideas are likely trained on data predominantly sourced from high-resource domains, which leads to enhancing their ability to comprehend scientific concepts and produce relevant research ideas.</p>
<p>B.3 Analysis with Different Entity Retrieval</p>
<p>To see the effectiveness of different entity retrieval strategies, we perform additional experiments, replacing the co-occurrence-based entity retrieval in Equation 2 to the contextual embedding-based retrieval.Notably, this contextual embedding-based retrieval approach uses the entities that have the highest similarity to the entities appearing in the current literature (i.e., core paper and its references) used for idea generation, where the similarities are calculated based on embedding-level similarities between entities over the latent space represented by the entity linker (Wu et al., 2020).Therefore, unlike the previous co-occurrence-based entity retrieval that may retrieve entities that have opposite concepts to the main idea of the current core paper (since we often mention limitations of previous work along with the proposed ideas), this embedding-based approach may provide the Re-searchAgent with mostly the entities having similar concepts to the core paper.Nevertheless, as shown in Table 5, the results with the strategy of entity cooccurrence-based retrieval are comparable to the results with the new embedding-based contextual retrieval.These results might confirm that there is not much difference in the quality of entity retrieval among those two strategies, i.e., most entities retrieved from the co-occurrence-based retrieval are contextually relevant for generating research ideas.</p>
<p>Table 6: The prompt used in the full instantiation of ResearchAgent for problem identification.</p>
<p>Types Texts</p>
<p>System Message</p>
<p>You are an AI assistant whose primary goal is to identify promising, new, and key scientific problems based on existing scientific literature, in order to aid researchers in discovering novel and significant research opportunities that can advance the field.</p>
<p>User Message</p>
<p>You are going to generate a research problem that should be original, clear, feasible, relevant, and significant to its field.This will be based on the title and abstract of the target paper, those of {len(references)} related papers in the existing literature, and {len(entities)} entities potentially connected to the research area.</p>
<p>Understanding of the target paper, related papers, and entities is essential:</p>
<p>-The target paper is the primary research study you aim to enhance or build upon through future research, serving as the central source and focus for identifying and developing the specific research problem.</p>
<p>-The related papers are studies that have cited the target paper, indicating their direct relevance and connection to the primary research topic you are focusing on, and providing additional context and insights that are essential for understanding and expanding upon the target paper.</p>
<p>-The entities can include topics, keywords, individuals, events, or any subjects with possible direct or indirect connections to the target paper or the related studies, serving as auxiliary sources of inspiration or information that may be instrumental in formulating the research problem.</p>
<p>Your approach should be systematic:</p>
<p>-Start by thoroughly reading the title and abstract of the target paper to understand its core focus.</p>
<p>-Next, proceed to read the titles and abstracts of the related papers to gain a broader perspective and insights relevant to the primary research topic.</p>
<p>-Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of inspiration and information, while keeping in mind that not all may be relevant.</p>
<p>I am going to provide the target paper, related papers, and entities, as follows: Rationale:</p>
<p>Table 7: The prompt used in the full instantiation of ResearchAgent for method development.</p>
<p>Types Texts</p>
<p>System Message</p>
<p>You are an AI assistant whose primary goal is to propose innovative, rigorous, and valid methodologies to solve newly identified scientific problems derived from existing scientific literature, in order to empower researchers to pioneer groundbreaking solutions that catalyze breakthroughs in their fields.</p>
<p>User Message</p>
<p>You are going to propose a scientific method to address a specific research problem.Your method should be clear, innovative, rigorous, valid, and generalizable.This will be based on a deep understanding of the research problem, its rationale, existing studies, and various entities.</p>
<p>Understanding of the research problem, existing studies, and entities is essential:</p>
<p>-The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities, which should be the cornerstone of your method development.</p>
<p>-The existing studies refer to the target paper that has been pivotal in identifying the problem, as well as the related papers that have been additionally referenced in the problem discovery phase, all serving as foundational material for developing the method.</p>
<p>-The entities can include topics, keywords, individuals, events, or any subjects with possible direct or indirect connections to the existing studies, serving as auxiliary sources of inspiration or information that may be instrumental in method development.</p>
<p>Your approach should be systematic:</p>
<p>-Start by thoroughly reading the research problem and its rationale, to understand your primary focus.</p>
<p>-Next, proceed to review the titles and abstracts of existing studies, to gain a broader perspective and insights relevant to the primary research topic.</p>
<p>-Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of inspiration and information, while keeping in mind that not all may be relevant.Then, following your review of the above content, please proceed to propose your method with its rationale, in the format of Method: Rationale:
I
Table 8: The prompt used in the full instantiation of ResearchAgent for experiment design.</p>
<p>Types Texts</p>
<p>System Message</p>
<p>You are an AI assistant whose primary goal is to design robust, feasible, and impactful experiments based on identified scientific problems and proposed methodologies from existing scientific literature, in order to enable researchers to systematically test hypotheses and validate groundbreaking discoveries that can transform their respective fields.</p>
<p>User Message</p>
<p>You are going to design an experiment, aimed at validating a proposed method to address a specific research problem.Your experiment design should be clear, robust, reproducible, valid, and feasible.This will be based on a deep understanding of the research problem, scientific method, existing studies, and various entities.</p>
<p>Understanding of the research problem, scientific method, existing studies, and entities is essential:</p>
<p>-The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities.</p>
<p>-The scientific method has been proposed to tackle the research problem, which has been informed by insights gained from existing studies and relevant entities.</p>
<p>-The existing studies refer to the target paper that has been pivotal in identifying the problem and method, as well as the related papers that have been additionally referenced in the discovery phase of the problem and method, all serving as foundational material for designing the experiment.</p>
<p>-The entities can include topics, keywords, individuals, events, or any subjects with possible direct or indirect connections to the existing studies, serving as auxiliary sources of inspiration or information that may be instrumental in your experiment design.</p>
<p>Your approach should be systematic:</p>
<p>-Start by thoroughly reading the research problem and its rationale followed by the proposed method and its rationale, to pinpoint your primary focus.</p>
<p>-Next, proceed to review the titles and abstracts of existing studies, to gain a broader perspective and insights relevant to the primary research topic.</p>
<p>-Finally, explore the entities to further broaden your perspective, drawing upon a diverse pool of inspiration and information, while keeping in mind that not all may be relevant.With the provided research problem, scientific method, existing studies, and entities, your objective now is to design an experiment that not only leverages these resources but also strives to be clear, robust, reproducible, valid, and feasible.Before crafting the experiment design, revisit the research problem and proposed method, to ensure they remain at the center of your experiment design process.
I
Research problem: {researchProblem} Rationale: {researchProblemRationale} Scientific method: {scientificMethod} Rationale: {scientificMethodRationale} Then, following your review of the above content, please proceed to outline your experiment with its rationale, in the format of Experiment: Rationale:</p>
<p>Table 9: The prompt used in the full instantiation of ReviewingAgent for problem validation.</p>
<p>Types Texts System Message</p>
<p>You are an AI assistant whose primary goal is to assess the quality and validity of scientific problems across diverse dimensions, in order to aid researchers in refining their problems based on your evaluations and feedback, thereby enhancing the impact and reach of their work.</p>
<p>User Message</p>
<p>You are going to evaluate a research problem for its {metric}, focusing on how well it is defined in a clear, precise, and understandable manner.</p>
<p>As part of your evaluation, you can refer to the existing studies that may be related to the problem, which will help in understanding the context of the problem for a more comprehensive assessment.</p>
<p>-The existing studies refer to the target paper that has been pivotal in identifying the problem, as well as the related papers that have been additionally referenced in the discovery phase of the problem.</p>
<p>The Now, proceed with your {metric} evaluation approach that should be systematic:</p>
<p>-Start by thoroughly reading the research problem and its rationale, keeping in mind the context provided by the existing studies mentioned above.</p>
<p>-Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the {metric} of the problem.</p>
<p>-Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified: {criteria}</p>
<p>Table 10: The prompt used in the full instantiation of ReviewingAgent for method validation.</p>
<p>Types Texts</p>
<p>System Message</p>
<p>You are an AI assistant whose primary goal is to assess the quality and soundness of scientific methods across diverse dimensions, in order to aid researchers in refining their methods based on your evaluations and feedback, thereby enhancing the impact and reach of their work.</p>
<p>User Message</p>
<p>You are going to evaluate a scientific method for its {metric} in addressing a research problem, focusing on how well it is described in a clear, precise, and understandable manner that allows for replication and comprehension of the approach.</p>
<p>As part of your evaluation, you can refer to the research problem, and existing studies, which will help in understanding the context of the proposed method for a more comprehensive assessment.</p>
<p>-The research problem has been used as the cornerstone of the method development, formulated based on an in-depth review of existing studies and a potential exploration of relevant entities.</p>
<p>-The existing studies refer to the target paper that has been pivotal in identifying the problem and method, as well as the related papers that have been additionally referenced in the discovery phase of the problem and method.Now, proceed with your {metric} evaluation approach that should be systematic:</p>
<p>-Start by thoroughly reading the proposed method and its rationale, keeping in mind the context provided by the research problem, and existing studies mentioned above.</p>
<p>-Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the {metric} of the method.</p>
<p>-Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified: {criteria} I am going to provide the proposed method with its rationale, as follows: Scientific method: {scientificMethod} Rationale: {scientificMethodRationale} After your evaluation of the above content, please provide your review, feedback, and rating, in the format of Review: Feedback: Rating (1-5):</p>
<p>Table 11: The prompt used in the full instantiation of ReviewingAgent for experiment design validation.</p>
<p>Types Texts</p>
<p>System Message</p>
<p>You are an AI assistant whose primary goal is to meticulously evaluate the experimental designs of scientific papers across diverse dimensions, in order to aid researchers in refining their experimental approaches based on your evaluations and feedback, thereby amplifying the quality and impact of their scientific contributions.</p>
<p>User Message</p>
<p>You are going to evaluate an experiment design for its {metric} in validating a scientific method to address a research problem, focusing on how well it is described in a clear, precise, and understandable manner, enabling others to grasp the setup, procedure, and expected outcomes.</p>
<p>As part of your evaluation, you can refer to the research problem, scientific method, and existing studies, which will help in understanding the context of the designed experiment for a more comprehensive assessment.</p>
<p>-The research problem has been formulated based on an in-depth review of existing studies and a potential exploration of relevant entities.</p>
<p>-The scientific method has been proposed to tackle the research problem, which has been informed by insights gained from existing studies and relevant entities.</p>
<p>-The existing studies refer to the target paper that has been pivotal in identifying the problem, method, and experiment, as well as the related papers that have been additionally referenced in their discovery phases.Now, proceed with your {metric} evaluation approach that should be systematic: -Start by thoroughly reading the experiment design and its rationale, keeping in mind the context provided by the research problem, scientific method, and existing studies mentioned above.</p>
<p>-Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the {metric} of the experiment.</p>
<p>-Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified: {criteria} I am going to provide the designed experiment with its rationale, as follows: Experiment design: {experimentDesign} Rationale: {experimentDesignRationale} After your evaluation of the above content, please provide your review, feedback, and rating, in the format of Review: Feedback: Rating (1-5):</p>
<p>Table 12: The criteria used for evaluating research ideas: problems, methods, and experiment designs.</p>
<p>Types Criteria Texts</p>
<p>Problem Clarity It assesses whether the problem is defined in a clear, precise, and understandable manner.</p>
<p>Relevance</p>
<p>It measures whether the problem is pertinent and applicable to the current field or context of study.</p>
<p>Originality</p>
<p>It evaluates whether the problem presents a novel challenge or unique perspective that has not been extensively explored before.</p>
<p>Feasibility</p>
<p>It examines whether the problem can realistically be investigated or solved with the available resources and within reasonable constraints.</p>
<p>Significance</p>
<p>It assesses the importance and potential impact of solving the problem, including its contribution to the field or its broader implications.</p>
<p>Method Clarity</p>
<p>It assesses whether the method is described in a clear, precise, and understandable manner that allows for replication and comprehension of the approach.</p>
<p>Validity</p>
<p>It measures the accuracy, relevance, and soundness of the method in addressing the research problem, ensuring that it is appropriate and directly relevant to the objectives of the study.</p>
<p>Rigorousness</p>
<p>It examines the thoroughness, precision, and consistency of the method, ensuring that the approach is systematic, well-structured, and adheres to high standards of research quality.</p>
<p>Innovativeness</p>
<p>It evaluates whether the method introduces new techniques, approaches, or perspectives to the research field that differ from standard research practices and advance them in the field.</p>
<p>Generalizability</p>
<p>It assesses the extent to which the method can be applied to or is relevant for other contexts, populations, or settings beyond the scope of the study.</p>
<p>Experiment Clarity</p>
<p>It determines whether the experiment design is described in a clear, precise, and understandable manner, enabling others to grasp the setup, procedure, and expected outcomes.</p>
<p>Validity</p>
<p>It measures the appropriateness and soundness of the experimental design in accurately addressing the research questions or effectively validating the proposed methods, ensuring that the design effectively tests what it is intended to examine.</p>
<p>Robustness</p>
<p>It evaluates the durability of the experimental design across a wide range of conditions and variables, ensuring that the outcomes are not reliant on a few specific cases and remain consistent across a broad spectrum of scenarios.</p>
<p>Feasibility</p>
<p>It evaluates whether the experiment design can realistically be implemented with the available resources, time, and technological or methodological constraints, ensuring that the experiment is practical and achievable.</p>
<p>Reproducibility</p>
<p>It examines whether the information provided is sufficient and detailed enough for other researchers to reproduce the experiment using the same methodology and conditions, ensuring the reliability of the findings.2. The problem has limited significance, with a narrow scope of impact and minor contributions to the field, offering little to no practical implications.</p>
<ol>
<li>The problem demonstrates average significance, with some contributions to the field and potential practical implications, but lacks innovation or broader impact.4. The problem is significant, offering notable contributions to the field and valuable practical implications, with evidence of potential for broader impact and advancement.5.The problem presents exceptional significance, with groundbreaking contributions to the field, broad and transformative potential impacts, and substantial practical applications across diverse domains.</li>
</ol>
<p>Table 14: The criteria induced from human judgments for validating the developed methods, which used to align model-based evaluations with actual human preferences.</p>
<p>Types Criteria Texts</p>
<p>Method Clarity 1.The method is explained in an extremely vague or ambiguous manner, making it impossible to understand or replicate the approach without additional information or clarification.</p>
<ol>
<li>
<p>The method is described with some detail, but significant gaps in explanation or logic leave the reader with considerable confusion and uncertainty about how to apply or replicate the approach.</p>
</li>
<li>
<p>The method is described with sufficient detail to understand the basic approach, but lacks the precision or specificity needed to fully replicate or grasp the nuances of the methodology without further guidance.4. The method is clearly and precisely described, with most details provided to allow for replication and comprehension, though minor areas may benefit from further clarification or elaboration.5.The method is articulated in an exceptionally clear, precise, and detailed manner, enabling straightforward replication and thorough understanding of the approach with no ambiguities.</p>
</li>
</ol>
<p>Validity 1.The method shows a fundamental misunderstanding of the research problem and lacks any credible alignment with established scientific principles or relevant studies.</p>
<ol>
<li>
<p>The method partially addresses the research problem but exhibits significant flaws in its scientific underpinning, making its validity questionable despite some alignment with existing literature.</p>
</li>
<li>
<p>The method adequately addresses the research problem but with some limitations in its scientific validity, showing a mix of strengths and weaknesses in its alignment with related studies.4. The method effectively addresses the research problem, demonstrating a strong scientific basis and sound alignment with existing literature, albeit with minor areas for improvement.5.The method exemplifies an exceptional understanding of the research problem, grounded in a robust scientific foundation, and shows exemplary integration and advancement of existing studies' findings.</p>
</li>
</ol>
<p>Rigorousness</p>
<p>1.The method demonstrates a fundamental lack of systematic approach, with significant inconsistencies and inaccuracies in addressing the research problem, showing a disregard for established research standards.</p>
<ol>
<li>
<p>The method shows a minimal level of systematic effort but is marred by notable inaccuracies, lack of precision, and inconsistencies that undermine the rigorousness of the method in tackling the research problem.</p>
</li>
<li>
<p>The method exhibits an average level of systematic structure and adherence to research standards but lacks the thoroughness, precision, and consistency required for a rigorous scientific inquiry.4. The method is well-structured and systematic, with a good level of precision and consistency, indicating a strong adherence to research standards, though it falls short of exemplifying the highest level of rigorousness.5.The method exemplifies exceptional rigorousness, with outstanding thoroughness, precision, and consistency in its systematic approach, setting a benchmark for high standards in scientific research quality.</p>
</li>
</ol>
<p>Innovativeness 1.The method introduces no novel elements, fully relying on existing techniques without any attempt to modify or adapt them for the specific research problem, showing a lack of innovativeness.</p>
<ol>
<li>
<p>The method shows minimal innovation, with only slight modifications to existing techniques that do not substantially change or improve the approach to the research problem.</p>
</li>
<li>
<p>The method demonstrates moderate innovativeness, incorporating known techniques with some new elements or combinations that offer a somewhat fresh approach to the research problem but fall short of a significant breakthrough.4. The method is highly innovative, introducing new techniques or novel combinations of existing methods that significantly differ from standard practices, offering a new perspective or solution to the research problem. 5.The method represents a groundbreaking innovation, fundamentally transforming the approach to the research problem with novel techniques or methodologies that redefine the field's standard practices.</p>
</li>
</ol>
<p>Generalizability</p>
<p>1.The method shows no adaptability, failing to extend its applicability beyond its original context or dataset, showing a complete lack of generalizability.</p>
<ol>
<li>
<p>The method demonstrates minimal adaptability, with limited evidence of potential applicability to contexts slightly different from the original.</p>
</li>
<li>
<p>The method exhibits some level of adaptability, suggesting it could be applicable to related contexts or datasets with modifications.4. The method is adaptable and shows evidence of applicability to a variety of contexts or datasets beyond the original.5.The method is highly adaptable, demonstrating clear evidence of broad applicability across diverse contexts, populations, and settings.4. The experiment design is well-aligned with the research problem and scientific method, providing strong evidence of validity and effectively addressing the research questions and testing the proposed methods, despite minor limitations.5.The experiment design excellently aligns with the research problem and scientific method, demonstrating robust evidence of validity and outstandingly addressing the research questions and testing the proposed methods without significant limitations.</p>
</li>
</ol>
<p>Robustness</p>
<p>1.The experiment design demonstrates a fundamental lack of understanding of the scientific method, with no evidence of durability or adaptability across varying conditions, leading to highly unreliable and non-replicable results.</p>
<ol>
<li>
<p>The experiment design shows minimal consideration for robustness, with significant oversights in addressing variability and ensuring consistency across different scenarios, resulting in largely unreliable outcomes.</p>
</li>
<li>
<p>The experiment design adequately addresses some aspects of robustness but lacks comprehensive measures to ensure durability and consistency across a wide range of conditions, leading to moderate reliability.4. The experiment design incorporates a solid understanding of robustness, with clear efforts to ensure the experiment's durability and consistency across diverse conditions, though minor improvements are still possible for optimal reliability.5.The experiment design exemplifies an exceptional commitment to robustness, with meticulous attention to durability and adaptability across all possible conditions, ensuring highly reliable and universally applicable results.</p>
</li>
</ol>
<p>Feasibility 1.The experiment design is fundamentally unfeasible, with insurmountable resource, time, or technological constraints that make implementation virtually impossible within the proposed framework.</p>
<ol>
<li>
<p>The experiment design faces significant feasibility challenges, including major resource, time, or technological limitations, that heavily compromise its practical execution and likelihood of success.</p>
</li>
<li>
<p>The experiment design is somewhat feasible, with moderate constraints on resources, time, or technology that could be addressed with adjustments, though these may not guarantee success.4. The experiment design is largely feasible, with minor resource, time, or technological limitations that can be effectively managed or mitigated, ensuring a high probability of successful implementation.5.The experiment design is highly feasible, with no significant constraints on resources, time, or technology, indicating that it can be implemented smoothly and successfully within the proposed framework.</p>
</li>
</ol>
<p>Reproducibility</p>
<p>1.The experiment design lacks critical details, making it virtually impossible for other researchers to replicate the study under the same conditions or methodologies.</p>
<ol>
<li>The experiment provides some essential information but omits significant details needed for replication, leading to considerable ambiguity in methodology or conditions.3. The experiment design includes sufficient details for replication, but lacks clarity or completeness in certain areas, posing challenges for seamless reproducibility.4. The experiment is well-documented with clear, detailed instructions and methodologies that allow for consistent replication, albeit with minor areas for improvement.5.The experiment design is exemplary in its clarity, detail, and comprehensiveness, ensuring that other researchers can precisely and effortlessly replicate the study under identical conditions and methodologies.Abstract: Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training.However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers.Furthermore, fine-tuning LLMs to update their knowledge is expensive.To this end, we propose to augment the knowledge directly in the input of LLMs.Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts.After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer.Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot.We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.</li>
</ol>
<p>Entities: ['Natural language', 'Learning management system', 'Question answering', 'Wikipedia', 'Artificial intelligence', 'Namedentity recognition', 'Quality assurance', 'Long short-term memory', 'English language', 'Google Books', 'Natural-language programming', 'LMS color space', 'GitHub', 'Logic learning machine', 'Machine translation', 'DBpedia', 'Integrated library system', 'Spanish language', 'Neural machine translation', 'ArXiv', 'Artificial general intelligence', 'Natural-language understanding', 'Nonlinear programming</p>
<p>Method</p>
<p>The further refined and enhanced method for developing a multimodal knowledge-aware prompting system for multilingual zero-shot question answering (MMKP-ZSQA) incorporates advanced refinements in the following components to address the requirements for innovativeness, clarity, and generalizability:</p>
<ol>
<li>Interconnected Multimodal Knowledge Orchestration (IMKO):</li>
</ol>
<p>-IMKO is an advanced evolution of SMKI, incorporating state-of-the-art language and vision models to handle data verbalization with context-aware disambiguation methods.</p>
<p>-Techniques like attention mechanisms are employed to weigh the relevance of different data pieces when integrated, revealing how textual and visual data augment each other in a synthesized representation.</p>
<ol>
<li>Linguistically Inclusive Retrieval Engine (LIRE):</li>
</ol>
<p>-LIRE extends EMKA with an emphasis on semantic understanding, using transformer-based models trained on diverse datasets, including idiomatic and cultural nuances across languages.</p>
<p>-Specific algorithms to handle linguistic phenomena such as code-switching and transliteration are included, enhancing the application to a broader set of languages and dialects.</p>
<p>Prompt Learning and Optimization Nexus (PLON):</p>
<p>-Building on AMPL, PLON focuses on creating a library of optimized prompts categorized by linguistic features and data modalities, using Bayesian optimization algorithms.</p>
<p>-It includes domain adaptation techniques and a wider array of meta-learning strategies with case studies for high and low-resource languages, and outlines theoretical frameworks for their implementation.</p>
<p>Cross-Modal Integrative Learning System (C-MILS):</p>
<p>-C-MILS advances SCAS by detailing the use of multi-head attention across modalities for effective and scalable reasoning, with exemplifications on how each modality can enhance comprehension synergistically.</p>
<p>-The component now incorporates a layer of abstraction to distill knowledge into a modality-agnostic format, aiding reasoning and facilitating interpretability across languages and data types.</p>
<ol>
<li>User-Centric Adaptation and Privacy Framework (UCAPF):</li>
</ol>
<p>-UCAPF enriches ALCUM with a user-centric design, focusing on interaction protocols that describe user engagement cycles, feedback loops, and privacy-preserving active learning paradigms, with practical workflows and pseudocode.</p>
<p>-Aligns with GDPR and other privacy frameworks to fortify trust and address the ethical use of data in dynamic learning environments.</p>
<p>Global Evaluation and Reflective Testing System (GERTS):</p>
<p>-GERTS expands CEB by presenting a multi-tiered validation and reflection methodology to adjust system components based on a diverse set of metrics, including fairness, bias, interpretability, and computational efficiency.</p>
<p>-Offers a structured approach to cross-cultural evaluation, including the use of demographic and regional diversity in forming test cohorts.</p>
<p>Formally, p = LLM(T p (L)) indicates the problem identification step, followed by m = LLM(T m (p, L)) for method development and d = LLM(T e (p, m, L)) for experiment design, which constitutes the full idea: o = [p, m, d].</p>
<p>{l 0 ,...,ln } P (e j |e i )) × P (e i ), (2)</p>
<p>Figure 2 :
2
Figure 2: Main results on our research idea generation task with human-(top) and model-based (bottom) evaluations, where we report the score of each idea (problem, method, or experiment design) based on its own five criteria and their average score.</p>
<p>Figure 3 :
3
Figure 3: Results of pairwise comparisons between ideas from two of any different approaches, where we report the win ratio.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Distributions of model-based evaluation results with and without the human-induced score criteria alignment (middle and right), as well as human evaluation results (left).</p>
<p>Figure 7 :
7
Figure 7: Visualization of the distribution of disciplines for all core papers, selected for research idea generation.</p>
<p>Figure 8 :
8
Figure 8: Results on our research idea generation task with model-based evaluation, where we exclude refinement steps.</p>
<p>Figure 9 :
9
Figure 9: Breakdown results of the research ideas generated from our full ResearchAgent across different domains.</p>
<p>existing studies (target paper &amp; related papers) are as follows: Target paper title: {paper['title']} Target paper abstract: {paper['abstract']} Related paper titles: {relatedPaper['titles']} Related paper abstracts: {relatedPaper['abstracts']}</p>
<p>The research problem and existing studies (target paper &amp; related papers) are as follows: Research problem: {researchProblem} Rationale: {researchProblemRationale} Target paper title: {paper['title']} Target paper abstract: {paper['abstract']} Related paper titles: {relatedPaper['titles']} Related paper abstracts: {relatedPaper['abstracts']}</p>
<p>Table 1 :
1
Results of agreements between two human annotation results and between human and model evaluation results.
CategoriesMetrics Problem Method ExperimentHuman and HumanScoring Pairwise0.83 0.620.76 0.620.67 0.41Human and ModelScoring Pairwise0.64 0.710.58 0.620.49 0.52
which number 50,091 in total.We provide detailed prompts used to elicit responses for research idea generation in Appendix A.3.</p>
<p>Table 2 :
2
Results with varying the number of refinement steps.Results of ablation study on references and entities.
5.00Problem5.00Method4.50ExperimentScores3.50 3.75 4.00 4.25 4.50 4.7501 Refinement Steps 2 3 Average Clarity Relevance Originality Feasibility Significance 44.25 4.50 4.75 3.50 3.75 4.0001 Refinement Steps 2 3 Average Clarity Validity Rigorousness Innovativeness Generalizability 44.00 4.25 3.50 3.7501 Refinement Steps 2 3 Average Clarity Validity Robustness Feasibility Reproducibility 4Figure 4: MethodsProblem Method ExperimentResearchAgent4.524.284.18-w/o Entities4.354.134.02-w/ Random Entities4.414.194.13-w/o References4.264.083.97-w/ Random References4.354.164.02-w/o Entities &amp; References4.204.033.92Analysis on Inter-Annotator Agreements Tovalidate the quality and reliability of human anno-tations, we measure the inter-annotator agreements,where 20% of the generated ideas are evaluated bytwo human judges, and report results in Table</p>
<p>Table 3 :
3
(Wang et al., 2023b;Yang et al., 2023)thesis generation methods(Wang et al., 2023b;Yang et al., 2023).
MethodsClarity Relevance Originality Feasibility SignificanceSciMON4.044.374.563.984.15Hypothesis Proposer 3.974.144.074.014.11ResearchAgent4.114.884.774.054.81</p>
<p>Table 4 :
4
Results with different, open and proprietary LLMs.
LLMsModelsProblem Method ExperimentGPT-4.0Naive ResearchAgent ResearchAgent (Ours)4.20 4.524.03 4.283.92 4.18GPT-3.5Naive ResearchAgent ResearchAgent (Ours)3.56 3.583.56 3.583.63 3.60Llama3 (8B)Naive ResearchAgent ResearchAgent (Ours)3.76 4.183.69 4.033.54 3.95Mixtral (8x7B)Naive ResearchAgent ResearchAgent (Ours)3.31 3.283.27 3.353.20 3.31Qwen1.5 (32B)Naive ResearchAgent ResearchAgent (Ours)3.64 4.023.74 3.973.66 3.94</p>
<p>Table 5 :
5
Results with different entity retrieval strategies.
MethodsProblem Method ExperimentResearchAgent-w/ Co-occurrence-based Retrieval4.524.284.18-w/ Embedding-based Retrieval4.494.344.16-w/o Entity Retrieval4.354.134.02</p>
<p>With the provided target paper, related papers, and entities, your objective now is to formulate a research problem that not only builds upon these existing studies but also strives to be original, clear, feasible, relevant, and significant.Before crafting the research problem, revisit the title and abstract of the target paper, to ensure it remains the focal point of your research problem identification process.
Target paper title: {paper['title']}Target paper abstract: {paper['abstract']}Related paper titles: {relatedPaper['titles']}Related paper abstracts: {relatedPaper['abstracts']}Entities: {Entities}Target paper title: {paper['title']}Target paper abstract: {paper['abstract']}Then, following your review of the above content, please proceed to generate one researchproblem with the rationale, in the format ofProblem:</p>
<p>With the provided research problem, existing studies, and entities, your objective now is to formulate a method that not only leverages these resources but also strives to be clear, innovative, rigorous, valid, and generalizable.Before crafting the method, revisit the research problem, to ensure it remains the focal point of your method development process.
am going to provide the research problem, existing studies (target paper &amp; related papers), andentities, as follows:Research problem: {researchProblem}Rationale: {researchProblemRationale}Target paper title: {paper['title']}Target paper abstract: {paper['abstract']}Related paper titles: {relatedPaper['titles']}Related paper abstracts: {relatedPaper['abstracts']}Entities: {Entities}Research problem: {researchProblem}Rationale: {researchProblemRationale}</p>
<p>Table 13 :
13
The criteria induced from human judgments for validating the identified problems, which are used to align model-based evaluations with actual human preferences.The problem is somewhat defined but suffers from vague terms and insufficient detail, making it challenging to grasp the full scope or objective.3.The problem is stated in a straightforward manner, but lacks the depth or specificity needed to fully convey the nuances and boundaries of the research scope.4.The problem is clearly articulated with precise terminology and sufficient detail, providing a solid understanding of the scope and objectives with minimal ambiguity.5.The problem is exceptionally clear, concise, and specific, with every term and aspect well-defined, leaving no room for misinterpretation and fully encapsulating the research scope and aims.The problem shows almost no relevance to the current field, failing to connect with the established context or build upon existing work.2.The problem has minimal relevance, with only superficial connections to the field and a lack of meaningful integration with prior studies.3.The problem is somewhat relevant, making a moderate attempt to align with the field but lacking significant innovation or depth.4. The problem is relevant and well-connected to the field, demonstrating a good understanding of existing work and offering promising contributions. 5.The problem is highly relevant, deeply integrated with the current context, and represents a significant advancement the field.
TypesCriteriaTexts</p>
<p>Table 15 :
15
The criteria induced from human judgments for validating the experiment designs, which are used to align model-based evaluations with actual human preferences.The experiment design is extremely unclear, with critical details missing or ambiguous, making it nearly impossible for others to understand the setup, procedure, or expected outcomes.2.The experiment design lacks significant clarity, with many important aspects poorly explained or omitted, challenging others to grasp the essential elements of the setup, procedure, or expected outcomes.3.The experiment design is moderately clear, but some aspects are not detailed enough, leaving room for interpretation or confusion about the setup, procedure, or expected outcomes.4.The experiment design is mostly clear, with most aspects well-described, allowing others to understand the setup, procedure, and expected outcomes with minimal ambiguity.5.The experiment design is exceptionally clear, precise, and detailed, enabling easy understanding of the setup, procedure, and expected outcomes, with no ambiguity or need for further clarification.Validity 1.The experiment design demonstrates a fundamental misunderstanding of the research problem, lacks alignment with scientific methods, and shows no evidence of validity in addressing the research questions or testing the proposed methods.2.The experiment design has significant flaws in its approach to the research problem and scientific method, with minimal or questionable evidence of validity, making it largely ineffective in addressing the research questions or testing the proposed methods.3.The experiment design is generally aligned with the research problem and scientific method but has some limitations in its validity, offering moderate evidence that it can somewhat effectively address the research questions or test the proposed methods.
TypesCriteriaTexts</p>
<p>Table 16 :
16
The examples of research idea generation results from the proposed full ResearchAgent.Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering
IndexTypesTextsTitle:Input1</p>
<p>Table 16 -
16
', 'Codex', 'LoRa', 'Adhesive', 'Common Crawl', 'ROUGE (metric)', 'Transformer', 'Tomas Mikolov'] Problem Developing a Multimodal Knowledge-Aware Prompting System for Multilingual Zero-Shot Question Answering across Structured and Unstructured Data Sources Continued on the next page Continued from the previous page Index Types Texts</p>
<p>We focus on the existing literature-based idea generation by following the paradigm that a new idea is more often than not just a new combination of old elements(Young, 2003).
As extracting entities on all articles is computationally infeasible, we target papers appearing after May 01, 2023.
Due to the extensive length of scientific publications, the target of entity extraction is restricted to titles and abstracts.
There may be additional knowledge sources (beyond the existing literature and entities) for research idea generation, and we leave exploring them as future work.
We select the top five criteria which we consider as the most important, and leave exploring others as future work.
We additionally ask five human annotators, who evaluate research ideas, to judge the quality of the induced criteria; two of them agree strongly, while the other three agree moderately.
https://www.semanticscholar.org/product/api
AcknowledgementsThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No.RS-2023-00256259), the grant of the Korea Machine Learning Ledger Orchestration for Drug Discovery Project (K-MELLODDY) funded by the Ministry of Health &amp; Welfare and Ministry of Science and ICT, Republic of Korea (grant number: RS-2024-12345678), the Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) &amp; Gwangju Metropolitan City, and the Institute for Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2019-II190075, Artificial Intelligence Graduate School Program (KAIST)).ExperimentThe experiment, named "Refined Experiment for Multimodal Knowledge-Aware Prompting System for Multilingual Zero-shot Question Answering (RE-MKP-ZSQA)", aims to methodically develop and validate an advanced AI system.The experiment is streamlined to address feasibility, clarity, and reproducibility concerns while upholding robustness and validity by adhering to the following refined phases:1. Detailed System Implementation Plan: -Provide a publicly accessible project roadmap with specific milestones, resource allocation, and timelines.Dataset Curation with Clear Guidelines:-Publish precise annotation guidelines with strategies to prevent bias.-Document the dataset assembly process, including source selection and data processing procedures.Transparent System Training:-Offer a detailed training protocol with hyperparameters, optimization strategies, and Bayesian optimization processes used in PLON.Structured Zero-Shot Evaluation:-Outline evaluation metrics derived from GERTS with benchmark datasets to test zero-shot capabilities.Clearer Interdisciplinary Evaluation Protocol:-Specify the composition of the evaluation committee, criteria for assessments, and methods for integrating the feedback.Iterative Improvement with Validation Metrics:-Describe statistical methods for reflective assessment and continuous improvement, aligned with multi-tiered GERTS methodology.User-Centric Design and Privacy Compliance Evaluation:-Structure user studies with targeted data points to assess usability and cultural adaptability.-Outline privacy compliance protocols to adhere to international standards.Detailed Global Scalability Evaluation Method:-Define evaluation metrics for scalability tests and describe diverse infrastructural setups.9. Enhanced Reporting for Reproducibility: -Commit to creating a comprehensive report with precise specifications, configurations, and instructions for replication purposes.-Utilize GitHub for version-controlled deposition of code and datasets, and arXiv for openly accessible experiment protocols and findings.Abstract: Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA).However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks.Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data.In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data.In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels.Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable.Continued on the next pageEntities: ['Codex', 'Natural language', 'English language', 'United States', 'Question answering', 'Natural-language programming', 'GTRI Information and Communications Laboratory', 'Artificial intelligence', 'LoRa', 'Llama', 'Python (programming language)', 'Learning management system', 'Natural language processing', 'Reinforcement learning', 'LMS color space', 'Wikipedia', 'GitHub', 'Natural-language understanding', 'London, Midland and Scottish Railway', 'Integrated library system', 'Language model', 'Chinese language', 'Lumen (unit)', 'Spanish language', 'English Wikipedia', 'Logic learning machine', 'Gradient descent', 'Alternative public offering', 'Technology transfer', ' Evaluate domain adaptability for each language via an iterative process, ensuring models learn to prioritize resource efficiency.6. Model Robustness and Generalization: Perform robustness tuning (RT) to prepare models for unforeseen linguistic variations and conduct thorough evaluations across multiple domains to ensure models can generalize their learning effectively.7. Human-In-The-Loop Evaluation: Conduct evaluations with native speakers and domain experts to validate the relevance and accuracy of the QA outputs, incorporating feedback into the iterative training process.8. Open-Sourcing and Community Collaboration: Make the TTT protocol, trained models, and evaluation benchmarks publicly available for the research community, fostering collaboration and further innovation.-Identify potential compact language models (CLMs) suitable for domain adaptation and test-time training, focusing on those with minimal computational requirement and the ability to be fine-tuned or adapted in an unsupervised manner.Continued on the next page-Prepare a diverse set of low-resource languages and corresponding text corpora, ensuring linguistic diversity and sociocultural significance.Select benchmark datasets for these languages if available.Training and Adaptation Procedure:-Create a Test-Time Training (TTT) framework that allows selected CLMs to adapt to various domains in the selected low-resource languages during the inference phase.-Implement unsupervised learning techniques and pseudo-label generation to produce QA pairs, utilizing back-translation and context-based question synthesis to generate synthetic datasets for languages with limited or no available QA datasets.-Integrate components and meta-learning algorithms into the CLMs to enable domain-specific adaptations at test time.Iterative Evaluation and Refinement:-Begin adaptation and training with a single low-resource language and gradually add additional languages, monitoring the domain adaptability and model performance metrics after each addition.-Perform robustness tuning and cross-domain evaluations for each CLM and language adaptation to ensure generalizability and prevent overfitting.4. Human-In-The-Loop Assessment:-Enlist native speakers and domain experts to evaluate the relevance and accuracy of the model's QA outputs for each language.-Incorporate feedback into the iterative training process, refining and re-adapting the models accordingly.Open-Sourcing and Community Feedback:-Make the TTT protocol, adaptive CLMs, evaluation benchmarks, and any synthetic datasets publicly available for the research community.Experiment Monitoring and Documentation:-Record all the parameters, datasets, model configurations, and evaluation metrics meticulously to ensure robustness and reproducibility.-Document any challenges faced, unexpected results, or adaptions made during the experiment for open-sourcing purposes.Data Analysis and Reporting:-Analyze the collected performance data quantitatively, using appropriate statistical methods to compare with non-adaptive baselines.-Report qualitative findings from human-in-the-loop evaluations, interpreting the implications for language model performance in low-resource language domains.3 Input Title: Whole-brain annotation and multi-connectome cell typing quantifies circuit stereotypy in Drosophila Abstract: The fruit fly Drosophila melanogaster combines surprisingly sophisticated behaviour with a highly tractable nervous system.A large part of the fly's success as a model organism in modern neuroscience stems from the concentration of collaboratively generated molecular genetic and digital resources.As presented in our FlyWire companion paper1, this now includes the first full brain connectome of an adult animal.Here we report the systematic and hierarchical annotation of this 130,000-neuron connectome including neuronal classes, cell types and developmental units (hemilineages).This enables any researcher to navigate this huge dataset and find systems and neurons of interest, linked to the literature through the Virtual Fly Brain database2.Crucially, this resource includes 4,552 cell types.3,094 are rigorous consensus validations of cell types previously proposed in the "hemibrain" connectome3.In addition, we propose 1,458 new cell types, arising mostly from the fact that the FlyWire connectome spans the whole brain, whereas the hemibrain derives from a subvolume.Comparison of FlyWire and the hemibrain showed that cell type counts and strong connections were largely stable, but connection weights were surprisingly variable within and across animals.Further analysis defined simple heuristics for connectome interpretation: connections stronger than 10 unitary synapses or providing &gt;1% of the input to a target cell are highly conserved.Some cell types showed increased variability across connectomes: the most common cell type in the mushroom body, required for learning and memory, is almost twice as numerous in FlyWire as the hemibrain.We find evidence for functional homeostasis through adjustments of the absolute amount of excitatory input while maintaining the excitation-inhibition ratio.Finally, and surprisingly, about one third of the cell types proposed in the hemibrain connectome could not yet be reliably identified in the FlyWire connectome.We therefore suggest that cell types should be defined to be robust to inter-individual variation, namely as groups of cells that are quantitatively more similar to cells in a different brain than to any other cell in the same brain.Joint analysis of the FlyWire and hemibrain connectomes demonstrates the viability and utility of this new definition.Our work defines a consensus cell type atlas for the fly brain and provides both an intellectual framework and open source toolchain for brain-scale comparative connectomics.Entities: ['Virtual Fly Brain', 'Central nervous system', 'Transposable element', 'SUMO protein', 'Kenyon cell', 'Romani people', 'Induced stem cells', 'Ventral nerve cord', 'FlyBase', "Parkinson's disease", 'Virtual Network Computing', 'P element', 'Piwiinteracting RNA', 'Drosophila Genetic Reference Panel', 'Bateson-Dobzhansky-Muller model', 'J.B. S. Haldane', 'ATG7', "Haldane's rule", 'Oxford Nanopore Technologies', 'Drosophila mauritiana', 'Germline', 'PINK1', 'Migratory locust', 'CRISPR', 'Heliconius', 'GINS (protein complex)', 'Parkin (ligase)', 'Lepidoptera', 'Illumina, Inc.', 'Drosophila']ProblemInvestigating the Functional Implications of Connectome Variability in Drosophila's Learning and Memory Circuits Across Different Environmental and Genetic ContextsContinued on the next pageTable16-Continued from the previous page Index Types TextsMethodThe proposed method involves a multi-tiered approach that integrates connectomics, behavioral assays, genetic manipulation, and computational modeling to investigate the functional implications of connectome variability in Drosophila's learning and memory circuits.The method consists of the following steps:1. Connectome Mapping and Variability Analysis: a. Utilize the Virtual Fly Brain database to identify and compare individual connectomes, focusing on the mushroom body.b.Quantify the variability in connection weights and cell type counts using statistical methods and machine learning algorithms to identify patterns of variability.2. Behavioral Assays: a. Design a series of learning and memory tasks for Drosophila, such as olfactory conditioning or visual pattern recognition.b.Test groups of flies with known connectome profiles under controlled environmental conditions to establish baseline behavioral data.Environmental and
arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint</p>
<p>Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, 10.48550/arXiv.2312.11805arXiv:2312.11805Gemini: A family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. Jinheon Baek, Alham Fikri Aji, Amir Saffari, 10.18653/v1/2023.nlrse-1.7Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE). the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Knowledge-augmented large language models for personalized contextual query suggestion. Jinheon Baek, Nirupama Chandrasekaran, Silviu Cucerzan, Allen Herring, Sujay Kumar, Jauhar , 2024WWW</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Qwen technical report. Xiaohuan Zhou, and Tianhang Zhu2023arXiv preprint</p>
<p>Sam Andrés M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, Chemcrow: Augmenting large-language models with chemistry tools. 2023</p>
<p>A coefficient of agreement for nominal scales. Educational and Psychological Measurement. Jacob Cohen, 196020</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, 10.48550/ARXIV.2305.14325arXiv:2305.143252023arXiv preprint</p>
<p>Overoptimization of academic publishing metrics: Observing goodhart's law in action. Michael Fire, Carlos Guestrin, 10.1093/gigascience/giz0532019GigaScience8</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova Dassarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, ; Samuel, R Bowman, Jared Kaplan, 10.48550/ARXIV.2302.07459arXiv:2302.07459The capacity for moral self-correction in large language models. Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark,2023arXiv preprint</p>
<p>Sam Henry, Bridget T Mcinnes, Literature based discovery: Models, methods, and trends. Journal of biomedical informatics. 201774</p>
<p>Oren Etzioni, and Eric Horvitz. 2023. A computational inflection for scientific discovery. Tom Hope, Doug Downey, Daniel S Weld, 10.1145/3576896Commun. ACM. 668</p>
<p>Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, 10.48550/arXiv.2310.03302arXiv:2310.03302Benchmarking large language models as AI research agents. 2023arXiv preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Emma Bou Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, L Lample, Renard Elio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024arXiv preprint</p>
<p>Internetaugmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, arXiv:2203.051152022arXiv preprint</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing, Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024</p>
<p>Ying-Chun Lin, Jennifer Neville, Jack W Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, Jaime Teevan, arXiv:2403.12388Interpretable user satisfaction estimation for conversational systems with large language models. 2024arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, Transactions of the Association for Computational Linguistics. 122023a</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/V1/2023.EMNLP-MAIN.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023b. December 6-10, 2023</p>
<p>How AI processing delays foster creativity: Exploring research question co-creation with an llm-based agent. Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang, 10.1145/3613904.3642698Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024. the CHI Conference on Human Factors in Computing Systems, CHI 2024Honolulu, HI, USAACM2024. May 11-16, 20241725</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Scientific language models for biomedical knowledge base completion: An empirical study. R K Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, ArXiv, abs/2106.097002021</p>
<p>10.48550/ARXIV.2303.08774arXiv:2303.08774GPT-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Spearman Rank Correlation Coefficient. W Pirie, 10.1002/0471667196.ess2499.pub220068</p>
<p>Jason Portenoy, Marissa Radensky, Jevin West, Eric Horvitz, Daniel S Weld, Tom Hope, arXiv:2108.05669Toward bursting scientific filter bubbles and boosting innovation via novel author discovery. Bridger2021arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, 10.48550/arXiv.2311.05965arXiv:2311.059652023arXiv preprint</p>
<p>In-context retrieval-augmented language models. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Mathematical discoveries from program search with large language models. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, J R Francisco, Jordan S Ruiz, Pengming Ellenberg, Omar Wang, Pushmeet Fawzi, Alhussein Kohli, Josh Fawzi, Andrea Grochow, Jean-Baptiste Lodi, Talia Mouret, Tao Ringer, Yu, Nature. 6252023</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, Wen Tau, Yih , arXiv:2301.12652Replug: Retrievalaugmented black-box language models. 2023arXiv preprint</p>
<p>Kumar Shridhar, Koustuv Sinha, Andrew Cohen, Tianlu Wang, Ping Yu, Ram Pasunuru, Mrinmaya Sachan, Jason Weston, Asli Celikyilmaz, 10.48550/ARXIV.2311.07961arXiv:2311.07961The ART of LLM refinement: Ask, refine, and trust. 2023arXiv preprint</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly. 561986</p>
<p>Agatha: Automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, M Ilya Tyagin, Ilya Shtutman, Safro, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan</p>
<p>Aurélien Rodriguez, Robert Stojnic. Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel Khabsa, Artem Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xavier Mao, Todor Martinet, Pushkar Mihaylov, Igor Mishra, Yixin Molybog, Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, 10.48550/arXiv.2307.09288arXiv:2307.09288Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Olga Vitalievna Kononova, Kristin A Persson, Gerbrand Ceder, Anubhav Jain, Leigh Weston, Alex Dunn, Ziqin Rong2019571arXiv preprintVahe Tshitoyan, John Dagdelen. Unsupervised word embeddings capture latent knowledge from materials science literature</p>
<p>Applications of machine learning in drug discovery and development. Jessica Vamathevan, Dominic Clark, Paul Czodrowski, Ian Dunham, Edgardo Ferran, George Lee, Bin Li, Anant Madabhushi, Parantu Shah, Michaela Spitzer, Shanrong Zhao, 10.1038/s41573-019-0024-5Nature reviews. Drug discovery. 1862019</p>
<p>We are who we cite: Bridges of influence between natural language processing and other academic fields. Jan Philip Wahle, Terry Ruas, Mohamed Abdalla, Bela Gipp, Saif M Mohammad, 10.18653/v1/2023.emnlp-main.797Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora S Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W Coley, 10.1038/S41586-023-06221-2Yoshua Bengio, and Marinka Zitnik. 2023a. Scientific discovery in the age of artificial intelligence. 620</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.48550/arXiv.2305.14259arXiv:2305.142592023barXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai Hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Scalable zeroshot entity linking with dense entity retrieval. Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, Luke Zettlemoyer, 10.18653/v1/2020.emnlp-main.519Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Retrieve-rewriteanswer: A kg-to-text enhanced llms framework for knowledge graph question answering. Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, J Ren, Anhuan Xie, Wei Song, arXiv:2309.112062023arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, 10.48550/arXiv.2309.02726arXiv:2309.027262023arXiv preprint</p>
<p>A Technique for Producing Ideas. J Young, 2003Mc-Graw Hill LLC</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica, arXiv:2306.05685Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
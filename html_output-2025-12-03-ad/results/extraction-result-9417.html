<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9417 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9417</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9417</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-269773167</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.09395v2.pdf" target="_blank">Matching domain experts by training from scratch on domain knowledge</a></p>
                <p><strong>Paper Abstract:</strong> Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance. To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9417.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9417.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-Finetuned-124M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (124M parameters) finetuned on neuroscience literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained GPT-2 (124M) model (originally trained on WebText) that was finetuned on ~1.3 billion tokens / 20 years of neuroscience literature and evaluated on BrainBench by using perplexity to choose between original and altered abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 architecture (autoregressive transformer) loaded from Huggingface pretrained on WebText and then finetuned on neuroscience-only training data (~1.3B tokens). Evaluation used next-token likelihoods / perplexity over full abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct experimental outcome described in a published neuroscience abstract (original vs minimally altered/contradicted version) from BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Autoregressive next-token prediction; compute sequence log-likelihoods and perplexities for each abstract and select the abstract with lower perplexity as the model's prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Perplexity (exponentiated average negative log-likelihood) per abstract; absolute difference in perplexities used as a continuous confidence score; final decision is binary (choose lower-perplexity abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Accuracy on BrainBench (200 curated test cases) compared to ground-truth original abstracts and to 171 human neuroscientists; calibration assessed by sorting items by model confidence into 20 bins and computing mean accuracy per bin, linear regression of bin vs accuracy, and logistic regression between perplexity differences and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Finetuned GPT-2 (124M) achieved 63.5% accuracy on BrainBench, matching human experts (63.4%). Confidence (absolute perplexity difference) positively correlated with accuracy (models are reasonably calibrated); logistic regression shows significant positive relationship between perplexity differences and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Model is much smaller than state-of-the-art LLMs and still shows a performance gap (~15%) relative to more advanced LLMs reported elsewhere; performance depends on domain-relevant training data and tokenization; possible sensitivity to amount/quality of domain data and tokenizer; decisions reduce output to binary choice, not explicit probabilistic forecast of real-world future discoveries beyond distinguishing two candidate abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Matches human expert baseline (63.4%); outperforms untrained or pretrained-only GPT-2 variants that were not finetuned on neuroscience; underperforms larger LLM variants trained on neuroscience data (e.g., 774M variant surpasses human-level performance).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Finetuning on domain literature, use of specialized tokenization (neuro-tokenizer), increasing model size, calibration analyses (binning, logistic regression) and possibly better/noisier domain data were identified as avenues to improve performance and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Matching domain experts by training from scratch on domain knowledge', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9417.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9417.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-Neuro-124M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (124M) trained from scratch with a neuroscience-specific tokenizer (neurotokenizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 (124M) model trained from scratch on neuroscience literature using a tokenizer trained anew on neuroscience text (neurotokenizer) that preserves domain terminology and achieved human-level performance on BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (trained from scratch with neuro-tokenizer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 architecture trained from scratch on the neuroscience corpus (~1.3B tokens) using a custom BPE-based tokenizer (neuro-tokenizer) that increased neuroscience-specific token coverage; vocabulary size 50,257 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct experimental outcome described in a published neuroscience abstract (original vs altered) from BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Autoregressive next-token prediction; compute sequence log-likelihoods and perplexities using the model and choose the abstract with lower perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Perplexity per abstract; absolute difference used as continuous confidence metric; decision is binary (lower perplexity wins).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Accuracy on BrainBench compared to ground truth and human experts; analysis of tokenization effects by comparing vocabularies and examples; calibration assessed via binning of confidence and regression as for other variants.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>When trained from scratch with the neuro-tokenizer, GPT-2 (124M) reached 63.0% accuracy on BrainBench, on par with human experts (63.4%). The neuro-tokenizer doubled the proportion of neuroscience-related tokens relative to the pretrained tokenizer and improved handling of domain terminology, enabling competitive performance with much less pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training from scratch without a domain-specialized tokenizer was less effective; success depended critically on specialized tokenization and domain-relevant data; still smaller than many contemporary LLMs and may lack some capabilities of larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performs much better than GPT-2 variants trained from scratch without neuro-tokenizer and matches the finetuned GPT-2 (124M) and human experts; larger GPT-2 variants trained on neuroscience can outperform this model.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Specialized tokenization (neuro-tokenizer), more domain data, larger model sizes, and finetuning or pretraining strategies were suggested to further improve accuracy and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Matching domain experts by training from scratch on domain knowledge', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9417.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9417.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-774M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 variant (774M parameters) pretrained on neuroscience literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger GPT-2 variant (774M) pretrained on the neuroscience corpus; reported to surpass human expert performance on BrainBench and to show improved confidence calibration useful for human-model teaming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (774M, pretrained on neuroscience)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A larger GPT-2 family model (774M parameters) pretrained on the neuroscience literature; trained with similar autoregressive next-token objective and evaluated on BrainBench using perplexity comparisons between original and altered abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct experimental outcome described in a published neuroscience abstract (original vs altered) from BrainBench.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Autoregressive next-token prediction; compute perplexities for each candidate abstract and select the lower-perplexity version; use absolute perplexity difference as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Perplexity and differences in perplexity as continuous confidence measure; binary decision by lower perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Accuracy on BrainBench compared to human experts; calibration assessed via binning by confidence and regression, logistic regressions between perplexity differences and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The 774M GPT-2 pretrained on neuroscience outperformed human experts on BrainBench (explicit accuracy number not reported in-text but stated to 'surpass human-level performance'); also demonstrated improved confidence calibration compared to smaller variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Exact numerical performance not provided in main text (reported in figures/appendix); still far smaller than the largest modern LLMs, and performance depends on domain-specific pretraining data quality; potential for leakage or dataset overlap must be managed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Surpasses human experts and closes gap with much larger LLMs reported in prior work; outperforms smaller GPT-2 variants that were not pretrained on domain data or that lacked neuro-tokenizer specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Larger model size, domain-specific pretraining, and improved calibration methods were associated with higher accuracy and better confidence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Matching domain experts by training from scratch on domain knowledge', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9417.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9417.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity-based prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity (exponentiated average negative log-likelihood) based prediction/confidence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses sequence perplexity computed from autoregressive LLMs as the mechanism for choosing between candidate scientific claims (original versus altered abstracts) and as a continuous confidence measure by taking the absolute difference in perplexities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Perplexity ranking / perplexity difference</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Perplexity is computed as exp(- (1/t) * sum_i log p(x_i | x_<i)), where p is the model's conditional token probability; lower perplexity indicates the model considers a sequence more probable under its learned distribution. The method uses perplexity to rank competing textual hypotheses (two abstract versions) and uses the magnitude of perplexity differences as confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Selecting the empirically correct experimental result among two candidate abstracts (used as a proxy for predicting scientific outcomes); more generally, assigning relative likelihood to textual descriptions of scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Compute full-sequence log-likelihoods (next-token probabilities) for each candidate abstract, exponentiate the negative average log-likelihood to get perplexity, pick the candidate with lower perplexity; use absolute perplexity difference as model confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Continuous real-valued perplexity and differences (not converted to explicit percentages); decision output is binary (which abstract is more probable).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Accuracy on BrainBench (binary correctness), calibration analysis by binning items by perplexity-difference into 20 bins and plotting mean accuracy per bin, linear regression of bin index vs accuracy, logistic regression linking perplexity-difference to correctness (and comparison to human confidence).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Perplexity-based selection yielded human-level accuracy for finetuned GPT-2 (124M, 63.5%) and neuro-tokenizer-trained GPT-2 (124M, 63.0%); larger GPT-2 (774M) pretrained on neuroscience surpassed human-level performance. The absolute perplexity difference correlated positively with accuracy, indicating useful confidence calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Perplexity is a model-internal measure and reflects the model's learned statistical distribution over text rather than an explicit real-world probability of an outcome; using perplexity to assert probabilities about real-world future discoveries conflates textual plausibility with empirical truth and may be biased by training-data frequencies; comparisons are constrained to binary choices between two candidate abstracts rather than free-form forecasting; calibration is relative to model confidence but does not guarantee well-calibrated probabilities for real-world events.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Perplexity-based decisions matched or exceeded human expert performance when models were properly finetuned or tokenized; untrained/pretrained-only models without domain adaptation underperformed; larger domain-pretrained models outperformed smaller ones.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Improve tokenization to preserve domain terms (neuro-tokenizer), increase model size and domain data, finetune pretrained models on domain corpora, and perform calibration analyses (binning, logistic regression) to quantify and potentially adjust confidence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Matching domain experts by training from scratch on domain knowledge', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9417.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9417.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior large LLMs (Luo et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models evaluated in Luo et al. (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work reporting that large LLMs (trained on substantially larger datasets and models with far more parameters) outperformed human experts on BrainBench; cited as motivating the present study of whether domain-specific training on smaller models can achieve similar results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models surpass human experts in predicting neuroscience results</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various large LLMs (as reported in Luo et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the cited study as large models trained on trillions of tokens and larger parameter counts, which showed exceptional forward-looking capability in predicting neuroscience experimental outcomes; specifics of which LLMs and their sizes are in the cited work rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct experimental outcomes in neuroscience abstracts (BrainBench), i.e., which experimental result is true given background and methods.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Autoregressive next-token probability/perplexity based selection of candidate abstracts (as described in Luo et al. and leveraged conceptually here), or other LLM evaluation methods described in the prior work (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Perplexity / model likelihoods used as a proxy for relative plausibility; outcomes reported as accuracy on BrainBench and comparisons to human experts in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison of model accuracy on BrainBench against human expert judgments; calibration and other analyses were reported in Luo et al. (2024) (details not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Cited finding: these larger LLMs outperformed neuroscientists in predicting the results of neuroscience experiments (Luo et al., 2024). The present paper references this to motivate exploring whether much smaller models trained on domain data can match that capability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper does not provide the detailed methodologies or numeric metrics from Luo et al. (2024); those specifics must be obtained from the cited work. The present authors raise the question whether the larger models' success is due to vast pretraining vs. domain-specific statistical patterns in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>In Luo et al., large LLMs reportedly surpassed human experts; the present work compares small, domain-trained GPT-2 variants to the human-expert baseline and to the large-LLM results qualitatively (not reproducing all prior-model numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>The present paper explores two approaches inspired by the prior results: finetuning pretrained small models on domain literature, and training from scratch with a domain-specific tokenizer to approximate expert performance with far fewer parameters/data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Matching domain experts by training from scratch on domain knowledge', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models surpass human experts in predicting neuroscience results <em>(Rating: 2)</em></li>
                <li>BrainBench <em>(Rating: 2)</em></li>
                <li>Textbooks Are All You Need <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9417",
    "paper_id": "paper-269773167",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "GPT2-Finetuned-124M",
            "name_full": "GPT-2 (124M parameters) finetuned on neuroscience literature",
            "brief_description": "A pretrained GPT-2 (124M) model (originally trained on WebText) that was finetuned on ~1.3 billion tokens / 20 years of neuroscience literature and evaluated on BrainBench by using perplexity to choose between original and altered abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (finetuned)",
            "model_description": "GPT-2 architecture (autoregressive transformer) loaded from Huggingface pretrained on WebText and then finetuned on neuroscience-only training data (~1.3B tokens). Evaluation used next-token likelihoods / perplexity over full abstracts.",
            "model_size": "124M",
            "prediction_target": "Correct experimental outcome described in a published neuroscience abstract (original vs minimally altered/contradicted version) from BrainBench.",
            "prediction_method": "Autoregressive next-token prediction; compute sequence log-likelihoods and perplexities for each abstract and select the abstract with lower perplexity as the model's prediction.",
            "probability_format": "Perplexity (exponentiated average negative log-likelihood) per abstract; absolute difference in perplexities used as a continuous confidence score; final decision is binary (choose lower-perplexity abstract).",
            "evaluation_method": "Accuracy on BrainBench (200 curated test cases) compared to ground-truth original abstracts and to 171 human neuroscientists; calibration assessed by sorting items by model confidence into 20 bins and computing mean accuracy per bin, linear regression of bin vs accuracy, and logistic regression between perplexity differences and correctness.",
            "results": "Finetuned GPT-2 (124M) achieved 63.5% accuracy on BrainBench, matching human experts (63.4%). Confidence (absolute perplexity difference) positively correlated with accuracy (models are reasonably calibrated); logistic regression shows significant positive relationship between perplexity differences and correctness.",
            "limitations_or_challenges": "Model is much smaller than state-of-the-art LLMs and still shows a performance gap (~15%) relative to more advanced LLMs reported elsewhere; performance depends on domain-relevant training data and tokenization; possible sensitivity to amount/quality of domain data and tokenizer; decisions reduce output to binary choice, not explicit probabilistic forecast of real-world future discoveries beyond distinguishing two candidate abstracts.",
            "comparison_to_baselines": "Matches human expert baseline (63.4%); outperforms untrained or pretrained-only GPT-2 variants that were not finetuned on neuroscience; underperforms larger LLM variants trained on neuroscience data (e.g., 774M variant surpasses human-level performance).",
            "methods_for_improvement": "Finetuning on domain literature, use of specialized tokenization (neuro-tokenizer), increasing model size, calibration analyses (binning, logistic regression) and possibly better/noisier domain data were identified as avenues to improve performance and calibration.",
            "uuid": "e9417.0",
            "source_info": {
                "paper_title": "Matching domain experts by training from scratch on domain knowledge",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT2-Neuro-124M",
            "name_full": "GPT-2 (124M) trained from scratch with a neuroscience-specific tokenizer (neurotokenizer)",
            "brief_description": "A GPT-2 (124M) model trained from scratch on neuroscience literature using a tokenizer trained anew on neuroscience text (neurotokenizer) that preserves domain terminology and achieved human-level performance on BrainBench.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (trained from scratch with neuro-tokenizer)",
            "model_description": "GPT-2 architecture trained from scratch on the neuroscience corpus (~1.3B tokens) using a custom BPE-based tokenizer (neuro-tokenizer) that increased neuroscience-specific token coverage; vocabulary size 50,257 tokens.",
            "model_size": "124M",
            "prediction_target": "Correct experimental outcome described in a published neuroscience abstract (original vs altered) from BrainBench.",
            "prediction_method": "Autoregressive next-token prediction; compute sequence log-likelihoods and perplexities using the model and choose the abstract with lower perplexity.",
            "probability_format": "Perplexity per abstract; absolute difference used as continuous confidence metric; decision is binary (lower perplexity wins).",
            "evaluation_method": "Accuracy on BrainBench compared to ground truth and human experts; analysis of tokenization effects by comparing vocabularies and examples; calibration assessed via binning of confidence and regression as for other variants.",
            "results": "When trained from scratch with the neuro-tokenizer, GPT-2 (124M) reached 63.0% accuracy on BrainBench, on par with human experts (63.4%). The neuro-tokenizer doubled the proportion of neuroscience-related tokens relative to the pretrained tokenizer and improved handling of domain terminology, enabling competitive performance with much less pretraining data.",
            "limitations_or_challenges": "Training from scratch without a domain-specialized tokenizer was less effective; success depended critically on specialized tokenization and domain-relevant data; still smaller than many contemporary LLMs and may lack some capabilities of larger models.",
            "comparison_to_baselines": "Performs much better than GPT-2 variants trained from scratch without neuro-tokenizer and matches the finetuned GPT-2 (124M) and human experts; larger GPT-2 variants trained on neuroscience can outperform this model.",
            "methods_for_improvement": "Specialized tokenization (neuro-tokenizer), more domain data, larger model sizes, and finetuning or pretraining strategies were suggested to further improve accuracy and calibration.",
            "uuid": "e9417.1",
            "source_info": {
                "paper_title": "Matching domain experts by training from scratch on domain knowledge",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT2-774M",
            "name_full": "GPT-2 variant (774M parameters) pretrained on neuroscience literature",
            "brief_description": "A larger GPT-2 variant (774M) pretrained on the neuroscience corpus; reported to surpass human expert performance on BrainBench and to show improved confidence calibration useful for human-model teaming.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (774M, pretrained on neuroscience)",
            "model_description": "A larger GPT-2 family model (774M parameters) pretrained on the neuroscience literature; trained with similar autoregressive next-token objective and evaluated on BrainBench using perplexity comparisons between original and altered abstracts.",
            "model_size": "774M",
            "prediction_target": "Correct experimental outcome described in a published neuroscience abstract (original vs altered) from BrainBench.",
            "prediction_method": "Autoregressive next-token prediction; compute perplexities for each candidate abstract and select the lower-perplexity version; use absolute perplexity difference as confidence.",
            "probability_format": "Perplexity and differences in perplexity as continuous confidence measure; binary decision by lower perplexity.",
            "evaluation_method": "Accuracy on BrainBench compared to human experts; calibration assessed via binning by confidence and regression, logistic regressions between perplexity differences and correctness.",
            "results": "The 774M GPT-2 pretrained on neuroscience outperformed human experts on BrainBench (explicit accuracy number not reported in-text but stated to 'surpass human-level performance'); also demonstrated improved confidence calibration compared to smaller variants.",
            "limitations_or_challenges": "Exact numerical performance not provided in main text (reported in figures/appendix); still far smaller than the largest modern LLMs, and performance depends on domain-specific pretraining data quality; potential for leakage or dataset overlap must be managed.",
            "comparison_to_baselines": "Surpasses human experts and closes gap with much larger LLMs reported in prior work; outperforms smaller GPT-2 variants that were not pretrained on domain data or that lacked neuro-tokenizer specialization.",
            "methods_for_improvement": "Larger model size, domain-specific pretraining, and improved calibration methods were associated with higher accuracy and better confidence estimates.",
            "uuid": "e9417.2",
            "source_info": {
                "paper_title": "Matching domain experts by training from scratch on domain knowledge",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Perplexity-based prediction",
            "name_full": "Perplexity (exponentiated average negative log-likelihood) based prediction/confidence",
            "brief_description": "The paper uses sequence perplexity computed from autoregressive LLMs as the mechanism for choosing between candidate scientific claims (original versus altered abstracts) and as a continuous confidence measure by taking the absolute difference in perplexities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Perplexity ranking / perplexity difference",
            "model_description": "Perplexity is computed as exp(- (1/t) * sum_i log p(x_i | x_&lt;i)), where p is the model's conditional token probability; lower perplexity indicates the model considers a sequence more probable under its learned distribution. The method uses perplexity to rank competing textual hypotheses (two abstract versions) and uses the magnitude of perplexity differences as confidence.",
            "model_size": null,
            "prediction_target": "Selecting the empirically correct experimental result among two candidate abstracts (used as a proxy for predicting scientific outcomes); more generally, assigning relative likelihood to textual descriptions of scientific discoveries.",
            "prediction_method": "Compute full-sequence log-likelihoods (next-token probabilities) for each candidate abstract, exponentiate the negative average log-likelihood to get perplexity, pick the candidate with lower perplexity; use absolute perplexity difference as model confidence.",
            "probability_format": "Continuous real-valued perplexity and differences (not converted to explicit percentages); decision output is binary (which abstract is more probable).",
            "evaluation_method": "Accuracy on BrainBench (binary correctness), calibration analysis by binning items by perplexity-difference into 20 bins and plotting mean accuracy per bin, linear regression of bin index vs accuracy, logistic regression linking perplexity-difference to correctness (and comparison to human confidence).",
            "results": "Perplexity-based selection yielded human-level accuracy for finetuned GPT-2 (124M, 63.5%) and neuro-tokenizer-trained GPT-2 (124M, 63.0%); larger GPT-2 (774M) pretrained on neuroscience surpassed human-level performance. The absolute perplexity difference correlated positively with accuracy, indicating useful confidence calibration.",
            "limitations_or_challenges": "Perplexity is a model-internal measure and reflects the model's learned statistical distribution over text rather than an explicit real-world probability of an outcome; using perplexity to assert probabilities about real-world future discoveries conflates textual plausibility with empirical truth and may be biased by training-data frequencies; comparisons are constrained to binary choices between two candidate abstracts rather than free-form forecasting; calibration is relative to model confidence but does not guarantee well-calibrated probabilities for real-world events.",
            "comparison_to_baselines": "Perplexity-based decisions matched or exceeded human expert performance when models were properly finetuned or tokenized; untrained/pretrained-only models without domain adaptation underperformed; larger domain-pretrained models outperformed smaller ones.",
            "methods_for_improvement": "Improve tokenization to preserve domain terms (neuro-tokenizer), increase model size and domain data, finetune pretrained models on domain corpora, and perform calibration analyses (binning, logistic regression) to quantify and potentially adjust confidence estimates.",
            "uuid": "e9417.3",
            "source_info": {
                "paper_title": "Matching domain experts by training from scratch on domain knowledge",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Prior large LLMs (Luo et al., 2024)",
            "name_full": "Large language models evaluated in Luo et al. (2024)",
            "brief_description": "Referenced prior work reporting that large LLMs (trained on substantially larger datasets and models with far more parameters) outperformed human experts on BrainBench; cited as motivating the present study of whether domain-specific training on smaller models can achieve similar results.",
            "citation_title": "Large language models surpass human experts in predicting neuroscience results",
            "mention_or_use": "mention",
            "model_name": "Various large LLMs (as reported in Luo et al., 2024)",
            "model_description": "Described in the cited study as large models trained on trillions of tokens and larger parameter counts, which showed exceptional forward-looking capability in predicting neuroscience experimental outcomes; specifics of which LLMs and their sizes are in the cited work rather than this paper.",
            "model_size": null,
            "prediction_target": "Correct experimental outcomes in neuroscience abstracts (BrainBench), i.e., which experimental result is true given background and methods.",
            "prediction_method": "Autoregressive next-token probability/perplexity based selection of candidate abstracts (as described in Luo et al. and leveraged conceptually here), or other LLM evaluation methods described in the prior work (details not provided in this paper).",
            "probability_format": "Perplexity / model likelihoods used as a proxy for relative plausibility; outcomes reported as accuracy on BrainBench and comparisons to human experts in the cited study.",
            "evaluation_method": "Comparison of model accuracy on BrainBench against human expert judgments; calibration and other analyses were reported in Luo et al. (2024) (details not reproduced here).",
            "results": "Cited finding: these larger LLMs outperformed neuroscientists in predicting the results of neuroscience experiments (Luo et al., 2024). The present paper references this to motivate exploring whether much smaller models trained on domain data can match that capability.",
            "limitations_or_challenges": "This paper does not provide the detailed methodologies or numeric metrics from Luo et al. (2024); those specifics must be obtained from the cited work. The present authors raise the question whether the larger models' success is due to vast pretraining vs. domain-specific statistical patterns in the literature.",
            "comparison_to_baselines": "In Luo et al., large LLMs reportedly surpassed human experts; the present work compares small, domain-trained GPT-2 variants to the human-expert baseline and to the large-LLM results qualitatively (not reproducing all prior-model numbers).",
            "methods_for_improvement": "The present paper explores two approaches inspired by the prior results: finetuning pretrained small models on domain literature, and training from scratch with a domain-specific tokenizer to approximate expert performance with far fewer parameters/data.",
            "uuid": "e9417.4",
            "source_info": {
                "paper_title": "Matching domain experts by training from scratch on domain knowledge",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models surpass human experts in predicting neuroscience results",
            "rating": 2,
            "sanitized_title": "large_language_models_surpass_human_experts_in_predicting_neuroscience_results"
        },
        {
            "paper_title": "BrainBench",
            "rating": 2,
            "sanitized_title": "brainbench"
        },
        {
            "paper_title": "Textbooks Are All You Need",
            "rating": 1,
            "sanitized_title": "textbooks_are_all_you_need"
        }
    ],
    "cost": 0.0119925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Matching domain experts by training from scratch on domain knowledge
2 Jul 2024</p>
<p>Xiaoliang Luo 
Guangzhi Sun 
Bradley C Love 
Matching domain experts by training from scratch on domain knowledge
2 Jul 2024E5A94277312B1F94334DA945D430E311arXiv:2405.09395v2[q-bio.NC]
Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments(Luo et al., 2024).What is the basis for this performance?One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance.To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results.Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2.Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, autoregressive training approaches.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are statistical machines typically designed to predict the next token-whether it's a word, pixel, or protein sequence.Leveraging vast amounts of training data, LLMs have demonstrated impressive capabilities, including passing professional exams, reasoning (though with limitations), translation, solving mathematics problems, and writing computer code (Strack, 2023;Srivastava et al., 2022;Gunasekar et al., 2023).</p>
<p>Traditionally, the human-level performance of large language models (LLMs) has been evaluated using benchmarks that focus on their backward-looking capabilities, such as1 Department of Experimental Psychology, University College London, UK2 Department of Engineering, University of Cambridge, UK 3 The Alan Turing Institute, UK.Correspondence to: Xiaoliang Luo <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#120;&#105;&#97;&#111;&#46;&#108;&#117;&#111;&#46;&#49;&#55;&#64;&#117;&#99;&#108;&#46;&#97;&#99;&#46;&#117;&#107;">&#120;&#105;&#97;&#111;&#46;&#108;&#117;&#111;&#46;&#49;&#55;&#64;&#117;&#99;&#108;&#46;&#97;&#99;&#46;&#117;&#107;</a>.core knowledge retrieval and reasoning within a given context.Notable benchmarks include MMLU (Hendrycks et al., 2021), PubMedQA (Jin et al., 2019), and MedMCQA (Pal et al., 2022).However, recent research by Luo et al. (2024) has highlighted LLMs' exceptional forward-looking capabilities, particularly in predicting novel outcomes of neuroscience studies.With the development of BrainBench, a forward-looking neuroscience benchmark, Luo et al. (2024) have shown that LLMs can outperform neuroscientists in predicting the results of neuroscientific experiments when provided with the experiment's background and methodologies.These findings raise important questions about the nature of scientific progress, suggesting that many discoveries might largely be iterations of noisy signals from decades of scientific literature.Additionally, they prompt a reevaluation of the extent to which accurate predictions of the future rely more on pattern recognition by auto-regressive models than on traditional scientific reasoning.</p>
<p>In this contribution, we explore the effects of training on domain-specific data by employing a significantly smaller language model, GPT-2 with 124 million parameters (Radford et al., 2019), on a neuroscience-focused dataset containing 1.3 billion tokens.This approach helps assess the effectiveness of auto-regressive training on specialized data in approximating human-level performance.Despite the model size being only about 0.056% to 1% 1 of those evaluated by Luo et al. (2024) and the training data being about 0.065% 2 of those used in Luo et al. (2024), we show both finetuning a pretrained 124M-parameter GPT-2 and training it from scratch with a custom tokenizer for neuroscience yield models that achieve 63.5% and 63% accuracy on BrainBench, matching the performance of human experts (63.4%).Larger GPT-2 variant (774M) pretrained on the same data yield even stronger BrainBench performance (surpassing human experts; Fig 1) with improved confidence calibration beneficial for human-model teaming (see Appendix B, D).Each test case includes a published abstract alongside a modified version crafted by neuroscientists.These modifications, though minimal, significantly alter the results-for instance, by changing the roles of brain regions or reversing a result's direction (e.g., from "decreases" to "increases").Despite these changes, the altered abstracts remain logically coherent.</p>
<p>Method</p>
<p>The test-taker's challenge is to identify the correct study outcome by choosing between the original abstract and its altered counterpart.</p>
<p>Model evaluation</p>
<p>We presented models with two versions of the abstracts from each test case separately.We prefixed each abstract with the prompt "You are a neuroscientist with deep knowledge in neuroscience.Here is an abstract from a neuroscience publication:".We then measured the perplexity of both passages and used perplexity as the indicator of whether models favor one abstract or the other.</p>
<p>Perplexity measures the degree of uncertainty of a model when generating a particular sequence of text and is defined as the exponentiated average negative log-likelihood of a tokenized sequence.If we have a tokenized abstract X = (x 0 , x 1 , . . ., x t ), then the perplexity of X, given a model parameterized by  is,
P P L(X) = exp  1 t t i log p  (x i |x &lt;i )(1)
where log p  (x i |x &lt;i ) is the log-likelihood of the ith token conditioned on the preceding tokens x &lt;i according to the model.Given both the original and the altered abstracts, we used the abstract with lower perplexity as the model's decision and evaluated the overall accuracy across the entire BrainBench dataset accordingly.</p>
<p>Human evaluation</p>
<p>Previous work (Luo et al., 2024) collected human judgements from 171 neuroscience experts on BrainBench.These data are publicly available3 and provide a useful comparison to LLM performance.</p>
<p>Model configurations</p>
<p>We considered a number of variants of GPT-2 differ by their training strategies including training data and tokenization.</p>
<p>Model variants are summarized in Table 1.</p>
<p>The pretrained GPT-2 and the tokenizer were loaded from Huggingface hub4 , which were trained on the WebText dataset collected by OpenAI (Radford et al., 2019).The neuroscience training data was collected by Luo et al. (2024) (see Sec. 2.5).The models trained from scratch and finetuned used the neuroscience data only.The neurotokenizer employs GPT-2's tokenization strategy (Radford et al., 2019), adapted from Byte Pair Encoding (BPE) (Gage, 1994) for word segmentation (Sennrich et al., 2016).It's trained anew on neuroscience data used for model training, maintaining a vocabulary size of 50,257 tokens.</p>
<p>Neuroscience training data</p>
<p>The data we used to train GPT-2 from scratch, finetune the pretrained GPT-2 as well as train the neuro-tokenizer were collected by Luo et al. (2024)</p>
<p>Results</p>
<p>We explored various training strategies and found that finetuning the pretrained GPT-2 on 20 years of neuroscience literature allowed it to achieve human-level performance on BrainBench, recording a 63.5% accuracy (Fig. 1; human experts: 63.4%).Training GPT-2 from scratch solely with neuroscience literature was less effective.However, developing a new tokenizer tailored to neuroscience literature and using it to retrain GPT-2 from scratch with the same data resulted in a performance on par with human experts, achieving 63% accuracy (Fig. 1).Notably, the amount of domain-specific data used to train GPT-2 from scratch is only about one-seventh of the text used to pretrain the original model.This indicates two effective approaches to reach human-level performance: pretraining on a broad general corpus followed by finetuning on domain-specific data, or using a specialized tokenizer and significantly less domain-specific data.</p>
<p>Figure 1.Performance of human experts and models on Brain-Bench.Two configurations of GPT-2 models achieve human-level performance on BrainBench: one by fine-tuning the pretrained GPT-2 on neuroscience literature, and the other by using a new tokenizer (GPT2-Neuro) trained on neuroscience literature and retraining GPT-2 from scratch with only neuroscience data.Versions of GPT-2 that are untrained, pretrained, or trained solely on neuroscience data without these modifications underperform compared to experts on BrainBench.Larger GPT-2 (774M) trained using neuroscience literature surpassed human-level performance.</p>
<p>To assess the impact of a specialized tokenizer, we compared the tokens generated by the pretrained GPT-2 tokenizer with those from our neuro-tokenizer, trained on neuroscience data.The two tokenizers shared 47.9% of their vocabularies (Fig 2A).We utilized GPT-4 (zero-shot prompting) to analyze each vocabulary and identify tokens frequently associated with neuroscience.Our findings showed that the neurotokenizer contained twice the proportion of neurosciencerelated tokens compared to the pretrained tokenizer (Fig. 2B-C).This significant improvement in specialized tokenization suggests that it is possible to pretrain GPT-2 from scratch with significantly less neuroscience data using the neurotokenizer, yet achieve performance comparable to both the finetuned model and human experts.</p>
<p>To better understand the differences in tokenization by the two tokenizers, we analyzed examples from BrainBench test cases where the pretrained GPT-2 answered incorrectly, whereas the GPT-2 trained with the neuro-tokenizer responded correctly.Figure 3 illustrates how the neurotokenizer more effectively preserves domain-specific terminologies, such as brain regions or neurotransmitters.We believe that this specialized tokenization allows the model to utilize limited domain knowledge more effectively and to consider a broader context within the fixed context window of the training data.</p>
<p>Discussion</p>
<p>In this contribution, we demonstrated that training a relatively small LLM (GPT-2) on limited domain-specific data can match the predictive performance of human experts on BrainBench.By finetuning GPT-2 with just a fraction of its pretraining data, we elevated its performance to the level of trained neuroscientists.Additionally, we showed that training GPT-2 from scratch, with domainspecific knowledge incorporated into the tokenizer, yields comparable results.This highlights the importance of preserving domain-specific terminologies during tokenization to improve language models' performance on specialized tasks, as suggested by Yang et al. (2024) in the clinical science domain.Pretraining on small-scale knowledge with specialized tokenization offers a more efficient method for achieving human-like performance on domain-specific tasks.</p>
<p>In addition, the resulting models show good calibration and ability to integrate information across context (Appendix B, C).</p>
<p>Achieving parity with and surpassing human experts using our simplified setup prompts questions about the essence of scientific progress.It suggests that a statistical machine, even one as basic as predicting the next word, can discern the intricate structure of a knowledge-rich field.Despite a significant performance gap (15%) between GPT-2 (124M) and more advanced LLMs on BrainBench tests, larger GPT-2 models-still much smaller than their counterparts-are narrowing this gap when pretrained with neuroscience data (Appendix D).We suspect that the performance is influenced by both model size and the quality and relevance of the training data (see Luo et al. 2024 for a discussion).</p>
<p>Working with smaller models does have benefits, such as enabling teams with modest resources to have full control over the training procedure.This control can minimize the risk of leakage and allow for additional hypotheses to be evaluated.For instance, in future work, we will evaluate whether training on adjacency fields like psychology impacts performance on BrainBench, a neuroscience benchmark.That degree of control is not possible using pretrained LLMs and will allow us to evaluate the structure of scientific disciplines.</p>
<p>Software and Data</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>A. Comparison between pretrained and neuro-tokenizer</p>
<p>C. Contextual integration analysis</p>
<p>To investigate the extent to which GPT-2 models pretrained on neuroscience can integrate broad context from abstracts, we conducted an experiment involving the removal of contextual information from BrainBench test cases.Following the same evaluation procedure outlined for full abstract cases, we assessed the models using individual sentences extracted from abstracts containing at least one result alternation.In cases with multiple alternations, we computed the mean accuracy across these alternations as the final accuracy for the abstract.We then compared the level of performance degradation when these models were evaluated on full-length abstracts versus individual sentences where background and method information from the abstracts were removed.As a result, models performed worse when wider context was removed (Fig. 5), which provides strong evidence that these models are integrating information across the abstract, including information on background and methods.</p>
<p>D. BrainBench performance across model sizes and training data</p>
<p>E. Training details</p>
<p>Variants of GPT-2 models using Huggingface implementations.We used a batch size of 16 for GPT-2 124M (8 for GPT-2 355M and 4 for GPT-2 774M) and a chunk size of 1024.Training involved the use of the AdamW optimizer (Loshchilov &amp; Hutter, 2019) with a learning rate of 2e-5 and a cosine learning rate scheduler.We applied gradient accumulation steps set at 8. Five training epochs were performed, along with a warm-up step of 0.03 and a weight decay rate of 0.001.bf16 mixed precision training and data parallelism were employed.We used 4 Nvidia A100 (80GB) GPUs hosted on Microsoft Azure.</p>
<p>BrainBench has curated 200 test cases from abstracts in the Journal of Neuroscience published in 2023.These abstracts are categorized into five sections: Behavioral/Cognitive, Systems/Circuits, Neurobiology of Disease, Development/Plasticity/Repair, and Cellular/Molecular.</p>
<p>Figure 3 .
3
Figure 3. Tokenization examples.Compared to a pretrained tokenizer trained on general text, a neuro-tokenizer trained specifically on neuroscience literature better preserves domain-specific terminology, such as brain regions, in neuroscience.</p>
<p>Figure 5 .
5
Figure5.GPT-2 variants pretrained on neuroscience literature integrate contextual information to succeed on BrainBench.The removal of background and method sections from abstracts, with an evaluation based solely on individual sentences and result alternations, significantly impairs the performance of models on BrainBench.Much of the human-level performance appears to arise from integrating information across the abstract.</p>
<p>Figure 6 .
6
Figure6.BrainBench performance across models of varying sizes and training data.The GPT-2 variants (124M, 355M, 774M) pretrained entirely on the neuroscience literature from scratch shows progressively better results, matching or surpassing human performance and closing the gap to larger models tested inLuo et al. (2024).Phi3, with half the size of the 7B models, achieves competitive results likely due to its high-quality training data.In contrast, TinyLlama, with 1.1 billion parameters, lags behind on BrainBench.Overall, performance on domain-specific tasks such as BrainBench is influenced by both model size and the quality and relevance of the training data.</p>
<p>Table 1 .
1
Model variants.
VariantTrainingDataTokenizerGPT2-Untrained--pretrainedGPT2-Pretrained from scratchWebTextpretrainedGPT2-Scratchfrom scratch neuroscience pretrainedGPT2-Finetunedfinetuneneuroscience pretrainedGPT2-Neurofrom scratch neurosciencecustom
Estimated using 7B and 180B LLMs.
Estimated based on reported Llama-2 training data size (2 trillion tokens).
https://github.com/braingpt-lovelab/BrainBench
https://huggingface.co/openai-community/gpt2
AcknowledgementsThis work was supported the ESRC (ES/W007347/1), Microsoft (Accelerate Foundation Models Research Program), and a Royal Society Wolfson Fellowship (18302) to B.C.L.B. Confidence calibration of domain-specific pretrained modelsThe absolute difference of perplexities of two versions of the abstract was used as a measure of model confidence.To assess the calibration of GPT-2 variants pretrained on neuroscience literature, we compared their accuracies with their confidence levels.First, we ranked and sorted model confidence across all test cases.Subsequently, we created 20 bins based on this sort.Within each bin, we calculated the mean accuracy.A well-calibrated model will exhibit a higher accuracy in bins associated with higher confidence rankings.We fit a linear regression model using the bin number as the independent variable and the mean accuracy of each bin as the dependent variable to evaluate calibration.We observed that human experts and models all show positive correlations between confidence and accuracy, indicating calibration (Fig.4)In addition, we fitted logistic regressions using perplexity differences to models' answers and from self-reported confidences of human experts to their responses.We confirmed a positive and significant correlation between model (355M and 774M) perplexities and their answers as well as human confidences and their responses (Table2).When human experts and models are confident in their BrainBench judgments, they are more likely to be correct.Confidence ratings were sorted and placed in equally-sized bins with the mean accuracy for items in that bin plotted.The positive slope of the black regression lines for human experts and all models indicates that confidence is well calibrated (i.e., higher confidence corresponds to higher accuracy).Calibration is beneficial for human-machine teams.Table2. Calibration analysis using logistic regression fits.For models, logistic regressions were fitted between perplexity differences of a test case and its correctness given a LLM.For human experts, logistic regressions were fitted between their confidence of a test case and its correctness.Model/Human
A new algorithm for data compression. P Gage, The C Users Journal archive. 12598040301994</p>
<p>Textbooks Are All You Need. S Gunasekar, Y Zhang, J Aneja, C C T Mendes, A Del Giorno, S Gopi, M Javaheripi, P Kauffmann, G De Rosa, O Saarikivi, A Salim, S Shah, H S Behl, X Wang, S Bubeck, R Eldan, A T Kalai, Y T Lee, Y Li, 10.48550/ARXIV.2306.116442023</p>
<p>Measuring Massive Multitask Language Understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300January 2021</p>
<p>. Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, Pub-Medqa, arXiv:1909.06146A Dataset for Biomedical Research Question Answering. September 2019cs, qbio</p>
<p>Decoupled Weight Decay Regularization. I Loshchilov, F Hutter, arXiv:1711.05101January 2019cs, math</p>
<p>Large language models surpass human experts in predicting neuroscience results. X Luo, A Rechardt, G Sun, K K Nejad, F Yez, B Yilmaz, K Lee, A O Cohen, V Borghesani, A Pashkov, D Marinazzo, J Nicholas, A Salatiello, I Sucholutsky, P Minervini, S Razavi, R Rocca, E Yusifov, T Okalova, N Gu, M Ferianc, M Khona, K R Patil, P.-S Lee, R Mata, N E Myers, J K Bizley, S Musslick, I P Bilgin, G Niso, J M Ales, M Gaebler, N A R Murty, L Loued-Khenissi, A Behler, C M Hall, J Dafflon, S D Bao, B C Love, arXiv:2403.03230March 2024cs, q-bio</p>
<p>A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. A Pal, L K Umapathi, M Sankarasubbu, Medm-Cqa, arXiv:2203.14371March 2022</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>Neural Machine Translation of Rare Words with Subword Units. R Sennrich, B Haddow, A Birch, arXiv:1508.07909June 2016</p>
<p>. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, A Xiang, A Parrish, A Nie, A Hussain, A Askell, A Dsouza, A Slone, A Rahane, A S Iyer, A Andreassen, A Madotto, A Santilli, A Stuhlmller, A Dai, A La, A Lampinen, A Zou, A Jiang, A Chen, A Vuong, A Gupta, A Gottardi, A Norelli, A Venkatesh, A Gholamidavoodi, A Tabassum, A Menezes, A Kirubarajan, A Mullokandov, A Sabharwal, A Herrick, A Efrat, A Erdem, A Karaka, B R Roberts, B S Loe, B Zoph, B Bojanowski, B zyurt, B Hedayatnia, B Neyshabur, B Inden, B Stein, B Ekmekci, B Y Lin, B Howald, B Orinion, C Diao, C Dour, C Stinson, C Argueta, C F Ramrez, C Singh, C Rathkopf, C Meng, C Baral, C Wu, C Callison-Burch, C Waites, C Voigt, C D Manning, C Potts, C Ramirez, C E Rivera, C Siro, C Raffel, C Ashcraft, C Garbacea, D Sileo, D Garrette, D Hendrycks, D Kilman, D Roth, D Freeman, D Khashabi, D Levy, D M Gonzlez, D Perszyk, D Hernandez, D Chen, D Ippolito, D Gilboa, D Dohan, D Drakard, D Jurgens, D Datta, D Ganguli, D Emelin, D Kleyko, D Yuret, D Chen, D Tam, D Hupkes, D Misra, D Buzan, D C Mollo, D Yang, D.-H Lee, D Schrader, E Shutova, E D Cubuk, E Segal, E Hagerman, E Barnes, E Donoway, E Pavlick, E Rodola, E Lam, E Chu, E Tang, E Erdem, E Chang, E A Chi, E Dyer, E Jerzak, E Kim, E E Manyasi, E Zheltonozhskii, F Xia, F Siar, F Martnez-Plumed, F Happ, F Chollet, F Rong, G Mishra, G I Winata, G De Melo, G Kruszewski, G Parascandolo, G Mariani, G Wang, G Jaimovitch-Lpez, G Betz, G Gur-Ari, H Galijasevic, H Kim, H Rashkin, H Hajishirzi, H Mehta, H Bogar, H Shevlin, H Schtze, H Yakura, H Zhang, H M Wong, I Ng, I Noble, J Jumelet, J Geissinger, J Kernion, J Hilton, J Lee, J F Fisac, J B Simon, J Koppel, J Zheng, J Zou, J Koco, J Thompson, J Wingfield, J Kaplan, J Radom, J Sohl-Dickstein, J Phang, J Wei, J Yosinski, J Novikova, J Bosscher, J Marsh, J Kim, J Taal, J Engel, J Alabi, J Xu, J Song, J Tang, J Waweru, J Burden, J Miller, J U Balis, J Batchelder, J Berant, J Frohberg, J Rozen, J Hernandez-Orallo, J Boudeman, J Guerr, J Jones, J B Tenenbaum, J S Rule, J Chua, K Kanclerz, K Livescu, K Krauth, K Gopalakrishnan, K Ignatyeva, K Markert, K D Dhole, K Gimpel, K Omondi, K Mathewson, K Chiafullo, K Shkaruta, K Shridhar, K Mcdonell, K Richardson, L Reynolds, L Gao, L Zhang, L Dugan, L Qin, L Contreras-Ochando, L.-P Morency, L Moschella, L Lam, L Noble, L Schmidt, L He, L O Coln, L Metz, L K enel, M Bosma, M Sap, M Ter Hoeve, M Farooqi, M Faruqui, M Mazeika, M Baturan, M Marelli, M Maru, M J R Quintana, M Tolkiehn, M Giulianelli, M Lewis, M Potthast, M L Leavitt, M Hagen, M Schubert, M O Baitemirova, M Arnaud, M Mcelrath, M A Yee, M Cohen, M Gu, M Ivanitskiy, M Starritt, M Strube, M Swdrowski, M Bevilacqua, M Yasunaga, M Kale, M Cain, M Xu, M Suzgun, M Walker, M Tiwari, M Bansal, M Aminnaseri, M Geva, M Gheini, T , M V Peng, N Chi, N A Lee, N Krakover, N G , .-A Cameron, N Roberts, N Doiron, N Martinez, N Nangia, N Deckers, N Muennighoff, N Keskar, N S Iyer, N S Constant, N Fiedel, N Wen, N Zhang, O Agha, O Elbaghdadi, O Levy, O Evans, O Casares, P A M Doshi, P Fung, P Liang, P P Vicol, P Alipoormolabashi, P Liao, P Liang, P Chang, P Eckersley, P Htut, P M Hwang, P Mikowski, P Patil, P Pezeshkpour, P Oli, P Mei, Q Lyu, Q Chen, Q Banjade, R Rudolph, R E Gabriel, R Habacker, R Risco, R Millire, R Garg, R Barnes, R Saurous, R A Arakawa, R Raymaekers, R Frank, R Sikand, R Novak, R Sitelew, R Lebras, R Liu, R Jacobs, R Zhang, R Salakhutdinov, R Chi, R Lee, R Stovall, R Teehan, R Yang, R Singh, S Mohammad, S M Anand, S Dillavou, S Shleifer, S Wiseman, S Gruetter, S Bowman, S R Schoenholz, S S Han, S Kwatra, S Rous, S A Ghazarian, S Ghosh, S Casey, S Bischoff, S Gehrmann, S Schuster, S Sadeghi, S Hamdan, S Zhou, S Srivastava, S Shi, S Singh, S Asaadi, S Gu, S S Pachchigar, S Toshniwal, S Upadhyay, S Shyamolima, Debnath, S Shakeri, S Thormeyer, S Melzi, S Reddy, S P Makini, S.-H Lee, S Torene, S Hatwar, S Dehaene, S Divic, S Ermon, S Biderman, S Lin, S Prasad, S T Piantadosi, S M Shieber, S Misherghi, S Kiritchenko, S Mishra, T Linzen, T Schuster, T Li, T Yu, T Ali, T Hashimoto, T.-L Wu, T Desbordes, T Rothschild, T Phan, T Wang, T Nkinyili, T Schick, T Kornev, T Tunduny, T Gerstenberg, T Chang, T Neeraj, T Khot, T Shultz, U Shaham, V Misra, V Demberg, V Nyamai, V Raunak, V Ramasesh, V U Prabhu, V Padmakumar, V Srikumar, W Fedus, W Saunders, W Zhang, W Vossen, X Ren, X Tong, X Zhao, X Wu, X Shen, Y Yaghoobzadeh, Y Lakretz, Y Song, Y Bahri, Y Choi, Y Yang, Y Hao, Y Chen, Y Belinkov, Y Hou, Y Hou, Y Bai, Z Seid, Z Zhao, Z Wang, Z J Wang, Z Wang, Wu, 10.48550/ARXIV.2206.046153Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. 2022</p>
<p>Visual proteomics. R Strack, 10.1038/s41592-023-02104-6Nature Methods. 1548-70912012December 2023</p>
<p>URL. </p>
<p>exKidneyBERT: a language model for kidney transplant pathology reports and the crucial role of extended vocabularies. T Yang, I Sucholutsky, K.-Y Jen, M Schonlau, 10.7717/peerj-cs.1888PeerJ Computer Science. 2376-599210e1888February 2024</p>            </div>
        </div>

    </div>
</body>
</html>
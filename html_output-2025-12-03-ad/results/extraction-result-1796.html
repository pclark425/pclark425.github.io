<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1796 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1796</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1796</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-31366ff634fc905affd78dbd8ddc9a872c006a87</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/31366ff634fc905affd78dbd8ddc9a872c006a87" target="_blank">CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.</p>
                <p><strong>Paper Abstract:</strong> Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1796.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1796.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeBERTScore_vs_HumanPreference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeBERTScore compared to human preference ratings (CoNaLa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of CodeBERTScore and other automated metrics (BLEU, CodeBLEU, METEOR, chrF, ROUGE, CrystalBLEU, etc.) against human preference judgments on NL→code generations from the CoNaLa dataset; correlations reported with Kendall-Tau, Pearson and Spearman.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics (CodeBERTScore, BLEU, CodeBLEU, METEOR, chrF, ROUGE, CrystalBLEU, chrF, ROUGE-*)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated code snippets)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>NL→Python (CoNaLa dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>human preference (annotator rated usefulness/correctness on a 0–4 scale)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Experienced software developers graded generated snippets (scale 0–4); Evtikhiev et al. (2022) annotations reused by this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td>4.5</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>experienced software developers (annotators from Evtikhiev et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall-Tau (τ), Pearson (r_p), Spearman (r_s)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>CodeBERTScore vs human preference: Kendall-Tau τ=0.517 (±0.024), Pearson r_p=0.674 (±0.012), Spearman r_s=0.662 (±0.012); best baseline chrF: τ≈0.470 (±0.029), r_p≈0.635 (±0.023), r_s≈0.623 (±0.018). (Values averaged across 3 runs; standard deviations reported in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Using CodeBERTScore with language-appropriate pretrained model and tuned hyperparameters (choice of hidden layer), including the natural-language context when encoding, and weighing recall (F3) when optimizing for functional correctness — these choices increased correlation with human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Surface-form token-based metrics (BLEU, CrystalBLEU, simple n-gram overlap) and AST/dataflow metrics can fail when implementations are lexically different but semantically equivalent; adversarial lexical perturbations reduce alignment for token-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not reported as a simple scalar; however, robustness analyses show lexical variations, API substitutions, and non-lexical equivalence reduce agreement for token-based metrics, while CodeBERTScore is more robust; language-specific pretraining and proper layer selection improve alignment, suggesting complexity/semantic subtlety affects metric–human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Including the NL context (instruction/comment) in encoding increased Kendall-Tau (example: from 0.50 to 0.52 reported), and using an Fβ that emphasizes recall (F3) improved correlation with functional correctness — indicating clearer/appropriate criteria and metric design increase agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>2860</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>CodeBERTScore achieved the highest correlation with human preference compared to all listed baselines (e.g., τ=0.517 for CodeBERTScore vs τ≈0.470 for chrF and lower for BLEU/CodeBLEU), but inter-human agreement was not reported for direct comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>CodeBERTScore uses pretrained CodeBERT models; authors further continued unsupervised MLM pretraining on language-specific corpora and tuned only evaluation hyperparameters (choice of layer, F1 vs F3). It was not fine-tuned on human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CodeBERTScore correlates better with human preference than traditional lexical or AST/dataflow metrics across CoNaLa; including NL context, using language-specific pretrained models (when available), and selecting an appropriate transformer layer improve alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Human annotations come from an external dataset (Evtikhiev et al., 2022) and inter-human agreement is not reported here; CodeBERTScore depends on the underlying pretrained model and hyperparameter choices; computational cost (GPU) and sensitivity to layer choice are practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1796.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1796.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics_vs_ExecutionCorrectness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated metrics compared to execution-based (functional) correctness (HumanEval / HumanEval-X)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of automated similarity/semantic metrics (CodeBERTScore and baselines) against execution-based binary functional correctness (pass all tests) across multiple programming languages (HumanEval and translated variants); correlations reported with Kendall-Tau and Spearman.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>automated metrics (CodeBERTScore, BLEU, CodeBLEU, METEOR, chrF, ROUGE-*) and execution-based correctness used as ground truth for correlation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>source code (generated program implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>HumanEval: Python and translated HumanEval-X: Java, C++, JavaScript, Python (experiments used Java, C++, Python, JavaScript)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>functional correctness (execution on provided test cases; ground truth binary pass/fail)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Functional correctness computed by executing generated code on hand-written test suites (HumanEval/HumanEval-X); execution-based labels treated as ground truth rather than human preference ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>not applicable (execution-based ground truth); HumanEval reference solutions are human-written but not re-annotated for this paper</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall-Tau (τ) and Spearman (r_s) correlations between metric scores and execution-based correctness; Pearson used in other comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>CodeBERTScore correlations with functional correctness (averaged/ per-language): Kendall-Tau τ — Java: 0.553, C++: 0.327, Python: 0.422, JavaScript: 0.319; Spearman r_s — Java: 0.369, C++: 0.393, Python: 0.415, JavaScript: 0.402. Overall, CodeBERTScore is more correlated with execution-correctness than baseline metrics on average across languages (values averaged across runs; stds reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Using F3 (recall-weighted Fβ) for functional-correctness correlation experiments, language-specific pretrained CodeBERT models, and selecting appropriate hidden-layer embeddings (deeper layers up to a point) increased agreement with execution-based correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Metrics that rely on exact lexical or syntactic match (BLEU, pure n-gram) and CodeBLEU can have low correlation with execution because valid implementations can differ in AST/dataflow or be partial/non-parsable; CrystalBLEU shows poor performance in some languages (e.g., very low in C++).</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Functional correctness correlation varies by language and likely by semantic complexity of tasks — CodeBERTScore performs better overall but shows lower τ in some languages (e.g., C++), indicating task/language specifics and implementation diversity affect agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Weighting recall (F3) when comparing to execution-based correctness improves alignment, suggesting that emphasizing presence of required semantic content (recall) increases agreement with execution outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>164</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>The paper compares metric scores to execution-based correctness (treated as objective ground truth) rather than to direct human raters in this experiment; CodeBERTScore shows higher correlation to execution than baselines, but no direct comparison to inter-human agreement is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>CodeBERTScore models received continued MLM pretraining on language-specific corpora; no supervised fine-tuning on execution labels or human judgments was performed for the evaluation metric itself (only hyperparameter selection).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CodeBERTScore exhibits higher correlation with execution-based functional correctness than other commonly used metrics across multiple languages, particularly when using language-specific pretrained models, appropriate layer choice, and recall-weighted scoring (F3). Execution-based evaluation remains the most direct measure but is expensive and hazardous to run.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Execution-based ground truth requires hand-written test suites (costly) and sandboxing for security; CodeBERTScore requires GPUs and depends on underlying model quality; correlations vary across languages and can be lower for some languages (e.g., C++), showing incomplete alignment in all cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of the bleu: how should we assess quality of the code generation models? <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Crystalbleu: precisely and efficiently measuring the similarity of code <em>(Rating: 2)</em></li>
                <li>Codebleu: a method for automatic evaluation of code synthesis <em>(Rating: 2)</em></li>
                <li>BERTScore: Evaluating text generation with BERT <em>(Rating: 1)</em></li>
                <li>T5score: Discriminative fine-tuning of generative evaluation metrics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1796",
    "paper_id": "paper-31366ff634fc905affd78dbd8ddc9a872c006a87",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "CodeBERTScore_vs_HumanPreference",
            "name_full": "CodeBERTScore compared to human preference ratings (CoNaLa)",
            "brief_description": "Comparison of CodeBERTScore and other automated metrics (BLEU, CodeBLEU, METEOR, chrF, ROUGE, CrystalBLEU, etc.) against human preference judgments on NL→code generations from the CoNaLa dataset; correlations reported with Kendall-Tau, Pearson and Spearman.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics (CodeBERTScore, BLEU, CodeBLEU, METEOR, chrF, ROUGE, CrystalBLEU, chrF, ROUGE-*)",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated code snippets)",
            "artifact_domain": "NL→Python (CoNaLa dataset)",
            "evaluation_criteria": "human preference (annotator rated usefulness/correctness on a 0–4 scale)",
            "human_evaluation_setup": "Experienced software developers graded generated snippets (scale 0–4); Evtikhiev et al. (2022) annotations reused by this paper.",
            "human_expert_count": 4.5,
            "human_expert_expertise": "experienced software developers (annotators from Evtikhiev et al., 2022)",
            "agreement_metric": "Kendall-Tau (τ), Pearson (r_p), Spearman (r_s)",
            "agreement_score": "CodeBERTScore vs human preference: Kendall-Tau τ=0.517 (±0.024), Pearson r_p=0.674 (±0.012), Spearman r_s=0.662 (±0.012); best baseline chrF: τ≈0.470 (±0.029), r_p≈0.635 (±0.023), r_s≈0.623 (±0.018). (Values averaged across 3 runs; standard deviations reported in paper.)",
            "high_agreement_conditions": "Using CodeBERTScore with language-appropriate pretrained model and tuned hyperparameters (choice of hidden layer), including the natural-language context when encoding, and weighing recall (F3) when optimizing for functional correctness — these choices increased correlation with human preference.",
            "low_agreement_conditions": "Surface-form token-based metrics (BLEU, CrystalBLEU, simple n-gram overlap) and AST/dataflow metrics can fail when implementations are lexically different but semantically equivalent; adversarial lexical perturbations reduce alignment for token-based metrics.",
            "artifact_complexity_effect": "Not reported as a simple scalar; however, robustness analyses show lexical variations, API substitutions, and non-lexical equivalence reduce agreement for token-based metrics, while CodeBERTScore is more robust; language-specific pretraining and proper layer selection improve alignment, suggesting complexity/semantic subtlety affects metric–human alignment.",
            "criteria_clarity_effect": "Including the NL context (instruction/comment) in encoding increased Kendall-Tau (example: from 0.50 to 0.52 reported), and using an Fβ that emphasizes recall (F3) improved correlation with functional correctness — indicating clearer/appropriate criteria and metric design increase agreement.",
            "sample_size": 2860,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "CodeBERTScore achieved the highest correlation with human preference compared to all listed baselines (e.g., τ=0.517 for CodeBERTScore vs τ≈0.470 for chrF and lower for BLEU/CodeBLEU), but inter-human agreement was not reported for direct comparison.",
            "calibration_or_training": "CodeBERTScore uses pretrained CodeBERT models; authors further continued unsupervised MLM pretraining on language-specific corpora and tuned only evaluation hyperparameters (choice of layer, F1 vs F3). It was not fine-tuned on human judgments.",
            "key_findings": "CodeBERTScore correlates better with human preference than traditional lexical or AST/dataflow metrics across CoNaLa; including NL context, using language-specific pretrained models (when available), and selecting an appropriate transformer layer improve alignment with human judgments.",
            "limitations_noted": "Human annotations come from an external dataset (Evtikhiev et al., 2022) and inter-human agreement is not reported here; CodeBERTScore depends on the underlying pretrained model and hyperparameter choices; computational cost (GPU) and sensitivity to layer choice are practical limitations.",
            "uuid": "e1796.0",
            "source_info": {
                "paper_title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Metrics_vs_ExecutionCorrectness",
            "name_full": "Automated metrics compared to execution-based (functional) correctness (HumanEval / HumanEval-X)",
            "brief_description": "Comparison of automated similarity/semantic metrics (CodeBERTScore and baselines) against execution-based binary functional correctness (pass all tests) across multiple programming languages (HumanEval and translated variants); correlations reported with Kendall-Tau and Spearman.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "automated metrics (CodeBERTScore, BLEU, CodeBLEU, METEOR, chrF, ROUGE-*) and execution-based correctness used as ground truth for correlation",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "source code (generated program implementations)",
            "artifact_domain": "HumanEval: Python and translated HumanEval-X: Java, C++, JavaScript, Python (experiments used Java, C++, Python, JavaScript)",
            "evaluation_criteria": "functional correctness (execution on provided test cases; ground truth binary pass/fail)",
            "human_evaluation_setup": "Functional correctness computed by executing generated code on hand-written test suites (HumanEval/HumanEval-X); execution-based labels treated as ground truth rather than human preference ratings.",
            "human_expert_count": null,
            "human_expert_expertise": "not applicable (execution-based ground truth); HumanEval reference solutions are human-written but not re-annotated for this paper",
            "agreement_metric": "Kendall-Tau (τ) and Spearman (r_s) correlations between metric scores and execution-based correctness; Pearson used in other comparisons.",
            "agreement_score": "CodeBERTScore correlations with functional correctness (averaged/ per-language): Kendall-Tau τ — Java: 0.553, C++: 0.327, Python: 0.422, JavaScript: 0.319; Spearman r_s — Java: 0.369, C++: 0.393, Python: 0.415, JavaScript: 0.402. Overall, CodeBERTScore is more correlated with execution-correctness than baseline metrics on average across languages (values averaged across runs; stds reported in paper).",
            "high_agreement_conditions": "Using F3 (recall-weighted Fβ) for functional-correctness correlation experiments, language-specific pretrained CodeBERT models, and selecting appropriate hidden-layer embeddings (deeper layers up to a point) increased agreement with execution-based correctness.",
            "low_agreement_conditions": "Metrics that rely on exact lexical or syntactic match (BLEU, pure n-gram) and CodeBLEU can have low correlation with execution because valid implementations can differ in AST/dataflow or be partial/non-parsable; CrystalBLEU shows poor performance in some languages (e.g., very low in C++).",
            "artifact_complexity_effect": "Functional correctness correlation varies by language and likely by semantic complexity of tasks — CodeBERTScore performs better overall but shows lower τ in some languages (e.g., C++), indicating task/language specifics and implementation diversity affect agreement.",
            "criteria_clarity_effect": "Weighting recall (F3) when comparing to execution-based correctness improves alignment, suggesting that emphasizing presence of required semantic content (recall) increases agreement with execution outcomes.",
            "sample_size": 164,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "The paper compares metric scores to execution-based correctness (treated as objective ground truth) rather than to direct human raters in this experiment; CodeBERTScore shows higher correlation to execution than baselines, but no direct comparison to inter-human agreement is reported.",
            "calibration_or_training": "CodeBERTScore models received continued MLM pretraining on language-specific corpora; no supervised fine-tuning on execution labels or human judgments was performed for the evaluation metric itself (only hyperparameter selection).",
            "key_findings": "CodeBERTScore exhibits higher correlation with execution-based functional correctness than other commonly used metrics across multiple languages, particularly when using language-specific pretrained models, appropriate layer choice, and recall-weighted scoring (F3). Execution-based evaluation remains the most direct measure but is expensive and hazardous to run.",
            "limitations_noted": "Execution-based ground truth requires hand-written test suites (costly) and sandboxing for security; CodeBERTScore requires GPUs and depends on underlying model quality; correlations vary across languages and can be lower for some languages (e.g., C++), showing incomplete alignment in all cases.",
            "uuid": "e1796.1",
            "source_info": {
                "paper_title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of the bleu: how should we assess quality of the code generation models?",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "Crystalbleu: precisely and efficiently measuring the similarity of code",
            "rating": 2
        },
        {
            "paper_title": "Codebleu: a method for automatic evaluation of code synthesis",
            "rating": 2
        },
        {
            "paper_title": "BERTScore: Evaluating text generation with BERT",
            "rating": 1
        },
        {
            "paper_title": "T5score: Discriminative fine-tuning of generative evaluation metrics",
            "rating": 1
        }
    ],
    "cost": 0.013274999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code</h1>
<p>Shuyan Zhou<em> Uri Alon ${ }^{</em> \dagger}$ Sumit Agarwal Graham Neubig<br>Language Technologies Institute, Carnegie Mellon University<br>{shuyanzh, ualon, sumita, gneubig}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Since the rise of neural natural-language-tocode models (NL $\rightarrow$ Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than $\mathbf{1 , 0 0 0 , 0 0 0}$ times from the Huggingface Hub. ${ }^{\dagger}$</p>
<h2>1 Introduction</h2>
<p>Natural-language-to-code generation (NL $\rightarrow$ Code) has seen sharply growing popularity recently due to the emergence of large language models (LLMs) trained on vast amounts of natural language and code (Chen et al., 2021; Fried et al., 2022; Zhou et al., 2023; Austin et al., 2021; Allal et al., 2023). LLMs have reached such a high $\mathrm{NL} \rightarrow$ Code accuracy that they are now useful for the broad programming audience and actually save</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>developers' time when implemented in tools such as GitHub's Copilot. This sharp rise in LLMs' usability was achieved thanks to their ability to accurately generate long completions, which span multiple tokens and even lines, rather than only a single next-token as in early models (Allamanis and Sutton, 2013; Movshovitz-Attias and Cohen, 2013). Nevertheless, evaluating and comparing different models has remained a challenging problem (Xu et al., 2022) that requires an accurate and reliable evaluation metric for the quality of the models' generated outputs, and existing metrics are sub-optimal.</p>
<p>Existing evaluation approaches The most common evaluation metrics are token-matching methods such as BLEU (Papineni et al., 2002), adopted from natural language processing. These metrics are based on counting overlapping ngrams in the generated code and the reference code. CrystalBLEU (Eghbali and Pradel, 2022) extends BLEU by ignoring the 500 most occurring n-grams, arguing that they are trivially shared between the prediction and the reference. Nonetheless, both BLEU and CrystalBLEU rely on the lexical exact match of tokens, which does not account for diversity in implementation, variable names, and code conventions. Figure 1 shows an example: given the reference code in Figure 1(a), both BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) over the functionally equivalent code in Figure 1(c).</p>
<p>CodeBLEU (Ren et al., 2020) attempts to lower the requirement for a lexical exact match, by relying on data-flow and Abstract Syntax Tree (AST) matching as well; nevertheless, valid generations may have different ASTs and data flow from the reference code, which may lead to low CodeBLEU score even when the prediction is correct. Further, partial predictions may be useful for a program-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) The ground truth reference - find the index of target in this. elements.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Non-equivalent candidate:</th>
<th style="text-align: center;">Equivalent candidate:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">boolean f(Object target) {</td>
<td style="text-align: center;">int f(Object target) {</td>
</tr>
<tr>
<td style="text-align: center;">for (Object elem: this.elements) {</td>
<td style="text-align: center;">for (int i=0; i(this.elements.size(); i++) {</td>
</tr>
<tr>
<td style="text-align: center;">if (elem.equals(target)) {</td>
<td style="text-align: center;">Object elem = this.elements.get(i);</td>
</tr>
<tr>
<td style="text-align: center;">return true;</td>
<td style="text-align: center;">if (elem.equals(target)) {</td>
</tr>
<tr>
<td style="text-align: center;">}</td>
<td style="text-align: center;">return i;</td>
</tr>
<tr>
<td style="text-align: center;">return false;</td>
<td style="text-align: center;">}</td>
</tr>
<tr>
<td style="text-align: center;">(b) Preferred by BLEU \&amp; CrystalBLEU - find</td>
<td style="text-align: center;">(c) Preferred by CodeBERTScore - find the index of target in</td>
</tr>
<tr>
<td style="text-align: center;">whether or not target is in this.elements.</td>
<td style="text-align: center;">this.elements.</td>
</tr>
</tbody>
</table>
<p>Figure 1: An intuitive example for the usefulness of CodeBERTScore in measuring generated code: Figure 1(a) shows a reference code snippet in Java. Figure 1(b) and Figure 1(c) show two generated predictions. Among these two candidates and given the reference, both BLEU and CrystalBLEU prefer (score higher) the snippet in Figure 1(b), which is not functionally equivalent to the reference, while our proposed CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the code in Figure 1(a).
mer, but accepting them may lead to partial code that does not parse, and thus cannot be fully evaluated by CodeBLEU (for example, predicting the first line of a for loop, without the loop's body).</p>
<p>Execution-based evaluation attempts to address these problems by running tests on the generated code to verify its functional correctness (Chen et al., 2021; Athiwaratkun et al., 2022; Li et al., 2022; Wang et al., 2022; Lai et al., 2022). This provides a direct measure of the functionality of the generated code while being agnostic to diversity in implementation and style. However, execution-based evaluation requires datasets that are provided with hand-written test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. Additionally, executing model-generated code is susceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively.</p>
<p>Our approach In this work, we introduce CodeBERTScore, an evaluation metric for code generation, leveraging self-supervised pretrained models of code such as CodeBERT (Feng et al., 2020), and adopting best practices BERTScore (Zhang et al., 2020). First, CodeBERTScore encodes the generated code and the reference code independently with pretrained models, with the inclusion of natural language instructions or comments. Then, we compute the cosine similarity between the encoded representations of each token in the generated code and each token in the reference code. Finally, the best matching token vector pairs are used to compute precision and recall. CodeBERTScore allows comparing code pairs that are lexically different while taking into account the (1) programmatic- or natural-language-context, if such provided; the (2) contextual information of each token; and (3) implementation diversity. Our approach is illustrated in Figure 2.</p>
<p>Example A concrete example is shown in Figure 1: while BLEU and CrystalBLEU prefer (rank higher) the non-equivalent code in Figure 1(b) given the reference code in Figure 1(a), CodeBERTScore prefers the code in Figure 1(c), which is functionally equivalent to the reference (Figure 1(a)). We note that in this example, the variable names are identical across all three code snippets. When the variable names of the reference are different than the candidate's, it is even harder</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A diagram illustrating CodeBERTScore: We use a language-specific CodeBERT model to encode each of <em>natural_language</em>, <em>reference_code</em> and <em>natural_language</em>, <em>generated_code</em>. We then compute the pairwise cosine similarity between every encoded token in the reference and every encoded token in the generated code, ignoring the encoded natural language context tokens and encoded punctuation tokens; finally, we take the <em>max</em> across the rows of the resulting matrix to compute <em>Precision</em> and across columns to compute <em>Recall</em>.</p>
<p>For token-matching approaches such as BLEU and CrystalBLEU to compare the reference with the candidates, while CodeBERTScore can trivially match variable names according to their semantic similarity and their functional role in the code.</p>
<h3>Contributions</h3>
<p>In summary, our main contributions are: (a) CodeBERTScore: a self-supervised metric for NL→Code evaluation, based on BERTScore, which leverages the benefits of pretrained models, while not requiring labeling or manually annotated data. (b) An extensive empirical evaluation across four programming languages, showing that CodeBERTScore is more correlated with human preference <em>and</em> more correlated with execution correctness than all previous approaches including BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this submission, our models have been downloaded from the Huggingface Hub more than <strong>1,000,000</strong> times.</p>
<h2>2 Evaluating Generated Code</h2>
<h3>2.1 Problem Formulation</h3>
<p>Given a context <em>x</em> ∈ X (<em>e.g.,</em> a natural language instruction or comment), a code generation model <strong>M</strong> : <strong>X</strong> → <strong>Y</strong> produces a code snippet <em>ŷ</em> ∈ <strong>Y</strong> by conditioning on the intent specified by <em>x</em>. The quality of the generation is evaluated by comparing <em>ŷ</em> ∈ <strong>Y</strong> with the reference implementation <em>y</em><em> ∈ </em><em>Y</em><em>, using a metric function </em>f<em> : </em><em>Y</em><em> × </em><em>Y</em><em> → ℝ, essentially computing </em>f<em>(</em>ŷ<em>, </em>y**).</p>
<p>A larger value of <em>f</em>(<em>ŷ</em>, <em>y</em><em>) indicates that the generated code is more accurate with respect to the reference code, and the way </em>f<em> ranks different candidates is more important than the absolute value of </em>f<em>(</em>ŷ<em>, </em>y<strong>)<em>. That is, ideally, if a prediction </em>ŷ<em><sup>1</sup> is more functionally equivalent to </em>y</strong> and more preferable by human programmers over a prediction <em>ŷ</em><sup>2</sup>, we wish that a good metric would rank <em>ŷ</em><sup>1</sup> higher than <em>ŷ</em><sup>2</sup>. That is, we seek an <em>f</em> function such that <em>f</em>(<em>ŷ</em><sub>1</sub>, <em>y</em><em>) &gt; </em>f<em>(</em>ŷ<em><sub>2</sub>, </em>y**).</p>
<h3>2.2 Background: BERTScore</h3>
<p>BERTScore [zhang2020bertscore] was proposed as a method for evaluating mainly machine translation outputs. The idea in BERTScore is to encode the candidate sentence (the prediction) and the reference sentence (the ground truth) separately, using a BERT-based model, which encodes each sequence of tokens as a sequence of vectors. Then, BERTScore computes the cosine similarity between every vector from the candidate sequence and every vector from the reference sequences.</p>
<p>Given these similarity scores, BERTScore computes sentence-level precision by taking the maximum similarity score for every candidate vector and averaging, and computes recall by taking the average of the maximum similarity scores for every reference vector. Intuitively, a high BERTScore-recall is obtained, for example, if every vector from the reference sentence has at least one vector from the candidate sentence that is highly cosine-similar to it; a high BERTScore-precision is obtained if every vector from the candidate sentence is highly cosine-similar to at least one vector from the reference sentence. Ultimately, the final score is the F<sup>1</sup> score, computed as the harmonic mean of precision and recall.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">CodeBERTScore ${ }_{\mathrm{P}}$</th>
<th style="text-align: center;">$=\frac{1}{|\hat{y}[\hat{\boldsymbol{m}}]|} \sum_{\hat{y}<em y__i="y_{i">{j} \in \hat{y}[\hat{\boldsymbol{m}}]} \max </em>^{<em>} \in y^{</em>}\left[\boldsymbol{m}^{<em>}\right]} \operatorname{sim}\left(y_{i}^{</em>}, \hat{y}_{j}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CodeBERTScore ${ }_{\mathrm{R}}$</td>
<td style="text-align: center;">$=\frac{1}{\left</td>
</tr>
<tr>
<td style="text-align: center;">CodeBERTScore ${ }<em 1="1">{\mathrm{F}</em>$}</td>
<td style="text-align: center;">$=\frac{2 \cdot \text { CodeBERTScore }<em _mathrm_R="\mathrm{R">{\mathrm{P}} \cdot \text { CodeBERTScore }</em>}}}{\text { CodeBERTScore <em _mathrm_R="\mathrm{R">{\mathrm{P}}+\text { CodeBERTScore }</em>$}}</td>
</tr>
<tr>
<td style="text-align: center;">CodeBERTScore ${ }<em 3="3">{\mathrm{F}</em>$}</td>
<td style="text-align: center;">$=\frac{10 \cdot \text { CodeBERTScore }<em _mathrm_R="\mathrm{R">{\mathrm{P}} \cdot \text { CodeBERTScore }</em>}}}{9 \cdot \text { CodeBERTScore <em _mathrm_R="\mathrm{R">{\mathrm{P}}+\text { CodeBERTScore }</em>$}}</td>
</tr>
</tbody>
</table>
<p>Figure 3: Main equations for CodeBERTScore</p>
<h3>2.3 CodeBERTScore</h3>
<p>Our approach generally follows BERTScore, with the following main differences:</p>
<ol>
<li>We encode the context (the natural language instruction or comment) along with each of the generated and reference code snippets, but without using the encoded context in the final similarity computation, essentially computing $f\left(\hat{y}, y^{<em>}, x\right)$ rather than $f\left(\hat{y}, y^{</em>}\right)$.</li>
<li>Given the precision and recall, instead of computing the $\mathrm{F}<em 3="3">{1}$ score, we also compute $\mathrm{F}</em>$ to weigh recall higher than precision, following METEOR (Banerjee and Lavie, 2005).</li>
<li>As our underlying BERT-like model, we use programming language-specific models that we pretrain and release, rather than models that were intended for natural language only.
We use a BERT-like pretrained model $\mathcal{B}$ to encode the reference and candidate. In our experiments, $\mathcal{B}$ is a CodeBERT model that we further pretrained using the masked language modeling objective (Devlin et al., 2019) on languagespecific corpora, but $\mathcal{B}$ can be any transformerbased model which we have access to its internal hidden states.</li>
</ol>
<p>Token Representation We concatenate the context $x$ with each of the reference and the candidate, resulting in $x \cdot y^{*}$ and $x \cdot \hat{y}$. We use the tokenizer $\mathcal{T}_{\mathcal{B}}$ provided with the model $\mathcal{B}$ :</p>
<p>$$
\begin{aligned}
\mathcal{T}<em m="m">{\mathcal{B}}\left(x \cdot y^{<em>}\right) &amp; =\left\langle x_{1}, \ldots, x_{k}, y_{1}^{</em>}, \ldots, y</em>\right\rangle \
\mathcal{T}}^{*<em 1="1">{\mathcal{B}}(x \cdot \hat{y}) &amp; =\left\langle x</em>}, \ldots, x_{k}, \hat{y<em n="n">{1}, \ldots, \hat{y}</em>\right\rangle
\end{aligned}
$$</p>
<p>to get a sequences of tokens. We run a standard "forward pass" with the model $\mathcal{B}$ for each tok-
enized sequence, resulting in sequences of vectors:
$\mathcal{B}\left(\left\langle x_{1}, \ldots, x_{k}, y_{1}^{<em>}, \ldots, y_{m}^{</em>}\right\rangle\right)=\left\langle\boldsymbol{x}<em k="k">{1}, \ldots, \boldsymbol{x}</em>}, \boldsymbol{y<em 1="1">{1}^{<em>}, \ldots, \boldsymbol{y}_{m}^{</em>}\right\rangle$
$\mathcal{B}\left(\left\langle x</em>}, \ldots, x_{k}, \hat{y<em n="n">{1}, \ldots, \hat{y}</em>}\right\rangle\right)=\left\langle\boldsymbol{x<em k="k">{1}, \ldots, \boldsymbol{x}</em>}, \hat{\boldsymbol{y}<em n="n">{1}, \ldots, \hat{\boldsymbol{y}}</em>\right\rangle$
Finally, we mask out the encoded context tokens $\boldsymbol{x}<em k="k">{1}, \ldots, \boldsymbol{x}</em>^{}$ as well as all non-alphanumeric tokens (parentheses, brackets, dots, commas, whitespaces, etc.) except for arithmetic operators, from each of the encoded reference and encoded candidate. This results in encoded reference tokens $\boldsymbol{y<em>}=\left\langle\boldsymbol{y}_{1}^{</em>}, \ldots, \boldsymbol{y}_{m}^{<em>}\right\rangle$, encoded candidate tokens $\hat{\boldsymbol{y}}=$ $\left\langle\hat{\boldsymbol{y}}<em n="n">{1}, \ldots, \hat{\boldsymbol{y}}</em>^{}\right\rangle$, and their corresponding masks $\boldsymbol{m</em>}$ and $\hat{\boldsymbol{m}}$. We denote $\boldsymbol{y}[\boldsymbol{m}]$ as the remaining encoded tokens in $\boldsymbol{y}$ after selecting only alphanumeric token vectors according to the mask $\boldsymbol{m}$.</p>
<p>Similarity Computation We compute the cosine similarity between the encoded reference and candidate tokens, following Zhang et al. (2020):</p>
<p>$$
\operatorname{sim}\left(y_{i}^{<em>}, \hat{y}<em i="i">{j}\right)=\frac{\boldsymbol{y}</em>^{</em> \top} \cdot \hat{\boldsymbol{y}}<em i="i">{j}}{\left|\boldsymbol{y}</em>
$$}^{*}\right| \cdot\left|\hat{\boldsymbol{y}}_{j}\right|</p>
<p>Although this compares the individual tokens $y_{i}^{<em>}$ and $\hat{y}<em i="i">{j}$, their vector representations $\boldsymbol{y}</em>^{</em>}$ and $\hat{\boldsymbol{y}}_{j}$ contain information about their context, and thus about their semantic role in the code.</p>
<p>CodeBERTScore We use the similarity matrix (see Figure 2), formed by the similarity scores between $\boldsymbol{y}^{*}$ and $\hat{\boldsymbol{y}}$, to compute precision, recall, and $\mathrm{F}<em 3="3">{1}$, by taking the maximum across the rows and columns of the similarity matrix, and then averaging. Following Banerjee and Lavie (2005), we also compute $\mathrm{F}</em>$ by giving more weight to recall, as shown in Figure 3. Additional details regarding token weighting and scaling are provided in Appendix A.</p>
<p>3 Experimental Setup</p>
<p>We evaluate CodeBERTScore across multiple datasets and programming languages. We first show that CodeBERTScore is more correlated with human preference than previous metrics, using human-rated solutions for the CoNaLa dataset <em>Yin et al. (2018a); Evtikhiev et al. (2022)</em>. We then show that CodeBERTScore is more correlated with functional correctness, using the HumanEval dataset <em>Chen et al. (2021)</em>. We also show that CodeBERTScore achieves a higher newly proposed distinguishability than other metrics (Appendix F). Finally, we analyze some of the design decisions and their implications.</p>
<h3>3.1 Training Language-specific CodeBERT models</h3>
<p>Training We used CodeBERT <em>Feng et al. (2020)</em> as our base model (B) and continued its self-supervised pretraining <em>Gururangan et al. (2020)</em> with the masked language modeling (MLM) objective <em>Devlin et al. (2019)</em> on Python, Java, C++, C, and JavaScript corpora. We trained a separate model for each programming language, for 1,000,000 steps for each language, using a batch size of 32, an initial learning rate of $5e^{-5}$, decayed linearly to $3e^{-5}$. Our implementation is based on the widely used HuggingFace Transformers library <em>Wolf et al. (2019)</em> and BERTScore, and it supports any transformer-based model available on the HuggingFace hub.</p>
<p>Dataset We trained each model on the languagespecific subset of the CodeParrot <em>Tunstall et al. (2022)</em> dataset, which consists of overall 115M code files from GitHub, further filtered by keeping only files having average line length lower than 100, more than 25% alphanumeric characters, and non-auto-generated files. Even after 1,000,000 training steps, none of the models have completed even a single epoch, meaning that every training example was seen only once at most.</p>
<h3>3.2 Comparing Different Metrics</h3>
<p>We compare CodeBERTScore with existing metrics that are commonly used on code generation evaluation. We use human annotated preference and execution-based results as the ground truth and measure their correlation with these metrics.</p>
<p>[table]capposition=right,top</p>
<p>Correlation metrics We used three major correlation metrics. Following best practices in natural language evaluation, we used Kendall-Tau $(\tau)$, Pearson $\left(r_{p}\right)$ and Spearman $\left(r_{s}\right)$ to measure the correlation between each metric’s scores and the references. The detailed equations can be found in Appendix C.</p>
<p>Human preference experiments We evaluate different metrics on CoNaLa <em>Yin et al. (2018b)</em>, a natural language to Python code generation benchmark collected from StackOverflow. We use the human annotation of <em>Evtikhiev et al. (2022)</em> to measure the correlation between each metric and human preference. More details are provided in Appendix B.1.</p>
<p>Functional correctness experiments We evaluate functional correctness using the HumanEval <em>Chen et al. (2021)</em> benchmark. Each example in HumanEval contains a natural language goal, hand-written input-output test cases, and a human-written reference solution. While the original HumanEval is in Python, <em>Cassano et al. (2022)</em> translated HumanEval to 18 programming languages, and provided the predictions of the Codex model <em>Chen et al. (2021)</em> (code-davinci-002) and their corresponding functional correctness. We used Java, C++, Python, and JavaScript for these experiments, which are some of the most popular programming languages in open-source projects. More details are provided in Appendix B.2.</p>
<p>Hyperparameters We tuned only the following hyperparameters for CodeBERTScore: whether to use $\mathrm{F}<em 3="3">{1}$ or $\mathrm{F}</em>}$, and which layer of the underlying model to extract the encoded tokens from, which we examine in Section 5. We used $\mathrm{F<em 3="3">{1}$ in the human preference experiments and $\mathrm{F}</em>$ in the functional correctness experiments. We perform 3-fold cross-validation and report average results across the three folds. As for the layer to extract the token vectors from, we used layer 7 for CoNaLa, and in HumanEval we used layer 7 for Java, 10 for C++, 11 for JavaScript, and 9 for Python.</p>
<h2>4 Results</h2>
<p>Correlation with human preference Table 2 shows the correlation between different metrics</p>
<p>[table]capposition=right,top</p>
<p>[table]capposition=top</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<p>[table]capposition=left</p>
<table>
<thead>
<tr>
<th></th>
<th>Java</th>
<th></th>
<th>C++</th>
<th></th>
<th>Python</th>
<th></th>
<th>JavaScript</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
</tr>
<tr>
<td>BLEU</td>
<td>.481</td>
<td>.361</td>
<td>.112</td>
<td>.301</td>
<td>.393</td>
<td>.352</td>
<td>.248</td>
<td>.343</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>.496</td>
<td>.324</td>
<td>.175</td>
<td>.201</td>
<td>.366</td>
<td>.326</td>
<td>.261</td>
<td>.299</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>.516</td>
<td>.318</td>
<td>.262</td>
<td>.260</td>
<td>.368</td>
<td>.334</td>
<td>.279</td>
<td>.280</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>.525</td>
<td>.315</td>
<td>.270</td>
<td>.273</td>
<td>.365</td>
<td>.322</td>
<td>.261</td>
<td>.292</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>.508</td>
<td>.344</td>
<td>.258</td>
<td>.288</td>
<td>.338</td>
<td>.350</td>
<td>.271</td>
<td>.293</td>
</tr>
<tr>
<td>METEOR</td>
<td>.558</td>
<td>.383</td>
<td>.301</td>
<td>.321</td>
<td>.418</td>
<td>.402</td>
<td>.324</td>
<td>.415</td>
</tr>
<tr>
<td>chrF</td>
<td>.532</td>
<td>.319</td>
<td>.319</td>
<td>.321</td>
<td>.394</td>
<td>.379</td>
<td>.302</td>
<td>.374</td>
</tr>
<tr>
<td>CrystalBLEU</td>
<td>.471</td>
<td>.273</td>
<td>.046</td>
<td>.095</td>
<td>.391</td>
<td>.309</td>
<td>.118</td>
<td>.059</td>
</tr>
<tr>
<td>CodeBERTScore</td>
<td>.553</td>
<td>.369</td>
<td>.327</td>
<td>.393</td>
<td>.422</td>
<td>.415</td>
<td>.319</td>
<td>.402</td>
</tr>
</tbody>
</table>
<p>Table 1: Kendall-Tau $(\tau)$ and Spearman $\left(r_{s}\right)$ correlations of each metric with the functional correctness on HumanEval in multiple languages. The correlation coefficients are reported as the average across three runs. Standard deviation is provided in Table 3.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>$\tau$</th>
<th>$r_{p}$</th>
<th>$r_{s}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU</td>
<td>.374</td>
<td>.604</td>
<td>.543</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>.350</td>
<td>.539</td>
<td>.495</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>.397</td>
<td>.604</td>
<td>.570</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>.429</td>
<td>.629</td>
<td>.588</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>.420</td>
<td>.619</td>
<td>.574</td>
</tr>
<tr>
<td>METEOR</td>
<td>.366</td>
<td>.581</td>
<td>.540</td>
</tr>
<tr>
<td>chrF</td>
<td>.470</td>
<td>.635</td>
<td>.623</td>
</tr>
<tr>
<td>CrystalBLEU</td>
<td>.411</td>
<td>.598</td>
<td>.576</td>
</tr>
<tr>
<td>CodeBertScore</td>
<td>.517</td>
<td>.674</td>
<td>.662</td>
</tr>
</tbody>
</table>
<p>Table 2: The Kendall-Tau $(\tau)$, Pearson $\left(r_{p}\right)$ and Spearman $\left(r_{s}\right)$ correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Standard deviations are provided in Table 4. and human preference. CodeBERTScore achieves the highest correlation with human preference, across all correlation metrics. While Evtikhiev et al. (2022) suggested that chrF and ROUGE-L are the most suitable metrics for evaluating code generation models in CoNaLa, CodeBERTScore outperforms these metrics by a significant margin. For example, CodeBERTScore achieves Kendall-Tau correlation of 0.517 compared to 0.470 of chrF and 0.420 of ROUGE-L. These results show that generated code that is preferred by CodeBERTScore- also tends to be preferred by human programmers.</p>
<p>Correlation with functional correctness Table 1 shows the correlation between different metrics and functional correctness: CodeBERTScore achieves the highest or comparable Kendall-Tau and Spearman correlation with functional correctness across all four languages. METEOR achieves a comparable correlation with CodeBERTScore in Java and JavaScript, and its correlation is surprisingly better than other baseline metrics. However, in C++ and Python, CodeBERTScore is strictly better. Overall on average across languages, CodeBERTScore is more correlated with functional correctness than all baselines.</p>
<h2>5 Analysis</h2>
<p>We conducted a series of additional experiments to understand the importance of different design decisions, and to gain insights on applying CodeBERTScore to new datasets and scenarios.</p>
<p>Can we use CodeBERTScore in a new language without a language-specific CodeBERT? In all experiments in Section 4, we used the language-specific model which we continued to pretrain on each language. But what if we wish to use CodeBERTScore in a language in which we don't have a language-specific model? We compare the language-specific models to CodeBERTbase in Figure 4. Generally, CodeBERT-base achieves close performance to a language-specific model. However, in most HumanEval experiments and correlation metrics, using the languagespecific model is beneficial. These results show that language-specific models are often preferred if such models are available, but the CodeBERTbase can still provide close performance even without language-specific pretraining.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The Kendall-Tau and Spearman on the development set of different datasets with the language-specific pretrained model (Lang-specific) and with the base CodeBERT (Base model).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The average of Kendall-Tau and Spearman on the development set of HumanEval when using the embeddings from different layers.</p>
<p><strong>Which transformer layer should we use?</strong> We further investigate the impact of using hidden states from different layers of the model — the layer which the vectors in Equation (2) come from, in the computation of CodeBERTScore. The results are shown in Figure 5: generally, the deeper the layer – the higher the average correlation between CodeBERTScore and functional correctness, across all programming languages. However, in almost all languages, performance reaches its maximum before the last layer, and decreases at the following layers. This suggests that higher layers encode the semantic information of each token more accurately, but the final layers may be more task-specific. These observations are consistent with Tenney et al. (2019), who found that lower layers in BERT tend to process shallow information, while higher layers encode deeper semantic meaning in natural language.</p>
<p><strong>Does encoding natural language context help?</strong> One major difference between CodeBERTScore and BERTScore is that CodeBERTScore leverages the <em>context</em> for the generated code, such as the natural language instruction or intent that was given as input for generation. We find that using context increases the correlation, for example, the Kendall-Tau of CodeBERTScore from 0.50 to 0.52. While this paper mainly focuses on natural language instructions, we believe that CodeBERTScore can thus benefit other programming scenarios as well, for example when generating code given the human-written comments, or generating code given the preceding code context.</p>
<p><strong>CodeBERTScore allows soft matching of tokens</strong> The heatmaps in Figure 6 show the similarity scores between tokens in CodeBERTScore. For example, both shutil.rmtree and os.rmdir in Figure 6(a) delete a folder; CodeBERTScore aligns each token to a respective token in the other expression, even though the two spans do not share many identical tokens.</p>
<p>In Figure 6(b), both code snippets calculate a square root, where one uses math.sqrt(x) and the other uses x ** 0.5. An exact surface-form-matching metric such as chrF would assign a low similarity score to this code pair, as they only share the token x. However, CodeBERTScore assigns non-zero scores to each token with meaningful alignments, such as matching [sq, rt] with [_0, 5], since a square root is the 0.5-th power.</p>
<p>Additionally, we study the robustness of CodeBERTScore to adversarial perturbations. We found that token-based metrics such as chrF are</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Heatmaps of the similarity scores between two pieces of code that achieve the same goal. Figure 6(a) shows the similarity scores between os.rmdir(folder) and shutil.rmtree(folder). Figure 6(b) shows the similarity scores between math.sqrt(x) and x ** 0.5.</p>
<p>Much more prone to matching trivial tokens rather than tokens that preserve the semantic meaning of the code. Examples can be found in Appendix E.</p>
<p>Additional discussion and experiments regarding the distinguishability of CodeBERTScore are provided in Appendix F. Additional general examples are provided in Appendix G.</p>
<h2>6 Related Work</h2>
<p><strong>Token-based metrics</strong> Metrics such as BLEU [papineni2002bleu] evaluate code generation by counting matching n-grams between generated and reference code. CrystalBLEU [eghbali2022crystalBLEU] refines this approach by disregarding trivially shared n-grams, while ROUGE [lin2004rouge] and METEOR [banerjee2005metre] emphasize recall and balance of precision and recall respectively. However, these metrics, relying on <em>exact</em> lexical matches, often fail to capture semantically equivalent but lexically different code snippets. Unlike these, CodeBERTScore captures the wide, two-sided <em>context</em> of each token, which n-grams cannot capture.</p>
<p><strong>Static analysis-based metrics</strong> CodeBLEU [ren2020codeBLEU] incorporates data-flow and Abstract Syntax Tree (AST) matching, in addition to token-matching. However, valid code may not always align in ASTs and data-flows. Additionally, partial code, although potentially useful, may not parse, thus cannot be fully evaluated by CodeBLEU. Further, as highlighted by subsequent studies [wang2022codeBLEU], CodeBLEU does not correlate well with execution accuracy.</p>
<p><strong>Execution-based Metrics</strong> To alleviate previous issues, execution-based evaluation counts a generated code snippet as correct if it produces the required outputs when run with given inputs [chen2021codebertscore; athiwaratkun2022codebertscore; li2022codebertscore; wang2022codebertscore; lai2022codebertscore; huang2022codebertscore]. However, execution-based evaluation requires datasets that are provided with manually crafted test cases for each example, which is costly and labor-intensive to create; thus, only few such datasets exist. In contrast, CodeBERTScore is completely unsupervised and does not depend on any specific dataset. Further, executing model-generated code is susceptible to security threats, and thus should be run in an isolated sandbox, which makes it technically cumbersome to work with iteratively.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we present CodeBERTScore, a simple evaluation metric for code generation, which builds on BERTScore [zhang2020bertscore], using pretrained language models of code, and leveraging the natural language context of the generated code. We perform an extensive evaluation across four programming languages which shows that CodeBERTScore is more correlated with human preference than all prior metrics. Further, we show that generated code that receives a higher score by CodeBERTScore is more likely to function correctly when executed. Finally, we</p>
<p>release five programming language-specific pretrained models to use with our publicly available code. These models were downloaded more than 1,000,000 times from the HuggingFace Hub. Our code and data are available at https://github.com/ neulab/code-bert-score.</p>
<h2>Acknowledgement</h2>
<p>We thank Misha Evtikhiev, Egor Bogomolov, and Timofey Bryksin for the discussions, and for the data from their paper (Evtikhiev et al., 2022). We thank anonymous reviewers for the valuable feedback. We are grateful to Yiwei Qin for the discussions regarding the T5Score paper (Qin et al., 2022); the idea to use functional correctness as a meta-metric was born thanks to the discussion with her. We are also grateful to Aryaz Eghbali and Michael Pradel for the discussions about CrystalBLEU (Eghbali and Pradel, 2022). This material is partly based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government. This project was also partially supported by a gift from AWS AI.</p>
<h2>Limitations</h2>
<p>CodeBERTScore requires a GPU for computing the metric, while traditional metrics such as BLEU require only a CPU. This adds a hardware requirement to the evaluation of models of code, while most previous approaches are computationally cheaper (e.g., by counting n-grams). However, since training and testing neural models require GPU anyways, we can safely assume that a GPU is available. Further, BERT-base models are encoder-only and non-autoregressive; this means that they require only a single "forward pass", compared to encoder-decoder models (e.g., T5) and decoder-only models (e.g., GPT-3) that need to autoregressively generate token after token, using a forward pass for each output token. Thus, the additional time consumption by encoder-only models (e.g., BERT) is negligible, especially when
evaluating encoder-decoder or decoder-only as the $\mathrm{NL} \rightarrow$ Code generator models.</p>
<p>Another point to consider is that CodeBERTScore relies on a strong underlying BERTbased model, while methods such as BLEU do not have many "moving parts" or hyperparameters to tune. However, this is mostly an advantage, since CodeBERTScore can be further improved in the future using stronger base models.</p>
<h2>References</h2>
<p>Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988.</p>
<p>Miltiadis Allamanis and Charles Sutton. 2013. Mining source code repositories at massive scale using language modeling. In 2013 10th working conference on mining software repositories (MSR), pages 207216. IEEE.</p>
<p>Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual evaluation of code generation models. ArXiv preprint, abs/2210.14868.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. ArXiv preprint, abs/2108.07732.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. A scalable and extensible approach to benchmarking nl2code for 18 programming languages. ArXiv preprint, abs/2208.08227.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Aryaz Eghbali and Michael Pradel. 2022. Crystalbleu: precisely and efficiently measuring the similarity of code. In 37th IEEE/ACM International Conference on Automated Software Engineering, pages 1-12.</p>
<p>Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, and Timofey Bryksin. 2022. Out of the bleu: how should we assess quality of the code generation models? ArXiv preprint, abs/2208.03133.</p>
<p>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1536-1547, Online. Association for Computational Linguistics.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wentau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A generative model for code infilling and synthesis. ArXiv preprint, abs/2204.05999.</p>
<p>Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics.</p>
<p>Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin Clement, and Nan Duan. 2022. Execution-based evaluation for data science code generation models. In Proceedings of the Fourth Workshop on Data Science with Human-in-the-Loop (Language Advances), pages 28-36, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. Ds-1000: A natural and reliable benchmark for data science code generation. ArXiv preprint, abs/2211.11501.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin</p>
<p>Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Dana Movshovitz-Attias and William Cohen. 2013. Natural language models for predicting programming comments. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 35-40.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5score: Discriminative fine-tuning of generative evaluation metrics. arXiv preprint arXiv:2212.05726.</p>
<p>Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. ArXiv preprint, abs/2009.10297.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2022. Natural Language Processing with Transformers. " O'Reilly Media, Inc.".</p>
<p>Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022. Execution-based evaluation for open-domain code generation. ArXiv preprint, abs/2212.10481.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771.</p>
<p>Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. 2022. A systematic evaluation of large language models of code.</p>
<p>Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018a. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Mining Software Repositories, MSR, pages 476-486. ACM.</p>
<p>Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018b. Learning to mine aligned code and natural language pairs from stack overflow. In Proceedings of the 15th International Conference on Mining Software Repositories, pages 476-486.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. ArXiv preprint, abs/2210.02414.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. In International Conference on Learning Representations (ICLR), Kigali, Rwanda.</p>
<h2>Appendix A Additional Details</h2>
<p>$\mathbf{F}<em 1="1">{\beta}$ The well-known $\mathrm{F}</em>$ score is computed as:</p>
<p>$$
F_{1}=\frac{2}{\frac{1}{\text { recall }}+\frac{1}{\text { precision }}}=\frac{2 \cdot \text { precision } \cdot \text { recall }}{\text { precision }+ \text { recall }}
$$</p>
<p>A more general F score $F_{\beta}$ uses a positive factor $\beta$, where recall is considered $\beta$ times as important as precision:</p>
<p>$$
F_{\beta}=\frac{\left(1+\beta^{2}\right) \cdot \text { precision } \cdot \text { recall }}{\beta^{2} \cdot \text { precision }+ \text { recall }}
$$</p>
<p>As found in METEOR (Banerjee and Lavie, 2005), using $\mathrm{F}_{\beta}$ with $\beta=3$, thus preferring recall over precision, results in a higher correlation with human preference in machine translation. In our experiments, we found that this applies to NL $\rightarrow$ Code as well.</p>
<p>Token Weighting Following Zhang et al. (2020), we compute the inverse document frequency (idf), according to a language-specific test set, and weigh each token according to its negative log frequency.</p>
<p>Scaling Following Zhang et al. (2020), the cosine similarity scores of hidden states tend to lie in a limited range. Thus, we can linearly scale the resulting scores, using an empirical base scalar $b$ :</p>
<p>$$
\text { Code } \widehat{\mathrm{BERT}} \mathrm{Score}=\frac{\text { CodeBERTScore }-b}{1-b}
$$</p>
<p>This typically spreads the CodeBERTScore $\mathrm{F}<em _Java="{Java" _text="\text">{1}$ scores to the $[0,1]$ range, and is merely a cosmetical change: this scaling does not change the way CodeBERTScore ranks different prediction, but can be slightly more intuitive and easier to interpret. We computed $b$ empirically by sampling random unrelated code pairs and measuring their average similarity score. For Java, the empirical $b</em>$ it was 0.76 .}}$ was 0.78 and for $\mathrm{C}++$, $b_{\mathrm{C}++</p>
<h2>B Evaluation Details</h2>
<h2>B. 1 Human Preference</h2>
<p>For each example, Evtikhiev et al. (2022) asked experienced software developers to grade the generated code snippets from five different models. The grade scales from zero to four, with zero denoting that the generated code is irrelevant and unhelpful, and four meaning that the generated code solves the problem accurately. Overall, there are</p>
<p>2860 annotated code snippets ( 5 generations $\times$ 472 examples) where each snippet is graded by 4.5 annotators.</p>
<h2>B. 2 Functional Correctness</h2>
<p>We evaluate functional correctness using the HumanEval (Chen et al., 2021) benchmark. Each example in HumanEval contains a natural language goal, hand-written input-output test cases, and a human-written reference solution. On average, each example has 7.7 test cases and there are 164 examples in total. While the original HumanEval is in Python, Cassano et al. (2022) translated HumanEval to 18 programming languages, and provided the predictions of the Codex model (Chen et al., 2021) (code-davinci-002) and their corresponding functional correctness. ${ }^{6}$ We used Java, C++, Python, and JavaScript for these experiments, which are some of the most popular programming languages in open-source projects. ${ }^{7}$ Notably, Cassano et al. (2022) did not translate the reference solutions to the other languages, so, we collected these from HumanEval-X (Zeng et al., 2022). ${ }^{8}$ The reference score of every example is either 1 ("correct", if it passes all test cases) or 0 ("incorrect", otherwise).</p>
<h2>C Correlation Metrics</h2>
<p>Kendall-Tau $(\tau) \quad \tau$ measures the ordinal/rank association between a metric such as CodeBERTScore and the reference measurement. It is calculated as:</p>
<p>$$
\tau=\frac{|\text { concordant }|-|\text { discordant }|}{|\text { concordant }|+|\text { discordant }}}
$$</p>
<p>where |concordant| represents the number of pairs where two measurements agree on their relative rank. That is, if $f\left(\hat{y_{1}}, y_{1}^{<em>}\right)&gt;f\left(\hat{y_{2}}, y_{2}^{</em>}\right)$, the reference measurement also yields $f^{<em>}\left(\hat{y_{1}}, y_{1}^{</em>}\right)&gt;$ $f^{<em>}\left(\hat{y_{2}}, c_{2}^{</em>}\right)$. Similarly, |discordant| represents the number of pairs where two measurements yield opposite ranks. Notably, in our experiments, we restrict the comparisons of ranks within the generations of the same question.</p>
<p>Pearson $\left(r_{p}\right) \quad r_{p}$ measures the linear correlation between a metric and the reference measurement.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>It is defined as:</p>
<p>$r_{p}=\frac{\sum_{i=1}^{N}\left(f\left(\hat{y}<em i="i">{i},y</em>^{<em>}\right)-\hat{f}\right)\left(f^{</em>}\left(\hat{y}<em i="i">{i},y</em>^{<em>}\right)-\hat{f^{</em>}}\right)}{\sqrt{\sum_{i=1}^{N}\left(f\left(\hat{y}<em i="i">{i},y</em>^{<em>}\right)-\hat{f}\right)^{2}\sum_{i=1}^{N}\left(f^{</em>}\left(\hat{y}<em i="i">{i},y</em>^{<em>}\right)-\hat{f^{</em>}}\right)^{2}}}$</p>
<p>where $N$ is the number of generations in the dataset, $\hat{f}$ is the mean CodeBERTScore of the dataset, and $\hat{f^{*}}$ is the mean similarity score calculated by the reference measurement.</p>
<p>Spearman $\left(r_{s}\right)$ $r_{s}$ measures the Pearson correlation coefficient between the ranks produced by a metric and the reference measurement:</p>
<p>$r_{p}=\frac{\operatorname{cov}(R(f(\hat{\mathbf{Y}}),R(f^{<em>}(\mathbf{Y}^{</em>})))}{\sigma_{R(f(\mathbf{Y}))^{\boldsymbol{P}}R(f^{<em>}(\mathbf{Y}^{</em>}))}}$</p>
<p>where $R$ returns the ranks of code snippets in a collection of code snippets $\mathbf{Y} . \operatorname{cov}(\cdot, \cdot)$ is the covariance of two variables and $\sigma(\cdot)$ is the standard deviation.</p>
<h2>D Standard Deviation</h2>
<p>Table 3 shows the same results as in Table 1, but with standard deviations. Table 4 shows the results from Table 2, with standard deviations.</p>
<h2>E Robustness to adversarial perturbations</h2>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Candidate</td>
<td>CodeBERTScore</td>
<td>chrF</td>
</tr>
<tr>
<td>os.rmdir(folder)</td>
<td>1st</td>
<td>1st</td>
</tr>
<tr>
<td>os.rmdir(f)</td>
<td>2nd</td>
<td>3rd</td>
</tr>
<tr>
<td>(folder)</td>
<td>3rd</td>
<td>2nd</td>
</tr>
</tbody>
</table>
<p>Figure 7: The similarity rankings of three code snippets given the reference code shutil.rmtree(folder). While CodeBERTScore correctly ranks os.rmdir(f) over the the non-equivalent (folder), chrF prefers just (folder) over os.rmdir(f).</p>
<p>We conducted a qualitative evaluation of CodeBERTScore under various perturbations. An example is shown in Figure 7, which shows the CodeBERTScore and chrF rankings of three code snippets based on the similarity to the reference shutil.rmtree(folder). CodeBERTScore gives a higher ranking to the code snippet that employs the appropriate API (os.rmdir) than the trivial (folder) that has the same variable name but without any function call. Contrarily, chrF assigns a higher ranking to (folder) which has a longer common sequence of characters, although semantically inequivalent.</p>
<h2>F Distinguishing Code with Different Semantics</h2>
<p>We study how well can CodeBERTScore perform as a generic similarity function that measures the similarity between two arbitrary code snippets $y_{i}$ and $y_{j}$.</p>
<h2>F.1 Distinguishability Metric</h2>
<p>We evaluate CodeBERTScore using the distinguishability metric $d$ proposed by Eghbali and Pradel (2022) which is calculated as follows:</p>
<p>$$ d=\frac{\sum_{y_{i}, y_{j} \in \text { Pairs }<em i="i">{\text {intra }}} f\left(y</em>}, y_{j}\right)}{\sum_{y_{i}, y_{j} \in \text { Pairs <em i="i">{\text {inter }}} f\left(y</em> $$}, y_{j}\right)</p>
<p>where Pair ${ }_{\text {intra }}$ defines a set of code pairs from the same semantically equivalent clusters, and Pair inter defines a set of code pairs from two clusters of different functionality. Formally,</p>
<p>Pair intra $=\left{\left(y_{i}, y_{j}\right) \mid \exists k\right.$ such that $\left.y_{i}, y_{j} \in C_{k}\right}$ $\operatorname{Pair}<em i="i">{\text {inter }}=\left{\left(y</em>$ is the $k$-th cluster with semantically equivalent code snippets. Intuitively, a similarity function $f$ that can distinguish between similar and dissimilar code will produce $d$ larger than 1 , meaning that a pair of code snippets from the same semantic cluster has a higher similarity score than a pair of snippets from different clusters. Since the number of intra-class and inter-class pairs grows quadratically with the number of code snippets, in our experiments we followed Eghbali and Pradel (2022) to sample $N$ inter- and $N$ intra-class pairs instead.}, y_{j}\right) \mid \exists k\right.$ such that $\left.y_{i} \in C_{k}, y_{j} \notin C_{k}\right}$ where $C_{k</p>
<h2>F. 2 Dataset with Semantically equivalent clusters</h2>
<p>We follow Eghbali and Pradel (2022) to evaluate whether CodeBERTScore can distinguish similar and dissimilar code mined from ShareCode ${ }^{9}$, an online coding competition platform. Semantically equivalent code snippets are from the same coding problem, and they all pass the unit tests provided by the platform. The dataset consists 6958 code</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{9}$ https://sharecode.io/</p>
<table>
<thead>
<tr>
<th></th>
<th>Java</th>
<th></th>
<th>C++</th>
<th></th>
<th>Python</th>
<th></th>
<th>JavaScript</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
<td>$\tau$</td>
<td>$r_{s}$</td>
</tr>
<tr>
<td>BLEU</td>
<td>$.481(\pm .030)$</td>
<td>$.361(\pm .037)$</td>
<td>$.112(\pm .059)$</td>
<td>$.301(\pm .054)$</td>
<td>$.393(\pm .083)$</td>
<td>$.352(\pm .064)$</td>
<td>$.248(\pm .075)$</td>
<td>$.343(\pm .052)$</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>$.496(\pm .034)$</td>
<td>$.324(\pm .037)$</td>
<td>$.175(\pm .021)$</td>
<td>$.201(\pm .037)$</td>
<td>$.366(\pm .079)$</td>
<td>$.326(\pm .075)$</td>
<td>$.261(\pm .065)$</td>
<td>$.299(\pm .043)$</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>$.516(\pm .052)$</td>
<td>$.318(\pm .043)$</td>
<td>$.262(\pm .073)$</td>
<td>$.260(\pm .024)$</td>
<td>$.368(\pm .092)$</td>
<td>$.334(\pm .054)$</td>
<td>$.279(\pm .092)$</td>
<td>$.280(\pm .068)$</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>$.525(\pm .049)$</td>
<td>$.315(\pm .047)$</td>
<td>$.270(\pm .073)$</td>
<td>$.273(\pm .036)$</td>
<td>$.365(\pm .094)$</td>
<td>$.322(\pm .077)$</td>
<td>$.261(\pm .077)$</td>
<td>$.292(\pm .057)$</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>$.508(\pm .060)$</td>
<td>$.344(\pm .038)$</td>
<td>$.258(\pm .091)$</td>
<td>$.288(\pm .027)$</td>
<td>$.338(\pm .103)$</td>
<td>$.350(\pm .064)$</td>
<td>$.271(\pm .078)$</td>
<td>$.293(\pm .046)$</td>
</tr>
<tr>
<td>METEOR</td>
<td>$\mathbf{. 5 5 8}(\pm .058)$</td>
<td>$\mathbf{. 3 8 3}(\pm .027)$</td>
<td>$.301(\pm .061)$</td>
<td>$.321(\pm .023)$</td>
<td>$.418(\pm .090)$</td>
<td>$.402(\pm .049)$</td>
<td>$\mathbf{. 3 2 4}(\pm .075)$</td>
<td>$\mathbf{. 4 1 5}(\pm .022)$</td>
</tr>
<tr>
<td>chrF</td>
<td>$.532(\pm .067)$</td>
<td>$.319(\pm .035)$</td>
<td>$.319(\pm .056)$</td>
<td>$.321(\pm .020)$</td>
<td>$.394(\pm .096)$</td>
<td>$.379(\pm .058)$</td>
<td>$.302(\pm .073)$</td>
<td>$.374(\pm .044)$</td>
</tr>
<tr>
<td>CrystalBLEU</td>
<td>$.471(\pm .024)$</td>
<td>$.273(\pm .067)$</td>
<td>$.046(\pm .009)$</td>
<td>$.095(\pm .064)$</td>
<td>$.391(\pm .080)$</td>
<td>$.309(\pm .073)$</td>
<td>$.118(\pm .057)$</td>
<td>$.059(\pm .069)$</td>
</tr>
<tr>
<td>CodeBERTScore</td>
<td>$\mathbf{. 5 5 3}(\pm .068)$</td>
<td>$.369(\pm .049)$</td>
<td>$\mathbf{. 3 2 7}(\pm .086)$</td>
<td>$\mathbf{. 3 9 3}(\pm .048)$</td>
<td>$\mathbf{. 4 2 2}(\pm .090)$</td>
<td>$\mathbf{. 4 1 5}(\pm .071)$</td>
<td>$\mathbf{. 3 1 9}(\pm .054)$</td>
<td>$.402(\pm .030)$</td>
</tr>
</tbody>
</table>
<p>Table 3: Kendall-Tau $(\tau)$ and Spearman $\left(r_{s}\right)$ correlations of each metric with the functional correctness on HumanEval in multiple languages. The correlation coefficients are reported as the average across three runs, along with the standard deviation.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>$\tau$</th>
<th>$r_{p}$</th>
<th>$r_{s}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU</td>
<td>$.374(\pm .025)$</td>
<td>$.604(\pm .016)$</td>
<td>$.543(\pm .018)$</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>$.350(\pm .037)$</td>
<td>$.539(\pm .033)$</td>
<td>$.495(\pm .037)$</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>$.397(\pm .023)$</td>
<td>$.604(\pm .016)$</td>
<td>$.570(\pm .018)$</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>$.429(\pm .025)$</td>
<td>$.629(\pm .015)$</td>
<td>$.588(\pm .022)$</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>$.420(\pm .037)$</td>
<td>$.619(\pm .014)$</td>
<td>$.574(\pm .022)$</td>
</tr>
<tr>
<td>METEOR</td>
<td>$.366(\pm .033)$</td>
<td>$.581(\pm .016)$</td>
<td>$.540(\pm .022)$</td>
</tr>
<tr>
<td>chrF</td>
<td>$.470(\pm .029)$</td>
<td>$.635(\pm .023)$</td>
<td>$.623(\pm .018)$</td>
</tr>
<tr>
<td>CrystalBLEU</td>
<td>$.411(\pm .030)$</td>
<td>$.598(\pm .019)$</td>
<td>$.576(\pm .034)$</td>
</tr>
<tr>
<td>CodeBertScore</td>
<td>$\mathbf{. 5 1 7}(\pm .024)$</td>
<td>$\mathbf{. 6 7 4}(\pm .012)$</td>
<td>$\mathbf{. 6 6 2}(\pm .012)$</td>
</tr>
</tbody>
</table>
<p>Table 4: The Kendall-Tau $(\tau)$, Pearson $\left(r_{p}\right)$ and Spearman $\left(r_{s}\right)$ correlation with human preference. The best performance is bold. The correlation coefficients are reported as the average across three runs. Numbers inside parentheses indicate the standard deviations.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Java</th>
<th>C++</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLEU</td>
<td>2.36</td>
<td>2.51</td>
</tr>
<tr>
<td>CodeBLEU</td>
<td>1.44</td>
<td>1.42</td>
</tr>
<tr>
<td>CrystalBLEU</td>
<td>5.96</td>
<td>6.94</td>
</tr>
<tr>
<td>CodeBERTScore</td>
<td>$\mathbf{9 . 5 6}$</td>
<td>$\mathbf{9 . 1 3}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Distinguishability with different metrics as the similarity function. CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, on the same datasets.
snippets covering 278 problems in Java and C++. We use CodeBERTScore to calculate the similarity score for code pairs that share the same semantic class and code pairs that do not. We then measure the distinguishability of CodeBERTScore according to Equation 7. The results are shown in Table 5.</p>
<p>Table 5 shows that CodeBERTScore achieves a higher distinguishability than CrystalBLEU, which proposed this meta-metric, in both Java and C++. CodeBERTScore achieves distinguishability scores of 9.56 in Java while CrystalBLEU achieves 5.96; in C++, CodeBERTScore achieves 9.13 while CrystalBLEU achieves only 6.94.</p>
<p>This result confirms that CodeBERTScore assigns higher similarity scores to semantically similar code pairs, compared to randomly paired snippets that belong to different semantic classes.</p>
<h2>Can We Hack the Distinguishability Metric?</h2>
<p>Despite the encouraging results in Table 5, we also found that distinguishability can be easily manipulated since it compares absolute scores across different metrics. For example, while CrystalBLEU achieves a distinguishability score of 5.96, we can craft a variant of CodeBERTScore that achieves a distinguishability score of 120,000 by simple exponentiation of CodeBERTScore's output score.</p>
<p>To illustrate this, we conducted a distinguishability evaluation with the same configurations as before, but with a variant of CodeBERTScore that we call CodeBERTScore ${ }^{k}$, and defined as the composition of CodeBERTScore with the $f(x)=$ $x^{k}$ function, that is: $\operatorname{CodeBERTScore}^{k}\left(y_{1}, y_{2}\right)=$ $\left(\operatorname{CodeBERTScore}\left(y_{1}, y_{2}\right)\right)^{k}$.</p>
<p>As Figure 8 shows, distinguishability of CodeBERTScore ${ }^{k}$ increases almost exponentially while increasing $k$, although the base CodeBERTScore metric has not changed.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Distinguishability by exponentiating the original CodeBERTScore by $k$.</p>
<p>We thus argue that distinguishability is not a reliable meta-metric and is no substitute for execution-based- or human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores.</p>
<p>The distinguishability results of CodeBERTScore ${ }^{k}$ with different values of $k$ are shown in Figure 8. As Figure 8 shows, the distinguishability increases almost exponentially with the increasing value of $k$. We thus argue that distinguishability is not a reliable metametric and is no substitute for execution-basedor human-rating. We further suspect that any meta-metric that compares exact, absolute, scores across different metrics is susceptible to such manipulations, and the reliable way to compare metrics is according to the way they rank different examples, rather than the exact scores.</p>
<h1>G Additional Examples</h1>
<p>In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally incorrect prediction, which is inequivalent to the reference. Figure 9 shows an example in Java, and Figure 10 shows a C++ example.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 9: An example in HumanEval-Java, in which METEOR assigns a higher score to Figure 9(c) which is not functionally equivalent to the reference (Figure 9(b)), while CodeBERTScore prefers the functionally correct candidate Figure 9(d).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Natural Language Question:</th>
<th style="text-align: center;">Reference:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">/*</td>
<td style="text-align: center;">long specialFilter(</td>
</tr>
<tr>
<td style="text-align: center;">Write a function that takes a vector of numbers as input and returns the number of elements in the vector that are greater than 10 and both first and last digits of a number are odd $(1,3,5,7,9)$. For example: specialFilter((15, -73, 14, -15)) $\quad=&gt;1$ specialFilter((33, -2, -3, 45, 21, 109)) $\quad=&gt;2$</td>
<td style="text-align: center;"><code>std::vector&lt;long&gt; nums) {     int num=0;     for(int i=0;i&lt;nums.size();i++)     if (nums[i]&gt;10) {         string w=to_string(nums[i]);         if (w[0]%2==1 and w[w.length()-1] % 2 == 1)         num+=1;     }     }     return num;</code></td>
</tr>
<tr>
<td style="text-align: center;">(a) The natural language question.</td>
<td style="text-align: center;">(b) The ground truth reference.</td>
</tr>
<tr>
<td style="text-align: center;">Non-equivalent candidate:</td>
<td style="text-align: center;">Equivalent candidate:</td>
</tr>
<tr>
<td style="text-align: center;"><code>long specialFilter(     std::vector&lt;long&gt; nums) {     // Write your code here     long sum = 0;     for(int i=0; i&lt;nums.size(); i++)     std::string str =         std::to_string(nums[i]);         long firstDigit = nums[i] /             abs(nums[i]);         long lastDigit = nums[i]%10;         if(nums[i] &gt; 10 &amp;&amp;                 (firstDigit % 2 == 1)){         lastDigit % 2 == 1)){         sum++;     }     }     return sum;</code></td>
<td style="text-align: center;">(d) Preferred by CodeBERTScore.</td>
</tr>
</tbody>
</table>
<p>(c) Preferred by chrF.</p>
<p>Figure 10: An example in HumanEval-C++, in which chrF assigns a higher score to Figure 10(c) which is not functionally equivalent to the reference (Figure 10(b)), while CodeBERTScore assigns a higher score to the functionally correct candidate Figure 10(d).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://huggingface.co/datasets/nuprl/MultiPL-E
${ }^{7}$ https://octoverse.github.com/2022/
top-programming-languages
${ }^{8}$ https://huggingface.co/datasets/THUDM/humaneval-x&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
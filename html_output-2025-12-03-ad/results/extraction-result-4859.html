<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4859 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4859</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4859</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-07b14c24833400b79978b0a5f084803337e30a15</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07b14c24833400b79978b0a5f084803337e30a15" target="_blank">REPLUG: Retrieval-Augmented Black-Box Language Models</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> REPLUG is introduced, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model, and can be easily applied to any existing language models.</p>
                <p><strong>Paper Abstract:</strong> We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4859.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4859.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REPLUG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REPLUG (Retrieve and Plug)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented framework that treats large language models as black boxes and prepends retrieved documents to the LM input, ensembling multiple forward passes (one per retrieved doc) to produce final predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REPLUG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A plug-and-play retrieval augmentation for frozen/black-box LMs that (1) retrieves top-k documents with a retriever, (2) prepends each document separately to the input, (3) runs the LM separately for each prepended document and (4) ensembles output probabilities weighted by document-LM similarity; designed to be applicable to any LM available only via API.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (document datastore / vector index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieval from an external corpus using a dense dual-encoder retriever (Contriever by default) with precomputed document embeddings stored in a FAISS index; top-k documents are prepended to the LM input and ensembled across k forward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling, few-shot in-context learning (MMLU), open-domain QA (NQ, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Language modeling (bits-per-byte on The Pile, Wikitext-103 perplexity), 5-shot in-context multiple-choice MMLU, and few-shot open-domain QA (16-shot/64-shot settings) where retrieval supplies supporting documents from an external corpus (Pile or Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>The Pile (language modeling), Wikitext-103 (perplexity analysis), MMLU (5-shot), Natural Questions (NQ) and TriviaQA (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples reported: Pile BPB improvements across models (e.g., GPT-3 Davinci (175B): BPB 0.80 -> +REPLUG 0.77), average improvement of REPLUG across 8 models: ~4.7% relative BPB improvement; MMLU (Codex + REPLUG): 68.3 -> 71.4 (absolute points) on All; NQ 16-shot (Codex + REPLUG): 40.6 -> 44.7 exact-match (EM); TriviaQA (Codex + REPLUG) 73.6 -> 76.8 EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (no retrieval) numbers: example GPT-3 Davinci (175B) BPB 0.80; Codex MMLU All: 68.3; Codex NQ 16-shot: 40.6; Codex TQA: 73.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Adding retrieved, relevant documents via REPLUG consistently improves performance across language modeling and downstream tasks; ensembling retrieved documents (top-10) yields monotonic gains (small k ~10 sufficient); ensembling random documents hurts performance, showing gains are due to relevant retrieved memory rather than ensembling alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Always-on retrieval (no gating) can present irrelevant documents and increase compute; interpretability is limited (unclear when LM uses retrieved vs parametric knowledge); retrieval is limited by the chosen corpus size and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Simple prepend-and-ensemble retrieval augmentation applied to frozen black-box LMs yields consistent improvements; relevant retrieved documents are crucial (random documents do not help); a small number of retrieved docs (â‰ˆ10) suffices for large gains; this approach is widely applicable because it does not require LM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4859.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REPLUG with LM-Supervised Retrieval (LSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of REPLUG that fine-tunes the dense retriever using supervision derived from a frozen language model's likelihoods so the retriever prioritizes documents that reduce LM perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REPLUG LSR (LM-Supervised Retriever + REPLUG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same black-box LM augmentation pipeline as REPLUG but with a retriever fine-tuned to the specific LM: retrieved candidates are scored by the LM (probability / perplexity of the ground-truth continuation) and the retriever is trained to match that LM-induced distribution via a KL loss, updating only the retriever and periodically rebuilding the FAISS index.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (LM-supervised dense retriever + FAISS datastore)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Dense dual-encoder retriever (Contriever initialized) is fine-tuned so retrieval likelihoods match LM likelihoods over candidate documents; uses top-20 candidates during training, temperatures for both retrieval and LM likelihood softmax, and asynchronous index rebuilds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling (Pile BPB / Wikitext-103), in-context learning (MMLU), open-domain QA (NQ, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as REPLUG but with the retriever adapted to the target LM using LM-provided supervisory signal (LM probability of true continuation) to prioritize documents that lower LM perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>The Pile (language modeling), Wikitext-103, MMLU (5-shot), Natural Questions and TriviaQA (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples reported: Pile BPB: GPT-3 Davinci BPB 0.80 -> +REPLUG LSR 0.75 (6.3% relative gain reported for Davinci); average over 8 models: REPLUG LSR gives ~7.7% relative BPB improvement over baselines; MMLU (Codex + REPLUG LSR): 68.3 -> 71.8 (All); NQ 16-shot (Codex + REPLUG LSR): 40.6 -> 45.5 EM (reported 12.0% relative improvement); TriviaQA (Codex + REPLUG LSR): 73.6 -> 77.3 EM (reported 5.0% relative improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (no retrieval) numbers: same baselines as REPLUG (e.g., Codex MMLU All: 68.3; Codex NQ: 40.6; Codex TQA: 73.6; GPT-3 Davinci BPB 0.80).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Fine-tuning the retriever with LM supervision (LSR) consistently outperforms the off-the-shelf dense retrievers and REPLUG with an untuned retriever; average gains across language-modeling tasks are larger for REPLUG LSR (~7.7% BPB) vs REPLUG (~4.7%). On retrieval comparisons, LSR retriever outperforms other dense/sparse retrievers (though BM25 was best among off-the-shelf baselines in some settings but still behind LSR).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires running the LM to compute supervisory signals during retriever training (black-box LM queries used as scoring function), which can be expensive; retriever updates necessitate periodic re-embedding and index rebuilds; still limited by corpus coverage and potential retrieval of distracting/irrelevant documents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Adapting the retriever to the specific frozen LM using LM-derived likelihoods yields substantial additional gains over using a generic retriever; LM-supervised retrieval is an effective way to align retrieval with the behaviors/needs of a black-box LM without touching LM parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4859.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex + REPLUG / REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (175B) augmented with REPLUG / REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of REPLUG and REPLUG LSR to the Codex 175B model in few-shot MMLU and open-domain QA and language modeling settings, showing measurable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Codex + REPLUG / Codex + REPLUG LSR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Codex 175B (black-box LM) augmented by REPLUG's retrieval-and-ensemble pipeline (or REPLUG LSR with LM-supervised retriever) to supply external document context to in-context examples and questions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (Wikipedia for MMLU/QA; Pile for LM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves top-10 Wikipedia passages for MMLU/QA (or Pile documents for LM) and prepends each to the prompt, runs Codex separately for each retrieved doc and ensembles probabilities using retriever similarity weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (5-shot), Natural Questions (16-shot), TriviaQA (16-shot), language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>5-shot multiple-choice MMLU; few-shot open-domain QA (k-shot) where the test question is used as the retrieval query; language modeling on Pile using retrieved training-documents to assist continuation prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MMLU, Natural Questions (NQ), TriviaQA, The Pile</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>MMLU All: Codex baseline 68.3 -> +REPLUG 71.4 -> +REPLUG LSR 71.8 (absolute points). NQ (16-shot) EM: baseline 40.6 -> +REPLUG 44.7 -> +REPLUG LSR 45.5. TriviaQA EM: baseline 73.6 -> +REPLUG 76.8 -> +REPLUG LSR 77.3. Pile BPB: not individually listed for Codex in table but authors report improvements for GPT-family and Codex in LM settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline Codex performance: MMLU All 68.3; NQ 40.6 EM; TriviaQA 73.6 EM.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>REPLUG and REPLUG LSR both improve Codex substantially on MMLU and open-domain QA, with LSR yielding modest additional gains (e.g., +0.4 point on MMLU All versus REPLUG). Gains are comparable to much larger models (e.g., Codex+REPLUG approaches PaLM/Flan-PaLM performance on some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance still lags behind retrieval-augmented models fine-tuned on full training data in some QA settings; gains depend on corpus coverage and quality of retrieval; always-on retrieval may present irrelevant documents.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Retrieval augmentation (even without LM fine-tuning) produces sizable improvements in reasoning and QA tasks for Codex; adapting the retriever to the LM (LSR) yields further, consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4859.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbor Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented approach that interpolates a language model's next-token distribution with a kNN distribution computed from cached internal LM representations of training tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The model retrieves nearest neighbor token representations from a datastore of model-hidden states and interpolates their next-token distributions with the LM's own distribution to improve generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric memory via token-level datastore (nearest neighbors of LM representations)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores internal LM representations and nearest-neighbor lookup at inference to adjust the next-token distribution; requires access to internal LM representations to compute kNN distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling / next-token prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improves next-token prediction by augmenting LM probabilities with distributions derived from similar context tokens stored in a datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Referenced as prior work (no specific benchmark numbers reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper notes kNN-LM is effective but requires access to internal LM representations (not compatible with black-box LM APIs), motivating REPLUG's black-box approach that prepends retrieved text instead of using internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires access to internal LM activations (infeasible for black-box LMs); storage and retrieval at token-level can be expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>kNN-LM demonstrates benefits of non-parametric memory but is unsuitable for the black-box LM setting addressed by REPLUG.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4859.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RETRO (Retrieval-Enhanced Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LM that modifies the decoder-only architecture to incorporate retrieved text through cross-attention and is pretrained with retrieval integrated into training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RETRO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model that integrates retrieval into the transformer architecture (notably changing the decoder to attend to retrieved chunks) and is pretrained from scratch with retrieval-augmented objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>architectural retrieval-augmentation (in-model cross-attention to retrieved chunks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieved chunks are incorporated into model layers via specialized attention mechanisms rather than prepending raw text; retrieval is deeply integrated during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling at scale (trained with retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pretrained with retrieval to enable large-scale language modeling that draws on external chunks at inference/ training time.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Referenced as prior work (no fresh results reported beyond citation in this paper); compared conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RETRO is a white-box approach that requires architecture changes and pretraining; REPLUG contrasts by being black-box and applicable to API-only LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires changing model architecture and pretraining from scratch; not applicable to frozen/black-box LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Demonstrates benefits of close integration of retrieval into model internals, but motivates REPLUG's pragmatic black-box approach for API-constrained scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4859.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Atlas (Retrieval-augmented LM by Izacard et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented language model that trains both retriever and generator jointly (white-box) and has been applied to few-shot learning and open-domain QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Atlas</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A white-box retrieval-augmented model that models documents as latent variables, jointly trains retriever and generator, and has been evaluated on few-shot QA and other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory with joint training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retriever and generator are trained together so the model learns to use retrieved passages during generation; retrieval integrated into training rather than applied post-hoc to a frozen LM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot learning, open-domain QA (NQ, TriviaQA), and other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Atlas is evaluated in few-shot QA settings (e.g., 64-shot) and in full-data finetuned settings, showing strong performance when retriever and LM are jointly trained.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MMLU (reported baseline), Natural Questions and TriviaQA (reported baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baselines in this paper: MMLU All 47.9 (Atlas), NQ full 60.4 EM, TriviaQA full 79.8 EM (Atlas).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>REPLUG LSR (black-box) outperforms Atlas (white-box) on the MMLU few-shot evaluations reported here despite Atlas jointly training the LM and retriever; Atlas was finetuned on few-shot examples while REPLUG LSR uses black-box LM scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Atlas requires white-box access and finetuning of the LM, making it less applicable to API-only LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Joint training of retriever and LM can be powerful, but REPLUG shows training only the retriever (with LM supervision) can approach or outperform white-box methods on some few-shot tasks while keeping the LM frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4859.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4859.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R2-D2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>R2-D2 (Modular retrieval baseline for open-domain QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular retrieval-and-reading baseline for open-domain QA used as a state-of-the-art retrieval-augmented system in some full-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R2-D2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A modular pipeline combining retrieval and a reader model for open-domain QA; used as a competitive baseline on NQ and TriviaQA in full-data trained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external memory (passage retriever + reader)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Retrieves passages from a corpus (often via dense retriever) and feeds them to a reader model trained on QA supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain question answering (NQ, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>QA with full-data training where retriever + reader are finetuned to maximize exact-match on answers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Natural Questions (NQ), TriviaQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in this paper (Table 3): R2-D2 full NQ 55.9 EM; full TriviaQA 69.9 EM (as reported for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>R2-D2 represents strong finetuned retrieval-augmented baselines on full-data settings; REPLUG LSR improves black-box few-shot performance but does not necessarily surpass full-data finetuned retrieval systems on all metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires supervised finetuning on task-specific training data (not a zero/few-shot black-box approach).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Strong retrieval+reader pipelines set high bars for full-data QA; REPLUG's black-box approach narrows the gap in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REPLUG: Retrieval-Augmented Black-Box Language Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 2)</em></li>
                <li>Improving language models by retrieving from trillions of tokens <em>(Rating: 2)</em></li>
                <li>Few-shot learning with retrieval augmented language models <em>(Rating: 2)</em></li>
                <li>Retrieval augmented generation for knowledge-intensive NLP tasks <em>(Rating: 1)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 1)</em></li>
                <li>R2-D2: A modular baseline for open-domain question answering <em>(Rating: 1)</em></li>
                <li>Retrieval augmented language model pre-training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4859",
    "paper_id": "paper-07b14c24833400b79978b0a5f084803337e30a15",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "REPLUG",
            "name_full": "REPLUG (Retrieve and Plug)",
            "brief_description": "A retrieval-augmented framework that treats large language models as black boxes and prepends retrieved documents to the LM input, ensembling multiple forward passes (one per retrieved doc) to produce final predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REPLUG",
            "agent_description": "A plug-and-play retrieval augmentation for frozen/black-box LMs that (1) retrieves top-k documents with a retriever, (2) prepends each document separately to the input, (3) runs the LM separately for each prepended document and (4) ensembles output probabilities weighted by document-LM similarity; designed to be applicable to any LM available only via API.",
            "memory_type": "external retrieval-augmented memory (document datastore / vector index)",
            "memory_description": "Retrieval from an external corpus using a dense dual-encoder retriever (Contriever by default) with precomputed document embeddings stored in a FAISS index; top-k documents are prepended to the LM input and ensembled across k forward passes.",
            "task_name": "Language modeling, few-shot in-context learning (MMLU), open-domain QA (NQ, TriviaQA)",
            "task_description": "Language modeling (bits-per-byte on The Pile, Wikitext-103 perplexity), 5-shot in-context multiple-choice MMLU, and few-shot open-domain QA (16-shot/64-shot settings) where retrieval supplies supporting documents from an external corpus (Pile or Wikipedia).",
            "benchmark_name": "The Pile (language modeling), Wikitext-103 (perplexity analysis), MMLU (5-shot), Natural Questions (NQ) and TriviaQA (few-shot)",
            "performance_with_memory": "Examples reported: Pile BPB improvements across models (e.g., GPT-3 Davinci (175B): BPB 0.80 -&gt; +REPLUG 0.77), average improvement of REPLUG across 8 models: ~4.7% relative BPB improvement; MMLU (Codex + REPLUG): 68.3 -&gt; 71.4 (absolute points) on All; NQ 16-shot (Codex + REPLUG): 40.6 -&gt; 44.7 exact-match (EM); TriviaQA (Codex + REPLUG) 73.6 -&gt; 76.8 EM.",
            "performance_without_memory": "Baseline (no retrieval) numbers: example GPT-3 Davinci (175B) BPB 0.80; Codex MMLU All: 68.3; Codex NQ 16-shot: 40.6; Codex TQA: 73.6.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Adding retrieved, relevant documents via REPLUG consistently improves performance across language modeling and downstream tasks; ensembling retrieved documents (top-10) yields monotonic gains (small k ~10 sufficient); ensembling random documents hurts performance, showing gains are due to relevant retrieved memory rather than ensembling alone.",
            "limitations_or_challenges": "Always-on retrieval (no gating) can present irrelevant documents and increase compute; interpretability is limited (unclear when LM uses retrieved vs parametric knowledge); retrieval is limited by the chosen corpus size and coverage.",
            "key_insights": "Simple prepend-and-ensemble retrieval augmentation applied to frozen black-box LMs yields consistent improvements; relevant retrieved documents are crucial (random documents do not help); a small number of retrieved docs (â‰ˆ10) suffices for large gains; this approach is widely applicable because it does not require LM fine-tuning.",
            "uuid": "e4859.0",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "REPLUG LSR",
            "name_full": "REPLUG with LM-Supervised Retrieval (LSR)",
            "brief_description": "An extension of REPLUG that fine-tunes the dense retriever using supervision derived from a frozen language model's likelihoods so the retriever prioritizes documents that reduce LM perplexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REPLUG LSR (LM-Supervised Retriever + REPLUG)",
            "agent_description": "Same black-box LM augmentation pipeline as REPLUG but with a retriever fine-tuned to the specific LM: retrieved candidates are scored by the LM (probability / perplexity of the ground-truth continuation) and the retriever is trained to match that LM-induced distribution via a KL loss, updating only the retriever and periodically rebuilding the FAISS index.",
            "memory_type": "external retrieval-augmented memory (LM-supervised dense retriever + FAISS datastore)",
            "memory_description": "Dense dual-encoder retriever (Contriever initialized) is fine-tuned so retrieval likelihoods match LM likelihoods over candidate documents; uses top-20 candidates during training, temperatures for both retrieval and LM likelihood softmax, and asynchronous index rebuilds.",
            "task_name": "Language modeling (Pile BPB / Wikitext-103), in-context learning (MMLU), open-domain QA (NQ, TriviaQA)",
            "task_description": "Same tasks as REPLUG but with the retriever adapted to the target LM using LM-provided supervisory signal (LM probability of true continuation) to prioritize documents that lower LM perplexity.",
            "benchmark_name": "The Pile (language modeling), Wikitext-103, MMLU (5-shot), Natural Questions and TriviaQA (few-shot)",
            "performance_with_memory": "Examples reported: Pile BPB: GPT-3 Davinci BPB 0.80 -&gt; +REPLUG LSR 0.75 (6.3% relative gain reported for Davinci); average over 8 models: REPLUG LSR gives ~7.7% relative BPB improvement over baselines; MMLU (Codex + REPLUG LSR): 68.3 -&gt; 71.8 (All); NQ 16-shot (Codex + REPLUG LSR): 40.6 -&gt; 45.5 EM (reported 12.0% relative improvement); TriviaQA (Codex + REPLUG LSR): 73.6 -&gt; 77.3 EM (reported 5.0% relative improvement).",
            "performance_without_memory": "Baseline (no retrieval) numbers: same baselines as REPLUG (e.g., Codex MMLU All: 68.3; Codex NQ: 40.6; Codex TQA: 73.6; GPT-3 Davinci BPB 0.80).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Fine-tuning the retriever with LM supervision (LSR) consistently outperforms the off-the-shelf dense retrievers and REPLUG with an untuned retriever; average gains across language-modeling tasks are larger for REPLUG LSR (~7.7% BPB) vs REPLUG (~4.7%). On retrieval comparisons, LSR retriever outperforms other dense/sparse retrievers (though BM25 was best among off-the-shelf baselines in some settings but still behind LSR).",
            "limitations_or_challenges": "Requires running the LM to compute supervisory signals during retriever training (black-box LM queries used as scoring function), which can be expensive; retriever updates necessitate periodic re-embedding and index rebuilds; still limited by corpus coverage and potential retrieval of distracting/irrelevant documents.",
            "key_insights": "Adapting the retriever to the specific frozen LM using LM-derived likelihoods yields substantial additional gains over using a generic retriever; LM-supervised retrieval is an effective way to align retrieval with the behaviors/needs of a black-box LM without touching LM parameters.",
            "uuid": "e4859.1",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Codex + REPLUG / REPLUG LSR",
            "name_full": "Codex (175B) augmented with REPLUG / REPLUG LSR",
            "brief_description": "Application of REPLUG and REPLUG LSR to the Codex 175B model in few-shot MMLU and open-domain QA and language modeling settings, showing measurable improvements.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Codex + REPLUG / Codex + REPLUG LSR",
            "agent_description": "Codex 175B (black-box LM) augmented by REPLUG's retrieval-and-ensemble pipeline (or REPLUG LSR with LM-supervised retriever) to supply external document context to in-context examples and questions.",
            "memory_type": "external retrieval-augmented memory (Wikipedia for MMLU/QA; Pile for LM)",
            "memory_description": "Retrieves top-10 Wikipedia passages for MMLU/QA (or Pile documents for LM) and prepends each to the prompt, runs Codex separately for each retrieved doc and ensembles probabilities using retriever similarity weights.",
            "task_name": "MMLU (5-shot), Natural Questions (16-shot), TriviaQA (16-shot), language modeling",
            "task_description": "5-shot multiple-choice MMLU; few-shot open-domain QA (k-shot) where the test question is used as the retrieval query; language modeling on Pile using retrieved training-documents to assist continuation prediction.",
            "benchmark_name": "MMLU, Natural Questions (NQ), TriviaQA, The Pile",
            "performance_with_memory": "MMLU All: Codex baseline 68.3 -&gt; +REPLUG 71.4 -&gt; +REPLUG LSR 71.8 (absolute points). NQ (16-shot) EM: baseline 40.6 -&gt; +REPLUG 44.7 -&gt; +REPLUG LSR 45.5. TriviaQA EM: baseline 73.6 -&gt; +REPLUG 76.8 -&gt; +REPLUG LSR 77.3. Pile BPB: not individually listed for Codex in table but authors report improvements for GPT-family and Codex in LM settings.",
            "performance_without_memory": "Baseline Codex performance: MMLU All 68.3; NQ 40.6 EM; TriviaQA 73.6 EM.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "REPLUG and REPLUG LSR both improve Codex substantially on MMLU and open-domain QA, with LSR yielding modest additional gains (e.g., +0.4 point on MMLU All versus REPLUG). Gains are comparable to much larger models (e.g., Codex+REPLUG approaches PaLM/Flan-PaLM performance on some metrics).",
            "limitations_or_challenges": "Performance still lags behind retrieval-augmented models fine-tuned on full training data in some QA settings; gains depend on corpus coverage and quality of retrieval; always-on retrieval may present irrelevant documents.",
            "key_insights": "Retrieval augmentation (even without LM fine-tuning) produces sizable improvements in reasoning and QA tasks for Codex; adapting the retriever to the LM (LSR) yields further, consistent gains.",
            "uuid": "e4859.2",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbor Language Model (kNN-LM)",
            "brief_description": "A retrieval-augmented approach that interpolates a language model's next-token distribution with a kNN distribution computed from cached internal LM representations of training tokens.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM",
            "agent_description": "The model retrieves nearest neighbor token representations from a datastore of model-hidden states and interpolates their next-token distributions with the LM's own distribution to improve generation.",
            "memory_type": "non-parametric memory via token-level datastore (nearest neighbors of LM representations)",
            "memory_description": "Stores internal LM representations and nearest-neighbor lookup at inference to adjust the next-token distribution; requires access to internal LM representations to compute kNN distributions.",
            "task_name": "Language modeling / next-token prediction",
            "task_description": "Improves next-token prediction by augmenting LM probabilities with distributions derived from similar context tokens stored in a datastore.",
            "benchmark_name": "Referenced as prior work (no specific benchmark numbers reported in this paper)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper notes kNN-LM is effective but requires access to internal LM representations (not compatible with black-box LM APIs), motivating REPLUG's black-box approach that prepends retrieved text instead of using internal representations.",
            "limitations_or_challenges": "Requires access to internal LM activations (infeasible for black-box LMs); storage and retrieval at token-level can be expensive.",
            "key_insights": "kNN-LM demonstrates benefits of non-parametric memory but is unsuitable for the black-box LM setting addressed by REPLUG.",
            "uuid": "e4859.3",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RETRO",
            "name_full": "RETRO (Retrieval-Enhanced Transformer)",
            "brief_description": "A retrieval-augmented LM that modifies the decoder-only architecture to incorporate retrieved text through cross-attention and is pretrained with retrieval integrated into training.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RETRO",
            "agent_description": "A model that integrates retrieval into the transformer architecture (notably changing the decoder to attend to retrieved chunks) and is pretrained from scratch with retrieval-augmented objectives.",
            "memory_type": "architectural retrieval-augmentation (in-model cross-attention to retrieved chunks)",
            "memory_description": "Retrieved chunks are incorporated into model layers via specialized attention mechanisms rather than prepending raw text; retrieval is deeply integrated during pretraining.",
            "task_name": "Language modeling at scale (trained with retrieval)",
            "task_description": "Pretrained with retrieval to enable large-scale language modeling that draws on external chunks at inference/ training time.",
            "benchmark_name": "Referenced as prior work (no fresh results reported beyond citation in this paper); compared conceptually.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "RETRO is a white-box approach that requires architecture changes and pretraining; REPLUG contrasts by being black-box and applicable to API-only LMs.",
            "limitations_or_challenges": "Requires changing model architecture and pretraining from scratch; not applicable to frozen/black-box LMs.",
            "key_insights": "Demonstrates benefits of close integration of retrieval into model internals, but motivates REPLUG's pragmatic black-box approach for API-constrained scenarios.",
            "uuid": "e4859.4",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Atlas",
            "name_full": "Atlas (Retrieval-augmented LM by Izacard et al.)",
            "brief_description": "A retrieval-augmented language model that trains both retriever and generator jointly (white-box) and has been applied to few-shot learning and open-domain QA tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Atlas",
            "agent_description": "A white-box retrieval-augmented model that models documents as latent variables, jointly trains retriever and generator, and has been evaluated on few-shot QA and other tasks.",
            "memory_type": "retrieval-augmented external memory with joint training",
            "memory_description": "Retriever and generator are trained together so the model learns to use retrieved passages during generation; retrieval integrated into training rather than applied post-hoc to a frozen LM.",
            "task_name": "Few-shot learning, open-domain QA (NQ, TriviaQA), and other tasks",
            "task_description": "Atlas is evaluated in few-shot QA settings (e.g., 64-shot) and in full-data finetuned settings, showing strong performance when retriever and LM are jointly trained.",
            "benchmark_name": "MMLU (reported baseline), Natural Questions and TriviaQA (reported baselines)",
            "performance_with_memory": "Reported baselines in this paper: MMLU All 47.9 (Atlas), NQ full 60.4 EM, TriviaQA full 79.8 EM (Atlas).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "REPLUG LSR (black-box) outperforms Atlas (white-box) on the MMLU few-shot evaluations reported here despite Atlas jointly training the LM and retriever; Atlas was finetuned on few-shot examples while REPLUG LSR uses black-box LM scoring.",
            "limitations_or_challenges": "Atlas requires white-box access and finetuning of the LM, making it less applicable to API-only LMs.",
            "key_insights": "Joint training of retriever and LM can be powerful, but REPLUG shows training only the retriever (with LM supervision) can approach or outperform white-box methods on some few-shot tasks while keeping the LM frozen.",
            "uuid": "e4859.5",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "R2-D2",
            "name_full": "R2-D2 (Modular retrieval baseline for open-domain QA)",
            "brief_description": "A modular retrieval-and-reading baseline for open-domain QA used as a state-of-the-art retrieval-augmented system in some full-data settings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "R2-D2",
            "agent_description": "A modular pipeline combining retrieval and a reader model for open-domain QA; used as a competitive baseline on NQ and TriviaQA in full-data trained settings.",
            "memory_type": "retrieval-augmented external memory (passage retriever + reader)",
            "memory_description": "Retrieves passages from a corpus (often via dense retriever) and feeds them to a reader model trained on QA supervision.",
            "task_name": "Open-domain question answering (NQ, TriviaQA)",
            "task_description": "QA with full-data training where retriever + reader are finetuned to maximize exact-match on answers.",
            "benchmark_name": "Natural Questions (NQ), TriviaQA",
            "performance_with_memory": "Reported in this paper (Table 3): R2-D2 full NQ 55.9 EM; full TriviaQA 69.9 EM (as reported for comparison).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "R2-D2 represents strong finetuned retrieval-augmented baselines on full-data settings; REPLUG LSR improves black-box few-shot performance but does not necessarily surpass full-data finetuned retrieval systems on all metrics.",
            "limitations_or_challenges": "Requires supervised finetuning on task-specific training data (not a zero/few-shot black-box approach).",
            "key_insights": "Strong retrieval+reader pipelines set high bars for full-data QA; REPLUG's black-box approach narrows the gap in few-shot settings.",
            "uuid": "e4859.6",
            "source_info": {
                "paper_title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 2
        },
        {
            "paper_title": "Improving language models by retrieving from trillions of tokens",
            "rating": 2
        },
        {
            "paper_title": "Few-shot learning with retrieval augmented language models",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmented generation for knowledge-intensive NLP tasks",
            "rating": 1
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 1
        },
        {
            "paper_title": "R2-D2: A modular baseline for open-domain question answering",
            "rating": 1
        },
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 1
        }
    ],
    "cost": 0.01688625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RePlug: Retrieval-Augmented Black-Box Language Models</h1>
<p>Weijia Shi ${ }^{1,2}$ Sewon Min ${ }^{1}$ Michihiro Yasunaga ${ }^{3}$ Minjoon Seo ${ }^{4}$<br>Rich James ${ }^{2}$ Mike Lewis ${ }^{2}$ Luke Zettlemoyer ${ }^{1,2}$ Wen-tau Yih ${ }^{2}$<br>${ }^{1}$ University of Washington, Seattle, WA, ${ }^{2}$ FAIR, Meta<br>${ }^{3}$ Stanford University ${ }^{4}$ KAIST<br>swj0419@uw.edu</p>
<h4>Abstract</h4>
<p>We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that RePLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by $6.3 \%$, as well as the performance of Codex on five-shot MMLU by $5.1 \%$. Code is publicly released at github.com/swj0419/REPLUG.</p>
<h2>1 Introduction</h2>
<p>Large language models (LMs) such as GPT3 (Brown et al., 2020) and Codex (Chen et al., 2021), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2023), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022; Izacard et al., 2022b) or to index the datastore (Khandelwal
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the LM as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes RePLUG applicable to large LMs, which are often served via APIs.
et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.</p>
<p>In this work, we introduce RePLUG (Retrieve and Plug), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, RePLUG first retrieves relevant documents from an external corpus using an off-theshelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also adopt an ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy.</p>
<p>As shown in Figure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.</p>
<p>We also introduce REPLUG LSR (REPLUG with LM-Supervised Retrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a blackbox language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.</p>
<p>Our experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by $4.5 \%$, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) outperforms various off-theshelf retrievers and leads to additional improvements, including up to $6.3 \%$ increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs ( $&gt;100 \mathrm{~B}$ model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:</p>
<ul>
<li>We introduce REPLUG (Â§3), the first retrievalaugmented language modeling framework for enhancing black-box LMs with retrieval. Unlike previous methods that require updating the LM's parameters, REPLUG could be easily plugged into any existing LM without additional finetuning.</li>
<li>We propose a training scheme (Â§4) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.</li>
<li>We are the first to demonstrate that retrieval can benefit large-scale, state-of-the-art LMs on language modeling (Â§6) and in-context learning tasks. Evaluations show that REPLUG can improve the performance of var-
ious language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.</li>
</ul>
<h2>2 Background and Related Work</h2>
<p>Black-box Language Models Large language models, such as GPT-3 (Brown et al., 2020), Codex (Chen et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as BLOOM-176B (Scao et al., 2022) require significant computational resources to run and finetune locally. For example, finetuning BLOOM176B requires 72 A100 GPUs (Younes Belkda, 2022), making them inaccessible to researchers and developers with limited resources. Traditionally, retrieval-augmented model frameworks (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have focused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents. However, the increasing scale and blackbox nature of LLMs makes this approach infeasible. To address these challenges, we investigate retrieval-augmentation in the black-box setting, where users only have access to the model predictions and cannot access or modify its parameters.</p>
<p>Retrieval-augmented Models Augmenting language models with relevant information retrieved from knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. Previous retrieval-augmented LMs require updating the model parameters, which cannot be applied to black-box LMs, which cannot be applied to black-box LMs. For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoder-only architecture to incorporate retrieved texts and pretrains the language model from scratch. Another line of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RePlug at inference (Â§3). Given an input context, RePlug first retrieves a small set of relevant documents from an external corpus using a retriever ( $\S 3.1$ Document Retrieval). Then it prepends each document separately to the input context and ensembles output probabilities from different passes ( $\S 3.2$ Input Reformulation).
retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020; Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. kNNLM requires access to internal LM representations to compute the kNN distribution, which are not available for black-box LMs such as GPT-3. In this work, we investigate ways to improve large blackbox language models with retrieval. While concurrent work (Mallen et al., 2022; Si et al., 2023) has demonstrated that using a frozen retriever can improve GPT-3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We additionally adopt an ensemble method to incorporate more documents and a training scheme to further adapt the retriever to large LMs.</p>
<h2>3 REPLUG</h2>
<p>We introduce RePlug (Retrieve and Plug), a new retrieval-augmented LM paradigm where the LM is treated as black box and the retrieval component is added as a potentially tuneable module.</p>
<p>As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (Â§3.1). Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities (Â§3.2).</p>
<h3>3.1 Document Retrieval</h3>
<p>Given an input context $x$, the retriever aims to retrieve a small set of documents from a corpus $\mathcal{D}=\left{d_{1} \ldots d_{m}\right}$ that are relevant to $x$. Following prior work (Qu et al., 2021; Izacard and Grave, 2021; Ni et al., 2022), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context $x$ and the document $d$. Specifically, the encoder maps each document $d \in D$ to an embedding $\mathbf{E}(d)$ by taking the mean pooling of the last hidden representation over the tokens in $d$. At query time, the same encoder is applied to the input context $x$ to obtain a query embedding $\mathbf{E}(x)$. The similarity between the query embedding and the document embedding is computed by their cosine similarity:</p>
<p>$$
s(d, x)=\cos (\mathbf{E}(d), \mathbf{E}(x))
$$</p>
<p>The top- $k$ documents that have the highest similarity scores when compared with the input $x$ are retrieved in this step. For efficient retrieval, we precompute the embedding of each document $d \in D$ and construct FAISS index (Johnson et al., 2019) over these embeddings.</p>
<h3>3.2 Input Reformulation</h3>
<p>The retrieved top- $k$ documents provide rich information about the original input context $x$ and can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend $x$ with all $k$ documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., $k$ ) we can include, given the language</p>
<p>model's context window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume $\mathcal{D}^{\prime} \subset \mathcal{D}$ consists of $k$ most relevant documents to $x$, according to the scoring function in Eq. (1). We prepend each document $d \in \mathcal{D}^{\prime}$ to $x$, pass this concatenation to the LM separately, and then ensemble output probabilities from all $k$ passes. Formally, given the input context $x$ and its top- $k$ relevant documents $\mathcal{D}^{\prime}$, the output probability of the next token $y$ is computed as a weighted average ensemble:</p>
<p>$$
p\left(y \mid x, \mathcal{D}^{\prime}\right)=\sum_{d \in \mathcal{D}^{\prime}} p(y \mid d \circ x) \cdot \lambda(d, x)
$$</p>
<p>where $\circ$ denotes the concatenation of two sequences and the weight $\lambda(d, x)$ is based on the similarity score between the document $d$ and the input context $x$ :</p>
<p>$$
\lambda(d, x)=\frac{e^{s(d, x)}}{\sum_{d \in \mathcal{D}^{\prime}} e^{s(d, x)}}
$$</p>
<h2>4 REPLUG LSR: Training the Dense Retriever</h2>
<p>Instead of relying only on existing neural dense retrieval models (Karpukhin et al., 2020; Izacard et al., 2022a; Su et al., 2023), we further propose REPLUG LSR (REPLUG with LM-Supervised Retrieval), which adapts the retriever in REPLUG by using the LM itself to provide supervision about which documents should be retrieved.</p>
<p>Inspired by Sachan et al. (2023), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model. In other words, we would like the retriever to find documents that result in lower perplexity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (Â§4.1), (2) scoring the retrieved documents by the language model (Â§4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (Â§4.3), and (4) asynchronous update of the datastore index (Â§4.4).</p>
<h3>4.1 Computing Retrieval Likelihood</h3>
<p>We retrieve $k$ documents $\mathcal{D}^{\prime} \subset \mathcal{D}$ with the highest similarity scores from a corpus $\mathcal{D}$ given an input context $x$, as described in $\S 3.1$. We then compute
the retrieval likelihood of each retrieved document $d$ :</p>
<p>$$
P_{R}(d \mid x)=\frac{e^{s(d, x) / \gamma}}{\sum_{d \in \mathcal{D}^{\prime}} e^{s(d, x) / \gamma}}
$$</p>
<p>where $\gamma$ is a hyperparameter that controls the temerature of the softmax. Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpus $\mathcal{D}$, which is intractable in practice. Therefore, we approximate the retrieval likelihood by only marginalizing over the retrieved documents $\mathcal{D}^{\prime}$.</p>
<h3>4.2 Computing LM likelihood</h3>
<p>We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifically, we first compute $P_{L M}(y \mid d, x)$, the LM probability of the ground truth output $y$ given the input context $x$ and a document $d$. The higher the probability, the better the document $d_{i}$ is at improving the LM's perplexity. We then compute the LM likelihood of each document $d$ as follows:</p>
<p>$$
Q(d \mid x, y)=\frac{e^{P_{L M}(y \mid d, x) / \beta}}{\sum_{d \in \mathcal{D}^{\prime}} e^{P_{L M}(y \mid d, x) / \beta}}
$$</p>
<p>where $\beta$ is another hyperparameter.</p>
<h3>4.3 Loss Function</h3>
<p>Given the input context $x$ and the corresponding ground truth continuation $y$, we compute the retrieval likelihood and the language model likelihood. The dense retriever is trained by minimizing the KL divergence between these two distributions:</p>
<p>$$
\mathcal{L}=\frac{1}{|\mathcal{B}|} \sum_{x \in \mathcal{B}} K L\left(Q_{\mathrm{LM}}(d \mid x, y) | P_{R}(d \mid x)\right)
$$</p>
<p>where $\mathcal{B}$ is a set of input contexts. When minimizing the loss, we can only update the retrieval model parameters. The LM parameters are fixed due to our black-box assumption.</p>
<h3>4.4 Asynchronous Update of the Datastore Index</h3>
<p>Because the parameters in the retriever are updated during the training process, the previously computed document embeddings are no longer up to date. Therefore, following Guu et al. (2020), we recompute the document embeddings and rebuild the efficient search index using the new embeddings every $T$ training steps. Then we use the new document embeddings and index for retrieval, and repeat the training procedure.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: RePlug LSR training process (\$4). The retriever is trained using the output of a frozen language model as supervision signals.</p>
<h2>5 Training Setup</h2>
<p>In this section, we describe the details of our training procedure. We first describe the model setting in REPLUG ( $\$ 5.1$ ) and then describe the procedure for training the retriever in REPLUG LSR ( $\$ 5.2$ ).</p>
<h3>5.1 RePlug</h3>
<p>In theory, any type of retriever, either dense (Karpukhin et al., 2020; Ni et al., 2022) or sparse (Robertson et al., 2009), could be used for RePlug. Following prior work (Izacard et al., 2022b), we use the Contriever (Izacard et al., 2022a) as the retrieval model for REPLUG, as it has demonstrated strong performance.</p>
<h3>5.2 REPLUG LSR</h3>
<p>For REPLUG LSR, we initialize the retriever with the Contriever model (Izacard et al., 2022a). We use GPT-3 Curie (Brown et al., 2020) as the supervision LM to compute the LM likelihood.</p>
<p>Training data We use 800 K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2021), as our training queries. Each query is split into two parts: the first 128 tokens are used as the input context $x$, and the last 128 tokens are used as the ground truth continuation $y$. For the external corpus $D$, we sample 36 M documents of 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled.</p>
<p>Training details To make the training process more efficient, we pre-compute the document embeddings of the external corpus $D$ and create a</p>
<p>FAISS index (Johnson et al., 2019) for fast similarity search. Given a query $x$, we retrieve the top 20 documents from the FAISS index and compute the retrieval likelihood and the LM likelihood with a temperature of 0.1 . We train the retriever using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of $2 \mathrm{e}-5$, a batch size of 64 , and a warmup ratio of 0.1 . We re-compute the document embeddings every 3 k steps and fine-tune the retriever for a total of 25 k steps.</p>
<h2>6 Experiments</h2>
<p>We perform evaluations on both language modeling (\$6.1) and downstream tasks such as MMLU (\$6.2) and open-domain QA ( $\$ 6.3$ ). In all settings, REPlug improve the performance of various blackbox language models, showing the effectiveness and generality of our approach.</p>
<h3>6.1 Language Modeling</h3>
<p>Datasets The Pile (Gao et al., 2021) is a language modeling benchmark that consists of text sources from diverse domains such as web pages, code and academic papers. Following prior work, we report bits per UTF-8 encoded byte (BPB) as the metric on each subset domain.</p>
<p>Baselines We consider GPT-3 and GPT-2 family LMs as the baselines. The four models from GPT-3 (Davinci, Curie, Baddage and Ada) are black-box models that are only accessible through API.</p>
<p>Our model We add RePlug and RePlug LSR to the baselines. We randomly subsampled Pile training data ( 36 M documents of 128 tokens) and use them as the retrieval corpus for all models. As</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Parameters</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">+ REPLUG</th>
<th style="text-align: center;">Gain \%</th>
<th style="text-align: center;">+ REPLUG LSR</th>
<th style="text-align: center;">Gain \%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">Small</td>
<td style="text-align: center;">117 M</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">1.26</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">9.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">345 M</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">1.14</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Large</td>
<td style="text-align: center;">774 M</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">1.15</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">8.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">XL</td>
<td style="text-align: center;">1.5 B</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.07</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">Ada</td>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">8.6</td>
</tr>
<tr>
<td style="text-align: left;">(black-box)</td>
<td style="text-align: left;">Babbage</td>
<td style="text-align: center;">1.3 B</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">7.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Curie</td>
<td style="text-align: center;">6.7 B</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">6.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Davinci</td>
<td style="text-align: center;">175 B</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">6.3</td>
</tr>
</tbody>
</table>
<p>Table 1: Both RePlug and RePlug LSR consistently enhanced the performance of different language models. Bits per byte (BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrievalaugmented versions (+REPLUG and +REPLUG LSR. The gain \% shows the relative improvement of our models compared to the original language model.
the Pile dataset has made efforts to deduplicate documents across train, validation and test splits (Gao et al., 2021), we did not do additional filtering. For both RePlug and RePlug LSR, we use a length of 128 -token context to do retrieval and adopt the ensemble method (Section 3.2) to incorporate top 10 retrieved documents during inference.</p>
<p>Results Table 1 reports the results of the original baselines, baselines augmented with the REPLUG, and baselines augmented with the REPLUG LSR. We observe that both RePlug and RePlug LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks. Furthermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in $7.7 \%$ improvement over baselines compared to $4.7 \%$ improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.</p>
<h3>6.2 MMLU</h3>
<p>Datasets MMLU (Hendrycks et al., 2021) is a multiple choice QA dataset that covers exam questions from 57 tasks including mathematics, US history and etc. The 57 tasks are grouped into 4 categories: humanities, STEM, social sciences and other. Following Chung et al. (2022a), we evaluate REPLUG in the 5-shot in-context learning setting.</p>
<p>Baselines We consider two groups of strong previous models as baselines for comparisons. The first group of baselines is the state-of-the-art LLMs including Codex $^{1}$ (Chen et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2021), PaLM (Chowdhery et al., 2022), and FlanPaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. Additionally, we include strong open-source LMs such as LLaMA (Touvron et al., 2023). The second group of baselines consists of retrieval-augmented language models. We only include Atlas (Izacard et al., 2022b) in this group, as no other retrieval-augmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting.</p>
<p>Our model We add RePlug and RePlug LSR to Codex and LLaMA because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retrieve 10 relevant documents from Wikipedia (2018, December) and prepend each retrieved document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together. The retriever interacts with Codex and LLaMA through black-box access.</p>
<p>Results Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG and REPLUG LSR improve the original Codex model by $4.5 \%$ and $5.1 \%$, respectively. In addition, REPLUG LSR largely outperforms the previous retrievalaugmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting. Although our models slightly underperform Flan-PaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;"># Parameters</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Social.</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Other</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: left;">540B</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: left;">Flan-PaLM</td>
<td style="text-align: left;">540B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.2</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.6</td>
</tr>
<tr>
<td style="text-align: left;">Atlas</td>
<td style="text-align: left;">11B</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug LSR</td>
<td style="text-align: left;">175B</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug LSR</td>
<td style="text-align: left;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.3</td>
</tr>
</tbody>
</table>
<p>Table 2: RePlug and RePlug LSR improves Codex by $\mathbf{4 . 5 \%}$ and $\mathbf{5 . 1 \%}$ respectively. Performance on MMLU broken down into 4 categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning with direct prompting.</p>
<p>Another interesting observation is that the REPlug LSR outperforms the original model by $1.9 \%$ even in the STEM category. This suggests that retrieval may improve a language model's problem-solving abilities.</p>
<h3>6.3 Open Domain QA</h3>
<p>Lastly, we conduct evaluation on two opendomain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">k-shot</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">k-shot</td>
<td style="text-align: center;">Full</td>
</tr>
<tr>
<td style="text-align: left;">Chinchilla</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PaLM</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Codex</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">RETRO $^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">R2-D2 ${ }^{\dagger}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: left;">Atlas $^{\dagger}$</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">$\mathbf{6 0 . 4}$</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Codex + RePlug LSR</td>
<td style="text-align: center;">$\mathbf{4 5 . 5}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 7 . 3}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA + RePlug LSR</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance on NQ and TQA. We report results for both k-shot ( 64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full data settings. Note that models with $\dagger$ are finetuned using training examples, while others use in-context learning.</p>
<p>Datasets NQ and TriviaQA are two open-domain QA datasets. Following prior work (Izacard and Grave, 2021; Si et al., 2023), we report Exact Match for the filtered set of TriviaQA. We consider the k -shot setting where the model is only given a few training examples and full data setting where the model is given all the training examples.</p>
<p>Baselines We compare our model with several state-of-the-art baselines, both in a few-shot set- ting and with full training data. The first group of models consists of powerful large language models, including Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), Codex and LLaMA 13B (Touvron et al., 2023). These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots. The second group of models for comparison includes retrieval-augmented language models such as RETRO (Borgeaud et al., 2022), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard et al., 2022b). All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting.</p>
<p>Our model We add RePlug and RePlug LSR to Codex and LLaMA 13B with Wikipedia as the retrieval corpus and evaluate them in a 16-shot in context learning. We incorporate top-10 retrieved documents using our proposed ensemble method.
Results As shown in Table 3, RePlug LSR significantly improves the performance of the original Codex by $12.0 \%$ on NQ and $5.0 \%$ on TQA. It outperforms the previous best model, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of near-duplicate test questions in the training set (e.g., Lewis et al. (2021) found that $32.5 \%$ of test questions overlap with the training sets in NQ).</p>
<h2>7 Analysis</h2>
<h3>7.1 REPLUG is applicable to diverse models</h3>
<p>Here we further study whether RePlug could enhance diverse language model families that have</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG. The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103.</p>
<p>been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT-2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Merity et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khandelwal et al., 2020), we use Wikitext-103 training data as the retrieval corpus.</p>
<p>Figure 4 shows the performance of differentsized LMs with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT-125M achieves 6.9% perplexity improvement, while OPT-66B achieves 5.6% perplexity improvement. Additionally, REPLUG improves the perplexity of all the model families, which indicates that REPLUG is applicable to diverse language models with different sizes.</p>
<h3>7.2 REPLUG performance gain does not simply come from the ensembling effect</h3>
<p>The core of our method design is the use of an ensemble method that combines output probabilities of different passes, in which each retrieved document is prepended separately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concatenated each random document with the input, and ensemble the outputs of different runs (referred to as "random"). As shown in Figure 5, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by REPLUG, and documents retrieved by REPLUG LSR. We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not come from the ensembling effect. Instead, ensembling the <strong>relevant</strong> documents is crucial for the success of REPLUG. Additionally, as more documents were ensembled, the performance of REPLUG and REPLUG LSR improved monotonically. However, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.</p>
<h3>7.3 LSR retriever outperforms other off-the-shelf retrievers</h3>
<p>We investigate the effectiveness of tunable retriever (LSR) compared with off-the-shelf retrievers. Specifically, we compare LM-supervised contriever (LSR) with other dense retrievers such as BERT-base (Borgeaud et al., 2022), DPR (Karpukhin et al., 2020) and a sparse retriever BM25 (Robertson et al., 2009). Figure 6 shows Wikitext-103 perplexity of GPT-2 XL (1.5B) and GPT-2 Large (774M) augmented with different retrievers. Among all off-the-shelf retrievers, the sparse retriever BM25 performs best. However, it still lags behind our LM supervised retriever (Contriever LSR), demonstrating the effectiveness of our training scheme that adapts the retriever to LMs.</p>
<h2>8 Conclusion</h2>
<p>We introduce REPLUG, a retrieval-augmented LM paradigm that augments black-box LMs with a tuneable retriever. This work opens up new possibilities for integrating retrieval into large black-box</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG and REPLUG LSR) when varying the number of documents.</p>
<p>LMs and is the first to demonstrate even the state-of-the-art LLMs could benefit from retrieval.</p>
<h2>9 Limitations</h2>
<p><strong>Interpretability</strong> REPLUG exhibits limitations in interpretability. It's unclear when the model relies on retrieved knowledge or on knowledge encoded within its own parameters. Future research could work towards the development of more interpretable retrieval-augmented language models. Such models could trace the source of the generated answers, whether it's from retrieved data or internal parameters, thus providing a clear knowledge provenance.</p>
<p><strong>On-demand retrieval</strong> REPLUG always perform retrieval no matter if the external information is needed. This approach runs the risk of presenting irrelevant documents, which can potentially distract the models, while also incurring additional computational overheads. Future studies could explore methods that allow the language model to determine when external knowledge is required.</p>
<p><strong>Database size</strong> In line with prior research, REPLUG uses Wikipedia and Pile as the targeted search databases. However, these resources might only encompass a minor fraction of the external knowledge needed by LMs. Future research should explore methods to efficiently expand these databases and examine how an LM's performance scales with the size of the database.</p>
<h2>References</h2>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: LM-supervised retriever (Contriever LSR) outperforms other off-the-shelf retrievers.</p>
<p>Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In <em>International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em>Proceedings of Machine Learning Research</em>, pages 2206â€“2240. PMLR.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya</p>
<p>Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022a. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022b. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416.</p>
<p>Martin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. 2021. R2-D2: A modular baseline for opendomain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 854-870, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The Pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint, abs/2101.00027.</p>
<p>Anirudh Goyal, Abram L. Friesen, Andrea Banino, Theophane Weber, Nan Rosemary Ke, AdriÃ  PuigdomÃ¨nech Badia, Arthur Guez, Mehdi Mirza, Peter C. Humphreys, Ksenia Konyushkova, Michal Valko, Simon Osindero, Timothy P. Lillicrap, Nicolas Heess, and Charles Blundell. 2022. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 7740-7765. PMLR.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3929-3938. PMLR.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. ArXiv preprint, abs/2203.15556.</p>
<p>Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. 2022. Promptcap: Prompt-guided task-aware image captioning. ArXiv preprint, abs/2211.09699.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Few-shot learning with retrieval augmented language models. ArXiv preprint, abs/2208.03299.</p>
<p>Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535-547.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. 2021. Question and answer test-train overlap in opendomain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1000-1008, Online. Association for Computational Linguistics.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. ArXiv preprint, abs/2212.10511.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Nonparametric masked language modeling. ArXiv preprint, abs/2212.01349.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9844-9855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847, Online. Association for Computational Linguistics.</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600-616.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. ArXiv preprint, abs/2211.05100.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2023. Prompting GPT-3 to be reliable. In Proc. of ICLR.</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1102-1121, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023. Retrieval-augmented multimodal language modeling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 39755-39769. PMLR.</p>
<p>Tim Dettmers Younes Belkda. 2022. A gentle introduction to 8-bit matrix multiplication.</p>
<p>Wenhao Yu. 2022. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, pages 52-58, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. ArXiv preprint, abs/2205.01068.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5657-5673, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Qualitative Analysis: Rare Entities Benefit from Retrieval</h2>
<p>To understand why the REPLUG improves language modeling performance, we conducted manual analysis of examples in which the REPLUG results in a decrease in perplexity. We find that REPLUG is more helpful when texts contain rare entities. Figure 7 shows a test context and its continuation from the Wikitext-103 test set. For REPLUG, we use the test context as a query to retrieve a relevant document from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its REPLUG enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by $11 \%$. Among all tokens in the continuation, we found that REPLUG is most helpful for the rare entity name "Li Bai". This is likely because the original LM does not have sufficient information about this rare entity name. However, by incorporating the retrieved document, REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Rare entities benefit from retrieval. After incorporating the retrieved document during inference, the entity "Li Bai" and the token "greatest" in the continuation show the most improvement in perplexity ( $15 \%$ for "Li Bai" and 5\% for "greatest"). Other tokens' perplexity changes are within $5 \%$.</p>
<h2>B Dense Retriever vs. Sparse Retriever</h2>
<p>The proposed model uses Contriever, a dense retriever, as its retriever backbone. Additionally, we investigate the performance of a sparse retriever in comparison to the dense retriever. For our sparse model, we employ BM25. As depicted in Figure 8, we observe that BM25 consistently outperforms Contriever but falls short when compared to LMsupervised Contriever, thus highlighting the effec-
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: PPL of GPT-2 models on Witext-103 with no retrieval (Origin), Contriever (REPLUG), LMsupervised Contriever (REPLUG LSR) and BM25.
tiveness of our proposed training scheme.</p>
<h2>C Prompts used for MMLU and open-domain QA</h2>
<p>Please see Table 4 and Table 5.</p>
<p>Knowledge: Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European
Question: As of 2015, since 1990 forests have $\qquad$ in Europe and have $\qquad$ in Africa and the Americas.
A. "increased, increased" B. "increased, decreased" C. "decreased, increased" D. "decreased, decreased"</p>
<p>Answer: B
Knowledge: Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, $56 \%$ of those age 18 to 29 favor gay marriage, $68 \%$ state environmental protection to be as important as job creation, $52 \%$ "think immigrants strengthen the country with their hard work and talents, ${ }^{4} 62 \%$ favor a "tax financed, government-administrated universal health care" program and 74\% "say peopleÅ› willÅ›hould have more influence on U.S. laws than the Bible, compared to $37 \%, 49 \%, 38 \%, 47 \%$ and $58 \%$ among the Question: As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?
A. $31 \%$ B. $46 \%$ C. $61 \%$ D. $76 \%$</p>
<p>Answer: B
Knowledge: last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule." Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries
Question: Which of the following countries generated the most total energy from solar sources in 2019?
A. China B. United States C. Germany D. Japan</p>
<p>Table 4: Prompt for MMLU</p>
<p>Knowledge: received 122,000 buys (excluding WWE Network views), down from the previous yearÅ› 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of "Raw", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the
Question: Who won the mens money in the bank match?
Answer: Braun Strowman
Knowledge: in 3D on March 17, 2017. The first official presentation of the film took place at DisneyÅ› three-day D23 Expo in August 2015. The world premiere of "Beauty and the Beast" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in
Question: When does beaty and the beast take place
Answer: Rococo-era
Knowledge: Love Yourself "Love Yourself" is a song recorded by Canadian singer Justin Bieber for his fourth studio album "Purpose" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the albumÅ› third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, "Love Yourself" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers. Lyrically, the song is a kiss-off to a narcissistic ex-lover who did
Question: love yourself by justin bieber is about who</p>
<p>Table 5: Prompt for open-domain QA</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Code-Davinci-002&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
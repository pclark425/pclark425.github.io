<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4976 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4976</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4976</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-9ffefdf1fcd780cb71450b0a7a29247c66aa87be</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ffefdf1fcd780cb71450b0a7a29247c66aa87be" target="_blank">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions.</p>
                <p><strong>Paper Abstract:</strong> Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4976.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4976.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-P (Explain-then-Predict) - OPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explain-then-Predict prompting paradigm applied to OPT (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that prepends an explanation before the label in each in-context example; the model is expected to generate explanation first then the prediction. Evaluated with OPT (175B) on three textual reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT (175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OPT 175B, an autoregressive transformer language model trained with causal language modeling (Zhang et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Explain-then-Predict (E-P)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompt examples contain an explicit human-written explanation before the label; at test time the model is prompted to generate an explanation first and then the answer. This uses a single explicit reasoning trace per example (no ensembling/diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT, E-SNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>SYNTH: synthetic multi-hop QA with controlled facts/distractors; ADVHOTPOT: adversarial HotpotQA multi-hop QA; E-SNLI: NLI with human explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SYNTH accuracy 29.6%; ADVHOTPOT accuracy 52.66%; E-SNLI accuracy 39.37% (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>P-E (Predict-then-Explain) on OPT: SYNTH 40.22%; ADVHOTPOT 43.34%; E-SNLI 43.41%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For OPT, E-P often failed to improve over few-shot and in SYNTH substantially degraded performance; E-P did slightly better than P-E on ADVHOTPOT but worse on other tasks, indicating no consistent benefit from this 'single-trace' explanation style for OPT.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>E-P decreased SYNTH accuracy drastically (29.6% vs few-shot 40.5%) showing that providing explanations before labels can hurt performance for OPT on some textual reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4976.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-P (Explain-then-Predict) - GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explain-then-Predict prompting paradigm applied to GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Explain-then-Predict prompting applied to GPT-3 (davinci); explanations are included before labels in exemplars and the model is asked to generate explanations then answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 'davinci' variant, large autoregressive LM trained with causal LM objective.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Explain-then-Predict (E-P)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single explicit explanation per example, generated prior to prediction; model produces explanation then label (pipeline-style).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT, E-SNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as above: synthetic multi-hop QA; adversarial HotpotQA; E-SNLI NLI explanations dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SYNTH accuracy 47.12%; ADVHOTPOT accuracy 54.14%; E-SNLI accuracy 40.42% (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>P-E on GPT-3: SYNTH 51.31%; ADVHOTPOT 48.74%; E-SNLI 48.72%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>E-P produced mixed results for GPT-3: helped on ADVHOTPOT but underperformed P-E (or few-shot) on some tasks; no uniform benefit from this single-explanation prompting across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On SYNTH and E-SNLI P-E or few-shot sometimes outperformed E-P, indicating E-P is not universally superior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4976.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-P (Explain-then-Predict) - InstructGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explain-then-Predict prompting paradigm applied to InstructGPT (text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>E-P prompting used with instruction-tuned InstructGPT; explanations inserted before the label and the model generates explanation then label.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant trained with human feedback to follow instructions better than base GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Explain-then-Predict (E-P)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-generated explanation is output before the prediction; exemplars include explanations to encourage model to produce reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT, E-SNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic controlled multi-hop QA; adversarial HotpotQA; E-SNLI NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SYNTH accuracy 58.57%; ADVHOTPOT accuracy 58.24%; E-SNLI accuracy 41.82% (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>P-E on InstructGPT: SYNTH 53.65%; ADVHOTPOT 51.53%; E-SNLI 59.42%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>E-P improved InstructGPT on the two QA tasks (notably SYNTH and ADVHOTPOT) relative to few-shot and sometimes relative to P-E, but on E-SNLI E-P substantially lagged P-E and even few-shot, demonstrating task-specific effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>E-P performed poorly on E-SNLI (41.82%) compared to P-E (59.42%), a strong negative result showing E-P can substantially hurt NLI performance for InstructGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4976.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-P (Explain-then-Predict) - text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explain-then-Predict prompting paradigm applied to text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>E-P prompting applied to the text-davinci-002 model; this model showed the largest gains from explanation prompting across datasets tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A more capable Instruct-series model (OpenAI) with instruction fine-tuning and RLHF modifications; exact training details not public.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Explain-then-Predict (E-P)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single explicit explanation generated before answer; exemplars contain explanations to guide reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT, E-SNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same three textual reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SYNTH accuracy 86.91%; ADVHOTPOT accuracy 82.45%; E-SNLI accuracy 75.67% (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>P-E on text-davinci-002: SYNTH 81.12%; ADVHOTPOT 77.24%; E-SNLI 69.45%</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>text-davinci-002 benefited substantially and consistently from E-P across all three tasks, with E-P outperforming P-E and few-shot; this model is an exception to the general pattern and shows that explanation prompting can dramatically help some instruction-tuned LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Despite strong gains for text-davinci-002, the paper notes lack of transparency about model differences and cautions against generalizing; other LMs did not show comparable gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4976.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P-E (Predict-then-Explain) - summary across models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predict-then-Explain prompting paradigm applied to multiple LLMs (GPT-3, OPT, InstructGPT, text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>P-E is a prompting style where the model first outputs a prediction and then an explanation; since the explanation is generated after the label (with greedy decoding) it does not influence the predicted label directly, but exemplar explanations still affect behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (OPT, GPT-3 davinci, InstructGPT, text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various LLMs tested in the paper: OPT (175B), GPT-3 davinci, InstructGPT (text-davinci-001), text-davinci-002 (more instruction tuned).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Predict-then-Explain (P-E)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Model is prompted to output an answer first and an explanation afterwards; exemplar explanations appear in prompt but the generated explanation follows the prediction and thus (with greedy decoding) does not influence the label.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT, E-SNLI</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same textual reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregated reported P-E results: OPT: SYNTH 39.6%, ADVHOTPOT 43.34%, E-SNLI 43.41%; GPT-3: SYNTH 51.31%, ADVHOTPOT 48.74%, E-SNLI 48.72%; InstructGPT: SYNTH 53.65%, ADVHOTPOT 51.53%, E-SNLI 59.42%; text-davinci-002: SYNTH 81.12%, ADVHOTPOT 77.24%, E-SNLI 69.45% (Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>E-P results for same models (see corresponding E-P entries); performance varies by model and task, no uniform superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>P-E is sometimes superior (e.g., E-SNLI for InstructGPT) and sometimes inferior; overall, there is no single best way to incorporate explanations — effectiveness is task- and model-dependent. LLMs frequently generate consistent explanations under P-E but these can be nonfactual.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>P-E did not consistently improve accuracy; on ADVHOTPOT E-P often outperformed P-E, while on some NLI settings P-E was better, illustrating negative results for a single-trace approach depending on context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4976.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-k candidate filtering via explanation factuality (InstructGPT on SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative top-5 candidate answer rejection using model-generated explanation factuality (applied to InstructGPT on SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diverse/ensemble-style post-hoc method: generate multiple candidate answers with explanations, reject any candidate whose explanation is nonfactual, and return the first candidate with a factual explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant used with greedy decoding; API returns top-k candidate completions which are checked sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Top-k candidate filtering using explanation factuality</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate up to top-5 candidate answer+explanation pairs; automatically check explanation factuality (exact matching to context in SYNTH) and reject nonfactual ones until a factual explanation is found; effectively ensembles multiple candidate reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH (synthetic multi-hop QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controlled synthetic multi-hop QA where factuality can be exactly checked by matching explanation sentences to context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>P-E baseline accuracy 52.4%; after iterative top-5 candidate rejection based on factuality, accuracy improved to 74.8% (reported in Section 4.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-candidate P-E without filtering: 52.4% (instructGPT on SYNTH)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple candidate reasoning traces plus a factuality-check filter (a form of diverse reasoning/ensemble) produced a large performance gain (≈22.4 percentage points) on SYNTH, demonstrating that diversity plus verification can substantially help when explanations can be reliably judged.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>This approach depends on a reliable factuality checker and on the ability to obtain multiple candidate outputs; authors note the factuality checking is trivial in SYNTH but harder in real data, so gains may not generalize without robust verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4976.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>"Let's think step by step" trigger</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>"Let's think step by step" chain-of-thought trigger in exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short prompt trigger appended to explanations intended to induce multi-step chain-of-thought style reasoning. Tested on SYNTH and ADVHOTPOT for some models and found not to meaningfully improve and sometimes degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci) and InstructGPT (text-davinci-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LMs used with instruction prompts containing the chain-of-thought trigger.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-thought trigger ('Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Appending an explicit 'think step by step' sentence to exemplar explanations to elicit multi-step internal reasoning traces during generation (single reasoning trace per example).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH, ADVHOTPOT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic multi-hop QA and adversarial HotpotQA multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No statistically significant improvements; per Table 8 E-P + TRIGGER: davinci SYNTH 48.6% (vs E-P 47.1%), davinci ADVHOTPOT 50.1% (vs E-P 54.1% — actually degraded); text-davinci-001 E-P+TRIGGER: SYNTH 58.0% (vs E-P 58.5%), ADVHOTPOT 58.0% (vs E-P 58.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>E-P without trigger (see numbers above); chain-of-thought trigger generally did not help and sometimes hurt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding 'let's think step by step' in exemplars did not consistently improve performance for these textual reasoning tasks and sometimes degraded it; thus prompting for explicit chain-of-thought traces does not guarantee gains in this domain and is task/model dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For davinci on ADVHOTPOT the trigger decreased performance (E-P 54.1% → E-P+TRIGGER 50.1%), showing a notable negative result of this similar-chain prompting approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4976.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alternative explanation order (reversed) - summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alternative exemplar explanation style where the sentence order of multi-sentence explanations is reversed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variation in explanation formatting: reversing the order of two sentences in SYNTH explanations (A [verb] B and B is [profession] vs B is [profession] and A [verb] B), tested to see if explanation style/ordering affects learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (davinci), InstructGPT (text-davinci-001), text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various LLMs tested with the alternative explanation ordering prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Alternative explanation order (prompt format change)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Changing the order/format of the explanatory sentences in exemplars to alter the presented reasoning path; still a single reasoning trace per exemplar.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SYNTH (synthetic multi-hop QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Synthetic dataset with two-sentence reasoning chains; used to test effects of explanation formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 6: e.g., text-davinci-002: FEW-SHOT 72.0% → E-P (ALT) 75.3% → P-E (ALT) 80.5%; for GPT-3 and InstructGPT the alternative style often yields inferior or similar performance compared to original formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Original explanation style E-P/P-E: text-davinci-002 E-P 86.91% / P-E 81.12% (original) vs alternative: E-P 75.3% / P-E 80.5%; thus alternative ordering usually underperformed original E-P for most models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small changes in explanation formatting can affect which paradigm (E-P vs P-E) performs better; in the alternative (reversed) style, P-E tended to outperform E-P for several models and factuality remained a strong indicator of correctness, but overall the reversed style often produced inferior performance compared to the original formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Reversing sentence order degraded E-P performance for GPT-3 and InstructGPT on SYNTH, showing that even small prompt-format choices can negatively impact single-trace explanation prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4976.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4976.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanation-based calibration (EXPLCal) - InstructGPT/text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning-based calibrator using features extracted from model-generated explanations to reweight prediction probabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-hoc calibration framework that uses a factuality score derived from model-generated explanations (lexical overlap) together with predicted probabilities to learn a small affine transform that improves accuracy/confidence calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (primary experiments) and text-davinci-002 (evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs; calibrator is a lightweight trained module on a small additional dataset, applied to black-box model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Explanation-based calibration (EXPLCal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Extract a scalar factuality score v from the generated explanation (lexical overlap or heuristics) and combine with model's probability vector p in a learned linear transform softmax(W[p; v]+b) to adjust confidences and optionally change predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>E-SNLI, ADVHOTPOT, SYNTH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NLI and QA tasks where explanations can be heuristically matched to inputs to produce factuality signals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>E-SNLI: P-E + EXPLCal achieved up to 68.5% accuracy with 128 training examples (≈ +12% over vanilla few-shot), ADVHOTPOT AUC: E-P+EXPLCal 68.8 (vs FEW-SHOT 59.6, E-P 64.4) per Table 4; SYNTH improvements also reported when using factuality checks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Probability-only calibrator (ProbCal) and non-explanation calibrators: weaker – e.g., P-E+ProbCal and Few-Shot+ProbCal gave lower improvements than EXPLCal (see Table 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using features derived from explanations (even simple lexical overlap) in a calibrated reweighting substantially improves selective QA calibration and accuracy compared to probability-only calibration or uncalibrated prompting, demonstrating that explanation-derived signals (even imperfect) are useful for post-hoc selection/abstention.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The factuality proxy (lexical overlap) is a weak signal and domain-dependent; gains rely on being able to extract a meaningful factuality score, which is easy for SYNTH but harder for open text, limiting generality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners. <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models. <em>(Rating: 2)</em></li>
                <li>Can language models learn from explanations in context? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4976",
    "paper_id": "paper-9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "E-P (Explain-then-Predict) - OPT",
            "name_full": "Explain-then-Predict prompting paradigm applied to OPT (175B)",
            "brief_description": "A prompting paradigm that prepends an explanation before the label in each in-context example; the model is expected to generate explanation first then the prediction. Evaluated with OPT (175B) on three textual reasoning datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT (175B)",
            "model_description": "OPT 175B, an autoregressive transformer language model trained with causal language modeling (Zhang et al., 2022).",
            "reasoning_method_name": "Explain-then-Predict (E-P)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Prompt examples contain an explicit human-written explanation before the label; at test time the model is prompted to generate an explanation first and then the answer. This uses a single explicit reasoning trace per example (no ensembling/diversity).",
            "task_name": "SYNTH, ADVHOTPOT, E-SNLI",
            "task_description": "SYNTH: synthetic multi-hop QA with controlled facts/distractors; ADVHOTPOT: adversarial HotpotQA multi-hop QA; E-SNLI: NLI with human explanations.",
            "performance": "SYNTH accuracy 29.6%; ADVHOTPOT accuracy 52.66%; E-SNLI accuracy 39.37% (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "P-E (Predict-then-Explain) on OPT: SYNTH 40.22%; ADVHOTPOT 43.34%; E-SNLI 43.41%",
            "key_findings": "For OPT, E-P often failed to improve over few-shot and in SYNTH substantially degraded performance; E-P did slightly better than P-E on ADVHOTPOT but worse on other tasks, indicating no consistent benefit from this 'single-trace' explanation style for OPT.",
            "counter_examples_or_negative_results": "E-P decreased SYNTH accuracy drastically (29.6% vs few-shot 40.5%) showing that providing explanations before labels can hurt performance for OPT on some textual reasoning tasks.",
            "uuid": "e4976.0",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "E-P (Explain-then-Predict) - GPT-3 (davinci)",
            "name_full": "Explain-then-Predict prompting paradigm applied to GPT-3 (davinci)",
            "brief_description": "Explain-then-Predict prompting applied to GPT-3 (davinci); explanations are included before labels in exemplars and the model is asked to generate explanations then answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci)",
            "model_description": "GPT-3 'davinci' variant, large autoregressive LM trained with causal LM objective.",
            "reasoning_method_name": "Explain-then-Predict (E-P)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single explicit explanation per example, generated prior to prediction; model produces explanation then label (pipeline-style).",
            "task_name": "SYNTH, ADVHOTPOT, E-SNLI",
            "task_description": "Same as above: synthetic multi-hop QA; adversarial HotpotQA; E-SNLI NLI explanations dataset.",
            "performance": "SYNTH accuracy 47.12%; ADVHOTPOT accuracy 54.14%; E-SNLI accuracy 40.42% (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "P-E on GPT-3: SYNTH 51.31%; ADVHOTPOT 48.74%; E-SNLI 48.72%",
            "key_findings": "E-P produced mixed results for GPT-3: helped on ADVHOTPOT but underperformed P-E (or few-shot) on some tasks; no uniform benefit from this single-explanation prompting across tasks.",
            "counter_examples_or_negative_results": "On SYNTH and E-SNLI P-E or few-shot sometimes outperformed E-P, indicating E-P is not universally superior.",
            "uuid": "e4976.1",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "E-P (Explain-then-Predict) - InstructGPT",
            "name_full": "Explain-then-Predict prompting paradigm applied to InstructGPT (text-davinci-001)",
            "brief_description": "E-P prompting used with instruction-tuned InstructGPT; explanations inserted before the label and the model generates explanation then label.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-001)",
            "model_description": "Instruction-tuned GPT-3 variant trained with human feedback to follow instructions better than base GPT-3.",
            "reasoning_method_name": "Explain-then-Predict (E-P)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-generated explanation is output before the prediction; exemplars include explanations to encourage model to produce reasoning traces.",
            "task_name": "SYNTH, ADVHOTPOT, E-SNLI",
            "task_description": "Synthetic controlled multi-hop QA; adversarial HotpotQA; E-SNLI NLI.",
            "performance": "SYNTH accuracy 58.57%; ADVHOTPOT accuracy 58.24%; E-SNLI accuracy 41.82% (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "P-E on InstructGPT: SYNTH 53.65%; ADVHOTPOT 51.53%; E-SNLI 59.42%",
            "key_findings": "E-P improved InstructGPT on the two QA tasks (notably SYNTH and ADVHOTPOT) relative to few-shot and sometimes relative to P-E, but on E-SNLI E-P substantially lagged P-E and even few-shot, demonstrating task-specific effectiveness.",
            "counter_examples_or_negative_results": "E-P performed poorly on E-SNLI (41.82%) compared to P-E (59.42%), a strong negative result showing E-P can substantially hurt NLI performance for InstructGPT.",
            "uuid": "e4976.2",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "E-P (Explain-then-Predict) - text-davinci-002",
            "name_full": "Explain-then-Predict prompting paradigm applied to text-davinci-002",
            "brief_description": "E-P prompting applied to the text-davinci-002 model; this model showed the largest gains from explanation prompting across datasets tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "A more capable Instruct-series model (OpenAI) with instruction fine-tuning and RLHF modifications; exact training details not public.",
            "reasoning_method_name": "Explain-then-Predict (E-P)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single explicit explanation generated before answer; exemplars contain explanations to guide reasoning.",
            "task_name": "SYNTH, ADVHOTPOT, E-SNLI",
            "task_description": "Same three textual reasoning tasks.",
            "performance": "SYNTH accuracy 86.91%; ADVHOTPOT accuracy 82.45%; E-SNLI accuracy 75.67% (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "P-E on text-davinci-002: SYNTH 81.12%; ADVHOTPOT 77.24%; E-SNLI 69.45%",
            "key_findings": "text-davinci-002 benefited substantially and consistently from E-P across all three tasks, with E-P outperforming P-E and few-shot; this model is an exception to the general pattern and shows that explanation prompting can dramatically help some instruction-tuned LLMs.",
            "counter_examples_or_negative_results": "Despite strong gains for text-davinci-002, the paper notes lack of transparency about model differences and cautions against generalizing; other LMs did not show comparable gains.",
            "uuid": "e4976.3",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "P-E (Predict-then-Explain) - summary across models",
            "name_full": "Predict-then-Explain prompting paradigm applied to multiple LLMs (GPT-3, OPT, InstructGPT, text-davinci-002)",
            "brief_description": "P-E is a prompting style where the model first outputs a prediction and then an explanation; since the explanation is generated after the label (with greedy decoding) it does not influence the predicted label directly, but exemplar explanations still affect behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (OPT, GPT-3 davinci, InstructGPT, text-davinci-002)",
            "model_description": "Various LLMs tested in the paper: OPT (175B), GPT-3 davinci, InstructGPT (text-davinci-001), text-davinci-002 (more instruction tuned).",
            "reasoning_method_name": "Predict-then-Explain (P-E)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Model is prompted to output an answer first and an explanation afterwards; exemplar explanations appear in prompt but the generated explanation follows the prediction and thus (with greedy decoding) does not influence the label.",
            "task_name": "SYNTH, ADVHOTPOT, E-SNLI",
            "task_description": "Same textual reasoning datasets.",
            "performance": "Aggregated reported P-E results: OPT: SYNTH 39.6%, ADVHOTPOT 43.34%, E-SNLI 43.41%; GPT-3: SYNTH 51.31%, ADVHOTPOT 48.74%, E-SNLI 48.72%; InstructGPT: SYNTH 53.65%, ADVHOTPOT 51.53%, E-SNLI 59.42%; text-davinci-002: SYNTH 81.12%, ADVHOTPOT 77.24%, E-SNLI 69.45% (Table 1)",
            "comparison_with_other_method": true,
            "performance_other_method": "E-P results for same models (see corresponding E-P entries); performance varies by model and task, no uniform superiority.",
            "key_findings": "P-E is sometimes superior (e.g., E-SNLI for InstructGPT) and sometimes inferior; overall, there is no single best way to incorporate explanations — effectiveness is task- and model-dependent. LLMs frequently generate consistent explanations under P-E but these can be nonfactual.",
            "counter_examples_or_negative_results": "P-E did not consistently improve accuracy; on ADVHOTPOT E-P often outperformed P-E, while on some NLI settings P-E was better, illustrating negative results for a single-trace approach depending on context.",
            "uuid": "e4976.4",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Top-k candidate filtering via explanation factuality (InstructGPT on SYNTH)",
            "name_full": "Iterative top-5 candidate answer rejection using model-generated explanation factuality (applied to InstructGPT on SYNTH)",
            "brief_description": "A diverse/ensemble-style post-hoc method: generate multiple candidate answers with explanations, reject any candidate whose explanation is nonfactual, and return the first candidate with a factual explanation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-001)",
            "model_description": "Instruction-tuned GPT-3 variant used with greedy decoding; API returns top-k candidate completions which are checked sequentially.",
            "reasoning_method_name": "Top-k candidate filtering using explanation factuality",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate up to top-5 candidate answer+explanation pairs; automatically check explanation factuality (exact matching to context in SYNTH) and reject nonfactual ones until a factual explanation is found; effectively ensembles multiple candidate reasoning traces.",
            "task_name": "SYNTH (synthetic multi-hop QA)",
            "task_description": "Controlled synthetic multi-hop QA where factuality can be exactly checked by matching explanation sentences to context.",
            "performance": "P-E baseline accuracy 52.4%; after iterative top-5 candidate rejection based on factuality, accuracy improved to 74.8% (reported in Section 4.1).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-candidate P-E without filtering: 52.4% (instructGPT on SYNTH)",
            "key_findings": "Using multiple candidate reasoning traces plus a factuality-check filter (a form of diverse reasoning/ensemble) produced a large performance gain (≈22.4 percentage points) on SYNTH, demonstrating that diversity plus verification can substantially help when explanations can be reliably judged.",
            "counter_examples_or_negative_results": "This approach depends on a reliable factuality checker and on the ability to obtain multiple candidate outputs; authors note the factuality checking is trivial in SYNTH but harder in real data, so gains may not generalize without robust verification.",
            "uuid": "e4976.5",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "\"Let's think step by step\" trigger",
            "name_full": "\"Let's think step by step\" chain-of-thought trigger in exemplars",
            "brief_description": "A short prompt trigger appended to explanations intended to induce multi-step chain-of-thought style reasoning. Tested on SYNTH and ADVHOTPOT for some models and found not to meaningfully improve and sometimes degrade performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci) and InstructGPT (text-davinci-001)",
            "model_description": "Large autoregressive LMs used with instruction prompts containing the chain-of-thought trigger.",
            "reasoning_method_name": "Chain-of-thought trigger ('Let's think step by step')",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Appending an explicit 'think step by step' sentence to exemplar explanations to elicit multi-step internal reasoning traces during generation (single reasoning trace per example).",
            "task_name": "SYNTH, ADVHOTPOT",
            "task_description": "Synthetic multi-hop QA and adversarial HotpotQA multi-hop QA.",
            "performance": "No statistically significant improvements; per Table 8 E-P + TRIGGER: davinci SYNTH 48.6% (vs E-P 47.1%), davinci ADVHOTPOT 50.1% (vs E-P 54.1% — actually degraded); text-davinci-001 E-P+TRIGGER: SYNTH 58.0% (vs E-P 58.5%), ADVHOTPOT 58.0% (vs E-P 58.2%).",
            "comparison_with_other_method": true,
            "performance_other_method": "E-P without trigger (see numbers above); chain-of-thought trigger generally did not help and sometimes hurt.",
            "key_findings": "Adding 'let's think step by step' in exemplars did not consistently improve performance for these textual reasoning tasks and sometimes degraded it; thus prompting for explicit chain-of-thought traces does not guarantee gains in this domain and is task/model dependent.",
            "counter_examples_or_negative_results": "For davinci on ADVHOTPOT the trigger decreased performance (E-P 54.1% → E-P+TRIGGER 50.1%), showing a notable negative result of this similar-chain prompting approach.",
            "uuid": "e4976.6",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Alternative explanation order (reversed) - summary",
            "name_full": "Alternative exemplar explanation style where the sentence order of multi-sentence explanations is reversed",
            "brief_description": "A variation in explanation formatting: reversing the order of two sentences in SYNTH explanations (A [verb] B and B is [profession] vs B is [profession] and A [verb] B), tested to see if explanation style/ordering affects learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 (davinci), InstructGPT (text-davinci-001), text-davinci-002",
            "model_description": "Various LLMs tested with the alternative explanation ordering prompt.",
            "reasoning_method_name": "Alternative explanation order (prompt format change)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Changing the order/format of the explanatory sentences in exemplars to alter the presented reasoning path; still a single reasoning trace per exemplar.",
            "task_name": "SYNTH (synthetic multi-hop QA)",
            "task_description": "Synthetic dataset with two-sentence reasoning chains; used to test effects of explanation formatting.",
            "performance": "Table 6: e.g., text-davinci-002: FEW-SHOT 72.0% → E-P (ALT) 75.3% → P-E (ALT) 80.5%; for GPT-3 and InstructGPT the alternative style often yields inferior or similar performance compared to original formatting.",
            "comparison_with_other_method": true,
            "performance_other_method": "Original explanation style E-P/P-E: text-davinci-002 E-P 86.91% / P-E 81.12% (original) vs alternative: E-P 75.3% / P-E 80.5%; thus alternative ordering usually underperformed original E-P for most models.",
            "key_findings": "Small changes in explanation formatting can affect which paradigm (E-P vs P-E) performs better; in the alternative (reversed) style, P-E tended to outperform E-P for several models and factuality remained a strong indicator of correctness, but overall the reversed style often produced inferior performance compared to the original formatting.",
            "counter_examples_or_negative_results": "Reversing sentence order degraded E-P performance for GPT-3 and InstructGPT on SYNTH, showing that even small prompt-format choices can negatively impact single-trace explanation prompting.",
            "uuid": "e4976.7",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Explanation-based calibration (EXPLCal) - InstructGPT/text-davinci-002",
            "name_full": "Learning-based calibrator using features extracted from model-generated explanations to reweight prediction probabilities",
            "brief_description": "A post-hoc calibration framework that uses a factuality score derived from model-generated explanations (lexical overlap) together with predicted probabilities to learn a small affine transform that improves accuracy/confidence calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (primary experiments) and text-davinci-002 (evaluations)",
            "model_description": "Instruction-tuned LLMs; calibrator is a lightweight trained module on a small additional dataset, applied to black-box model outputs.",
            "reasoning_method_name": "Explanation-based calibration (EXPLCal)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Extract a scalar factuality score v from the generated explanation (lexical overlap or heuristics) and combine with model's probability vector p in a learned linear transform softmax(W[p; v]+b) to adjust confidences and optionally change predictions.",
            "task_name": "E-SNLI, ADVHOTPOT, SYNTH",
            "task_description": "NLI and QA tasks where explanations can be heuristically matched to inputs to produce factuality signals.",
            "performance": "E-SNLI: P-E + EXPLCal achieved up to 68.5% accuracy with 128 training examples (≈ +12% over vanilla few-shot), ADVHOTPOT AUC: E-P+EXPLCal 68.8 (vs FEW-SHOT 59.6, E-P 64.4) per Table 4; SYNTH improvements also reported when using factuality checks.",
            "comparison_with_other_method": true,
            "performance_other_method": "Probability-only calibrator (ProbCal) and non-explanation calibrators: weaker – e.g., P-E+ProbCal and Few-Shot+ProbCal gave lower improvements than EXPLCal (see Table 3 and 4).",
            "key_findings": "Using features derived from explanations (even simple lexical overlap) in a calibrated reweighting substantially improves selective QA calibration and accuracy compared to probability-only calibration or uncalibrated prompting, demonstrating that explanation-derived signals (even imperfect) are useful for post-hoc selection/abstention.",
            "counter_examples_or_negative_results": "The factuality proxy (lexical overlap) is a weak signal and domain-dependent; gains rely on being able to extract a meaningful factuality score, which is easy for SYNTH but harder for open text, limiting generality.",
            "uuid": "e4976.8",
            "source_info": {
                "paper_title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners.",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models.",
            "rating": 2
        },
        {
            "paper_title": "Can language models learn from explanations in context?",
            "rating": 1
        }
    ],
    "cost": 0.019964,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</h1>
<p>Xi Ye Greg Durrett<br>Department of Computer Science<br>The University of Texas at Austin<br>{xiye, gdurrett}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good-logically consistent with the input and the prediction-more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets. ${ }^{1}$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Prompting GPT-3 with explanations. By including explanations in the in-context examples, we can cause GPT-3 to generate an explanation for the test example as well. In this case, the generated explanation is nonfactual, despite the simple reasoning involved here. However, we show this nonfactuality actually provides a signal that can help calibrate the model.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Recent scaling of pre-training has empowered large language models (LLMs) to learn NLP tasks from just a few training examples "in context," without updating the model’s parameters Brown2020. However, this learning process is still poorly understood: models are biased by the order of in-context examples Zhao2021 and may not leverage the instructions or even the labels of the examples in the ways one expects Min2022, Webson2022. Existing tools for interpreting model predictions have high computational cost Ribeiro2016 or require access to gradients Simonyan2014, Sundararajan2017, making them unsuitable for investigating in-context learning or explaining the predictions of prompted models.</p>
<p>One appealing way to gain more insight into predictions obtained through in-context learning is to let the language model "explain itself" Nye2021, Wei2022, Chowdhery2022, Marasovic2022, Lampinen2022. In addition to input-label training pairs in context, one can prompt the language model with an explanation for each pair and trigger the model to generate an explanation for its prediction Figure 1. Prompting with explanations introduces much richer information compared to using labels alone, which might guide the inference process and allow the model to learn more information from the examples.</p>
<p>In this work, we investigate the nature of the explanations that LLMs generate and whether they can improve few-shot in-context learning for textual reasoning tasks, specifically QA and NLI. Recent prior work that finds success with this approach largely targets symbolic reasoning tasks with a very different structure, such as math word problem solving Nye2021, Wei2022. We experiment on three different datasets spanning QA and NLI with four LLMs: OPT, GPT-3 (davinci), InstructGPT (text-davinci-001), and text-davinci-002. The results suggest that explanations only substantially improve accuracy for text-davinci-002, but give a smaller improvement or even hurt the performance with the other LLMs.</p>
<p>Surprisingly, we find that the explanations generated by LLMs can be unreliable, even for a very simple synthetic dataset. We evaluate the explanations along two axes: <em>factuality</em>, whether the explanation is correctly grounded in the input, and <em>consistency</em>, whether the explanation entails the final prediction. LLMs tend to generate consistent explanations that account for the predictions, but the explanations may not be factual, as as shown in Figure 1. Furthermore, our analysis suggests an unreliable explanation more likely indicates a wrong prediction compared to a reliable explanation.</p>
<p>Despite LLMs’ failures here, we can still benefit from model-generated explanations by using them for calibration. If we are able to automatically assess the reliability of an explanation, we can allow an LLM to return a null answer when its explanation is unreliable, since the prediction in this case is less likely to be correct. Unfortunately, there is no automated way to perfectly assess the reliability, but we can extract features that approximately reflect it. We use these features to calibrate InstructGPT’s predictions, and successfully improve the in-context learning performance across all the datasets.</p>
<p>In summary, our main findings are: (1) Simply plugging explanations into the prompt does not always substantially boost the in-context learning performance for textual reasoning. (2) LLMs generate explanations consistent with their predictions, but these explanations might not be factually grounded in the inputs. (3) The factuality of an explanation can serve as an indicator for the correctness of the corresponding prediction. (4) Using features that can approximate the factuality of explanations, we successfully use explanations to improve the in-context learning performance across all tasks.</p>
<h2>2 Does Prompting with Explanations Improve In-Context Learning?</h2>
<p>In this paper, we specifically focus on tasks involving reasoning over natural language. These are tasks where explanations have been traditionally studied Camburu2018, Rajani2019, but which are more complex than tasks like sentiment analysis which are well explained by extractive rationales Zaidan2007, DeYoung2020. We experiment on two tasks,</p>
<p>[2] Throughout our paper, we primarily test on InstructGPT for two reasons. First, it was the most capable model available at the time we were conducting the majority of our experiments. Second, it still has significant room to improve on the datasets we explore in this work. This setting is a representative testbed for the situation where an LLM-based system does not yet give satisfactory performance on a target task, causing the system designer to turn to explanations in prompts to improve things.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Context:</th>
<th style="text-align: center;">Christopher agrees with Kevin. Tiffany agrees with Matthew. Mary hangs out with Danielle. James hangs out</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">with Thomas. Kevin is a student. Matthew is a plumber. Danielle is a student. Thomas is a plumber.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Question:</td>
<td style="text-align: center;">Who hangs out with a student?</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Answer:</td>
<td style="text-align: center;">Mary Explanation: Danielle is a student and Mary hangs out with Danielle.</td>
</tr>
</tbody>
</table>
<p>Figure 2: A SYNTH example and an E-SNLI example. See Figure 3 for ADVHOTPOT examples.
reading comprehension question answering (QA) and natural language inference (NLI), on three English-language datasets. For each dataset, we create a test set with 250 examples.</p>
<h1>2.1 Datasets</h1>
<p>Synthetic Multi-hop QA (Synth) In order to have a controlled setting where we can easily understand whether explanations are factual and consistent with the answer, we create a synthetic multi-hop QA dataset. Shown in Figure 2, each example in this dataset asks a bridge question (using the terminology of Yang et al. (2018)) over a context consisting of supporting facts paired with controlled distractors. This dataset is carefully designed to avoid spurious correlations, giving us full understanding over the correct reasoning process and the explanation for every example, which naturally consists of the two supporting sentences. See Appendix B for full details of this dataset. ${ }^{3}$</p>
<p>Adversarial HotpotQA (ADVHotpot) We also test on the English-language Adversarial HotpotQA dataset (Yang et al., 2018; Jiang and Bansal, 2019). We use the adversarially augmented version since InstructGPT achieves high performance on the distractor setting of the original dataset. We make a challenging set of examples by balancing sets of questions on which InstructGPT makes correct and incorrect predictions. The context of each question includes two ground truth supporting paragraphs and two adversarial paragraphs. Full details of preprocessing the ADVHOTPOT dataset can be found in Appendix C.</p>
<p>For ADVHOTPOT, we manually annotated explanations for the training examples. Figure 1 shows an example of such an explanation, highlighted in orange. We could use the supporting sentences as the explanations, but we found they are usually too verbose and not sufficient, e.g., with anaphors that resolve outside of the supporting sentences. Therefore, we manually annotate a set of explanations which clearly describe the reasoning path for each question.</p>
<p>E-SNLI E-SNLI (Camburu et al., 2018) is an English-language classification dataset commonly used to study explanations, released under the MIT license. Shown in Figure 2, each example consists of a premise and a hypothesis, and the task is to classify the hypothesis as entailed by, contradicted by, or neutral with respect to the premise. As a notable contrast to the other datasets, the explanations here are more abstract natural language written by human annotators, as opposed to mostly constructed from extracted snippets of context.</p>
<h3>2.2 Baselines</h3>
<p>We study the effectiveness of plugging in explanations by comparing the in-context learning performance of prompting with or without explanations. Prompting without explanations resembles the standard few-shot in-context learning approach (Few-Shot). To incorporate explanations into the prompt, we consider the following two most commonly used paradigms:
Explain-then-Predict (E-P) prepends an explanation before the label (Figure 1). The language model is expected to generate an explanation first followed by the prediction. The prompting style of past work involving computational traces can be categorized into this paradigm, including Nye et al. (2021) and Wei et al. (2022). This approach is also called a pipeline model in other literature on training models using explanations (Jacovi and Goldberg, 2021; Wiegreffe et al., 2021).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results of prompting with explanations on four large language models. Using explanations leads to small to moderate improves performance on OPT, GPT-3, and InstructGPT, and has more prominent effects on text-davinci-002.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SYNTH</th>
<th style="text-align: center;">ADVHOTPOT</th>
<th style="text-align: center;">E-SNLI</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OPT (175B)</td>
<td style="text-align: center;">FEw-Short</td>
<td style="text-align: center;">40.52 .8</td>
<td style="text-align: center;">49.72 .6</td>
<td style="text-align: center;">44.02 .8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">29.60 .5</td>
<td style="text-align: center;">52.66 .5</td>
<td style="text-align: center;">39.37 .8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E</td>
<td style="text-align: center;">40.22 .6</td>
<td style="text-align: center;">43.34 .5</td>
<td style="text-align: center;">43.41 .6</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">FEw-Short</td>
<td style="text-align: center;">49.50 .6</td>
<td style="text-align: center;">49.16 .2</td>
<td style="text-align: center;">43.35 .7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">47.12 .8</td>
<td style="text-align: center;">54.14 .1</td>
<td style="text-align: center;">40.42 .5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E</td>
<td style="text-align: center;">51.31 .8</td>
<td style="text-align: center;">48.74 .6</td>
<td style="text-align: center;">48.72 .4</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">FEw-Short</td>
<td style="text-align: center;">54.85 .1</td>
<td style="text-align: center;">53.22 .3</td>
<td style="text-align: center;">56.82 .0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">58.57 .1</td>
<td style="text-align: center;">58.24 .1</td>
<td style="text-align: center;">41.82 .5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E</td>
<td style="text-align: center;">53.65 .0</td>
<td style="text-align: center;">51.53 .4</td>
<td style="text-align: center;">59.42 .0</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">FEw-Short</td>
<td style="text-align: center;">72.01 .4</td>
<td style="text-align: center;">77.77 .2</td>
<td style="text-align: center;">69.12 .0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">86.91 .8</td>
<td style="text-align: center;">82.45 .1</td>
<td style="text-align: center;">75.67 .6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E</td>
<td style="text-align: center;">81.12 .8</td>
<td style="text-align: center;">77.24 .8</td>
<td style="text-align: center;">69.45 .0</td>
</tr>
</tbody>
</table>
<p>Predict-then-Explain (P-E) generates the explanation after the prediction. Unlike E-P, the predicted explanation does not influence the predicted label, since we use greedy inference and the explanation comes afterwards. However, the explanations in the prompt still impact the predictions.</p>
<h1>2.3 Setup</h1>
<p>For few-shot learning, we use roughly the maximum allowed shots in the prompt that can fit the length limit of OPT (Zhang et al., 2022) and GPT-3 (Brown et al., 2020), which is 16 for SYNTH, 6 for ADVHOTPOT, and 32 for E-SNLI, respectively. ${ }^{4}$ We experiment with four LLMs, including OPT (175B), GPT-3 (davinci), InstructGPT (text-davinci-001), and text-davinci-002. OPT and GPT-3 are trained using the standard causal language modeling objective, whereas InstructGPT and text-davinci-002 are trained with special instruction data and human annotations. We generate outputs with greedy decoding (temperature set to be 0 ). Our prompt formats follow those in Brown et al. (2020). The explanations are inserted before/after the prediction with conjunction words like because. Please refer to Appendix A for full prompts. Because the results of in-context learning vary with the examples presented in the input prompt, for each dataset, we randomly sample multiple groups of training shots, and report the mean and standard deviation of the results (subscript). We use 5 groups for InstructGPT, the primary LM we are using throughout our paper, and 3 groups for the rest.</p>
<h3>2.4 Results</h3>
<p>As shown in Table 1, OPT, GPT-3, and InstructGPT show small to moderate improvements from using explanations for textual reasoning tasks. On the two QA tasks, SYNTH and ADVHOTPOT, E-P improves the performance of InstructGPT, the best among these three LMs, from 54.8 to 58.5 and 56.8 to 59.4 , respectively. ${ }^{5}$ On E-SNLI, P-E outperforms FEw-Short by 2.6, whereas E-P substantially lags FEw-SHOT. Comparing E-P against P-E on SYNTH and E-SNLI, E-P typically degrades performance (except on SYNTH for InstructGPT) and P-E is inconsistent across the different models, whereas E-P consistently leads to performance improvements on ADVHOTPOT. There is no single winner between the two paradigms of using explanations; choosing the most effective way is task-specific. Overall, vanilla LLMs (OPT and GPT-3) see limited benefit from producing explanations, and even the Instruct-series InstructGPT does not see substantial improvements.</p>
<p>The only exception is text-davinci-002. text-davinci-002 greatly benefits from explanations in the prompt across all three tasks, and E-P is consistently more effective than P-E. However, it is unclear what contributes to this difference. As far as we are aware, the differences between text-davinci-002</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>|  | Pedro Rubens! The individual chapters were published into 64 "tankóbon" by Kodansha. <br> Yôko Shôjì (born 4 June 1950, in Mobara, Chiba) is a Japanese manga artist. She is best known for writing "Seito Shokan! Mulder Scully! The individual chapters were published into 14 "tankôbon" by Kodansha. <br> Seito Shokan! The individual chapters were published into 24 "tankôbon" by Kodansha between. <br> Q: How many chapters does Yôko Shôjì's most famous manga have? <br> A: First, Yôko Shôjì's most famous manga is "Seito Shokan!". Second, "Seito Shokan!" has 64 chapters. The answer is 64. |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  | Tim Minchin (December 29, 1808 July 31, 1875) was the President of the United States. <br> Andrew Johnson (December 29, 1808 July 31, 1875) was the President of the United States. <br> George Andrew Atzerodt (June 12, 1835 - July 7, 1865) was a conspirator, with John Wilkes Booth. <br> Jesse Andrew Williams (June 12, 1835 - July 7, 1865) was a conspirator, with John Wilkes Booth. <br> Q: Who was older, George Atzerodt or Andrew Johnson? <br> A: First, George Atzerodt was born on June 12, 1835. Second, Andrew Johnson was born on December 29, 1808. The answer is George Atzerodt. |</p>
<p>Figure 3: Explanations generated for ADVHOTPOT. InstructGPT may generate nonfactual explanations containing hallucination (red) or inconsistent explanations contradicting the answer (red).</p>
<p>Table 2: Left: factuality (Fac) and consistency (Con) of the generated explanations. Right: the \% of the examples whose explanation factuality/consistency is congruent with the prediction accuracy. In general, LLMs tend to generate consistent but less likely factual explanations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Fac</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Acc=Fac</th>
<th style="text-align: center;">Acc=Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">reliability of explanations generated by InstructGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (E-P)</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (P-E)</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ADVHP (E-P)</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ADVHP (P-E)</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-SNLI (P-E)</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">62.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">reliability of explanations generated by other LLMs on SYNTH</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OPT (175B)</td>
<td style="text-align: center;">SYNTH (E-P)</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (P-E)</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">SYNTH (E-P)</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">61.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (P-E)</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">SYNTH (E-P)</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">84.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (P-E)</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">82.8</td>
</tr>
</tbody>
</table>
<p>and InstructGPT are not described in any publication or blog post. ${ }^{6}$ Comparing GPT-3 and InstructGPT, we see the move to Instruct series models is not sufficient to explain the difference. Given the lack of transparency with this model, we hesitate to make scientific claims about the results it yields.
Our results do not suggest immediate strong improvements from incorporating explanations across all LLMs, even for our synthetic dataset, contradicting recent prior work. This can be attributed to the difference between the tasks we study. The tasks that receive significant benefits from using explanations in Nye et al. (2021) and Wei et al. (2022) are all program-like (e.g., integer addition and program execution), whereas the tasks in this work emphasize textual reasoning grounded in provided inputs. In fact, in Wei et al. (2022) and Chowdhery et al. (2022), explanations only show mild benefit on open-domain QA tasks like StrategyQA (Geva et al., 2021) that are closer to our setting.</p>
<h1>3 Can LLMs Generate Factual and Consistent Explanations?</h1>
<p>Prompting LLMs with explanations and having models generate them may not guarantee higher performance on our tasks. But what about the quality of the model-generated explanations themselves? We assess the reliability of the explanations for the three datasets, measured in terms of two aspects.</p>
<p>Factuality refers to whether a generated explanation is faithfully grounded in the corresponding input context (context for QA and premise/hypothesis pair for NLI). A factual explanation should not contain hallucinations that contradict the context. See Figure 3 for a nonfactual explanation.</p>
<p>Consistency measures if the explanation entails the prediction. Our concept of consistency resembles plausibility as described in Jacovi and Goldberg (2021), in that we assess whether the prediction follows from the explanation as perceived by a human. See Figure 3 for an inconsistent explanation.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For SYNTH, we use rules to automatically judge whether an explanation is factual and consistent on all four LLMs. For AdvHotpot and E-SNLI, the authors manually inspected the explanations generated by InstructGPT and annotated them for these two characteristics (more details in Appendix D). Note for each setting, the results are based on the explanations and predictions obtained with a single set of training shots. We only show the results of P-E on E-SNLI, as E-P is substantially worse here.</p>
<p>Results We summarize the results in Table 2. We only report consistency on E-SNLI, as the explanations for ESNLI often require some external commonsense knowledge which cannot be easily grounded in the inputs or judged as true or false (examples in Appendix F). The results suggest a disconnect between the model predictions and the "reasoning" in explanations. On InstructGPT, though using explanations improves its performance across three tasks, the generated explanations are unreliable (upper section), even for the straightforward synthetic setting. Comparing the factuality of explanations for SYNTH generated by GPT-3, InstructGPT, and text-davinci-002, we see that instruction tuning improves the factuality, but even the most powerful text-davinci-002 still fails to generate explanations that are perfectly grounded in the input context. Overall, LLMs tend to generate consistent explanations ( $&gt;80 \%$ for all three datasets with the right prompt structure), but the explanations are less likely to be factual, which is concerning as they can deceive a user of the system into believing the model's answer.</p>
<h1>3.1 Reliability of Explanations and Prediction Accuracy</h1>
<p>LLMs may hallucinate problematic explanations, but this could actually be advantageous if it gives us a way of spotting when the model's "reasoning" has failed. We investigate the connection between the reliability of an explanation and the accuracy of a prediction and ask whether a reliable explanation indicates an accurate prediction. (This resembles the linguistic calibration of Mielke et al. (2022), but using a different signal for calibration.)
As shown in Table 2 (right), accuracy and factuality/consistency are typically correlated, especially factuality. By knowing whether an explanation is factual, we can guess the model's accuracy a high fraction of the time (Accuracy = Factuality). A nonfactual explanation very likely means an incorrect prediction on the SYNTH dataset across all four LLMs. On AdvHotpot, factuality and InstructGPT's prediction correspond $80.0 \%$ of the time, substantially surpassing the prediction accuracy itself. We show fractions of correct and incorrect predictions when the explanations are factual/nonfactual and consistent/inconsistent in Figure 4 for two of our settings. Factual explanations are much more likely paired with correct predictions compared to nonfactual explanations. Consistency is also connected to accuracy but is an inferior indicator compared to factuality in general (Table 2).</p>
<h2>4 Calibrating In-Context Learning using Explanations</h2>
<p>From Section 3.1, we see that a human oracle assessment of the factuality of an explanation could be of substantial use for calibrating the corresponding prediction. Can we automate this process?
We first show how to achieve this goal on the perfectly controlled SYNTH dataset (Section 4.1). On our other two datasets, we use surface lexical matching to approximate semantic matching and give real-valued scores approximately reflecting factuality. Following past work on supervised calibration (Kamath et al., 2020; Chen et al., 2021; Ye and Durrett, 2022), we can learn a calibrator that tunes the probabilities of a prediction based on the score of its explanation (Section 4.2). We show such a calibrator can be trained with a handful of examples beyond those used for in-context learning and successfully improve the in-context learning performance on realistic datasets. ${ }^{7}$ We note that, as mentioned before, the experiments in this section are conducted on InstructGPT.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.1 Motivating Example: Improving Synth Dataset</h1>
<p>We first show how post-hoc calibration functions in the controlled Synth setting, where we can simply check the factuality of an explanation. Since the generated explanation always follows the format "B is [profession] and A [verb] B." (example in Figure 2), we can split the explanation into two sentences. The explanation is factual if and only if each of the two sentences exactly matches one of the sentences in the context.</p>
<p>We use the assessment to improve the performance of P-E for Synth, where a nonfactual explanation typically indicates an incorrect prediction. This gives us a way to reject presumably incorrect answers. Specifically, we iterate through the top 5 candidate answers (restricted by the API) given by InstructGPT and reject any answer-explanation pair if the explanation is nonfactual until we find a factual one. This procedure dramatically improves the accuracy from $52.4 \%$ to $74.8 \%$. Note that this Synth dataset is a challenging task given its lack of reasoning shortcuts: for reference, neither ROBERTA (Liu et al., 2019) nor DEBERTA (He et al., 2021) finetuned with 16 examples can achieve an accuracy surpassing $50 \%$. With the help of the explanations and the checking procedure, we can use InstructGPT to achieve strong results using few-shot learning.</p>
<h3>4.2 Learning-based Calibration Framework</h3>
<p>Framework We now introduce the framework that can leverage the factuality assessment of an explanation to calibrate a prediction. Let $\boldsymbol{p}$ be the vector of predicted probabilities associated with each class label in NLI (or the probability score of predicted answer in QA). Let $v$ be a scalar value extracted from the explanation to describe the factuality. Then, we can adjust the probabilities accordingly using a linear model: $\hat{\boldsymbol{p}}=\operatorname{softmax}(W[\boldsymbol{p} ; v]+b)$, where $\hat{\boldsymbol{p}}$ is the tuned probabilities.
Our calibration framework is extended from classical calibration methods (Platt, 1999; Guo et al., 2017; Zhao et al., 2021), which apply an affine transformation on the probabilities alone: $\hat{\boldsymbol{p}}=$ $\operatorname{softmax}(W \boldsymbol{p}+b)$. In contrast, we use an additional factor $v$ in calibration to incorporate the factuality assessment of the explanation.
There are a small number of parameters ( $W$ and $b$ ) that need to be trained in such a calibration framework. We will rely on a few more examples in addition to the shots we use in the prompt to train the calibrator. Specifically, we use the prompt examples to generate the predictions and explanations for these extra examples, and extract predicted probabilities, factors, and target probabilities triples to construct training data points used to train the calibrator. Note this procedure requires no explanation annotations for the extra examples.</p>
<p>Approximating Factuality We approximate the factuality using lexical overlap between the explanations and the inputs, which we found to work fairly well for our tasks.
AdvHotpot: We use an explanation consisting of two sentences (examples in Figure 3) as an illustration. Let $\mathcal{E}=\left(E^{(1)}, E^{(2)}\right)$ be the generated explanation, where $E^{(1)}$ and $E^{(2)}$ are the two sentences, and the $E^{(i)}=\left(e_{1}, e_{2}, \cdots\right)$ contain tokens $e_{1}, e_{2}, \cdots$. Similarly, let $\mathcal{P}=$ $\left(P^{(1)}, P^{(2)}, P^{(3)}, P^{(4)}\right)$ be the context paragraphs, and $P^{(i)}=\left(p_{1}, p_{2}, \cdots\right)$ be the tokens. The factuality estimation of one explanation sentence $E^{(i)}$ is defined as: $\mathcal{V}\left(E^{(i)}\right)=\max <em E="E" _in="\in" _mathcal_E="\mathcal{E">{P \in \mathcal{P}} \frac{\left|E^{(i)}\right| \cap P \mid}{\left|E^{(i)}\right|}$.
Intuitively, the factuality score for a sentence $E$ is defined as the maximum number of overlapping tokens over all paragraphs $P$, normalized by the number of tokens in $E$. We then define the factuality score for the whole explanation as $\mathcal{V}(\mathcal{E})=\min </em>$
E-SNLI: The explanations of E-SNLI do not really involve a concept of factuality. Nevertheless, we use an analogous score following the same principle by viewing the premise as the context. Let $E=\left(e_{1}, e_{2}, \cdots\right)$ be the explanation and $P=\left(p_{1}, p_{2}, \cdots\right)$ be the premise. We simply score the explanation by $\mathcal{V}(E)=\frac{|E| \cap|P|}{|E|}$. The more an explanation overlaps with the premise, the more factual we judge it to be.}} \mathcal{V}(E)$, as it requires all sentences to be factual in order to make the entire explanation factual. ${ }^{8</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.3 Calibrating E-SNLI</h1>
<p>Setup For E-SNLI, we use calibration methods to postprocess the final probabilities. Unlike classical temperature scaling (Platt, 1999), note that the methods we use here can actually change the prediction; we will therefore evaluate on accuracy of the calibrated model.
We study the effectiveness of our explanationbased calibrator under different training data sizes varying from 32 to 128 . Recall that we only require explanation annotations for 32 data points, and only need the labels for the rest to train the calibrator. For E-SNLI, we calibrate P-E, which is shown to be more effective than E-P in this setting (Section 2.4).</p>
<p>Baselines We provide the performance of finetuned RoBERTA (Liu et al., 2019) model as a reference, finding this to work better than DeBERTa (He et al., 2021). To isolate the effectiveness of using explanations for calibration, we introduce three additional baselines using nonexplanation-based calibrators. We apply the probability-based calibrator as described in Section 4.2 on the results obtained on few-shot learning (Few-Shot+ProbCal) and predict-then-explain pipeline (P-E+ProbCal). We note that the parameters of these calibrators are trained using the additional data points, as opposed to being heuristically determined as in Zhao et al. (2021). Furthermore, we experiment with a recently proposed supervised calibrator from Zhang et al. (2021), which uses the CLS representations from an additional language model as features in the calibrator. The probabilities are tuned using $\hat{\boldsymbol{p}}=\operatorname{softmax}(W[\boldsymbol{p} ; \boldsymbol{h}]+b)$, where $\boldsymbol{h}$ is the CLS representation. Since we do not have access to the embeddings obtained by GPT-3, we use RoBERTA to extract the vectors instead. We use such a calibrator on top of our best-performing base model, P-E, resulting P-E+ ZHANG ET AL. (2021).
Limited by the maximum prompt length, in-context learning is not able to take as input the additional data used for training the calibrator. For a fair comparison, we can allow the in-context model to use this data by varying the prompts across test examples, dynamically choosing the prompt examples to maximize performance. Choosing closer data points for prompting is a common and effective way of scaling up the training data size for in-context learning (Shin et al., 2021; Liu et al., 2021). Following Liu et al. (2021), we test the performance of choosing nearest neighbors for the prompt based on CLS embedding produced by a RoBERTA model (Liu et al., 2019), referred as Few-Shot(NN). It is worth clarifying that the Few-Shot and Few-Shot+ProbCal approaches use the same set of 32 training shots in the prompt for every test example, whereas the shot sets vary from example to example in Few-Shot(NN).</p>
<p>Results We show the results in Table 3. We use 5 different groups of training examples and report the mean and standard deviation across the groups. For Few-Shot(NN), we only report the results obtained using 128 examples; results using a smaller number of examples will be worse than this.
Under 128 training examples, applying a trained calibrator on top of prompting with explanation (i.e., P-E+EXPLCal) achieves the best accuracy of $68.5 \%$, which is $12 \%$ higher than the performance of the vanilla uncalibrated few-shot in-context learning (Few-Shot). P-E+EXPLCal also outperforms Few-Shot+ProbCal and P-E+ProbCal by 5\% and 3\%, respectively. Using explanations is more effective than using probabilities alone. In addition, P-E+EXPLCal also outperforms P-E+ZHANG ET AL. (2021), whose performance is on par with P-E+ProbCal. This suggests the additional CLS information is not very helpful in this setting.
As the data size increases from 32 to 128 , the performance of the explanation-based calibrator keeps improving notably, whereas the performance of probability-based calibrators nearly saturates at a data size of 96 . The performance of Few-Shot(NN) with 128 training instances only improves the performance by $3.3 \%$, compared to Few-Shot with 32 training instances. Choosing nearest</p>
<p>Table 4: AUC scores (mean ${ }_{\text {std dev }}$ ) on ADVHOTPOT under different data conditions. $\mathbf{L}$ and $\mathbf{E}$ denotes the number of label annotations and explanation annotations, respectively. Explanationbased calibration successfully improves the performance on top of prompting with explanations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">w/o Explanation</th>
<th style="text-align: center;">6L</th>
<th style="text-align: center;">32L</th>
<th style="text-align: center;">64L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FEw-SHOT</td>
<td style="text-align: center;">$59.6_{2,4}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FEw-SHOT(NN)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.3_{0.9}$</td>
</tr>
<tr>
<td style="text-align: left;">w/ Explanation</td>
<td style="text-align: center;">$\mathbf{6 L + 6 E}$</td>
<td style="text-align: center;">$\mathbf{3 2 L + 6 E}$</td>
<td style="text-align: center;">$\mathbf{6 4 L + 6 E}$</td>
</tr>
<tr>
<td style="text-align: left;">E-P</td>
<td style="text-align: center;">$\mathbf{6 4 . 4}_{2.9}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">E-P+EXPLCAI</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{6 6 . 0}_{3.9}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 8}_{3.0}$</td>
</tr>
<tr>
<td style="text-align: left;">E-P+ZHANG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$65.6_{3.9}$</td>
<td style="text-align: center;">$66.1_{3.2}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Coverage-Acc curves of various methods on AdvHotpot. E-P+EXPLCAI is better calibrated compared to uncalbrated E-P as well as the other approaches.
neighbors as the shots, while being effective when having access to a large amount of data, is not helpful in the extreme data-scarce regime. Calibrating using explanations is an effective way of using a few extra data points that cannot fit in the prompt, which is a pitfall of standard in-context learning.
Finally, RoBERTA finetuned using 128 shots only achieves an accuracy of $54.9 \%$, lagging the performance of GPT-3 based models. The limited training data size is insufficient for finetuning smaller language models like RoBERTA, but is sufficient for P-E+EXPLCAI to be effective.</p>
<h1>4.4 Calibrating ADVHOTPOT</h1>
<p>Setup For the ADVHOTPOT dataset, our calibration takes the form of tuning the confidence scores of the predicted answers to better align them with the correctness of predictions. These confidence scores can be used in a "selective QA" setting (Kamath et al., 2020), where the model can abstain on a certain fraction of questions where it assigns low confidence to its answers. We use the area under coverage-accuracy curve (AUC) to evaluate how well a model is calibrated as in past literature (Kamath et al., 2020; Chen et al., 2021; Zhang et al., 2021; Garg and Moschitti, 2021; Ye and Durrett, 2022). The curve plots the average accuracy with varying fractions (coverage) of questions being answered (examples in Figure 5). For any given coverage, a better calibrated model should be able to identify questions that it performs best on, hence resulting a higher AUC.
We experiment with training data set sizes of 6,32 , and 64 . We report the results averaged from 5 trials using different training sets. For ADVHOTPOT, we calibrate E-P, which is shown to be more effective than P-E in this setting (Section 2.4). Our approach is also effective for calibrating P-E; please refer to Appendix E for details.</p>
<p>Results We show the AUC scores in Table 4. By leveraging explanations, E-P+EXPLCAI successfully achieves an AUC of 68.8 , surpassing both FEw-SHOT by 7 points and E-P by 4 points. We note this is a substantial improvement, given that the upperbound of AUC is constrained by the accuracy of the answers and cannot reach 100. Figure 5 shows the coverage-accuracy curves of various methods averaged across the 5 training runs. E-P+EXPLCAI always achieves a higher accuracy than its uncalibrated counterpart, E-P, under a certain coverage, and the gap is especially large in the most confident intervals (coverage $&lt;50 \%$ ). E-P+ZHANG ET AL. (2021) is able to calibrate the predictions on this dataset, but still lags our explanation-based calibrator, E-P+EXPLCAI.
In addition, the explanation-based calibrator can be effective with as few as 32 examples. This is because there are only two parameters (the probability of predicted answer and the explanationbased factor) in the calibrator, which can be easily learned in this few-shot setting. Comparing E-P+EXPLCAI against FEw-SHOT(NN), using nearest neighbors in the prompt is also able to improve the performance compared to using a fixed set of shots (FEw-SHOT), yet our lightweight calibrator can better utilize such a small amount of data, and learn to distinguish more accurate predictions based on the explanations.</p>
<h1>5 Related Work</h1>
<p>Our investigation is centered around in-context learning (Brown et al., 2020), which has garnered increasing interest since the breakthrough of various large pretrained language models. Recent work has been devoted to studying different aspects of in-context learning, including its wayward behaviors (Min et al., 2022; Webson and Pavlick, 2022) and approaches to overcome them (Zhao et al., 2021), whereas our exploration focuses on using explanations.</p>
<p>The utility of explanations for few-shot in-context learning has also been discussed concurrently (Nye et al., 2021; Wei et al., 2022; Marasović et al., 2022; Chowdhery et al., 2022; Lampinen et al., 2022; Wiegreffe et al., 2022), especially in symbolic reasoning tasks. We differ in that we study more free-form explanations in tasks (QA and NLI, specifically) focusing on textual reasoning over provided contexts. Furthermore, our work focuses on the nature of the explanations generated by LLMs, which are found to be unreliable. Regarding our use of calibration, similar ideas of explanation-based performance estimation have been applied to other tasks (Rajani and Mooney, 2018; Ye et al., 2021; Ye and Durrett, 2022), but we rely on the free-text explanations generated by the model instead of interpretations obtained through post-hoc interpretation techniques.</p>
<p>More broadly, how to use explanations in various forms (textual explanation, highlights, etc.) to train better models is a longstanding problem (Zaidan et al., 2007). Past work has built a series of pipeline models that first generate the explanations and then make predictions purely based on the generated explanations (Wiegreffe et al., 2021; Zhou and Tan, 2021; Chen et al., 2022). Prior research has also explored using explanations as additional supervision to train joint models (Hancock et al., 2018; Dua et al., 2020; Lamm et al., 2021; Stacey et al., 2022). Another line of work seeks to align the reasoning process of a trained model with the explanations, which is typically done by interpreting a prediction post-hoc through explanation techniques and optimizing the distance between the obtained explanation and ground truth explanation (Liu and Avci, 2019; Rieger et al., 2020; Plumb et al., 2020; Erion et al., 2021; Yao et al., 2021). These aforementioned methods all update the model parameters and typically require a considerable amount of explanation annotations to be effective. By contrast, our setting treats language models as pure black boxes and only requires few-shot explanations.</p>
<h2>6 Discussion \&amp; Conclusion</h2>
<p>Caveats and Risks of Explanations from Large Language Models Our analysis suggests that LLMs' internal "reasoning" does not always align with explanations that it generates, as shown by our consistency results. More concerning, the explanations might not be factually grounded in the provided prompt. This shortcoming should caution against any deployment of this technology in practice: because the explanations are grammatical English and look very convincing, they may deceive users into believing the system's responses even when those responses are incorrect. Section 6 of Bender et al. (2021) discusses these risks in additional detail. The fact that language models can hallucinate explanations is also found in other work (Zhou and Tan, 2021). This result is unsurprising in some sense: without sufficient supervision or grounding, language models do not learn meaning as distinct from form (Bender and Koller, 2020), so we should not expect their explanations to be strongly grounded.</p>
<p>We have shown that even explanations which don't lead to accuracy gains can still be useful for calibration. However, the lexical overlap feature we use here is a weak signal of explanation correctness (see the example in Figure 1). Strong enough entailment models should theoretically be able to perform this task and work across a range of tasks without fine-tuning. This explanation assessment model can even be a language model itself trained for this particular propose to approach the verification tasks for a given domain by in-context learning.</p>
<p>Conclusion We have explored the capabilities of LLMs in using explanations in in-context learning for textual reasoning. Through our experiments with four LLMs and on two QA datasets and an NLI dataset, we find that simply including explanations in the prompt does not always improve the performance of in-context learning. Our manual analysis demonstrates that LLMs tend to generate nonfactual explanations when making wrong predictions, which can be a useful leverage to assess the correctness of the predictions. Lastly, we showcase how to use explanations to build lightweight calibrators, which successfully improve InstructGPT's in-context learning performance across all three datasets.</p>
<h1>Acknowledgments</h1>
<p>We would like to thank Eunsol Choi, Ruiqi Zhong, Jocelyn Chen, Zayne Sprague, and Jiacheng Xu for their helpful feedback on drafts of this work, as well as the anonymous reviewers for their thoughtful reviews. This work was partially supported by NSF Grant IIS-1814522, NSF CAREER Award IIS-2145280, a grant from Open Philanthropy, a gift from Salesforce Inc., and a gift from Adobe.</p>
<h2>References</h2>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT'21, page 610-623, New York, NY, USA. Association for Computing Machinery.</p>
<p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, and Tal Schuster. 2022. Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation. arXiv preprint arXiv:2202.07654.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language Inference with Natural Language Explanations. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness? In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Jifan Chen, Eunsol Choi, and Greg Durrett. 2021. Can NLI models verify QA systems' predictions? In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Baindoor Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. MeierHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. ArXiv, abs/2204.02311.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Gabriel Erion, Joseph D Janizek, Pascal Sturmfels, Scott M Lundberg, and Su-In Lee. 2021. Improving performance of deep learning models with axiomatic attribution priors and expected gradients. Nature machine intelligence, 3(7):620-631.</p>
<p>Siddhant Garg and Alessandro Moschitti. 2021. Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics, 9:346-361.</p>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (ICML).</p>
<p>Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Ré. 2018. Training classifiers with natural language explanations. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Alon Jacovi and Yoav Goldberg. 2021. Aligning faithful interpretations with their social attribution. Transactions of the Association for Computational Linguistics (TACL), 9:294-310.</p>
<p>Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Amita Kamath, Robin Jia, and Percy Liang. 2020. Selective question answering under domain shift. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.</p>
<p>Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2021. QED: A framework and dataset for explanations in question answering. Transactions of the Association for Computational Linguistics (TACL), 9:790-806.</p>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. 2022. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329.</p>
<p>Frederick Liu and Besim Avci. 2019. Incorporating priors with feature attribution on text classification. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? ArXiv, abs/2101.06804.
Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.</p>
<p>Ana Marasović, Iz Beltagy, Doug Downey, and Matthew E. Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of the North American Chapter of the Association for Computational Linguistics (NAACL Findings).</p>
<p>Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. 2022. Reducing conversational agents' overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114.</p>
<p>John Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61-74.</p>
<p>Gregory Plumb, Maruan Al-Shedivat, Ángel Alexander Cabrera, Adam Perer, Eric Xing, and Ameet Talwalkar. 2020. Regularizing black-box models for improved interpretability. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Nazneen Fatema Rajani and Raymond Mooney. 2018. Stacking with auxiliary features for visual question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), New Orleans, Louisiana.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).</p>
<p>Laura Rieger, Chandan Singh, William Murdoch, and Bin Yu. 2020. Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings.</p>
<p>Joe Stacey, Yonatan Belinkov, and Marek Rei. 2022. Supervising model attention with human explanations for robust natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the International Conference on Machine Learning (ICML).</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing Human-AI Collaboration for Generating Free-Text Explanations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p>
<p>Sarah Wiegreffe, Ana Marasović, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Refining language models with compositional explanations. In Proceedings of the Conference on Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Xi Ye and Greg Durrett. 2022. Can Explanations be Useful for Calibrating Black Box Models? In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Xi Ye, Rohan Nair, and Greg Durrett. 2021. Connecting attributions and QA model behavior on realistic counterfactuals. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using "annotator rationales" to improve machine learning for text categorization. In Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).</p>
<p>Shujian Zhang, Chengyue Gong, and Eunsol Choi. 2021. Knowing more about questions can help: Improving calibration in question answering. In Findings of the Annual Conference of the Association for Computational Linguistics (ACL Findings).</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. ArXiv, abs/2205.01068.</p>
<p>Tony Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the International Conference on Learning Representations (ICLR).</p>
<p>Yangqiaoyu Zhou and Chenhao Tan. 2021. Investigating the effect of natural language explanations on out-of-distribution generalization in few-shot NLI. In Proceedings of the Workshop on Insights from Negative Results in NLP.</p>
<h1>Checklist</h1>
<ol>
<li>
<p>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 6.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 6.
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</p>
</li>
<li>
<p>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</p>
</li>
<li>If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes]
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We use the GPT-3 Instruct-series API (text-davinci-001).</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] See reference (Jiang and Bansal, 2019) and (Camburu et al., 2018).
(b) Did you mention the license of the assets? [Yes] See Section 2.1.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We included the Synthetic dataset in the supplementary material.
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No]</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</li>
</ol>
<h1>A Details of Prompts</h1>
<p>We show examples of the prompts used for Synth, AdvHotpot, and E-SNLI in Figure 6, Figure 7, and Figure 8, respectively. Our prompts follow the original formats in Brown et al. (2020). For approaches that use explanations (E-P and P-E), we insert explanations before/after with necessary conjunction words.</p>
<h2>Synthetic: Few-Shot</h2>
<p>Christopher agrees with Kevin. Tiffany agrees with Matthew. Mary hangs out with Danielle. James hangs out with Thomas. Kevin is a student. Matthew is a plumber. Danielle is a student. Thomas is a plumber.
Q: Who hangs out with a student?
A: Mary
Synthetic: E-P
Christopher agrees with Kevin. Tiffany agrees with Matthew. Mary hangs out with Danielle. James hangs out with Thomas. Kevin is a student. Matthew is a plumber. Danielle is a student. Thomas is a plumber.
Q: Who hangs out with a student?
A: Because Danielle is a student and Mary hangs out with Danielle, the answer is Mary.
Synthetic: P-E
Christopher agrees with Kevin. Tiffany agrees with Matthew. Mary hangs out with Danielle. James hangs out with Thomas. Kevin is a student. Matthew is a plumber. Danielle is a student. Thomas is a plumber.
Q: Who hangs out with a student?
A: Mary, because Danielle is a student and Mary hangs out with Danielle .
Figure 6: Examples of prompts for Synth.</p>
<h2>ADVHOTPOT: FEW-SHOT</h2>
<p>Sir Luigi Arthur Pirandello (12 August 1895 - 4 October 1952) was an John journalist.
Sir Keith Arthur Murdoch (12 August 1885 - 4 October 1952) was an Australian journalist.
Australian Associated Press (AAP) is an Australian news agency. The organisation was established in 1935 by Keith Murdoch.
Sir Nikolai Arthur Trubetzkoy (12 August 1896 - 4 October 1952) was an Covington journalist.
Q: Australian Associated Press was established by a journalist born in which year?
A: 1885
ADVHOTPOT: E-P
Sir Luigi Arthur Pirandello (12 August 1895 - 4 October 1952) was an John journalist.
Sir Keith Arthur Murdoch (12 August 1885 - 4 October 1952) was an Australian journalist.
Australian Associated Press (AAP) is an Australian news agency. The organisation was established in 1935 by Keith Murdoch.
Sir Nikolai Arthur Trubetzkoy (12 August 1896 - 4 October 1952) was an Covington journalist.
Q: Australian Associated Press was established by a journalist born in which year?
A: First, Australian Associated Press was established by Keith Murdoch in 1935. Second, Keith Murdoch was born in 1885. The answer is 1885 .</p>
<h2>ADVHOTPOT: P-E</h2>
<p>Sir Luigi Arthur Pirandello (12 August 1895 - 4 October 1952) was an John journalist.
Sir Keith Arthur Murdoch (12 August 1885 - 4 October 1952) was an Australian journalist.
Australian Associated Press (AAP) is an Australian news agency. The organisation was established in 1935 by Keith Murdoch.
Sir Nikolai Arthur Trubetzkoy (12 August 1896 - 4 October 1952) was an Covington journalist.
Q: Australian Associated Press was established by a journalist born in which year?
A: 1885. The reasons are as follows. First, Australian Associated Press was established by Keith Murdoch in 1935. Second, Keith Murdochwas born in 1885. The answer is 1885.</p>
<p>Figure 7: Examples of prompts for ADVHOTPOT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">E-SNLI: Few-Shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A person in black tries to knock the last pin down in a game of bowling.</td>
</tr>
<tr>
<td style="text-align: left;">Q: The person is a girl. True, False, or Neither?</td>
</tr>
<tr>
<td style="text-align: left;">A: Neither</td>
</tr>
<tr>
<td style="text-align: left;">E-SNLI: E-P</td>
</tr>
<tr>
<td style="text-align: left;">A person in black tries to knock the last pin down in a game of bowling.</td>
</tr>
<tr>
<td style="text-align: left;">Q: The person is a girl. True, False, or Neither?</td>
</tr>
<tr>
<td style="text-align: left;">A: Because not every person is a girl, this answer is Neither.</td>
</tr>
<tr>
<td style="text-align: left;">E-SNLI: P-E</td>
</tr>
<tr>
<td style="text-align: left;">A person in black tries to knock the last pin down in a game of bowling.</td>
</tr>
<tr>
<td style="text-align: left;">Q: The person is a girl. True, False, or Neither?</td>
</tr>
<tr>
<td style="text-align: left;">A: Neither, because not every person is a girl.</td>
</tr>
</tbody>
</table>
<p>Figure 8: Examples of prompts for E-SNLI.</p>
<h1>B Details of the Synth Dataset</h1>
<p>We create a controlled synthetic multi-hop QA dataset. Each context consists of four reasoning chains, where each chain contains two sentences following a template: "A [verb] B. B is [profession].". We fill in A and B in the reasoning chain templates using randomly selected names from a pool of 50 names. To fill in the [verb] and [profession] in the four reasoning chain templates, we first select two verbs from a pool of 30 verbs and two professions from a pool of 30 professions. Next, we fill in the four chains using the combination of these two verbs and professions, which give a set of completely symmetric chains. Finally, we sample one reasoning chain from all of the four to derive a asking: "Who [verb] [profession]?" (example in Figure 2).
Such a design ensures there are no reasoning shortcuts (Chen and Durrett, 2019), making it a difficult dataset even despite the regular structure of the task. A RoBERTa model needs roughly 500 data points to tackle this problem and achieve near $100 \%$ accuracy on the test set.</p>
<h2>C Details of the AdvHotpot Dataset</h2>
<p>We preprocess the original Adversarial HotpotQA dataset (Yang et al., 2018; Jiang and Bansal, 2019) in a few ways. We reduce the context length to make it better fit the purpose of testing in-context learning. We use two ground truth supporting paragraphs joined with two adversarial paragraphs to construct the context for each question, instead of using all eight distractors. In addition, we simplify each paragraph by only keeping relevant sentences needed for answering the question (or distracting the prediction); otherwise, the prompt length limit only allows 2-3 examples fit in the input prompt.
We make a challenging test test set of 250 examples by balancing the mix of examples on which prompted GPT-3 makes correct and incorrect predictions. This is done by first running few-shot inference over 1000 examples, and then randomly sampling 125 examples with correct and incorrect predictions, respectively.
Since assessing the accuracy of an answer in QA is hard, and F1 scores do not correlate with the true quality of the answers (e.g., "United States" is a correct answer but has 0 F1 score with respect to the provided ground truth answer "US") (Bulian et al., 2022), we manually assess the correctness of the answers. We observed a high inter-annotator agreement (Cohen's Kappa of 0.84) between the correctness annotations of 100 examples on which the annotations of the authors intersected. Please refer to the supplementary material for these annotations.
This dataset is licensed under the MIT license.</p>
<h2>D Details of Reliability Annotations</h2>
<p>The authors manually inspected the predictions and explanations generated for the 250 ADVHOTPOT test examples using a single set of training shots, and annotated them for factuality and consistency.</p>
<p>We observed a Cohen's Kappa of 0.85 between the factuality annotations of 100 examples (obtained using the E-P paradigm) on which the annotations of the authors overlapped.</p>
<h1>E Calibrating P-E on AdvHotpot</h1>
<p>Table 5: AUC scores of various methods on ADVHOTPOT under different data conditions. Explanations are also effective for calibrating P-E.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">w/o Explanation</th>
<th style="text-align: center;">6L</th>
<th style="text-align: center;">32L</th>
<th style="text-align: center;">64L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FEw-SHOT</td>
<td style="text-align: center;">$\mathbf{5 9 . 6}_{2.4}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FEw-SHOT(NN)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$61.3_{0.9}$</td>
</tr>
<tr>
<td style="text-align: left;">w/ Explanation</td>
<td style="text-align: center;">$\mathbf{6 L + 6 E}$</td>
<td style="text-align: center;">$\mathbf{3 2 L + 6 E}$</td>
<td style="text-align: center;">$\mathbf{6 4 L + 6 E}$</td>
</tr>
<tr>
<td style="text-align: left;">P-E</td>
<td style="text-align: center;">$58.4_{2.6}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">P-E+EXPL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}_{2.8}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 6}_{3.1}$</td>
</tr>
</tbody>
</table>
<p>We also experiment with calibrating predictions obtained using P-E on ADVHOTPOT, following the same setup as described in Section 4.4.</p>
<p>As in Table 5, using explanations is also effective for calibrating the predictions obtained using P-E. P-E+EXPL CAL successfully improves the AUC of P-E from 58.4 to 62.6, outperforming FEw-SHOT and FEw-SHOT (NN).</p>
<h2>F Additional Output Examples</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Synth: E-P</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Nonfactual</td>
<td style="text-align: center;">Jason helps Christopher. Kevin helps Stephanie. James punches Jonathan. Daniel is a typist. Jonathan is a tailor. Christopher is a typist. Richard punches Daniel. Stephanie is a tailor. <br> Q: Who punches a typist? <br> A: Because Jonathan is a typist and James punches Jonathan, the answer is James.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Matthew blames Tiffany. Lisa is a chef. Christopher helps Kelly. Angela helps Jessica. <br> Rachel blames Lisa. Jessica is a farmer. Kelly is a chef. Tiffany is a farmer. <br> Q: Who helps a farmer? <br> A: Because Kelly is a farmer and Christopher helps Kelly, the answer is Christopher.</td>
</tr>
<tr>
<td style="text-align: center;">Synth: P-E</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Nonfactual</td>
<td style="text-align: center;">Richard is a governor. Megan supervises Adam. Tiffany calls Nicholas. Nicholas is a musician. Elizabeth is a musician. Matthew calls Richard. Robert supervises Elizabeth. Adam is a governor. <br> Q: Who calls a governor? <br> A: Tiffany, because Nicholas is a governor and Tiffany calls Nicholas.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Anthony believes Justin. John is an engineer. Eric answers Kevin. Amy is an engineer. Justin is a pilot. Sarah answers Amy. Kevin is a pilot. Brandon believes John. <br> Q: Who answers a pilot? <br> A: Sarah, because Amy is a pilot and Sarah answers Amy.</td>
</tr>
</tbody>
</table>
<p>Figure 9: Examples of failure cases on Synth. GPT-3 almost always generates nonfactual explanations when giving an incorrect answer, using either P-E or E-P.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ADVHOTPOT: P-E</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Nonfactual</td>
<td style="text-align: center;">"Crestfallen" is a track on "Adore". The single's artwork is by Yelena Yemchuk. <br> Johnny McDaid is a Croatian professional photographer, painter and film director. <br> Arnljot Berg is a French professional photographer, painter and film director. <br> Yelena Yemchuk is a Ukrainian professional photographer, painter and film director. <br> Q: Crestfallen's artwork is done by a photographer of which nationality? <br> A: Croatian. The reasons are as follows. First, Crestfallen's artwork is done by Yelena Yemchuk. <br> Second, Yelena Yemchuk is Croatian photographer.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">The Pequannoc Spillway is a professional basketball team based in Ellesmere city, United Kingdom. The Silver Lake is a professional basketball team based in Ellesmere harbour, United Kingdom. The Cheshire Phoenix is a professional basketball team based in Ellesmere Port, United Kingdom. Mike DiNunno (born January 29, 1990) is an American-Italian professional basketball player for the Cheshire Phoenix, playing the point guard position. <br> Q: Where is the basketball team that Mike DiNunno plays for based? <br> A: Cheshire Phoenix. The reason is as follows. First, Mike DiNunno plays for the Cheshire Phoenix. Second, the Cheshire Phoenix is based in Ellesmere Port.</td>
</tr>
</tbody>
</table>
<p>Figure 10: Examples of unreliable explanations generated for ADVHOTPOT using P-E.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">E-SNLI: E-P</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GT: True</td>
<td style="text-align: center;">A female is looking through a microscope. <br> Q: A lady is observing something True, False, or Neither? <br> A: True, because if you're looking through a microscope you are observing something.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A person wearing sunglasses, a visor, and a British flag is carrying 6 Heineken bottles. <br> Q: The person has beer bottles and has a england flag and glasses. True, False, or Neither? <br> A: False , because the person could be carrying any number of items and not just six Heineken bottles.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A woman and a man are sword fighting. <br> Q: Two people have engaged in a sword fight and both are dead. True, False, or Neither? <br> A: False, because it is impossible for two people to sword fight and both be dead.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Two construction workers in a truck having a conversation. <br> Q: Two people are outside a truck. True, False, or Neither? <br> A: True, because two people are outside a truck.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A dog running on grass <br> Q: A dog is playing outside. True, False, or Neither? <br> A: True, because a dog playing outside implies that the dog is playing outside.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">A silhouette at the bottom of an escalator. <br> Q: The silhoutte is creeping out the children. True, False, or Neither? <br> A: False, because the silhouette is not necessarily creeping out the children.</td>
</tr>
</tbody>
</table>
<p>Figure 11: The completions generated for E-SNLI examples with different ground truth labels (GT) using E-P. GPT-3 sometimes ignores the information from premises when explaining its predictions (examples in the bottom section).</p>
<h1>G Details of Automatically Assessing Consistency and Factuality on SYNTH</h1>
<p>Our questions follow the template Who $V_{1} P_{1}$ ?. Our generated explanations follow the template $N_{1}$ is $P_{2}$ and $N_{2} V_{2} N_{3}$. Our answers are always a name, e.g., $N_{4}$. Because large language models almost always produce well-formed explanations, we can match the output against these patterns and extract variables $V_{1}, P_{1}$, etc. using simple regular expressions.
We say that an explanation is consistent if and only if the following conditions are satisfied: (1) $N_{2}=$ $N_{4}$ and $N_{1}=N_{3}$. (2) $P_{2}=P_{1}$ and $V_{2}=V_{1}$. These rules ensure the explanation matches the intent of the question and entails the answer at the same time.
We say an explanation is factual if and only if both $N_{1}$ is $P_{2}$ and $N_{2} V_{2} N_{3}$ appear exactly in the context.</p>
<h1>H Results of Using Explanations in an Alternative Style on SYNTH</h1>
<p>Table 6: Performance of text-davinci-001 of using explanations in an alternative style on SYNTH.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SYNTH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">FEW-SHOT</td>
<td style="text-align: center;">49.5 $\pm$ 0.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P (ALTERNATIVE)</td>
<td style="text-align: center;">$48.0 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E (ALTERNATIVE)</td>
<td style="text-align: center;">$49.5 \pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;">InstructGPT</td>
<td style="text-align: center;">FEW-SHOT</td>
<td style="text-align: center;">54.8 $\pm$ 2.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P (ALTERNATIVE)</td>
<td style="text-align: center;">$50.6 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E (ALTERNATIVE)</td>
<td style="text-align: center;">$53.3 \pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">FEW-SHOT</td>
<td style="text-align: center;">$72.0 \pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P (ALTERNATIVE)</td>
<td style="text-align: center;">$75.3 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P-E (ALTERNATIVE)</td>
<td style="text-align: center;">80.5 $\pm$ 2.4</td>
</tr>
</tbody>
</table>
<p>We also experimented with using an alternative style of explanations for SYNTH, where we reversed the order of the two sentences in the explanations shown in Table 2. These explanations follow the format: A [verb] B and B is [profession]. (instead of B is [profession] and A [verb] B.) By changing the order in which the sentences are extracted, we might expect that E-P can more easily follow the reasoning chain.</p>
<p>We show the performance of using reversed explanations in Table 6 and the reliability in Table 7. In general, this alternative style of explanations yields inferior performance compared to the original style (Table 1). Using explanations leads to no improvements on GPT-3, and InstructGPT. P-E is consistently better than E-P across GPT-3, InstructGPT, and text-davinci-002.
Furthermore, using such a reversed style, language models almost always generate consistent explanations when being prompted in either E-P or P-E paradigm. The factuality almost always indicates the correctness of predictions.</p>
<p>We believe these two prompts cover the most natural explanation styles for this problem. While small format changes or modifications to the general QA prompt format are also possible, we observed these to have minor impacts on the results (as we see in Appendix I).</p>
<h2>I Results of Adding "Step by Step" Trigger in Prompts</h2>
<p>We test whether including a trigger for multi-step reasoning can help LLMs better learn from explanations in the prompt for multi-step reasoning. Following Kojima et al. (2022), we prepend "Let's think step by step." to the exemplar explanations used in the E-P paradigm. For this experiment, we only test on SYNTH and ADVHOTPOT, which involve multi-step reasoning. We do not experiment with text-davinci-002, which has already achieved substantial performance improvement from using explanations, and we omit OPT because its performance is too low.</p>
<p>Table 7: Reliability of explanations in an alternative style.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acc</th>
<th style="text-align: center;">Fac</th>
<th style="text-align: center;">Con</th>
<th style="text-align: center;">Acc=Fac</th>
<th style="text-align: center;">Acc=Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; E-P)</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">$\mathbf{9 4 . 8}$</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; P-E)</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">100.</td>
<td style="text-align: center;">$\mathbf{9 8 . 4}$</td>
<td style="text-align: center;">51.6</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-001</td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; E-P)</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">$\mathbf{9 7 . 2}$</td>
<td style="text-align: center;">53.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; P-E)</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">$\mathbf{9 8 . 4}$</td>
<td style="text-align: center;">54.8</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; E-P)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">100.</td>
<td style="text-align: center;">$\mathbf{9 5 . 6}$</td>
<td style="text-align: center;">75.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SYNTH (ALTERNATIVE; P-E)</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">100.</td>
<td style="text-align: center;">$\mathbf{9 6 . 8}$</td>
<td style="text-align: center;">82.8</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of adding "let's think step by step" trigger in prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Synth</th>
<th style="text-align: center;">AdvHotpot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">FEw-Shot</td>
<td style="text-align: center;">49.5 $5_{0.6}$</td>
<td style="text-align: center;">$49.1_{6.2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">$47.1_{2.8}$</td>
<td style="text-align: center;">54.1 $1_{4.1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P + TRIGGER</td>
<td style="text-align: center;">$48.6_{2.6}$</td>
<td style="text-align: center;">$50.1_{5.2}$</td>
</tr>
<tr>
<td style="text-align: center;">text-davinci-001</td>
<td style="text-align: center;">FEw-Shot</td>
<td style="text-align: center;">$54.8_{2.5}$</td>
<td style="text-align: center;">$53.2_{2.3}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P</td>
<td style="text-align: center;">58.5 $5_{2.1}$</td>
<td style="text-align: center;">58.2 $2_{4.1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">E-P + TRIGGER</td>
<td style="text-align: center;">$58.0_{3.4}$</td>
<td style="text-align: center;">$58.0_{6.2}$</td>
</tr>
</tbody>
</table>
<p>As shown in Table 8, adding triggers in the prompts does not lead to statistically significantly improvements in E-P for GPT-3 and InstructGPT. In fact, it typically causes a performance degradation.</p>
<h1>J Information about Cost of Running Experiments</h1>
<p>The cost of our experiments, described as follows, is estimated based on using the GPT-3 API with the largest models available (davinci, text-davinci-001, and text-davinci-002) as of August 2022 ( $\$ 0.06$ per 1,000 tokens). The setting in Table 1 uses 250 examples for each result, with roughly 1400 tokens per example using the FEw-SHOT paradigm and 2000 tokens per example using the E-P or E-P paradigm. The cost of evaluating FEw-SHOT, P-E, and E-P for 5 trials on a single dataset is roughly $\$ 105, \$ 150$, and $\$ 150$, respectively. The total price for reproducing results on three datasets as in Table 1 using a single language model is roughly $\$ 1200$.
We subsample 250 -example sets to reduce cost rather than running on full datasets. Based on the significance tests in this paper and the reported confidence intervals, this size dataset is sufficient to distinguish between the performance of different approaches.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Alternatively, one might use a fine-tuned NLI model as a proxy (Chen et al., 2021). However, our focus is on the pure black-box setting, and we avoid models that require substantial amounts of data to make work.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
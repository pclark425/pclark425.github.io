<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7578 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7578</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7578</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-3827ec14b1b6a1152f1b54c2339d98953dfcfae9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3827ec14b1b6a1152f1b54c2339d98953dfcfae9" target="_blank">Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work proposes AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets.</p>
                <p><strong>Paper Abstract:</strong> Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network’s feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-the-art performance on 15 datasets, improving accuracy by at least 14.5% in the UCR dataset.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7578.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7578.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnomalyLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-distillation time-series anomaly detector that uses a pretrained LLM (GPT-2) as a teacher to produce generalizable representations and trains a prototype-guided student to mimic the teacher; anomalies are flagged by large L2 discrepancies between student and teacher embeddings, with synthetic-augmentation and contrastive regularization to enlarge gaps on anomalous patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only pretrained transformer (GPT-2) used as the teacher; in this work a 6-layer GPT-2 variant is adopted, attention and feed-forward layers are frozen and positional embeddings + layer normalization are fine-tuned on time-series inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Knowledge distillation / embedding‑discrepancy: fine-tuned LLM teacher + prototype-based student; anomaly score = squared L2 distance between student and teacher embeddings; training uses synthetic anomalies (augmentation) and a teacher-side contrastive regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Fine-tuned on the target time-series training splits (unsupervised assumption that training data are mostly normal). Student trained with unsupervised objective using synthetic anomalies created by augmentation (jittering, scaling, warping). Hyperparameters reported: Adam lr=1e-4, batch size 32 (paper hyperparam table also lists batch size 128 in one table), epochs up to 100 with patience 10.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series (univariate and multivariate)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>UCR Anomaly Archive (9 univariate domains split into ABP, Acceleration, AirTemperature, ECG, EPG, Gait, NASA, PowerDemand, RESP) and 6 multivariate datasets: SMD, MSL, SMAP, PSM, NIPS-TS-GECCO (GECCO), NIPS-TS-SWAN (SWAN).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Event-wise (affiliation) precision, recall and F1 (AF1); for UCR also report accuracy; point-adjusted metrics noted for some baselines but authors primarily use affiliation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported state-of-the-art across 15 datasets. Paper reports an average improvement of 22.2% in accuracy and 10.0% in AF1 over the second-best method and states the method detects 82% of anomalies in total. Example numbers reported in the paper: UCR/ABP Acc=0.857 AF1=0.920 (Ours); on multivariate SMD/MSL/SMAP/PSM the reported average F1 for Ours is 0.969 (Table 2); on NIPS benchmarks Ours: GECCO F1=0.620, SWAN F1=0.804 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against DeepSVDD, AnoTrans, DCdetector, MEMTO, MTGFlow, GPT4TS, TS-TCC, THOC, NCAD, COCA, Anomaly Transformer, etc.; AnomalyLLM outperformed all listed baselines on the reported datasets (per-table breakdown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fine‑tuning of pretrained LLM on time-series training data (teacher fine-tuned on time-series, student trained unsupervised with synthetic anomalies); not presented as zero‑ or few‑shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note risks of the student overlearning the teacher (addressed via prototypes and synthetic anomalies); direct application of LLM autoencoder (prior work GPT4TS) tends to reconstruct anomalies causing false negatives — motivating their distillation approach. Paper also remarks that large pretrained teacher models are heavy and suggests exploring lightweight teacher variants for deployment; ablations show performance drops without augmentation or contrastive regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Experiments run on a single RTX 3090 (reported). No per-example latency, GPU-hours, or parameter-count compute-costs are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7578.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7578.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4TS (LLM-based time-series anomaly detection baseline referenced in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier LLM-based method for time-series anomaly detection that uses a GPT-2 based encoder-decoder reconstruction architecture; reported to be prone to reconstructing abnormal signals and producing false negatives when used directly as an autoencoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>One fits all: Power general time series analysis by pretrained 1m.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (used as encoder within GPT4TS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder reconstruction architecture where a pretrained GPT-2 is applied as (part of) the encoder to produce representations for reconstruction of time-series signals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Generative reconstruction using a pretrained LLM; anomaly score derived from reconstruction error (autoencoder-style).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Prior work (referenced) adapts/fine-tunes GPT-2 on time-series data for reconstruction; in this paper GPT4TS is included as a baseline and evaluated on the same benchmarks (UCR domains and the multivariate datasets). Exact fine-tuning details for GPT4TS are from the original GPT4TS reference (not fully specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Time-series (univariate and multivariate)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Evaluated by the authors of this paper as a baseline on UCR subdatasets and multivariate sets (SMD, MSL, SMAP, PSM, GECCO, SWAN).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Affiliation/event-wise precision, recall, F1 (AF1); accuracy on UCR domains (same metrics used in tables comparing baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in this paper as a baseline with generally weaker performance than AnomalyLLM. Example numbers from the paper's tables: UCR/ABP Acc=0.476 AF1=0.681; Acceleration Acc=0.429 AF1=0.504; AirTemperature Acc=0.462 AF1=0.686. In many domains GPT4TS underperforms the proposed AnomalyLLM due to over-strong reconstruction of anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>In tables GPT4TS is listed alongside other baselines (DeepSVDD, THOC, TS-TCC, etc.) and is generally outperformed by representation-based approaches and the proposed AnomalyLLM in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not described as zero-shot in this paper; implemented as a pretrained-LLM-based reconstruction model that is adapted/fine-tuned for time-series reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper explicitly states GPT4TS's reconstruction ability can reconstruct abnormal time-series segments, resulting in false negatives and poor anomaly-detection performance in reconstruction-based evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>One fits all: Power general time series analysis by pretrained 1m. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 2)</em></li>
                <li>Time-LLM: Time series forecasting by reprogramming large language models <em>(Rating: 1)</em></li>
                <li>TEST: Text prototype aligned embedding to activate LLM's ability for time series <em>(Rating: 1)</em></li>
                <li>Tempo: Prompt-based generative pre-trained transformer for time series forecasting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7578",
    "paper_id": "paper-3827ec14b1b6a1152f1b54c2339d98953dfcfae9",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "AnomalyLLM",
            "name_full": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
            "brief_description": "A knowledge-distillation time-series anomaly detector that uses a pretrained LLM (GPT-2) as a teacher to produce generalizable representations and trains a prototype-guided student to mimic the teacher; anomalies are flagged by large L2 discrepancies between student and teacher embeddings, with synthetic-augmentation and contrastive regularization to enlarge gaps on anomalous patterns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_description": "Decoder-only pretrained transformer (GPT-2) used as the teacher; in this work a 6-layer GPT-2 variant is adopted, attention and feed-forward layers are frozen and positional embeddings + layer normalization are fine-tuned on time-series inputs.",
            "model_size": null,
            "anomaly_detection_approach": "Knowledge distillation / embedding‑discrepancy: fine-tuned LLM teacher + prototype-based student; anomaly score = squared L2 distance between student and teacher embeddings; training uses synthetic anomalies (augmentation) and a teacher-side contrastive regularizer.",
            "prompt_template": null,
            "training_data": "Fine-tuned on the target time-series training splits (unsupervised assumption that training data are mostly normal). Student trained with unsupervised objective using synthetic anomalies created by augmentation (jittering, scaling, warping). Hyperparameters reported: Adam lr=1e-4, batch size 32 (paper hyperparam table also lists batch size 128 in one table), epochs up to 100 with patience 10.",
            "data_type": "Time-series (univariate and multivariate)",
            "dataset_name": "UCR Anomaly Archive (9 univariate domains split into ABP, Acceleration, AirTemperature, ECG, EPG, Gait, NASA, PowerDemand, RESP) and 6 multivariate datasets: SMD, MSL, SMAP, PSM, NIPS-TS-GECCO (GECCO), NIPS-TS-SWAN (SWAN).",
            "evaluation_metric": "Event-wise (affiliation) precision, recall and F1 (AF1); for UCR also report accuracy; point-adjusted metrics noted for some baselines but authors primarily use affiliation metrics.",
            "performance": "Reported state-of-the-art across 15 datasets. Paper reports an average improvement of 22.2% in accuracy and 10.0% in AF1 over the second-best method and states the method detects 82% of anomalies in total. Example numbers reported in the paper: UCR/ABP Acc=0.857 AF1=0.920 (Ours); on multivariate SMD/MSL/SMAP/PSM the reported average F1 for Ours is 0.969 (Table 2); on NIPS benchmarks Ours: GECCO F1=0.620, SWAN F1=0.804 (Table 3).",
            "baseline_comparison": "Compared against DeepSVDD, AnoTrans, DCdetector, MEMTO, MTGFlow, GPT4TS, TS-TCC, THOC, NCAD, COCA, Anomaly Transformer, etc.; AnomalyLLM outperformed all listed baselines on the reported datasets (per-table breakdown in paper).",
            "zero_shot_or_few_shot": "Fine‑tuning of pretrained LLM on time-series training data (teacher fine-tuned on time-series, student trained unsupervised with synthetic anomalies); not presented as zero‑ or few‑shot prompting.",
            "limitations_or_failure_cases": "Authors note risks of the student overlearning the teacher (addressed via prototypes and synthetic anomalies); direct application of LLM autoencoder (prior work GPT4TS) tends to reconstruct anomalies causing false negatives — motivating their distillation approach. Paper also remarks that large pretrained teacher models are heavy and suggests exploring lightweight teacher variants for deployment; ablations show performance drops without augmentation or contrastive regularization.",
            "computational_cost": "Experiments run on a single RTX 3090 (reported). No per-example latency, GPU-hours, or parameter-count compute-costs are reported.",
            "uuid": "e7578.0",
            "source_info": {
                "paper_title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT4TS",
            "name_full": "GPT4TS (LLM-based time-series anomaly detection baseline referenced in the paper)",
            "brief_description": "An earlier LLM-based method for time-series anomaly detection that uses a GPT-2 based encoder-decoder reconstruction architecture; reported to be prone to reconstructing abnormal signals and producing false negatives when used directly as an autoencoder.",
            "citation_title": "One fits all: Power general time series analysis by pretrained 1m.",
            "mention_or_use": "use",
            "model_name": "GPT-2 (used as encoder within GPT4TS)",
            "model_description": "Encoder-decoder reconstruction architecture where a pretrained GPT-2 is applied as (part of) the encoder to produce representations for reconstruction of time-series signals.",
            "model_size": null,
            "anomaly_detection_approach": "Generative reconstruction using a pretrained LLM; anomaly score derived from reconstruction error (autoencoder-style).",
            "prompt_template": null,
            "training_data": "Prior work (referenced) adapts/fine-tunes GPT-2 on time-series data for reconstruction; in this paper GPT4TS is included as a baseline and evaluated on the same benchmarks (UCR domains and the multivariate datasets). Exact fine-tuning details for GPT4TS are from the original GPT4TS reference (not fully specified in this paper).",
            "data_type": "Time-series (univariate and multivariate)",
            "dataset_name": "Evaluated by the authors of this paper as a baseline on UCR subdatasets and multivariate sets (SMD, MSL, SMAP, PSM, GECCO, SWAN).",
            "evaluation_metric": "Affiliation/event-wise precision, recall, F1 (AF1); accuracy on UCR domains (same metrics used in tables comparing baselines).",
            "performance": "Reported in this paper as a baseline with generally weaker performance than AnomalyLLM. Example numbers from the paper's tables: UCR/ABP Acc=0.476 AF1=0.681; Acceleration Acc=0.429 AF1=0.504; AirTemperature Acc=0.462 AF1=0.686. In many domains GPT4TS underperforms the proposed AnomalyLLM due to over-strong reconstruction of anomalies.",
            "baseline_comparison": "In tables GPT4TS is listed alongside other baselines (DeepSVDD, THOC, TS-TCC, etc.) and is generally outperformed by representation-based approaches and the proposed AnomalyLLM in the reported experiments.",
            "zero_shot_or_few_shot": "Not described as zero-shot in this paper; implemented as a pretrained-LLM-based reconstruction model that is adapted/fine-tuned for time-series reconstruction.",
            "limitations_or_failure_cases": "Paper explicitly states GPT4TS's reconstruction ability can reconstruct abnormal time-series segments, resulting in false negatives and poor anomaly-detection performance in reconstruction-based evaluation.",
            "computational_cost": null,
            "uuid": "e7578.1",
            "source_info": {
                "paper_title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "One fits all: Power general time series analysis by pretrained 1m.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 2
        },
        {
            "paper_title": "Time-LLM: Time series forecasting by reprogramming large language models",
            "rating": 1
        },
        {
            "paper_title": "TEST: Text prototype aligned embedding to activate LLM's ability for time series",
            "rating": 1
        },
        {
            "paper_title": "Tempo: Prompt-based generative pre-trained transformer for time series forecasting",
            "rating": 1
        }
    ],
    "cost": 0.017118500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection</h1>
<p>Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng<br>Zhejiang University<br>{liu777ch, s18he, zqhang, lisz, wmengzju} @zju.edu.cn</p>
<h4>Abstract</h4>
<p>Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-theart performance on 15 datasets, improving accuracy by at least $14.5 \%$ in the UCR dataset.</p>
<h2>1 Introduction</h2>
<p>Time series anomaly detection (TSAD) aims to identify abnormal data whose patterns deviate from the majority of the data [Blázquez-García et al., 2021]. It plays critical roles in numerous applications such as industrial fault diagnosis, network intrusion detection, and health monitoring [Kieu et al., 2022].</p>
<p>The primary challenge for TSAD lies in the laborious process of acquiring annotations [Ruff et al., 2021]. Consequently, most previous works follow the unsupervised setting where no labels are provided, and the majority of the data is assumed to be normal [Audibert et al., 2020]. These methods can be broadly categorized into one-class classificationbased methods [Ruff et al., 2018; Shen et al., 2020; Carmona et al., 2021], density estimation-based methods [Dai and Chen, 2022; Zhou et al., 2023a], and self-supervised methods
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Knowledge distillation-based framework: the discrepancy between outputs of the student and teacher networks is expected to be small on normal samples while large on abnormal samples.
[Jeong et al., 2023]. With the advancement of representation learning, self-supervised methods have garnered growing attention and dominated the field [Zhang et al., 2023]. They employ pretext tasks such as reconstruction [Su et al., 2019; Xu et al., 2021; Song et al., 2023], forecasting [Deng and Hooi, 2021; Li et al., 2023a], imputation [Chen et al., 2023], and contrastive learning [Yang et al., 2023; Wang et al., 2023] to learn a representation map that distinguishes the normal and abnormal samples.</p>
<p>However, learning generalizable representations typically requires a vast amount of training data [Zhang et al., 2023], which conflicts with scenarios of limited available samples, thereby limiting the performance of these self-supervised methods. To overcome this limitation, we introduce AnomalyLLM, a novel approach that integrates knowledge distillation and large language models (LLMs). The fundamental idea is to train a student network to mimic the output of a teacher network that is pretrained on a large-scale dataset. During the testing phase, anomalies are identified when a significant discrepancy exists between the outputs of the student and teacher networks, as shown in Fig. 1. To this end, we need to address two key challenges.</p>
<p>How to pretrain the teacher network without large-scale time series datasets? Large-scale datasets for pretraining are abundant in computer vision (CV) and natural language processing (NLP), playing a critical role in generalizable representation learning. However, there is currently a lack of universal large-scale time series datasets, with the largest available dataset being less than 10 GB , a size significantly smaller than that in CV and NLP [Godahewa et al., 2021]. Consequently, pretraining the teacher network remains a challenge. Recent studies have explored the modal similarity between language and time series, revealing the remarkable potential</p>
<p>of pretrained LLMs in generating time series representations [Zhou et al., 2023b]. LLMs can be fine-tuned on time series data under few-shot [Jin et al., 2023a] or even zero-shot [Gruver et al., 2023] settings. This observation motivates us to use a pretrained LLM as the teacher network. We follow the time series embedding layer with a pretrained LLM, adapting it to generate time series representations.</p>
<p>How to circumvent the student network from 'overlearning' representations produced by the teacher network? We anticipate the discrepancy between the outputs of the student and teacher networks to be small on normal samples but large on abnormal samples [Zhou et al., 2022]. However, given the absence of abnormal samples to enlarge their representation gap, this can easily lead to overlearning of the student network, where the two networks consistently generate similar representations, even for abnormal samples. To circumvent this problem, we implement two designs. First, we incorporate prototypical signals into the student network, enabling its representations to focus more on the historical normal patterns [Song et al., 2023]. Second, we employ data augmentations to produce synthetic anomalies [Sun et al., 2023b], which are used to enlarge the representation discrepancy. Furthermore, the teacher network's representations of original and augmented samples are treated as positive pairs, and a contrastive loss is applied to bring them closer together, serving as a regularization term to encourage the teacher network to capture more general patterns. Comprehensive experiments are conducted to demonstrate the superiority of our method on 9 univariate datasets and 6 multivariate datasets.</p>
<p>The main contributions are summarized as follows:</p>
<ul>
<li>As far as we know, AnomalyLLM is the first knowledge distillation-based time series anomaly detection method.</li>
<li>We devise a teacher network that is adapted from the pretrained LLM, capable of learning a rich generalizable representation for time series after fine-tuning.</li>
<li>To maintain the discrepancy between the teacher and student networks, we integrate prototypical signals into the student network and design a data augmentationbased training strategy.</li>
<li>Extensive experiments show that the proposed model achieves SOTA performance on 15 real-world datasets.</li>
</ul>
<h2>2 Related works</h2>
<h3>2.1 Time Series Anomaly Detection</h3>
<p>Time series anomaly detection plays a pivotal role in various real-world applications [Kieu et al., 2022]. Early studies employ statistical methods or machine learning-based methods [Blázquez-García et al., 2021], which fail to describe complex patterns of time series signals. In recent years, with the success of neural networks such as variational autoencoder [Park et al., 2018] and generative adversarial network [Zhou et al., 2019], numerous deep learningbased methods have emerged for time series anomaly detection. These methods can be roughly categorized into one-class classification-based methods [Ruff et al., 2018; Shen et al., 2020; Carmona et al., 2021], density estimationbased methods [Dai and Chen, 2022; Zhou et al., 2023a; Zhou et al., 2024], and self-supervised-based methods [Jeong et al., 2023]. With the rapid development of representation learning, self-supervised methods have dominated the field. Reconstruction is the most usual self-supervised method, where the reconstruction error indicates the outlyingness of the samples [Xu et al., 2021; Li et al., 2023b; Song et al., 2023]. Forecasting [Deng and Hooi, 2021], imputation [Chen et al., 2023], and contrastive learning [Yang et al., 2023; Wang et al., 2023; Sun et al., 2023b] also emerge as other pretext tasks for self-supervised anomaly detection. While they have achieved SOTA results on various datasets, the small data size used by these self-supervised methods hinders them from learning generalizable representations, thereby limiting their performance [Zhang et al., 2023]. In this paper, we introduce a teacher network adapted from LLM, which has demonstrated a strong ability to generate generalizable time series representations. A student network is trained to mimic the output of the teacher network, and the discrepancy between their outputs serves as the anomaly score in the testing phase.</p>
<h3>2.2 Large Language Model</h3>
<p>Pretrained foundation models have proven excellent performance in NLP and CV, prompting its progress in time series analysis [Jin et al., 2023b]. Despite the increasing interest in foundation models for time series [Garza and MergenthalerCanseco, 2023], it remains a significant challenge due to the limited availability of large-scale datasets. The largest time series dataset is currently less than 10GB, a size smaller than that of NLP datasets [Godahewa et al., 2021]. However, recent studies suggest that pretrained LLMs can be adapted to time series analysis through fine-tuning on time series data [Zhou et al., 2023b]. Gruver et al. [Gruver et al., 2023] even argue that a pretrained LLM can serve as a zeroshot time series forecaster, thanks to its capability to model flexible distributions over sequences of numbers. Consequently, various studies leverage LLM for time series analysis, with a predominant focus on forecasting [Jin et al., 2023a; Cao et al., 2023] and classification [Sun et al., 2023a]. Notably, GPT4TS [Zhou et al., 2023b] stands as the sole LLMbased time series anomaly detection method, employing an encoder-decoder reconstruction architecture with GPT2 as the encoder. However, the pretrained GPT2's strong generalization ability makes it prone to reconstructing abnormal signals and yielding false negatives. In contrast, we propose a novel knowledge distillation-based method, where the student network is trained from scratch, which will not generalize to those unseen anomalies compared to GPT4TS.</p>
<h3>2.3 Knowledge Distillation</h3>
<p>Knowledge distillation is proposed by [Hinton et al., 2015], aiming to push the student network to regress the output of the teacher network. It is first employed in vision anomaly detection by [Bergmann et al., 2020]. The fundamental principle is that anomalies are identified when there is a substantial discrepancy between the outputs of the student and teacher networks. While this principle has been widely explored in vision anomaly detection, [Salehi et al., 2021; Zhou et al., 2022], its application in time series remains an</p>
<p>unexplored territory. Our method can be seen as the first attempt to introduce knowledge distillation into time series anomaly detection. Moreover, our method diverges from existing works in CV in two key aspects. First, unlike in CV where large datasets are commonly available for pretraining the teacher network, large time series datasets are scarce, impeding the effective pretraining. In this work, we employ the pretrained LLM as our teacher network and adapt time series signals to features LLM can understand by an input embedding layer. Second, to prevent the student network from overlearning the representation of the teacher network, we propose reminding the student network of the prototypical signals and devising a data augmentation-based training strategy. Through these efforts, we demonstrate that knowledge distillation can provide another approach for time series anomaly detection, which has not been explored previously.</p>
<h2>3 Methodology</h2>
<p>Given a $D$-dimension multivariate time series $X=$ $\left[x_{1}, x_{2}, \ldots, x_{L}\right] \in \mathbf{R}^{D \times L}$ of length $L$, where $x_{t} \in \mathbf{R}^{D}$ denotes the data collected at the $t$-th time step and $D$ denotes the number of variables, we aim to train an anomaly detector. During the testing phase, we utilize the trained detector to predict an unseen multivariate time series $\hat{X}=$ $\left[\hat{x_{1}}, \hat{x_{2}}, \ldots, \hat{x_{L^{\prime}}}\right]$ with $\hat{Y}=\left[\hat{y_{1}}, \hat{y_{2}}, \ldots, \hat{y_{L^{\prime}}}\right]$, where $\hat{y}_{l} \in{0,1}$ indicates whether anomalies occur at the $l$-th time step.</p>
<h3>3.1 Overall Architecture</h3>
<p>Following previous works [Carmona et al., 2021], we partition the entire time series into several time windows of fixed length $T$. Given a time window $\mathbf{w}<em i="i">{i} \in \mathbf{R}^{D \times T}$, we aim to identify whether anomalies occur within it. Our method adopts the knowledge distillation architecture [Bergmann et al., 2020]. The architecture consists of a student network $\phi: \mathbf{R}^{D \times T} \rightarrow \mathbf{R}^{d}$ and a teacher network $\varphi: \mathbf{R}^{D \times T} \rightarrow \mathbf{R}^{d}$, both transforming original signals into $D$-dimensional vectors. The time window is fed into the two networks, and outputs are denoted as $z</em>}=\phi\left(\mathbf{w<em i="i">{i}\right)$ and $c</em>\right)$. We anticipate these two representations to be close for normal samples and distant for abnormal samples. The hypersphere classifier loss [Ruff et al., 2020] is utilized to calculate the discrepancy between two representations:}=\varphi\left(\mathbf{w}_{i</p>
<p>$$
\mathcal{L}=-\left(1-y_{i}\right) \log \ell\left(z_{i}, c_{i}\right)-y_{i} \log \left(1-\ell\left(z_{i}, c_{i}\right)\right)
$$</p>
<p>where $\ell\left(z_{i}, c_{i}\right)=\exp \left(-\left|z_{i}-c_{i}\right|<em i="i">{2}^{2}\right)$, and $y</em>=0$.}$ denotes the ground truth label. In the unsupervised setting, all samples are assumed to be normal, and $y_{i</p>
<h3>3.2 Prototype-based Student Network</h3>
<p>To prevent the student network from learning overly generalizable representations like the teacher network, we guide it with prototypes. These prototypes represent characteristic segments in the entire time series and are trainable parameters in our method. We select prototypes that closely resemble the input time window to assist in generating the representation.</p>
<p>Prior study [Nie et al., 2022] has demonstrated the effectiveness of channel independence in multivariate time series analysis. Therefore, we select the most similar prototype for
each channel. First, we initialize a prototype pool $\mathcal{M}=$ $\left{\mathcal{M}<em 2="2">{1}, \mathcal{M}</em>}, \ldots, \mathcal{M<em i="i">{D}\right}$, where $\mathcal{M}</em>}=\left{\mathbf{m<em i="i">{i}^{1}, \mathbf{m}</em>$, we calculate the similarity between the time windows of each channel and their corresponding prototypes, and then select the most similar prototype:}^{2}, \ldots, \mathbf{m}_{i}^{M}\right}$ represents the collection of $M$ prototypes of length $T$ for the $i$-th channel. Given an input time window $\mathbf{w</p>
<p>$$
\mathbf{m}<em M="M" _ldots_="\ldots," i="1,2,">{j}^{s}=\max </em>\right)
$$} \operatorname{sim}\left(\mathbf{m}_{j}^{i}, \mathbf{w}^{j</p>
<p>where $\mathbf{w}^{j}$ is the $j$-th channel of input, and $\operatorname{sim}(\cdot, \cdot): \mathbf{R}^{T} \times$ $\mathbf{R}^{T} \rightarrow \mathbf{R}^{+}$represents the function to measure similarity. We use cosine similarity in the paper. The selected prototype for the entire time window is denoted as $\mathbf{m}_{s}$.</p>
<p>Instance normalization [Kim et al., 2021] is employed to mitigate the distribution shift effect and patching [Nie et al., 2022] is utilized to extract local semantic information. Both the input time window and prototype are addressed by these two techniques. It is noteworthy that the hyperparameters of the prototype normalization are the same as those of the input. Next, both the input and prototype are fed into an input embedding layer. The input embedding layer consists of linear probing which extracts the in-patch information and a positional embedding which records the position information of the sequences. The layer generates the input embedding $\mathbf{w}<em e="e">{e}$ and prototype embedding $\mathbf{m}</em>$.</p>
<p>To incorporate prototypical features into the original time window, we devise a prototype-based Transformer encoder. The traditional Transformer encoder is stacked by blocks, each consisting of an attention layer, a feed-forward layer, and layer normalization [Song et al., 2023]. In this paper, we provide information about prototypes for each attention layer. Specifically, the queries and keys are produced for both prototypes and inputs. The correlations between the $i$-th patch of the input embedding $\mathbf{w}_{e}^{i}$ and other patches are calculated as follows:</p>
<p>$$
\begin{aligned}
&amp; s_{i, t}^{w}=\frac{\exp \left(\left\langle\mathbf{q}<em m="m">{w}^{i}, \mathbf{k}</em>}^{t}\right\rangle\right)}{\sum_{j=1}^{n} \exp \left(\left\langle\mathbf{q<em w="w">{w}^{i}, \mathbf{k}</em>}^{j}\right\rangle\right)+\sum_{j=1}^{n} \exp \left(\left\langle\mathbf{q<em m="m">{w}^{i}, \mathbf{k}</em> \
&amp; s_{i, t}^{m}=\frac{\exp \left(\left\langle\mathbf{q}}^{j}\right\rangle\right)<em m="m">{w}^{i}, \mathbf{k}</em>}^{t}\right\rangle\right)}{\sum_{j=1}^{n} \exp \left(\left\langle\mathbf{q<em w="w">{w}^{i}, \mathbf{k}</em>}^{j}\right\rangle\right)+\sum_{j=1}^{n} \exp \left(\left\langle\mathbf{q<em m="m">{w}^{i}, \mathbf{k}</em>
\end{aligned}
$$}^{j}\right\rangle\right)</p>
<p>where $\mathbf{q}<em m="m">{w}, \mathbf{q}</em>}, \mathbf{k<em m="m">{w}, \mathbf{k}</em>$ represents the input patch query, prototype patch query, input patch key and prototype patch key, respectively, and $\langle$,$\rangle represents the inner product . n$ denotes the number of patches in the time window. Next, we calculate the representation of the input as follows:</p>
<p>$$
\mathbf{o}=\sum_{t=1}^{n} s_{i, t}^{w} \mathbf{v}<em t="1">{w}^{t}+\sum</em>
$$}^{n} s_{i, t}^{m} \mathbf{v}_{m}^{t</p>
<p>where $\mathbf{v}<em m="m">{w}^{t}, \mathbf{v}</em>$ represent the value of input and prototype, respectively. The multi-head mechanism is also utilized to capture patterns of different scales. A flattened layer and a linear layer are subsequently employed to transform the input into the final representations $z$.}^{t</p>
<h3>3.3 LLM-based Teacher Network</h3>
<p>The teacher network is expected to produce generalizable representations. Previous works have unveiled the potential of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The framework of AnomalyLLM. It consists of three main components: prototype-based student network, LLM-based teacher network, and data augmentation-based training strategy.</p>
<p>pretrained NLP models such as GPT2 in time series representation generation <em>Zhou et al. (2023b)</em>. As a result, we devise the teacher network based on the pretrained LLM.</p>
<p>The input time series undergoes normalization and patching before being fed into the input embedding layer. Notably, the input embedding layer consists only of linear embedding, as positional embedding is inherent in the pretrained LLM. The linear embeddings in the teacher network and the student network utilize different parameters. The linear embedding in the student network is designed to extract correlations within the time series, while that in the teacher network focuses on transforming original time series signals into a representation comprehensible to the language model.</p>
<p>Next, the preprocessed inputs are fed into a network that retains the positional embedding layer and $B$ self-attention blocks of the pretrained LLM. We use GPT2 in this paper. To preserve the knowledge from pretrained LLM, we freeze the attention layer and the feed-forward layer which are crucial components for sequence modeling. The positional embedding layer and the layer normalization are fine-tuned on the input time series, adapting the LLM to understand the time series representations for anomaly detection. The output of the last self-attention block is fed into a flattened and linear layer to generate the eventual representation $c$.</p>
<h3>3.4 Model Training</h3>
<p>We aim to distinguish anomaly representations of the student and teacher networks. Given the absence of anomaly labels under unsupervised settings, we propose a data augmentation-based training strategy.</p>
<p>Firstly, we apply data augmentation to generate synthetic anomalies. To ensure that the general pattern of the sample does not change significantly, we randomly select a segment from the entire time window and apply augmentation to this segment. We use augmentation methods including jittering <em>Iwana and Uchida (2021)</em>, scaling <em>Wang et al. (2023)</em>, and warping <em>Sun et al. (2023a)</em>. The synthetic sample based on the original sample $\mathbf{w}<em i="i">{i}$ is denoted as $\mathbf{w}</em>$.}^{a</p>
<p>Next, both original and synthetic samples are fed into the teacher and student networks. The generated representation pairs of original and synthetic samples are denoted as $(z_{i},c_{i})$ and $(z_{i}^{a},c_{i}^{a})$, respectively. We push away the representation pair of the synthetic sample while pulling together that of the original samples. This knowledge distillation loss is calculated based on Eq. 1:</p>
<p>$\mathcal{L}<em i="1">{kd}=\frac{1}{N}\sum</em>\right|}^{N}\left|z_{i}-c_{i<em i="i">{2}^{2}-\log(1-\exp(-\left|z</em>)),$ (5)}^{a}-c_{i}^{a}\right|_{2}^{2</p>
<p>where $N$ is the total number of training samples.</p>
<p>Additionally, to enable the teacher network to focus on more general patterns and produce representations robust to noise, we consider the teacher’s representations of the original and synthetic samples as positive pairs and aim to minimize the distance between them. This negative-sample-free contrastive loss <em>Wang et al. (2023)</em> is defined as:</p>
<p>$\mathcal{L}<em i="1">{ce}=\frac{1}{N}\sum</em>\right|}^{N}-\frac{c_{i}}{\left|c_{i<em i="i">{2}}\cdot\frac{c</em>.$ (6)}^{a}}{\left|c_{i}^{a}\right|_{2}</p>
<p>Integrating this contrastive loss, the complete loss function is defined as:</p>
<p>$\mathcal{L}<em kd="kd">{total}=\mathcal{L}</em>,$ (7)}+\lambda\mathcal{L}_{ce</p>
<p>where $\lambda$ is a hyperparameter that controls the weight of knowledge distillation loss and contrastive loss.</p>
<p>During the testing phase, we calculate the anomaly score of the given time window $\mathbf{w}$ according to:</p>
<p>$A(\mathbf{w})=\left|\phi(\mathbf{w})-\varphi(\mathbf{w})\right|_{2}^{2}.$ (8)</p>
<table>
<thead>
<tr>
<th></th>
<th>ABP</th>
<th></th>
<th></th>
<th></th>
<th>Acceleration</th>
<th></th>
<th></th>
<th></th>
<th>Air Temperature</th>
<th></th>
<th></th>
<th></th>
<th>ECG</th>
<th></th>
<th></th>
<th></th>
<th>EPG</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
</tr>
<tr>
<td>Deep SVDD</td>
<td>0.333</td>
<td>0.559</td>
<td>0.568</td>
<td>0.564</td>
<td>0.714</td>
<td>0.750</td>
<td>0.735</td>
<td>0.743</td>
<td>0.385</td>
<td>0.727</td>
<td>0.725</td>
<td>0.726</td>
<td>0.297</td>
<td>0.586</td>
<td>0.588</td>
<td>0.587</td>
<td>0.360</td>
<td>0.703</td>
<td>0.698</td>
<td>0.700</td>
</tr>
<tr>
<td>AnoTrans</td>
<td>0.357</td>
<td>0.647</td>
<td>0.643</td>
<td>0.645</td>
<td>0.286</td>
<td>0.583</td>
<td>0.577</td>
<td>0.580</td>
<td>0.538</td>
<td>0.767</td>
<td>0.752</td>
<td>0.759</td>
<td>0.297</td>
<td>0.656</td>
<td>0.655</td>
<td>0.655</td>
<td>0.400</td>
<td>0.757</td>
<td>0.753</td>
<td>0.755</td>
</tr>
<tr>
<td>DCdetector</td>
<td>0.571</td>
<td>0.541</td>
<td>0.572</td>
<td>0.556</td>
<td>0.571</td>
<td>0.474</td>
<td>0.514</td>
<td>0.493</td>
<td>0.846</td>
<td>0.619</td>
<td>0.757</td>
<td>0.681</td>
<td>0.220</td>
<td>0.259</td>
<td>0.365</td>
<td>0.303</td>
<td>0.640</td>
<td>0.663</td>
<td>0.781</td>
<td>0.717</td>
</tr>
<tr>
<td>MEMTO</td>
<td>0.405</td>
<td>0.660</td>
<td>0.650</td>
<td>0.655</td>
<td>0.714</td>
<td>0.758</td>
<td>0.743</td>
<td>0.750</td>
<td>0.615</td>
<td>0.745</td>
<td>0.754</td>
<td>0.750</td>
<td>0.319</td>
<td>0.617</td>
<td>0.614</td>
<td>0.616</td>
<td>0.440</td>
<td>0.734</td>
<td>0.732</td>
<td>0.733</td>
</tr>
<tr>
<td>MTGFlow</td>
<td>0.500</td>
<td>0.539</td>
<td>0.578</td>
<td>0.558</td>
<td>0.714</td>
<td>0.907</td>
<td>0.901</td>
<td>0.904</td>
<td>0.462</td>
<td>0.682</td>
<td>0.692</td>
<td>0.687</td>
<td>0.286</td>
<td>0.603</td>
<td>0.602</td>
<td>0.602</td>
<td>0.520</td>
<td>0.662</td>
<td>0.660</td>
<td>0.661</td>
</tr>
<tr>
<td>GPT4TS</td>
<td>0.476</td>
<td>0.686</td>
<td>0.676</td>
<td>0.681</td>
<td>0.429</td>
<td>0.506</td>
<td>0.501</td>
<td>0.504</td>
<td>0.462</td>
<td>0.678</td>
<td>0.694</td>
<td>0.686</td>
<td>0.330</td>
<td>0.610</td>
<td>0.607</td>
<td>0.608</td>
<td>0.360</td>
<td>0.760</td>
<td>0.759</td>
<td>0.759</td>
</tr>
<tr>
<td>TS-TCC</td>
<td>0.690</td>
<td>0.763</td>
<td>0.745</td>
<td>0.754</td>
<td>0.286</td>
<td>0.555</td>
<td>0.543</td>
<td>0.549</td>
<td>1.000</td>
<td>0.980</td>
<td>0.957</td>
<td>0.969</td>
<td>0.627</td>
<td>0.785</td>
<td>0.782</td>
<td>0.784</td>
<td>0.880</td>
<td>0.928</td>
<td>0.935</td>
<td>0.931</td>
</tr>
<tr>
<td>THOC</td>
<td>0.762</td>
<td>0.822</td>
<td>0.808</td>
<td>0.815</td>
<td>0.714</td>
<td>0.782</td>
<td>0.770</td>
<td>0.776</td>
<td>1.000</td>
<td>0.984</td>
<td>0.958</td>
<td>0.971</td>
<td>0.604</td>
<td>0.762</td>
<td>0.758</td>
<td>0.760</td>
<td>0.880</td>
<td>0.911</td>
<td>0.905</td>
<td>0.908</td>
</tr>
<tr>
<td>NCAD</td>
<td>0.680</td>
<td>0.802</td>
<td>0.786</td>
<td>0.794</td>
<td>0.846</td>
<td>0.855</td>
<td>0.842</td>
<td>0.849</td>
<td>0.714</td>
<td>0.762</td>
<td>0.747</td>
<td>0.758</td>
<td>0.593</td>
<td>0.737</td>
<td>0.732</td>
<td>0.735</td>
<td>0.760</td>
<td>0.795</td>
<td>0.783</td>
<td>0.789</td>
</tr>
<tr>
<td>COCA</td>
<td>0.714</td>
<td>0.748</td>
<td>0.732</td>
<td>0.740</td>
<td>0.428</td>
<td>0.545</td>
<td>0.546</td>
<td>0.545</td>
<td>1.000</td>
<td>0.959</td>
<td>0.935</td>
<td>0.946</td>
<td>0.677</td>
<td>0.769</td>
<td>0.765</td>
<td>0.767</td>
<td>0.640</td>
<td>0.788</td>
<td>0.783</td>
<td>0.785</td>
</tr>
<tr>
<td>Ours</td>
<td>0.857</td>
<td>0.931</td>
<td>0.910</td>
<td>0.920</td>
<td>1.000</td>
<td>0.965</td>
<td>0.948</td>
<td>0.956</td>
<td>1.000</td>
<td>0.989</td>
<td>0.959</td>
<td>0.974</td>
<td>0.758</td>
<td>0.768</td>
<td>0.808</td>
<td>0.787</td>
<td>0.920</td>
<td>0.935</td>
<td>0.932</td>
<td>0.933</td>
</tr>
<tr>
<td></td>
<td>Gait</td>
<td></td>
<td></td>
<td></td>
<td>NASA</td>
<td></td>
<td></td>
<td></td>
<td>PowerDemand</td>
<td></td>
<td></td>
<td></td>
<td>RESP</td>
<td></td>
<td></td>
<td></td>
<td>Avg</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
<td>Acc</td>
<td>AP</td>
<td>AR</td>
<td>AF1</td>
</tr>
<tr>
<td>Deep SVDD</td>
<td>0.242</td>
<td>0.486</td>
<td>0.505</td>
<td>0.495</td>
<td>0.182</td>
<td>0.437</td>
<td>0.433</td>
<td>0.435</td>
<td>0.182</td>
<td>0.418</td>
<td>0.418</td>
<td>0.418</td>
<td>0.000</td>
<td>0.207</td>
<td>0.246</td>
<td>0.225</td>
<td>0.288</td>
<td>0.552</td>
<td>0.558</td>
<td>0.555</td>
</tr>
<tr>
<td>AnoTrans</td>
<td>0.364</td>
<td>0.685</td>
<td>0.682</td>
<td>0.683</td>
<td>0.636</td>
<td>0.821</td>
<td>0.815</td>
<td>0.819</td>
<td>0.455</td>
<td>0.663</td>
<td>0.658</td>
<td>0.660</td>
<td>0.117</td>
<td>0.667</td>
<td>0.667</td>
<td>0.667</td>
<td>0.348</td>
<td>0.680</td>
<td>0.677</td>
<td>0.679</td>
</tr>
<tr>
<td>DCdetector</td>
<td>0.424</td>
<td>0.381</td>
<td>0.515</td>
<td>0.438</td>
<td>0.909</td>
<td>0.608</td>
<td>0.931</td>
<td>0.736</td>
<td>0.455</td>
<td>0.491</td>
<td>0.493</td>
<td>0.492</td>
<td>0.117</td>
<td>0.517</td>
<td>0.682</td>
<td>0.588</td>
<td>0.424</td>
<td>0.431</td>
<td>0.538</td>
<td>0.479</td>
</tr>
<tr>
<td>MEMTO</td>
<td>0.364</td>
<td>0.674</td>
<td>0.671</td>
<td>0.673</td>
<td>0.364</td>
<td>0.662</td>
<td>0.665</td>
<td>0.664</td>
<td>0.455</td>
<td>0.683</td>
<td>0.677</td>
<td>0.680</td>
<td>0.059</td>
<td>0.598</td>
<td>0.598</td>
<td>0.598</td>
<td>0.368</td>
<td>0.658</td>
<td>0.654</td>
<td>0.656</td>
</tr>
<tr>
<td>MTGFlow</td>
<td>0.364</td>
<td>0.574</td>
<td>0.571</td>
<td>0.572</td>
<td>0.364</td>
<td>0.747</td>
<td>0.751</td>
<td>0.749</td>
<td>0.182</td>
<td>0.573</td>
<td>0.575</td>
<td>0.574</td>
<td>0.235</td>
<td>0.512</td>
<td>0.514</td>
<td>0.513</td>
<td>0.372</td>
<td>0.606</td>
<td>0.612</td>
<td>0.608</td>
</tr>
<tr>
<td>GPT4TS</td>
<td>0.212</td>
<td>0.463</td>
<td>0.460</td>
<td>0.461</td>
<td>0.364</td>
<td>0.850</td>
<td>0.848</td>
<td>0.849</td>
<td>0.182</td>
<td>0.597</td>
<td>0.598</td>
<td>0.598</td>
<td>0.353</td>
<td>0.544</td>
<td>0.543</td>
<td>0.544</td>
<td>0.348</td>
<td>0.624</td>
<td>0.622</td>
<td>0.623</td>
</tr>
<tr>
<td>TS-TCC</td>
<td>0.697</td>
<td>0.798</td>
<td>0.790</td>
<td>0.794</td>
<td>0.364</td>
<td>0.512</td>
<td>0.508</td>
<td>0.511</td>
<td>0.545</td>
<td>0.767</td>
<td>0.759</td>
<td>0.763</td>
<td>0.412</td>
<td>0.561</td>
<td>0.560</td>
<td>0.560</td>
<td>0.656</td>
<td>0.773</td>
<td>0.766</td>
<td>0.770</td>
</tr>
<tr>
<td>THOC</td>
<td>0.636</td>
<td>0.788</td>
<td>0.780</td>
<td>0.784</td>
<td>0.909</td>
<td>0.902</td>
<td>0.891</td>
<td>0.896</td>
<td>0.455</td>
<td>0.777</td>
<td>0.772</td>
<td>0.775</td>
<td>0.294</td>
<td>0.382</td>
<td>0.395</td>
<td>0.389</td>
<td>0.671</td>
<td>0.783</td>
<td>0.777</td>
<td>0.780</td>
</tr>
<tr>
<td>NCAD</td>
<td>0.848</td>
<td>0.864</td>
<td>0.852</td>
<td>0.858</td>
<td>0.818</td>
<td>0.869</td>
<td>0.853</td>
<td>0.861</td>
<td>0.545</td>
<td>0.724</td>
<td>0.723</td>
<td>0.723</td>
<td>0.353</td>
<td>0.613</td>
<td>0.612</td>
<td>0.613</td>
<td>0.663</td>
<td>0.772</td>
<td>0.763</td>
<td>0.767</td>
</tr>
<tr>
<td>COCA</td>
<td>0.545</td>
<td>0.703</td>
<td>0.694</td>
<td>0.699</td>
<td>0.818</td>
<td>0.849</td>
<td>0.833</td>
<td>0.841</td>
<td>0.364</td>
<td>0.633</td>
<td>0.631</td>
<td>0.632</td>
<td>0.235</td>
<td>0.563</td>
<td>0.562</td>
<td>0.562</td>
<td>0.620</td>
<td>0.746</td>
<td>0.738</td>
<td>0.742</td>
</tr>
<tr>
<td>Ours</td>
<td>0.878</td>
<td>0.891</td>
<td>0.852</td>
<td>0.871</td>
<td>1.000</td>
<td>0.969</td>
<td>0.953</td>
<td>0.961</td>
<td>0.818</td>
<td>0.888</td>
<td>0.884</td>
<td>0.886</td>
<td>0.471</td>
<td>0.736</td>
<td>0.736</td>
<td>0.736</td>
<td>0.820</td>
<td>0.857</td>
<td>0.860</td>
<td>0.858</td>
</tr>
</tbody>
</table>
<p>Table 1: Overall results on the UCR datasets.</p>
<h2>4 Experiment</h2>
<h3>4.1 Datasets</h3>
<p>UCR Anomaly Archive (UCR). Wu et al. <em>Wu and Keogh (2021)</em> identify several flaws in previously used benchmarks and introduce the UCR time-series anomaly archive. This archive comprises 250 diverse univariate time series signals spanning various domains. Following <em>Goswami et al. (2022)</em>, we partition the complete UCR archive into 9 separate datasets based on the domain to which each signal belongs, (1) Arterial Blood Pressure, ABP, (2) Acceleration, (3) Air Temperature, (4) Electrocardiogram, ECG, (5) Electrical Penetration Graph, EPG, (6) Gait, (7) NASA, (8) Power Demand, and (9) Respiration, RESP.
Previous benchmarks. We also evaluate our method on 6 previously commonly used multivariate datasets, including (1) SMD: It contains five-week-long metrics from 28 server machines in an internet company. (2) MSL: It records the conditions of sensors from the Mars rover provided by NASA (3) SMAP: It is also collected by NASA and gathers the soil samples and telemetry information used by the Mars rover. (4) PSM: Provided by eBay, it records 25-dimensional metrics from the server machines. (5) NIPS-TS-GECCO: It is a drinking water quality dataset collected through the Internet of Things. (6) NIPS-TS-SWAN: It contains space weather data from Harvard Dataverse.</p>
<p>In total, we use 15 datasets (9 univariate datasets and 6 multivariate datasets) from various domains for evaluation.</p>
<h3>4.2 Settings</h3>
<p>Evaluation Metrics. Traditional metrics for anomaly detection include precision, recall, and F1 score. Point adjustment and revised point adjustment are also utilized to postprocess the metrics. However, previous works have demonstrated that these metrics might lead to an overestimation of method performance <em>Kim et al. (2022)</em>. In this paper, we adopt affiliation metrics to assess the performance from an event-wise perspective <em>Huet et al. (2022)</em>. Precision, recall, and F1-score are calculated based on the affiliation between ground truth and prediction sets. For UCR datasets, we also employ accuracy as a metric <em>Wang et al. (2023)</em>, indicating the probability of correctly predicting subdatasets.
Hyperparameters. The baselines are implemented based on the hyperparameters reported in previous literature. For our model, we use the pretrained GPT2 with 6 layers as our teacher network. Regarding the student network, we use a pool with 32 prototypes and an attention mechanism with an intermediate dimension of 64 and a head number of 8. During the training stage, we utilize an Adam optimizer with a learning rate of 0.0001 and a batch size of 32. All experiments are conducted on a single RTX 3090.
Baselines. We compare our method with 10 baselines for evaluation, including the one-class classification-based methods: DeepSVDD <em>Ruff et al. (2018)</em>, THOC <em>Shen et al. (2020)</em>, NCAD <em>Carmona et al. (2021)</em>; the density estimation-based method: MTGFlow <em>Zhou et al. (2023a)</em>; the self-supervised methods: AnoTrans <em>Xu et al. (2021)</em>, MEMTO <em>Song et al. (2023)</em>, TS-TCC <em>Eldele et al. (2021)</em>, COCA <em>Wang et al. (2023)</em>, DCdetector <em>Yang et al. (2023)</em>; the LLM-based method: GPT4TS <em>Zhou et al. (2023b)</em>.</p>
<h3>4.3 Model Comparison</h3>
<p>Table 1 presents the performance of all methods on the UCR datasets. Our method consistently achieves the highest accuracy and affiliated F1 (AF1) score across all domains, demonstrating an average improvement of 22.2% in accuracy and 10.0% in AF1 compared to the second-best method. Notably, our method detects 82% of anomalies in total. Furthermore, three key observations can be made. First, while con-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Case studies of anomaly score visualization.</p>
<table>
<thead>
<tr>
<th></th>
<th>SMD</th>
<th></th>
<th></th>
<th>MSL</th>
<th></th>
<th></th>
<th>SMAP</th>
<th></th>
<th></th>
<th>PSM</th>
<th></th>
<th></th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>F1</td>
</tr>
<tr>
<td>LOF</td>
<td>0.563</td>
<td>0.399</td>
<td>0.467</td>
<td>0.477</td>
<td>0.853</td>
<td>0.612</td>
<td>0.589</td>
<td>0.563</td>
<td>0.576</td>
<td>0.579</td>
<td>0.905</td>
<td>0.706</td>
<td>0.609</td>
</tr>
<tr>
<td>OC-SVM</td>
<td>0.443</td>
<td>0.767</td>
<td>0.562</td>
<td>0.598</td>
<td>0.869</td>
<td>0.708</td>
<td>0.539</td>
<td>0.591</td>
<td>0.563</td>
<td>0.628</td>
<td>0.809</td>
<td>0.707</td>
<td>0.603</td>
</tr>
<tr>
<td>Isolation Foreset</td>
<td>0.423</td>
<td>0.733</td>
<td>0.536</td>
<td>0.539</td>
<td>0.865</td>
<td>0.665</td>
<td>0.524</td>
<td>0.591</td>
<td>0.555</td>
<td>0.761</td>
<td>0.925</td>
<td>0.835</td>
<td>0.612</td>
</tr>
<tr>
<td>MMPCACD</td>
<td>0.712</td>
<td>0.793</td>
<td>0.750</td>
<td>0.814</td>
<td>0.613</td>
<td>0.700</td>
<td>0.886</td>
<td>0.758</td>
<td>0.817</td>
<td>0.763</td>
<td>0.784</td>
<td>0.773</td>
<td>0.757</td>
</tr>
<tr>
<td>DAGMM</td>
<td>0.673</td>
<td>0.499</td>
<td>0.573</td>
<td>0.896</td>
<td>0.639</td>
<td>0.746</td>
<td>0.865</td>
<td>0.567</td>
<td>0.685</td>
<td>0.935</td>
<td>0.700</td>
<td>0.801</td>
<td>0.702</td>
</tr>
<tr>
<td>Deep-SVDD</td>
<td>0.785</td>
<td>0.797</td>
<td>0.791</td>
<td>0.919</td>
<td>0.766</td>
<td>0.836</td>
<td>0.899</td>
<td>0.560</td>
<td>0.690</td>
<td>0.954</td>
<td>0.865</td>
<td>0.907</td>
<td>0.810</td>
</tr>
<tr>
<td>THOC</td>
<td>0.798</td>
<td>0.910</td>
<td>0.850</td>
<td>0.885</td>
<td>0.910</td>
<td>0.897</td>
<td>0.921</td>
<td>0.893</td>
<td>0.907</td>
<td>0.881</td>
<td>0.910</td>
<td>0.895</td>
<td>0.880</td>
</tr>
<tr>
<td>LSTM-VAE</td>
<td>0.758</td>
<td>0.901</td>
<td>0.823</td>
<td>0.855</td>
<td>0.799</td>
<td>0.826</td>
<td>0.922</td>
<td>0.678</td>
<td>0.781</td>
<td>0.736</td>
<td>0.899</td>
<td>0.810</td>
<td>0.812</td>
</tr>
<tr>
<td>BeatGAN</td>
<td>0.729</td>
<td>0.841</td>
<td>0.781</td>
<td>0.898</td>
<td>0.854</td>
<td>0.875</td>
<td>0.924</td>
<td>0.559</td>
<td>0.696</td>
<td>0.903</td>
<td>0.938</td>
<td>0.920</td>
<td>0.802</td>
</tr>
<tr>
<td>OmniAnomaly</td>
<td>0.837</td>
<td>0.868</td>
<td>0.852</td>
<td>0.890</td>
<td>0.864</td>
<td>0.877</td>
<td>0.925</td>
<td>0.820</td>
<td>0.869</td>
<td>0.814</td>
<td>0.843</td>
<td>0.828</td>
<td>0.847</td>
</tr>
<tr>
<td>InterFusion</td>
<td>0.870</td>
<td>0.854</td>
<td>0.862</td>
<td>0.813</td>
<td>0.927</td>
<td>0.866</td>
<td>0.898</td>
<td>0.885</td>
<td>0.891</td>
<td>0.836</td>
<td>0.835</td>
<td>0.835</td>
<td>0.857</td>
</tr>
<tr>
<td>Anomaly Transformer</td>
<td>0.880</td>
<td>0.947</td>
<td>0.912</td>
<td>0.911</td>
<td>0.901</td>
<td>0.906</td>
<td>0.940</td>
<td>0.985</td>
<td>0.962</td>
<td>0.968</td>
<td>0.986</td>
<td>0.977</td>
<td>0.936</td>
</tr>
<tr>
<td>DCdetector</td>
<td>0.836</td>
<td>0.911</td>
<td>0.872</td>
<td>0.937</td>
<td>0.997</td>
<td>0.966</td>
<td>0.956</td>
<td>0.989</td>
<td>0.970</td>
<td>0.971</td>
<td>0.987</td>
<td>0.979</td>
<td>0.946</td>
</tr>
<tr>
<td>MEMTO</td>
<td>0.891</td>
<td>0.984</td>
<td>0.935</td>
<td>0.921</td>
<td>0.968</td>
<td>0.944</td>
<td>0.938</td>
<td>0.996</td>
<td>0.966</td>
<td>0.975</td>
<td>0.992</td>
<td>0.983</td>
<td>0.957</td>
</tr>
<tr>
<td>Ours</td>
<td>0.937</td>
<td>0.979</td>
<td>0.958</td>
<td>0.944</td>
<td>0.969</td>
<td>0.956</td>
<td>0.934</td>
<td>0.998</td>
<td>0.965</td>
<td>0.996</td>
<td>0.998</td>
<td>0.997</td>
<td>0.969</td>
</tr>
</tbody>
</table>
<p>Table 2: Overall results on SMD, MSL, SMAP, and PSM.</p>
<table>
<thead>
<tr>
<th></th>
<th>NIPS-TS-GECCO</th>
<th></th>
<th></th>
<th>NIPS-TS-SWAN</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>P</td>
<td>R</td>
<td>F1</td>
<td>P</td>
<td>R</td>
<td>F1</td>
</tr>
<tr>
<td>OCSVM</td>
<td>0.021</td>
<td>0.341</td>
<td>0.040</td>
<td>0.193</td>
<td>0.001</td>
<td>0.001</td>
</tr>
<tr>
<td>MatrixProfile</td>
<td>0.046</td>
<td>0.185</td>
<td>0.074</td>
<td>0.167</td>
<td>0.175</td>
<td>0.171</td>
</tr>
<tr>
<td>GBRT</td>
<td>0.175</td>
<td>0.140</td>
<td>0.156</td>
<td>0.447</td>
<td>0.375</td>
<td>0.408</td>
</tr>
<tr>
<td>LSTM-RNN</td>
<td>0.343</td>
<td>0.275</td>
<td>0.305</td>
<td>0.527</td>
<td>0.221</td>
<td>0.312</td>
</tr>
<tr>
<td>Autoregression</td>
<td>0.392</td>
<td>0.314</td>
<td>0.349</td>
<td>0.421</td>
<td>0.354</td>
<td>0.385</td>
</tr>
<tr>
<td>IForest</td>
<td>0.439</td>
<td>0.353</td>
<td>0.391</td>
<td>0.569</td>
<td>0.598</td>
<td>0.583</td>
</tr>
<tr>
<td>AutoEncoder</td>
<td>0.424</td>
<td>0.340</td>
<td>0.377</td>
<td>0.497</td>
<td>0.522</td>
<td>0.509</td>
</tr>
<tr>
<td>AnomalyTrans</td>
<td>0.257</td>
<td>0.285</td>
<td>0.270</td>
<td>0.907</td>
<td>0.474</td>
<td>0.623</td>
</tr>
<tr>
<td>MTGFlow</td>
<td>0.333</td>
<td>0.125</td>
<td>0.182</td>
<td>1.000</td>
<td>0.494</td>
<td>0.662</td>
</tr>
<tr>
<td>DCdetector</td>
<td>0.383</td>
<td>0.597</td>
<td>0.466</td>
<td>0.955</td>
<td>0.596</td>
<td>0.734</td>
</tr>
<tr>
<td>Ours</td>
<td>0.511</td>
<td>0.793</td>
<td>0.620</td>
<td>0.873</td>
<td>0.745</td>
<td>0.804</td>
</tr>
</tbody>
</table>
<p>Table 3: Overall results on the NIPS benchmark.</p>
<p>ventional DeepSVDD struggles to deliver satisfactory performance, one-class classification methods like THOC and NCAD show great potential in time series anomaly detection when they incorporate temporal information within the time window. Second, contrastive learning-based methods (TSTCC and COCA) outperform reconstruction-based methods (AnoTrans, MEMTO), suggesting that approaching anomaly detection from a representation perspective has advantages over using original signals. Third, the direct application of LLM (GPT4TS) falls short of expectations. This is attributed to the overstrong ability of the LLM-based autoencoder to reconstruct even abnormal time series, resulting in poor performance. Our method overcomes this limitation by introducing a small student network to extract features without overgeneralization.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablation studies. <strong>Top:</strong> It depicts the performance of the model with different center predictors. <strong>Middle:</strong> It depicts the performance of the models with different projectors. <strong>Bottom:</strong> It depicts the performance of different training strategies.</p>
<p>We extend the evaluation of our method to multivariate datasets and present results in Table 2 and Table 3. Notably, most previous methods report results after employing the point adjustment strategy. To ensure a fair comparison, we also include the adjusted metrics for these datasets. For datasets such as SMD, MSL, SMAP, and PSM which contain a large number of obvious anomalies [Wu and Keogh, 2021], our method demonstrates comparable performance with SOTA methods. In the case of NIPS-TS-GECCO and NIPS-TS-SWAN, which present diverse and challenging anomalies, our method outperforms DCdetector significantly, achieving a 62% F1 score compared to 47% on GECCO and an 80% F1 score compared to 73% on SWAN.</p>
<h3>4.4 Model Analysis</h3>
<p><strong>Anomaly score visualization.</strong> We present three case studies in Fig. 3 to illustrate the functionality of our method.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />Figure 5: Parameter sensitivity studies of main hyperparameters in AnomalyLLM.</p>
<p>Anomaly scores are normalized and plotted for each subdataset. Anomalies are identified by assigning a high anomaly score, indicative of the disparity between the output of the student network and the LLM-based teacher network. In the UCR 113 dataset, the original signals exhibit nonstationary characteristics, with varying mean values across different stages. Our method demonstrates robustness to this domain shift and successfully identifies the most anomalous segment, characterized by a distinct shape.</p>
<p>Ablation study. In this section, our goal is to investigate the role of each component in our method.</p>
<p>1) LLM-based teacher network: To assess the significance of the LLM in representation generation, we replace it with three main transformer-based blocks (Informer, Reformer, and Autoformer). The blocks are trained from scratch on each dataset. The results reveal that the F1-score of variants with these replaced blocks fluctuates by less than 2.6%, with an average AF1 score of 73.6%. Our method utilizes GPT2 with pretrained parameters and only fine-tunes the positional embedding and layer normalization, resulting in an enhanced AF1 score of 12%. The reason for this improvement can be ascribed to the fact that LLM has been pretrained by large NLP datasets, enabling it to capture temporal sequence patterns that can be shared by time series after fine-tuning, as demonstrated in previous research <em>Zhou et al. (2023b)</em>.</p>
<p>2) Prototype-based student network: To validate the effectiveness of our student network, we experiment with different choices (TCN, TimesNet, and TST). TCN is a classical 1D CNN-based feature extractor commonly used in time series analysis and has been applied in previous one-class classification methods such as DeepSVDD, THOC, and NCAD. TimesNet, proposed by <em>Wu et al. (2022)</em>, rearranges time series data as a 2D tensor using Fast Fourier Transform (FFT) and leverages 2D CNN to extract features. TST is a SOTA Transformer-based feature extractor proposed by <em>Nie et al. (2022)</em>. The results show that TST alone does not significantly improve upon TCN, as it applies the attention mechanism like the teacher network does, which can easily result in similar representations of these two networks. In contrast, we incorporate prototypes into TST, enabling it to focus more on those historical frequent patterns and resulting in a notable improvement of 14.7% in AF1 score.</p>
<p>3) Training strategy: Finally, we explore different training strategies. The first strategy ’NonAug’ excludes augmentation techniques and trains the model solely using original data, resulting in the poorest performance. The second strategy ’W/O CT’ incorporates data augmentation but omits the contrastive loss between the teacher networks’ representations in Eq. 6. This strategy improves AF1 by 4.2% but still falls behind our strategy. We force the teacher network to learn the representations that are robust to the noises by employing a contrastive regularization term. The third strategy ’W/ CS’ introduces an additional contrastive loss aiming to maximize the discrepancy between the student network’s representations of the original and augmented samples. This additional loss leads to an 8.0% reduction in AF1 score, suggesting that our loss, which prioritizes the discrepancy between the representations of the student and teacher network, is more effective.</p>
<p>Parameter Sensitivity. In this part, we discuss the relationship between model performance and hyperparameters. Fig. 5(a) depicts the performance under various window sizes, showing that the performance remains relatively stable as the window size increases, maintaining an accuracy of over 72% and an AF1 score of over 80%. Fig. 5(b) showcases the performance with different augmentation methods (Jittering, scaling, and warping). We can find that the introduction of warping leads to an increase in performance. The performance under different numbers of layers in GPT2 is shown in Fig. 5(c). The performance stabilizes when the number is above 3, indicating that GPT2 with 3 frozen layers is sufficient to describe the distribution of time series signals. Fig. 5(d) shows the results under different numbers of layers in the projector. It can be seen that the performance reaches its peak at 3. Finally, we find that the 48 prototype provides the most useful information for our projector to extract the representations, as shown in Fig. 5(e). Complete results are reported in the appendix.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose the first knowledge distillation-based TSAD method, named AnomalyLLM. Anomaly scores are determined by the representation discrepancy between the student and teacher networks. The teacher network is finetuned from a pretrained LLM to generate generalizable representations for time series signals when only limited samples are available. The student network incorporates prototypical signals to produce more domain-specific representations. Besides, we propose a data augmentation-based training strategy to enhance the representation gap on anomalous samples. AnomalyLLM surpasses SOTA approaches on 9 univariate datasets and 6 multivariate datasets, highlighting the remarkable potential of knowledge distillation and LLM in time series anomaly detection. Future research should ex-</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th>Test</th>
<th>Anomaly Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>ABP</td>
<td>1036746</td>
<td>1841461</td>
<td>0.53%</td>
</tr>
<tr>
<td>Acceleration</td>
<td>38400</td>
<td>62337</td>
<td>2.45%</td>
</tr>
<tr>
<td>AirTemperature</td>
<td>52000</td>
<td>54392</td>
<td>2.86%</td>
</tr>
<tr>
<td>ECG</td>
<td>1795083</td>
<td>6047314</td>
<td>0.53%</td>
</tr>
<tr>
<td>EPG</td>
<td>119000</td>
<td>410415</td>
<td>1.29%</td>
</tr>
<tr>
<td>Gait</td>
<td>1157571</td>
<td>2784520</td>
<td>0.43%</td>
</tr>
<tr>
<td>NASA</td>
<td>38500</td>
<td>86296</td>
<td>2.35%</td>
</tr>
<tr>
<td>PowerDemand</td>
<td>197149</td>
<td>311629</td>
<td>0.81%</td>
</tr>
<tr>
<td>RESP</td>
<td>868000</td>
<td>2452953</td>
<td>0.19%</td>
</tr>
</tbody>
</table>
<p>Table 4: Details of univariate datasets</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Dimension</th>
<th>Train</th>
<th>Test</th>
<th>Anomaly Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td>SMD</td>
<td>38</td>
<td>708377</td>
<td>708393</td>
<td>4.16%</td>
</tr>
<tr>
<td>MSL</td>
<td>55</td>
<td>58317</td>
<td>73729</td>
<td>10.50%</td>
</tr>
<tr>
<td>SMAP</td>
<td>25</td>
<td>135183</td>
<td>427617</td>
<td>12.79%</td>
</tr>
<tr>
<td>PSM</td>
<td>25</td>
<td>132481</td>
<td>87841</td>
<td>27.75%</td>
</tr>
<tr>
<td>GECCO</td>
<td>10</td>
<td>60000</td>
<td>60000</td>
<td>1.25%</td>
</tr>
<tr>
<td>SWAN</td>
<td>39</td>
<td>69260</td>
<td>69261</td>
<td>23.80%</td>
</tr>
</tbody>
</table>
<p>Table 5: Details of multivariate datasets</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Choices</th>
</tr>
</thead>
<tbody>
<tr>
<td>Window size</td>
<td>[16,32,48,64,80,96,112,128 ]</td>
</tr>
<tr>
<td>Feature dimension</td>
<td>[32,64,96,128]</td>
</tr>
<tr>
<td>Teacher layers</td>
<td>[1,2,3,4,5,6]</td>
</tr>
<tr>
<td>Student layers</td>
<td>[1,2,3,4,5,6]</td>
</tr>
<tr>
<td>Prototype number</td>
<td>[16,32,48,64,80]</td>
</tr>
<tr>
<td>Patch size</td>
<td>8</td>
</tr>
<tr>
<td>Head number</td>
<td>8</td>
</tr>
<tr>
<td>Epochs</td>
<td>100</td>
</tr>
<tr>
<td>Patience</td>
<td>10</td>
</tr>
<tr>
<td>Learning rate</td>
<td>0.0001</td>
</tr>
<tr>
<td>Contrastive weight</td>
<td>0.1</td>
</tr>
<tr>
<td>Batch size</td>
<td>128</td>
</tr>
</tbody>
</table>
<p>Table 6: Main hyperparameters</p>
<p>plore lightweight versions of the teacher network within our framework, tailored for deployment in scenarios with limited computational and memory resources.</p>
<h2>A Experiment Setups</h2>
<p>Datasets. We evaluate our method on nine univariate datasets from the UCR archive [Wu and Keogh, 2021], including: (1) ABP, (2) Acceleration, (3) AirTemperature, (4) ECG, (5) EPG, (6) Gait, (7) NASA, (8) PowerDemand, (9) RESP, and six multivariate datasets including (1) SMD, (2) MSL, (3) SMAP, (4) PSM, (5) GECCO, (6) SWAN. The details are introduced in Table 4 and Table 5.</p>
<h2>References</h2>
<ul>
<li>[Audibert et al.2020] Julien Audibert, Pietro Michiardi, Frédéric Guyard, Sébastien Marti, and Maria A Zuluaga. Usad: Unsupervised anomaly detection on multivariate time series. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pages 3395–3404, 2020.</li>
<li>[Bergmann et al.2020] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4183–4192, 2020.</li>
<li>[Blázquez-García et al.2021] Ane Blázquez-García, Angel Conde, Usue Mori, and Jose A Lozano. A review on outlier/anomaly detection in time series data. ACM Computing Surveys (CSUR), 54(3):1–33, 2021.</li>
<li>[Cao et al.2023] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. arXiv preprint arXiv:2310.04948, 2023.</li>
</ul>
<p>[Carmona et al., 2021] Chris U Carmona, François-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus. Neural contextual anomaly detection for time series. arXiv preprint arXiv:2107.07702, 2021.
[Chen et al., 2023] Yuhang Chen, Chaoyun Zhang, Minghua Ma, Yudong Liu, Ruomeng Ding, Bowen Li, Shilin He, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. Imdiffusion: Imputed diffusion models for multivariate time series anomaly detection. arXiv preprint arXiv:2307.00754, 2023.
[Dai and Chen, 2022] Enyan Dai and Jie Chen. Graphaugmented normalizing flows for anomaly detection of multiple time series. arXiv preprint arXiv:2202.07857, 2022.
[Deng and Hooi, 2021] Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time series. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4027-4035, 2021.
[Eldele et al., 2021] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. arXiv preprint arXiv:2106.14112, 2021.
[Garza and Mergenthaler-Canseco, 2023] Azul Garza and Max Mergenthaler-Canseco. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023.
[Godahewa et al., 2021] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. arXiv preprint arXiv:2105.06643, 2021.
[Goswami et al., 2022] Mononito Goswami, Cristian Challu, Laurent Callot, Lenon Minorics, and Andrey Kan. Unsupervised model selection for time-series anomaly detection. arXiv preprint arXiv:2210.01078, 2022.
[Gruver et al., 2023] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.07820, 2023.
[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[Huet et al., 2022] Alexis Huet, Jose Manuel Navarro, and Dario Rossi. Local evaluation of time series anomaly detection algorithms. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 635-645, 2022.
[Iwana and Uchida, 2021] Brian Kenji Iwana and Seiichi Uchida. An empirical survey of data augmentation for time series classification with neural networks. Plos one, 16(7):e0254841, 2021.
[Jeong et al., 2023] Yungi Jeong, Eunseok Yang, Jung Hyun Ryu, Imseong Park, and Myungjoo Kang. Anomalybert: Self-supervised transformer for time series anomaly detection using data degradation scheme. arXiv preprint arXiv:2305.04468, 2023.
[Jin et al., 2023a] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.
[Jin et al., 2023b] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023.
[Kieu et al., 2022] Tung Kieu, Bin Yang, Chenjuan Guo, Razvan-Gabriel Cirstea, Yan Zhao, Yale Song, and Christian S Jensen. Anomaly detection in time series with robust variational quasi-recurrent autoencoders. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pages 1342-1354. IEEE, 2022.
[Kim et al., 2021] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.
[Kim et al., 2022] Siwon Kim, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. Towards a rigorous evaluation of time-series anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7194-7201, 2022.
[Li et al., 2023a] Shizhong Li, Wenchao Meng, Shibo He, Jichao Bi, and Guanglun Liu. Staged: A spatial-temporal aware graph encoder-decoder for fault diagnosis in industrial processes. IEEE Transactions on Industrial Informatics, 2023.
[Li et al., 2023b] Yuxin Li, Wenchao Chen, Bo Chen, Dongsheng Wang, Long Tian, and Mingyuan Zhou. Prototypeoriented unsupervised anomaly detection for multivariate time series. In International Conference on Machine Learning, pages 19407-19424. PMLR, 2023.
[Nie et al., 2022] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.
[Park et al., 2018] Daehyung Park, Yuuna Hoshi, and Charles C Kemp. A multimodal anomaly detector for robot-assisted feeding using an lstm-based variational autoencoder. IEEE Robotics and Automation Letters, 3(3):1544-1551, 2018.
[Ruff et al., 2018] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. In International conference on machine learning, pages 4393-4402. PMLR, 2018.
[Ruff et al., 2020] Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert Müller, and Marius Kloft. Rethinking assumptions in deep anomaly detection. arXiv preprint arXiv:2006.00339, 2020.</p>
<p>[Ruff et al., 2021] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Müller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756-795, 2021.
[Salehi et al., 2021] Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and Hamid R Rabiee. Multiresolution knowledge distillation for anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14902-14912, 2021.
[Shen et al., 2020] Lifeng Shen, Zhuocong Li, and James Kwok. Timeseries anomaly detection using temporal hierarchical one-class network. Advances in Neural Information Processing Systems, 33:13016-13026, 2020.
[Song et al., 2023] Junho Song, Keonwoo Kim, Jeonglyul Oh, and Sungzoon Cho. Memto: Memory-guided transformer for multivariate time series anomaly detection. arXiv preprint arXiv:2312.02530, 2023.
[Su et al., 2019] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pages 2828-2837, 2019.
[Sun et al., 2023a] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm's ability for time series. arXiv preprint arXiv:2308.08241, 2023.
[Sun et al., 2023b] Yuting Sun, Guansong Pang, Guanhua Ye, Tong Chen, Xia Hu, and Hongzhi Yin. Unraveling theanomaly'in time series anomaly detection: A self-supervised tri-domain solution. arXiv preprint arXiv:2311.11235, 2023.
[Wang et al., 2023] Rui Wang, Chongwei Liu, Xudong Mou, Kai Gao, Xiaohui Guo, Pin Liu, Tianyu Wo, and Xudong Liu. Deep contrastive one-class time series anomaly detection. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pages 694-702. SIAM, 2023.
[Wu and Keogh, 2021] Renjie Wu and Eamonn Keogh. Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress. IEEE Transactions on Knowledge and Data Engineering, 2021.
[Wu et al., 2022] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186, 2022.
[Xu et al., 2021] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. arXiv preprint arXiv:2110.02642, 2021.
[Yang et al., 2023] Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. Dcdetector: Dual attention
contrastive representation learning for time series anomaly detection. arXiv preprint arXiv:2306.10347, 2023.
[Zhang et al., 2023] Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. Selfsupervised learning for time series analysis: Taxonomy, progress, and prospects. arXiv preprint arXiv:2306.10125, 2023.
[Zhou et al., 2019] Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. Beatgan: Anomalous rhythm detection using adversarially generated time series. In IJCAI, volume 2019, pages 4433-4439, 2019.
[Zhou et al., 2022] Qihang Zhou, Shibo He, Haoyu Liu, Tao Chen, and Jiming Chen. Pull \&amp; push: Leveraging differential knowledge distillation for efficient unsupervised anomaly detection and localization. IEEE Transactions on Circuits and Systems for Video Technology, 2022.
[Zhou et al., 2023a] Qihang Zhou, Jiming Chen, Haoyu Liu, Shibo He, and Wenchao Meng. Detecting multivariate time series anomalies with zero known label. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 4963-4971, 2023.
[Zhou et al., 2023b] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained 1m. arXiv preprint arXiv:2302.11939, 2023.
[Zhou et al., 2024] Qihang Zhou, Shibo He, Haoyu Liu, Jiming Chen, and Wenchao Meng. Label-free multivariate time series anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 2024.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.828</td>
</tr>
<tr>
<td style="text-align: center;">Reformer</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.340</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.881</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: center;">TCN</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.680</td>
</tr>
<tr>
<td style="text-align: center;">TimesNet</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.613</td>
</tr>
<tr>
<td style="text-align: center;">TST</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.740</td>
</tr>
<tr>
<td style="text-align: center;">Nonaug</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.652</td>
</tr>
<tr>
<td style="text-align: center;">w/o center</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.673</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.672</td>
</tr>
<tr>
<td style="text-align: center;">w/ feature</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.718</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">Informer</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: center;">Reformer</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.675</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.737</td>
</tr>
<tr>
<td style="text-align: center;">Autoformer</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.748</td>
</tr>
<tr>
<td style="text-align: center;">TCN</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.669</td>
</tr>
<tr>
<td style="text-align: center;">TimesNet</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.601</td>
</tr>
<tr>
<td style="text-align: center;">TST</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.595</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.705</td>
</tr>
<tr>
<td style="text-align: center;">Nonaug</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.676</td>
</tr>
<tr>
<td style="text-align: center;">w/o center</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.718</td>
</tr>
<tr>
<td style="text-align: center;">w/ feature</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
</tbody>
</table>
<p>Table 7: Overall results of ablation studies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.751</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.793</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.904</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.859</td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.960</td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.925</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.897</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.752</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.801</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.842</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.836</td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.781</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.841</td>
<td style="text-align: center;">0.842</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.888</td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.846</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.826</td>
</tr>
</tbody>
</table>
<p>Table 8: Overall results of different window sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">J</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.880</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.919</td>
</tr>
<tr>
<td style="text-align: center;">W</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.879</td>
</tr>
<tr>
<td style="text-align: center;">J + S</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.903</td>
</tr>
<tr>
<td style="text-align: center;">J + W</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">9.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">S + W</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.938</td>
</tr>
<tr>
<td style="text-align: center;">J + S + W</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.920</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">J</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.788</td>
</tr>
<tr>
<td style="text-align: center;">S</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.777</td>
<td style="text-align: center;">0.773</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.809</td>
</tr>
<tr>
<td style="text-align: center;">W</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.809</td>
</tr>
<tr>
<td style="text-align: center;">J + S</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.800</td>
</tr>
<tr>
<td style="text-align: center;">J + W</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: center;">S + W</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.865</td>
</tr>
<tr>
<td style="text-align: center;">J + S + W</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.830</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.837</td>
</tr>
</tbody>
</table>
<p>Table 9: Overall results of different augmentation methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.709</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.774</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.882</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.923</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.833</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.920</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.908</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.845</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.757</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.822</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.850</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.839</td>
<td style="text-align: center;">0.837</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.850</td>
</tr>
</tbody>
</table>
<p>Table 10: Overall results of different different teacher layers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.914</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.785</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.847</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.803</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.538</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.723</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.748</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.788</td>
<td style="text-align: center;">0.789</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.682</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.799</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.772</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.770</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.814</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.770</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.761</td>
</tr>
</tbody>
</table>
<p>Table 11: Overall results of different student layers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ABP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Acceleration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Air Temperature</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ECG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">EPG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.780</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">9.956</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.947</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.937</td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.715</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.795</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gait</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NASA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PowerDemand</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RESP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">AR</td>
<td style="text-align: center;">AF1</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.765</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.767</td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.858</td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.875</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.848</td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.792</td>
</tr>
</tbody>
</table>
<p>Table 12: Overall results of different prototype sizes.</p>            </div>
        </div>

    </div>
</body>
</html>
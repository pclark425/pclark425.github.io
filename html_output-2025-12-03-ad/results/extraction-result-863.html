<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-863 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-863</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-863</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-f5f323e62acb75f785e00b4c90ace16f1690076f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f5f323e62acb75f785e00b4c90ace16f1690076f" target="_blank">Deep Recurrent Q-Learning for Partially Observable MDPs</a></p>
                <p><strong>Paper Venue:</strong> AAAI Fall Symposia</p>
                <p><strong>Paper TL;DR:</strong> The effects of adding recurrency to a Deep Q-Network is investigated by replacing the first post-convolutional fully-connected layer with a recurrent LSTM, which successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens.</p>
                <p><strong>Paper Abstract:</strong> Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep learning for real-time atari game play using offline monte-carlo tree search planning <em>(Rating: 1)</em></li>
                <li>Solving deep memory POMDPs with recurrent policy gradients <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-863",
    "paper_id": "paper-f5f323e62acb75f785e00b4c90ace16f1690076f",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning",
            "rating": 1
        },
        {
            "paper_title": "Solving deep memory POMDPs with recurrent policy gradients",
            "rating": 1
        }
    ],
    "cost": 0.0041215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Recurrent Q-Learning for Partially Observable MDPs</h1>
<p>Matthew Hausknecht and Peter Stone<br>Department of Computer Science<br>The University of Texas at Austin<br>{mhauskn, pstone}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent $Q$-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.</p>
<h2>Introduction</h2>
<p>Deep Q-Networks (DQNs) have been shown to be capable of learning human-level control policies on a variety of different Atari 2600 games (Mnih et al. 2015). True to their name, DQNs learn to estimate the Q-Values (or long-term discounted returns) of selecting each possible action from the current game state. Given that the network's Q-Value estimate is sufficiently accurate, a game may be played by selecting the action with the maximal Q-Value at each timestep. Learning policies mapping from raw screen pixels to actions, these networks have been shown to achieve state-of-the-art performance on many Atari 2600 games.</p>
<p>However, Deep Q-Networks are limited in the sense that they learn a mapping from a limited number of past states, or game screens in the case of Atari 2600. In practice, DQN is trained using an input consisting of the last four states the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Nearly all Atari 2600 games feature moving objects. Given only one frame of input, Pong, Frostbite, and Double Dunk are all POMDPs because a single observation does not reveal the velocity of the ball (Pong, Double Dunk) or the velocity of the icebergs (Frostbite).
agent has encountered. Thus DQN will be unable to master games that require the player to remember events more distant than four screens in the past. Put differently, any game that requires a memory of more than four frames will appear non-Markovian because the future game states (and rewards) depend on more than just DQN's current input. Instead of a Markov Decision Process (MDP), the game becomes a Partially-Observable Markov Decision Process (POMDP).</p>
<p>Real-world tasks often feature incomplete and noisy state information resulting from partial observability. As Figure 1 shows, given only a single game screen, many Atari 2600 games are POMDPs. One example is the game of Pong in which the current screen only reveals the location of the paddles and the ball, but not the velocity of the ball. Knowing the direction of travel of the ball is a crucial component for determining the best paddle location.</p>
<p>We observe that DQN's performance declines when given incomplete state observations and hypothesize that DQN may be modified to better deal with POMDPs by leveraging advances in Recurrent Neural Networks. Therefore we introduce the Deep Recurrent $Q$-Network (DRQN), a combination of a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber 1997) and a Deep Q-Network. Crucially, we demonstrate that DRQN is capable of handing partial observability, and that when trained with full observations and evaluated with partial observations, DRQN better handles</p>
<p>the loss of information than does DQN. Thus, recurrency confers benefits as the quality of observations degrades.</p>
<h2>Deep Q-Learning</h2>
<p>Reinforcement Learning (Sutton and Barto 1998) is concerned with learning control policies for agents interacting with unknown environments. Such environments are often formalized as a Markov Decision Processes (MDPs), described by a 4-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R})$. At each timestep $t$ an agent interacting with the MDP observes a state $s_{t} \in \mathcal{S}$, and chooses an action $a_{t} \in \mathcal{A}$ which determines the reward $r_{t} \sim \mathcal{R}\left(s_{t}, a_{t}\right)$ and next state $s_{t+1} \sim \mathcal{P}\left(s_{t}, a_{t}\right)$.</p>
<p>Q-Learning (Watkins and Dayan 1992) is a model-free off-policy algorithm for estimating the long-term expected return of executing an action from a given state. These estimated returns are known as Q-values. A higher Q-value indicates an action $a$ is judged to yield better long-term results in a state $s$. Q-values are learned iteratively by updating the current Q-value estimate towards the observed reward plus the max Q-value over all actions $a^{\prime}$ in the resulting state $s^{\prime}$ :</p>
<p>$$
Q(s, a):=Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)
$$</p>
<p>Many challenging domains such as Atari games feature far too many unique states to maintain a separate estimate for each $\mathcal{S} \times \mathcal{A}$. Instead a model is used to approximate the Q-values (Mnih et al. 2015). In the case of Deep QLearning, the model is a neural network parameterized by weights and biases collectively denoted as $\theta$. Q-values are estimated online by querying the output nodes of the network after performing a forward pass given a state input. Such Q-values are denoted $Q(s, a \mid \theta)$. Instead of updating individual Q-values, updates are now made to the parameters of the network to minimize a differentiable loss function:</p>
<p>$$
\begin{gathered}
L\left(s, a \mid \theta_{i}\right)=\left(r+\gamma \max <em i="i">{a^{\prime}} Q\left(s^{\prime}, a^{\prime} \mid \theta</em> \
\theta_{i+1}=\theta_{i}+\alpha \nabla_{\theta} L\left(\theta_{i}\right)
\end{gathered}
$$}\right)-Q\left(s, a \mid \theta_{i}\right)\right)^{2</p>
<p>Since $|\theta| \ll|\mathcal{S} \times \mathcal{A}|$, the neural network model naturally generalizes beyond the states and actions it has been trained on. However, because the same network is generating the next state target Q-values that are used in updating its current Q-values, such updates can oscillate or diverge (Tsitsiklis and Roy 1997). Deep Q-Learning uses three techniques to restore learning stability: First, experiences $e_{t}=\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ are recorded in a replay memory $\mathcal{D}$ and then sampled uniformly at training time. Second, a separate, target network $\hat{Q}$ provides update targets to the main network, decoupling the feedback resulting from the network generating its own targets. $\hat{Q}$ is identical to the main network except its parameters $\theta^{-}$are updated to match $\theta$ every 10,000 iterations. Finally, an adaptive learning rate method such as RMSProp (Tieleman and Hinton 2012) or ADADELTA (Zeiler 2012) maintains a per-parameter learning rate $\alpha$, and adjusts $\alpha$ according to the history of gradient updates to that parameter. This step serves to compensate for the lack of a fixed training dataset; the ever-changing nature
of $\mathcal{D}$ may require certain parameters start changing again after having reached a seeming fixed point.</p>
<p>At each training iteration $i$, an experience $e_{t}=$ $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ is sampled uniformly from the replay memory $\mathcal{D}$. The loss of the network is determined as follows:</p>
<p>$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}<em t="t">{\left(s</em>\right]
$$}, a_{t}, r_{t}, s_{t+1}\right) \sim \mathcal{D}}\left[\left(y_{i}-Q\left(s_{t}, a_{t} ; \theta_{i}\right)\right)^{2</p>
<p>where $y_{i}=r_{t}+\gamma \max <em t_1="t+1">{a^{\prime}} \hat{Q}\left(s</em>$. Updates performed in this manner have been empirically shown to be tractable and stable (Mnih et al. 2015).}, a^{\prime} ; \theta^{-}\right)$is the stale update target given by the target network $\hat{Q</p>
<h2>Partial Observability</h2>
<p>In real world environments it's rare that the full state of the system can be provided to the agent or even determined. In other words, the Markov property rarely holds in real world environments. A Partially Observable Markov Decision Process (POMDP) better captures the dynamics of many realworld environments by explicitly acknowledging that the sensations received by the agent are only partial glimpses of the underlying system state. Formally a POMDP can be described as a 6-tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \Omega, \mathcal{O}) . \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}$ are the states, actions, transitions, and rewards as before, except now the agent is no longer privy to the true system state and instead receives an observation $o \in \Omega$. This observation is generated from the underlying system state according to the probability distribution $o \sim \mathcal{O}(s)$. Vanilla Deep Q-Learning has no explicit mechanisms for deciphering the underlying state of the POMDP and is only effective if the observations are reflective of underlying system states. In the general case, estimating a Q-value from an observation can be arbitrarily bad since $Q(o, a \mid \theta) \neq Q(s, a \mid \theta)$.</p>
<p>Our experiments show that adding recurrency to Deep QLearning allows the Q-network network to better estimate the underlying system state, narrowing the gap between $Q(o, a \mid \theta)$ and $Q(s, a \mid \theta)$. Stated differently, recurrent deep Q-networks can better approximate actual Q-values from sequences of observations, leading to better policies in partially observed environments.</p>
<h2>DRQN Architecture</h2>
<p>To isolate the effects of recurrency, we minimally modify the architecture of DQN, replacing only its first fully connected layer with a recurrent LSTM layer of the same size. Depicted in Figure 2, the architecture of DRQN takes a single $84 \times 84$ preprocessed image. This image is processed by three convolutional layers (Cun et al. 1998) and the outputs are fed to the fully connected LSTM layer (Hochreiter and Schmidhuber 1997). Finally, a linear layer outputs a QValue for each action. During training, the parameters for both the convolutional and recurrent portions of the network are learned jointly from scratch. We settled on this architecture after experimenting with several variations; see Appendix A for details.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: DRQN convolves three times over a single-channel image of the game screen. The resulting activations are processed through time by an LSTM layer. The last two timesteps are shown here. LSTM outputs become Q-Values after passing through a fully-connected layer. Convolutional filters are depicted by rectangular sub-boxes with pointed tops.</p>
<h2>Stable Recurrent Updates</h2>
<p>Updating a recurrent, convolutional network requires each backward pass to contain many time-steps of game screens and target values. Additionally, the LSTM's initial hidden state may either be zeroed or carried forward from its previous values. We consider two types of updates:</p>
<p><strong>Bootstrapped Sequential Updates</strong>: Episodes are selected randomly from the replay memory and updates begin at the beginning of the episode and proceed forward through time to the conclusion of the episode. The targets at each timestep are generated from the target Q-network, <em>Q̂</em>. The RNN's hidden state is carried forward throughout the episode.</p>
<p><strong>Bootstrapped Random Updates</strong>: Episodes are selected randomly from the replay memory and updates begin at random points in the episode and proceed for only <em>unroll iterations</em> timesteps (e.g., one backward call). The targets at each timestep are generated from the target Q-network, <em>Q̂</em>. The RNN's initial state is zeroed at the start of the update.</p>
<p>Sequential updates have the advantage of carrying the LSTM's hidden state forward from the beginning of the episode. However, by sampling experiences sequentially for a full episode, they violate DQN's random sampling policy.</p>
<p>Random updates better adhere to the policy of randomly sampling experience, but, as a consequence, the LSTM's hidden state must be zeroed at the start of each update. Zeroing the hidden state makes it harder for the LSTM to learn functions that span longer time scales than the number of timesteps reached by back propagation through time.</p>
<p>Experiments indicate that both types of updates are viable and yield convergent policies with similar performance across a set of games. Therefore, to limit complexity, all results herein use the randomized update strategy. We expect that all presented results would generalize to the case of sequential updates.</p>
<p>Having addressed the architecture and updating of a Deep Recurrent Q-Network, we now show how it performs on domains featuring partial observability.</p>
<h2>Atari Games: MDP or POMDP?</h2>
<p>The state of an Atari 2600 game is fully described by the 128 bytes of console RAM. Humans and agents, however, observe only the console-generated game screens. For many games, a single game screen is insufficient to determine the state of the system. DQN infers the full state of an Atari game by expanding the state representation to encompass the last four game screens. Many games that were previously POMDPs now become MDPs. Of the 49 games investigated by (Mnih et al. 2015), the authors were unable to identify any that were partially observable given the last four frames of input. Since the explored games are fully observable given four input frames, we need a way to introduce partial observability without reducing the number of input frames given to DQN.</p>
<h2>Flickering Atari Games</h2>
<p>To address this problem, we introduce the <em>Flickering Pong</em> POMDP - a modification to the classic game of Pong such that at each timestep, the screen is either fully revealed or fully obscured with probability <em>p</em> = 0.5. Obscuring frames in this manner probabilistically induces an incomplete memory of observations needed for Pong to become a POMDP.</p>
<p>In order to succeed at the game of Flickering Pong, it is necessary to integrate information across frames to estimate relevant variables such as the location and velocity of the ball and the location of the paddle. Since half of the frames are obscured in expectation, a successful player must be robust to the possibility of several potentially contiguous obscured inputs.</p>
<p>Perhaps the most important opportunity presented by a history of game screens is the ability to convolutionally detect object velocity. Figure 3 visualizes the game screens maximizing the activations of different convolutional filters and confirms that the 10-frame DQN's filters do detect object velocity, though perhaps less reliably than normal unobscured Pong.</p>
<p><sup>1</sup>Some Atari games are undoubtedly POMDPs such as Blackjack in which the dealer's cards are hidden from view. Unfortunately, Blackjack is not supported by the ALE emulator.</p>
<p><sup>2</sup>(Guo et al. 2014) also confirms that convolutional filters learn to respond to patterns of movement seen in game objects.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Sample convolution filters learned by 10-frame DQN on the game of Pong. Each row plots the input frames that trigger maximal activation of a particular convolutional filter in the specified layer. The red bounding box illustrates the portion of the input image that caused the maximal activation. Most filters in the first convolutional layer detect only the paddle. Conv2 filters begin to detect ball movement in particular directions and some jointly track the ball and the paddle. Nearly all Conv3 filters track ball and paddle interactions including deflections, ball velocity, and direction of travel. Despite seeing a single frame at a time, individual LSTM units also detect high level events, respectively: the agent missing the ball, ball reflections off of paddles, and ball reflections off the walls. Each image superimposes the last 10-frames seen by the agent, giving more luminance to the more recent frames.</p>
<p>Remarkably, DRQN performs well at this task even when given only one input frame per timestep. With a single frame it is impossible for DRQN's convolutional layers to detect any type of velocity. Instead, the higher-level recurrent layer must compensate for both the flickering game screen and the lack of convolutional velocity detection. Figure 3d confirms that individual units in the LSTM layer are capable of integrating noisy single-frame information through time to detect high-level Pong events such as the player missing the ball, the ball reflecting on a paddle, or the ball reflecting off the wall.</p>
<p>DRQN is trained using backpropagation through time for the last ten timesteps. Thus both the non-recurrent 10-frame DQN and the recurrent 1-frame DRQN have access to the same history of game screens. Thus, when dealing with partial observability, a choice exists between using a nonrecurrent deep network with a long history of observations or using a recurrent network trained with a single observation at each timestep. The results in this section show that recurrent networks can integrate information through time and serve as a viable alternative to stacking frames in the input layer of a convolutional network.</p>
<h3>Evaluation on Standard Atari Games</h3>
<p>We selected the following nine Atari games for evaluation: <em>Asteroids</em> and <em>Double Dunk</em> feature naturally-flickering sprites making them good potential candidates for recurrent learning. <em>Beam Rider</em>, <em>Centipede</em>, and <em>Chopper Command</em> are shooters. <em>Frostbite</em> is a platformer similar to Frogger. <em>Ice Hockey</em> and <em>Double Dunk</em> are sports games that require positioning players, passing and shooting the puck/ball, and require the player to be capable of both offense and defense. <em>Bowling</em> requires actions to be taken at a specific time in order to guide the ball. <em>Ms Pacman</em> features flickering ghosts and power pills.</p>
<p>Given the last four frames of input, all of these games are MDPs rather than POMDPs. Thus there is no reason to expect DRQN to outperform DQN. Indeed, results in Table 1 indicate that on average, DRQN does roughly as well DQN. Specifically, our re-implementation of DQN performs similarly to the original, outperforming the original on five out of the nine games, but achieving less than half the original score on Centipede and Chopper Command. DRQN performs outperforms our DQN on the games of Frostbite and Double Dunk, but does significantly worse on the game of Beam Rider (Figure 4). The game of Frostbite (Figure 1b) requires the player to jump across all four rows of moving icebergs and return to the top of the screen. After traversing the icebergs several times, enough ice has been collected to build an igloo at the top right of the screen. Subsequently the player can enter the igloo to advance to the next level. As shown in Figure 4, after 12,000 episodes DRQN discovers a policy that allows it to reliably advance past the first level of Frostbite. For experimental details, see Appendix B.</p>
<p>However, Karpathy, Johnson, and Li 2015 show that LSTMs can learn functions at training time over a limited set of timesteps and then generalize them at test time to longer sequences. Statistical significance of scores determined by independent t-</p>
<p>| | DRQN $\pm std$ | DQN $\pm std$ | |
| Game | | Ours | Mnih et al. |
| --- | --- | --- | --- |
| Asteroids | 1020 $(\pm 312)$ | 1070 $(\pm 345)$ | 1629 $(\pm 542)$ |
| Beam Rider | 3269 $(\pm 1167)$ | 6923 $(\pm 1027)$ | 6846 $(\pm 1619)$ |
| Bowling | 62 $(\pm 5.9)$ | 72 $(\pm 11)$ | 42 $(\pm 88)$ |
| Centipede | 3534 $(\pm 1601)$ | 3653 $(\pm 1903)$ | 8309 $(\pm 5237)$ |
| Chopper Cmd | 2070 $(\pm 875)$ | 1460 $(\pm 976)$ | 6687 $(\pm 2916)$ |
| Double Dunk | -2 $(\pm 7.8)$ | -10 $(\pm 3.5)$ | -18.1 $(\pm 2.6)$ |
| Frostbite | 2875 $(\pm 535)$ | 519 $(\pm 363)$ | 328.3 $(\pm 250.5)$ |
| Ice Hockey | -4.4 $(\pm 1.6)$ | -3.5 $(\pm 3.5)$ | -1.6 $(\pm 2.5)$ |
| Ms. Pacman | 2048 $(\pm 653)$ | 2363 $(\pm 735)$ | 2311 $(\pm 525)$ |</p>
<p>Table 1: On standard Atari games, DRQN performance parallels DQN, excelling in the games of Frostbite and Double Dunk, but struggling on Beam Rider. Bolded font indicates statistical significance between DRQN and our DQN.</p>
<p>Table 1: When trained on normal games (MDPs) and then evaluated on flickering games (POMDPs), DRQN’s performance degrades more gracefully than DQN’s. Each data point shows the average percentage of the original game score over all 9 games in Table 1.</p>
<h2>MDP to POMDP Generalization</h2>
<p>Can a recurrent network be trained on a standard MDP and then generalize to a POMDP at evaluation time? To address this question, we evaluate the highest-scoring policies of DRQN and DQN over the flickering equivalents of all 9 games in Table 1. Figure 5 shows that while both algorithms incur significant performance decreases on account of the missing information, DRQN captures more of its previous performance than DQN across all levels of flickering. We conclude that recurrent controllers have a certain degree of robustness against missing information, even trained with full state information.</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>[table]table</p>
<p>POMDPs by combining a Long Short Term Memory with a Deep Q-Network. The resulting Deep Recurrent Q-Network (DRQN), despite seeing only a single frame at each step, is still capable integrating information across frames to detect relevant information such as velocity of on-screen objects. Additionally, on the game of Pong, DRQN is better equipped than a standard Deep Q-Network to handle the type of partial observability induced by flickering game screens.</p>
<p>Furthermore, when trained with partial observations, DRQN can generalize its policies to the case of complete observations. On the Flickering Pong domain, performance scales with the observability of the domain, reaching nearperfect levels when every game screen is observed. This result indicates that the recurrent network learns policies that are both robust enough to handle to missing game screens and scalable enough to improve performance as observability increases. Generalization also occurs in the opposite direction: when trained on standard Atari games and evaluated against flickering games, DRQN's performance generalizes better than DQN's at all levels of partial information.</p>
<p>Our experiments suggest that Pong represents an outlier among the examined games. Across a set of ten Flickering MDPs we observe no systematic improvement when employing recurrency. Similarly, across non-flickering Atari games, there are few significant differences between the recurrent and non-recurrent player. This observation leads us to conclude that while recurrency is a viable method for handling state observations, it confers no systematic benefit compared to stacking the observations in the input layer of a convolutional network. One avenue for future is identifying the relevant characteristics of the Pong and Frostbite that lead to better performance by recurrent networks.</p>
<h2>Acknowledgments</h2>
<p>This work has taken place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (CNS-1330072, CNS-1305287), ONR (21C184-01), AFRL (FA8750-14-1-0070), AFOSR (FA9550-14-1-0087), and Yujin Robot. Additional support from the Texas Advanced Computing Center, and Nvidia Corporation.</p>
<h2>References</h2>
<p>Bakker, B. 2001. Reinforcement learning with long shortterm memory. In NIPS, 1475-1482. MIT Press.
Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research 47:253-279.
Cun, Y. L. L.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998. Gradient-based learning applied to document recognition. Proceedings of IEEE 86(11):2278-2324.
Guo, X.; Singh, S.; Lee, H.; Lewis, R. L.; and Wang, X. 2014. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Ghahramani, Z.; Welling, M.; Cortes, C.; Lawrence, N.; and Weinberger, K.,
eds., Advances in Neural Information Processing Systems 27. Curran Associates, Inc. 3338-3346.</p>
<p>Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural Comput. 9(8):1735-1780.
Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.; Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.
Karpathy, A.; Johnson, J.; and Li, F.-F. 2015. Visualizing and understanding recurrent networks. arXiv preprint.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D. 2015. Human-level control through deep reinforcement learning. Nature 518(7540):529-533.
Narasimhan, K.; Kulkarni, T.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. CoRR abs/1506.08941.
Sutton, R. S., and Barto, A. G. 1998. Reinforcement Learning: An Introduction. MIT Press.
Tieleman, T., and Hinton, G. 2012. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.
Tsitsiklis, J. N., and Roy, B. V. 1997. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control 42(5):674-690.
Watkins, C. J. C. H., and Dayan, P. 1992. Q-learning. Machine Learning 8(3-4):279-292.
Wierstra, D.; Foerster, A.; Peters, J.; and Schmidthuber, J. 2007. Solving deep memory POMDPs with recurrent policy gradients.
Zeiler, M. D. 2012. ADADELTA: An adaptive learning rate method. CoRR abs/1212.5701.</p>
<table>
<thead>
<tr>
<th>Description</th>
<th>Percent Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM replaces IP1</td>
<td>709%</td>
</tr>
<tr>
<td>ReLU-LSTM replaces IP1</td>
<td>533%</td>
</tr>
<tr>
<td>LSTM over IP1</td>
<td>418%</td>
</tr>
<tr>
<td>ReLU-LSTM over IP1</td>
<td>0%</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Description</th>
<th></th>
<th>Percent Improvement</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1.</td>
<td></td>
<td>4</td>
<td>10</td>
</tr>
<tr>
<td>2.</td>
<td></td>
<td>13.6</td>
<td>26.7</td>
</tr>
<tr>
<td>3.</td>
<td></td>
<td>22.3</td>
<td>33.7</td>
</tr>
<tr>
<td>4.</td>
<td></td>
<td>111.3</td>
<td>180.5</td>
</tr>
<tr>
<td>5.</td>
<td></td>
<td>263.4</td>
<td>491.1</td>
</tr>
<tr>
<td>6.</td>
<td></td>
<td></td>
<td>2.5</td>
</tr>
</tbody>
</table>
<p><strong>Appendix A: Alternative Architectures</strong></p>
<p>Several alternative architectures were evaluated on the game of Beam Rider. We explored the possibility of either replacing the first non-convolutional fully connected layer with an LSTM layer (LSTM replaces IP1) or adding the LSTM layer between the first and second fully connected layers (LSTM over IP1). Results strongly indicated LSTM should replace IP1. We hypothesize this allows LSTM direct access to the convolutional features. Additionally, adding a Rectifier layer after the LSTM layer consistently reduced performance.</p>
<p>Another possible architecture combines frame stacking from DQN with the recurrency of LSTM. This architecture accepts a stack of the four latest frames at every timestep. The LSTM portion of the architecture remains the same and is unrolled over the last 10 timesteps. In theory, this modification should allow velocity detection to happen in the convolutional layers of the network, leaving the LSTM free to perform higher-order processing. This architecture has the largest number of parameters and requires the most training time. Unfortunately, results show that the additional parameters do not lead to increased performance on the set of games examined. It is possible that the network has too many parameters and is prone to overfitting the training experiences it has seen.</p>
<p><strong>Appendix B: Computational Efficiency</strong></p>
<p>Computational efficiency of RNNs is an important concern. We conducted experiments by performing 1000 backwards and forwards passes and reporting the average time in milliseconds required for each pass. Experiments used a single Nvidia GTX Titan Black using CuDNN and a fully optimized version of Caffe. Results indicate that computation scales sub-linearly in both the number of frames stacked in the input layer and the number of iterations unrolled. Even so, models trained on a large number of stacked frames and unrolled for many iterations are often computationally intractable. For example a model unrolled for 30 iterations with 10 stacked frames would require over 56 days to reach 10 million iterations.</p>
<p><strong>Appendix C: Experimental Details</strong></p>
<p>Policies were evaluated every 50,000 iterations by playing 10 episodes and averaging the resulting scores. Networks were trained for 10 million iterations and used a replay memory of size 400,000. Additionally, all networks used ADADELTA (Zeiler 2012) optimizer with a learning rate of 0.1 and momentum of 0.95. LSTM's gradients were clipped to a value of ten to ensure learning stability. All other settings were identical to those given in (Mnih et al. 2015).</p>
<p>All networks were trained using the Arcade Learning Enviroment ALE (Bellemare et al. 2013). The following ALE options were used: color averaging, minimal action set, and death detection.</p>
<p>DRQN is implemented in Caffe (Jia et al. 2014). The source is available at [removed for blind review].</p>
<p><strong>Appendix C: Flickering Results</strong></p>
<table>
<thead>
<tr>
<th>Flickering</th>
<th>DRQN ±std</th>
<th>DQN ±std</th>
</tr>
</thead>
<tbody>
<tr>
<td>Asteroids</td>
<td>1032 (±410)</td>
<td>1010 (±535)</td>
</tr>
<tr>
<td>Beam Rider</td>
<td>618 (±115)</td>
<td>1685.6 (±875)</td>
</tr>
<tr>
<td>Bowling</td>
<td>65.5 (±13)</td>
<td>57.3 (±8)</td>
</tr>
<tr>
<td>Centipede</td>
<td>4319.2 (±4378)</td>
<td>5268.1 (±2052)</td>
</tr>
<tr>
<td>Chopper Cmd</td>
<td>1330 (±294)</td>
<td>1450 (±787.8)</td>
</tr>
<tr>
<td>Double Dunk</td>
<td>-14 (±2.5)</td>
<td>-16.2 (±2.6)</td>
</tr>
<tr>
<td>Frostbite</td>
<td>414 (±494)</td>
<td>436 (±462.5)</td>
</tr>
<tr>
<td>Ice Hockey</td>
<td>-5.4 (±2.7)</td>
<td>-4.2 (±1.5)</td>
</tr>
<tr>
<td>Ms. Pacman</td>
<td>1739 (±942)</td>
<td>1824 (±490)</td>
</tr>
<tr>
<td>Pong</td>
<td>12.1 (±2.2)</td>
<td>-9.9 (±3.3)</td>
</tr>
</tbody>
</table>
<p>Table 3: Each screen is obscured with probability 0.5, resulting in a partially-observable, flickering equivalent of the standard game. Bold font indicates statistical significance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Copyright (c) 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
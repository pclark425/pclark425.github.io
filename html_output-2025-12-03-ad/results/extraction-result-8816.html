<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8816 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8816</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8816</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-c39eeb0669c727ac606ec7fcb9f0136794739672</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c39eeb0669c727ac606ec7fcb9f0136794739672" target="_blank">NABU - Multilingual Graph-based Neural RDF Verbalizer</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on the Semantic Web</p>
                <p><strong>Paper TL;DR:</strong> This work presents NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English that outperforms state-of-the-art approaches on English and achieves consistent results across all languages on the multilingual scenario.</p>
                <p><strong>Paper Abstract:</strong> The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8816.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8816.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NABU (reified GAT-Trans)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NABU: Multilingual Graph-based Neural RDF Verbalizer (GAT encoder + Transformer decoder with reification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based end-to-end RDF-to-text system that (1) reifies RDF triples (mapping predicates to nodes with A0/A1 binary relations), (2) encodes the resulting graph with a Graph Attention Network (GAT)-inspired encoder, and (3) decodes text with a Transformer decoder using subword tokenization and an optional copy mechanism; designed for multilingual generation (EN/DE/RU).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reified graph encoding (predicate-to-node) + GAT encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>RDF triples are transformed by reification: each triple <subject, predicate, object> is replaced by two binary relations/edges by introducing the predicate as a node and two relation labels A0 (subject→predicate) and A1 (predicate→object). The reified graph thus contains subject/object nodes, predicate nodes, and labeled A0/A1 edges; node features (node embeddings, source/destination embeddings, label embeddings) are combined and passed through a multi-head Graph Attention Network encoder to obtain contextual node representations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples (DBpedia-derived KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Reification: transform each triple into two edges by creating predicate-nodes and A0/A1 relations; build input vectors comprising node embeddings H, source S, destination D, and label L; compute an edge vector E = f(S,D) and feed H+L+E into a multi-head GAT encoder; decode with a Transformer decoder (with BPE/unigram subword tokenization and beam search); copy mechanism applied for OOV/unseen entities.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Data-to-text generation / RDF-to-text (WebNLG benchmark), multilingual verbalization (English, German, Russian)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Monolingual EN (NABU_GAT-Trans): BLEU 66.21, METEOR 41.47, chrF++ 71.98. Monolingual DE: BLEU 53.08, METEOR 37.42, chrF++ 64.57. Monolingual RU: BLEU 46.86, METEOR 28.84, chrF++ 58.37. Bilingual EN-DE: BLEU 61.99, METEOR 39.51, chrF++ 69.68. Bilingual EN-RU: BLEU 49.15, METEOR 33.41, chrF++ 64.00. Multilingual EN-DE-RU: BLEU 56.04, METEOR 38.34, chrF++ 62.04. Training time: monolingual ~6h (Tesla P100), multilingual ~8h.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms previous state-of-the-art English approaches (e.g., prior transformer-based models) on WebNLG (reported +~15 BLEU compared to some baselines; specifically NABU 66.21 BLEU vs prior Transformer in [17] lower — paper reports 28.15% relative increase in one comparison). NABU also outperforms the paper's Transformer baseline (linearized input) across most multilingual/monolingual settings; however, Transformer-baseline performed better on discourse-ordering in some cases due to explicit linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Encodes graph structure explicitly (reified predicates become nodes so encoder has relation-level hidden states); alleviates parameter explosion in attention over labeled edges; enables multilingual generation from language-agnostic KG inputs; strong empirical gains on English and consistent multilingual performance; supports copy mechanism for unseen entities; uses subword tokenization to handle OOVs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Reification interferes with discourse ordering (linearization can better represent verbalization order); some predicate subject-assignment errors when multiple identical predicates exist (subject–predicate alignment failures if encoder cannot disambiguate identical predicate nodes); parameter/architecture choices still sensitive to language-family differences (EN-RU bilingual performance degraded vs EN-DE).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>1) Discourse ordering errors: reification can cause incorrect assignment of subject/object when multiple identical predicates are present (example: swapping birthPlaces between subjects). 2) Similar predicates (e.g., dbo:artist vs dbo:producer) have similar embeddings and can be verbalized interchangeably, producing wrong predicate lexicalizations. 3) Typographical / inflection issues in target languages (German possessive/prepositional errors). 4) Unseen-entity verbalization issues in languages with different scripts (Russian Cyrillic): copy mechanism sometimes produced English-form entity strings rather than target-language verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8816.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reification (RDF triple → predicate node)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reification strategy for RDF graph encoding (predicate-as-node with A0/A1 relations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph representation technique that converts each triple into two binary relations by turning the predicate into a node and linking subject→predicate (A0) and predicate→object (A1), enabling node-centric graph encoders to produce embeddings for predicates and model arbitrary numbers of predicates efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep graph convolutional encoders for structured data to text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Reification (predicate-to-node) representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transforms each RDF triple <s,p,o> into two triples <s, A0, p> and <p, A1, o>, where p becomes a node; label embeddings (A0/A1) and node embeddings are combined to form inputs for graph neural encoders (here, GAT). Advantages include producing explicit predicate node representations and controlling parameter explosion from edge-typed attention.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph transformation prior to encoding: introduce predicate nodes and two binary labeled relations per original triple; construct node-level feature vectors (node embeddings H), source/destination embeddings (S, D), and label embeddings (L); combine into edge vectors E = f(S,D) and form H + L + E as encoder input.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation (WebNLG), multilingual NLG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within NABU results above (contributed to NABU's BLEU/METEOR/chrF++ scores listed in NABU entry). No isolated metric reported solely for reification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper states reification alleviates the parameter explosion problem for attention-over-edge-labels compared to approaches that encode edge labels as parameters directly; compared qualitatively to linearization where linearization better captures explicit discourse ordering but loses some graph-structure advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Yields predicate-level hidden states; models arbitrary number of edge types efficiently; reduces number of edge-typed parameters in attention-based graph encoders; integrates naturally with node-focused GNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Alters surface representation of original triple ordering, making discourse ordering less explicit; can cause subject–predicate assignment ambiguity when multiple identical predicates occur for different subjects; may require additional features/structuring steps to recover verbalization order.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples where two triples share the same predicate for different subjects led to swapped verbalizations (e.g., birthPlace for two subjects verbalized with swapped countries). Also contributes to discourse-ordering weakness compared to linearized input approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8816.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization (sequence-of-triples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized triple serialization (sequence input used in WebNLG participants and Transformer baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversion method that serializes a set of RDF triples into a linear sequence (text-like sequence) which is then consumed by sequence encoders (RNNs or Transformers) to generate target text; used broadly in prior WebNLG submissions and as the Transformer baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The webnlg challenge: Generating text from dbpedia data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearization / triple serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples are concatenated into a linear token sequence (e.g., subject predicate object tokens in a specific order) and fed into a sequence encoder/decoder architecture; optionally augmented by an explicit special token indicating target language. Linearization can carry explicit discourse order since the input order maps to output order.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples (sets of triples up to 7 triples as in WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize triples into a single input sequence (e.g., S1 P1 O1 ; S2 P2 O2 ; ...), add language token for multilingual models, apply subword tokenization (BPE/unigram) and feed into Transformer encoder-decoder model; beam search used for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text / data-to-text generation (WebNLG), used as baseline for multilingual and monolingual experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Transformer baseline results (linearized input): Monolingual ENG BLEU 54.96, METEOR 38.43, chrF++ 69.11; Monolingual GER BLEU 50.07; Monolingual RUS BLEU 46.42; Bilingual ENG-GER BLEU 58.30; Bilingual ENG-RUS BLEU 55.30; Multilingual ENG-GER-RUS BLEU 53.39 (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly in experiments: NABU (graph/GAT + reification) outperforms the linearized Transformer baseline in most metrics and settings (notably English monolingual), but Transformer-baseline performed better on discourse ordering (ordering triples) due to the explicit order in the serialized input.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement and compatible with standard seq2seq models; preserves a verbalization order that helps discourse ordering; can be competitive when combined with strong tokenization (BPE) and copying mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Loses explicit graph structure (adjacency and global graph context) compared to graph-based encoders; may fail to exploit structure-dependent features; can be suboptimal for capturing multi-hop relations or global node contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Graph-structure-sensitive phenomena (e.g., fluency that depends on neighborhood context) may be less well captured; previous work [31] argued linearization has drawbacks relative to graph encoders for structured input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8816.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network encoder (multi-head GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph neural network encoder that computes node representations with attention-weighted aggregation of neighbor features; used in NABU to encode reified RDF graphs so each node attends dynamically to its neighbors with learned attention coefficients.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph attention networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Attention Network (GAT) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each node's representation is updated by applying attention scores over its neighbors: attention coefficients e_ij = a(h_i, h_j) are computed and normalized via softmax to produce α_ij; node representations are aggregated as weighted sums and passed through feed-forward/normalization layers; multi-head attention yields multiple representation subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / reified RDF graphs (nodes include subjects, objects, predicate-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply multi-head self-attention at node level on the reified graph where node inputs are H + L + E vectors; compute attention coefficients among neighbors and aggregate to produce context-aware node embeddings that are passed to the Transformer decoder via cross-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation (WebNLG), multilingual verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of NABU's overall model metrics (see NABU entry) — the paper attributes performance gains over transformer-baseline to the GAT-based encoder combined with reification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared implicitly against other graph encoders (e.g., GCN in [31]) and sequence linearization; GAT is argued to better weight neighbor importance dynamically versus GCN's fixed normalization, and when combined with reification it reduces parameter explosion versus labeling edges directly.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Dynamic attention lets the encoder emphasize more informative neighbors for each node; produces richer local and contextual node representations; integrates naturally with attention-based decoders (Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potential parameter explosion if edge labels are encoded as parameters (mitigated here via reification); computational cost can be higher for large graphs; requires careful design to avoid ambiguity in subject–predicate alignments after reification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When predicates/relations share very similar embeddings or domains/ranges, attention may not disambiguate properly leading to wrong predicate lexicalizations (e.g., artist vs producer). Also, identical predicate nodes for different triples can cause subject assignment errors after reification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8816.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCN encoder (Marcheggiani & Perez)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Convolutional Network encoder for structured data-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GCN-based encoder that directly exploits graph structure (nodes and labeled edges) to compute node-level representations for downstream text generation; reported previously to improve over sequential encoders for RDF-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep graph convolutional encoders for structured data to text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Convolutional Network (GCN) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Uses graph convolutional layers where each node updates its embedding by aggregating (normalized) transformed neighbor features via fixed normalization constants (1/c_ij), followed by non-linearities (e.g., ReLU); supports end-to-end graph-to-text when combined with decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Feed node feature matrix into stacked GCN layers (multiplying by weight matrices and summing normalized neighbor contributions) to produce node embeddings; pass embeddings to decoder (e.g., GRU or Transformer) for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation (WebNLG / structured data-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior work (Marcheggiani & Perez) reported improvements over LSTM baselines on WebNLG; in this paper GCN is referenced as prior art but no new numeric results are provided for GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper cites that GCN outperformed LSTM/GRU sequential models and that graph-based approaches (like GCN) address some linearization drawbacks; GAT is presented as an improvement over GCN by allowing dynamic attention weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly exploits graph connectivity, preserves structural inductive bias, and can outperform sequence encoders on structured inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Convolution operation uses fixed normalization that may not capture differential importance of neighbors (addressed by GAT); may require careful feature/edge handling for labeled edges.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Fixed neighbor normalization can underweight important neighbors and overemphasize others; handling labeled relations can lead to parameter growth if not addressed (e.g., via reification).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8816.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTR-LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GTR-LSTM: Triple encoder capturing intra- and inter-triple relationships</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that encodes RDF KGs by capturing both relationships within a triple and relations between triples, using a specialized triple-focused encoder architecture (GTR-LSTM) feeding an LSTM decoder for sentence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gtr-lstm: A triple encoder for sentence generation from rdf data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple-based encoder (GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes triples with mechanisms designed to capture local triple structure and global inter-triple dependencies (distinct encoding for each triple and aggregation across triples), producing representations used by a sequential decoder (LSTM) for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode each triple with dedicated encoders that model subject-predicate-object relationships and interactions across triples to capture global KG context; decode with LSTM into text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation / sentence generation from RDF data</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as prior work in related work; no direct metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Acknowledged among graph-aware approaches that attempt to capture triple-local and cross-triple information; compared thematically to GCN/GAT works in literature review rather than via direct experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Designed to model both local triple semantics and global KG structure; can better capture cross-triple dependencies than naive serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Uses LSTM decoders which may be less parallelizable than Transformer decoders; potentially more complex to scale to many triples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not discussed in detail here; general limitations relate to modeling long-range dependencies and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8816.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated Graph Neural Networks / Graph-to-sequence (Beck et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence approach using gated graph neural networks (GGNN) to encode graph structure for sequence generation tasks; referenced as related prior work demonstrating graph encoders for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Gated Graph Neural Network (GGNN) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>GGNNs propagate and update node hidden states using gated recurrent units across graph edges, enabling multi-step message passing that captures global graph context before feeding node/graph representation to sequence decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs / knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply recurrent gated updates across graph edges for several propagation steps to obtain context-rich node embeddings; aggregate node representations for decoder input in a sequence generation model.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-sequence / RDF-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced as prior art; no numerical results provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Listed among graph-based approaches; contrasted with GAT (attention-based aggregation) and GCN (convolutional aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows flexible, multi-step message passing and gated updates, capturing complex dependencies in graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potential parameter and time cost for multiple propagation steps; may suffer parameter explosion for labeled/typed edges unless mitigated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Parameter explosion when encoding many relation types as separate parameters; cited as a problem motivating reification in NABU.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8816.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Copy / pointer mechanism for seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism used in sequence-to-sequence generation that first attempts to copy source tokens with high attention weights to handle out-of-vocabulary (OOV) or unseen entities and otherwise falls back to generating vocabulary tokens; used in NABU to handle unseen RDF entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Incorporating copying mechanism in sequence-to-sequence learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Copying / Pointer mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At decoding time, if a target token is OOV, the decoder identifies the source token with highest attention weight and substitutes that token; if no appropriate substitution is found, the source token is copied directly into the output.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies when input is serialized or encoded (RDF triples / graph tokens); relevant to RDF-to-text where entity surface forms are needed</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Integrate copy/pointer scoring into decoder: combine generation probability over target vocabulary with copy probability from source tokens weighted by decoder attention; select via beam search/decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>RDF-to-text generation (handling unseen entities), multilingual generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used within NABU to mitigate OOVs and unseen-entity issues; no isolated metrics presented, but paper reports some failures remain (e.g., copying English entity form into Russian outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper uses copy mechanism in both Transformer baseline and NABU as part of experiments; copy helps but does not fully resolve unseen-entity generation in certain languages/scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Helps handle rare/unseen entity strings without requiring them in vocabulary; improves surface realization of entity names in generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May copy source-surface forms into the wrong target language (e.g., copying English form into Russian output); does not ensure proper inflection/lemmatization for target language; limited when script differs (Cyrillic vs Latin).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples where Russian outputs included English-form entities (e.g., 'Visvesvaraya Technical University' in English instead of Cyrillic/Russian verbalization) due to copy behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8816.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8816.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language token / language node</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Special language token (for seq2seq) / language node (for graph encoders) for multilingual control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that marks the target language by adding a special token at the beginning of the encoder input sequence (or adding a language node to the input graph), enabling a single multilingual model to condition generation on the desired output language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multilingual neural machine translation with knowledge distillation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Language token / language-node conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For sequence models: prepend a special token indicating the target language to the serialized triple sequence. For graph models: add a special language node to the graph input so the graph encoder can propagate a language signal. Both approaches condition the decoder to produce text in the indicated language.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Serialized triple sequences and reified graphs (multilingual RDF-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Add a language-designating element to the encoder input (special token or graph node); shuffle and mix multilingual training examples; the decoder learns to attend to the language signal to produce the correct target language.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Multilingual RDF-to-text generation, bilingual/multilingual modeling (EN/DE/RU in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multilingual NABU (with language node) BLEU 56.04, METEOR 38.34, chrF++ 62.04; bilingual/monolingual results reported in NABU entry show language conditioning success particularly for EN-DE.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Technique aligns with multilingual NMT literature; paper notes that adding a language token/node is standard practice ([46]) and was used for both Transformer baseline and GAT models; contributed to successful multilingual training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, lightweight conditioning signal enabling one model to generate multiple languages; leverages shared KG input across languages; supports zero-shot/transfer phenomena observed in multilingual NMT literature.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May not be sufficient alone when languages have very different vocabularies/scripts (observed weaker EN-RU bilingual performance); requires shared subword vocabulary or careful tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>EN-RU bilingual model showed inconsistent scores and inferior BLEU/METEOR compared to baseline, attributed to large lexical/script differences despite the presence of language tokens/nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NABU - Multilingual Graph-based Neural RDF Verbalizer', 'publication_date_yy_mm': '2020-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Gtr-lstm: A triple encoder for sentence generation from rdf data <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from dbpedia data <em>(Rating: 2)</em></li>
                <li>Incorporating copying mechanism in sequence-to-sequence learning <em>(Rating: 1)</em></li>
                <li>Multilingual neural machine translation with knowledge distillation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8816",
    "paper_id": "paper-c39eeb0669c727ac606ec7fcb9f0136794739672",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "NABU (reified GAT-Trans)",
            "name_full": "NABU: Multilingual Graph-based Neural RDF Verbalizer (GAT encoder + Transformer decoder with reification)",
            "brief_description": "A graph-based end-to-end RDF-to-text system that (1) reifies RDF triples (mapping predicates to nodes with A0/A1 binary relations), (2) encodes the resulting graph with a Graph Attention Network (GAT)-inspired encoder, and (3) decodes text with a Transformer decoder using subword tokenization and an optional copy mechanism; designed for multilingual generation (EN/DE/RU).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Reified graph encoding (predicate-to-node) + GAT encoding",
            "representation_description": "RDF triples are transformed by reification: each triple &lt;subject, predicate, object&gt; is replaced by two binary relations/edges by introducing the predicate as a node and two relation labels A0 (subject→predicate) and A1 (predicate→object). The reified graph thus contains subject/object nodes, predicate nodes, and labeled A0/A1 edges; node features (node embeddings, source/destination embeddings, label embeddings) are combined and passed through a multi-head Graph Attention Network encoder to obtain contextual node representations.",
            "graph_type": "Knowledge graphs / RDF triples (DBpedia-derived KGs)",
            "conversion_method": "Reification: transform each triple into two edges by creating predicate-nodes and A0/A1 relations; build input vectors comprising node embeddings H, source S, destination D, and label L; compute an edge vector E = f(S,D) and feed H+L+E into a multi-head GAT encoder; decode with a Transformer decoder (with BPE/unigram subword tokenization and beam search); copy mechanism applied for OOV/unseen entities.",
            "downstream_task": "Data-to-text generation / RDF-to-text (WebNLG benchmark), multilingual verbalization (English, German, Russian)",
            "performance_metrics": "Monolingual EN (NABU_GAT-Trans): BLEU 66.21, METEOR 41.47, chrF++ 71.98. Monolingual DE: BLEU 53.08, METEOR 37.42, chrF++ 64.57. Monolingual RU: BLEU 46.86, METEOR 28.84, chrF++ 58.37. Bilingual EN-DE: BLEU 61.99, METEOR 39.51, chrF++ 69.68. Bilingual EN-RU: BLEU 49.15, METEOR 33.41, chrF++ 64.00. Multilingual EN-DE-RU: BLEU 56.04, METEOR 38.34, chrF++ 62.04. Training time: monolingual ~6h (Tesla P100), multilingual ~8h.",
            "comparison_to_others": "Outperforms previous state-of-the-art English approaches (e.g., prior transformer-based models) on WebNLG (reported +~15 BLEU compared to some baselines; specifically NABU 66.21 BLEU vs prior Transformer in [17] lower — paper reports 28.15% relative increase in one comparison). NABU also outperforms the paper's Transformer baseline (linearized input) across most multilingual/monolingual settings; however, Transformer-baseline performed better on discourse-ordering in some cases due to explicit linearization.",
            "advantages": "Encodes graph structure explicitly (reified predicates become nodes so encoder has relation-level hidden states); alleviates parameter explosion in attention over labeled edges; enables multilingual generation from language-agnostic KG inputs; strong empirical gains on English and consistent multilingual performance; supports copy mechanism for unseen entities; uses subword tokenization to handle OOVs.",
            "disadvantages": "Reification interferes with discourse ordering (linearization can better represent verbalization order); some predicate subject-assignment errors when multiple identical predicates exist (subject–predicate alignment failures if encoder cannot disambiguate identical predicate nodes); parameter/architecture choices still sensitive to language-family differences (EN-RU bilingual performance degraded vs EN-DE).",
            "failure_cases": "1) Discourse ordering errors: reification can cause incorrect assignment of subject/object when multiple identical predicates are present (example: swapping birthPlaces between subjects). 2) Similar predicates (e.g., dbo:artist vs dbo:producer) have similar embeddings and can be verbalized interchangeably, producing wrong predicate lexicalizations. 3) Typographical / inflection issues in target languages (German possessive/prepositional errors). 4) Unseen-entity verbalization issues in languages with different scripts (Russian Cyrillic): copy mechanism sometimes produced English-form entity strings rather than target-language verbalizations.",
            "uuid": "e8816.0",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Reification (RDF triple → predicate node)",
            "name_full": "Reification strategy for RDF graph encoding (predicate-as-node with A0/A1 relations)",
            "brief_description": "A graph representation technique that converts each triple into two binary relations by turning the predicate into a node and linking subject→predicate (A0) and predicate→object (A1), enabling node-centric graph encoders to produce embeddings for predicates and model arbitrary numbers of predicates efficiently.",
            "citation_title": "Deep graph convolutional encoders for structured data to text generation",
            "mention_or_use": "use",
            "representation_name": "Reification (predicate-to-node) representation",
            "representation_description": "Transforms each RDF triple &lt;s,p,o&gt; into two triples &lt;s, A0, p&gt; and &lt;p, A1, o&gt;, where p becomes a node; label embeddings (A0/A1) and node embeddings are combined to form inputs for graph neural encoders (here, GAT). Advantages include producing explicit predicate node representations and controlling parameter explosion from edge-typed attention.",
            "graph_type": "Knowledge graphs / RDF triples",
            "conversion_method": "Graph transformation prior to encoding: introduce predicate nodes and two binary labeled relations per original triple; construct node-level feature vectors (node embeddings H), source/destination embeddings (S, D), and label embeddings (L); combine into edge vectors E = f(S,D) and form H + L + E as encoder input.",
            "downstream_task": "RDF-to-text generation (WebNLG), multilingual NLG",
            "performance_metrics": "Used within NABU results above (contributed to NABU's BLEU/METEOR/chrF++ scores listed in NABU entry). No isolated metric reported solely for reification.",
            "comparison_to_others": "Paper states reification alleviates the parameter explosion problem for attention-over-edge-labels compared to approaches that encode edge labels as parameters directly; compared qualitatively to linearization where linearization better captures explicit discourse ordering but loses some graph-structure advantages.",
            "advantages": "Yields predicate-level hidden states; models arbitrary number of edge types efficiently; reduces number of edge-typed parameters in attention-based graph encoders; integrates naturally with node-focused GNNs.",
            "disadvantages": "Alters surface representation of original triple ordering, making discourse ordering less explicit; can cause subject–predicate assignment ambiguity when multiple identical predicates occur for different subjects; may require additional features/structuring steps to recover verbalization order.",
            "failure_cases": "Examples where two triples share the same predicate for different subjects led to swapped verbalizations (e.g., birthPlace for two subjects verbalized with swapped countries). Also contributes to discourse-ordering weakness compared to linearized input approaches.",
            "uuid": "e8816.1",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Linearization (sequence-of-triples)",
            "name_full": "Linearized triple serialization (sequence input used in WebNLG participants and Transformer baseline)",
            "brief_description": "A conversion method that serializes a set of RDF triples into a linear sequence (text-like sequence) which is then consumed by sequence encoders (RNNs or Transformers) to generate target text; used broadly in prior WebNLG submissions and as the Transformer baseline in this paper.",
            "citation_title": "The webnlg challenge: Generating text from dbpedia data",
            "mention_or_use": "use",
            "representation_name": "Linearization / triple serialization",
            "representation_description": "Triples are concatenated into a linear token sequence (e.g., subject predicate object tokens in a specific order) and fed into a sequence encoder/decoder architecture; optionally augmented by an explicit special token indicating target language. Linearization can carry explicit discourse order since the input order maps to output order.",
            "graph_type": "Knowledge graphs / RDF triples (sets of triples up to 7 triples as in WebNLG)",
            "conversion_method": "Serialize triples into a single input sequence (e.g., S1 P1 O1 ; S2 P2 O2 ; ...), add language token for multilingual models, apply subword tokenization (BPE/unigram) and feed into Transformer encoder-decoder model; beam search used for decoding.",
            "downstream_task": "RDF-to-text / data-to-text generation (WebNLG), used as baseline for multilingual and monolingual experiments",
            "performance_metrics": "Transformer baseline results (linearized input): Monolingual ENG BLEU 54.96, METEOR 38.43, chrF++ 69.11; Monolingual GER BLEU 50.07; Monolingual RUS BLEU 46.42; Bilingual ENG-GER BLEU 58.30; Bilingual ENG-RUS BLEU 55.30; Multilingual ENG-GER-RUS BLEU 53.39 (see paper tables).",
            "comparison_to_others": "Compared directly in experiments: NABU (graph/GAT + reification) outperforms the linearized Transformer baseline in most metrics and settings (notably English monolingual), but Transformer-baseline performed better on discourse ordering (ordering triples) due to the explicit order in the serialized input.",
            "advantages": "Simple to implement and compatible with standard seq2seq models; preserves a verbalization order that helps discourse ordering; can be competitive when combined with strong tokenization (BPE) and copying mechanisms.",
            "disadvantages": "Loses explicit graph structure (adjacency and global graph context) compared to graph-based encoders; may fail to exploit structure-dependent features; can be suboptimal for capturing multi-hop relations or global node contexts.",
            "failure_cases": "Graph-structure-sensitive phenomena (e.g., fluency that depends on neighborhood context) may be less well captured; previous work [31] argued linearization has drawbacks relative to graph encoders for structured input.",
            "uuid": "e8816.2",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GAT encoder",
            "name_full": "Graph Attention Network encoder (multi-head GAT)",
            "brief_description": "A graph neural network encoder that computes node representations with attention-weighted aggregation of neighbor features; used in NABU to encode reified RDF graphs so each node attends dynamically to its neighbors with learned attention coefficients.",
            "citation_title": "Graph attention networks",
            "mention_or_use": "use",
            "representation_name": "Graph Attention Network (GAT) encoding",
            "representation_description": "Each node's representation is updated by applying attention scores over its neighbors: attention coefficients e_ij = a(h_i, h_j) are computed and normalized via softmax to produce α_ij; node representations are aggregated as weighted sums and passed through feed-forward/normalization layers; multi-head attention yields multiple representation subspaces.",
            "graph_type": "Knowledge graphs / reified RDF graphs (nodes include subjects, objects, predicate-nodes)",
            "conversion_method": "Apply multi-head self-attention at node level on the reified graph where node inputs are H + L + E vectors; compute attention coefficients among neighbors and aggregate to produce context-aware node embeddings that are passed to the Transformer decoder via cross-attention.",
            "downstream_task": "RDF-to-text generation (WebNLG), multilingual verbalization",
            "performance_metrics": "As part of NABU's overall model metrics (see NABU entry) — the paper attributes performance gains over transformer-baseline to the GAT-based encoder combined with reification.",
            "comparison_to_others": "Compared implicitly against other graph encoders (e.g., GCN in [31]) and sequence linearization; GAT is argued to better weight neighbor importance dynamically versus GCN's fixed normalization, and when combined with reification it reduces parameter explosion versus labeling edges directly.",
            "advantages": "Dynamic attention lets the encoder emphasize more informative neighbors for each node; produces richer local and contextual node representations; integrates naturally with attention-based decoders (Transformer).",
            "disadvantages": "Potential parameter explosion if edge labels are encoded as parameters (mitigated here via reification); computational cost can be higher for large graphs; requires careful design to avoid ambiguity in subject–predicate alignments after reification.",
            "failure_cases": "When predicates/relations share very similar embeddings or domains/ranges, attention may not disambiguate properly leading to wrong predicate lexicalizations (e.g., artist vs producer). Also, identical predicate nodes for different triples can cause subject assignment errors after reification.",
            "uuid": "e8816.3",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GCN encoder (Marcheggiani & Perez)",
            "name_full": "Graph Convolutional Network encoder for structured data-to-text",
            "brief_description": "A GCN-based encoder that directly exploits graph structure (nodes and labeled edges) to compute node-level representations for downstream text generation; reported previously to improve over sequential encoders for RDF-to-text.",
            "citation_title": "Deep graph convolutional encoders for structured data to text generation",
            "mention_or_use": "mention",
            "representation_name": "Graph Convolutional Network (GCN) encoding",
            "representation_description": "Uses graph convolutional layers where each node updates its embedding by aggregating (normalized) transformed neighbor features via fixed normalization constants (1/c_ij), followed by non-linearities (e.g., ReLU); supports end-to-end graph-to-text when combined with decoders.",
            "graph_type": "Knowledge graphs / RDF triples",
            "conversion_method": "Feed node feature matrix into stacked GCN layers (multiplying by weight matrices and summing normalized neighbor contributions) to produce node embeddings; pass embeddings to decoder (e.g., GRU or Transformer) for generation.",
            "downstream_task": "RDF-to-text generation (WebNLG / structured data-to-text)",
            "performance_metrics": "Prior work (Marcheggiani & Perez) reported improvements over LSTM baselines on WebNLG; in this paper GCN is referenced as prior art but no new numeric results are provided for GCN.",
            "comparison_to_others": "Paper cites that GCN outperformed LSTM/GRU sequential models and that graph-based approaches (like GCN) address some linearization drawbacks; GAT is presented as an improvement over GCN by allowing dynamic attention weighting.",
            "advantages": "Directly exploits graph connectivity, preserves structural inductive bias, and can outperform sequence encoders on structured inputs.",
            "disadvantages": "Convolution operation uses fixed normalization that may not capture differential importance of neighbors (addressed by GAT); may require careful feature/edge handling for labeled edges.",
            "failure_cases": "Fixed neighbor normalization can underweight important neighbors and overemphasize others; handling labeled relations can lead to parameter growth if not addressed (e.g., via reification).",
            "uuid": "e8816.4",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "GTR-LSTM",
            "name_full": "GTR-LSTM: Triple encoder capturing intra- and inter-triple relationships",
            "brief_description": "A model that encodes RDF KGs by capturing both relationships within a triple and relations between triples, using a specialized triple-focused encoder architecture (GTR-LSTM) feeding an LSTM decoder for sentence generation.",
            "citation_title": "Gtr-lstm: A triple encoder for sentence generation from rdf data",
            "mention_or_use": "mention",
            "representation_name": "Triple-based encoder (GTR-LSTM)",
            "representation_description": "Encodes triples with mechanisms designed to capture local triple structure and global inter-triple dependencies (distinct encoding for each triple and aggregation across triples), producing representations used by a sequential decoder (LSTM) for text generation.",
            "graph_type": "Knowledge graphs / RDF triples",
            "conversion_method": "Encode each triple with dedicated encoders that model subject-predicate-object relationships and interactions across triples to capture global KG context; decode with LSTM into text.",
            "downstream_task": "RDF-to-text generation / sentence generation from RDF data",
            "performance_metrics": "Referenced as prior work in related work; no direct metrics reported in this paper.",
            "comparison_to_others": "Acknowledged among graph-aware approaches that attempt to capture triple-local and cross-triple information; compared thematically to GCN/GAT works in literature review rather than via direct experiments in this paper.",
            "advantages": "Designed to model both local triple semantics and global KG structure; can better capture cross-triple dependencies than naive serializations.",
            "disadvantages": "Uses LSTM decoders which may be less parallelizable than Transformer decoders; potentially more complex to scale to many triples.",
            "failure_cases": "Not discussed in detail here; general limitations relate to modeling long-range dependencies and scalability.",
            "uuid": "e8816.5",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Gated Graph Neural Networks / Graph-to-sequence (Beck et al.)",
            "name_full": "Graph-to-sequence learning using gated graph neural networks",
            "brief_description": "A graph-to-sequence approach using gated graph neural networks (GGNN) to encode graph structure for sequence generation tasks; referenced as related prior work demonstrating graph encoders for text generation.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Gated Graph Neural Network (GGNN) encoding",
            "representation_description": "GGNNs propagate and update node hidden states using gated recurrent units across graph edges, enabling multi-step message passing that captures global graph context before feeding node/graph representation to sequence decoders.",
            "graph_type": "General graphs / knowledge graphs",
            "conversion_method": "Apply recurrent gated updates across graph edges for several propagation steps to obtain context-rich node embeddings; aggregate node representations for decoder input in a sequence generation model.",
            "downstream_task": "Graph-to-sequence / RDF-to-text generation",
            "performance_metrics": "Referenced as prior art; no numerical results provided in this paper.",
            "comparison_to_others": "Listed among graph-based approaches; contrasted with GAT (attention-based aggregation) and GCN (convolutional aggregation).",
            "advantages": "Allows flexible, multi-step message passing and gated updates, capturing complex dependencies in graphs.",
            "disadvantages": "Potential parameter and time cost for multiple propagation steps; may suffer parameter explosion for labeled/typed edges unless mitigated.",
            "failure_cases": "Parameter explosion when encoding many relation types as separate parameters; cited as a problem motivating reification in NABU.",
            "uuid": "e8816.6",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Copy mechanism",
            "name_full": "Copy / pointer mechanism for seq2seq models",
            "brief_description": "A mechanism used in sequence-to-sequence generation that first attempts to copy source tokens with high attention weights to handle out-of-vocabulary (OOV) or unseen entities and otherwise falls back to generating vocabulary tokens; used in NABU to handle unseen RDF entities.",
            "citation_title": "Incorporating copying mechanism in sequence-to-sequence learning",
            "mention_or_use": "use",
            "representation_name": "Copying / Pointer mechanism",
            "representation_description": "At decoding time, if a target token is OOV, the decoder identifies the source token with highest attention weight and substitutes that token; if no appropriate substitution is found, the source token is copied directly into the output.",
            "graph_type": "Applies when input is serialized or encoded (RDF triples / graph tokens); relevant to RDF-to-text where entity surface forms are needed",
            "conversion_method": "Integrate copy/pointer scoring into decoder: combine generation probability over target vocabulary with copy probability from source tokens weighted by decoder attention; select via beam search/decoding.",
            "downstream_task": "RDF-to-text generation (handling unseen entities), multilingual generation",
            "performance_metrics": "Used within NABU to mitigate OOVs and unseen-entity issues; no isolated metrics presented, but paper reports some failures remain (e.g., copying English entity form into Russian outputs).",
            "comparison_to_others": "Paper uses copy mechanism in both Transformer baseline and NABU as part of experiments; copy helps but does not fully resolve unseen-entity generation in certain languages/scripts.",
            "advantages": "Helps handle rare/unseen entity strings without requiring them in vocabulary; improves surface realization of entity names in generated text.",
            "disadvantages": "May copy source-surface forms into the wrong target language (e.g., copying English form into Russian output); does not ensure proper inflection/lemmatization for target language; limited when script differs (Cyrillic vs Latin).",
            "failure_cases": "Examples where Russian outputs included English-form entities (e.g., 'Visvesvaraya Technical University' in English instead of Cyrillic/Russian verbalization) due to copy behavior.",
            "uuid": "e8816.7",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        },
        {
            "name_short": "Language token / language node",
            "name_full": "Special language token (for seq2seq) / language node (for graph encoders) for multilingual control",
            "brief_description": "A technique that marks the target language by adding a special token at the beginning of the encoder input sequence (or adding a language node to the input graph), enabling a single multilingual model to condition generation on the desired output language.",
            "citation_title": "Multilingual neural machine translation with knowledge distillation",
            "mention_or_use": "use",
            "representation_name": "Language token / language-node conditioning",
            "representation_description": "For sequence models: prepend a special token indicating the target language to the serialized triple sequence. For graph models: add a special language node to the graph input so the graph encoder can propagate a language signal. Both approaches condition the decoder to produce text in the indicated language.",
            "graph_type": "Serialized triple sequences and reified graphs (multilingual RDF-to-text)",
            "conversion_method": "Add a language-designating element to the encoder input (special token or graph node); shuffle and mix multilingual training examples; the decoder learns to attend to the language signal to produce the correct target language.",
            "downstream_task": "Multilingual RDF-to-text generation, bilingual/multilingual modeling (EN/DE/RU in this paper)",
            "performance_metrics": "Multilingual NABU (with language node) BLEU 56.04, METEOR 38.34, chrF++ 62.04; bilingual/monolingual results reported in NABU entry show language conditioning success particularly for EN-DE.",
            "comparison_to_others": "Technique aligns with multilingual NMT literature; paper notes that adding a language token/node is standard practice ([46]) and was used for both Transformer baseline and GAT models; contributed to successful multilingual training.",
            "advantages": "Simple, lightweight conditioning signal enabling one model to generate multiple languages; leverages shared KG input across languages; supports zero-shot/transfer phenomena observed in multilingual NMT literature.",
            "disadvantages": "May not be sufficient alone when languages have very different vocabularies/scripts (observed weaker EN-RU bilingual performance); requires shared subword vocabulary or careful tokenization.",
            "failure_cases": "EN-RU bilingual model showed inconsistent scores and inferior BLEU/METEOR compared to baseline, attributed to large lexical/script differences despite the presence of language tokens/nodes.",
            "uuid": "e8816.8",
            "source_info": {
                "paper_title": "NABU - Multilingual Graph-based Neural RDF Verbalizer",
                "publication_date_yy_mm": "2020-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Gtr-lstm: A triple encoder for sentence generation from rdf data",
            "rating": 2,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_global_and_local_node_contexts_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "The webnlg challenge: Generating text from dbpedia data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_dbpedia_data"
        },
        {
            "paper_title": "Incorporating copying mechanism in sequence-to-sequence learning",
            "rating": 1,
            "sanitized_title": "incorporating_copying_mechanism_in_sequencetosequence_learning"
        },
        {
            "paper_title": "Multilingual neural machine translation with knowledge distillation",
            "rating": 1,
            "sanitized_title": "multilingual_neural_machine_translation_with_knowledge_distillation"
        }
    ],
    "cost": 0.01881175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NABU - Multilingual Graph-based Neural RDF Verbalizer</h1>
<p>Diego Moussallem ${ }^{1 * \dagger}$, Dwaraknath Gnaneshwar ${ }^{2 * \dagger}$, Thiago Castro Ferreira<br>$\otimes^{3,4 \dagger}$, and Axel-Cyrille Ngonga Ngomo ${ }^{1}$<br>${ }^{1}$ Data Science Group, University of Paderborn, Germany<br>first.lastname@upb.de<br>${ }^{2}$ DL group, Manipal Institute of Technology, India<br>dwarakasharma@gmail.com<br>${ }^{3}$ Federal University of Minas Gerais (UFMG), Brazil<br>${ }^{4}$ Tilburg center for Cognition and Communication (TiCC)<br>Tilburg University, The Netherlands<br>tcastrof@tilburguniversity.edu</p>
<h4>Abstract</h4>
<p>The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU.</p>
<p>Keywords: Knowledge Graphs $\cdot$ Natural Language Generation $\cdot$ Semantic Web.</p>
<h2>1 Introduction</h2>
<p>Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data [40]. Despite community agreement on the text and speech output of these systems, there is far less consensus on what the input should be [22]. A large number of inputs have hence been employed for NLG systems, including images [50], numeric data [24], and Semantic Web (SW) data [36]. Practical applications can be found in domains such as weather forecasts [32], feedback for car drivers [8], diet management [1].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Presently, the generation of natural language from Resource Description Framework (RDF) data has gained substantial attention [7]. The RDF-to-text task has hence been proposed to investigate the quality of automatically generated texts from RDF Knowledge Graphs (KGs) [11]. With the emergence of neural methods, end-to-end data-to-text models have been introduced to learn input-output mappings directly. These approaches rely much less on explicit intermediate representations compared to rulebased approaches [23].</p>
<p>Although Neural NLG models have been achieving very good results [20], English is the only language that has been widely targeted. In this work, we alleviate this language limitation by proposing a multilingual approach, named NABU. The motivation behind multilingual models lies in several directions, mainly in (1) transfer learning; when low-resource language pairs are trained together with high-resource languages, the translation quality improves; (2) zero-shot translation, where multilingual models are able to translate between language pairs from similar families that were never seen during training; (3) Easy deploy, a multilingual model achieving same performance on many languages in comparison to several separate language-specific models are much more desirable for companies in terms of deployment [26].</p>
<p>Our approach, NABU, is based on the fact that knowledge graphs are languageagnostic and hence can be used on the encoder side to generate multilingual text. NABU consists of an encoder-decoder architecture which incorporates structural information of RDF triples using an encoding mechanism inspired by Graph Attention Network (GAT) [49]. In contrast to recent related work [41], NABU relies on the use of a reification strategy for modeling the graph structure of RDF input. The decoder part is based on the vanilla Transformer model [48] along with an unsupervised tokenization model.</p>
<p>We evaluate NABU on the standard benchmarking WebNLG datasets[19] in three settings: monolingual, bilingual and multilingual. For the monolingual setting, we compare NABU with state-of-the-art English approaches and also perform experiments on Russian and German. The goal of the bilingual setting is to analyze the performance of NABU for language families. To achieve this goal, we train and evaluate bilingual models using NABU on English-German and on English-Russian. In the multilingual setting, we compare NABU with a multilingual Transformer model on English, German and Russian. Our results show that NABU outperforms state-of-the-art approaches on English and achieves 66.21 BLEU. NABU also achieves consistent results across all languages on multilingual settings with 56.04 BLEU. In addition, NABU presents promising results on the bilingual models with 61.99 BLEU. Our findings suggest that NABU is able to generate multilingual text with similar quality to that generated by humans. The main contributions of this paper can be summarized as follows:</p>
<ul>
<li>We present a novel approach dubbed NABU based on a GAT-Transformer architecture for generating multilingual text from RDF KGs.</li>
<li>NABU outperforms English state-of-the-art approaches with consistent average improvements of +10 BLEU, METEOR and chrF3 on the WebNLG datasets.</li>
<li>NABU exploits the benefits of modeling of language families in the generation task.</li>
</ul>
<p>The version of NABU used in this paper and also all experimental data are publicly available. ${ }^{5}$.</p>
<h1>2 Related Work</h1>
<p>A significant body of research has investigated the generation of Natural Language (NL) texts from RDF data. A plenty of research is based on template- and rule-based approaches such as $[10,14,15,36]$. Recently, the WebNLG [11] challenge made this research area more prominent by providing a benchmark corpus of English texts verbalizing RDF triples in 15 different semantic domains. Among the participating models, the works based on sequence-to-sequence Neural Networks (NNs) achieved some of the best results [45,34]. Moreover, RDF has also been showing promising benefits to the generation of benchmarks for evaluating NLG systems [35].</p>
<p>The choice of neural architectures for RDF-to-text has evolved constantly along the last couple of years. All end-to-end models submitted to the WebNLG challenge [20] received the set of triples in a linearized form as input. However, researchers have recently been experimenting with graph-based approaches, which take the RDF input formatted as a graph, with promising results. Marcheggiane and Perez [31] proposed a structured data encoder based on Graph Convolutional Network (GCN) that directly exploits the graph structure and presented better results than Long Short-Term Memories (LSTM) models. Distiawan et al. [13] presented a GTR-LSTM architecture which captures the global information of a KG by encoding the relationships both within a triple and between the triples. Ferreira et al. [17] introduced a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Although Marcheggiane and Perez [31] showed that the linearisation of the input graph has several drawbacks, the authors implemented Gated recurrent unit (GRU) and Transformer architectures which showed results superior to those of the former architecture. Recently, Ribeiro et.al [41] devised an unified graph attention network structure which investigates graph-to-text architectures that combined global and local graph representations to improve fluency in text generation. Their experiments demonstrated significant improvements on seen categories in the WebNLG dataset.</p>
<p>Despite the plethora of graph-based neural approaches on handling RDF data, English is the only language which has been widely targeted. Recent efforts were made to create German and Russian language versions of WebNLG [16,44]. However, no work that investigates these languages has been published at the time of writing. To the best of our knowledge, NABU is hence the first approach which tackles multilinguality in the RDF-to-text task.</p>
<h2>3 The NABU Approach</h2>
<p>NABU tackles RDF-to-text based on the formal description of a translation problem. The RDF-to-text task takes an RDF graph as input and generates an output text which</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Example of a set of triples (left) and the corresponding verbalization (right).
reflects its meaning. Figure 1 depicts an example of a set of 3 RDF triples and the corresponding text. Therefore, the underlying idea behind our approach is as follows: Given that KGs are language-agnostic and represent facts often extracted from text, we can regard the facts (i.e., RDF triples) as sentences and train a model to translate the facts from a language-agnostic graph representation to several languages. In the following, we give an overview of GAT architecture and Transformer. Thereafter, we present NABU in detail. Throughout the description of our methodology and our experiments, we use DBpedia [2] as reference Knowledge Base (KB) since the benchmarking datasets are based on this KB.</p>
<h1>3.1 Background</h1>
<p>Transformer Transformer-based models consist of an encoder and a decoder, i.e., a two-tier architecture where the encoder reads an input sequence $x=\left(x_{1}, \ldots, x_{n}\right)$ and the decoder predicts a target sequence $y=\left(y_{1}, \ldots, y_{n}\right)$. The encoder and decoder interact via a soft-attention mechanism [3]30, which comprises one or multiple attention layers. We follow the notations from Tang et al. 47] in the subsequent sections: Let $m$ stand for the word embedding size and $n$ for the number of hidden units. Further, let $K$ be the vocabulary size of the source language. Then, $h_{i}^{l}$ corresponds to the hidden state at step $i$ of layer $l . h_{i-1}^{l}$ represents the hidden state at the previous step of layer $l$ while $h_{i}^{l-1}$ means the hidden state at $i$ of layer $l-1 . E \in \mathbb{R}^{m \times K}$ is a word embedding matrix, $W \in \mathbb{R}^{n \times m}, U \in \mathbb{R}^{n \times n}$ are weight matrices, $E_{x_{i}}$ refers to the embedding of $x_{i}$, and $e_{p o s, i}$ indicates a positional embedding at position $i$.</p>
<p>Transformer models rely deeply on self-attention networks. Each token is connected to every other token in the same sentence directly via self-attention. Thus, the path length between any two tokens is 1 . Due to lack of recurrence found in Recurrent Neural Network (RNN), Transformers implement positional encoding to input and output. Additionally, these models rely on multi-head attention to feature attention networks, which are more complex in comparison to the 1-head attention mechanism used in RNNs. In contrast to RNN, the positional information is also preserved in positional embeddings. Equation 1 describes the hidden state $h_{i}^{l}$, which is calculated from all hidden states of the previous layer. $f$ represents a feed-forward network with the rectified</p>
<p>linear unit (ReLU) as the activation function and layer normalization. The first layer is implemented as $h_{i}^{0}=W E_{x_{i}}+e_{p o s, i}$. Moreover, the decoder has a multi-head attention over the encoder's hidden states:</p>
<p>$$
h_{i}^{l}=h_{i}^{l-1}+f\left(\text { self-attention }\left(h_{i}^{l-1}\right)\right)
$$</p>
<p>Graph Attention Networks Deep Learning on non-euclidean data has recently gained substantial research interest due to the abundance of its availability. A plethora of problems can be solved efficiently by representing data in a data structure that can utilize the inherent structure and inter-entity relationships. Kipf and Welling [28] introduced GCN, through which they generalize the convolution operation of Convolutional Neural Network (CNN) to graph structures. Every layer in a GCN has a weight matrix $W$ that transforms nodes feature vectors from a low-dimensional representation space to high-dimensional representation space, which aims to preserve the structure of the graph.</p>
<p>Consider a graph of $z$ nodes and a set of node features $\left(\boldsymbol{h}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{h}</em>}}, . ., \boldsymbol{h<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$. A GCN layer computes a net set of features $\left(\boldsymbol{h}</em>}}^{\prime}, \boldsymbol{h<em _mathbf_z="\mathbf{z">{\mathbf{2}}^{\prime}, . ., \boldsymbol{h}</em>$ to stabilize the update rule. Finally,}}^{\prime}\right)$. First the feature matrix is multiplied with $W \boldsymbol{g}=W \boldsymbol{h}$. Then, the aggregated sum of node features are normalized using normalization constant $\frac{1}{c_{i j}</p>
<p>$$
\boldsymbol{h}<em N__i="N_{i" _epsilon="\epsilon" j="j">{\boldsymbol{i}}^{\prime}=\sigma\left(\sum</em>\right)
$$}} \frac{1}{c_{i j}} \boldsymbol{g}_{j</p>
<p>However, the convolution operation in GCN does not take into account the fact that some nodes are more important than others to generate a particular segment of the target sentence. To alleviate this problem, Velickovic et al. [49] devised GAT, which converts the normalization constant into dynamic attention coefficients. The attention coefficients are calculated by applying self-attention over node features. In one forward pass, a GAT layer calculate a score of a given node that quantifies the importance of neighbors to its representation:</p>
<p>$$
e_{i j}=a\left(\boldsymbol{h}<em _boldsymbol_j="\boldsymbol{j">{\boldsymbol{i}}, \boldsymbol{h}</em>\right)
$$}</p>
<p>The attention scores are then normalized using softmax:</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k \epsilon N_{i}} \exp \left(e_{i k}\right)}
$$</p>
<h1>3.2 Approach</h1>
<p>Graph-based NNs have been used successfully to parse and support the generation of natural-language sentences from RDF KG. Although GAT models have shown to alleviate the loss of node information, the network still suffers from parameter explosion depending on the size of the graph structure [5]. To alleviate the parameters explosion problem, we follow the same strategy used in [31], named reification, ${ }^{6}$ to slightly</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>modify how the RDF graph is encoded. We describe below how reification is applied. Afterward, we explain the encoder and decoder parts of NABU. An overview of NABU architecture after reification can be found in Figure 2.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: NABU architecture</p>
<p>Reification. RDF triples are represented as a graph in which (i) the subjects and objects are nodes and (ii) predicates (relationships) between them are labeled edges. For example, <Albert_Einstein, birthPlace, Germany> can be seen as a sub-KG in DBpedia where Albert_ Einstein and Germany are the nodes and birthPlace is the edge. However, the edges are encoded as parameters by the GAT, and the parameters explosion problem stated by Beck et al. [5] often occurs.</p>
<p>Therefore, we follow the reification strategy, which maps the relations to nodes in the KG and creates new binary relations for each relation in the RDF triples. We rely on two binary relations, which model the relationship between the subject and predicate (A0) and predicate and object (A1) only. For example, 〈Albert_Einstein, birthPlace, Germany〉 becomes 〈Albert_Einstein, A0, birthPlace〉 and〈birthPlace, A1, Germany〉. Apart from handling the parameter explosion problem, reification is useful in two ways. First, the encoder generates a hidden state for each relation in the input. Second, it allows for modeling an arbitrary number of edges (predicates) efficiently. Figure 3 illustrates the reification strategy for our example.</p>
<p>Encoder. Here, the reified graph is sent as input to the GAT that applies a self-attention mechanism to compute the importance of each node in the graph. The GAT encoder represents nodes in a high-dimensional vector space whilst taking into account the representations of their neighbors. Note that NABU follows the same strategy of recent literature on multilingual Neural Machine Translation (NMT) models in which a special token is used in the encoder to determine to what target language to translate [46]. Figure 4 shows how a single forward step/pass works in NABU approach.</p>
<p>In one forward pass of our model, we have four dense vectors as inputs, namely (i) the node vector $\boldsymbol{H}=\left(\boldsymbol{h}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{h}</em>}}, . ., \boldsymbol{h<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$ with embeddings of all nodes in the graphs, (ii) the source vector, $\boldsymbol{S}=\left(s</em>\right)$ with embeddings of source nodes in edges}}, s_{\mathbf{2}}, . ., s_{\mathbf{z}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Reification used on our example.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: An overview of a single forward pass in NABU.
of the graph, (iii) the destination vector, $\boldsymbol{D}=\left(\boldsymbol{d}<em _mathbf_2="\mathbf{2">{\mathbf{1}}, \boldsymbol{d}</em>}}, \ldots, \boldsymbol{d<em _mathbf_1="\mathbf{1">{\mathbf{z}}\right)$ with embeddings of target nodes in edges of the graphs and (iv) the label vector, $\boldsymbol{L}=\left(\boldsymbol{l}</em>}}, \boldsymbol{l<em _mathbf_z="\mathbf{z">{\mathbf{2}}, \ldots, \boldsymbol{l}</em>)$ to form the input vector to our encoder:}}\right)$ with embedding labels. The source $\boldsymbol{S}$ and destination $\boldsymbol{D}$ vectors are concatenated and are passed through dense layer which encodes them into a vector of the same shape as the label vector. We call this new vector the edge vector, $\boldsymbol{E}$. We then add the edge vector $(\boldsymbol{E})$, node vector $(\boldsymbol{H})$ and label vector $(\boldsymbol{L</p>
<p>$$
\begin{gathered}
\boldsymbol{E}=f(\boldsymbol{S}, \boldsymbol{D}), \text { and } \
\boldsymbol{H}^{\prime}=|_{h \epsilon \eta} G(\boldsymbol{H}+\boldsymbol{L}+\boldsymbol{E})
\end{gathered}
$$</p>
<p>where $\eta$ is the number of heads in the multi-head attention layer.</p>
<p>Decoder Our decoder follows the standard architecture of the Transformer decoder, which takes into account the intermediate representation generated by the encoder. The decoder gives a probability distribution over the target language's vocabulary. We also</p>
<p>rely on an unsupervised tokenizer, which implements Byte Pair Encoding (BPE) [43] and unigram language model [29] for handling multilinguality and out-of-vocabulary words. Afterward, we apply a beam search for selecting the most likely word in the output sentence.</p>
<h1>4 Evaluation</h1>
<h3>4.1 Goals</h3>
<p>In our evaluation, we address the following research questions:
Q1: How does our multilingual approach compare with state-of-the-art results in English?
Q2: Is NABU able to generate bilingual text while modelling two languages from distinct families?
Q3: How accurate are the multilingual texts generated by NABU?
We designed our evaluation as follows: First, we measured the performance of NABU on English by using the WebNLG dataset and compared it with state-of-theart approaches. Additionally, we evaluated NABU on two other languages-German and Russian. Second, we evaluated NABU on bilingual models -English-German and English-Russian. Third, we combined all three languages in a multilingual setting and compared it with a multilingual Transformer baseline model. For measuring the quality of our approach, we used the automatic evaluation metrics BLEU, METEOR, and CHRF++ .</p>
<h3>4.2 Data</h3>
<p>The experiments presented in this work were conducted on the WebNLG corpus [18,21], which consists of sets of RDF triples mapped to target texts. In comparison with other popular NLG benchmarks [6,37,33], WebNLG is the most semantically varied corpus. Its English version contains 25,298 texts which describe 9,674 sets of up to 7 RDF triples in 15 domains: Astronaut, University, Monument, Building, Comics Character, Food, Airport, Sports Team, Written Work, City, Athlete, Artist, Means of Transportation, Celestial Body and Politician. Out of these domains, five ((Athlete, Artist, MeanOfTransportation, CelestialBody, Politician)) are exclusively present in the test set, being unseen during the training and validation processes.</p>
<p>For German and Russian, we relied on the translated versions of WebNLG corpus [9,44]. The German version comprises 20,370 texts describing 7,812 sets of up to 7 RDF triples in 15 domains. Additionally, the German datasets provide gold-standard representations for traditional pipeline steps, such as discourse ordering (i.e., the order in which the source triples are verbalized in the target text), text structuring (i.e., the organization of the triples into paragraph and sentences), lexicalization (i.e., verbalization of the predicates) and referring expression generation (i.e., verbalization of the entities). The Russian datasets contain 20,800 texts describing 5,185 sets of up to 7 RDF triples in 9 domains. Both were automatically created and manually analyzed. The English and</p>
<p>Russian datasets abide by the criteria to gold standards as they were manually assessed by several native speakers. The German version can be regarded as a silver standard given that it did not go through the same process and contains some known errors.For the monolingual experiments, we relied on the standard WebNLG parts of train, dev, and test sets across all languages. Note that the German version does not contain a test set originally. Therefore we relied on a k-Fold Cross-Validation technique to create the test set. For the multilingual set of experiments, we concatenated all English, German and Russian datasets and shuffled their training sets randomly to facilitate an end-to-end training of the model.</p>
<h1>4.3 Tasks</h1>
<p>We designed three tasks for carrying out our evaluation, (1) Monolingual, (2) Bilingual, (3) Multilingual. (1) In the monolingual task, we train our models to work in each language separately. Hence, we generate three models, one for English, one for German, and another for Russian. Each model receives RDF triples from its given DBPedia language version. For example, the German model receives triples from the German DBpedia. Afterward, we evaluate the models on each WebNLG language-specific dataset. (2) The bilingual task was divided into two sets; the first set, we train one English-German model. This model receives RDF triples from the English and German DBpedia versions as input and has to generate text in English and German, as output. For the second set, we trained one English-Russian model that receives RDF triples from the English and Russian DBpedia versions and generates text in English and Russian, respectively. (3) In the third task, we train one multilingual model which receives as input the triples from the English, German, and Russian DBpedia versions. This model has to output text in three languages, English, German, and Russian, respectively. The input relies on WebNLG triples containing resources from the English, German, and Russian DBpedia KGs, all entities are found across the three KGs via sameAs relations for the sake of completeness.</p>
<h3>4.4 Model settings</h3>
<p>In this section, we describe the parameters and hyper-parameters used to train NABU models. We experimented with two encoder-decoder architectures for RDF verbalization. First, Transformer ${ }<em A="A" G="G" T-T="T-T" a="a" n="n" r="r" s="s">{\text {baseline }}$ which is an encoder-decoder model with a pure transformer architecture used to both encode triples into intermediate representation and decode it into tokens. Second, NABU $</em>$, which comprises a GAT encoder and Transformer as the decoder.</p>
<p>For both models, we relied on the same settings. We used a Transformer 6-layer encoder-decoder model with an 8 -headed multi-head attention mechanism [48]. The training used a batch size of 32 and Adam optimizer with an initial maximum learning rate of 0.001 . We set a source and target word embedding's size of 256, and hidden layers to size 256, dropout $=0.3$ (naive). We used a vocabulary of 32000 words for the word based models and a beam size of 5 . All our vocabularies were trained using the sentencepiece library. ${ }^{7}$ In addition, we used a copy mechanism for investigating the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>out-of-vocabulary (OOV) words issue. This mechanism first tries to substitute the OOV words with target words that have the highest attention weight according to their source words [30]. If the words are not found, it copies the source words to the position of the not-found target word [25]. Note that we added an extra language token at the beginning of our input sentences for the Transformer model, and a language node to the input graph in our GAT model for performing the bilingual and multilingual experiments. This technique of adding a special language token is in line with [46].</p>
<h1>4.5 Evaluation Metrics</h1>
<p>We used three automatic Machine Translation (MT) standard metrics to ensure consistent and clear evaluation of the common evaluation datasets of the WebNLG challenge. BLEU [38] uses a modified precision metric for comparing the MT output with the reference (human) translation. The precision is calculated by measuring the n-gram similarity $(\mathrm{n}=1, . .4)$ at the word level. BLEU also applies a brevity penalty by comparing the length of the MT output with the reference translation. METEOR [4] was mainly introduced to overcome the semantic weakness of BLEU. To this end, METEOR considers stemming and paraphrasing along with exact standard word (or phrase) matching. The synonymy overlap through a shared WordNet synset of the words. Along with exact standard word (or phrase) matching, it has additional features, i.e., stemming and paraphrasing. CHRF++ [39] exploits the use of character n-gram precision and recall (Fscore) for automatic evaluation of MT outputs. chrF++ has shown a good correlation with human rankings of different MT outputs and is simple and does not require any additional information. Additionally, chrF++ is language- and tokenization-independent.</p>
<h3>4.6 Results</h3>
<p>Monolingual. Our experiments report that NABU consistently outperforms state-of-the-art models on English data. Table 1 shows that NABU achieved a BLEU score of 66.21 , which is $28.15 \%$ higher than the previous state-of-the-art Transformer model [17]. We decided to run our experiments on all WebNLG categories to elucidate the strengths and limitations of NABU. According to [17], the main drawback in current NN models is the incapability of generating text for unseen entities and that the experiments should be on all categories. NABU, in turn, shows that it is capable of predicting correctly both seen and unseen entities and their relations. In addition, NABU shows an improvement in METEOR up to +2 points. We report NABU's chrF++ as our intention is to follow recent literature which has adopted this metric due to its good correlation with human results. We can now answer [Q1] as follows: NABU surpasses state-of-the-art results on WebNLG in English.</p>
<p>Table 2 shows that NABU outperforms the transformer baseline on German and Russian. It is important to note that our Transformer baseline, Transformer ${ }<em _baseline="{baseline" _text="\text">{\text {baseline }}$, already outperforms the previous state-of-the-art approaches on English. The difference between our Transformer ${ }</em>$ and the Transformer presented by [17] is that we rely on BPE and character-level tokenizer on the decoder side. Our results suggest that we can refrain from running the related work (see Table 1) on the German and Russian datasets, especially as they were designed and tested to work on English, thus there is}</p>
<p>Table 1: Results on WebNLG English test set with all categories (seen and unseen), comparison with the state-of-the-art approaches</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">UPF-FORGe</td>
<td style="text-align: left;">38.65</td>
<td style="text-align: left;">39.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Melbourne</td>
<td style="text-align: left;">45.13</td>
<td style="text-align: left;">37.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Moryossef et al., 2019)</td>
<td style="text-align: left;">47.40</td>
<td style="text-align: left;">39.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Castro et al. (2019)</td>
<td style="text-align: left;">51.68</td>
<td style="text-align: left;">32.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">$\mathbf{6 6 . 2 1}$</td>
<td style="text-align: left;">$\mathbf{4 1 . 1 1}$</td>
<td style="text-align: left;">$\mathbf{7 1 . 9 8}$</td>
</tr>
</tbody>
</table>
<p>currently no baseline for German and Russian. With these results, NABU demonstrates its language agnosticism and presents improvements in German and Russian over the baseline.</p>
<p>Table 2: Monolingual Results on WebNLG language testsets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Language</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Monolingual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">54.96</td>
<td style="text-align: center;">38.43</td>
<td style="text-align: center;">69.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GER</td>
<td style="text-align: center;">50.07</td>
<td style="text-align: center;">34.51</td>
<td style="text-align: center;">63.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RUS</td>
<td style="text-align: center;">46.42</td>
<td style="text-align: center;">27.74</td>
<td style="text-align: center;">56.80</td>
</tr>
<tr>
<td style="text-align: center;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: center;">ENG</td>
<td style="text-align: center;">66.21</td>
<td style="text-align: center;">41.47</td>
<td style="text-align: center;">71.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GER</td>
<td style="text-align: center;">53.08</td>
<td style="text-align: center;">37.42</td>
<td style="text-align: center;">64.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RUS</td>
<td style="text-align: center;">46.86</td>
<td style="text-align: center;">28.84</td>
<td style="text-align: center;">58.37</td>
</tr>
</tbody>
</table>
<p>Bilingual. Table 3 presents the results of NABU $<em _baseline="{baseline" _text="\text">{G A T-}$ Trans on two bilingual models. The results show that NABU on English-German outperformed the Transformer ${ }</em>$ on all metrics. On English-Russian, NABU $}<em _baseline="{baseline" _text="\text">{G A T-}$ Trans presented worse results on BLEU and METEOR than Transformer ${ }</em>$. However, NABU $}<em A="A" G="G" T-="T-">{G A T-}$ Trans showed superior results on chrF++ which is the metric that best correlates with human results. On the one hand, we analyzed that the English-German model leveraged both languages properly due to their vocabulary overlap. German and English share a word vocabulary of $33 \%$, thus training both languages with NABU $</em>$ Trans, which employs a graph representation on the encoder side and a character level on decoder could actually model both languages correctly and generate coherent text. On the other hand, English-Russian presented inconsistent results because both languages are significantly different, and they do not share any vocabulary. We reckoned these conflicting scores are due to the language family of both languages. Looking manually at the results, we concluded that encoding distinct language families requires additional features, and we, therefore, plan to investigate this phenomenon in the future. The results presented herein answer our sec-</p>
<p>ond research question, [Q2], by showing that NABU is capable of modeling languages from distinct families in a bilingual approach, but a deeper investigation is required.</p>
<p>Table 3: Bilingual Results on WebNLG language test sets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bilingual</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-GER</td>
<td style="text-align: left;">58.30</td>
<td style="text-align: left;">36.46</td>
<td style="text-align: left;">66.72</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-GER</td>
<td style="text-align: left;">$\mathbf{6 1 . 9 9}$</td>
<td style="text-align: left;">$\mathbf{3 9 . 5 1}$</td>
<td style="text-align: left;">$\mathbf{6 9 . 6 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-RUS</td>
<td style="text-align: left;">$\mathbf{5 5 . 3 0}$</td>
<td style="text-align: left;">$\mathbf{3 7 . 9 0}$</td>
<td style="text-align: left;">61.63</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-RUS</td>
<td style="text-align: left;">49.15</td>
<td style="text-align: left;">33.41</td>
<td style="text-align: left;">$\mathbf{6 4 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Multilingual. Table 4 shows that NABU $_{G A T-}$ Trans performed better than Transformerbaseline by presenting consistent improvement of +2 BLEU, METEOR, and chrF++. This result exhibits that NABU can effectively generate multilingual text, thus answering our third research question, [Q3]. Comparing the multilingual results of NABU with its bilingual results on English-Russian, we concluded that the characteristics of the German language, namely its three gender types, contributed to the better alignment of the languages in the decoder side of multilingual NABU model. Russian also contains three genders as German; therefore, NABU made use of it as features for generating coherent texts. We also noticed that the English texts generated by the multilingual NABU model are comparable to those of the English state-of-the-art models. NABU's multilingual model is also better than the previous English state-of-the-art by 4 BLEU and presents comparable results on METEOR. This result also reaffirms the capability of NABU for achieving English state-of-the-art results and contributes to our first research question, $[\mathrm{Q} 1]$.</p>
<p>Table 4: Multilingual Results on WebNLG language testsets</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: left;">Language</th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
<th style="text-align: left;">chrF++</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Multilingual</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Transformer $_{\text {baseline }}$</td>
<td style="text-align: left;">ENG-GER-RUS</td>
<td style="text-align: left;">53.39</td>
<td style="text-align: left;">36.86</td>
<td style="text-align: left;">60.72</td>
</tr>
<tr>
<td style="text-align: left;">NABU $_{\text {GAT-Trans }}$</td>
<td style="text-align: left;">ENG-GER-RUS</td>
<td style="text-align: left;">$\mathbf{5 6 . 0 4}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 3 4}$</td>
<td style="text-align: left;">$\mathbf{6 2 . 0 4}$</td>
</tr>
</tbody>
</table>
<p>Time-Performance. All models were trained on NVIDIA Tesla P100. Both NABU$<em _baseline="{baseline" _text="\text">{G A T-}$ Trans and Transformer $</em>$ models took the same amount of time since they contain the same number of weights. Therefore, the monolingual models took 6 hours to be trained, while the multilingual models took 8 hours on average. This difference of 2 hours lies in the size of the multilingual training dataset, which contains all English, German, and Russian training sets.}</p>
<h1>4.7 Error Analysis and Discussion</h1>
<p>In this section, we report some of the errors found in NABU's output while carrying out a human evaluation. First, we analyzed the discrepancy between BLEU, METEOR, and chrF++: NABU outperformed the previous state-of-the-art approach for English by roughly 15 BLEU, while the difference in METEOR is considerable smaller. Our analysis shows that some entities contained typos and were not generated correctly by NABU. In addition, we found a low variance in the generated synonyms. BLEU ignores these aspects while METEOR penalizes based on them, thus explaining the discrepancy between the scores.</p>
<p>Additionally, we noticed some wrong verbalization of similar predicates (edges) that were responsible for decreasing NABU scores across all languages. For example, NABU was sometimes not able to generate text correctly in the Artist domain. The problem lies in the triples which contain both dbo:artist or dbo:producer relations as predicates. Both predicates are often verbalized to "artist". This happens because the predicates share the same domain and range and therefore have a similar vector representation in the embeddings. We plan to address this issue in future work by using a more appropriate embedding model.</p>
<p>We also analyzed the multilingual texts generated by $\mathrm{NABU}<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ and Transfor$\operatorname{mer}</em>}}$. We noticed that the $\mathrm{NABU<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ performed better at structuring the RDF graph as input and verbalizing a structured set of RDF triples, whereas Transformer ${ }</em>}}$ presented better results than $\mathrm{NABU<em _baseline="{baseline" _text="\text">{G A T-T r a n s}$ at ordering (also known as Discourse Ordering step) the triples for a better verbalization. The advantage of Transformer ${ }</em>}}$ over $\mathrm{NABU<em A="A" G="G" T-T="T-T" a="a" n="n" r="r" s="s">{G A T-T r a n s}$ in Discourse Ordering seems to be related to the linearized form of its input, which explicitly represents in what order the triples have to be verbalized. Additionally, our reification strategy affected the Discourse Ordering, we noticed it by analyzing the generated text from an input with two equal predicates for different subjects. For example, "Albert_Einstein dbo:birthPlace Germany" and "Michael_Jackson dbo:birthPlace USA". NABU $</em>$ verbalized this two triples as "Albert Einstein was born in the United States of America and Michael Jackson was born in Germany". This problem occurs because NABU can not identify the subjects of each predicate correctly as they are identical in the encoder side. We plan to address this drawback by investigating new approaches for the structuring and ordering steps.</p>
<p>Another interesting insight is related to the inflections of words in German, similar to [9]. The possessive was often a source of errors when verbalizing into German. The translation "Elliot See 's Besatzung war ein Testpilot." is not perfect as the apostrophe ('s) is placed wrongly. However, this problem did not happen when generating the sentence, "Bill Oddies Tochter ist Kate Hardie", where the possessive of "Oddie" is built correctly. Similar insights can be derived pertaining to the preposition "von" (en: of). For example, the entity Texas_University was wrongly verbalized as "Universität von Texas" instead of the correct form "Universität Texas". The possessive and related constructions are well-known challenges in MT from English to German. Therefore, we plan to explore this phenomenon in future research deeply.</p>
<p>On the Russian results, we observed that the main challenge was related to the verbalization of unseen entities. In $\mathrm{NABU}_{G A T-T r a n s}$, some entities were copied from their source sentences due to the use of the copy mechanism in NABU. For example,</p>
<p>the entity "Visvesvaraya_Technological_University" was generated as "Visvesvaraya Technical University" in the English form instead of being verbalized in the Russian language. Additionally, we perceived that $\mathrm{NABU}_{G A T-T v a n s}$ displayed problems similar to those reported in [44] for generating Entities. However, these problems were mostly detected in the unseen category. Our current hypothesis is that the generation of unseen entities in Russian is more challenging than German and English due to the Cyrillic alphabet.</p>
<h1>5 Conclusion</h1>
<p>We presented a multilingual RDF verbalizer which relies on graph attention NN along with a reification strategy. Our experiments suggest that our approach, named NABU, outperforms state-of-the-art approaches in English. Additionally, NABU presented consistent results across the languages used in our evaluation. NABU is language-agnostic, which means it can be ported easily to languages other than those considered in this paper. To the best of our knowledge, we are the first approach to exploit and achieve the multilinguality successfully in the RDF-to-text task. As future work, we aim to exploit other graph-based neural architecture and other reification approaches for improving NABU's performance. Additionally, we plan to investigate how to deal with the similarity of relations by combining language models and new evaluation metrics [42]. Moreover, we plan to investigate our methodology in the context of low-resource scenarios as well as on different KGs [27,12].</p>
<h2>Acknowledgments</h2>
<p>Research funded by the German Federal Ministry of Economics and Technology (BMWI) in the project RAKI (no. 01MD19012D) and by the H2020 KnowGraphs (GA no. 860801). This work also has been supported by the German Federal Ministry of Education and Research (BMBF) within the project DAIKIRI under the grant no 01IS19085B as well as by the German Federal Ministry for Economic Affairs and Energy (BMWi) within the project SPEAKER under the grant no 01MK20011U. Finally, we also would like to thank the funding provided by the Coordination for the Improvement of Higher Education Personnel (CAPES) from Brazil under the grant 88887.367980/2019-00.</p>
<h2>References</h2>
<ol>
<li>Luca Anselma and Alessandro Mazzei. Designing and testing the messages produced by a virtual dietitian. In Proceedings of the 11th International Conference on Natural Language Generation, pages 244-253, Tilburg University, The Netherlands, November 2018. Association for Computational Linguistics.</li>
<li>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. The semantic web, pages 722-735, 2007.</li>
<li>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
</li>
<li>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65-72. ACL, 2005.</p>
</li>
<li>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283, 2018.</li>
<li>Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 217-226, Nancy, France, 2011. Association for Computational Linguistics.</li>
<li>Nadjet Bouayad-Agha, Gerard Casamayor, and Leo Wanner. Natural language generation in the context of the semantic web. Semantic Web, 5(6):493-513, 2014.</li>
<li>Daniel Braun, Ehud Reiter, and Advaith Siddharthan. Saferdrive: An NLG-based behaviour change support system for drivers. Natural Language Engineering, 24(4):551-588, 2018.</li>
<li>Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. Enriching the WebNLG corpus. In Proceedings of the 11th International Conference on Natural Language Generation, pages 171-176. Association for Computational Linguistics, 2018.</li>
<li>Philipp Cimiano, Janna Lüker, David Nagel, and Christina Unger. Exploiting ontology lexica for generating natural language texts from rdf data. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 10-19, Sofia, Bulgaria, August 2013. ACL.</li>
<li>Emilie Colin, Claire Gardent, Yassine Mrabet, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from dbpedia data. In Proceedings of the 9th INLG conference, pages 163-167, 2016.</li>
<li>Diego Moussallem, Thiago Castro Ferreira, Marcos Zampieri, Maria Claudia Cavalcanti, Geraldo XexÃ̃o, Mariana Neves, and Axel-Cyrille Ngonga Ngomo. RDF2PT: Generating Brazilian Portuguese Texts from RDF Data. In The 11th edition of the Language Resources and Evaluation Conference, 7-12 May 2018, Miyazaki (Japan), 2018.</li>
<li>Bayu Distiawan, Jianzhong Qi, Rui Zhang, and Wei Wang. Gtr-lstm: A triple encoder for sentence generation from rdf data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1627-1637, 2018.</li>
<li>Daniel Duma and Ewan Klein. Generating natural language from linked data: Unsupervised template extraction. In IWCS, pages 83-94, 2013.</li>
<li>Basil Ell and Andreas Harth. A language-independent method for the extraction of rdf verbalization templates. In INLG, pages 26-34, 2014.</li>
<li>Thiago Castro Ferreira, Diego Moussallem, Emiel Krahmer, and Sander Wubben. Enriching the webnlg corpus. In Proceedings of the 11th International Conference on Natural Language Generation, pages 171-176, 2018.</li>
<li>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 552-562, 2019.</li>
<li>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179-188. Association for Computational Linguistics, 2017.</li>
<li>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. Creating training corpora for nlg micro-planning. In Proceedings of ACL, 2017.</p>
</li>
<li>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, 2017.</p>
</li>
<li>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg challenge: Generating text from rdf data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, 2017.</li>
<li>Albert Gatt and Emiel Krahmer. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. arXiv preprint arXiv:1703.09902, 2017.</li>
<li>Sebastian Gehrmann, Falcon Dai, Henry Elder, and Alexander Rush. End-to-end content and plan selection for data-to-text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 46-56, Tilburg University, The Netherlands, November 2018. Association for Computational Linguistics.</li>
<li>Dimitra Gkatzia, Helen F Hastie, and Oliver Lemon. Comparing multi-label classification with reinforcement learning for summarisation of time-series data. In ACL (1), pages 12311240, 2014.</li>
<li>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1631-1640, 2016.</li>
<li>Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. Googleǖ̌̌̌̌s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339-351, 2017.</li>
<li>Lucie-Aimée Kaffee, Hady Elsahar, Pavlos Vougiouklis, Christophe Gravier, Frédérique Laforest, Jonathon Hare, and Elena Simperl. Mind the (language) gap: Generation of multilingual wikipedia summaries from wikidata for articleplaceholders. In European Semantic Web Conference, pages 319-334. Springer, 2018.</li>
<li>Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.</li>
<li>Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66-75, 2018.</li>
<li>Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attentionbased neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1412-1421. ACL, 2015.</li>
<li>Diego Marcheggiani and Laura Perez. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9. Association for Computational Linguistics, 2018.</li>
<li>Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, HLT-NAACL'16, pages 720-730, San Diego, California, 2016. Association for Computational Linguistics.</li>
<li>Simon Mille, Anja Belz, Bernd Bohnet, Yvette Graham, Emily Pitler, and Leo Wanner. The first multilingual surface realisation shared task (SR'18): Overview and evaluation results. In Proceedings of the First Workshop on Multilingual Surface Realisation, pages 1-12, Melbourne, Australia, July 2018. Association for Computational Linguistics.</li>
<li>
<p>Yassine Mrabet, Pavlos Vougiouklis, Halil Kilicoglu, Claire Gardent, Dina DemnerFushman, Jonathon Hare, and Elena Simperl. Aligning texts and knowledge bases with semantic sentence simplification. WebNLG 2016, 2016.</p>
</li>
<li>
<p>Axel-Cyrille Ngonga Ngomo, Michael Röder, Diego Moussallem, Ricardo Usbeck, and René Speck. Bengal: An automatic benchmark generator for entity recognition and linking. In Proceedings of the 11th International Conference on Natural Language Generation, pages 339-349, 2018.</p>
</li>
<li>Axel-Cyrille Ngonga Ngomo, Diego Moussallem, and Lorenz BÄijhman. A Holistic Natural Language Generation Framework for the Semantic Web. In Proceedings of the International Conference Recent Advances in Natural Language Processing, page 8. ACL (Association for Computational Linguistics), 2019.</li>
<li>Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. The E2E dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201-206, Saarbrücken, Germany, 2017.</li>
<li>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on Association for Computational Linguistics, 2002.</li>
<li>Maja Popović. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, 2017.</li>
<li>Ehud Reiter and Robert Dale. Building natural language generation systems. Cambridge university press, 2000.</li>
<li>Leonardo FR Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. Modeling global and local node contexts for text generation from knowledge graphs. arXiv preprint arXiv:2001.11003, 2020.</li>
<li>Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online, July 2020. Association for Computational Linguistics.</li>
<li>Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL'16, pages 1715-1725, Berlin, Germany, 2016. Association for Computational Linguistics.</li>
<li>Anastasia Shimorina, Elena Khasanova, and Claire Gardent. Creating a corpus for russian data-to-text generation using neural machine translation and post-editing. In Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 44-49, 2019.</li>
<li>Amin Sleimi and Claire Gardent. Generating paraphrases from dbpedia using deep learning. WebNLG 2016, page 54, 2016.</li>
<li>Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Multilingual neural machine translation with knowledge distillation. arXiv preprint arXiv:1902.10461, 2019.</li>
<li>Gongbo Tang, Mathias Müller, Annette Rios, and Rico Sennrich. Why self-attention? a targeted evaluation of neural machine translation architectures. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4263-4272, 2018.</li>
<li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</li>
<li>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</li>
<li>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048-2057, 2015.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/google/sentencepiece&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
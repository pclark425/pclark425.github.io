<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9017 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9017</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9017</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-274192378</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.14491v3.pdf" target="_blank">A Survey on Human-Centric LLMs</a></p>
                <p><strong>Paper Abstract:</strong> The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their ability to perform tasks traditionally performed by humans, namely those involving cognition, decision-making, and social interaction. This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics). We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills. Then, we explore real-world applications of LLMs in human-centric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions. Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration. This survey aims to provide a foundational understanding of LLMs from a human-centric perspective, offering insights into their current capabilities and potential for future development.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9017.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9017.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogiQA/GPT-3 Deductive Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogiQA 2.0 evaluation of GPT-3 (deductive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey reports GPT-3 evaluated on the LogiQA 2.0 deductive-reasoning benchmark, showing substantially lower accuracy than human benchmarks on complex logical inference tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive large transformer language model from OpenAI evaluated on natural-language logical inference tasks (as reported by the surveyed works).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>LogiQA 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A dataset for evaluating complex deductive logical reasoning in natural language, covering categorical, necessary/sufficient conditional, conjunctive, and disjunctive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>68.65% accuracy (reported for GPT-3 on logical inference tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>~90% accuracy (reported human benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM below human baseline (substantially lower than humans on these deductive tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used LogiQA 2.0 items focusing on multiple types of logical reasoning; results reported as accuracy. (Survey cites the LogiQA 2.0 evaluation.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Performance gap increases for multi-step and complex chained logical inferences; reported figure is dataset-specific and may not generalize to all deduction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9017.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9017.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inductive Reasoning (GPT-4, Davinci)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inductive/property-induction evaluations of GPT-4 and Davinci</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites studies evaluating LLMs on inductive/property-induction tasks: GPT-4 attains very high partial-rule-application scores in some settings, while other models (Davinci) can drop substantially on nuanced rule-validation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inductive reasoning in humans and large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; Davinci (GPT-3 family variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer LLMs evaluated on property-induction and rule-application tasks comparing model inferences to human inductive patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Property induction / inductive reasoning tasks (as used in cited studies)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks probing inductive generalization and rule application from examples (property induction), measuring how models generalize rules and capture premise non-monotonicity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4: up to 99.5% partial accuracy on rule-application in reported settings; Davinci: accuracy fell to ~51% on nuanced tasks (reported declines)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported numerically in survey for those specific items; cited work emphasizes human-like reasoning patterns but also differences (e.g., premise non-monotonicity not fully captured by models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed: GPT-4 can closely align with some human reasoning patterns and show very high partial-rule accuracy in constrained settings, while other LLM variants perform substantially worse on nuanced inductive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Studies used property-induction paradigms; Chain-of-Thought and other prompting strategies discussed as improving some outcomes; reported partial-accuracy and decline under rule-validation challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High partial accuracy can break down on larger problems or minimal-example regimes; models struggle with premise non-monotonicity and rule integration despite good surface performance in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9017.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9017.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalBench (causal reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CausalBench evaluation of causal reasoning in LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey reports CausalBench as a benchmark used to assess causal reasoning across models (LLaMA, OPT, InternLM, GPT-3.5, GPT-4, etc.), finding that models struggle on complex, text-based and code-related causal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causalbench: A comprehensive benchmark for evaluating causal reasoning capabilities of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-Turbo, LLaMA, OPT, InternLM, Fal-Con (as reported participants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various large pretrained LLMs evaluated on causal-inference tasks spanning abstract and contextualized domains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CausalBench</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmark assessing causal reasoning ability (cause-and-effect inference) across diverse domains including text-based and coding problems; metrics include F1, structural Hamming distance, and structural intervention distance.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as struggling on complex text-based and code-related causal problems; metrics reported in source include F1, SHD, and SID but specific numeric scores not provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs show limited causal reasoning on complex tasks and do not robustly match expected human-level causal inference in these domains (survey reports struggles).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Benchmark includes abstract and contextualized causal tasks; multiple model families evaluated; survey notes deterioration when arguments are flawed or conflicting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Causal reasoning often exceeds mere correlation detection and requires handling of conflicting evidence; even top models (GPT-4) show inconsistent causal inferences; human baselines not always reported in the cited benchmark (per survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9017.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9017.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory of Mind (ToM) tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Testing Theory of Mind in LLMs and humans (false belief, irony, faux pas tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites work that evaluates LLMs (e.g., GPT-4, GPT-3.5, LLaMA variants) on Theory-of-Mind tasks (false belief, indirect requests, irony, faux pas), reporting mixed performance with some strengths and clear limitations on nuanced social reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Testing theory of mind in large language models and humans</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5, LLaMA2-70B (models mentioned across cited ToM studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLMs assessed on tasks requiring attribution of beliefs, intentions, emotions, and understanding of others' mental states.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (LLaMA2-70B mentioned for some evaluations); other model sizes not specified in survey</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Theory of Mind tasks (false belief tasks, irony, faux pas, indirect requests)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A set of cognitive-psychology tests probing whether agents can infer others' beliefs, predict behavior from mental states, and understand social-pragmatic cues.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described as 'mixed': models (e.g., GPT-4) perform well on understanding indirect requests and some belief/emotion predictions but falter on nuanced social cues (false-belief reasoning, irony, faux pas).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided numerically in the survey; implied to be superior on nuanced ToM items (humans excel at subtle social cues).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed performance: LLMs can succeed on many ToM items but do not consistently match human-level nuance; lag on more complex or subtle social reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations used established ToM paradigms adapted to language prompts and role-play scenarios; some works used multi-agent simulated environments and role-playing to probe ToM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Survey highlights difficulty in nuanced social cues and potential anthropomorphism; models may appear capable on surface-level items yet fail deeper tests of mentalistic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9017.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9017.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SECEU / GPT-4 Emotional Understanding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SECEU psychometric Emotional Understanding test (as applied to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey reports SECEU, a psychometric evaluation of emotional understanding, found GPT-4 achieved emotional-intelligence scores higher than a large fraction of human participants in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal/text LLM evaluated on standardized emotional-understanding psychometric tasks (SECEU) per the survey's citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SECEU (Standardized Emotional Comprehension / Emotional Understanding psychometric test)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Psychometric evaluation of emotional understanding (empathic inference, emotion recognition and reasoning), using standardized EQ-like scoring compared to human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reportedly achieved higher emotional-intelligence scores than 89% of human participants (survey reports GPT-4 outperformed 89% of participants on SECEU).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants' distribution used as baseline (GPT-4 reported to exceed 89% of that participant sample); exact human mean/SD not provided in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms the majority of tested human participants on this specific standardized emotion-understanding metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Survey indicates use of standardized EQ scores and psychometric comparison; details of prompt format and sampling not enumerated in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Survey cautions that superior performance on psychometric emotion-recognition/understanding tests does not equate to genuine empathy or robust social-adaptive behavior in real-world interactions; ecological validity and cultural/contextual sensitivity remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9017.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9017.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LEAS / ChatGPT Emotional Awareness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levels of Emotional Awareness Scale (LEAS) evaluation of ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites a study using the LEAS psychometric scale and reports ChatGPT (ChatGPT/GPT-3.5 family) exceeded average human performance on emotional-awareness measures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt outperforms humans in emotional awareness evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (GPT-3.5-family conversational model) evaluated on emotional-awareness psychometric tasks (LEAS) and iterative learning modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Levels of Emotional Awareness Scale (LEAS)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A psychometric instrument measuring emotional awareness/empathy; assesses ability to describe and reason about emotional states in social scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to 'exceed average human performance' on LEAS in the cited study (no absolute numeric score provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Average human participant scores are referenced as baseline (exact numeric values not provided in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM outperforms average human participants on the LEAS in the cited evaluation, per the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Study used LEAS items as prompts and allowed iterative interactions; survey notes improvements via iterative learning but does not provide detailed prompting protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Despite LEAS results, survey emphasizes LLMs still lag in applying emotional understanding in complex, real-world social interactions; psychometric success does not imply genuine affective experience or robust in-situ empathy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on Human-Centric LLMs', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding <em>(Rating: 2)</em></li>
                <li>Inductive reasoning in humans and large language models <em>(Rating: 2)</em></li>
                <li>Causalbench: A comprehensive benchmark for evaluating causal reasoning capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Testing theory of mind in large language models and humans <em>(Rating: 2)</em></li>
                <li>Chatgpt outperforms humans in emotional awareness evaluations <em>(Rating: 2)</em></li>
                <li>Emobench: Evaluating the emotional intelligence of large language models <em>(Rating: 2)</em></li>
                <li>Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence <em>(Rating: 1)</em></li>
                <li>Using cognitive psychology to understand gpt-3 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9017",
    "paper_id": "paper-274192378",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "LogiQA/GPT-3 Deductive Reasoning",
            "name_full": "LogiQA 2.0 evaluation of GPT-3 (deductive reasoning)",
            "brief_description": "Survey reports GPT-3 evaluated on the LogiQA 2.0 deductive-reasoning benchmark, showing substantially lower accuracy than human benchmarks on complex logical inference tasks.",
            "citation_title": "Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Autoregressive large transformer language model from OpenAI evaluated on natural-language logical inference tasks (as reported by the surveyed works).",
            "model_size": null,
            "test_battery_name": "LogiQA 2.0",
            "test_description": "A dataset for evaluating complex deductive logical reasoning in natural language, covering categorical, necessary/sufficient conditional, conjunctive, and disjunctive reasoning.",
            "llm_performance": "68.65% accuracy (reported for GPT-3 on logical inference tasks)",
            "human_baseline_performance": "~90% accuracy (reported human benchmark)",
            "performance_comparison": "LLM below human baseline (substantially lower than humans on these deductive tasks)",
            "experimental_details": "Evaluation used LogiQA 2.0 items focusing on multiple types of logical reasoning; results reported as accuracy. (Survey cites the LogiQA 2.0 evaluation.)",
            "limitations_or_caveats": "Performance gap increases for multi-step and complex chained logical inferences; reported figure is dataset-specific and may not generalize to all deduction tasks.",
            "uuid": "e9017.0",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Inductive Reasoning (GPT-4, Davinci)",
            "name_full": "Inductive/property-induction evaluations of GPT-4 and Davinci",
            "brief_description": "Survey cites studies evaluating LLMs on inductive/property-induction tasks: GPT-4 attains very high partial-rule-application scores in some settings, while other models (Davinci) can drop substantially on nuanced rule-validation tasks.",
            "citation_title": "Inductive reasoning in humans and large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-4; Davinci (GPT-3 family variant)",
            "model_description": "Large pretrained transformer LLMs evaluated on property-induction and rule-application tasks comparing model inferences to human inductive patterns.",
            "model_size": null,
            "test_battery_name": "Property induction / inductive reasoning tasks (as used in cited studies)",
            "test_description": "Tasks probing inductive generalization and rule application from examples (property induction), measuring how models generalize rules and capture premise non-monotonicity.",
            "llm_performance": "GPT-4: up to 99.5% partial accuracy on rule-application in reported settings; Davinci: accuracy fell to ~51% on nuanced tasks (reported declines)",
            "human_baseline_performance": "Not reported numerically in survey for those specific items; cited work emphasizes human-like reasoning patterns but also differences (e.g., premise non-monotonicity not fully captured by models).",
            "performance_comparison": "Mixed: GPT-4 can closely align with some human reasoning patterns and show very high partial-rule accuracy in constrained settings, while other LLM variants perform substantially worse on nuanced inductive tasks.",
            "experimental_details": "Studies used property-induction paradigms; Chain-of-Thought and other prompting strategies discussed as improving some outcomes; reported partial-accuracy and decline under rule-validation challenges.",
            "limitations_or_caveats": "High partial accuracy can break down on larger problems or minimal-example regimes; models struggle with premise non-monotonicity and rule integration despite good surface performance in some metrics.",
            "uuid": "e9017.1",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "CausalBench (causal reasoning)",
            "name_full": "CausalBench evaluation of causal reasoning in LLMs",
            "brief_description": "Survey reports CausalBench as a benchmark used to assess causal reasoning across models (LLaMA, OPT, InternLM, GPT-3.5, GPT-4, etc.), finding that models struggle on complex, text-based and code-related causal tasks.",
            "citation_title": "Causalbench: A comprehensive benchmark for evaluating causal reasoning capabilities of large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-4, GPT-3.5-Turbo, LLaMA, OPT, InternLM, Fal-Con (as reported participants)",
            "model_description": "Various large pretrained LLMs evaluated on causal-inference tasks spanning abstract and contextualized domains.",
            "model_size": null,
            "test_battery_name": "CausalBench",
            "test_description": "Benchmark assessing causal reasoning ability (cause-and-effect inference) across diverse domains including text-based and coding problems; metrics include F1, structural Hamming distance, and structural intervention distance.",
            "llm_performance": "Reported as struggling on complex text-based and code-related causal problems; metrics reported in source include F1, SHD, and SID but specific numeric scores not provided in the survey.",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs show limited causal reasoning on complex tasks and do not robustly match expected human-level causal inference in these domains (survey reports struggles).",
            "experimental_details": "Benchmark includes abstract and contextualized causal tasks; multiple model families evaluated; survey notes deterioration when arguments are flawed or conflicting.",
            "limitations_or_caveats": "Causal reasoning often exceeds mere correlation detection and requires handling of conflicting evidence; even top models (GPT-4) show inconsistent causal inferences; human baselines not always reported in the cited benchmark (per survey).",
            "uuid": "e9017.2",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Theory of Mind (ToM) tests",
            "name_full": "Testing Theory of Mind in LLMs and humans (false belief, irony, faux pas tasks)",
            "brief_description": "Survey cites work that evaluates LLMs (e.g., GPT-4, GPT-3.5, LLaMA variants) on Theory-of-Mind tasks (false belief, indirect requests, irony, faux pas), reporting mixed performance with some strengths and clear limitations on nuanced social reasoning.",
            "citation_title": "Testing theory of mind in large language models and humans",
            "mention_or_use": "mention",
            "model_name": "GPT-4, GPT-3.5, LLaMA2-70B (models mentioned across cited ToM studies)",
            "model_description": "Large LLMs assessed on tasks requiring attribution of beliefs, intentions, emotions, and understanding of others' mental states.",
            "model_size": "70B (LLaMA2-70B mentioned for some evaluations); other model sizes not specified in survey",
            "test_battery_name": "Theory of Mind tasks (false belief tasks, irony, faux pas, indirect requests)",
            "test_description": "A set of cognitive-psychology tests probing whether agents can infer others' beliefs, predict behavior from mental states, and understand social-pragmatic cues.",
            "llm_performance": "Described as 'mixed': models (e.g., GPT-4) perform well on understanding indirect requests and some belief/emotion predictions but falter on nuanced social cues (false-belief reasoning, irony, faux pas).",
            "human_baseline_performance": "Not provided numerically in the survey; implied to be superior on nuanced ToM items (humans excel at subtle social cues).",
            "performance_comparison": "Mixed performance: LLMs can succeed on many ToM items but do not consistently match human-level nuance; lag on more complex or subtle social reasoning.",
            "experimental_details": "Evaluations used established ToM paradigms adapted to language prompts and role-play scenarios; some works used multi-agent simulated environments and role-playing to probe ToM.",
            "limitations_or_caveats": "Survey highlights difficulty in nuanced social cues and potential anthropomorphism; models may appear capable on surface-level items yet fail deeper tests of mentalistic inference.",
            "uuid": "e9017.3",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "SECEU / GPT-4 Emotional Understanding",
            "name_full": "SECEU psychometric Emotional Understanding test (as applied to LLMs)",
            "brief_description": "Survey reports SECEU, a psychometric evaluation of emotional understanding, found GPT-4 achieved emotional-intelligence scores higher than a large fraction of human participants in the cited work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large multimodal/text LLM evaluated on standardized emotional-understanding psychometric tasks (SECEU) per the survey's citation.",
            "model_size": null,
            "test_battery_name": "SECEU (Standardized Emotional Comprehension / Emotional Understanding psychometric test)",
            "test_description": "Psychometric evaluation of emotional understanding (empathic inference, emotion recognition and reasoning), using standardized EQ-like scoring compared to human participants.",
            "llm_performance": "Reportedly achieved higher emotional-intelligence scores than 89% of human participants (survey reports GPT-4 outperformed 89% of participants on SECEU).",
            "human_baseline_performance": "Human participants' distribution used as baseline (GPT-4 reported to exceed 89% of that participant sample); exact human mean/SD not provided in survey.",
            "performance_comparison": "LLM outperforms the majority of tested human participants on this specific standardized emotion-understanding metric.",
            "experimental_details": "Survey indicates use of standardized EQ scores and psychometric comparison; details of prompt format and sampling not enumerated in the survey text.",
            "limitations_or_caveats": "Survey cautions that superior performance on psychometric emotion-recognition/understanding tests does not equate to genuine empathy or robust social-adaptive behavior in real-world interactions; ecological validity and cultural/contextual sensitivity remain concerns.",
            "uuid": "e9017.4",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LEAS / ChatGPT Emotional Awareness",
            "name_full": "Levels of Emotional Awareness Scale (LEAS) evaluation of ChatGPT",
            "brief_description": "Survey cites a study using the LEAS psychometric scale and reports ChatGPT (ChatGPT/GPT-3.5 family) exceeded average human performance on emotional-awareness measures.",
            "citation_title": "Chatgpt outperforms humans in emotional awareness evaluations",
            "mention_or_use": "mention",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (GPT-3.5-family conversational model) evaluated on emotional-awareness psychometric tasks (LEAS) and iterative learning modifications.",
            "model_size": null,
            "test_battery_name": "Levels of Emotional Awareness Scale (LEAS)",
            "test_description": "A psychometric instrument measuring emotional awareness/empathy; assesses ability to describe and reason about emotional states in social scenarios.",
            "llm_performance": "Reported to 'exceed average human performance' on LEAS in the cited study (no absolute numeric score provided in survey).",
            "human_baseline_performance": "Average human participant scores are referenced as baseline (exact numeric values not provided in survey).",
            "performance_comparison": "LLM outperforms average human participants on the LEAS in the cited evaluation, per the survey.",
            "experimental_details": "Study used LEAS items as prompts and allowed iterative interactions; survey notes improvements via iterative learning but does not provide detailed prompting protocol.",
            "limitations_or_caveats": "Despite LEAS results, survey emphasizes LLMs still lag in applying emotional understanding in complex, real-world social interactions; psychometric success does not imply genuine affective experience or robust in-situ empathy.",
            "uuid": "e9017.5",
            "source_info": {
                "paper_title": "A Survey on Human-Centric LLMs",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding",
            "rating": 2,
            "sanitized_title": "logiqa_20an_improved_dataset_for_logical_reasoning_in_natural_language_understanding"
        },
        {
            "paper_title": "Inductive reasoning in humans and large language models",
            "rating": 2,
            "sanitized_title": "inductive_reasoning_in_humans_and_large_language_models"
        },
        {
            "paper_title": "Causalbench: A comprehensive benchmark for evaluating causal reasoning capabilities of large language models",
            "rating": 2,
            "sanitized_title": "causalbench_a_comprehensive_benchmark_for_evaluating_causal_reasoning_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Testing theory of mind in large language models and humans",
            "rating": 2,
            "sanitized_title": "testing_theory_of_mind_in_large_language_models_and_humans"
        },
        {
            "paper_title": "Chatgpt outperforms humans in emotional awareness evaluations",
            "rating": 2,
            "sanitized_title": "chatgpt_outperforms_humans_in_emotional_awareness_evaluations"
        },
        {
            "paper_title": "Emobench: Evaluating the emotional intelligence of large language models",
            "rating": 2,
            "sanitized_title": "emobench_evaluating_the_emotional_intelligence_of_large_language_models"
        },
        {
            "paper_title": "Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence",
            "rating": 1,
            "sanitized_title": "both_matter_enhancing_the_emotional_intelligence_of_large_language_models_without_compromising_the_general_intelligence"
        },
        {
            "paper_title": "Using cognitive psychology to understand gpt-3",
            "rating": 1,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        }
    ],
    "cost": 0.020497249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Human-Centric LLMs
1 Dec 2024</p>
<p>Y I Jing 
Wang 
Jing Yi Wang 
Tong Li tongli@mail.tsinghua.edu.cn </p>
<p>Tsinghua University
China</p>
<p>NICHOLAS SUKIENNIK*
Tsinghua University
China</p>
<p>Tsinghua University
China</p>
<p>Tsinghua University
BeijingChina</p>
<p>Nicholas Sukiennik*</p>
<p>Tsinghua University
BeijingChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>Tsinghua University
Weikang SuBeijingQianyue HaoChina</p>
<p>Tsinghua University
BeijingJingbo XuChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>Zihan Huang
Tsinghua University
BeijingChina</p>
<p>Tsinghua University
Beijing, Yong LiChina</p>
<p>Tsinghua University
BeijingChina</p>
<p>A Survey on Human-Centric LLMs
1 Dec 20249EA9ADF06B3912ED2793F10BDFAFCB1AarXiv:2411.14491v3[cs.CL]Large Language Models, Human-Centered Computing Executive Function Cognition Perception Analysis Sociability
The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their ability to perform tasks traditionally performed by humans, namely those involving cognition, decisionmaking, and social interaction.This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics).We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills.Then, we explore real-world applications of LLMs in humancentric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions.Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration.This survey aims to provide a foundational understanding of LLMs from a human-centric perspective, offering insights into their current capabilities and potential for future development.</p>
<p>INTRODUCTION</p>
<p>As large language models (LLMs) [1,2], such as OpenAI's GPT family [3,4] and Meta's LLaMA [5,6], continue to evolve, their ability to simulate, analyze, and influence human behavior is growing at an unprecedented rate.These models can now process and generate human-like text and perform cognitive tasks at levels comparable to humans in many situations, providing new tools for understanding human cognition, decision-making, and social dynamics.</p>
<p>As such, this survey aims to provide a comprehensive evaluation of LLMs from a human-centric perspective, focusing on their ability to simulate, complement, and enhance human cognition and behavior, both on an individual and collective level.While LLMs have traditionally been rooted in computer science and engineering [7,8], their increasing sophistication in replicating human-like reasoning, decision-making, and social interactions has expanded their use into domains where humans are the focal point.This has allowed researchers to address questions that were once too intricate or abstract for computational analysis.For example, in political science, LLMs are used interdisciplinary scenarios to both enhance LLM development and assist in human-centered tasks.</p>
<p>Section 5 explores open challenges and outlines future directions for advancing LLMs.Section 6 summarizes key insights and emphasizes the importance of interdisciplinary collaboration to enhance LLMs' understanding of human behavior.</p>
<p>OVERVIEW</p>
<p>2.1 Human-Centric Artifical Intelligence 2.1.1Traditional AI Approaches in Human-Centric Studies.The application of AI in various humancentered fields has undergone a long progression, now reaching a pinnacle with the rise of generative models, with AI methods to being used investigate various human phenomena.however, despite their relative naivety compared to LLMs, those traditional methods have nonetheless enabled researchers to address complex social phenomena through computation.</p>
<p>For almost as long as it has been investigated, AI has been used in areas that are highly impactful on society [14].Since then researchers have evaluated the many ways in which AI could emulate human behavior and thought procession, for example in cognition [15], perception [16], and executive function [17].More recently, though, with the rise of the web and social media, AI's uses come closer to our day-to-day lives.For example, in political communication research, the detection of political bias in news articles has emerged as a critical area of study, particularly given the increasing polarization in media and online spaces.Traditional methods for predicting political ideology, based on statistical modeling and network analysis, have become an urgent task due to the vast amount of content produced daily.For instance, research by [18] employed network analysis to estimate ideological preferences of social media users.Moreover, techniques like topic modeling and content analysis have been widely used to identify bias and misinformation in news articles using data-mining methods [19,20], highlighting the use of traditional AI techniques in understanding political discourse.Other works tackled the task of stance detection using methods like recursive neural networks [21] and clustering algorithms [22].Furthermore, Dezfouli et al. [23] explore adversarial vulnerabilities in decision-making models, which is crucial when considering the robustness of traditional bias detection systems under adversarial conditions.Furthermore, Dafoe et al. [24] emphasize the importance of systems designed to navigate social environments, such as political discourse, using more established multi-agent systems and game theory frameworks.Meanwhile, machine understanding of human preferences has also been used to optimize the learning of reward functions in reinforcement learning [25], showing us that AI methods not only help us explain human behavior, but can benefit by understanding them, highlighting the co-evolutionary nature of advancements in both AI techniques and human-centric studies.</p>
<p>Overall, the vast body of AI-empowered human-centric studies point to the burgeoning potential of using more advanced computational methods, such as LLMs, to both understand and better simulate human behavior and reasoning processes.LLMs can present new opportunities in the field by simulating human behaviors in areas where real-world data is scarce, as well as facilitate inquiry into laws and dynamics of human behavior based on LLM replicability.</p>
<p>A Paradigm Shift from</p>
<p>Traditional AI to LLMs.The rise of LLMs has transformed natural language processing (NLP) and artificial intelligence in general through key breakthroughs in model architecture, scale, and capabilities.Early models like Word2Vec and GloVe used word embeddings, but the introduction of the Transformer in 2017 [26], with its self-attention mechanism, enabled deeper contextual understanding and marked a turning point.OpenAI's GPT series, beginning in 2018 with GPT [3], capitalized on this, culminating in GPT-3 [27] and GPT-4 [28], which demonstrated unprecedented capabilities in reasoning, text generation, and multimodal tasks.Meanwhile, Google's PaLM 2 [29] advanced multilingualism and efficiency, and open-source models like Falcon [30] and Baidu's ERNIE Bot [31] broadened access and specialization.These developments reflect the growing impact of LLMs across diverse domains, from interdisciplinary research to ethical AI applications.</p>
<p>The rapid adoption of LLMs across academic disciplines has led to varying predictions about whether these systems will eventually match human cognitive abilities.While some experts foresee AI achieving human-like general intelligence in the near future, others remain more cautious, doubting whether AI can fully replicate the complex, abstract reasoning and creativity that define human cognition [32].Despite these differing viewpoints, AI is already a significant force in everyday life, influencing decision-making and information processing across numerous domains.However, a key distinction remains: human cognition is driven by forward-thinking, theory-based reasoning, while AI operates on patterns derived from vast datasets, often relying on probability and past data [33].This difference underscores the complementary nature of human and AI systems, with each excelling in distinct aspects of cognitive processing.</p>
<p>Unlike human intelligence, LLMs operate without inherent goals, values, or emotional experiences.Human cognition, driven by survival, social interaction, and creativity, is deeply connected to our physical and social environments.Even embodied AI, while capable of interacting with its surroundings, lacks the nuanced, purpose-driven intelligence that defines human thought.In contrast, LLMs generate responses based on probabilistic models derived from large datasets, without the lived experiences that inform human decision-making.Though LLMs can simulate certain human-like behaviors, they still fall short of the embodied understanding humans possess.</p>
<p>These distinctions raise critical questions about the limitations and potentials of AI, especially as we consider the diverse capabilities explored in Section 3, which discusses the capabilities of LLMs including cognitive, perceptual, social, analytical, executive, cultural, moral, and collaborative skills.Section 4 delves into how interdisciplinary fields, such as political science, economics, sociology, behavioral science, psychology, and linguistics, contribute to LLM development, offering insights into how human intelligence informs and shapes the evolution of artificial systems.This exploration emphasizes the importance of leveraging LLM strengths while recognizing the fundamental differences between human and artificial cognition.</p>
<p>EVALUATION OF HUMAN-CENTRIC LLMS</p>
<p>To evaluate human-centric LLMs, we showcase a holistic representation of LLM competencies, categorized into two domains: individual (e.g., cognitive, perceptual, analytical, executive functioning skills) and collective (e.g., social skills), as shown in Figure 2.This representation includes various key LLM skills, such as reasoning, pattern recognition, spatial awareness, adaptability, decision-making, interpersonal communication, and cultural competency.Following this, Figure 3 outlines the evaluation approaches used to assess LLMs, including benchmark and dataset testing, human-centric evaluations, interactive and simulation-based evaluations, ethical and bias assessments, and lastly, explainability and interpretability evaluations.Table 1 highlights both the strengths and areas for improvement in these domains.By outlining these abilities, we provide a comprehensive comparison of human-like skills, using benchmarks to assess their strengths and limitations.Additionally, Appendix Tables 2 and 3 provide a comprehensive overview of key papers, highlighting their contributions, the LLMs assessed, and comparisons to human performance.The subsequent section delves into each category, providing an in-depth exploration of the skills and benchmarks that define LLM performance across these domains.</p>
<p>Cognitive Skills</p>
<p>LLMs demonstrate cognitive competencies that mirror key elements of human intelligence, primarily through reasoning and learning.While LLMs show remarkable ability in processing vast amounts of information and generating coherent responses, their proficiency varies when it comes to complex cognitive tasks.These models showcase evolved abilities in structured reasoning and generalization but encounter challenges when faced with intricate logic or learning from real-time interactions.This section explores the strengths and limitations of LLMs in reasoning and learning, highlighting their progress and areas that require further advancement.</p>
<p>3.1.1Reasoning.Logical reasoning, a core element of human cognition and essential for daily functioning, consists of various types of reasoning, including deductive, inductive, and causal reasoning, each contributing to how we process information and make decisions.Deductive reasoning applies general principles to obtain specific conclusions, while inductive reasoning draws generalizations from specific observations [34], and causal reasoning helps to understand cause-and-effect relationships [35,36].Several benchmark datasets have been developed to assess these reasoning capabilities in LLMs.For deductive reasoning, the LogiQA 2.0 dataset [37] is a notable resource, focusing on five types of reasoning, including categorical, necessary conditional, sufficient conditional, conjunctive, and disjunctive reasoning.PrOntoQA [38] also evaluates deductive reasoning through first-order logic tasks where LLMs derive specific conclusions from logical premises.For inductive reasoning, CommonsenseQA 2.0 [39] requires generalization from everyday facts and commonsense knowledge, whereas the Creak dataset [40] further tests LLMs' ability to generalize from commonsense knowledge to identify inconsistencies.In turn, causal reasoning is assessed using CausalBench [41], which evaluates LLMs' ability to reason about cause-and-effect relationships across diverse domains.ContextHub [42], on the other hand, serves as another benchmark focusing on LLMs' causal reasoning in both abstract and contextualized tasks.Additional datasets like GSM8K [43] and BIG-Bench-Hard [44] are furthermore employed for mathematical reasoning and evaluating LLM performance across various reasoning domains, respectively.</p>
<p>Analyzing LLM performance with these datasets has revealed significant insights into their reasoning abilities and limitations.For deductive reasoning, although LLMs like GPT-3 have made progress, their accuracy remains at 68.65% in tasks involving logical inference, which is significantly below the 90% human benchmark [37].This gap indicates ongoing challenges in mastering complex logical structures, especially when multiple logical steps or intricate reasoning processes are required.LLMs like GPT-3.5, PaLM, and LLaMA perform well on simpler deductive reasoning tasks but struggle with more complex scenarios that involve chaining multiple logical premises together [45].For inductive reasoning, on the other hand, GPT-4 shows improvements in rule application with up to 99.5% partial accuracy [46], yet struggles with larger problems and minimal examples.Even with Chain-of-Thought (CoT) prompting, GPT-4 and Davinci face difficulties in rule validation and integrating complex rules, with Davinci's accuracy declining to 51% in nuanced tasks [47].In addition, Han et al. [47] evaluate GPT-3.5 and GPT-4 on property induction tasks, highlighting that while GPT-4 more closely aligns with human reasoning patterns, they still struggle to fully capture premise non-monotonicity, a critical element of human cognitive processing.</p>
<p>Causal reasoning remains a significant challenge for LLMs like GPT-4 and Davinci, as it requires a deep understanding of cause-and-effect across various contexts.Although these models show reasonable proficiency in mathematical causal tasks, the CausalBench benchmark highlights their struggles with more complex text-based and coding-related causal problems [41].Interpreting causal structures in narratives or code snippets often goes beyond simple data correlations, demanding robust reasoning to avoid producing misleading outputs.Even when GPT-4 initially performs well,  its reasoning capabilities frequently weaken when faced with flawed or conflicting arguments, raising concerns about its consistency in complex scenarios [48].</p>
<p>LLM Evaluations</p>
<p>The ContextHub benchmark is developed to assess LLMs like GPT-4, PaLM, and LLaMA in handling both abstract and contextualized logical problems [42].ContextHub focuses on the challenges these models encounter when transitioning from simple logic tasks to nuanced, realworld reasoning.While models perform well with straightforward problems, they often struggle to generalize in context-rich scenarios requiring deeper interpretative skills.Additional datasets like GSM8K emphasize deductive reasoning, and BIG-Bench-Hard evaluates multi-step reasoning, factual knowledge, and commonsense understanding [43,44].Together, these benchmarks reveal critical insights into the strengths and limitations of models like GPT-4 and Davinci, pinpointing areas that need improvement for handling complex, real-world reasoning tasks.</p>
<p>Overall, these benchmark datasets provide a comprehensive evaluation framework for assessing LLMs' reasoning capabilities, revealing both their advancements and limitations.While LLMs have shown progress in handling specific reasoning tasks, they continue to face significant challenges in multi-step logic, contextual problem-solving, and generalizing their reasoning abilities across diverse domains.</p>
<p>3.1.2Learning.LLMs' learning ability encompasses their capacity to adapt, generalize, and improve performance based on pre-existing training data and interactions with users or environments.Unlike traditional learning models, LLMs do not update their parameters during inference.Instead, they rely on pre-trained knowledge to perform few-shot or zero-shot tasks, highlighting their generalization capabilities.However, this comes with significant limitations when faced with evolving, real-world data.</p>
<p>Recent efforts have aimed at improving LLM adaptability through various strategies.For instance, the RL with Guided Feedback (RLGF) framework [49] optimizes learning from feedback, showing that guided strategies can significantly improve text generation in dynamic conditions.Similarly, error-driven learning approaches, like LEMA (Learning from MistAKes) [50], allow models like GPT-4 to refine reasoning by identifying and correcting errors.These approaches highlight the potential of leveraging feedback and error correction to boost adaptability, yet they still rely on static data at inference.While these feedback-based methods provide immediate gains in performance, they do not address the broader challenge of continual learning in real-time.Jovanovic et al. [51] offer a critical review of incremental learning paradigms, noting that current models, including GPT-4 and GPT-3.5, struggle with dynamic, real-time updates.Though these models excel in handling pre-defined tasks, their performance declines on more complex, evolving instances, reflects a broader limitation of inability to adapt to real-time adaptability.</p>
<p>The fine-tuning approaches explored by Ren et al. [52], which aim to clarify instruction-based and preference-based learning, also highlight challenges in maintaining model alignment without overfitting.Although models show improvement through fine-tuning, they still fall short in dynamic environments where adaptability and continual learning are crucial.</p>
<p>In general, these studies reveal both the strengths and limitations of LLMs in learning.While frameworks like RLGF [49] and LEMA [50] demonstrate short-term improvements in adaptability and error correction, and fine-tuning helps refine models, the inability to learn incrementally and in real-time remains a critical obstacle.Future research will need to focus on overcoming this gap to enable LLMs to fully adapt to real-world, evolving tasks, which is why several online-deployed models have begun to include a web search component to collect more recent relevant information [53,54].</p>
<p>Perceptual Skills</p>
<p>Perceptual skills in LLMs involve their ability to interpret, organize, and process complex information from their environment, which serves as a foundation for navigating and interacting within both virtual and real-world contexts.These skills are broadly divided into two critical areas: pattern recognition and spatial awareness.Pattern recognition focuses on the LLM's capability to identify and understand recurring structures, semantic relationships, and trends within data, which are essential for tasks involving prediction and interpretation.Spatial awareness, on the other hand, relates to the LLM's ability to comprehend spatial configurations, positions, and relationships between objects within defined environments, crucial for navigation and interaction tasks.Together, these perceptual skills lay the foundation for LLMs to engage effectively in complex scenarios, although challenges remain in achieving human-level proficiency, particularly in dynamic and unpredictable settings.Deductive reasoning: LogiQA 2.0 [37], PrOn-toQA [38]; Inductive reasoning: CommonsenseQA 2.0 [39], Creak [40]; Causal reasoning: CausalBench [41], Contex-tHub [42]; Mathematical reasoning: GSM8K [43]; General reasoning: BIG-Bench-Hard [44] Learning LLMs showcase strengths in generalization and feedback-based adaptation but lack traditional online learning and real-time parameter updates.</p>
<p>CommonGen tasks [49] , GSM8K [50], MATH [50], SVAMP [50], ASDiv [50], CIFAR-100 [51], ImageNet100 [51] Perceptual Skills</p>
<p>Pattern Recognition</p>
<p>LLMs achieve state-of-the-art performance in 3D shape understanding, approaching human-level performance.</p>
<p>ScanQA [55] Spatial Awareness LLMs demonstrate strong spatial awareness in structured virtual environments but struggle with adaptability and decision-making in dynamic real-world contexts, revealing limitations in flexible thinking and real-time processing.</p>
<p>Red Dead Redemption 2 (RDR2) [56], MP3D (Matterport3D) [57] Analytical Skills</p>
<p>Data Interpretation</p>
<p>LLMs are skilled at data extraction, visualization, and structured analysis but remain limited in handling complex, unstructured data, particularly in nuanced entity recognition and relation extraction, compared to human performance.</p>
<p>MOF IE [58], CORD-19 Direction Extraction [59] Information Processing LLMs are proficient in data integration, synthesis, and filtering, outperforming traditional deep neural networks with their natural language understanding, but they still lag behind humans in contextual and domainspecific comprehension.</p>
<p>DOC-BENCH [60], Cocktail [61] Creativity LLMs showcase advances in rapid, high-quality idea generation but lack human-like intuition, emotional depth, and true creative originality.</p>
<p>Purchase Intent Scores [62] Executive Functioning</p>
<p>Adaptability and Flexibility</p>
<p>LLMs demonstrate increasing adaptability and flexibility in structured tasks, handling real-time changes, but face significant challenges in dynamic, unpredictable environments and remain vulnerable to manipulation and harmful content extraction.</p>
<p>LLM-CI [63] Planning and Organization LLMs illustrate competency in generating and adapting plans but require external validation for reliability and logical consistency in complex tasks.</p>
<p>Embodied Agent Tasks [64] Decision-Making LLMs show considerable competency in decision-making, particularly in risk assessment and managing uncertainty, yet they struggle with cognitive biases inherent in their training data and require more robust mechanisms for responsible use in high-stakes and uncertain situations.</p>
<p>-Bench [65], BIASBUSTER [66] Social Skills</p>
<p>Interpersonal Communication</p>
<p>LLMs demonstrate significant progress in simulating human-like social interactions, excelling in structured scenarios and Theory of Mind tasks, but they still struggle with the depth and adaptability required for nuanced, real-world social dynamics.</p>
<p>False Belief Task [67], Book Genre Typicality [68] Emotional Intelligence LLMs outperform human participants in identifying and interpreting emotions.</p>
<p>LEAS [69], EIBENCH [70], EmoBench [71], SECEU [72] Collaboration LLMs showcase advances in collaboration and teamwork, but they continue to face challenges in matching the adaptability, strategic flexibility, and contextual awareness of human teams, particularly in complex and dynamic situations.</p>
<p>AI-to-AI: LLM-Arena [73], RoCoBench [74]; Human-to-AI: GOVSIM [75], Overcooked-AI [74] Cultural Competency LLMs show promise in understanding and simulating cultural behaviors, especially in recalling factual knowledge, but they struggle with nuanced cultural sensitivity, particularly for underrepresented cultures, where humans consistently outperform them in adapting responses based on complex cultural contexts.</p>
<p>BLEND [76], HCD [77] 3.2.1 Pattern Recognition.Pattern recognition refers to a model's ability to identify, interpret, and categorize recurring patterns within data [78].In the context of LLMs, this capability is crucial for recognizing trends, making predictions, and understanding complex multimodal inputs that combine language, vision, and other sensory information.Evaluating pattern recognition involves assessing a model's proficiency in object recognition, spatial inference, and semantic understanding-core skills required for interpreting both real-world and simulated environments [78,79].This ability can be analyzed through three critical aspects.Object identification and classification refers to the process where models categorize items based on their distinct features, enabling accurate recognition [80].Semantic and spatial relationship mapping involves understanding the relationships between objects and their spatial arrangements, which is essential for contextually accurate reasoning and interaction [81].Finally, temporal dynamics focuses on recognizing sequences of events and their interdependencies, allowing models to grasp patterns over time [81].These aspects highlight the complexity of pattern recognition in LLMs, showcasing their potential while emphasizing the challenges in achieving human-like perception and reasoning.</p>
<p>To evaluate these capabilities, several works have employed unique methods to help models generalize and recognize patterns in complex environments.For example, the R3M framework by Nair et al. [82] employ pre-trained visual representations with time-contrastive learning and videolanguage alignment, enhancing temporal dynamics in robotic manipulation and achieving over 20% improvement on benchmarks like Adroit and MetaWorld.Building on these ideas, ConceptGraphs by Qiao et al. [83] integrate 2D foundation model features into 3D scene graphs, advancing tasks like navigation and object manipulation by efficiently recognizing spatial relationships.Additionally, Qi et al. [84] introduce ShapeLLM, which combines visual-language alignment with multi-view image distillation to enhance 3D geometry understanding, useful for embodied interaction tasks like 3D visual grounding.Similarly, 3D-LLM by Hong et al. [55] incorporate 3D spatial understanding, significantly boosting performance in 3D captioning and object navigation using over 1 million 3Dlanguage pairs, whereas Huang et al. [85] develop the Inner Monologue system, fusing perception and control through real-time feedback to adapt behavior in dynamic environments.Lastly, Voltron by Karamcheti et al. [86] leverage language descriptions and video frames for visual representation learning, surpassing earlier frameworks like MVP and R3M in both low-level pattern recognition and high-level semantic reasoning.</p>
<p>These studies demonstrate the growing integration of visual, spatial, and temporal understanding into LLMs, significantly advancing their ability to recognize complex patterns, generalize across environments, and effectively interact with real-world tasks.</p>
<p>Spatial</p>
<p>Awareness.Spatial awareness in LLMs refers to their ability to interpret, analyze, and understand spatial configurations, positions, and relationships between objects within defined environments.This skill is essential for tasks involving precise navigation, object identification, and interaction, both in virtual and physical contexts.</p>
<p>LLMs have shown remarkable capabilities in structured virtual environments.For instance, Zhao et al. [87] develop the STEVE framework within Minecraft, demonstrating LLMs' effectiveness in tasks like block search and tech tree mastery.This highlights LLMs' ability to understand spatial configurations and object relationships in virtual spaces, enabling efficient interaction.Building on these capabilities, Tan et al. [56] introduce the CRADLE framework, where LLMs interpret visual cues from game screenshots to perform context-aware actions in games like Red Dead Redemption 2. These advancements illustrate significant progress in spatial reasoning, especially in digital environments.</p>
<p>Despite these advances, LLMs face challenges when transitioning from controlled virtual settings to dynamic real-world contexts.Wang et al. [88] develop the RoboGen system to evaluate spatial reasoning in physical environments, where LLMs need to adapt to unpredictable scenarios.While they show autonomy in generating robotic tasks and adapting to changes, their performances lack the precision and adaptability of human spatial awareness, especially in real-time adjustments.Liu et al. [89] explore these challenges further with the AerialVLN framework for UAV navigation in urban environments.Although LLMs effectively follow natural language instructions and recognize spatial landmarks, their handling of complex, real-world navigation remains limited.This emphasizes the gap in LLMs' ability to generalize across diverse and dynamic environments.Schumann et al. [90] and Zhang et al. [91] provide further evidence of these limitations in real-world navigation, showing that while LLMs can interpret spatial cues and adapt to changing conditions, they struggle with continuous decision-making and situational awareness.</p>
<p>The findings from the works about show that LLMs have made important processes in spatial awareness, particularly in structured virtual environments.However, their transition to real-world applications reveals substantial gaps in adaptability and decision-making.They perform well in tasks with predefined rules and clear spatial cues but encounter difficulties in unpredictable environments requiring flexible thinking and real-time processing.Bridging these gaps requires further advancements in contextual adaptability and learning mechanisms in order to bring LLMs closer to human-level spatial proficiency.</p>
<p>Analytical Skills</p>
<p>LLMs demonstrate strong analytical skills in areas such as data interpretation, creativity, and information processing.Below we outline notable works that evaluate LLMs' abilities in each of these areas.</p>
<p>Data</p>
<p>Interpretation.Data interpretation refers to the ability of LLMs to analyze, interpret, and extract meaningful insights from structured and unstructured data.This capability is crucial in fields such as scientific research, where data-driven decision-making relies on accurately understanding and processing complex datasets [59].Data interpretation in LLMs can be measured through aspects like structured data extraction, data visualization, and reasoning over data to generate actionable insights.</p>
<p>The assessment of data interpretation typically involves evaluating LLMs' ability to handle different data formats, understand domain-specific contexts, and perform accurate computations or visualizations based on the data.Benchmarks such as scientific papers, domain-specific datasets (e.g., chemistry, biology), and tools like MatPlotBench [92] are commonly used to assess these abilities, focusing particularly on visualization and data comprehension.</p>
<p>A recent development in this field is the Data Interpreter model by Hong et al. [93], an LLMbased agent designed to handle complex data science tasks through hierarchical dynamic planning and real-time data adaptation.The model demonstrates significant improvements in managing data dependencies and optimizing machine learning workflows, achieving a 26% enhancement in mathematical problem-solving and a 112% improvement in open-ended tasks.These results highlight the possible enhancement for LLMs to adapt to dynamic data relationships, a key challenge in data interpretation.</p>
<p>Data visualization, another aspect of data interpretation, has also been addressed using LLMs.MatPlotAgent [92] automates the generation of complex visualizations from user queries and raw data, iteratively refining output through feedback.This framework demonstrates the potential of LLMs to interpret data and produce high-quality visual representations, further aiding in data comprehension and analysis.</p>
<p>Dagdelen et al. [58] explore structured information extraction using LLMs to create knowledge bases from scientific texts.By combining named entity recognition with relation extraction, they successfully transform unstructured research papers into structured data that can be used in downstream applications like machine learning.This contributes significantly to automating the research process, making vast amounts of scientific literature accessible in structured formats.</p>
<p>From the above findings, it is clear that data interpretation is a key capability of LLMs, empowering them to process, analyze, and visualize complex datasets across domains.The assessments conducted in these works highlight LLMs' growing proficiency in extracting and leveraging insights from data, proving their value as helpful assistants to humans in various aspects of data-driven scientific research.</p>
<p>Information Processing.</p>
<p>LLMs demonstrate advanced analytical capabilities in information processing, distinguishing themselves through their ability to handle data integration, synthesis, and filtering.Unlike previously discussed perceptual skills, which primarily involve interpreting sensory inputs, these analytical functions rely on deeper cognitive processing, leveraging natural language understanding and generation to manage data in sophisticated ways beyond traditional neural networks.</p>
<p>While deep neural networks have proven effective in analytics and management, they often struggle with the generalization and semantic understanding required as data scales up.LLMs overcome these limitations by better processing language, allowing users to communicate with the system in a way that feels natural and efficient.For example, Zhang et al. [94] demonstrate how LLMs can perform data interpolation and detection through sophisticated prompt techniques, showing how they support human decision-making by simplifying data preprocessing tasks that would otherwise require more manual input.Additionally, interactive systems like NL2Rigel [95] allow users to refine tables from semi-structured text using natural language instructions, exemplifying how LLMs facilitate intuitive and efficient data synthesis.</p>
<p>LLMs also address challenges such as data scarcity by generating high-quality synthetic data.Auto-regressive generative LLMs, for instance, can create realistic tabular datasets, providing valuable solutions when real-world data is incomplete or unavailable [96].This capability to simulate realistic scenarios allows users to work with more robust datasets, further enhancing decision-making processes.</p>
<p>To further evaluate LLM-based document processing, the DOCBENCH benchmark [60] is introduced, specifically designed for assessing LLMs in tasks involving document reading, metadata extraction, and multi-modal information understanding.This benchmark reveals that while LLMs have made significant progress in document processing, they still lag behind human performance, particularly in scenarios that require nuanced comprehension and adaptability across diverse real-world contexts.In information filtering and relevance determination, on the other hand, LLMs surpass traditional recommender systems and search engines by enabling active user engagement and personalized information filtering [97].Eigner et al. [98] demonstrate that LLMs enhance decision-making quality in human-AI collaborations by effectively prioritizing and determining relevance in complex data streams.</p>
<p>In mixed-source environments, benchmarks such as Cocktail [61] assess LLMs' ability to process both human-and machine-generated content, highlighting the complexities of ensuring accuracy and fairness in information retrieval.While LLMs demonstrate strong capabilities in managing large and diverse datasets, they also emphasize the need for a balanced approach to address the variability and biases inherent in mixed-source data.</p>
<p>In sum, LLMs represent a significant step forward in information processing, offering enhanced capabilities in integrating, synthesizing, and filtering data.Their ability to engage with users in natural, intuitive ways emphasizes a more human-centered approach to technology, one that extends traditional data models by aligning more closely with how humans interact with and make sense of complex information.Yet, as these systems evolve, their ongoing development will need to focus on refining the balance between machine efficiency and human-like adaptability in increasingly dynamic and nuanced environments.</p>
<p>Creativity.</p>
<p>Large, pre-trained models exhibit remarkable creativity, generating fresh ideas and unique outputs across various domains from art to science, fundamentally transforming the creative industries and pushing the boundaries of human imagination.Girotra et al. [99] find that LLMs excel in generating innovative ideas, surpassing humans in both speed and quality, even outperforming students from elite universities in creating high-purchase-intent ideas.Similarly, Xu et al. [62] propose an LLM-enhanced design template that promotes critical thinking and idea iteration during the creative process, offering a template to guide users to analyze problems deeply and improve their creativity.</p>
<p>After reviewing 110 studies, Li et al. [100] discover that LLMs are increasingly used for advanced tasks like software development and creative content generation, lowering barriers to human-AI collaboration, particularly in industries like scriptwriting and advertising.In a related effort to enhance LLM creativity, Lu et al. [101] propose the "LLM Discussion" framework, which enhances creativity through multi-round role-playing discussions.This three-stage process-initiation, discussion, and convergence-guides LLMs to generate more innovative solutions, outperforming both single LLM methods and existing multi-model frameworks in creativity benchmarks.Together, these studies highlight LLMs' transformative potential in innovation and creativity.</p>
<p>Overall, these four studies demonstrate the transformative potential of LLMs in innovation and creativity, showing their superiority in idea generation, enhancing critical thinking, advancing human-AI collaboration, and improving creative output in traditionally human-dominated tasks.</p>
<p>Executive Functioning</p>
<p>Executive functioning in LLMs involves skills like adaptation, planning, and decision-making, enabling models to manage complex tasks and respond to changing conditions.This section examines these aspects of executive functioning, highlighting both strengths and areas for improvement.Contextual Adaptation refers to how well LLMs modify their behavior in response to evolving contexts.Luu et al. [102] demonstrate that LLMs can leverage past experiences to break tasks into context-sensitive components, achieving promising adaptability in controlled settings.However, these models falter in more dynamic, unstructured environments.Similarly, Kim et al. [103] show that LLMs guiding robotic agents through changing environments in the Dynacon system, adjusting to real-time inputs without explicit instructions.Despite this progress, LLMs still struggle to match the nuanced, context-aware decision-making humans exhibit when fast, flexible responses are needed.</p>
<p>To benchmark these capabilities, researchers have employed frameworks like LLM-CI [63], which evaluates how well LLMs align with societal norms and navigate human interactions.Although LLMs can partially understand the rules guiding human behavior, they lack depth in contextual understanding, particularly in social settings where subtle cues and ethical considerations are involved.This gap underscores LLMs' broader limitations in adapting to complex, real-world scenarios.</p>
<p>Adaptability also involves handling uncertainty and overcoming obstacles in decision-making.Eigner et al. [98] evaluate LLMs across factors like task difficulty and psychological influences, finding that models perform well in structured contexts but struggle with complexity and unpredictability.Zhang et al. [104] expose weaknesses by showing how interrogation techniques can force LLMs to produce harmful content even after alignment training, highlighting limitations in adapting to malicious manipulation.On the technical side, the Ctrl-G framework [105] improves adaptability by combining LLMs with Hidden Markov Models to generate constrained text output.This enhances LLM performance in specific tasks, though their ability to adapt to more open-ended scenarios remains limited.</p>
<p>While LLMs demonstrate increasing adaptability in structured tasks, significant challenges remain in replicating human-level flexibility in real-world situations.Benchmarks like those developed by Luu et al. and Kim et al. show that LLMs can adjust well in predictable, structured environments, but their limitations become clear in dynamic, uncertain contexts.These findings indicate that while LLMs show potential, their adaptability is still far from human capabilities, particularly in managing complexity and ambiguity.Future research should focus on improving learning mechanisms and algorithms that allow LLMs to better handle uncertainty and mimic human cognitive flexibility.</p>
<p>3.4.2Planning and Organization.LLMs possess advanced planning and organizational capabilities, allowing them to break down complex tasks into smaller, manageable steps, arrange information logically, and generate coherent plans, fostering efficient decision-making and execution.In the context of LLMs, planning and organization refer to different cognitive-like processes that the model can simulate when generating text or solving complex tasks.In particular, planning refers to the model's ability to structure its outputs in a coherent and logical way over multiple steps or turns.Organization refers to how well the model arranges and structures information within a single response or across multiple interactions.</p>
<p>Song et al. [64] introduce the LLM Planner, a method that uses LLMs to empower intelligent agents to follow natural language instructions and complete complex tasks in visually perceived environments.By learning from a small number of samples, LLM Planner significantly reduces the amount of annotated data needed to train embodied agents.The LLM Planner excels in breaking down complex tasks into smaller, actionable steps for the agent to execute while adapting to realworld challenges.In contrast, Kambhampati et al. [106] argue that while LLMs are strong in language comprehension and generation, they are not fully capable of effective planning or self-validation.To address these limitations, they propose the LLM Modulo framework, a bidirectional interaction system combining LLMs with external model-based validators.Their roles include generating candidate plans, refining problem specifications, and translating domain models.This framework positions LLMs as incomplete planners that contribute to the broader planning process but depend on external validation for accuracy and reliability.</p>
<p>Gundawar et al. [107] apply the LLM Modulo framework to travel planning, showing how LLMs can generate itineraries from natural language queries and use external critics for feedback.Their results demonstrated that GPT-4-Turbo improves planning success rates, outperforming traditional methods.Similarly, Sharan et al. [108] introduce LLM-ASSIST, which combines rule-based planners with LLMs for autonomous driving.The system leverages LLMs' reasoning to generate safe driving plans for complex scenarios beyond the scope of rule-based systems.</p>
<p>Overall, these studies emphasize the role of LLMs in planning, focusing on dynamic adaptability as well as the importance of external validation to ensure plan feasibility.</p>
<p>Decision-Making.</p>
<p>In the domain of decision-making, LLMs have both strengths and inherent limitations, particularly in risk assessment, managing uncertainty, and addressing cognitive biases.As LLMs become more widely deployed, their role in supporting decision-making processes must be rigorously evaluated.</p>
<p>In risk assessment, LLMs are expected to accurately identify and manage the risks tied to their integration.Given their growing use in various sectors, vulnerabilities must be carefully addressed.Pankajakshan et al. [109] develop a systematic framework using scenario analysis and dependency mapping to prioritize risks in LLM integration.GUARD-D-LLM [110] further assesses threats tied to specific use cases, highlighting the need for ongoing improvements in risk mitigation frameworks.</p>
<p>Evaluating LLMs' decision-making through the lens of game theory, Huang et al. [65] introduce GAMA-Bench (-Bench), a framework that assesses LLMs' decision-making abilities in multi-agent environments using classic game theory scenarios like the Public Goods Game and Sealed-Bid Auction.This benchmark evaluates LLMs on cooperative, betraying, and sequential decisionmaking, highlighting strengths in robustness but revealing limitations in generalizability compared to human decision-makers.While LLMs like GPT-3.5 show promising robustness, their performance in complex, multi-agent settings still lags behind humans, suggesting the need for approaches like CoT to improve their strategic capabilities.</p>
<p>When it comes to decision-making under uncertainty, LLMs are increasingly used by decisionmakers who may not fully grasp the nuances of these complex models.As LLM capabilities evolve, there is a growing temptation to rely on them for solving ambiguous and uncertain problems.However, this introduces risks, including the awareness and potential misuse of LLMs and the systemic risks of relying on AI for critical decisions calling for a dynamic, adaptive approach in AI development, especially when LLMs are used in high-stakes scenarios [111].</p>
<p>A key challenge in LLM-based decision-making is the presence of cognitive biases.Since LLMs are trained on large datasets that often contain societal biases, they can generate biased responses [112].Similarly, Schramowski et al. [113] note that these biases resemble the cognitive biases seen in human behavior.Such biases can distort decisions and undermine fairness and objectivity.To address this issue, Echterhoff et al. propose a self-debiasing technique that enables LLMs to automatically adjust prompts, removing bias-inducing elements and enhancing the fairness and consistency of their decision-making processes [66].</p>
<p>Overall, while LLMs show potential in risk assessment, uncertainty management, and decisionmaking, they still require significant improvements to handle complex scenarios, cognitive biases, and high-stakes environments effectively in order to assist humans effectively.</p>
<p>Social Skills</p>
<p>Social skills in LLMs are essential for enabling effective human-like interactions, emotional understanding, collaboration, and cultural awareness.Key areas such as interpersonal communication, emotional intelligence, collaboration, and cultural competency define how well LLMs navigate social dynamics.The works described below give us an overview of the strengths and limitations of LLMs is such scenarios.</p>
<p>Interpersonal Communication.</p>
<p>Interpersonal communication in LLMs refers to their ability to engage in human-like social behaviors, which is crucial for effective information exchange, understanding social dynamics, and participating in complex interactions [68,114,115].</p>
<p>To comprehensively assess LLMs' interpersonal communication abilities, the focus is often on two primary dimensions: communication skills and social reasoning.Communication skills are assessed by examining the LLM's ability to produce coherent and contextually relevant responses, while also understanding language subtleties like tone, humor, and implied meanings [116].Social reasoning involves the model's ability to interpret and predict the thoughts, intentions, and emotions of others [117].A critical aspect of social reasoning is Theory of Mind (ToM)-the capacity to recognize that others may hold beliefs, desires, or knowledge different from one's own [67].This capability is essential for predicting behavior and responding appropriately in dynamic social interactions.</p>
<p>Various experimental setups and datasets have been developed to test these aspects of interpersonal communication in LLMs.For instance, the Generative Agent Environment [118] provides a controlled virtual setting where LLM-based agents simulate everyday social behaviors, like forming relationships and making collective decisions.These agents operate in environments modeled after games like The Sims, demonstrating their ability to plan activities, organize events, and engage in realistic social dynamics, which showcases their evolving communication and coordination capabilities.</p>
<p>Role-playing has emerged as another significant method for assessing LLMs' interpersonal skills.Shanahan et al. [119] investigate how LLMs can take on different personas to simulate complex social scenarios, including deception, persuasion, and conflict resolution.This approach highlights the potential of LLMs to be utilized in training environments where realistic social interactions are required, such as in negotiation exercises or interview preparations, further proving their relevance in human-like simulations.Furthermore, Strachan et al. [67] explore LLMs' social reasoning capabilities by focusing on ToM tasks.Their research demonstrates that models like GPT-4 could perform well in understanding indirect requests and predicting the beliefs and emotions of others.However, these models still face challenges when dealing with more nuanced social situations, such as identifying faux pas or subtle social cues, indicating areas for further improvement.</p>
<p>Overall, the interpersonal communication capabilities of LLMs are evolving rapidly, with models demonstrating improved proficiency in simulating human-like social behaviors across various domains.While challenges remain in achieving the depth and adaptability of human interactions, especially in nuanced or unpredictable contexts, LLMs' growing competencies make them valuable assets for applications ranging from virtual environments to training simulations and social behavior studies.</p>
<p>3.5.2Emotional Intelligence.Emotional intelligence (EI) in LLMs is a crucial aspect of their social skills, enabling them to recognize, understand, and respond to users' emotions.In the field of "machine psychology," which integrates human psychology principles into AI systems [120], EI enhances human-computer interactions by fostering empathetic communication and more engaging responses.</p>
<p>EI in LLMs includes emotion perception, cognition, and expression, all vital for interpreting emotional cues and navigating social contexts.Researchers use psychometric tools like EIBENCH [70] and EmoBench [71] to evaluate these abilities.EIBENCH focuses on emotion recognition, causal reasoning, and generating empathetic responses, while EmoBench tests emotional understanding and reasoning in complex social scenarios.</p>
<p>Zhao et al. [70] develop the Modular Emotional Intelligence enhancement method (MoEI), improving models' EI without reducing their general intelligence (GI).Experiments show that models like Flan-T5 and LLaMA-2-Chat effectively balanced EI and GI, displaying enhanced empathy and emotional reasoning.Wang et al. [72] introduce the SECEU test, revealing that models like GPT-4 can outperform human participants in emotion understanding.Elyoseph et al. [69] use the Levels of Emotional Awareness Scale (LEAS) to find that ChatGPT exceeds average human performance in emotional awareness, showing improvements through iterative learning.Despite advancements in emotion recognition and regulation, LLMs still lag behind human-level social reasoning in complex scenarios.Frameworks like EmoBench [71] highlight that while models like GPT-4 perform well in emotional tasks, gaps remain in their ability to handle nuanced social communication.</p>
<p>OVerall, the above works imply that while LLMs demonstrate significant progress in emotional intelligence, especially in perception and empathy, they struggle to fully apply this understanding in real-world social interactions.</p>
<p>Collaboration.</p>
<p>LLM collaboration capabilities span two critical domains: working alongside AI agents and collaborating with human partners.These areas represent different aspects of how LLMs contribute to teamwork, strategic planning, and task execution, yet each has distinct challenges and benchmarks for evaluation.</p>
<p>In multi-agent systems (MAS), LLMs like GPT-3.5, GPT-4, and Claude-3 have demonstrated improvements in task execution and strategic decision-making [121].Frameworks such as the Dynamic LLM-Agent Network (DyLAN) allow multiple LLM agents to share information and coordinate strategies, improving reasoning and code generation tasks by up to 13% [121].However, they also show that while GPT-4 exhibits greater adaptability in strategy formation, models like Claude-3 tend to struggle more with maintaining complex collaborative dynamics, especially in scenarios requiring continuous adjustments.</p>
<p>Zhang et al. [74] introduce the Reinforced Advantage (ReAd) framework, enhancing collaboration in simulated environments through a feedback loop that refines actions based on learned advantage functions.Though ReAd improves strategic planning efficiency, LLMs still exhibit rigidity when faced with unpredictable and dynamic conditions, where adaptive collaboration is crucial.</p>
<p>Meanwhile, benchmarks such as LLMARENA [73], which assesses LLM collaboration in multiagent environments through TrueSkill scoring, illustrate that despite models like GPT-4 showing improvements in structured environments, they struggle to match the real-time adaptability and teamwork exhibited by human agents, particularly in the areas of opponent modeling and dynamic communication.</p>
<p>In human collaboration, LLMs like GPT-4 and LLaMA2-70B contribute effectively to decisionmaking, resource management, and strategic planning, but they often fail to grasp nuanced social cues and contextual awareness, both of which are essential for seamless human interaction.On platforms like GOVSIM [75], LLMs typically lag behind human teams in maintaining strategic adaptability, especially in situations requiring quick decision-making and responses to unforeseen developments.</p>
<p>The RoCoBench benchmark [74], which evaluates LLM-human collaboration, highlights the limitations in LLMs' strategic reasoning and interaction efficiency.While frameworks like ReAd provide incremental improvements, RoCoBench results indicate that LLMs remain rigid and taskfocused compared to the flexible and intuitive collaboration seen in human teams.</p>
<p>Overall, despite the advancements, LLMs face significant limitations in both AI-agent and human collaboration.In AI-agent interactions, LLMs lack the contextual understanding and flexibility that are vital in dynamic environments, where human reasoning excels.In human collaboration, LLMs struggle to handle the complexity of nuanced social cues and real-time adjustments essential for effective teamwork.Therefore, future advancements should focus on enhancing LLMs' ability to process real-time feedback and evolve strategies dynamically.</p>
<p>Cultural Competency.</p>
<p>LLMs have shown significant potential in understanding and simulating cultural behaviors, offering promising applications in diverse contexts.However, they face notable challenges in accurately applying this knowledge across cultures, particularly for minority or underrepresented groups.These challenges often lead to biased or stereotypical outputs, undermining cultural sensitivity and inclusivity.</p>
<p>For example, LLMs like GPT-4 Turbo have been tested for generating culturally relevant content in languages such as Indonesian and Sundanese [122].While these models produce linguistically accurate text, they struggle to represent deeper cultural elements.This shortcoming is compounded by the predominance of Western-centric values in training datasets, as highlighted by Kharchenko et al. [123], leading to cultural insensitivity and oversimplified stereotypes.</p>
<p>To assess cultural competency, researchers have developed benchmarks like BLEND, which includes 52.6k question-answer pairs spanning 16 countries and 13 languages, including lowresource languages such as Amharic and Sundanese [76].The results show that LLMs perform well for cultures that are well-represented online but struggle significantly with low-resource languages and nuanced cultural knowledge from underrepresented regions.</p>
<p>Additional frameworks, such as Hofstede's Cultural Dimensions [77] and Bloom's Taxonomy [124], have also been used to evaluate LLMs' recall of factual cultural knowledge.However, these assessments reveal that LLMs consistently fall short in tasks requiring deeper cultural sensitivity, such as providing culturally appropriate advice or creatively adapting content.In contrast, human evaluators excel at these tasks, demonstrating an intuitive ability to adjust to cultural contexts [125,126].Fig. 5.The conceptual framework for section 4, showing the human-centric domains that have been focused on using LLMs as a primary tool for investigation, the common LLM models that are employed, the most frequently seen techniques for using those models, which go from simple prompting to multi-agent systems and fine-tuning, or a combination of such methods, and finally, the research goals.</p>
<p>Although some methods have been proposed to improve cultural adaptation by dynamically incorporating external information during content generation [124], significant gaps remain.These gaps are especially evident in tasks requiring profound cultural understanding, particularly for underrepresented groups.Bridging these gaps will require advancements in training data diversity, more inclusive benchmarking, and techniques for adaptive content generation to better align with cultural nuances.</p>
<p>Summary</p>
<p>Overall, LLMs have made great advances in cognitive, perceptual, analytical, emotional, social, and cultural competencies.They are skilled at structured reasoning, pattern recognition, data analysis, and creativity.Yet, they fall short in handling multi-step logic, real-time learning, genuine empathy, and complex social dynamics.Their ability to adapt to cultural nuances remains limited, often defaulting to stereotypes for underrepresented groups.While effective in controlled environments and structured tasks, LLMs struggle in unpredictable, real-world scenarios and continue to rely on external validation for consistency.Our Figure 4 provides a more detailed summary of the competency comparison between LLMs and human.Bridging these gaps will require significant enhancements in real-time learning, contextual understanding, and emotional awareness to more closely align LLMs with human-like adaptability and intelligence.</p>
<p>LLMS IN HUMAN-CENTRIC STUDIES</p>
<p>In the following section, we transition from evaluating LLMs in isolated tasks to exploring their application in real-world studies where humans are central to the research.Specifically, we focus on how LLMs perform in domains at two scales: individual, where an LLM performs tasks typically done by a single human, and collective, where multiple LLMs collaborate to achieve dynamic, group-based outcomes.Individual domains include behavioral science, psychology, and linguistics, whereas collective domains are composed of economics, and political science, and sociology.These studies aim to assess LLMs' ability to replicate human-like reasoning and interaction in a variety of real-world scenarios.Furthermore, the methodologies used in these studies can be categorized into three main approaches and are laid out in Figure 5: (1) basic prompting, which involves sequentially querying a model to generate responses that build a broader understanding; (2) multiagent prompting, where several LLMs interact autonomously based on predefined rules and theories;</p>
<p>(3) fine-tuning, which involves retraining models with additional data to improve their performance in specific domains; and (4) human-in-the-loop, which involves a real person who interacts with the agents by inputting a stimulus into the system.These methods are often combined to create more human-like behaviors in LLMs, with frameworks drawing from established theories such as game theory, theory of mind, social learning theory, etc.In examining these applications, we consider various research goals of LLM-based contributions, including simulation, text content labeling and generation frameworks, methods for quantification and reduction of bias, and works that aim to discover new LLM human-centric capabilities.The following sections delve into the major works in this area, organized by their respective domains and sub-fields.The cited works and their respective contributions are also summarized in Appendix Tables 4 and 5.</p>
<p>Individual Domains</p>
<p>In this section, we provide an overview of the existing works that are human-centric with an individual emphasis, focusing on scenarios and applications where the LLM would be put to use in the capacity of an individual human or in small-scale group scenarios where emulating individual human behaviors can lead to realistic outcomes.The domains covered in this section include behavioral science, psychology, and linguistics, and the specific topics are shown in Figure 6.</p>
<p>Behavioral Science.</p>
<p>Behavioral science is the study of human actions, decision-making processes, and interactions, drawing from fields like psychology, sociology, and cognitive science and aims to understand how individuals and groups behave in various settings, including social, organizational, and economic environments [127].The knowledge from behavioral science provides valuable insights into the mechanisms behind cooperation, collaboration, and social learning, making it ideal for use in LLMs.</p>
<p>Several studies explore how LLMs can be enhanced by incorporating them into collaborative environments [128][129][130].He et al. [128] investigate how LLMs can effectively participate in collaborative work scenarios, aiming to improve LLMs' ability to engage in collaborative ideation, whereas Shaer et al. [129] explore how LLMs can assist in collaborative writing, aiming to refine LLMs' ability to generate creative and coherent content in teamwork settings.The previous two works use a similar virtual whiteboard design that allows agents to brainstorm together and come up with optimal ideas through mutual sharing.Similarly, Liu et al. present important methods for understanding and improving LLMs' problem-solving capabilities through collaboration.On a larger scale, Guo et al. [130] design a framework consisting of embodied agents and prompt-based organization structures to investigate the dynamics between teams and management.They find that with certain combinations of team composition, LLMs can optimize decision-making processes within organizational structures.</p>
<p>Some works also employ a simulation framework to discover how LLMs can interact in groups like humans do.Park et al. [118] create a community of LLM agents, each with unique characteristics and needs, to study social dynamics in an open-world game-like scenario.The agents are instantiated with a complex framework for realistic behavior, including an experience synthesis and memory architecture.Their work takes a bold step in exploring how LLM can be used to simulate complex human-like behaviors.CAMEL et al. [133] use a method they dub inception prompting in combination with role-playing to explore LLMs' ability to cooperate autonomously, leveraging theory of mind to enhance their collaborative potential.Inception prompting is a recursion-like method where the LLM assistant and agent prompt each other in a constant loop, where the prompts include tasks, roles, communication protocols, etc. Role-playing is a crucial part of the prompting technique, as it allows for the creation of a large number of varied conversations, namely 50,000.</p>
<p>Shaikh et al. [134] and Park et al. [135] develop multi-agent simulation scenarios to explore LLM's ability to resolve conflicts and simulate social media dynamics, respectively.Specifically, Shaik et al. design a system for testing conflict resolution using role-based agents and an interlocutor to help them resolve a series of conflicts, guided by social science theory.They find that their simulation tool serves as an effective tool for real conflict resolution in the workplace.On the other hand, Park et al. design a social media simulator using GPT-3.They infuse the system with a set of community guidelines and agents with specific roles.The results shed light on the way social media dynamics can be regulated with effective community design, providing insights to platform designers.</p>
<p>By incorporating behavioral theories, these works provide valuable insights into how LLMs can better understand and simulate human interactions at different scales and in different scenarios, leading to improved cooperation mechanisms and collaborative problem-solving.The works also pave the way for future research on more sophisticated frameworks for modeling human-like behaviors in AI systems and exploring the long-term effects of LLM-augmented collaboration.</p>
<p>4.1.2Psychology.Psychology investigates the mental processes and behaviors of individuals as they relate to their biological, cognitive, emotional, and social factors [136].These insights into human thought and behavior are critical for refining LLMs to exhibit more human-like reasoning and empathetic engagement.By incorporating psychological principles, LLMs can develop a deeper understanding of user emotions and intentions, leading to improved interactions that feel more personalized, emotionally intelligent, and aligned with human cognitive patterns.</p>
<p>Within this field, substantial of works investigated the approaches for improving the capability of LLMs in assisting psychological research.For example, both Dillion et al. [137] and Demszky et al. [138] discuss various protocols for applying LLMs into psychological research, working as substitutions for the human volunteers who are studied, as well as the reliability of such approaches.On the other hand, various studies explore the approaches for improving LLMs from the perspective of human psychology phenomena [67,[139][140][141].Among these studies, Strachan et al. [67] explore whether LLMs can produce behavior comparable to human behavior through the lens of theory of mind, highlighting the importance of systematic approaches to enhance the capability of LLMs in to align with human mentalistic inference processes.Both Chuang et al. [140] and et al. [141] studied the collective intelligence, which is an important mechanism in human collaboration, among multiple LLM agents.They shed light on an important way to improve LLM capabilities, namely by organizing multiple LLM agents to collaborate like humans.</p>
<p>In summary, research in this domain explores how LLMs can be integrated into psychological studies and improved through insights from human psychological phenomena.These works all contribute to better understandings of the working mechanism of LLMs from a psychological perspective, shedding light on various possible approaches that may improve the capability of LLMs to emulate humans.4.1.3Linguistics.Linguistics investigates the structure, evolution, and use of language, analyzing how meaning is constructed and communicated across different languages and contexts [142].By embedding linguistic theories in LLMs, models can better comprehend details in syntax and semantics of language, which allows for more accurate and contextually relevant language generation.This linguistic foundation is key to improving the LLM's ability to handle multilingual tasks and adapt to varying communicative styles across cultures.</p>
<p>Since natural language is the major interface for LLMs to interact with humans, substantial works in this domain focus on uncovering the linguistic shortcomings in LLMs, and thereby providing guidance for further improving their capabilities [115,143,144].For example, Mahowald et al. [115] evaluate LLMs by distinguishing between formal linguistic competence, which represents knowledge of linguistic rules and patterns, and functional linguistic competence, which requires understanding and using language in practical contexts.Their findings show that while LLMs perform well in formal competence, their abilities in functional tasks are inconsistent.Based on such discovery, they point out that possible solutions for improving this aspect of capability include specialized fine-tuning and integrating external modules.Also, Lai et al. [143] evaluate ChatGPT's performance beyond English.Testing with multilingual NLP tasks, which cover 37 diverse languages including English, French, Spanish, Chinese, and etc., their findings reveal that ChatGPT performs significantly better in English compared to many other languages, particularly for tasks requiring complex reasoning.Their study calls for further research that develop models with better multilingual understanding capability, where diverse data and language-specific finetuning are necessary.Moreover, Pavlick et al. [144] explore the potential of LLMs as models for Collective Fig. 7.The collective domains consists of politics, economics, and sociology, and vary from more simple employment methods such as prompting, relying on language comprehension and knowledge retrieval, all the way to more complex multi-agent and simulation systems which rely on LLMs advanced abilities in cognition and reasoning.</p>
<p>human language understanding, highlighting two key factors that challenge their plausibility: the absence of symbolic structure and the lack of grounding.The above works highlight the limitations of LLMs in linguistic competency, thereby emphasizing the necessity for addressing these gaps with diverse training data, fine-tuning designs, and external modules.</p>
<p>Collective Domains</p>
<p>In this section, we provide an overview of the existing works that are human-centric with a collective emphasis, focusing on scenarios and applications where LLMs are infused with theory and applied to test their ability to emulate realistic macro trends and behaviors.The domains covered in this section include politics, economics, and sociology, and the specific topics are shown in Figure 7.</p>
<p>Political Science.</p>
<p>Political science examines the ways in which governments allocate resources and make decisions, as well as the perceptions and behaviors of constituents in relation to bodies of authority [145].By leveraging knowledge from political science, LLMs can improve their ability to understand political discourse, predict political behaviors, and simulate complex political scenarios.Concepts like historical determinism, coalition formation, and the echo chamber effect offer pathways for LLMs to enhance their ability to model political phenomena, reduce bias, and generate more nuanced political analyses.</p>
<p>There are many recent studies that utilize LLMs to simulate political phenomena, with the aim of improving LLMs' ability to understand and predict political behaviors.For example, Hua et al. [146] examine how multi-agent LLM simulations can replicate large-scale historical events such as world wars.Using an architecture consisting of country agents, secretary agents, board, and stick, they reveal some of the factors that lead up to the outbreak of a conflict.Reflecting the components of a board game like risk, the board represents the inter-relationships between countries whereas the stick represents the internal record keeping mechanism for each country.This work represents a first step to understanding LLM's ability to model and replicate complex, collective human behaviors on a societal scale.On a slightly smaller scale, Moghimifar et al. [147] explore how LLMs can uncover the facts that lead to the formation of political coalitions, which can prevent domestic conflicts.By presenting a dataset and a hierarchical Markov decision process tailored to this task, they open up an avenue for future LLM works to predict outcomes of political negotiations.Similarly, Taubenfeld et al. [148] simulate political debates using LLMs to understand biases in political discussions.They find that the evolution of views of the agents in a debate differs from real human behaviors, with both participants of the debate converge towards the models' inherent bias.They also find that fine-tuning can make a certain bias more pronounced.These findings shed light into the differences between LLMs and real human tendencies and can inform methods for reducing bias in LLM outputs.</p>
<p>Further directions of LLMs' application towards the political arena include political content generation and content labeling.Hackenburg et al. [149] and Goldstein et al. [150] study how LLMs can be improved to tailor political messages and generate convincing content, honing LLMs' ability to adapt to audience profiles.More specifically, Hackenburg et al. develop a web-application that can integrate user-reported demographic information into prompts to generate indiviudally tailored political messaging, whereas Goldstein et al. leverages a user survey to determine the persuasiveness of LLM generated so-called propaganda theses.</p>
<p>Moreover, several studies address stance detection and misinformation evaluation using LLMs [151][152][153][154]. Zhu et al. [151] assess ChatGPT's ability to replicate human-annotated labels across various social computing tasks, including political stance detection.Zhang et al. [152] explore how LLMs can determine the relationship between explicit political standpoints and correlated views.Additionally, Li et al. [155] address bias in stance detection by introducing a calibration framework to help LLMs generate more balanced and accurate political analyses.</p>
<p>In sum, the described works demonstrate the application of LLMs in simulating political phenomena, while also exploring their potential in tailoring political messages and detecting political stances.These studies reveal both the capabilities and limitations of LLMs in modeling complex political behaviors, highlighting areas for improvement in reducing bias and enhancing the accuracy of political simulations and analyses.</p>
<p>Economics.</p>
<p>Economics involves understanding how individuals, businesses, and governments make choices about the allocation of resources, production, and consumption of goods and services [156].LLMs, which are increasingly being used to simulate economic behaviors and predict market outcomes, can benefit from the theoretical frameworks in the field of economics.</p>
<p>LLMs are being explored for their ability to simulate economic behaviors and improve their understanding of complex market dynamics.In several existing works, LLMs are evaluated for their ability to emulate human behavior across a handful of economic scenarios, including game theory, macroeconomic trends, and market dynamics.Various works approach different aspects of game theory.A work by Guo [157] represents an early attempt at using prompting and architectures to investigate how LLMs can be used in strategic game experiments like the ultimatum game and prisoner's dilemma.In a related vein, Horton et al. [158] endows LLMs with various features such as information and preferences, determining that the outcome of LLMs in a set of classic economic scenarios reinforces prior works' conclusions, showing that LLMs can reliably simulate economic decision-making.Similarly, Akata et al. [159] explore coordination and cooperation mechanisms in LLMs in a variety of prisoner's dilemma scenarios, finding consistently high performance and behavioral signatures among coordinating agents.Going even further, Suzuki et al. [160] simulate the evolution of human decision-making in economic scenarios, treating personality traits as "genes" in prisoner's dilemma studies.Such studies can be used to enhance the way LLMs' model micro-level economic behaviors.In turn, Li et al. [13] design an LLM architecture including memory, reflection, and decision modules in combination with macro-economic theory and heterogeneous decision-making mechanisms to explore how LLMs can model macro trends.</p>
<p>Finally, market dynamics are addressed by Weiss et al. [161], who study information markets by simulating a digital marketplace, drawing on social learning theory and buyer's paradox.This work exploits the unique qualities of human cognition such as forgetting and assessing information to create more realistic agents.Meanwhile, Zhao et al. [162] examine market competition by simulating a virtual town where LLM agents compete for customers.These works can help improve how LLMs agents with asymmetric information can better predict market dynamics.</p>
<p>In summary, recent research has explored the application of LLMs in simulating various economic scenarios, ranging from game theory experiments and macroeconomic trends to complex market dynamics.These studies demonstrate the potential of LLMs to model economic decision-making processes and predict outcomes in diverse contexts, while also highlighting areas for improvement in creating more realistic and nuanced economic simulations.4.2.3Sociology.Sociology examines the structures and dynamics of social life, social change, and the interactions between families as well as macro-scale human communities [163].These studies provide a deeper understanding of social bias, ethical norms, and human morality.By integrating sociological principles, LLMs can be prompted or fine-tuned to recognize and respect cultural contexts, making their responses more socially aware and ethically sensitive when addressing diverse populations.This improves their ability to adapt to different societal expectations and enhances the realism of their interactions.</p>
<p>One major category of works in this field aims to enhance the social capabilities of LLMs from various aspects by comprehensively uncovering their shortcomings [164][165][166][167].For example, Scherrer et al. [164] identify certain limitations in the moral decision-making processes of LLMs, particularly in ambiguous scenarios where the moral choice is unclear, pointing out the necessity of approaches, such targeted fine-tuning, to address these gaps.Rao et al. [165] and Dwivedi et al. [166] highlight deficiencies in LLMs' adaptability to different cultural contexts and regional etiquettes.These stress the need of incorporating culturally diverse data and region-specific training protocols to improve these capabilities.Also, Zheng et al. [167] develop a multi-agent framework to test the misalignment of LLMs with human values through scenario generation and refinement, suggesting a basis for continuous enhancement in value alignment.On the other hand, other researchers investigated using human-in-the-loop approach to improve the social capability of LLMs [168,169].For instance, Tao et al. [168] explore cultural bias for five commonly used LLMs, and design cultural prompting as a control strategy to increase cultural alignment of LLMs with specific country or territory.Duan et al. [169] design a framework that dynamically generates ethical prompts and evaluates model responses based on human-defined moral values, refining the LLMs with finetuning.</p>
<p>In summary, the works in this field primarily focus on identifying and addressing the social limitations of LLMs.Having uncovered the limitations, researchers propose solutions like finetuning, incorporating diverse data, and using human-in-the-loop approaches to enhance the social and ethical capabilities of LLMs.</p>
<p>Summary</p>
<p>Overall, the research in human-aware fields for improving LLM has validated advanced and often very realistic behaviors, decision-making, and thought processes being made by LLMs using a variety of unique and complex architecture paradigms.From the findings of such works, it is evident that the emergence of LLMs poses great implications for all aspects of society, from politics, to economics, to psychology and linguistics, among others, in turn shedding light on the limits of the capabilities of pre-trained generative models.The findings, thereby, can be used inform both the academic community and decision makers, both at a corporate and governmental level, how to adapt relevant systems of society to adapt to this groundbreaking new development in the way information is sought and generated.Furthermore, these findings can serve as a feedback stream to LLM developers to better mold their LLM outputs to users' and society's needs.However, the road forward is not entirely clear, as users of LLMs and society at large may have contradictory aims.As such, there is a need for an even deepening and broadening body of research on the capabilities of LLMs so as to uncover their functions that can be most conducive for positive human-centered outcomes.As the research becomes more nuanced and comprehensive, the answers may become more clear.</p>
<p>OPEN CHALLENGES AND FUTURE DIRECTIONS</p>
<p>While LLMs continue to demonstrate impressive advancements, several critical challenges remain that require further attention.In this section, we present the challenges within two main areas: (1) Human-Centric Evaluation of LLMs and (2) LLMs in Human-Centric Studies.The former focuses on identifying key skill areas where LLMs need improvement, such as their adaptability to real-world contexts, ability to understand and respond to human emotions, and sensitivity to cultural dynamics.The latter explores how LLMs can be effectively applied within individual and collective domains.We also outline potential directions for addressing these challenges.</p>
<p>Advancing Real-Time Learning and Adaptability in LLMs</p>
<p>Open Discussion: One significant challenge LLMs face is their inability to perform real-time learning and adapt to new information dynamically.While they excel in structured environments and can generate coherent plans, LLMs do not have the capability to update their parameters based on new data or experiences during inference.This limitation hinders their ability to evolve and improve in response to changing circumstances or user interactions, which is crucial for applications that require continuous learning and adaptability.</p>
<p>Future Direction: A potential direction to address this challenge is the development of hybrid learning architectures that combine LLMs' static pre-trained knowledge with online learning algorithms.Integrating reinforcement learning techniques, such as RLGF, could allow LLMs to adapt dynamically in real-world scenarios by continuously refining their decision-making processes based on real-time feedback.This approach would enable LLMs to make context-aware adjustments in unpredictable environments and better simulate human cognitive flexibility.</p>
<p>Enhancing Emotional Intelligence and Empathy in LLMs</p>
<p>Open Discussion: While LLMs have advanced in recognizing emotions, their capacity to express genuine emotional intelligence and empathy remains limited, particularly in nuanced human interactions.Despite progress in identifying and interpreting emotions, they often lack the depth required to generate responses that truly resonate with individuals' emotional states across diverse contexts.As a result, their interactions can come across as mechanical or overly neutral, missing the subtlety needed for empathetic communication.This limitation affects their effectiveness in sensitive areas like mental health support, customer service, and human-computer interaction, where a more human-like empathetic response is crucial.</p>
<p>Future Direction: Looking forward, enhancing the emotional intelligence of LLMs requires a multi-faceted approach that goes beyond basic emotional recognition.A key strategy involves integrating affective computing techniques to enable models to interpret and respond to emotional cues in text and speech with greater accuracy.Additionally, employing reinforcement learning strategies focused on emotional reinforcement can help fine-tune the model's responses based on real-time user feedback in emotionally charged interactions.Incorporating culturally diverse real-world data into LLM training is also essential to improve empathetic accuracy, ensuring these models become more sensitive to the emotional expressions unique to different cultures.By advancing these capabilities, LLMs can play a more meaningful role in applications that require human-like emotional support and culturally aware communication.</p>
<p>Improving Cultural Competency and Reducing Bias in AI Interactions</p>
<p>Open Discussion: While LLMs have shown advancements in their cultural competency, they still struggle to understand and respond accurately to nuanced cultural contexts, particularly for underrepresented or minority cultures.This limitation often leads to generalized or stereotypical outputs that fail to capture the subtleties of different cultural perspectives.The challenge lies in making LLMs more culturally adaptive to provide responses that respect and reflect diverse societal norms and values.</p>
<p>Future Direction: To address the issues, a possible direction is to develop more inclusive training datasets that better represent a wide range of cultural and linguistic contexts.Additionally, techniques like Retrieval-Augmented Generation (RAG) can be leveraged to dynamically incorporate relevant cultural knowledge from external databases during interactions, enhancing the accuracy and sensitivity of LLM responses.Future research should focus on creating adaptive models that can better handle context-specific nuances, promoting culturally aware interactions and reducing unintended biases in diverse global settings.This approach aims to improve the reliability and inclusivity of LLMs when engaging with users from various cultural backgrounds.</p>
<p>LLMs in Human-Centric Individual Domains</p>
<p>Open Discussion: In the context of individual domains such as behavioral science, psychology, and linguistics, LLMs face several key challenges.In behavioral science, LLMs struggle with accurately simulating human social learning and collaboration in dynamic, real-world scenarios.In psychology, LLMs have difficulty consistently emulating human cognitive and emotional processes, often failing to adapt to evolving user emotions and intentions.Additionally, in linguistics, LLMs have limited capabilities in handling deep semantic understanding and pragmatic language use across diverse contexts, especially in multilingual settings.These limitations hinder LLMs from fully replicating the depth of human cognition, emotional nuance, and cultural sensitivity required for authentic, human-like interactions.</p>
<p>Future Direction: To address these challenges, future research should focus on developing hybrid LLM architectures that integrate real-time feedback, psychological models, and external linguistic knowledge bases.Specifically, reinforcement learning techniques and multi-agent systems could enhance LLMs' ability to simulate human collaboration and social behaviors more effectively.In psychology, incorporating more detailed cognitive and emotional models into LLM training could improve the models' ability to generate adaptive, empathetic responses.For linguistics, improving multilingual performance through diverse training datasets, fine-tuning for specific languages, and coupling LLMs with symbolic reasoning systems can lead to more contextually accurate and culturally aware language generation.These improvements would enable LLMs to better navigate complex interactions in human-centric applications.</p>
<p>LLMs in Human-Centric Collective Domains</p>
<p>Open Discussion: In collective domains such as political science, economics, and sociology, LLMs face significant challenges in accurately simulating complex systems and predicting long-term behaviors.In political science, LLMs struggle with modeling intricate power dynamics, political alliances, and the outcomes of political negotiations.In economics, the models often fail to simulate complex market behaviors and economic decision-making, especially when dealing with irrational actors or unpredictable external factors.Additionally, in sociology, LLMs have difficulty modeling the complexities of social structures, ethical norms, and cultural dynamics, often reinforcing existing biases and stereotypes.These challenges limit the ability of LLMs to provide reliable and unbiased insights in large-scale human-centric studies.</p>
<p>Future Direction: To overcome these limitations, future research should focus on enhancing the integration of domain-specific frameworks into LLM architectures.In political science, incorporating game theory, coalition analysis, and real-time political data streams could improve the models' ability to simulate political behaviors and predict outcomes.In economics, coupling LLMs with macroeconomic models and integrating behavioral economics principles would enable more realistic simulations of both micro-and macro-level economic trends.For sociology, building more inclusive training datasets and employing human-in-the-loop approaches can help LLMs generate more socially and ethically aware responses.These advancements will improve the accuracy and cultural sensitivity of LLMs when applied to complex human-centric studies in collective domains.</p>
<p>APPENDIX</p>
<p>Fig. 1 .
1
Fig. 1.Our framework depicts how LLMs are evaluated on foundational human-like skills, divided into individual (e.g., cognition, perception, analysis, executive functioning) and collective (e.g., sociability) levels, and applied within various fields of study similarly categorized as individual (e.g., Behavioral Science, Psychology, Linguistics) and collective (e.g., Political Science, Economics, Sociology) domains.</p>
<p>Fig. 2 .
2
Fig. 2. Overview of LLM Capabilities Across Individual and Collective Domains.</p>
<p>Fig. 3 .
3
Fig. 3. Overview of LLM evaluations.</p>
<p>Fig. 4 .
4
Fig. 4. Competency comparison between LLMs and humans.Blue bars represent skills where LLMs excel over humans, especially in structured tasks and predictable environments, while orange bars indicate skills where humans excel over LLMs, particularly in adaptive, nuanced, and real-world contexts.</p>
<ol>
<li>4 . 1
41
Adaptability and Flexibility.Adaptability and flexibility are essential components of executive functioning in LLMs, reflecting their ability to adjust actions and strategies as task demands change.</li>
</ol>
<p>Fig. 6 .
6
Fig. 6.The individual domains consist of behavioral science, psychology, and linguistics, and each domain contains studies across a wide array of research problems from collaborative ideation and theory of mind, to plagiarism and conflict resolution, among several others.</p>
<p>4 , 4 , 5 -Accuracy
445
GPT-3.5, LLaMA2-70B Examines the capability of LLMs to exhibit theory of mind by testing them on a comprehensive set of psychological tasks, showing mixed performance across tasks like false beliefs, irony, and faux pas Performance accuracy on theory of mind tests Baichuan, ChatGLM-3 Proposes EMOBENCH, a comprehensive benchmark for evaluating Emotional Intelligence (EI) of LLMs, emphasizing gaps between LLMs and humans in understanding emotions Accuracy in emotional under-, LLaMA-2-Chat Introduces EIBENCH, a collection of EI-related tasks, and proposes MoEI, a modular EI enhancement method to improve LLMs' EI without compromising their General Intelligence (GI) Accuracy on EI and GI benchmarks  [72] 2024 GPT-4, Claude, LLaMA-based models Presents SECEU, a psychometric evaluation of Emotional Understanding (EU) for LLMs, showing that GPT-4 achieves higher emotional intelligence scores than 89% of human participants Standardized EQ scores, comparison to human performance Turbo, GPT-4 Introduces DyLAN (Dynamic LLM-Agent Network), a framework that optimizes LLM-agent collaboration through dynamic agent selection, enhancing efficiency and adaptability in complex tasks , Yi, Wiz-ardLM, Vicuna Presents LLMARENA, a benchmark designed to evaluate the abilities of LLMs in multi-agent, dynamic settings across seven different gaming environments TrueSkill, reward  [74] 2024 GPT-4, GPT-3.5-Turbo,Qwen, LLaMA-2, AgentLM, and DeepSeek Proposes ReAd (Reinforced Advantage), a closed-loop feedback mechanism for LLM planners to enhance decision-making and efficiency in multi-agent collaboration Success rate, environment steps, number of queries  a generative simulation benchmark designed to evaluate the cooperative decision-making abilities of LLM-based agents in multi-agent resource-sharing scenarios Survival rate and time, total gain, efficiency, Equality, Overusage to evaluate LLMs on their cultural knowledge of everyday life across diverse regions and languages Cultural knowledge, shortanswer questions, multiplechoice questions, performance gap, local languages, English performance, cultural sensitivity  J. ACM, Vol.V, No. N, Article .Publication date: December 2024.</p>
<p>Benchmark &amp; Dataset Testing Standardized Benchmarks Custom Benchmarks Performance Metrics Human-Centric Evaluations Expert Evaluations Crowdsourced Evaluations Human-in-the-Loop Testing Interactive &amp; Simulation-Based Evaluations Single-Agent Simulations Multi-Agent Simulations Task-Oriented Dialogues Ethical &amp; Bias Assessments Bias Detection Fairness Metrics Ethical Compliance Explainability &amp; Interpretability Transparency of Reasoning User Interpretability Technical Interpretability</p>
<p>Table 1 .
1
Summary of LLM Competencies and Benchmarks
CategorySkillLLM CompetencyBenchmarksReasoningLLMs perform well in simpler tasks like structured, rule-based, andCognitive Skillsabstract logic as models become more advanced, but they struggle with more complex challenges such as multi-step reasoning, handling multiplepremises, and adapting to contextual or generalization-based reasoning.</p>
<p>Language &amp; Knowledge Based Cogniton &amp; Reasoning Based Politics Content Creation Content Labeling Stance Detection International Relations Political Coalitions Debates Economics Game Theory Behavior Coordination Efficiency Digital Marketplace Virtual Town Sociology Cultural Bias Gender Bias Values Alignment Etiquette Morality Collaborative Evolution Simple Prompting Multi-Agent / Simulation</p>
<p>Table 2 .
2
Evaluation of LLM Capabilities Across Cognitive, Perceptual, and Analytical Domains
SkillPaper YearLLMsHighlightEvaluation MetricsCompareswithHu-mans[37]2023GPT-2, GPT-3Introduces LogiQA 2.0 dataset for evaluating LLM's capabilities inAccuracy, precision, recall, andReasoningcomplex logical reasoningF1-score[38]2022InstructGPT, GPT-3Creates PrOntoQA, a first-order logic-based synthetic dataset de-Strict to more relaxed version ofsigned to analyze LLMs' reasoning abilities by converting theirproof accuracychain-of-thought prompts into symbolic proofs[39]2022T5, UNICORN, GPT-3Designs CommonsenseQA 2.0, a gamification-based benchmark,Accuracy, consistencyfor exploring AI common sense limits[40]2021T5-3BPresents Creak, a dataset for commonsense reasoning on entityAccuracyknowledge[47]2024GPT-3.5, GPT-4Examines inductive reasoning in humans and LLMs using propertyArgument strength, sign tests,induction taskscorrelation, phenomenon capture[41]2024LLaMA, OPT, InternLM, Fal-Proposes CausalBench benchmark for measuring LLMs' causalF1 score, structural hamming dis-con, GPT-3.5-Turbo, GPT-4,learning capabilitytance, structural intervention dis-GPT-4-Turbo, etc.tance[42]2024Qwen, LLaMA, GPT-3.5Presents ContextHub to assess LLMs' reasoning across abstractAverage F1 scoreand contextual tasks[43]2021GPT-3Designs GSM8K, a dataset of grade school math word problems,Test solve ratethat evaluates LLMs' mathematical reasoning capabilities[44]2022PaLM, Codex (code-davinci-Showcases that CoT prompting significantly enhances LLMs' per-Exact match002)formance on the challenging BIG-Bench Hard tasks[49]2023GPT-4, Claude2, LLaMA2Utilizes CommonGen tasks to measure the learning ability of LLMs BLEU, CIDEr-D, SPICELearning[50]2023LLaMA, GPT-4, LLaMA,Introduces LEMA (Learning from Mistakes), a method to improveAccuracyLLaMA2, WizardMath, Meta-LLM reasoning by learning from mistakes, using error-correctionMath, CodeLLaMAdata pairs for fine-tuning[51]2024BERT, GPT-4Investigates incremental learning approaches including continualAccuracylearning, meta-learning, parameter-efficient learning, etc.Recognition Pattern[86]2023GPT-3.5learning for robotics Introduces a framework for language-driven visual representationics tasks Various metrics across five robot-[84]2024LLaMAPresents a 3D multimodal LLM designed for embodied interaction,Fine-tuned accuracy, zero-shot ac-achieving state-of-the-art performance in 3D geometry under-curacy on various benchmarksstanding and language-unified interaction tasks[55]2023GPT-4Proposes 3D-LLMs that incorporate the 3D physical world intoBLEU-1, CIDER, accuracy on 3DLLMs, enabling models to perform 3D captioning, task decomposi-question answering benchmarkstion, and spatially grounded dialogueAwareness Spatial[56]2024GPT-4opleting complex tasks in video games computer control, capable of operating diverse software and com-Proposes CRADLE, a multimodal agent framework for generalware and games Success rate across different soft-[88]2024GPT-4Automates the learning process of a generative robotic agent viaSkill diversity, success rate in sim-generative simulation with minimal human supervisionulations[90]2024GPT-4, CLIPIntroduces an embodied LLM agent for vision and language naviga-Task completion rate in VLNtion in Street View, achieving 25% improvement in task completionover state-of-the-art baselinesInterpretation Data[93]2024GPT-4and tool utilization enhance performance on data-centric tasks Introduces a data science agent that integrates dynamic planningended task success MATH dataset accuracy, open-Machine learning accuracy,[170]2023GPT-4Develops Coscientist, an AI system capable of autonomously de-Task success rate in experimentalsigning, planning, and performing chemical experiments, advanc-synthesis, optimization tasksing automated chemical research[171]2024GPT-4Proposes ChatExtract, a conversational LLM-based method forPrecision, recall for data extrac-automated data extraction from materials science literature viationprompt engineering and redundancyProcessing Information[96]2022GPT-2generate highly realistic synthetic tabular data Presents GReaT, a method that utilizes auto-regressive LLMs torecords histogram, ROCAUC, F1 Efficiency, distance to closest[60]2024GPT-4, GPT-4o, GLM-4,Designs DOCBENCH, a comprehensive benchmark designed toAccuracyKimiChat, Mistral, LLaMA-2,evaluate LLM-based document reading systemsLLaMA-3, etc.[61]2024BERT, RoBERTa, DistilBERT,Proposes Cocktail, a benchmark of 16 datasets integrating human-Retrieval accuracyMiniLM, T5written and LLM-generated content to evaluate information re-trieval models in mixed-source data[99]2023GPT-4ChatGPT-4 outperforms students in creative generation, with highAccuracy, Consistency, Relevance Creativityoutput and quality of ideas[62]2024GPT-4Introduces Jamplate system design to enhance LLM's creativeAccuracyreflection ability[101]2024GPT-4, GPT-3.5-turboProposes a framework and role-playing techniques to enhanceFluency, FlexibilityLLM creativity, and use Alternative Uses Task to evaluate thecreativity of LLM</p>
<p>Table 3 .
3
Evaluation of LLM Capabilities in Executive Functioning and Social Domains
SkillPaper YearLLMsHighlightEvaluation MetricsCompareswith HumansAdaptability[102]2024GPT-4Proposes a framework combining LLMs, Stochastic Gradient De-Deviation squared, Sum ofandscent, and optimization-based control to help autonomous systemssquared differencesFlexibilitymanage complex tasks with latent risks[103]2023GPT-3.5Presents DynaCon, a system that uses LLMs for context-awareSuccess Rate, Time-step, Num-robot navigation in unknown environments without mapsber of Experiments[63]2024GPT-4, LLaMA-3.1Introduces LLM-CI, an open-source framework to assess privacyPercentage of invalid responses norms encoded in LLMs using a multi-prompt methodology toaddress prompt sensitivityPlanning[64]2023GPT-3Uses the ALFRED dataset to evaluate the planning and organiza-Success Rate, Goal-Conditionandtional capabilities of LLMSuccess RateOrganization[106]2024GPT-4, GPT-3.5Leverages PlanBench benchmark to evaluate LLM's planning andSuccess Rate, Correctness Rate,organizational capabilitiesPlan Pass Rates[107]2024GPT-4 Turbo, GPT-3.5 TurboImplements Travel Planning Benchmark to evaluate LLM's plan-Delivery Rate, Final Pass Ratening and organizational capabilities[108]2023GPT-4, LLaMA2, GPT-3Applies nuPlan Benchmark to evaluate LLM's planning and orga-Score, Drivablenizational capabilitiesMaking Decision[65]2024LLaMA-3.1, Mixtral, Qwen-2 GPT-3.5, GPT-4, Gemini-Pro,decision-making abilities in multi-agent environments Introduces GAMA-Bench, a framework for evaluating LLM'sNash equilibrium[113]2022BERT, GPT-2, GPT-3Assesses the risks of relying on LLMs for reasoning under uncer-Toxicity probability, expectedtaintymaximum toxicity[66]2024GPT-3.5-turbo, GPT-4, LLaMA-Presents BIASBUSTER, a framework that evaluates and mitigatesEuclidean distance2cognitive biases in LLM-assisted decision-making tasksCommunication Interpersonal [118]2023ChatGPTsimulate social interactions, forming relationships, and executing havior, implementing them in a sandbox game environment to Proposes generative agents as believable simulacra of human be-emergent behavior analysis Observational consistency andplans autonomously[119]2023GPT-4, LLaMAIntroduces a conceptual framework for understanding LLM-basedQualitative analysis of role-playdialogue agents as role-playing entities, emphasizing metaphoricalinteractionsapproaches to avoid anthropomorphism in agent design
J. ACM, Vol. V, No. N, Article . Publication date: December 2024.
CONCLUSIONIn this paper, we take the pioneering step to treat LLMs as entities with human-like intelligence and systematically evaluate them through a humen-centric framework. We first present a comprehensive summary of the intersection between AI and the humanities, contextualizing the development of LLMs within broader interdisciplinary studies. Next, we elaborate on the evaluation of LLMs in key cognitive, perceptual, analytical, and social domains, bridging the gap between theoretical research and practical applications. We then emphasize interdisciplinary collaboration to improve LLMs future capabilities of understanding human mind and behavior, followed by a discussion of remaining open challenges and future research. We hope that our summaries and compilation of resources can be useful for researchers who wish to further uncover the abilities of LLMs to come into contact with our lives in various complex ways, and to improve the design and function of LLMs to act in a manner more consistent with human needs.</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>Explainability for large language models: A survey. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du, ACM Transactions on Intelligent Systems and Technology. 1522024</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>. J Acm, Article . Publication date. NVDecember 2024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Llm based generation of item-description for recommendation system. Arkadeep Acharya, Brijraj Singh, Naoyuki Onoe, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>Mobile-llama: Instruction fine-tuning open-source llm for network analysis in 5g networks. Bo Khen, Hyunsu Kan, Guohong Mun, Youngseok Cao, Lee, IEEE Network. 2024</p>
<p>How artificial intelligence can influence elections: Analyzing the large language models (llms) political bias. George-Cristinel Rotaru, Sorin Anagnoste, Vasile-Marian Oancea, Proceedings of the International Conference on Business Excellence. the International Conference on Business Excellence202418</p>
<p>Shaping the emerging norms of using large language models in social computing research. Hong Shen, Tianshi Li, Toby Jia-Jun Li, Joon , Sung Park, Diyi Yang, Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing. 2023</p>
<p>Using large language models in psychology. Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, Nature Reviews Psychology. 2112023</p>
<p>Using early llms for corpus linguistics: Examining chatgpt's potential and limitations. Satoru Uchida, Applied Corpus Linguistics. 411000892024</p>
<p>EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities. Nian Li, Chen Gao, Mingyu Li, Yong Li, Qingmin Liao, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>A Brief History of Artificial Intelligence: What It Is, Where We Are, and Where We Are Going. Michael Wooldridge, January 2021Flatiron Books</p>
<p>A.I. systems and human cognition: The missing link. Shimon Ullman, Behavioral and Brain Sciences. 11March 1978</p>
<p>A New Direction in AI: Toward a Computational Theory of Perceptions. A Lotfi, Zadeh, AI Magazine. 221March 2001</p>
<p>Management and Business Education in the Time of Artificial Intelligence: The Need to Rethink, Retrain, and Redesign. IAP. Agata Stachowicz, - Stanusch, Wolfgang Amann, May 2020</p>
<p>Tweeting From Left to Right: Is Online Political Communication More Than an Echo Chamber?. Pablo Barber, John T Jost, Jonathan Nagler, Joshua A Tucker, Richard Bonneau, Psychological Science. 2610October 2015</p>
<p>Classifying Party Affiliation from Political Speech. Bei Yu, Stefan Kaufmann, Daniel Diermeier, Journal of Information Technology &amp; Politics. 51July 2008</p>
<p>Fake News Detection on Social Media: A Data Mining Perspective. Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu, ACM SIGKDD Explorations Newsletter. 191September 2017</p>
<p>Political Ideology Detection Using Recursive Neural Networks. Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, Philip Resnik, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Kristina Toutanova, Hua Wu, the 52nd Annual Meeting of the Association for Computational LinguisticsBaltimore, MarylandAssociation for Computational LinguisticsJune 20141</p>
<p>Unsupervised User Stance Detection on Twitter. Kareem Darwish, Peter Stefanov, Michal Aupetit, Preslav Nakov, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social MediaMay 202014</p>
<p>Adversarial vulnerabilities of human decision-making. Amir Dezfouli, Richard Nock, Peter Dayan, Proceedings of the National Academy of Sciences. 11746November 2020</p>
<p>Allan Dafoe, Edward Hughes, Yoram Bachrach, Tantum Collins, Kevin R Mckee, Joel Z Leibo, Kate Larson, Thore Graepel, Open Problems in Cooperative AI. December 2020</p>
<p>Deep reinforcement learning from human preferences. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei, 201730Advances in neural information processing systems</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023. December 2024VarXiv preprintJ. ACM. Publication date</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mrouane Debbah, tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hua Hao Tian, Wu, arXiv:1904.09223Ernie: Enhanced representation through knowledge integration. 2019arXiv preprint</p>
<p>. George Siemens, Fernando Marmolejo-Ramos, Florence Gabriel, Kelsey Medeiros, Rebecca Marrone, Srecko Joksimovic, Maarten De, Laat , Human and artificial cognition. Computers and Education: Artificial Intelligence. 31001072022</p>
<p>Theory is all you need: Ai, human cognition, and decision making. Human Cognition, and Decision Making. Teppo Felin, Matthias Holweg, February 23, 20242024</p>
<p>Inductive and deductive reasoning. The Wiley-Blackwell handbook of childhood cognitive development. Usha Goswami, 2010</p>
<p>Rational decision and causality. Ellery Eells, 2016Cambridge University Press</p>
<p>Using cognitive psychology to understand gpt-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Logiqa 2.0-an improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, Speech, and Language Processing. 2023</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Alon Talmor, Ori Yoran, Le Ronan, Chandra Bras, Yoav Bhagavatula, Yejin Goldberg, Jonathan Choi, Berant, arXiv:2201.05320Commonsenseqa 2.0: Exposing the limits of ai through gamification. 2022arXiv preprint</p>
<p>Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, arXiv:2109.01653Creak: A dataset for commonsense reasoning over entity knowledge. 2021arXiv preprint</p>
<p>Causalbench: A comprehensive benchmark for evaluating causal reasoning capabilities of large language models. Zeyu Wang, Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10). the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)2024</p>
<p>Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, Jindong Wang, Yongfeng Zhang, arXiv:2406.02787Disentangling logic: The role of context in large language model reasoning capabilities. 2024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202436</p>
<p>A comprehensive evaluation of inductive reasoning capabilities and problem solving in large language models. Chen Bowen, Rune Saetre, Yusuke Miyao, Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Inductive reasoning in humans and large language models. Simon Jerome, Han , Keith J Ransom, Andrew Perfors, Charles Kemp, Cognitive Systems Research. 831011552024</p>
<p>Can chatgpt defend its belief in truth? evaluating llm reasoning via debate. Boshi Wang, Xiang Yue, Huan Sun, arXiv:2305.131602023arXiv preprint</p>
<p>Learning to generate better than your llm. Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun, arXiv:2306.118162023arXiv preprint</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen, arXiv:2310.20689Learning from mistakes makes llm better reasoner. 2023arXiv preprint</p>
<p>Towards incremental learning in large language models: A critical review. Mlaan Jovanovi, 2024</p>
<p>Yi Ren, Danica J Sutherland, arXiv:2407.10490Learning dynamics of llm finetuning. 2024arXiv preprint</p>
<p>Know where to go: Make LLM a relevant, responsible, and trustworthy searchers. Xiang Shi, Jiawei Liu, Yinpeng Liu, Qikai Cheng, Wei Lu, Decision Support Systems. 1143542024</p>
<p>When search engine services meet large language models: Visions and challenges. Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, Sumi Helal, IEEE Transactions on Services Computing, J. ACM. VNDecember 2024Article .. Publication date</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 202336</p>
<p>Towards general computer control: A multimodal agent for red dead redemption ii as a case study. Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, arXiv:2403.031862024arXiv preprint</p>
<p>Esc: Exploration with soft commonsense constraints for zero-shot object navigation. Kaiwen Zhou, Kaizhi Zheng, Connor Pryor, Yilin Shen, Hongxia Jin, Lise Getoor, Xin Eric, Wang , International Conference on Machine Learning. PMLR2023</p>
<p>Structured information extraction from scientific text with large language models. John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Gerbrand Andrew S Rosen, Kristin A Ceder, Anubhav Persson, Jain, Nature Communications. 15114182024</p>
<p>A search engine for discovery of scientific challenges and directions. Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Horng Duen, Diyi Chau, Eric Yang, Horvitz, Daniel S Weld, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Docbench: A benchmark for evaluating llm-based document reading systems. Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, Dong Yu, arXiv:2407.107012024arXiv preprint</p>
<p>Cocktail: A comprehensive information retrieval benchmark with llm-generated documents integration. Sunhao Dai, Weihao Liu, Yuqi Zhou, Liang Pang, Rongju Ruan, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen, arXiv:2405.165462024arXiv preprint</p>
<p>Jamplate: Exploring llm-enhanced templates for idea reflection. Xiaotong Xu, Jiayu Yin, Catherine Gu, Jenny Mar, Sydney Zhang, Jane L , E , Steven P Dow, Proceedings of the 29th International Conference on Intelligent User Interfaces. the 29th International Conference on Intelligent User Interfaces2024</p>
<p>Yan Shvartzshnaider, Vasisht Duddu, John Lacalamita, arXiv:2409.03735Llm-ci: Assessing contextual integrity norms in language models. 2024arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>How far are we on the decision-making of llms? evaluating llms' gaming ability in multi-agent environments. Jen-Tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R Lyu, arXiv:2403.118072024arXiv preprint</p>
<p>Cognitive bias in high-stakes decision-making with llms. Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian Mcauley, Zexue He, arXiv:2403.008112024arXiv preprint</p>
<p>Testing theory of mind in large language models and humans. Dalila James Wa Strachan, Giulia Albergo, Oriana Borghini, Eugenio Pansardi, Saurabh Scaliti, Krati Gupta, Alessandro Saxena, Stefano Rufo, Guido Panzeri, Manzi, Nature Human Behaviour. 2024</p>
<p>Uncovering the semantics of concepts using gpt-4. Le Gal, Balzs Mens, Kovcs, Guillem Michael T Hannan, Pros, Proceedings of the National Academy of Sciences. 12049e23093501202023</p>
<p>Chatgpt outperforms humans in emotional awareness evaluations. Zohar Elyoseph, Dorit Hadar-Shoval, Kfir Asraf, Maya Lvovsky, Frontiers in Psychology. 1411990582023</p>
<p>Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence. Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, Bing Qin, arXiv:2402.100732024arXiv preprint</p>
<p>Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M Liu, Jinfeng Zhou, Alvionna S Sunaryo, Juanzi Li, arXiv:2402.12071Tatia Lee, Rada Mihalcea, and Minlie Huang. Emobench: Evaluating the emotional intelligence of large language models. 2024arXiv preprint</p>
<p>Emotional intelligence of large language models. Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Jia Liu, Journal of Pacific Rim Psychology. 17183449092312139582023</p>
<p>Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Lijie Wen, arXiv:2402.16499Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments. 2024arXiv preprint</p>
<p>Towards efficient llm grounding for embodied multi-agent collaboration. Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, Zhen Wang, arXiv:2405.143142024arXiv preprint</p>
<p>Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Schlkopf, arXiv:2404.16698Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainability behaviors in a society of llm agents. 2024arXiv preprint</p>
<p>Blend: A benchmark for llms on everyday knowledge in diverse. Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Afina Rifki, Dimosthenis Putri, Hsuvas Antypas, Eunsu Borkakoty, Carla Kim, Abinew Perez-Almendros, Ali Ayele, arXiv:2406.09948Publication date: December 2024. cultures and languages. 2024VarXiv preprint</p>
<p>Extrinsic evaluation of cultural competence in large language models. Shaily Bhatt, Fernando Diaz, arXiv:2406.115652024arXiv preprint</p>
<p>Similar data points identification with llm: A human-in-the-loop strategy using summarization and hidden state insights. Xianlong Zeng, Fanghao Song, Ang Liu, arXiv:2404.042812024arXiv preprint</p>
<p>Toward human-level concept learning: Pattern benchmarking for ai algorithms. Andreas Holzinger, Anna Saranti, Alessa Angerschmid, Bettina Finzel, Ute Schmid, Heimo Mueller, 2023Patterns</p>
<p>Contextual object detection with multimodal large language models. Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, Chen Change Loy, International Journal of Computer Vision. 2024</p>
<p>Std-llm: Understanding both spatial and temporal properties of spatial-temporal data with llms. Yiheng Huang, Xiaowei Mao, Shengnan Guo, Yubin Chen, Youfang Lin, Huaiyu Wan, arXiv:2407.090962024arXiv preprint</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma, arXiv:2402.17766Shapellm: Universal 3d object understanding for embodied interaction. 2024arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Language-driven representation learning for robotics. Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang, arXiv:2302.127662023arXiv preprint</p>
<p>Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, arXiv:2311.15209Jenq-Neng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. 2023arXiv preprint</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan, arXiv:2311.014552023arXiv preprint</p>
<p>Aerialvln: Vision-and-language navigation for uavs. Shubo Liu, Hongsheng Zhang, Yuankai Qi, Peng Wang, Yanning Zhang, Qi Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Velma: Verbalization embodiment of llm agents for vision and language navigation in street view. Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang, Wang , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.024852023arXiv preprint</p>
<p>Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, arXiv:2402.114532024arXiv preprint</p>
<p>Data interpreter: An llm agent for data science. Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, arXiv:2402.186792024arXiv preprint</p>
<p>Large language models as data preprocessors. Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada, arXiv:2308.163612023arXiv preprint</p>
<p>Interactive table synthesis with natural language. Yanwei Huang, Yunfan Zhou, Ran Chen, Changhao Pan, Xinhuan Shu, Di Weng, Yingcai Wu, IEEE Transactions on Visualization and Computer Graphics. 2023</p>
<p>Language models are realistic tabular data generators. Vadim Borisov, Kathrin Seler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci, arXiv:2210.062802022arXiv preprint</p>
<p>Llm-based personalized recommendations in health. Boris A Galitsky, 2024</p>
<p>Determinants of llm-assisted decision-making. Eva Eigner, Thorsten Hndler, arXiv:2402.173852024arXiv preprint</p>
<p>Ideas are dimes a dozen: Large language models for idea generation in innovation. Karan Girotra, Lennart Meincke, Christian Terwiesch, Karl T Ulrich, SSRN 4526071. 2023</p>
<p>A map of exploring human interaction patterns with llm: Insights into collaboration and creativity. Jiayang Li, Jiale Li, Yunsheng Su, International Conference on Human-Computer Interaction. Springer2024</p>
<p>Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-Yi Lee, Shao-Hua Sun, arXiv:2405.06373Article. VNDecember 2024arXiv preprintJ. ACM. Publication date</p>
<p>Context-aware llm-based safe control against latent risks. Xiyu Quan Khanh Luu, Anh Deng, Yorie Van Ho, Nakahira, arXiv:2403.118632024arXiv preprint</p>
<p>Gyeongmin Kim, Taehyeon Kim, Shyam Sundar Kannan, L N Vishnunandan, Donghan Venkatesh, Byung-Cheol Kim, Min, arXiv:2309.16031Dynacon: Dynamic robot planner with contextual awareness via llms. 2023arXiv preprint</p>
<p>Large Language Model Powered Agents for Information Retrieval. An Zhang, Yang Deng, Yankai Lin, Xu Chen, Ji-Rong Wen, Tat-Seng Chua, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24New York, NY, USAAssociation for Computing MachineryJuly 2024</p>
<p>Adaptable logical control for large language models. Honghua Zhang, Po-Nien Kung, Masahiro Yoshida, arXiv:2406.138922024arXiv preprintGuy Van den Broeck, and Nanyun Peng</p>
<p>Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, arXiv:2402.01817Llms can't plan, but can help planning in llm-modulo frameworksgundawar2024robust. 2024arXiv preprint</p>
<p>Robust planning with llm-modulo framework: Case study in travel planning. Atharva Gundawar, Mudit Verma, Lin Guan, Karthik Valmeekam, Siddhant Bhambri, Subbarao Kambhampati, arXiv:2405.206252024arXiv preprint</p>
<p>Llm-assist: Enhancing closed-loop planning with language-based reasoning. Francesco Sp Sharan, Manmohan Pittaluga, Chandraker, arXiv:2401.001252023arXiv preprint</p>
<p>Mapping llm security landscapes: A comprehensive stakeholder risk assessment proposal. Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad Gressel, arXiv:2403.133092024arXiv preprint</p>
<p>Guard-d-llm: An llm-based risk assessment engine for the downstream uses of llms. Sandeep Vishwakarma, arXiv:2406.118512024arXiv preprint</p>
<p>Toby D Pilditch, arXiv:2402.01743The reasoning under uncertainty trap: A structural ai risk. 2024arXiv preprint</p>
<p>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang, arXiv:1804.06876Gender bias in coreference resolution: Evaluation and debiasing methods. 2018arXiv preprint</p>
<p>Large pre-trained language models contain human-like biases of what is right and wrong to do. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, Kristian Kersting, Nature Machine Intelligence. 432022</p>
<p>Epidemic modeling with generative agents. Ross Williams, Niyousha Hosseinichimeh, Aritra Majumdar, Navid Ghaffarzadegan, arXiv:2307.049862023arXiv preprint</p>
<p>Dissociating language and thought in large language models. Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Trends in Cognitive Sciences. 2024</p>
<p>Benchmarking the communication competence of code generation for llms and llm agent. J W Jie, Fatemeh H Wu, Fard, arXiv:2406.002152024arXiv preprint</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Frnken, Tobias Gerstenberg, Noah Goodman, Advances in Neural Information Processing Systems. 362024</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Role play with large language models. Murray Shanahan, Kyle Mcdonell, Laria Reynolds, Nature. 62379872023</p>
<p>Thilo Hagendorff, arXiv:2303.13988Machine psychology: Investigating emergent capabilities and behavior in large language models using psychological methods. 2023arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, arXiv:2310.021702023arXiv preprint</p>
<p>Can llm generate culturally relevant commonsense qa data? case study in indonesian and sundanese. Afina Rifki, Faiz Putri, Dea Ghifari Haznitrama, Alice Adhista, Oh, arXiv:2402.173022024arXiv preprint</p>
<p>How well do llms represent values across cultures? empirical analysis of llm responses based on hofstede cultural dimensions. Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah, arXiv:2406.148052024arXiv preprint</p>
<p>Chen-Chi Chang, Ching-Yuan Chen, Hung-Shin Lee, Chih-Cheng Lee, arXiv:2409.01556Benchmarking cognitive domains for llms: Insights from taiwanese hakka culture. 2024arXiv preprint</p>
<p>Analyzing cultural representations of emotions in llms through mixed emotion survey. Shiran Dudy, Ibrahim Said Ahmad, Ryoko Kitajima, Agata Lapedriza, arXiv:2408.021432024arXiv preprint</p>
<p>Culturalteaming: Ai-assisted interactive red-teaming for challenging llms'(lack of) multicultural knowledge. Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Stella Shuyue, Mehar Li, Sahithya Bhatia, Yulia Ravi, Vered Tsvetkov, Yejin Shwartz, Choi, arXiv:2404.066642024arXiv preprint</p>
<p>. J Acm, Article . Publication date. NVDecember 2024</p>
<p>Karen Glanz, Barbara K Rimer, Kasisomayajula Viswanath, Health Behavior: Theory, Research, and Practice. John Wiley &amp; Sons2015</p>
<p>AI and the Future of Collaborative Work: Group Ideation with an LLM in a Virtual Canvas. Jessica He, Stephanie Houde, Gabriel E Gonzalez, Daro Andrs, Silva Moran, Steven I Ross, Michael Muller, Justin D Weisz, Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work. the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for WorkNewcastle upon Tyne United KingdomACMJune 2024</p>
<p>AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation. Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L Kun, Hagit Ben Shoshan, Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing SystemsHonolulu HI USAACMMay 2024</p>
<p>PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning. Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vlez, Qingyun Wu, Huazheng Wang, Thomas L Griffiths, Mengdi Wang ; Jiawen, Yuanyuan Liu, Pengcheng Yao, Qi An, Wang, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024. May 2024Embodied LLM Agents Learn to Cooperate in Organized Teams</p>
<p>Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, March 2024</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 202336</p>
<p>Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, Michael S Bernstein, Rehearsal: Simulating Conflict to Teach Conflict Resolution. September 2023</p>
<p>Social Simulacra: Creating Populated Prototypes for Social Computing Systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and TechnologyBend OR USAACMOctober 2022</p>
<p>. Richard J Gerrig, Philip G Zimbardo, Andrew J Campbell, Steven R Cumming, Fiona J Wilkes, Psychology and Life. 2015Pearson Higher Education AU</p>
<p>Can ai language models replace human participants?. Danica Dillion, Niket Tandon, Yuling Gu, Kurt Gray, Trends in Cognitive Sciences. 2772023</p>
<p>Using large language models in psychology. Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, Michaela Jones, Danielle Krettek-Cobb, Leslie Lai, Nirel Jonesmitchell, Desmond C Ong, Carol S Dweck, James J Gross, James W Pennebaker, Nature Reviews Psychology. 211Nov 2023</p>
<p>Using cognitive psychology to understand gpt-like models needs to extend beyond human biases. Massimo Stella, Thomas T Hills, Yoed N Kenett, Proceedings of the National Academy of Sciences. 12043e23129111202023</p>
<p>The wisdom of partisan crowds: Comparing collective intelligence in humans and llm-based agents. Yun-Shiuan Chuang, Nikunj Harlalka, Siddharth Suresh, Agam Goyal, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T Rogers, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202446</p>
<p>Exploring collaboration mechanisms for llm agents: A social psychology view. Jintian Zhang, Xin Xu, Shumin Deng, arXiv:2310.021242023arXiv preprint</p>
<p>The Study of Language. George Yule, 2022Cambridge university press</p>
<p>ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning. Dac Viet, Lai, Trung Nghia, Amir Ngo, Ben Pouran, Hieu Veyseh, Franck Man, Trung Dernoncourt, Thien Huu Bui, Nguyen, April 2023</p>
<p>Symbols and grounding in large language models. Ellie Pavlick, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 381202200412251. July 2023</p>
<p>Power and Choice: An Introduction to Political Science. W Phillips Shively, David Schultz, 2022Rowman &amp; Littlefield</p>
<p>War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, January 2024</p>
<p>Modelling Political Coalition Negotiations Using LLM-based Agents. Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, Gholamreza Haffari, February 2024</p>
<p>Systematic Biases in LLM Simulations of Debates. Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein, February 2024</p>
<p>Evaluating the persuasive influence of political microtargeting with large language models. Kobi Hackenburg, Helen Margetts, Proceedings of the National Academy of Sciences. 12124e2403116121June 2024. December 2024Article .. Publication date</p>
<p>How persuasive is AI-generated propaganda?. Josh A Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, Michael Tomz, PNAS Nexus. 32e034February 2024</p>
<p>Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks. Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, Gareth Tyson, April 2023</p>
<p>LLM-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance Detection. Zhao Zhang, Yiming Li, Jin Zhang, Hui Xu, Proceedings of the 2024 Conference of the North American Chapter. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American ChapterMexico City, MexicoAssociation for Computational LinguisticsJune 20242Short Papers)</p>
<p>Stance Detection with Collaborative Role-Infused LLM-Based Agents. Xiaochong Lan, Chen Gao, Depeng Jin, Yong Li, Proceedings of the International AAAI Conference on Web and Social Media. the International AAAI Conference on Web and Social MediaMay 202418</p>
<p>The Silence of the LLMs: Cross-Lingual Analysis of Political Bias and False Information Prevalence in ChatGPT. Aleksandra Urman, Mykola Makhortykh, 2023Google Bard, and Bing Chat</p>
<p>Mitigating Biases of Large Language Models in Stance Detection with Calibration. Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Xingwei Liang, Kam-Fai Wong, Ruifeng Xu, June 2024</p>
<p>. N , Gregory Mankiw, Mark P Taylor, Economics. Cengage Learning EMEA. 2020</p>
<p>Fulin Guo, GPT in Game Theory Experiments. December 2023</p>
<p>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?. John J Horton, w31122April 2023National Bureau of Economic ResearchTechnical Report</p>
<p>Playing repeated games with Large Language Models. Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz, May 2023</p>
<p>An evolutionary model of personality traits related to cooperative behavior using a large language model. Reiji Suzuki, Takaya Arita, Scientific Reports. 1415989March 2024</p>
<p>Rethinking the Buyer's Inspection Paradox in Information Markets with Language Agents. Martin Weiss, Nasim Rahaman, Manuel Wuthrich, Yoshua Bengio, Li Erran Li, Bernhard Schlkopf, Christopher Pal, October 2023</p>
<p>CompeteAI: Understanding the Competition Dynamics in Large Language Model-based Agents. Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Chen Hao, Xing Xie, June 2024</p>
<p>Sociology: A Global Introduction. John J Macionis, Kenneth Plummer, 2005Pearson Education</p>
<p>Evaluating the moral beliefs encoded in llms. Nino Scherrer, Claudia Shi, Amir Feder, David Blei, Advances in Neural Information Processing Systems. 202436</p>
<p>Normad: A benchmark for measuring the cultural adaptability of large language models. Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, Maarten Sap, arXiv:2404.124642024arXiv preprint</p>
<p>Eticor: Corpus for analyzing llms for etiquettes. Ashutosh Dwivedi, Pradhyumna Lavania, Ashutosh Modi, arXiv:2310.189742023arXiv preprint</p>
<p>Ali-agent: Assessing llms' alignment with human values via agent-based evaluation. Jingnan Zheng, Han Wang, An Zhang, Tai D Nguyen, Jun Sun, Tat-Seng Chua, arXiv:2405.141252024arXiv preprint</p>
<p>Yan Tao, Olga Viberg, Ryan S Baker, Rene F Kizilcec, arXiv:2311.14096Auditing and mitigating cultural bias in llms. 2023arXiv preprint</p>
<p>Denevil: Towards deciphering and navigating the ethical values of large language models via instruction learning. Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu, arXiv:2310.110532023arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. P Maciej, Dane Polak, Morgan, Nature Communications. 15115692024</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, Social-network Simulation System with Large Language Model-Empowered Agents. October 20233</p>
<p>Large language models can infer psychological dispositions of social media users. H Peters, Matz, arXiv:2309.086312023arxiv. arXiv preprint</p>
<p>Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models. Yu Shang, Yu Li, Fengli Xu, Yong Li, August 2024</p>
<p>Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings. Cecilia Chen, Fajri Liu, Timothy Koto, Iryna Baldwin, Gurevych, arXiv:2309.085912023arXiv preprint</p>
<p>Gender bias and stereotypes in large language models. Hadas Kotek, Rikker Dockum, David Sun, Proceedings of the ACM collective intelligence conference. the ACM collective intelligence conference2023</p>
<p>. J Acm, Article . Publication date. NVDecember 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6243 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6243</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6243</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7" target="_blank">ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A multi-agent referee team called ChatEval is constructed to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks, offering a human-mimicking evaluation process for reliable assessments.</p>
                <p><strong>Paper Abstract:</strong> Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6243.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6243.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FairEval: Open-ended QA comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge and human evaluation on the FairEval open-ended question answering benchmark (reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison on FairEval showing humans achieve higher agreement and accuracy than LLM-based evaluators, while the proposed multi-agent ChatEval improves alignment with human judgments over single-agent LLM evaluators (quantified by accuracy and Kappa).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-ended question answering (FairEval benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo) and GPT-4 (both used as single-agent and as homogeneous multi-agent groups in ChatEval)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations (from Wu et al. / FairEval pipeline): three annotators per question; final results derived by majority vote among the three annotators; reported individual annotator accuracies 68.8%, 76.3%, and 70% (average 71.7%) and Kappa values 0.5, 0.62, 0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy (Acc., proportion of correctly classified instances) and Cohen-style Kappa correlation coefficient (Kap.) measuring agreement between model evaluations and human annotators (accounting for chance agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Humans outperformed LLM-based evaluators: average human annotation accuracy was 71.7%, higher than any LLM-based method reported. Multi-agent ChatEval improved alignment with human preference relative to single-agent: for ChatGPT accuracy rose from 53.8% (single-agent) to 60.0% (multi-agent, +6.2 percentage points); for GPT-4 accuracy rose from 61.3% to 63.8% (+2.5 points). Kappa values likewise increased modestly for multi-agent over single-agent.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Single-agent LLM evaluation exhibits bias and instability from relying on a single perspective; LLM-based evaluators (even multi-agent) still fall short of human agreement levels on FairEval. Multi-agent systems with homogeneous role prompts fail to improve performance. Increasing discussion turns or context length may degrade LLM evaluator performance (context-length-induced degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Multi-agent setup with identical role prompts produced no improvement over single-agent (Table 3: Multi-Agent same-role prompt accuracy 53.8%, Kap. 0.25 equals single-agent accuracy 53.8% Kap. 0.27), demonstrating that naive multi-agent deployment can fail; adding too many roles (5 roles) led to a decline in accuracy; increasing discussion turns did not improve and can stagnate or worsen performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use diverse role specifications (different persona prompts) for agents — this was essential for gains; prefer one-by-one communication strategy for ChatGPT in these experiments; tune number of diverse roles (performance peaked around 3–4 roles); limit discussion turns to avoid context-length degradation; apply position-calibration techniques (used in both single- and multi-agent baselines) and aggregate judgments by majority vote or averaging scores to ensure balanced final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6243.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6243.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Topical-Chat: Dialogue evaluation comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of LLM-as-a-judge and human evaluation on the Topical-Chat dialogue response generation benchmark (reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of ChatEval vs single-agent LLMs and prior LLM-based evaluators on Topical-Chat showing ChatEval (multi-agent) yields higher correlation with human judgments (Spearman and Kendall Tau) than single-agent and prior methods, especially for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dialogue response generation (Topical-Chat dataset), evaluated on aspects: naturalness, coherence, engagingness, groundedness</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo) and GPT-4 used as single-agent evaluators and as homogeneous multi-agent groups (ChatEval)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments from Mehri & Eskenazi on 60 dialogue contexts with responses from 6 systems, annotated on multiple dimensions (paper uses human judgments as the gold standard for correlation computations).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Turn-level Spearman (ρ) and Kendall-Tau (τ) correlations between model evaluator scores and human judgments on each aspect and averaged across aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ChatEval (multi-agent) improved correlation with humans relative to prior LLM-based baselines: for GPT-4, ChatEval improved average Spearman by 0.096 (16.3%) and Kendall-Tau by 0.057 (10.0%) compared to G-EVAL-4; relative to single-agent GPT-4, multi-agent GPT-4 also increased correlations (e.g., average Spearman from 0.658 to 0.684). Multi-agent improvements were observed for both ChatGPT and GPT-4, though gains were larger for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Different communication strategies produce varying effectiveness (one-by-one outperformed simultaneous strategies for ChatGPT); some multi-agent strategies (simultaneous-talk, simultaneous-talk-with-summarizer) were less robust than one-by-one though still better than naive single-agent; LLM-based metrics (n-gram and embedding based) performed poorly overall. LLM evaluators may still lack human-level nuanced judgment across dialogue dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Certain communication strategies underperformed (simultaneous-talk and simultaneous-talk-with-summarizer gave lower Acc./Kap. or correlations than one-by-one); single-agent methods lag behind human alignment; heterogeneity of agents was not explored (only homogeneous groups used), which may limit robustness in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Adopt multi-agent debate (ChatEval) with diverse persona role prompts; prefer one-by-one speaking order for the ChatGPT setting; aggregate multiple annotators' opinions via majority vote or averaged scores; employ position calibration techniques; carefully choose communication strategy and tune agent count/turns to maximize correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is chatgpt a good nlg evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 1)</em></li>
                <li>Are large language models good evaluators for abstractive summarization? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6243",
    "paper_id": "paper-ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "FairEval: Open-ended QA comparison",
            "name_full": "Comparison of LLM-as-a-judge and human evaluation on the FairEval open-ended question answering benchmark (reported in this paper)",
            "brief_description": "Empirical comparison on FairEval showing humans achieve higher agreement and accuracy than LLM-based evaluators, while the proposed multi-agent ChatEval improves alignment with human judgments over single-agent LLM evaluators (quantified by accuracy and Kappa).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-ended question answering (FairEval benchmark)",
            "llm_judge_model": "ChatGPT (GPT-3.5-turbo) and GPT-4 (both used as single-agent and as homogeneous multi-agent groups in ChatEval)",
            "human_evaluation_setup": "Human annotations (from Wu et al. / FairEval pipeline): three annotators per question; final results derived by majority vote among the three annotators; reported individual annotator accuracies 68.8%, 76.3%, and 70% (average 71.7%) and Kappa values 0.5, 0.62, 0.5.",
            "metrics_compared": "Accuracy (Acc., proportion of correctly classified instances) and Cohen-style Kappa correlation coefficient (Kap.) measuring agreement between model evaluations and human annotators (accounting for chance agreement).",
            "reported_differences": "Humans outperformed LLM-based evaluators: average human annotation accuracy was 71.7%, higher than any LLM-based method reported. Multi-agent ChatEval improved alignment with human preference relative to single-agent: for ChatGPT accuracy rose from 53.8% (single-agent) to 60.0% (multi-agent, +6.2 percentage points); for GPT-4 accuracy rose from 61.3% to 63.8% (+2.5 points). Kappa values likewise increased modestly for multi-agent over single-agent.",
            "llm_specific_limitations": "Single-agent LLM evaluation exhibits bias and instability from relying on a single perspective; LLM-based evaluators (even multi-agent) still fall short of human agreement levels on FairEval. Multi-agent systems with homogeneous role prompts fail to improve performance. Increasing discussion turns or context length may degrade LLM evaluator performance (context-length-induced degradation).",
            "notable_failure_cases": "Multi-agent setup with identical role prompts produced no improvement over single-agent (Table 3: Multi-Agent same-role prompt accuracy 53.8%, Kap. 0.25 equals single-agent accuracy 53.8% Kap. 0.27), demonstrating that naive multi-agent deployment can fail; adding too many roles (5 roles) led to a decline in accuracy; increasing discussion turns did not improve and can stagnate or worsen performance.",
            "mitigation_strategies": "Use diverse role specifications (different persona prompts) for agents — this was essential for gains; prefer one-by-one communication strategy for ChatGPT in these experiments; tune number of diverse roles (performance peaked around 3–4 roles); limit discussion turns to avoid context-length degradation; apply position-calibration techniques (used in both single- and multi-agent baselines) and aggregate judgments by majority vote or averaging scores to ensure balanced final outputs.",
            "uuid": "e6243.0",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Topical-Chat: Dialogue evaluation comparison",
            "name_full": "Comparison of LLM-as-a-judge and human evaluation on the Topical-Chat dialogue response generation benchmark (reported in this paper)",
            "brief_description": "Evaluation of ChatEval vs single-agent LLMs and prior LLM-based evaluators on Topical-Chat showing ChatEval (multi-agent) yields higher correlation with human judgments (Spearman and Kendall Tau) than single-agent and prior methods, especially for GPT-4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dialogue response generation (Topical-Chat dataset), evaluated on aspects: naturalness, coherence, engagingness, groundedness",
            "llm_judge_model": "ChatGPT (GPT-3.5-turbo) and GPT-4 used as single-agent evaluators and as homogeneous multi-agent groups (ChatEval)",
            "human_evaluation_setup": "Human judgments from Mehri & Eskenazi on 60 dialogue contexts with responses from 6 systems, annotated on multiple dimensions (paper uses human judgments as the gold standard for correlation computations).",
            "metrics_compared": "Turn-level Spearman (ρ) and Kendall-Tau (τ) correlations between model evaluator scores and human judgments on each aspect and averaged across aspects.",
            "reported_differences": "ChatEval (multi-agent) improved correlation with humans relative to prior LLM-based baselines: for GPT-4, ChatEval improved average Spearman by 0.096 (16.3%) and Kendall-Tau by 0.057 (10.0%) compared to G-EVAL-4; relative to single-agent GPT-4, multi-agent GPT-4 also increased correlations (e.g., average Spearman from 0.658 to 0.684). Multi-agent improvements were observed for both ChatGPT and GPT-4, though gains were larger for GPT-4.",
            "llm_specific_limitations": "Different communication strategies produce varying effectiveness (one-by-one outperformed simultaneous strategies for ChatGPT); some multi-agent strategies (simultaneous-talk, simultaneous-talk-with-summarizer) were less robust than one-by-one though still better than naive single-agent; LLM-based metrics (n-gram and embedding based) performed poorly overall. LLM evaluators may still lack human-level nuanced judgment across dialogue dimensions.",
            "notable_failure_cases": "Certain communication strategies underperformed (simultaneous-talk and simultaneous-talk-with-summarizer gave lower Acc./Kap. or correlations than one-by-one); single-agent methods lag behind human alignment; heterogeneity of agents was not explored (only homogeneous groups used), which may limit robustness in some cases.",
            "mitigation_strategies": "Adopt multi-agent debate (ChatEval) with diverse persona role prompts; prefer one-by-one speaking order for the ChatGPT setting; aggregate multiple annotators' opinions via majority vote or averaged scores; employ position calibration techniques; carefully choose communication strategy and tune agent count/turns to maximize correlation with human judgments.",
            "uuid": "e6243.1",
            "source_info": {
                "paper_title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 1
        },
        {
            "paper_title": "Are large language models good evaluators for abstractive summarization?",
            "rating": 1
        }
    ],
    "cost": 0.0110645,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ChatEval: Towards better LLM-based EVALUATORS THROUGH MULTI-AGENT DEBATE</h1>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Zhiyuan Liu*<br>Department of Computer Science and Technology<br>Tsinghua University<br>zorowin123@gmail.com</p>
<p>Jie Fu, Wei Xue
Hong Kong University of Science and Technology</p>
<p>Shanghai Zhang
Peking University</p>
<h4>Abstract</h4>
<p>Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agentbased approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. We derive insights and lessons from practical scenarios where humans instigate group discussions for brainstorming and propose different communication strategies within ChatEval. Our experiments on two benchmark tasks illustrate that ChatEval delivers superior accuracy and correlation in alignment with human assessment. Furthermore, we find that the diverse role prompts (different personas) are essential in the multi-agent debate process; that is, utilizing the same role description in the prompt can lead to a degradation in performance. Our qualitative analysis also shows that ChatEval transcends mere textual scoring, offering a humanmimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.</p>
<h2>1 INTRODUCTION</h2>
<p>Evaluating the quality of text generated by language models or written by humans has long been a challenging endeavor, consistently garnering substantial attention (Celikyilmaz et al., 2020). Traditional methodologies predominantly rely on human annotation of texts (Callison-Burch, 2009), an approach considered overly demanding in terms of time and cost. Automatic evaluation metrics based on n-grams, such as Rouge (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Banerjee \&amp; Lavie, 2005), have been proposed to tackle this issue (Kondrak, 2005). However, these methods have been shown to exhibit a relatively weak correlation with human judgments, particularly in the context of tasks involving open-ended generation or requiring domain-specific expertise (Novikova et al., 2017).</p>
<p>Recent advancements in the field of natural language processing have led to the emergence of billion-parameter scale LLMs, such as GPT-3 (Brown et al., 2020). These LLMs have demon-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>strated remarkable capabilities across diverse downstream tasks, presenting new opportunities for text quality evaluation using such models. Moreover, various training paradigms have been proposed to endow LLMs with the ability to accomplish tasks in a zero-shot manner and better adhere to human-provided instructions (Ouyang et al., 2022; Sanh et al., 2021; Wei et al., 2021). These advancements facilitate the prompting of LLMs to evaluate generated text, effectively simulating human evaluators in the assessment process.</p>
<p>In view of the impressive text understanding and instruction-following capabilities of recent LLMs, a body of literature (Liu et al., 2023b; Chiang \&amp; Lee, 2023; Gao et al., 2023; Shen et al., 2023) has adopted LLM as an evaluator to assess the quality of responses to open-ended questions or traditional NLG tasks, including dialogue response generation and summarization. This methodology is dubbed LLM-as-a-judge (Zheng et al., 2023). Findings from these researches indicate that LLM can mimic human behavior and provide evaluations that correspond with human judgments, revealing a potentially scalable and transparent alternative to costly and laborious human evaluations.</p>
<p>While a single powerful LLM can already tackle various missions, emerging studies suggest that multiple LLMs can further improve one another through debate and cooperation (Li et al., 2023a; Liang et al., 2023). By incorporating multiple LLMs into an integrated group and designing specific interaction mechanisms, different LLMs can engage in proposing and deliberating unique responses and thought processes across several rounds. This approach leads to enhanced factuality of generated responses (Du et al., 2023) and improvement in the completion of arduous tasks (Li et al., 2023a; Qian et al., 2023). Furthermore, the multi-agent group also addresses and mitigates the Degeneration-of-Thought (DOT) problem (Liang et al., 2023).</p>
<p>In the human evaluation processes, relying on a single perspective can introduce bias and instability in the results (Karpinska et al., 2021). Recognizing this, best practices often involve multiple human annotators collaborating in the evaluation (Van Der Lee et al., 2019). Drawing inspiration from this collaborative and iterative human evaluation approach, we propose ChatEval, a system that enables each agent to employ varied communication strategies in collaborative discussion, working towards formulating final judgments. Furthermore, to enrich the evaluation dynamics, every agent within ChatEval is endowed with a unique persona. This deliberate design ensures that each agent focuses on distinct perspectives or brings specific expertise to the table. By doing so, the collective evaluation benefits from a more comprehensive lens, capturing nuances and subtleties that a single perspective might overlook. We derive this idea primarily from the insight of 'There are a thousand Hamlets in a thousand people's eyes', meaning that every person has their unique interpretation or perspective, especially applicable to text evaluation. Indeed, these divergent perspectives shape the comprehensive and multifaceted assessment of Hamlet. Another underlying intuition of our work stems from renowned concepts in sociology and biology, including Collective Intelligence(Woolley et al., 2010) and Cognitive Synergy(Luppi et al., 2022), where multiple cognitive processes or systems interact and cooperate in a way that produces a combined effect greater than the sum of their separate effects.</p>
<p>To summarize, the main contribution of our work is as follows:</p>
<ol>
<li>We propose a multi-agent-based framework called ChatEval that aligns better with human preferences compared with single-agent-based approaches as depicted in Figure 1.</li>
<li>We propose various communication strategies and demonstrate the necessity of diverse role prompts in multi-agent debate scenarios.</li>
<li>We release our library. It's designed to be both composable and scalable, enabling researchers to implement their unique communication strategies easily. We hope this contributes to advancing research in the field of communicative agents and beyond.</li>
</ol>
<h1>2 Methodology</h1>
<p>In this section, we elaborate on the principal components in ChatEval including debater agents, diverse role specification, communication strategy, and provide a detailed overview of each component's role and functionality ${ }^{1}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: When several referees participate in the evaluation process, they can discuss with each other and finally give a judgment that is better aligned with human annotators.</p>
<p>Debater Agents. Debater agents are one of the most significant components in our framework. We treat each individual LLM as an agent and ask them to generate their response from the given prompt ${ }^{2}$. Responses from other agents are served as chat history which will be replaced in the prompt template. After configuring the agents, we then start the group debate where each agent autonomously receives responses from the others and, in turn, delivers its own responses to them. It should be noted that the whole process does not require human intervention.</p>
<p>Diverse Role Specification. As presented in Section 1, diverse role specification is necessary for the framework as well. Although all the agents share a common prompt template, we substitute the role_description slot with diverse role prompts, specifying distinct personalities for different agents. We take inspiration from Wu et al. (2023) and formulate an analogous role description.</p>
<p>Communication Strategy. How to maintain the chat history is another significant issue in ChatEval. In our work, we use a more intuitive term to illustrate the maintenance of the chat history called communication strategy. In a nutshell, different communication strategies can be seen as different approaches to maintaining and manipulating their chat history. As is shown in Figure 2, We primarily design three different communication strategies and illustrate them as follows:</p>
<ol>
<li>One-By-One. During each round of the debate, the debater agents take turns in a set order to generate their response based on the current observation. When it's time for a debater agent to respond, we directly concatenate what previous other agents have said into its chat history slot.</li>
<li>Simultaneous-Talk. Unlike the one-by-one strategy, we carry out an alternative communication strategy called simultaneous-talk, where debater agents are prompted to asynchronously generate responses in each iteration of the discussion to nullify the impact of the speaking order.</li>
<li>Simultaneous-Talk-with-Summarizer. The main difference between this strategy and simultaneous-talk is that we additionally employ another LLM as a summarizer. At the end of each iteration of the debate, we prompt this extra LLM to summarize the messages conveyed so far and concatenate this summarization into all debater agents' chat history slots.
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The overall schematic diagram of our proposed three different kinds of communication strategy. The direction of the arrows represents the flow of information, meaning that what this person says will be appended to the chat history of the person pointed to by the arrow. Full algorithm description of the above communication strategies can be found in Appendix B.</p>
<p>Unlike previous work like Du et al. (2023), we do not explicitly ask the debater agents to reach a consensus at the end of the debate. In situations where the response format relies on direct comparison, we derive the final results from the majority vote among various annotators. Conversely, if the response format requires a direct score, we calculate the average score obtained from multiple annotators. This methodological approach ensures the impartiality and balance of our evaluation process.</p>
<h1>3 EXPERIMENTS</h1>
<p>We evaluate ChatEval on two benchmarks, FairEval and Topical-Chat which represent the categories of open-ended question answer and dialogue response generation, respectively.</p>
<h3>3.1 IMPLEMENTATION DETAILS</h3>
<p>We choose to utilize models from OpenAI's GPT family as our LLMs in ChatEval, including GPT-4 and ChatGPT (GPT-3.5-turbo) and set the temperature to 0 to ensure reproducibility. The rationale behind this selection is the exceptional performance these models offer, being among the most advanced and powerful in the world. Additionally, their accessibility and ease of use through APIs enable us to directly call and interact with the models during our research, significantly simplifying the process. In our current research, we focus on homogeneous groups of LLMs. That is, within a given multi-agent group, all LLMs belong to the same GPT family model, either all GPT-4 or all ChatGPT. We acknowledge the potential of heterogeneous groups for future research, which could provide fascinating insights into how strong models and weak models can cooperate in a multi-agent setting.</p>
<h3>3.2 BENCHMARKS</h3>
<p>The detailed introduction of different categories and benchmarks are listed as follows:
Open-ended Question Answer is a key component within the field of NLP and generative AI. It necessitates an AI system to provide comprehensive, detailed, and human-like responses to questions that don't have a predefined or fixed set of possible answers. The work of Chiang et al. (2023) encompasses a collection of 80 open-ended questions originating from a wide array of categories, including common-sense, counterfactual, coding, etc. We then take the human annotation results from Wu et al. (2023) to conduct the experiments in this paper. For each question, they direct three annotators to evaluate the replies given by Vicuna-13B and ChatGPT through the given rules and finally derive the results by the majority votes among the annotators.</p>
<p>Dialogue Response Generation is a task involves creating a coherent and contextually appropriate response to a given input dialogue. We draw upon the Topical-Chat (Gopalakrishnan et al., 2019) dataset for our study. We then take the human annotation results from Mehri \&amp; Eskenazi (2020) where they carry out the annotations on 60 dialogue contexts with each response generated by 6 different systems. Human evaluators analyzed these responses based on natural, coherence, engagingness, groundedness, and understandable, where we take the first four dimensions for experiments in our paper following Zhong et al. (2022).</p>
<h1>3.3 BASELINES</h1>
<p>We evaluate ChatEval against following methods. As the main portion of our comparison, we primarily focuses on the single-agent-based method. Single-Agent means that we directly query an LLM to generate the response towards the evaluation ${ }^{3}$. We use Multi-Agent to represent ChatEval where several agents discuss towards the evaluation. By default, we configure the communication strategy to one-by-one, agent numbers to 2 , and discussion turns to 2 in this section and employ position calibration techniques in both single-agent and multi-agent settings. We will discuss more debate configurations in Section 4 for completeness. For the open-ended question answer task, we also compare our method with FairEval (Wang et al., 2023b). They propose various strategies to improve the evaluation performance of a LLM including Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC). For the dialogue response generation task, we also compare our method with G-EVAL (Liu et al., 2023b). They utilize CoT and probability-weighted summation for their method. Additionally, we include results from n-gram-based metrics, such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and embedding-based metrics such as BERTScore (Zhang et al., 2019).</p>
<h3>3.4 RESULTS FOR OPEN-ENDED QUESTION ANSWERS</h3>
<p>We adopt the same evaluation approach as Wang et al. (2023b) to assess the annotation results produced by different methods and annotators. Specifically, we calculate the Accuracy (Acc.), which measures the proportion of correctly classified instances out of the total instances, and the Kappa correlation coefficient (Kap.) (McHugh, 2012) which gauges the agreement between results from models and human annotators while taking into account the possibility of agreement occurring by chance. Both metrics provide insights into the reliability and consistency of the annotations. We take the human annotation results and FairEval's (Wang et al., 2023b) best results from their paper. As is shown in Table 1, different annotators can reach a relatively high agreement and perform better than any other LLM-based approach. Still, the average human annotations accuracy which is $71.7 \%$ shows there exists a certain degree of discrepancy among different unique individuals revealing that text evaluation is absolutely an arduous task. The second part and the third part of Table 1 show the results of FairEval's method and the results of our proposed method respectively. We find that (1) ChatEval can enhance the performance of the evaluation process, achieving higher alignment with human preference compared with single-agent evaluation. Specifically, the multi-agent-based method improves the accuracy by $6.2 \%$ for ChatGPT and $2.5 \%$ for GPT-4; (2) ChatEval surpasses FairEval's best results within both ChatGPT and GPT-4 settings showing the effectiveness of our proposed method.</p>
<h3>3.5 RESULTS FOR DIALOGUE RESPONSE GENERATION</h3>
<p>For the dialogue response generation benchmarks, we align the evaluation method with Zhong et al. (2022), calculating the turn-level Spearman and Kendall-Tau correlation in correspondence with human judgments on four aspects (naturalness, coherence, engagingness and groundedness). Results can be found in Table 2. In the first part of Table 2, we demonstrate that n-gram-based metrics and embedding-based metrics perform overall poorly on all the aspects evaluated illustrating that these methods can hardly reveal human preference. In the second part of Table 2, we show the results from the G-eval (Liu et al., 2023b) paper. They first ask the LLM to generate intermediate thought and finally calculate the weighted summation of the output scores based on the probability. The results show that their method outperforms previous traditional metrics depicting the fact that the LLM-based evaluator is effective and reliable for evaluating the dialogue response generation task. While their method delivers sound results, our proposed approach raises the bar in terms of performance for GPT-4. Specifically, ChatEval improves the average Spearman and KendallTau correlation by $0.096(16.3 \%)$ and $0.057(10.0 \%)$ respectively. Additionally, compared with the single-agent method, ChatEval amplifies the performance both for ChatGPT and GPT-4, showing the effectiveness of our method which is aligned with the results in Section 3.4.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Accuracy (Acc.) and Kappa correlation coefficient (Kap.) of different methods on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Acc. (\%)</th>
<th style="text-align: left;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Annotator1</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">68.8</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Annotator2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">76.3</td>
<td style="text-align: left;">0.62</td>
</tr>
<tr>
<td style="text-align: left;">Annotator3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">70</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">FairEval</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">MEC+BPC</td>
<td style="text-align: left;">58.7</td>
<td style="text-align: left;">0.31</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">MEC+BPC</td>
<td style="text-align: left;">62.5</td>
<td style="text-align: left;">0.37</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: left;">53.8</td>
<td style="text-align: left;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent</td>
<td style="text-align: left;">$\mathbf{6 0 . 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: left;">0.36</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Multi-Agent</td>
<td style="text-align: left;">$\mathbf{6 3 . 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 0}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Turn-level Spearman ( $\rho$ ) and Kendall-Tau ( $\tau$ ) correlations of different methods on TopicalChat benchmark, SA means Single-Agent and MA means Multi-Agent. Our ChatGPT settings should be compared to G-EVAL-3.5, and GPT-4 settings should be compared to G-EVAL-4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Engagingness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Groundedness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\tau$</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.244</td>
</tr>
<tr>
<td style="text-align: left;">BLEU-4</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.189</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.262</td>
</tr>
<tr>
<td style="text-align: left;">G-EVAL-3.5</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.574</td>
</tr>
<tr>
<td style="text-align: left;">G-EVAL-4</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 4}$</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.627</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.575</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(SA)</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.503</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT(MA)</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.510</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(SA)</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.591</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 0}$</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.611</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4(MA)</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 9}$</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 5}$</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">$\mathbf{0 . 6 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 2}$</td>
</tr>
</tbody>
</table>
<h1>4 ANALYSIS</h1>
<p>In this section, we further explore the key components encompassed in ChatEval. We discuss the importance of diverse role prompts in Section 4.1, the effect of different communication strategies in Section 4.2, and the impact of role numbers and discussion turns in Section 4.3. If not specified otherwise, we choose the FairEval benchmark and ChatGPT as the backbone LLM for the analysis.</p>
<h3>4.1 THE IMPORTANCE OF DIVERSE ROLE PROMPTS</h3>
<p>Previously in Table 1 and 2, we demonstrate that ChatEval equipped with diverse role configurations can significantly improve the performance of evaluation. We further consider whether it is necessary to design diverse role prompts for the evaluation system. To answer so, we carry out the experiments by replacing all the role prompt with "You are now an Annotator, one of the referees in the text evaluation task." and keeping other prompt unchanged. We experiment with the one-by-one communication strategy and 2 agents with 2 discussion turns. The results in Table 3 illustrate that ChatEval with the same role prompt design underperforms that with diverse role prompt design and cannot effectively enhance the performance compared with single-agent setting, highlighting the cruciality of diverse role prompt design in the multi-agent debate framework.</p>
<h3>4.2 THE STUDY OF COMMUNICATION STRATEGIES</h3>
<p>As shown in Figure 2, we also design three different communication strategy termed as one-by-one, simultaneous-talk, simultaneous-talk-with-summarizer. The detailed descriptions and formal for-</p>
<p>mulations can be found in Appendix B. We experiment with 3 agents and 2 discussion turns with diverse role prompts in this section. As is shown in Table 4, we can find that the one-by-one communication strategy is more effective than other strategies for ChatGPT setting. Although the other two communication strategies did not perform as robustly as the one-by-one strategy, it is noteworthy that they still exceeded the performance of the naive single-agent method. Furthermore, the variations in performance among three different communication strategies underscore the influence of different strategies on the effectiveness of the evaluation process, revealing the potential for further exploration and optimization of ChatEval. Thus, future studies could be aimed at a more comprehensive understanding of different communication strategies, and how they could be effectively employed to enhance performance. This could serve as an avenue for substantial improvements and novel insights in the multi-agent debate framework.</p>
<h1>4.3 THE IMPACT OF ROLE NUMBERS AND DISCUSSION TURNS</h1>
<p>We then study the impact of different role numbers and discussion turns. From Figure 3a, a discernible trend is observed in the relationship between the role number and both Acc. and Kap. within the FairEval dataset. As the role number increases, there is a corresponding growth in Acc. and Kap. This pattern reaches an apex with an Acc. of $62.5 \%$ at role numbers 3 and 4 before declining at role number 5. Despite this decline in accuracy, Kap. continues to ascend between role numbers 3 and 4, underscoring the effectiveness of incorporating diverse roles within ChatEval.</p>
<p>Conversely, no significant upward trend is detected with respect to the increase in discussion turns, as is shown in Figure 3b. This observation aligns with the findings in Liang et al. (2023); Du et al. (2023), highlighting a consistent phenomenon where continual discussion often leads to stagnation or even degradation of performance. Such a trend may be attributed to issues associated with the ever-increasing context length, which consequently diminishes the performance. This insight prompts a more nuanced understanding of the balance needed between role differentiation and discussion dynamics to optimize the performance of ChatEval.</p>
<h3>4.4 QUALITATIVE ANALYSIS</h3>
<p>Table 5 showcases the debate process towards the evaluation of two assistants' responses to the open-ended question "What are the most effective ways to deal with stress?".
Assistant 1's response is :
Here are some effective ways to deal with stress:</p>
<ol>
<li>Exercise regularly: Physical activity can help reduce stress and improve mood.</li>
<li>Practice relaxation techniques: Deep breathing, meditation, and yoga are some effective ways to relax and reduce stress levels.</li>
<li>Get enough sleep: Lack of sleep can increase stress levels. Try to get at least 7-8 hours of sleep each night.</li>
<li>Eat a healthy diet: A well-balanced diet can help reduce stress levels and improve overall health.</li>
<li>Connect with others: Spending time with friends and family, joining a support group, or participating in social activities can help reduce stress levels and improve mood.</li>
<li>Time management: Prioritizing tasks and creating a schedule can help reduce stress and increase productivity.</li>
<li>Seek professional help: If stress is impacting your daily life, consider seeking professional help such as therapy or counseling.</li>
</ol>
<p>Assistant 2's response is :
Stress can be caused by a variety of factors, including work, relationships, health, and financial issues. Here are some effective ways to manage stress:</p>
<ol>
<li>Exercise: Regular exercise releases endorphins, which are natural stressrelievers. It can be as simple as going for a walk or jog, practicing yoga, or joining a fitness class.</li>
<li>Mindfulness: Mindfulness practices, such as meditation or deep breathing, can</li>
</ol>
<p>help to reduce stress and improve focus.
3. Social Support: Spending time with family and friends, or seeking support from a mental health professional, can provide a sense of connection and help to reduce feelings of isolation and loneliness.
4. Healthy Eating: A balanced diet that includes plenty of fruits, vegetables, and whole grains can help to reduce stress levels.
5. Good Sleep: Getting enough sleep is essential for good mental and physical health. Aim for 7-8 hours per night and establish a regular sleep routine.
6. Time Management: Prioritizing tasks and setting realistic goals can help to reduce stress levels by giving a sense of control and direction.
7. Relaxation Techniques: Techniques such as progressive muscle relaxation, aromatherapy, and massage can help to reduce stress levels and promote relaxation.
8. Seek professional help: If stress is interfering with daily life, it may be time to seek professional help from a therapist or counselor.</p>
<p>We can find that both of the responses produce similar strategies and equally compelling descriptions for dealing with stress, making it challenging to discern significant disparity in terms of quality. It is in this context of nuanced evaluation that the significance of the ChatEval process emerges. To understand this complexity better, We first outline the ChatEval process and subsequently delve into the agents' constructive behaviors during discussions.</p>
<p>As is shown in Table 5, Alice first points out that the response of Assistant 2 contains more detailed information and he prefers to choose Assistant 2 as a better response. Bob then says that she agrees with Alice's assessments, but in the meantime, she also points out that Assistant 1's response is also concise and carries out a thought-provoking question. Carol then gives the feedback that she believes both responses are equally valuable. In the subsequent discussion, Bob indicates that Assistant 1's response is straightforward while Assistant 2's is detailed, suggesting that the effectiveness of the response should depend on the context and individual's needs. At the end of the debate, we finally extract the evaluation results that both responses are of the same quality which is identical to human annotation results.</p>
<p>From this sequence, we can pinpoint several fascinating behaviors exhibited by the agents: (1) Opening Statement: Alice initiates the debate with a clear stance, establishing the foundational argument and guiding the trajectory of the subsequent discourse. (2) Alternative Proposal: Bob introduces an alternative viewpoint, emphasizing the need to consider diverse interpretations. This not only broadens the discussion but also stimulates critical thinking. In the context of a debate, the introduction of an alternative proposal prevents the stagnation of thought, challenges pre-existing bias, and uncovers considerations that might otherwise be overlooked, ensuring that the discussions are well-rounded. (3) Stance Maintenance: Alice's persistent adherence to her initial stance, even when faced with opposing views, exemplifies commitment and challenges other participants to refine their perspectives. By firmly holding his position, Alice encourages depth in the discourse, prompting others to dive deeper into their arguments and perhaps consider aspects they hadn't previously. It ensures the conversation remains robust, focused, and continually evolving, driving all participants to a higher level of engagement and critical thinking. (4) Seeking Consensus: The discussion's climax reveals a collective agreement amongst the participants, which is reached through mutual understanding and compromise, underlining the value of each presented viewpoint.</p>
<p>In light of the above, ChatEval stands out not just as a tool for comparison but as an embodiment of interactive natural language dialogue. By simulating human argumentative interactions, it differentiates itself from static, single-presented opinions. This dynamic interaction showcases the richness and complexity of language, capturing nuances often missed in singular viewpoints. As such, ChatEval offers a reliable evaluation process that not only mirrors human discourse but also highlights the transformative power of collaborative dialogue. This positions it uniquely, underscoring its significant potential to execute text evaluation tasks both reliably and effectively.</p>
<h1>5 Related Work</h1>
<p>Automatic NLG evaluation In the landscape of NLG, evaluating the quality of generated text represents a particularly arduous task. For a significant period, evaluation was primarily dependent on</p>
<p>Table 3: Effect of diverse role specification on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Acc. (\%)</th>
<th style="text-align: center;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Single-Agent</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent (Same Role Prompt)</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Multi-Agent (Diverse Role Prompt)</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.33</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparing of different communication strategies on FairEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Evaluator</th>
<th style="text-align: left;">Communication Strategies</th>
<th style="text-align: center;">Acc. (\%)</th>
<th style="text-align: center;">Kap.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">One-by-One</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Simultaneous-Talk</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: left;">Simultaneous-Talk-with-Summarizer</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">0.27</td>
</tr>
</tbody>
</table>
<p>human annotations, a process that is labor-intensive and limited by scalability issues. Automatic NLG evaluation attempts to address these challenges by leveraging computational models to assess the quality of a generated text. Previous work lies on the following categories: (1) $n$-gram-based metrics: ROUGE (Lin, 2004) is a set of metrics that compute the amount of overlap between ngrams in the machine-generated summaries and the reference summaries. BLEU (Papineni et al., 2002) compare the generated text with reference translations, based on the co-occurrence of n-grams in both texts. In spite of being easily and widely used, the above method is incapable of capturing syntactic and semantic similarity (Stent et al., 2005). (2) embedding-based metrics: Word embeddings are vector representations of words that capture their semantic properties, such that words with similar meanings have similar embeddings. A bunch of work leverages word embeddings to evaluate the semantic similarity between two pieces of text. BERTScore (Zhang et al., 2019) use contextualized word embeddings from transformer models like BERT (Devlin et al., 2018), BLEURT (Sellam et al., 2020) utilize supervised training data to enhance the performance. MoverScore (Zhao et al., 2019) combine contextualized word embeddings with Earth Mover's Distance (Rubner et al., 2000). (3) LLM-based metrics: Amidst the flourishing advancement of LLM which embodies a wealth of information derived from extensive training data, using LLM as an evaluator has experienced notable progress. GPTScore (Fu et al., 2023) utilize conditional probability to assign the text a score representing its quality. Wang et al. (2023a) explore the potential of utilizing ChatGPT as an NLG evaluator by prompting it to score a text directly. Wang et al. (2023c) curate a reliable dataset containing pairwise comparison and evaluation explanation which can be used to train a foundation model making it a better evaluator. Bai et al. (2023) propose decentralized evaluation to provide fairer evaluation results. G-EVAL (Liu et al., 2023b) propose probability-weighted techniques to calibrate the score given by a single LLM.</p>
<p>Communicative Agents Most recently, significant attention has been dedicated to the development of communicative agents. These agents, often acted by LLMs like ChatGPT or GPT-4, are designed to interact and communicate effectively with other agents or human users using natural language. The primary goal is to facilitate more productive and efficient interaction and collaboration as different agents can autonomously communicate and negotiate to tackle a more complex task collectively. Several studies have explored various aspects of communicative agents. Li et al. (2023a) propose a cooperative agent framework dubbed as role-playing enabling agents to autonomously cooperate to solve complex tasks. Park et al. (2023) create a sandbox environment consisting of 25 individual virtual entities endowed with a character description and memory system. Every intelligent agent is capable of autonomously interacting with other agents and the environment simulating reliable human behavior. Qian et al. (2023) establish a chat-based software development framework that can complete a software design and produce executable software at a reduced cost compared to recruiting human programmers. Liu et al. (2023a) utilize a sandbox environment to curate reliable datasets in better alignment with human preference and train a socially-aligned LLM. Liang et al. (2023) and Du et al. (2023) also make use of the multi-agent debate framework in other scenarios such as translation and arithmetic problems resulting in better results. Wang et al. (2023d) propose an alternative method called self-collaboration to enable the communication of agents by utilizing a single LLM prompted by multi-persona descriptions. Mandi et al. (2023) propose a novel framework</p>
<p>Table 5: An illustrative example from ChatEval presenting a discussion process involving participants Alice ( ), Bob ( ) and Carol ( ). They initiate a group talk with the purpose of determining the superior response from two assistants. Due to spatial constraints within the layout, we use ... to omit some redundant text.</p>
<p>Both Assistant 1 and Assistant 2 provided comprehensive and detailed answers. They both suggested similar strategies for dealing with stress, such as exercise, relaxation techniques, getting enough sleep, eating a healthy diet, connecting with others, time management, and seeking professional help. However, Assistant 2 went a step further by mentioning mindfulness social support, and relaxation techniques like progressive muscle relaxation, aromatherapy, and massage. Therefore, I would lean towards Assistant 2 for providing a slightly more detailed response.</p>
<p>I agree with the Alice's assessments. However, I would like to point out that Assistant 1's answer was more concise and to the point that ... . On the other hand, Assistant 2's answer was more detailed and ... . In your opinion, does the additional detail provided by Assistant 2 make its response more helpful, or could the conciseness of Assistant 1's response be seen as more beneficial?</p>
<p>In terms of consistency with the question, both responses are equally consistent as they both provided effective ways to deal with stress. Assistant 1's response was indeed more concise, while Assistant 2 provided more detailed techniques. The effectiveness of these responses could depend on the individual's preference for brevity or detail. Therefore, I believe both responses are equally valuable.</p>
<p>I appreciate the points made by both of you. While Assistant 1's response was indeed more concise, I believe that the additional detail provided by Assistant 2 could be more beneficial for someone looking for a variety of strategies to manage stress. The inclusion of mindfulness practices and ... . Therefore, I still lean towards Assistant 2's response as being slightly more helpful.</p>
<p>However, I would like to emphasize that the effectiveness of a response can also depend on the context and the individual's needs. For someone who is overwhelmed and looking for quick, straightforward advice, the conciseness of Assistant 1's response might be more beneficial. On the other hand, for someone who is looking for a variety of strategies to try, the detailed response of Assistant 2 might be more helpful. Therefore, I believe both responses have their merits and it's difficult to definitively say one is better than the other. What do you think about this perspective?</p>
<p>I agree with the Bob's perspective that the effectiveness of a response can depend on the individual's needs and context. Therefore, I believe both responses are equally valuable and it's hard to definitively say one is better than the other.</p>
<p>After discussing with my co-workers, I believe both Assistants performed equally well in providing comprehensive and accurate responses.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Performance with Increased Different Roles on FairEval Dataset. We adopt one-by-one communication strategy and 2 discussion turns. Additional role descriptions are shown in Appendix A. (b) Performance with Increased Discussion Turns on FairEval Dataset. We adopt one-by-one communication strategy and 2 different roles.
designed for the collaboration of multiple robots, utilizing multiple LLMs to enhance coordination and strategic planning among the robots. Concurrent with our work, Li et al. (2023b) propose Peer Rank and Discussion (PRD) which is similar to our approach. However, they probe the different dimensions of evaluation by using different models as agents and did not explore alternative communication strategies.</p>
<h1>6 CONCLUSION</h1>
<p>In this paper, we present evidence that ChatEval contributes to improving the evaluation performance concerning text quality, aligning more closely with human preferences. We emphasize the necessity of the diverse role specification and propose distinct communication strategies as integral components within ChatEval. Our qualitative analysis of the discussion process conveys insightful intuitions about how a text is evaluated by ChatEval and substantiates our approach's ability to support comprehensive evaluations akin to human judgment, thereby demonstrating the reliability and efficacy of our framework.</p>
<h2>REFERENCES</h2>
<p>Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint arXiv:2306.04181, 2023.</p>
<p>Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 65-72, 2005.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chris Callison-Burch. Fast, cheap, and creative: Evaluating translation quality using amazon's mechanical turk. In Proceedings of the 2009 conference on empirical methods in natural language processing, pp. 286-295, 2009.</p>
<p>Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey. CoRR, abs/2006.14799, 2020. URL https://arxiv.org/abs/2006.14799.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554, 2023.</p>
<p>Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anushree Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tür. Topical-chat: Towards knowledgegrounded open-domain conversations. 2019.</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. The perils of using mechanical turk to evaluate open-ended text generation. arXiv preprint arXiv:2109.06835, 2021.</p>
<p>Grzegorz Kondrak. N-gram similarity and distance. In International symposium on string processing and information retrieval, pp. 115-126. Springer, 2005.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023a.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023b.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023a.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.</p>
<p>Andrea I Luppi, Pedro AM Mediano, Fernando E Rosas, Negin Holland, Tim D Fryer, John T O'Brien, James B Rowe, David K Menon, Daniel Bor, and Emmanuel A Stamatakis. A synergistic core for human brain evolution and cognition. Nature Neuroscience, 25(6):771-782, 2022.</p>
<p>Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.</p>
<p>Mary L McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282, 2012.
Shikib Mehri and Maxine Eskenazi. Usr: An unsupervised and reference free evaluation metric for dialog generation. arXiv preprint arXiv:2005.00456, 2020.</p>
<p>Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. Why we need new evaluation metrics for NLG. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2241-2252, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1238. URL https:// aclanthology.org/D17-1238.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.</p>
<p>Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover's distance as a metric for image retrieval. International journal of computer vision, 40:99-121, 2000.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696, 2020.</p>
<p>Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. Are large language models good evaluators for abstractive summarization? arXiv preprint arXiv:2305.13091, 2023.</p>
<p>Amanda Stent, Matthew Marge, and Mohit Singhai. Evaluating evaluation methods for generation in the presence of variation. In International conference on intelligent text processing and computational linguistics, pp. 341-351. Springer, 2005.</p>
<p>Chris Van Der Lee, Albert Gatt, Emiel Van Miltenburg, Sander Wubben, and Emiel Krahmer. Best practices for the human evaluation of automatically generated text. In Proceedings of the 12th International Conference on Natural Language Generation, pp. 355-368, 2019.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300, 2023d.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W Malone. Evidence for a collective intelligence factor in the performance of human groups. science, 330(6004):686-688, 2010.</p>
<p>Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are diverse role-players for summarization evaluation. arXiv preprint arXiv:2303.15078, 2023.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622, 2019.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. Towards a unified multi-dimensional evaluator for text generation. arXiv preprint arXiv:2210.07197, 2022.</p>
<h1>A PROMPT TEMPLATE AND DIVERSE ROLE PROMPT</h1>
<p>The overall prompt template is shown in Table 6, we draw inspiration from Wu et al. (2023) and design several different role descriptions as follows.</p>
<p>General Public You are now General Public, one of the referees in this task. You are interested in the story and looking for updates on the investigation. Please think critically by yourself and note that it's your responsibility to choose one of which is the better first.</p>
<p>Critic You are now Critic, one of the referees in this task. You will check fluent writing, clear sentences, and good wording in summary writing. Your job is to question others judgment to make sure their judgment is well-considered and offer an alternative solution if two responses are at the same level.</p>
<p>News Author You are News Author, one of the referees in this task. You will focus on the consistency with the original article. Please help other people to determine which response is the better one.</p>
<p>Psychologist You are Psychologist, one of the referees in this task. You will study human behavior and mental processes in order to understand and explain human behavior. Please help other people to determine which response is the better one.</p>
<p>Scientist You are Scientist, one of the referees in this task. You are a professional engaged in systematic study who possesses a strong background in the scientific method, critical thinking, and problem-solving abilities. Please help other people to determine which response is the better one.</p>
<h2>B FORMAL DEPICTION OF DIFFERENT COMMUNICATION STRATEGY</h2>
<div class="codehilite"><pre><span></span><code><span class="k">[Question]</span>
<span class="na">{source_text}</span>
<span class="k">[The Start of Assistant 1&#39;s Answer]</span>
<span class="na">{compared_text_one}</span>
<span class="k">[The End of Assistant 1&#39;s Answer]</span>
<span class="k">[The Start of Assistant 2&#39;s Answer]</span>
<span class="na">{compared_text_two}</span>
<span class="k">[The End of Assistant 2&#39;s Answer]</span>
<span class="k">[System]</span>
<span class="na">We would like to request your feedback on the performance of two AI assistants in response</span>
<span class="na">to the user question displayed above.</span>
<span class="na">Please consider the helpfulness, relevance, accuracy, and level of detail of their responses.</span>
<span class="na">Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates</span>
<span class="na">better overall performance.</span>
<span class="na">There are a few other referees assigned the same task, it&#39;s your responsibility to discuss with</span>
<span class="na">them and think critically before you make your final judgment.</span>
<span class="na">Here is your discussion history</span><span class="o">:</span>
<span class="na">{chat_history}</span>
<span class="na">{role_description}</span>
<span class="na">Now it&#39;s your time to talk, please make your talk short and clear, {agent_name} !</span>
</code></pre></div>

<p>Table 6: The prompt template for FairEval Dataset. We replace the colored slot with real text before querying the LLMs. Note that we use the same template when conducting single-agent-based experiments and ignore the chat history and role description slot.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">One-by-One</span>
<span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">discuss</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">group</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">debate</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">D_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">    </span><span class="nt">of</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="nt">either</span><span class="w"> </span><span class="nt">majority</span><span class="w"> </span><span class="nt">vote</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">average</span><span class="w"> </span><span class="nt">score</span><span class="o">)</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">)</span>
<span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">Final</span><span class="w"> </span><span class="nt">results</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">text</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="nt">utilize</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="nt">responses</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">n</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">m</span><span class="o">&gt;</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">H_</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="o">+</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">                </span><span class="o">//</span><span class="w"> </span><span class="nt">concatenate</span><span class="w"> </span><span class="nt">current</span><span class="w"> </span><span class="nt">response</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">later</span><span class="w"> </span><span class="nt">agents</span><span class="err">&#39;</span><span class="w"> </span><span class="nt">chat</span>
<span class="w">                </span><span class="nt">history</span>
<span class="w">            </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="nt">end</span>
<span class="nt">11</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="nt">12</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">return</span><span class="w"> </span>\<span class="ss">(</span><span class="nv">A</span><span class="w"> </span><span class="nv">N</span><span class="w"> </span><span class="nv">S</span>\<span class="ss">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Simultaneous-Talk</span>
<span class="nt">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">number</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">discuss</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">group</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">debate</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">D_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">D_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">        </span><span class="nt">of</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">agent</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="nt">either</span><span class="w"> </span><span class="nt">majority</span><span class="w"> </span><span class="nt">vote</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">average</span><span class="w"> </span><span class="nt">score</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">B</span><span class="w"> </span><span class="nt">U</span><span class="w"> </span><span class="nt">F</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">output</span><span class="o">:</span><span class="w"> </span><span class="nt">Final</span><span class="w"> </span><span class="nt">results</span><span class="w"> </span><span class="nt">for</span><span class="w"> </span><span class="nt">text</span><span class="w"> </span><span class="nt">evaluation</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">utilize</span><span class="w"> </span><span class="nt">agents</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">generate</span><span class="w"> </span><span class="nt">responses</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="w"> </span><span class="nt">u</span><span class="w"> </span><span class="nt">f</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">b</span><span class="w"> </span><span class="nt">u</span><span class="w"> </span><span class="nt">f</span><span class="o">+</span><span class="nt">h_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">add</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">responses</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">current</span><span class="w"> </span><span class="nt">turn</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">buffer</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">H_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">buf</span><span class="o">;</span>
<span class="w">            </span><span class="o">//</span><span class="w"> </span><span class="nt">add</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">all</span><span class="w"> </span><span class="nt">agents</span><span class="err">&#39;</span><span class="w"> </span><span class="nt">chat</span><span class="w"> </span><span class="nt">history</span>
<span class="w">        </span><span class="nt">end</span>
<span class="w">    </span><span class="nt">end</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">E</span><span class="w"> </span><span class="nt">X</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="cp">[</span><span class="nx">H_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="o">\</span><span class="nx">cdots</span><span class="p">,</span><span class="w"> </span><span class="nx">H_</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="o">\</span><span class="nx">right</span><span class="cp">]</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">N</span><span class="w"> </span><span class="nt">S</span><span class="err">\</span><span class="o">)</span>
<span class="nt">Algorithm</span><span class="w"> </span><span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="nt">Simultaneous-Talk-with-Summarizer</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\),</span><span class="w"> </span><span class="n">discuss</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="o">\(</span><span class="n">T</span><span class="o">\),</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">group</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">debate</span><span class="w"> </span><span class="n">agents</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">[</span><span class="n">D_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">D_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\),</span><span class="w"> </span><span class="n">chat</span><span class="w"> </span><span class="n">history</span>
<span class="w">        </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="o">\(\</span><span class="n">left</span><span class="o">[</span><span class="n">H_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\),</span><span class="w"> </span><span class="n">answer_extracter</span><span class="w"> </span><span class="o">(</span><span class="n">either</span><span class="w"> </span><span class="n">majority</span><span class="w"> </span><span class="n">vote</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">average</span><span class="w"> </span><span class="n">score</span><span class="o">)</span>
<span class="w">        </span><span class="o">\(</span><span class="n">E</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">T</span><span class="o">\),</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">\(</span><span class="n">B</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">F</span><span class="o">\),</span><span class="w"> </span><span class="n">summarizer</span><span class="w"> </span><span class="o">\(</span><span class="n">S</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">M</span><span class="o">\)</span>
<span class="w">    </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Final</span><span class="w"> </span><span class="n">results</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">text</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="o">\)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">t</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">T</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">n</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="o">\(</span><span class="n">h_</span><span class="o">{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">D_</span><span class="o">{</span><span class="n">n</span><span class="o">}\</span><span class="n">left</span><span class="o">(</span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}\</span><span class="n">right</span><span class="o">)</span><span class="w"> </span><span class="o">;\)</span>
<span class="w">            </span><span class="c1">// utilize agents to generate responses</span>
<span class="w">            </span><span class="n">buf</span><span class="w"> </span><span class="o">\(\</span><span class="n">leftarrow</span><span class="o">\)</span><span class="w"> </span><span class="n">buf</span><span class="w"> </span><span class="o">\(+</span><span class="n">h_</span><span class="o">{</span><span class="n">n</span><span class="o">}\);</span>
<span class="w">            </span><span class="c1">// add the responses in current turn to the buffer</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="o">\(</span><span class="n">n</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="o">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">\(</span><span class="n">N</span><span class="o">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">            </span><span class="o">\(</span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">n</span><span class="o">}+</span><span class="n">S</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">M</span><span class="o">(</span><span class="n">B</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">F</span><span class="o">)</span><span class="w"> </span><span class="o">;\)</span>
<span class="w">            </span><span class="c1">// add the summarized buffer to all agents&#39; chat history</span>
<span class="w">        </span><span class="n">end</span>
<span class="w">    </span><span class="n">end</span>
<span class="w">    </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="n">T</span><span class="o">\</span><span class="n">left</span><span class="o">(\</span><span class="n">left</span><span class="o">[</span><span class="n">H_</span><span class="o">{</span><span class="mi">1</span><span class="o">},</span><span class="w"> </span><span class="o">\</span><span class="n">cdots</span><span class="o">,</span><span class="w"> </span><span class="n">H_</span><span class="o">{</span><span class="n">N</span><span class="o">}\</span><span class="n">right</span><span class="o">]\</span><span class="n">right</span><span class="o">)\);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\(</span><span class="n">A</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">S</span><span class="o">\);</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We use the same prompt template as our multi-agent debate settings in single-agent baseline except that we ignore some slot.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
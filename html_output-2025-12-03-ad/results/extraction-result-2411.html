<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2411 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2411</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2411</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-268856609</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01663v6.pdf" target="_blank">CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2411.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2411.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CMAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collaborative Multi-Agent Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that coordinates multiple language-agent roles (User, Assistant, Checker) in an actor-critic, role-based pipeline with short-term and long-term memory, using feedback-driven parameter updates to improve small LLMs' task performance and adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CMAT (Collaborative Multi-Agent Tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CMAT is a multi-agent framework that organizes agents into three explicit roles — User (task/provider), Assistant (actor that generates actions/responses), and Checker (critic that evaluates actions and supplies feedback). It integrates supervised fine-tuning (LoRA, P-Tuning), actor-critic style policy updates, Chain-of-Thought (CoT) and ReAct reasoning patterns, short-term and long-term memory stores, and a Checker-in-the-loop mechanism to produce immediate corrective feedback and enable reflection-driven long-term updates to agent parameters. The system is intended to enable collaborative learning and real-time adaptation among language agents to improve decision-making, controllability, and efficiency of smaller models in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>3 (User, Assistant, Checker) — extensible to multiple assistants/checkers in principle</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>User: assigns tasks and provides input/context; Assistant: actor that produces intermediate reasoning (CoT) and final actions (e.g., natural-language commands or SQL), maintains short-term (M_S) and long-term (M_L) memory, performs self-reflection to produce s_t and applies gradient updates; Checker: critic that evaluates Assistant actions, returns feedback f_t which is converted into rewards r_t and value estimates for critic updates, and verifies correctness before environment execution.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Task reception/assignment (User), reasoning and action generation (Assistant; idea generation & implementation planning), execution (Assistant-generated commands executed in environment after verification), evaluation/verification (Checker), iterative refinement and long-term learning (self-reflection and memory updates). Applied across evaluation tasks including code correction, OS tasks, database (SQL) operations, WebShop e-commerce tasks, knowledge graph construction/use, web tasks (M2W), and embodied/text reasoning (ALFWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Centralized, role-based sequential pipeline with actor-critic dynamics: User -> Assistant (actor) -> Checker (critic) in the loop. Coordination is implemented as an explicit sequence of interaction steps with parameter updates for the Assistant guided by Checker feedback (θ_actor updated using feedback-derived reward signals and critic value estimates). Memory and reflection produce additional guidance for long-term updates. Overall coordination is centralized around role assignments and the Checker-in-the-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Natural-language messages and token streams (Chain-of-Thought reasoning tokens and final action strings such as SQL), plus internal structured signals: feedback f_t, reward r_t, critic value V_θcritic(s), and memory objects (short-term M_S and long-term M_L). Messages are exchanged as text/action outputs and evaluative feedback; the system uses these textual actions together with numeric feedback signals for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Checker supplies feedback f_t which is converted to a reward r_t and used in actor-critic style updates: Assistant updates θ_actor with gradient of log π_θactor(a_t|s_t) scaled by δ_t (where δ_t = r_t + γ V_θcritic(s_{t+1}) − V_θcritic(s_t)), and Checker updates its value function θ_critic via temporal-difference style updates. Additionally, Assistant performs self-reflection producing s_t used to update long-term memory and to compute auxiliary gradients (G(s_t)). The Checker-in-the-loop provides immediate corrective signals before environment execution.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Immediate / step-wise: agents communicate at each time step (after each Assistant action the Checker evaluates and returns feedback; short-term memory is updated every step; long-term memory is updated when self-reflection produces significant s_t).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General language-agent research tasks used for evaluation: code correction, operating system (OS) interactions, database (SQL) tasks, WebShop e-commerce tasks, knowledge graph tasks, M2W web tasks, ALFWorld embodied/text reasoning — i.e., general multi-domain agent benchmarks rather than a single scientific domain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports per-task numerical scores in tables (AgentBench style metrics and code-correction metrics). Representative reported numbers (as printed in the paper's tables): Table II per-task scores for TinyAgent variants (e.g., TinyAgent-7B listed as '23.1 41.3 28.0 8.0 58.7 12.0' across the table's task columns) and TinyAgent-1.8B as '17.7 28.3 48.0 6.0 32.7 11.0'. Table I (code correction) shows larger BLEU/ROUGE for the small fine-tuned model entries (e.g., the top small-model BLEU-4 value shown was 43.38 for the listed small model in that table). Exact per-column mappings are provided in the paper's tables; the paper emphasizes that TinyAgent models with CMAT match or approach GPT-3.5 performance on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against base open-source models (Qwen-1.8B, CodeLlama-7B, Llama-2-7B, etc.) and API models (GPT-3.5, GPT-4). Results show TinyAgent models (fine-tuned and used with CMAT) outperform their base models and in some tasks approach or match GPT-3.5 and compete with GPT-4 on particular benchmarks; TinyAgent-7B is reported to surpass CodeLlama-7B on the DB task and TinyAgent-1.8B shows strong cross-task performance vs CodeLlama series.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Immediate corrective feedback and structured role division improve decision-making, controllability, and sample efficiency for small models; enables small-parameter TinyAgent models to match or approach performance of larger API models (e.g., parity with GPT-3.5 on some tasks). Benefits highlighted include better task correctness (via Checker verification), improved adaptation through reflection and long-term memory, and robustness to poor prompts when using structured agent interactions. Quantitatively, the paper reports improved per-task scores for TinyAgent variants vs their base models in Tables I–V and notes CMAT amplifies performance of small models to approach GPT-3.5/GPT-4 in specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Reported limitations include task limit/time constraints (TLE) causing incomplete tasks; invalid actions or format errors in agent outputs; sensitivity to low-quality prompts (low-quality prompts can degrade performance); some models repeating errors without reflection (necessitating the reflection mechanism); dataset imbalance and limited sample sizes requiring augmentation/distillation; and potential overhead of real-time feedback loops (not quantified).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Table V ablation compares full configuration (composite) vs 'agent-only' and 'general-only' instructions for TinyAgent-7B — composite (full) yields best performance across tasks; agent-only and general-only reduce performance (general-only notably fails in KG and ALF). The paper also shows, via Fig. 3, that the reflection mechanism materially improves task success (TinyAgent-7B with reflection fixed earlier failures while Llama-2-7B still erred after reflection; without reflection TinyAgent repeated mistakes). Quantitative ablation numbers are presented in Table V (e.g., tinyagent-7b: '27.3 43.0 38.0 10.0 61.8 14.0'; agent only: '20.1 39.3 25.0 2.0 55.7 7.0'; general only: '9.7 5.4 0.0 0.0 26.6 5.0' across the table's task columns).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>The paper recommends a combined configuration: three-role setup (User/Assistant/Checker) with Checker-in-the-loop, CoT followed by action generation (CoT -> action), ReAct interleaving where appropriate, actor-critic style parameter updates driven by Checker feedback, and dual-memory management (short-term M_S for trajectory context and long-term M_L for self-reflective insights). It also advises combining agent-specific and general instructions (both are necessary per ablation) and using reflection/self-generated summaries s_t to drive long-term updates. These components together are presented as the optimal configuration in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2411.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2411.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TinyAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TinyAgent (fine-tuned small-language-agent models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A series of small LLMs (e.g., TinyAgent-1.8B, TinyAgent-7B) fine-tuned on a curated dataset and trained/operated inside the CMAT framework as Assistants; they use LoRA/P-Tuning, supervised fine-tuning and feedback-driven actor-critic style updates, and maintain short- and long-term memories plus self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TinyAgent (as deployed within CMAT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>TinyAgent denotes the family of compact LLMs fine-tuned and deployed as Assistant agents inside CMAT. They generate CoT reasoning and final actions (e.g., SQL or code corrections), maintain per-step short-term memory and aggregated long-term self-reflective memory, accept Checker feedback for policy updates, and are trained using parameter-efficient tuning (LoRA, P-Tuning) alongside supervised fine-tuning and feedback-driven updates.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (one or more Assistant instances used within CMAT; typical experiments use a single Assistant per User-Checker triad)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>As Assistant agents: reasoning and action generation (CoT + action), maintaining short-term and long-term memories, performing self-reflection to produce s_t, updating parameters via feedback-driven gradients, and executing domain-specific tasks (SQL generation, code correction, web actions).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Idea/reasoning (via CoT), implementation planning and action generation (e.g., SQL, code fixes), execution (after Checker validation), evaluation via Checker feedback, and long-term learning (via reflection and memory updates).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Operates as the Actor within CMAT's centralized actor-critic pipeline; coordination is mediated through the User (task assignment) and a Checker that provides feedback; TinyAgent receives immediate feedback each action step and updates parameters accordingly.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Textual/natural-language outputs (reasoning tokens and final actions such as SQL or code) and internal numeric feedback signals (f_t / r_t) from Checker; updates to short-term and long-term memory structures represented as internal state.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Receives Checker feedback f_t which is used to compute reward r_t and TD-error δ_t; performs gradient updates to θ_actor with log-probability gradients scaled by δ_t, and also updates based on reflection-derived gradients G(s_t).</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Step-wise: Assistant produces action each time step, receives immediate Checker feedback, and updates short-term memory every step; long-term memory updated on reflection events.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same multi-domain agent benchmarks as CMAT: code correction, DB (SQL), OS, WebShop, KG building/QA, M2W web tasks, ALFWorld reasoning/embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TinyAgent variants reported in the paper's tables (Table II shows TinyAgent-1.8B and TinyAgent-7B per-task scores; Table I shows code-correction BLEU/ROUGE for evaluated models). Paper states TinyAgent-7B achieves competitive scores approaching GPT-3.5 on several tasks and outperforms its base model (CodeLlama-7B) on certain tasks such as DB.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared with base open-source models (Qwen-1.8B, CodeLlama-7B, Llama-2-7B) and API models (GPT-3.5, GPT-4); TinyAgent fine-tuned models outperform their unfine-tuned base models and in CMAT approach the performance of GPT-3.5 / compete with GPT-4 on particular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>When deployed as Assistants inside CMAT, TinyAgent models benefit from immediate Checker feedback and memory/reflection loops resulting in improved task success, correction of repeated errors, and ability to match larger models on some benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>TinyAgent still subject to environment/interaction failure modes noted in paper: TLE (task limit exceeded), invalid actions/format outputs for some tasks, and susceptibility to low-quality prompts absent proper agent orchestration; without reflection they can repeat mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Included in paper (Table V) showing TinyAgent-7B performance drops when agent instructions are used alone or with only general instructions; composite instruction + CMAT yields best performance. Fig. 3 demonstrates the practical benefit of reflection for TinyAgent vs a baseline smaller model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Best performance achieved when TinyAgent is used within CMAT: CoT → action generation, Checker-in-the-loop, actor-critic feedback updates, short-term and long-term memory (reflection), and combined agent-specific + general instructions as shown by ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>Camel: Communicative agents for "mind" exploration of large scale language model society <em>(Rating: 2)</em></li>
                <li>Pangu-agent: A fine-tunable generalist agent with structured reasoning <em>(Rating: 2)</em></li>
                <li>Agenttuning: Enabling generalized agent abilities for llms <em>(Rating: 2)</em></li>
                <li>Multi-agent collaboration: Harnessing the power of intelligent llm agents <em>(Rating: 2)</em></li>
                <li>Emergent cooperation and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2411",
    "paper_id": "paper-268856609",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "CMAT",
            "name_full": "Collaborative Multi-Agent Tuning",
            "brief_description": "A framework that coordinates multiple language-agent roles (User, Assistant, Checker) in an actor-critic, role-based pipeline with short-term and long-term memory, using feedback-driven parameter updates to improve small LLMs' task performance and adaptability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CMAT (Collaborative Multi-Agent Tuning)",
            "system_description": "CMAT is a multi-agent framework that organizes agents into three explicit roles — User (task/provider), Assistant (actor that generates actions/responses), and Checker (critic that evaluates actions and supplies feedback). It integrates supervised fine-tuning (LoRA, P-Tuning), actor-critic style policy updates, Chain-of-Thought (CoT) and ReAct reasoning patterns, short-term and long-term memory stores, and a Checker-in-the-loop mechanism to produce immediate corrective feedback and enable reflection-driven long-term updates to agent parameters. The system is intended to enable collaborative learning and real-time adaptation among language agents to improve decision-making, controllability, and efficiency of smaller models in complex tasks.",
            "number_of_agents": "3 (User, Assistant, Checker) — extensible to multiple assistants/checkers in principle",
            "agent_specializations": "User: assigns tasks and provides input/context; Assistant: actor that produces intermediate reasoning (CoT) and final actions (e.g., natural-language commands or SQL), maintains short-term (M_S) and long-term (M_L) memory, performs self-reflection to produce s_t and applies gradient updates; Checker: critic that evaluates Assistant actions, returns feedback f_t which is converted into rewards r_t and value estimates for critic updates, and verifies correctness before environment execution.",
            "research_phases_covered": "Task reception/assignment (User), reasoning and action generation (Assistant; idea generation & implementation planning), execution (Assistant-generated commands executed in environment after verification), evaluation/verification (Checker), iterative refinement and long-term learning (self-reflection and memory updates). Applied across evaluation tasks including code correction, OS tasks, database (SQL) operations, WebShop e-commerce tasks, knowledge graph construction/use, web tasks (M2W), and embodied/text reasoning (ALFWorld).",
            "coordination_mechanism": "Centralized, role-based sequential pipeline with actor-critic dynamics: User -&gt; Assistant (actor) -&gt; Checker (critic) in the loop. Coordination is implemented as an explicit sequence of interaction steps with parameter updates for the Assistant guided by Checker feedback (θ_actor updated using feedback-derived reward signals and critic value estimates). Memory and reflection produce additional guidance for long-term updates. Overall coordination is centralized around role assignments and the Checker-in-the-loop.",
            "communication_protocol": "Natural-language messages and token streams (Chain-of-Thought reasoning tokens and final action strings such as SQL), plus internal structured signals: feedback f_t, reward r_t, critic value V_θcritic(s), and memory objects (short-term M_S and long-term M_L). Messages are exchanged as text/action outputs and evaluative feedback; the system uses these textual actions together with numeric feedback signals for optimization.",
            "feedback_mechanism": "Checker supplies feedback f_t which is converted to a reward r_t and used in actor-critic style updates: Assistant updates θ_actor with gradient of log π_θactor(a_t|s_t) scaled by δ_t (where δ_t = r_t + γ V_θcritic(s_{t+1}) − V_θcritic(s_t)), and Checker updates its value function θ_critic via temporal-difference style updates. Additionally, Assistant performs self-reflection producing s_t used to update long-term memory and to compute auxiliary gradients (G(s_t)). The Checker-in-the-loop provides immediate corrective signals before environment execution.",
            "communication_frequency": "Immediate / step-wise: agents communicate at each time step (after each Assistant action the Checker evaluates and returns feedback; short-term memory is updated every step; long-term memory is updated when self-reflection produces significant s_t).",
            "task_domain": "General language-agent research tasks used for evaluation: code correction, operating system (OS) interactions, database (SQL) tasks, WebShop e-commerce tasks, knowledge graph tasks, M2W web tasks, ALFWorld embodied/text reasoning — i.e., general multi-domain agent benchmarks rather than a single scientific domain.",
            "performance_metrics": "Paper reports per-task numerical scores in tables (AgentBench style metrics and code-correction metrics). Representative reported numbers (as printed in the paper's tables): Table II per-task scores for TinyAgent variants (e.g., TinyAgent-7B listed as '23.1 41.3 28.0 8.0 58.7 12.0' across the table's task columns) and TinyAgent-1.8B as '17.7 28.3 48.0 6.0 32.7 11.0'. Table I (code correction) shows larger BLEU/ROUGE for the small fine-tuned model entries (e.g., the top small-model BLEU-4 value shown was 43.38 for the listed small model in that table). Exact per-column mappings are provided in the paper's tables; the paper emphasizes that TinyAgent models with CMAT match or approach GPT-3.5 performance on several tasks.",
            "baseline_comparison": "Compared against base open-source models (Qwen-1.8B, CodeLlama-7B, Llama-2-7B, etc.) and API models (GPT-3.5, GPT-4). Results show TinyAgent models (fine-tuned and used with CMAT) outperform their base models and in some tasks approach or match GPT-3.5 and compete with GPT-4 on particular benchmarks; TinyAgent-7B is reported to surpass CodeLlama-7B on the DB task and TinyAgent-1.8B shows strong cross-task performance vs CodeLlama series.",
            "coordination_benefits": "Immediate corrective feedback and structured role division improve decision-making, controllability, and sample efficiency for small models; enables small-parameter TinyAgent models to match or approach performance of larger API models (e.g., parity with GPT-3.5 on some tasks). Benefits highlighted include better task correctness (via Checker verification), improved adaptation through reflection and long-term memory, and robustness to poor prompts when using structured agent interactions. Quantitatively, the paper reports improved per-task scores for TinyAgent variants vs their base models in Tables I–V and notes CMAT amplifies performance of small models to approach GPT-3.5/GPT-4 in specific tasks.",
            "coordination_challenges": "Reported limitations include task limit/time constraints (TLE) causing incomplete tasks; invalid actions or format errors in agent outputs; sensitivity to low-quality prompts (low-quality prompts can degrade performance); some models repeating errors without reflection (necessitating the reflection mechanism); dataset imbalance and limited sample sizes requiring augmentation/distillation; and potential overhead of real-time feedback loops (not quantified).",
            "ablation_studies": "Table V ablation compares full configuration (composite) vs 'agent-only' and 'general-only' instructions for TinyAgent-7B — composite (full) yields best performance across tasks; agent-only and general-only reduce performance (general-only notably fails in KG and ALF). The paper also shows, via Fig. 3, that the reflection mechanism materially improves task success (TinyAgent-7B with reflection fixed earlier failures while Llama-2-7B still erred after reflection; without reflection TinyAgent repeated mistakes). Quantitative ablation numbers are presented in Table V (e.g., tinyagent-7b: '27.3 43.0 38.0 10.0 61.8 14.0'; agent only: '20.1 39.3 25.0 2.0 55.7 7.0'; general only: '9.7 5.4 0.0 0.0 26.6 5.0' across the table's task columns).",
            "optimal_configurations": "The paper recommends a combined configuration: three-role setup (User/Assistant/Checker) with Checker-in-the-loop, CoT followed by action generation (CoT -&gt; action), ReAct interleaving where appropriate, actor-critic style parameter updates driven by Checker feedback, and dual-memory management (short-term M_S for trajectory context and long-term M_L for self-reflective insights). It also advises combining agent-specific and general instructions (both are necessary per ablation) and using reflection/self-generated summaries s_t to drive long-term updates. These components together are presented as the optimal configuration in the paper.",
            "uuid": "e2411.0",
            "source_info": {
                "paper_title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TinyAgent",
            "name_full": "TinyAgent (fine-tuned small-language-agent models)",
            "brief_description": "A series of small LLMs (e.g., TinyAgent-1.8B, TinyAgent-7B) fine-tuned on a curated dataset and trained/operated inside the CMAT framework as Assistants; they use LoRA/P-Tuning, supervised fine-tuning and feedback-driven actor-critic style updates, and maintain short- and long-term memories plus self-reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TinyAgent (as deployed within CMAT)",
            "system_description": "TinyAgent denotes the family of compact LLMs fine-tuned and deployed as Assistant agents inside CMAT. They generate CoT reasoning and final actions (e.g., SQL or code corrections), maintain per-step short-term memory and aggregated long-term self-reflective memory, accept Checker feedback for policy updates, and are trained using parameter-efficient tuning (LoRA, P-Tuning) alongside supervised fine-tuning and feedback-driven updates.",
            "number_of_agents": "variable (one or more Assistant instances used within CMAT; typical experiments use a single Assistant per User-Checker triad)",
            "agent_specializations": "As Assistant agents: reasoning and action generation (CoT + action), maintaining short-term and long-term memories, performing self-reflection to produce s_t, updating parameters via feedback-driven gradients, and executing domain-specific tasks (SQL generation, code correction, web actions).",
            "research_phases_covered": "Idea/reasoning (via CoT), implementation planning and action generation (e.g., SQL, code fixes), execution (after Checker validation), evaluation via Checker feedback, and long-term learning (via reflection and memory updates).",
            "coordination_mechanism": "Operates as the Actor within CMAT's centralized actor-critic pipeline; coordination is mediated through the User (task assignment) and a Checker that provides feedback; TinyAgent receives immediate feedback each action step and updates parameters accordingly.",
            "communication_protocol": "Textual/natural-language outputs (reasoning tokens and final actions such as SQL or code) and internal numeric feedback signals (f_t / r_t) from Checker; updates to short-term and long-term memory structures represented as internal state.",
            "feedback_mechanism": "Receives Checker feedback f_t which is used to compute reward r_t and TD-error δ_t; performs gradient updates to θ_actor with log-probability gradients scaled by δ_t, and also updates based on reflection-derived gradients G(s_t).",
            "communication_frequency": "Step-wise: Assistant produces action each time step, receives immediate Checker feedback, and updates short-term memory every step; long-term memory updated on reflection events.",
            "task_domain": "Same multi-domain agent benchmarks as CMAT: code correction, DB (SQL), OS, WebShop, KG building/QA, M2W web tasks, ALFWorld reasoning/embodied tasks.",
            "performance_metrics": "TinyAgent variants reported in the paper's tables (Table II shows TinyAgent-1.8B and TinyAgent-7B per-task scores; Table I shows code-correction BLEU/ROUGE for evaluated models). Paper states TinyAgent-7B achieves competitive scores approaching GPT-3.5 on several tasks and outperforms its base model (CodeLlama-7B) on certain tasks such as DB.",
            "baseline_comparison": "Compared with base open-source models (Qwen-1.8B, CodeLlama-7B, Llama-2-7B) and API models (GPT-3.5, GPT-4); TinyAgent fine-tuned models outperform their unfine-tuned base models and in CMAT approach the performance of GPT-3.5 / compete with GPT-4 on particular tasks.",
            "coordination_benefits": "When deployed as Assistants inside CMAT, TinyAgent models benefit from immediate Checker feedback and memory/reflection loops resulting in improved task success, correction of repeated errors, and ability to match larger models on some benchmarks.",
            "coordination_challenges": "TinyAgent still subject to environment/interaction failure modes noted in paper: TLE (task limit exceeded), invalid actions/format outputs for some tasks, and susceptibility to low-quality prompts absent proper agent orchestration; without reflection they can repeat mistakes.",
            "ablation_studies": "Included in paper (Table V) showing TinyAgent-7B performance drops when agent instructions are used alone or with only general instructions; composite instruction + CMAT yields best performance. Fig. 3 demonstrates the practical benefit of reflection for TinyAgent vs a baseline smaller model.",
            "optimal_configurations": "Best performance achieved when TinyAgent is used within CMAT: CoT → action generation, Checker-in-the-loop, actor-critic feedback updates, short-term and long-term memory (reflection), and combined agent-specific + general instructions as shown by ablation.",
            "uuid": "e2411.1",
            "source_info": {
                "paper_title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        },
        {
            "paper_title": "Camel: Communicative agents for \"mind\" exploration of large scale language model society",
            "rating": 2,
            "sanitized_title": "camel_communicative_agents_for_mind_exploration_of_large_scale_language_model_society"
        },
        {
            "paper_title": "Pangu-agent: A fine-tunable generalist agent with structured reasoning",
            "rating": 2,
            "sanitized_title": "panguagent_a_finetunable_generalist_agent_with_structured_reasoning"
        },
        {
            "paper_title": "Agenttuning: Enabling generalized agent abilities for llms",
            "rating": 2,
            "sanitized_title": "agenttuning_enabling_generalized_agent_abilities_for_llms"
        },
        {
            "paper_title": "Multi-agent collaboration: Harnessing the power of intelligent llm agents",
            "rating": 2,
            "sanitized_title": "multiagent_collaboration_harnessing_the_power_of_intelligent_llm_agents"
        },
        {
            "paper_title": "Emergent cooperation and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms",
            "rating": 1,
            "sanitized_title": "emergent_cooperation_and_strategy_adaptation_in_multiagent_systems_an_extended_coevolutionary_theory_with_llms"
        }
    ],
    "cost": 0.014001999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
15 Apr 2025</p>
<p>Xuechen Liang 
East China Jiaotong University
China</p>
<p>Equal contribution</p>
<p>Yangfan He 
University of Minnesota -Twin Cities
United States</p>
<p>Meiling Tao 
Guangdong University of Technology
China</p>
<p>Equal contribution</p>
<p>Yinghui Xia 
Autoagents.ai
China</p>
<p>Equal contribution</p>
<p>Jianhui Wang 
University of Electronic Science and Technology of China
China</p>
<p>Tianyu Shi tianyu.sh19@mail.mcgill.ca 
University of Toronto
Canada</p>
<p>Jun Wang wongjun@gmail.com 
East China Normal University
China</p>
<p>Jingsong Yang 
Autoagents.ai
China</p>
<p>CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
15 Apr 20259D7FB336E6EA310DA37F5B6916AB3414arXiv:2404.01663v6[cs.CL]Collaborative Multi-Agent FrameworkLanguage Model Fine-TuningReinforcement Learning from Human Feedback
Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset.We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback.This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory.In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors.Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.</p>
<p>I. INTRODUCTION</p>
<p>In the rapid development of the field of artificial intelligence, large language models (LLMs) such as BERT and GPT-4 [1] have become important cornerstones of natural language processing (NLP).These models utilize the Transformer architecture and effectively capture long-distance dependencies through multi-head self-attention mechanisms, demonstrating strong capabilities across various NLP tasks.With technological advancements, the performance and application scope of LLMs continue to expand, promising significant improvements in computational efficiency and functionality, including anticipated advanced features such as self-improvement, self-checking, and sparse expert models [2].</p>
<p>However, it is noteworthy that the success of these models largely depends on human input to guide the correct dialogue.Fig. 1.TinyAgent demonstrates outstanding performance, comparable to that of GPT-3.5.TinyAgent is a series of models fine-tuned based on Qwen [3] and Codellama [4].</p>
<p>This dependency requires users to provide relevant and precise prompts based on their intentions and the feedback from the chat agent, raising a critical question: Can we replace human intervention with autonomous communication agents capable of steering conversations towards task completion with minimal human supervision?</p>
<p>Our research is driven by the need to overcome the significant challenges faced by LLMs in real-world deployments, particularly the high computational resource requirements, data biases, and lack of robustness.These issues limit their applicability in resource-constrained environments and highlight the urgency of enhancing model efficiency and adaptability [5], [6].As demonstrated by Figure 1, we aim to address these limitations by optimizing models and training methods to enable smaller models to match the performance levels of larger models.</p>
<p>Additionally, recognizing the potential of MAS to improve processing efficiency and system adaptability through agent cooperation, we seek to develop a collaborative agent framework.This framework aims to facilitate effective cooperation among agents, thereby overcoming the performance gap and propelling further research and innovation in the field of LLMs [7], [8].</p>
<p>In our experiments, we evaluated the capabilities of large models with and without the use of prompts and observed that low-quality prompts can significantly degrade model performance.Consequently, we propose the Collaborative Multi-Agent Tuning (CMAT) framework.</p>
<p>The CMAT framework introduces a structured environment where individual agents, each with specialized roles and capabilities, work together to process information, make decisions, and solve complex tasks [9].By sharing insights and learning from interactions within this multi-agent ecosystem, the framework allows for a more scalable and flexible approach to training LLMs [10].This collaborative effort not only helps in bridging the gap in performance between smaller and larger models but also fosters a more resilient system capable of adapting to new challenges without extensive human intervention [11].Through CMAT, we aim to push the boundaries of what is possible with LLMs, making them more accessible and effective for a wider range of applications [12].</p>
<p>The main contributions of our work are as follows:</p>
<p>• We propose the CMAT framework which represents an innovative approach that allows for dynamic and real-time memory updates within multi-agent systems.• We design a novel role-playing mechanism for precise task allocation and enhanced agent communication, significantly boosting overall performance and cooperation.• We evaluated the fine-tuned TinyAgent models across multiple agent tasks, finding that in certain scenarios, their performance rivals that of advanced LLMs like GPT-4 and agentlm [13], demonstrating the potential efficiency and capabilities of compact models.</p>
<p>II. RELATED WORK</p>
<p>A. LLMs Applications in a Multi-Agent Framework</p>
<p>We explore the applications of LLMs within multi-agent systems, highlighting their role versatility as users, assistants, and checkers, and their capability to offer bespoke support and solutions across such environments [14], [15].LLMs showcase remarkable adaptability to tasks through methods like supervised fine-tuning and real-time feedback learning, notably in tasks that require a sophisticated understanding and execution related to operating systems or databases [16], [17].Furthermore, LLMs are adept at enhancing communication and collaboration among agents, a critical component for addressing complex issues that necessitate multi-role coordination [18].Nevertheless, LLMs encounter specific challenges within multiagent frameworks, especially in situations that demand a nuanced contextual comprehension and sustained memory retention, as well as adapting to fast-evolving environments and unforeseeable tasks [19].Issues such as data bias, security concerns, and the intricacies of crafting effective protocols for multi-agent cooperation stand as significant hurdles in this domain [20], [21].Thus, by summarizing LLMs' roles in multi-agent frameworks, we underscore the critical need for continued innovation and research exploration, aimed at overcoming these technological hurdles and leveraging the full potential of LLMs in complex systems [22].</p>
<p>To enhance the adaptability and collaborative capabilities of LLMs in multi-agent systems, we've implemented memory modes, including long-term support and short-term memory with environmental feedback [23].This allows LLMs to better interact, learn, and adapt in dynamic environments, leveraging past experiences and responding to changes swiftly.</p>
<p>B. The tuning method for LLMs</p>
<p>The main tuning methods include supervised fine-tuning and reinforcement learning [24].Supervised fine-tuning enhances performance by training models on specific task datasets, and is especially suitable for tasks such as natural language understanding (NLU) [25].On the other hand, reinforcement learning, guided by reward mechanisms, is suitable for handling complex and variable tasks [26].The effective combination of these two methods can significantly improve the performance of LLMs in various tasks.Notably, LLMs of reduced scale, such as those encompassing 1.8 billion parameters, can achieve performance levels akin to those of models with greater parameter counts, like 6 billion parameters, when supported by high-quality datasets [27].This demonstrates that excellent data quality and appropriate tuning strategies play a decisive role in the performance of LLMs.Therefore, investing efforts in improving data quality and choosing the right tuning methods is essential for achieving optimal performance of LLMs in various application scenarios [25].Through our work combining supervised fine-tuning with reinforcement learning, we've notably advanced LLM performance across a spectrum of tasks, showcasing significant improvements in task-specific benchmarks [24].</p>
<p>III. METHODOLOGY</p>
<p>The Collaborative Multi-Agent Language Model Tuning (CMAT) framework improves decision-making, controllability, and efficiency in complex systems by coordinating three roles: User (U), Assistant (A), and Checker (C).We finetune language models using LoRA [28], P-Tuning [29], and integrate ideas from RLHF [30], Chain of Thought (CoT), and ReAct to enhance reasoning and adaptability.</p>
<p>A. Agent Roles and Actor-Critic Dynamics</p>
<p>In CMAT, the User (U) provides inputs, the Assistant (A) acts as the Actor generating actions, and the Checker (C) serves as the Critic providing feedback.At time t, given input x t , the Assistant produces action a t based on its policy π θactor .The Checker evaluates this action and returns feedback f t , guiding continuous policy refinement.
L sup (θ actor ) = E (x,y)∼D ℓ(M θactor (x), y) ,(1)
where ℓ is the cross-entropy loss.This step ensures the model can produce useful actions before incorporating feedback loops.</p>
<p>2) Incorporating CoT and ReAct: To enhance reasoning, we adopt Chain of Thought (CoT) to generate intermediate reasoning steps before the final action:
c t = CoT(x t ), a t = π θactor (c t , x t ).(2)
Additionally, ReAct interleaves reasoning and acting tokens, encouraging more accurate and context-aware decisions.</p>
<p>Empirically, generating reasoning steps (CoT) first, then the final answer, improves correctness.This suggests that explicitly formulating the thought process guides the model towards better actions.</p>
<p>3) Feedback-Driven Policy Optimization: After supervised fine-tuning, the Assistant refines its policy using Checker feedback.We adopt an Actor-Critic-like scheme without full RL exploration.The Assistant updates its parameters via:
θ actor ← θ actor + α∇ θactor log π θactor (a t |s t )δ t ,(3)
where δ t is the error term:
δ t = r t + γV θcritic (s t+1 ) − V θcritic (s t ).(4)
Here, r t is derived from feedback f t .The Checker updates its value function:
θ critic ← θ critic + βδ t ∇ θcritic V θcritic (s t ).(5)
Unlike traditional RL that involves environment exploration, here the process relies on direct Checker feedback.The iterative cycle refines the Assistant's decision-making, ensuring alignment with the desired standards.</p>
<p>C. Checker-In-The-Loop Mechanism</p>
<p>By positioning the Checker directly in the training loop, we ensure that the Assistant receives immediate corrective signals whenever it deviates from expected behavior</p>
<p>D. Memory Management and Self-Reflection</p>
<p>The Assistant maintains two memories: short-term M S and long-term M L .Short-term memory tracks recent context:
M t+1 S = U(M t S , x t , a t , f t ),(6)
while long-term memory stores significant insights via selfreflection:
s t = φ(a t , f t , M t S ), M t+1 L = µ(M t L , s t ).(7)
Using the reflection, the Assistant updates its parameters:
θ actor ← θ actor − α∇ θactor L(f t , a t ) + γ∇ θactor G(s t ),(8)
where L(f t , a t ) encodes feedback-driven loss and G(s t ) incorporates improvements identified via reflection.</p>
<p>IV. EXPERIMENTS</p>
<p>Our evaluation framework rigorously tests intelligent agents in six key domains to ensure their readiness for diverse real-world challenges [31].These areas include seamless LLM integration into OS with an emphasis on security and user interaction; proficiency in real DB operations using SQL [32]; task execution on the simulated e-commerce platform WebShop(WS) [33]; constructing and using KGs for enhanced semantic understanding; employing the M2W dataset for complex web tasks, marking the first dataset for developing general web agents following language instructions; and applying abstract reasoning and visual tasks in the textbased ALFWorld(ALF) [34].</p>
<p>A. Dataset</p>
<p>The dataset for our research was meticulously constructed to comprehensively evaluate the capabilities of agents [35].It was established through self-collected methods, aimed at providing a rich and diverse testing environment to thoroughly assess the performance of deep learning models across various tasks [36].The construction of the dataset included key processes such as data collection, filtering, enhancement, and knowledge distillation [37].Through detailed screening and processing, we ensured the accuracy and consistency of the dataset, retaining only high-quality samples directly related to the testing objectives [36].Faced with issues of data imbalance and insufficient samples, we utilized data augmentation and knowledge distillation techniques.Knowledge distillation helped us to extract the most valuable and representative information from the vast amount of collected data, thus building an efficient and refined testing dataset.This process significantly improved the quality and applicability of the dataset, providing a solid foundation for evaluating the capabilities of model agents [38].</p>
<p>B. Evaluating Code Correction</p>
<p>As shown in the Table I, in this study, we conducted a comprehensive performance evaluation of TinyAgent-1.8Band the CodeLlama series models (CodeLlama7B and CodeLlama13B), aiming to explore their multi-task checking capabilities, includ-ing but not limited to code correction, OS configuration, DB query optimization, and WS.The experimental results showed that TinyAgent-1.8Bdemonstrated a significant advantage in cross-task performance evaluation compared to the CodeLlama series models.This performance was not only significant in code correction tasks but also prominent in other checking tasks such as OS configuration, DB query optimization, and WS management.These findings highlight that TinyAgent-1.8Bnot only possesses efficient code analysis capabilities but is also widely applicable to the inspection and optimization of other complex systems.</p>
<p>C. Baselines</p>
<p>In the baseline section of our study, we've selected Qwen-1.8B and CodeLlama-7B as pivotal benchmarks to assess the TinyAgent series' performance, excluding the CMAT framework's influence.</p>
<p>D. Results analysis</p>
<p>The results in Table II underscore the effectiveness of our fine-tuning methods, especially for the TinyAgent models.Tinyagent-1.8Bdemonstrates significant performance in the KG task, on par with advanced models like GPT-3.5.Tinyagent-7B also showcases its strengths, notably in the DB task, where it surpasses its foundational model [44], CodeLlama-7B, and offers competitive scores against GPT-4.These findings indicate the TinyAgent models' capacity to match or even surpass models with larger parameters in certain aspects.Moreover, the CMAT framework's potential to enhance the capabilities of smaller-scale models is highlighted, allowing the TinyAgent models to closely compete with the performance of advanced models such as GPT-4.</p>
<p>As illustrated in Figure 1, Our comparative analysis indicates that Tinyagent models, refined from Qwen-1.8B and CodeLlama-7B, exhibit superior performance to their base models.The incorporation of the CMAT framework further amplifies their functionality, equipping these small Models to match the capabilities of GPT-3.5.This performance boost is credited to CMAT's optimization of model interactions and its strategic use of memory modes for specific tasks, confirming its effectiveness in enhancing the sophistication of fine-tuned models [45].</p>
<p>Table III presents the impact of different prompting strategies on performance metrics.High-quality prompts significantly outperform low-quality prompts and scenarios without prompts across all evaluation metrics, demonstrating the importance of prompt design in optimizing model performance.</p>
<p>E. Error analysis</p>
<p>In our testing framework's error analysis, we observed common challenges in DB tasks faced by models, such as difficulties in understanding user requests, executing actions, and pre-action problem analysis.Many models simply respond with "OK" to specific instructions without performing actual SQL operations, indicating a gap in transforming user requests into database actions.Models often provide superficial acknowledgments without delivering precise execution or indepth problem analysis, failing to meet user expectations.In contrast, the TinyAgent series excels in understanding and converting user requests into actual SQL operations, effectively comprehending and executing tasks.It provides clear responses and adheres to user-specified SQL formats, fulfilling user expectations comprehensively.Additionally, TinyAgent's thorough pre-action problem analysis and reflection demonstrate its advanced problem-solving skills and deep understanding of issues.</p>
<p>As illustrated in Table IV, the distribution of various execution results across six tasks highlights the prevalence of specific error types, such as exceeding task limits (TLE) and invalid actions, which point to limitations in LLM agents' reasoning and decision-making within constrained timeframes.</p>
<p>F. Ablation Study</p>
<p>The Table V presents an ablation study on the TinyAgent-7B model, delineating the impact of agent-specific and general instructions on task performance.The composite model, TinyAgent-7B, demonstrates the highest efficacy, notably in WS and DB tasks, which implies its adeptness in handling complex e-commerce interactions and database management.The agentonly variant exhibits a decline in performance, suggesting that while task-specific instructions are crucial, they are not wholly sufficient for the breadth of tasks such as KG.The generalonly model's performance is considerably reduced across all tasks, with a complete inability to perform in KG and ALF, highlighting the indispensability of agent-specific instructions.This data underscores the necessity of integrating both agentspecific and general instructions to enhance the versatility and effectiveness of AI models in diverse task domains.</p>
<p>V. CONCLUSIONS</p>
<p>The main findings of our work reveal that carefully trained small-parameter models on excellent datasets can achieve performance comparable to that of large-parameter models.With the application of the CMAT framework, we further demonstrate the significant potential for performance improvement in large-parameter models, highlighting the importance of model design and optimization strategies for parameter size.In our evaluation, although most open-source LLMs performed poorly compared to API-provided models without optimization, some models displayed similar capabilities to API models after meticulous fine-tuning of the TinyAgent model.This finding emphasizes not only the importance of parameter size in handling real-world environmental interactions but also showcases the enormous potential of even smaller models through the CMAT framework and precise adjustment strategies.</p>
<p>Fig. 2 .
2
Fig. 2. In the CMAT framework, the user assigns tasks to an assistant, which generates SQL commands based on short-term and long-term memories: short-term memory provides immediate context from trajectory history, while self-reflective outputs are stored as long-term memory.The checker verifies the correctness of SQL commands before they are executed in the environment.B. Learning Strategy 1) Supervised Fine-Tuning: We start by fine-tuning the Assistant model M θactor on a labeled dataset D:</p>
<p>Fig. 3 .
3
Fig. 3. Comparative study of Llama-2-7b and TinyAgent-7b in DataBase cases.(1) In DataBase tasks with a reflection mechanism, Llama-2-7b still made errors after reflection, while TinyAgent-7b adjusted its operations after reflecting on its first failed attempt.(2) Without a reflection mechanism, TinyAgent-7b repeated the same operation and ultimately failed to complete the task.</p>
<p>TABLE I EVALUATION
I
OF CODE CORRECTION
ModelBLEU-4 ROUGE-1ROUGE-2 ROUGE-Lcodellama-7b25.0145.9129.8326.24codellama-13b26.9645.3129.5425.91tinyllama-1.8b43.3859.8637.8142.86</p>
<p>TABLE II TEST
II
SET RESULTS OF AGENTBENCH.COMPARISON BETWEEN API-BASED MODELS AND OPEN-SOURCE MODELS.BOLD: THE BEST AMONG API-BASED AND OPEN-SOURCE MODELS.
LLM TypeModelsVEROSDBKGALFWSM2Wgpt-3.5-turbo061331.615.725.916.064.116.0APIgpt-4 text-davinci-0030613 -42.4 20.132.0 16.358.8 34.978.0 20.061.6 61.729.0 26.0text-davinci-002-8.316.741.516.056.39.0tinyllama-1.1b [39]-2.80.00.00.00.00.0opt-1.3b [40]-0.70.00.00.00.00.0opt-2.7b-1.40.00.00.00.00.0qwen-1.8bchat10.422.676.80.026.65.0chatglm2-6b 1v1.14.21.30.00.00.00.0codellama-7binstruct9.72.70.00.014.35.0llama2-7b [41]chat0.04.28.00.011.67.0OSSzephyr-7b [42] baichuan2-6b [43]alpha chat12.5 2.89.7 9.75.0 0.08.0 0.045.0 6.111.0 11.0mpt-7b 2chat5.69.712.70.00.00.0qwen-7bchat12.513.07.034.30.00.0agentlm-7bchat14.633.09.016.418.410.0agentlm-7b(SFT)chat17.437.010.017.426.610.0tinyagent-1.8bchat17.728.3348.06.032.711.0tinyagent-7bchat23.141.328.08.058.712.0</p>
<p>TABLE IV DISTRIBUTION
IV
OF VARIOUS EXECUTION RESULTS ACROSS SIX TASKS.(CLE: EXCEEDED CONTEXT LIMIT, TLE: SURPASSED TASK LIMIT).TASK LIMITS EXCEEDED ARE THE MAIN REASON FOR INCOMPLETE TASKS, POINTING TO LIMITATIONS IN LLM AGENTS' REASONING AND DECISION-MAKING WITHIN CONSTRAINED TIMEFRAMES.
Execution ResultsOSDBKGALFWSM2WCompleted84.784.025.02.093.557.0CLE0.00.00.00.00.00.0Invalid Format0.03.00.00.00.00.0Invalid Action0.00.00.096.00.08.0TLE15.313.075.02.06.535.0</p>
<p>TABLE V ABLATION
V
STUDY ON THE EFFECT OF AGENT AND GENERAL INSTRUCTIONS.
ModelsOSDBKGALFWSM2Wtinyagent-7b27.343.038.010.061.814.0-agent only20.139.325.02.055.77.0-general only9.75.40.00.026.65.0</p>
<p>Gpt-4 technical report. Openai, 2023</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, arXiv:2308.129502023arXiv preprint</p>
<p>Large language models associate muslims with violence. Abubakar Abid, Maheen Farooqi, James Zou, Nature Machine Intelligence. 362021</p>
<p>Shortcut learning of large language models in natural language understanding: A survey. Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, Xia Hu, arXiv:2208.118572022arXiv preprint</p>
<p>Cloudmf: Model-driven management of multi-cloud applications. N Ferry, F Chauvel, Hui Song, A Rossini, Maksym Lushpenko, Arnor Solberg, ACM Trans. Internet Techn. 18242018</p>
<p>Comparison of approaches to service deployment. V Talwar, Qinyi Wu, C Pu, W Yan, G Jung, D Milojicic, 25th IEEE International Conference on Distributed Computing Systems (ICDCS'05). 2005</p>
<p>A new ai evaluation cosmos: Ready to play the game?. José Hernández-Orallo, Marco Baroni, Jordi Bieger, Nader Chmait, Katja David L Dowe, Fernando Hofmann, Claes Martínez-Plumed, Strannegård, Kristinn R Thórisson, AI Magazine. 3832017</p>
<p>Deal or no deal? end-to-end learning for negotiation dialogues. Mike Lewis, Denis Yarats, Devi Yann N Dauphin, Dhruv Parikh, Batra, arXiv:1706.051252017arXiv preprint</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.083612020arXiv preprint</p>
<p>Know what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, arXiv:1806.038222018arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Emergent cooperation and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms. J De Zarzà, Gemma De Curtò, Pietro Roig, Carlos T Manzoni, Calafate, Electronics. 121227222023</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Yashar Talebirad, Amirhossein Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Pangu-agent: A fine-tunable generalist agent with structured reasoning. Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, arXiv:2312.148782023arXiv preprint</p>
<p>Camel: Communicative agents for" mind" exploration of large scale language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, arXiv:2303.177602023arXiv preprint</p>
<p>Adaptive asymptotic tracking with global performance for nonlinear systems with unknown control directions. Kai Zhao, Yongduan Song, Long Philip Chen, Chen, IEEE Transactions on Automatic Control. 6732021</p>
<p>Coordinated behavior of cooperative agents using deep reinforcement learning. Elhadji Amadou, Oury Diallo, Ayumi Sugiyama, T Sugawara, Neurocomputing. 3962020</p>
<p>A novel consensus algorithm for second-order multi-agent systems without velocity measurements. Wentao Zhang, Yang Liu, Jianquan Lu, Jinde Cao, International Journal of Robust and Nonlinear Control. 272017</p>
<p>Periodic event-triggered synchronization of linear multi-agent systems with communication delays. Eloy García, Yongcan Cao, D Casbeer, IEEE Transactions on Automatic Control. 622015</p>
<p>Blockchain-based multiparty computation system. Kai Lu, Chongyang Zhang, 2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS). IEEE2020</p>
<p>Multiagent systems in construction: A ten-year review. Xin Liang, G Shen, Shanshan Bu, Journal of Computing in Civil Engineering. 302016</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, J Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, P Welinder, P Christiano, J Leike, Ryan J Lowe, abs/2203.02155ArXiv. 2022</p>
<p>Universal language model finetuning for text classification. Jeremy Howard, Sebastian Ruder, 2018</p>
<p>Human-level control through deep reinforcement learning. K Volodymyr Mnih, David Kavukcuoglu, Andrei A Silver, J Rusu, Marc G Veness, A Bellemare, Martin A Graves, A Riedmiller, Georg Fidjeland, Stig Ostrovski, Charlie Petersen, Amir Beattie, Ioannis Sadik, Helen Antonoglou, D King, Daan Kumaran, S Wierstra, D Legg, Hassabis, Nature. 5182015</p>
<p>Learning to summarize from human feedback. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan J Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano, ArXiv. 2009.01325. 2020</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.096852021arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Reinforcement learning for demand response: A review of algorithms and modeling techniques. Zoltán José R Vázquez-Canteli, Nagy, Applied energy. 2352019</p>
<p>The programmer's assistant: Conversational interaction with a large language model for software development. Steven I Ross, Fernando Martinez, Stephanie Houde, Michael J Muller, Justin D Weisz, abs/2302.07080ArXiv. 2023</p>
<p>The piazza peer data management system. A Halevy, Z Ives, J Madhavan, P Mork, Dan Suciu, I Tatarinov, IEEE Transactions on Knowledge and Data Engineering. 162004</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Jul 2022</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Knowledge distillation: A survey. Jianping Gou, B Yu, S Maybank, D Tao, International Journal of Computer Vision. 1292020</p>
<p>Data distillation: A survey. Noveen Sachdeva, Julian Mcauley, abs/2301.04272ArXiv. 2023</p>
<p>Broad learning system: An effective and efficient incremental learning system without the need for deep architecture. C L P Chen, Zhulin Liu, IEEE Transactions on Neural Networks and Learning Systems. 292018</p>
<p>Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. K Asit, Debbie Mishra, Marr, abs/1711.05852ArXiv. 2017</p>
<p>Tinyllama: An open-source small language model. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu, arXiv:2401.023852024arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Llama 2: Open foundation and finetuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Zephyr: Direct distillation of lm alignment. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, arXiv:2310.169442023arXiv preprint</p>
<p>Baichuan 2: Open large-scale language models. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Yan, arXiv:2309.103052023arXiv preprint</p>
<p>Selecting informative contexts improves language model fine-tuning. Richard J Antonello, Javier Turek, Alexander G Huth, ArXiv. 2005.00175, 2020</p>
<p>A linearized framework and a new benchmark for model selection for fine-tuning. A Deshpande, A Achille, Avinash Ravichandran, L Hao Li, Zancato, C Charless, Rahul Fowlkes, Stefano Bhotika, P Soatto, Perona, abs/2102.00084ArXiv. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
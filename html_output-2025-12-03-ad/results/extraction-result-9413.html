<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9413 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9413</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9413</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-271161651</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.08922v1.pdf" target="_blank">Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of artificial intelligence (AI), large language models (LLMs) such as GPT-4 have garnered significant attention in the scientific community, demonstrating great potential in advancing scientific discovery. This progress raises a critical question: are these LLMs well-aligned with real-world physicochemical principles? Current evaluation strategies largely emphasize fact-based knowledge, such as material property prediction or name recognition, but they often lack an understanding of fundamental physicochemical mechanisms that require logical reasoning. To bridge this gap, our study developed a benchmark consisting of 775 multiple-choice questions focusing on the mechanisms of gold nanoparticle synthesis. By reflecting on existing evaluation metrics, we question whether a direct true-or-false assessment merely suggests conjecture. Hence, we propose a novel evaluation metric, the confidence-based score (c-score), which probes the output logits to derive the precise probability for the correct answer. Based on extensive experiments, our results show that in the context of gold nanoparticle synthesis, LLMs understand the underlying physicochemical mechanisms rather than relying on conjecture. This study underscores the potential of LLMs to grasp intrinsic scientific mechanisms and sets the stage for developing more reliable and effective AI tools across various scientific domains.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9413.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9413.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>c-score</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>confidence-based score (c-score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable metric introduced in this paper that extracts the model's output logits for each multiple-choice option, applies an exponential normalization (softmax) and averages the probability mass assigned to the correct option across questions to quantify model confidence in mechanistic answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., GPT-4, Claude 3, Mixtral-8x7B, Mistral-7B, Vicuna variants, Gemma, Qwen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to many transformer-based LLMs evaluated in the study; the c-score is model-agnostic and computed from each model's logits over the fixed-answer vocabulary {A,B,C,D}.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Probability that a given multiple-choice option is the correct mechanistic explanation for a gold nanoparticle synthesis experimental case (i.e., correctness of an answer about physicochemical synthesis mechanism), not a prediction of a future real-world discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct extraction of logits from the model's output for the four answer tokens, followed by exponential normalization (softmax) to yield per-option probabilities; the correct-option probability is recorded for each question and averaged over the dataset (Eq. 1 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Normalized probability (softmax) per option, reported as a fraction/percentage; these per-question probabilities are averaged to produce the c-score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared c-score to binary accuracy over 775 multiple-choice questions across five temperature settings (0.1, 0.3, 0.5, 0.7, 0.9); c-score computed as mean of correct-option softmax probabilities (Eq. 1); accuracy computed as percent of questions where the model's top answer equals the gold answer. No Brier, log-likelihood, or calibration plots were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>c-score revealed differences beyond accuracy: e.g., Mixtral-8x7B's c-score was ~8% higher than its accuracy (indicating higher confidence mass on correct answers), while Vicuna-33B's c-score was ~6% lower than its accuracy. The study reports that c-score can uncover whether correct answers are chosen with high model-internal probability mass, but absolute c-score numbers per model are provided in figures and supplementary tables (not all numeric c-score values are listed in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>c-score reflects model-internal confidence on multiple-choice items, not real-world likelihood of future discoveries; it depends on faithful access to logits and consistent answer-tokenization; influenced by temperature setting; dataset biases and question construction (some questions generated/paraphrased using GPT-4) can contaminate evaluation; no external calibration metrics (e.g., Brier score) or longitudinal real-world outcome comparisons were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared implicitly to binary accuracy and to random-guess baseline (25%). c-score provided complementary information to accuracy, revealing cases where models put low/high probability mass on the correct option despite the binary outcome. No human expert probability elicitation or traditional probabilistic models were used as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Paper suggests temperature tuning, prompt/cue-word engineering (used in dataset construction), and selecting stronger-performing models; also implies that using c-score in conjunction with accuracy gives a more nuanced evaluation. No formal post-hoc calibration or ensembling methods were applied in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9413.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9413.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>knowledge probing / logit probing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>knowledge probing via logit inspection (probability probing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique used in this paper to assess whether LLM outputs reflect genuine knowledge by inspecting the model logits (pre-softmax scores) over answer tokens to infer confidence distributions among competing mechanistic options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., GPT-4, Claude 3, Mistral-7B, Mixtral-8x7B, Vicuna variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs evaluated in the benchmark; knowledge probing inspects each model's logits for the tokens A/B/C/D corresponding to the multiple-choice options.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Probability distribution over candidate mechanistic explanations for gold nanoparticle synthesis multiple-choice questions (i.e., per-option confidence), not predictions of future real-world discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Extract logits for the predefined answer-token vocabulary before Softmax; apply temperature scaling (varied in experiments) and Softmax to obtain option probabilities; analyze distribution sharpness and relative mass on the gold answer.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Per-option probabilities (softmax outputs) expressed as fractions/percentages; reported across temperature settings with error bars representing variability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared the probability mass assigned to the gold answer across models and temperatures; reported these alongside binary accuracy to judge whether high accuracy coincides with high internal confidence (sharp distributions) or appears to be guess-like (diffuse distributions). No formal calibration metrics were computed.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Examples show some models (e.g., Mistral-7B in a displayed case) gave near-100% confidence to the correct option, implying solid internal knowledge; other models showed confused distributions (e.g., ~50% on multiple options). Temperature changes impacted distribution sharpness and accuracy patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Logit probing reveals model-internal confidence but may not correspond to calibrated real-world probabilities; sensitive to temperature and tokenization; relies on the assumption that answer tokens map unambiguously to logits; does not by itself validate that high confidence corresponds to true mechanistic understanding beyond the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared relative behavior across models and to random guessing baseline; probing complements accuracy but was not compared to external probabilistic predictors or human probability judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Temperature tuning and prompt engineering (cue-word design) were used to stabilize probing; authors recommend combining c-score with accuracy for better assessment. They do not report applying explicit calibration methods (e.g., Platt scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9413.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9413.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source, high-performing LLM from OpenAI used in this paper both to assist dataset construction (paraphrasing and summarization) and as a primary evaluation baseline; logits were accessed via OpenAI API in a preview model (gpt-4-0125-preview).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based LLM from OpenAI; used in experiments and for cue-word engineering to extract and paraphrase mechanistic descriptions. Paper does not report parameter count or full training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (i.e., whether a mechanistic statement is true for a given experiment), not a prediction of a future scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Standard multiple-choice prompting with logits probed to produce per-option probabilities (softmax over answer tokens) and to compute c-score; temperature varied across {0.1,0.3,0.5,0.7,0.9} to study stability.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Softmax-derived probabilities per answer option (reported as percentages); averaged across 775 questions to produce c-score.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Binary accuracy (percentage of correct top answers) and c-score (mean correct-option probability) over the 775-question benchmark across five temperatures; comparisons to random guessing baseline (25%).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>GPT-4 achieved accuracy ~80.5% on the benchmark and was among the top-performing models; c-score results indicate decent internal confidence for many correct answers (specific c-score numeric value for GPT-4 is reported in figures/supplementary but not enumerated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not predict future scientific discoveries; performance depends on temperature and prompt format; dataset creation partially involved GPT-4 itself (paraphrasing), introducing potential data-model overlap/bias; no external calibration metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed random guessing (25%); slightly behind Claude-3 in accuracy (Claude 84.8% vs GPT-4 80.5%); outperformed most open-source models in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Temperature tuning, careful prompt/cue-word engineering for dataset and question construction, and combination of c-score with accuracy; authors note fine-tuning and higher-quality training data could improve open-source model performance but do not perform these interventions for GPT-4 in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9413.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9413.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 (as evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 (claude-3-ops-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM (Anthropic) evaluated in the benchmark; reported as the top-performing model in accuracy on the gold nanoparticle synthesis multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 (claude-3-ops-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source transformer-based conversational LLM from Anthropic; specific architecture and parameter count not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (not a forecast of future discoveries).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Multiple-choice prompting with logits inspected to produce per-option probabilities for c-score; performance evaluated across varying temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Per-option softmax probabilities (percentages); c-score is the mean correct-option probability across the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured binary accuracy and c-score across 775 questions and five temperature settings; compared to random guessing baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Claude achieved the highest reported accuracy (~84.8%) among models evaluated. c-score behavior shows strong confidence on many correct answers (detailed numeric c-scores appear in figures/supplementary).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same caveats as other evaluated LLMs: benchmark concerns, temperature sensitivity, and lack of calibration metrics relating model confidence to real-world outcome probabilities; does not perform predictions of future real-world discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed GPT-4 and all tested open-source models on this benchmark and far exceeded random guessing (25%).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Authors used temperature experiments and recommend combining c-score with accuracy; no further calibration or ensembling performed in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9413.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9413.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral-8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral (sparse mixture-of-experts) 8x7B variant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source sparse Mixture-of-Experts decoder-only LLM evaluated in the study; performed best among open-source models on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only model with a Sparse Mixture-of-Experts architecture; feedforward blocks select among 8 parameter groups (experts) with a router network selecting two experts per token. Evaluated in an instruction-tuned variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (not future discovery prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Standard multiple-choice prompting; logits extracted and softmaxed to compute per-option probabilities and c-score; temperature varied during experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Softmax probabilities per answer option; c-score averaged over questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Binary accuracy and c-score over 775 questions and multiple temperatures; results reported relative to other models and the random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Mixtral-8x7B achieved ~70.4% accuracy (best among open-source models) and its c-score was ~8% higher than its accuracy, indicating that when it was correct it often assigned substantial probability mass to the correct option.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower absolute performance than top closed-source models; sensitive to temperature; c-score/accuracy gap suggests over/under-confidence patterns that require further calibration; lacks human-expert probability comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Beat random guessing (25%) and other open-source models, but trailed closed-source Claude and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Temperature tuning, further fine-tuning, and improved training data suggested as ways to improve open-source model performance; authors used prompt/cue-word engineering for dataset creation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>A gpt-4 reticular chemist for guiding mof discovery. <em>(Rating: 2)</em></li>
                <li>The impact of large language models on scientific discovery: a preliminary study using gpt-4. <em>(Rating: 2)</em></li>
                <li>Generative Artificial Intelligence GPT-4 Accelerates Knowledge Mining and Machine Learning for Synthetic Biology. <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry. <em>(Rating: 2)</em></li>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9413",
    "paper_id": "paper-271161651",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "c-score",
            "name_full": "confidence-based score (c-score)",
            "brief_description": "An interpretable metric introduced in this paper that extracts the model's output logits for each multiple-choice option, applies an exponential normalization (softmax) and averages the probability mass assigned to the correct option across questions to quantify model confidence in mechanistic answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (e.g., GPT-4, Claude 3, Mixtral-8x7B, Mistral-7B, Vicuna variants, Gemma, Qwen)",
            "model_description": "Applied to many transformer-based LLMs evaluated in the study; the c-score is model-agnostic and computed from each model's logits over the fixed-answer vocabulary {A,B,C,D}.",
            "model_size": null,
            "prediction_target": "Probability that a given multiple-choice option is the correct mechanistic explanation for a gold nanoparticle synthesis experimental case (i.e., correctness of an answer about physicochemical synthesis mechanism), not a prediction of a future real-world discovery.",
            "prediction_method": "Direct extraction of logits from the model's output for the four answer tokens, followed by exponential normalization (softmax) to yield per-option probabilities; the correct-option probability is recorded for each question and averaged over the dataset (Eq. 1 in paper).",
            "probability_format": "Normalized probability (softmax) per option, reported as a fraction/percentage; these per-question probabilities are averaged to produce the c-score.",
            "evaluation_method": "Compared c-score to binary accuracy over 775 multiple-choice questions across five temperature settings (0.1, 0.3, 0.5, 0.7, 0.9); c-score computed as mean of correct-option softmax probabilities (Eq. 1); accuracy computed as percent of questions where the model's top answer equals the gold answer. No Brier, log-likelihood, or calibration plots were reported.",
            "results": "c-score revealed differences beyond accuracy: e.g., Mixtral-8x7B's c-score was ~8% higher than its accuracy (indicating higher confidence mass on correct answers), while Vicuna-33B's c-score was ~6% lower than its accuracy. The study reports that c-score can uncover whether correct answers are chosen with high model-internal probability mass, but absolute c-score numbers per model are provided in figures and supplementary tables (not all numeric c-score values are listed in main text).",
            "limitations_or_challenges": "c-score reflects model-internal confidence on multiple-choice items, not real-world likelihood of future discoveries; it depends on faithful access to logits and consistent answer-tokenization; influenced by temperature setting; dataset biases and question construction (some questions generated/paraphrased using GPT-4) can contaminate evaluation; no external calibration metrics (e.g., Brier score) or longitudinal real-world outcome comparisons were performed.",
            "comparison_to_baselines": "Compared implicitly to binary accuracy and to random-guess baseline (25%). c-score provided complementary information to accuracy, revealing cases where models put low/high probability mass on the correct option despite the binary outcome. No human expert probability elicitation or traditional probabilistic models were used as baselines.",
            "methods_for_improvement": "Paper suggests temperature tuning, prompt/cue-word engineering (used in dataset construction), and selecting stronger-performing models; also implies that using c-score in conjunction with accuracy gives a more nuanced evaluation. No formal post-hoc calibration or ensembling methods were applied in this work.",
            "uuid": "e9413.0",
            "source_info": {
                "paper_title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "knowledge probing / logit probing",
            "name_full": "knowledge probing via logit inspection (probability probing)",
            "brief_description": "A technique used in this paper to assess whether LLM outputs reflect genuine knowledge by inspecting the model logits (pre-softmax scores) over answer tokens to infer confidence distributions among competing mechanistic options.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs (e.g., GPT-4, Claude 3, Mistral-7B, Mixtral-8x7B, Vicuna variants)",
            "model_description": "Transformer-based LLMs evaluated in the benchmark; knowledge probing inspects each model's logits for the tokens A/B/C/D corresponding to the multiple-choice options.",
            "model_size": null,
            "prediction_target": "Probability distribution over candidate mechanistic explanations for gold nanoparticle synthesis multiple-choice questions (i.e., per-option confidence), not predictions of future real-world discoveries.",
            "prediction_method": "Extract logits for the predefined answer-token vocabulary before Softmax; apply temperature scaling (varied in experiments) and Softmax to obtain option probabilities; analyze distribution sharpness and relative mass on the gold answer.",
            "probability_format": "Per-option probabilities (softmax outputs) expressed as fractions/percentages; reported across temperature settings with error bars representing variability.",
            "evaluation_method": "Compared the probability mass assigned to the gold answer across models and temperatures; reported these alongside binary accuracy to judge whether high accuracy coincides with high internal confidence (sharp distributions) or appears to be guess-like (diffuse distributions). No formal calibration metrics were computed.",
            "results": "Examples show some models (e.g., Mistral-7B in a displayed case) gave near-100% confidence to the correct option, implying solid internal knowledge; other models showed confused distributions (e.g., ~50% on multiple options). Temperature changes impacted distribution sharpness and accuracy patterns.",
            "limitations_or_challenges": "Logit probing reveals model-internal confidence but may not correspond to calibrated real-world probabilities; sensitive to temperature and tokenization; relies on the assumption that answer tokens map unambiguously to logits; does not by itself validate that high confidence corresponds to true mechanistic understanding beyond the benchmark.",
            "comparison_to_baselines": "Compared relative behavior across models and to random guessing baseline; probing complements accuracy but was not compared to external probabilistic predictors or human probability judgments.",
            "methods_for_improvement": "Temperature tuning and prompt engineering (cue-word design) were used to stabilize probing; authors recommend combining c-score with accuracy for better assessment. They do not report applying explicit calibration methods (e.g., Platt scaling).",
            "uuid": "e9413.1",
            "source_info": {
                "paper_title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (as evaluated)",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A closed-source, high-performing LLM from OpenAI used in this paper both to assist dataset construction (paraphrasing and summarization) and as a primary evaluation baseline; logits were accessed via OpenAI API in a preview model (gpt-4-0125-preview).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0125-preview)",
            "model_description": "Closed-source transformer-based LLM from OpenAI; used in experiments and for cue-word engineering to extract and paraphrase mechanistic descriptions. Paper does not report parameter count or full training details.",
            "model_size": null,
            "prediction_target": "Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (i.e., whether a mechanistic statement is true for a given experiment), not a prediction of a future scientific discovery.",
            "prediction_method": "Standard multiple-choice prompting with logits probed to produce per-option probabilities (softmax over answer tokens) and to compute c-score; temperature varied across {0.1,0.3,0.5,0.7,0.9} to study stability.",
            "probability_format": "Softmax-derived probabilities per answer option (reported as percentages); averaged across 775 questions to produce c-score.",
            "evaluation_method": "Binary accuracy (percentage of correct top answers) and c-score (mean correct-option probability) over the 775-question benchmark across five temperatures; comparisons to random guessing baseline (25%).",
            "results": "GPT-4 achieved accuracy ~80.5% on the benchmark and was among the top-performing models; c-score results indicate decent internal confidence for many correct answers (specific c-score numeric value for GPT-4 is reported in figures/supplementary but not enumerated in main text).",
            "limitations_or_challenges": "Does not predict future scientific discoveries; performance depends on temperature and prompt format; dataset creation partially involved GPT-4 itself (paraphrasing), introducing potential data-model overlap/bias; no external calibration metrics reported.",
            "comparison_to_baselines": "Outperformed random guessing (25%); slightly behind Claude-3 in accuracy (Claude 84.8% vs GPT-4 80.5%); outperformed most open-source models in the study.",
            "methods_for_improvement": "Temperature tuning, careful prompt/cue-word engineering for dataset and question construction, and combination of c-score with accuracy; authors note fine-tuning and higher-quality training data could improve open-source model performance but do not perform these interventions for GPT-4 in this study.",
            "uuid": "e9413.2",
            "source_info": {
                "paper_title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude-3 (as evaluated)",
            "name_full": "Claude 3 (claude-3-ops-20240229)",
            "brief_description": "A closed-source LLM (Anthropic) evaluated in the benchmark; reported as the top-performing model in accuracy on the gold nanoparticle synthesis multiple-choice tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3 (claude-3-ops-20240229)",
            "model_description": "Closed-source transformer-based conversational LLM from Anthropic; specific architecture and parameter count not provided in the paper.",
            "model_size": null,
            "prediction_target": "Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (not a forecast of future discoveries).",
            "prediction_method": "Multiple-choice prompting with logits inspected to produce per-option probabilities for c-score; performance evaluated across varying temperature settings.",
            "probability_format": "Per-option softmax probabilities (percentages); c-score is the mean correct-option probability across the dataset.",
            "evaluation_method": "Measured binary accuracy and c-score across 775 questions and five temperature settings; compared to random guessing baseline.",
            "results": "Claude achieved the highest reported accuracy (~84.8%) among models evaluated. c-score behavior shows strong confidence on many correct answers (detailed numeric c-scores appear in figures/supplementary).",
            "limitations_or_challenges": "Same caveats as other evaluated LLMs: benchmark concerns, temperature sensitivity, and lack of calibration metrics relating model confidence to real-world outcome probabilities; does not perform predictions of future real-world discoveries.",
            "comparison_to_baselines": "Outperformed GPT-4 and all tested open-source models on this benchmark and far exceeded random guessing (25%).",
            "methods_for_improvement": "Authors used temperature experiments and recommend combining c-score with accuracy; no further calibration or ensembling performed in this work.",
            "uuid": "e9413.3",
            "source_info": {
                "paper_title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mixtral-8x7B",
            "name_full": "Mixtral (sparse mixture-of-experts) 8x7B variant",
            "brief_description": "An open-source sparse Mixture-of-Experts decoder-only LLM evaluated in the study; performed best among open-source models on the benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-Instruct-v0.1",
            "model_description": "Decoder-only model with a Sparse Mixture-of-Experts architecture; feedforward blocks select among 8 parameter groups (experts) with a router network selecting two experts per token. Evaluated in an instruction-tuned variant.",
            "model_size": "8x7B",
            "prediction_target": "Correct mechanistic option for each gold nanoparticle synthesis multiple-choice question (not future discovery prediction).",
            "prediction_method": "Standard multiple-choice prompting; logits extracted and softmaxed to compute per-option probabilities and c-score; temperature varied during experiments.",
            "probability_format": "Softmax probabilities per answer option; c-score averaged over questions.",
            "evaluation_method": "Binary accuracy and c-score over 775 questions and multiple temperatures; results reported relative to other models and the random baseline.",
            "results": "Mixtral-8x7B achieved ~70.4% accuracy (best among open-source models) and its c-score was ~8% higher than its accuracy, indicating that when it was correct it often assigned substantial probability mass to the correct option.",
            "limitations_or_challenges": "Lower absolute performance than top closed-source models; sensitive to temperature; c-score/accuracy gap suggests over/under-confidence patterns that require further calibration; lacks human-expert probability comparisons.",
            "comparison_to_baselines": "Beat random guessing (25%) and other open-source models, but trailed closed-source Claude and GPT-4.",
            "methods_for_improvement": "Temperature tuning, further fine-tuning, and improved training data suggested as ways to improve open-source model performance; authors used prompt/cue-word engineering for dataset creation.",
            "uuid": "e9413.4",
            "source_info": {
                "paper_title": "Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "A gpt-4 reticular chemist for guiding mof discovery.",
            "rating": 2,
            "sanitized_title": "a_gpt4_reticular_chemist_for_guiding_mof_discovery"
        },
        {
            "paper_title": "The impact of large language models on scientific discovery: a preliminary study using gpt-4.",
            "rating": 2,
            "sanitized_title": "the_impact_of_large_language_models_on_scientific_discovery_a_preliminary_study_using_gpt4"
        },
        {
            "paper_title": "Generative Artificial Intelligence GPT-4 Accelerates Knowledge Mining and Machine Learning for Synthetic Biology.",
            "rating": 2,
            "sanitized_title": "generative_artificial_intelligence_gpt4_accelerates_knowledge_mining_and_machine_learning_for_synthetic_biology"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry.",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks.",
            "rating": 1,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        }
    ],
    "cost": 0.0124865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?
12 Jul 2024</p>
<p>Yingming Pu 
Zhejiang University
310030HangzhouZhejiangChina</p>
<p>Department of Computer Science and Engineering
School of Engineering
Wetlake University
310030HangzhouZhejiangChina</p>
<p>Liping Huang 
Department of Chemistry
School of Science
Key Laboratory for Quantum Materials of Zhejiang Province
Wetlake University
310030HangzhouZhejiangChina</p>
<p>Tao Lin lintao@westlake.edu.cn 0000-0002-3246-6935
Department of Computer Science and Engineering
School of Engineering
Wetlake University
310030HangzhouZhejiangChina</p>
<p>Research Center for Industries of the Future
Wetlake University
310030HangzhouZhejiangChina</p>
<p>Hongyu Chen chenhongyu@westlake.edu.cn 0000-0002-5325-9249
Department of Chemistry
School of Science
Key Laboratory for Quantum Materials of Zhejiang Province
Wetlake University
310030HangzhouZhejiangChina</p>
<p>Institute of Natural Sciences
Westlake Institute for Advanced Study
310024HangzhouChina</p>
<p>Leveraging large language models for nano synthesis mechanism explanation: solid foundations or mere conjectures?
12 Jul 2024A3F2FAD454A7C547436608496256E1A8arXiv:2407.08922v1[cs.LG]Large langauge modelsevaluationphysicochemical mechanismsc-score
With the rapid development of artificial intelligence (AI), large language models (LLMs) such as GPT-4 have garnered significant attention in the scientific community, demonstrating great potential in advancing scientific discovery.This progress raises a critical question: are these LLMs well-aligned with real-world physicochemical principles?Current evaluation strategies largely emphasize factbased knowledge, such as material property prediction or name recognition, but they often lack an understanding of fundamental physicochemical mechanisms that require logical reasoning.To bridge this gap, our study developed a benchmark consisting of 775 multiple-choice questions focusing on the mechanisms of gold nanoparticle synthesis.By reflecting on existing evaluation metrics, we question whether a direct true-or-false assessment merely suggests conjecture.Hence, we propose a novel evaluation metric, the confidence-based score (c-score), which probes the output logits to derive the precise probability for the correct answer.Based on extensive experiments, our results show that in the context of gold</p>
<p>Introduction</p>
<p>Achieving precise synthesis has long been a dream for materials chemists.This involves using a range of controllable material synthesis techniques to create materials with specific structures and properties based on the underlying physicochemical mechanisms.[1][2][3][4] To overcome the limited scope of each set of synthetic conditions, building connection across a wide range of methods and scenarios is crucial.[5,6] This would expand the feasibility and adaptability of synthetic processes, ultimately enabling the tailored production of materials to meet specific scientific and technological requirements.[7][8][9] Standing at the forefront of the times, designing cutting-edge deep learning methods combined with existing knowledge is one of the most promising methods to achieve controllable material synthesis.[10][11][12] It is important to note that all literature are written in human languages.In this context, large language models (LLMs), such as GPT-4, are promising solution to complex problems.It has demonstrated exceptional results in autonomous biological and chemical synthesis experiments, amongst other domains, because of their learning ability.[13][14][15][16][17][18][19] Despite efforts to let LLMs deal with synthesis tasks, a critical question remains: do these LLMs grasp the realworld physicochemical principles?Solid foundations or mere conjectures?</p>
<p>Among existing investigations, the common and straightforward approach for answering this question is fact-based evaluation, which can measure the learning performance of the model.[20][21][22][23][24][25][26][27][28][29][30].Meanwhile, evaluating the cognitive logic behind the principles is far more challenging yet essential for addressing key scientific issues.[31] For instance, Alexander Fleming observed that bacteria could not survive where mold grew-a simple fact.Yet, this correlation alone could not explain why the mold inhibited bacterial growth.Through reasoning, Fleming discovered the penicillin.This case underlines the critical role of reasoning in scientific research.</p>
<p>Inspired by the research of nanomaterial synthesis, we embark on a feasibility study regarding whether LLMs can truly comprehend underlying physicochemical principles.Specifically, we construct 775 expert-level test questions, covering six primary methods of gold nanoparticle synthesis and six major categories of nanomaterial structures, to thoroughly assess the capabilities of current LLMs, more details are shown in Method section.In this work, our contribution encompasses the following key elements, also shown in Figure 1: 1.We propose a synthetic mechanistic descriptor, grouped by initial conditions, variable adjustments, and experimental observations, to deal with the material synthesis mechanism study.Nanosynthesis study loop: begins with basic conditions, leading to the discovery of novel synthesis rules through experiments involving variable adjustments.b) exemplifies the synthesis mechanism, dissected into causality and correlations, with an emphasis on correlations described through condition-observation pairs.c) outlines the process from sourcing relevant literature (using key area keywords) for benchmark construction and model evaluation.</p>
<ol>
<li>A benchmark of gold nanoparticle synthesis mechanisms, including 775 multiple selection questions focusing on synthesis experiments, is built by using the descriptor.3. A confidence-based score (c-score) is introduced, offering an interpretable measurement of LLMs to understand complex synthesis mechanisms.</li>
</ol>
<p>Method</p>
<p>Preparation of datasets</p>
<p>The dataset for evaluation was meticulously selected from high-quality papers boasting an IF &gt; 15, with a particular emphasis on synthesis methods for controlling the structure of gold nanoparticles.To ensure the relevance and quality of the papers to meet the theme of gold nanoparticle synthesis, we manually reviewed over 220 articles from a diverse range of publishers and esteemed scientific journals.Each paper underwent a process of key experimental information extraction, leading to the summarization and collection of 775 experimental records, categorized under a condition-observationmechanism fashion.This structure serves as a descriptor for expressing any controlled experiment and is a usual loop for discovering synthesis principles.This systematic design not only enabled the creation of a unified framework of descriptors to assist in compiling test questions, but also, given the unstructured nature of mechanistic expression in nanomaterial synthesis, emphasized the importance of using average sampling in the design of the evaluation set.In this approach, the dataset distribution and cases are shown in Figure 2, and we address three pivotal issues and challenges:</p>
<p>1.The keywords classification of evaluation content and the synthesis of knowledge for uniform sampling in this domain.</p>
<ol>
<li>The precise extraction of conclusions and the completeness of experiment conditions in each report.</li>
</ol>
<p>The completeness of correlation between experimental conclusions and mechanisms</p>
<p>of nanomaterial synthesis.</p>
<p>Regarding the keywords classification issue, our evaluation perspective is based on either material synthesis methods or material morphology.The former category predominantly focuses on the seed-mediated synthesis method, a commonly utilized approach for synthesizing nanoparticles with complex structures.The latter category emphasizes mechanisms more generally applicable to nanocyrstals, rod-shaped particles, and some unmarked systems, with their distribution illustrated in Figure 2a.Two distributions of the data indicate a wide range knowledge points of gold nanoparticle synthesis considered in this work.</p>
<p>For the insight extraction problem, we employed cue word engineering in conjunction with predefined descriptors to extract the descriptions of synthesis experiments from each paper.This was achieved by using pre-defined prompts and then, manual checking.It was determined that for each report, on average, experimental report included five particularly relevant initial conditions and three sets of experimental setups, such as the increase or decrease of certain parameters, the presence or absence thereof, along with a corresponding number or more of observations leaning towards conclusions, as shown in Figure 2b, each point represents a scientific report.To facilitate easier visualization, we applied a jittering technique, meaning that each point was moved slightly by adding a small random values to the integer coordinates.</p>
<p>For mechanism completeness, we posit that accurate division based on mechanistic tendencies can significantly enhance the precision of model evaluation.However, considering the inherent bias in dividing mechanisms-where a handful of studies fail to report complete and comprehensive descriptions of mechanisms compared to the average, and there exists a wide variance in the interpretation of complete mechanisms.To illustrate this point, we employ the GPT-4 via OpenAI API to model in conjunction with cue words for a more detailed division of mechanistic tendencies.Previous research has indicated that there is no large gap in evaluation between humans and LLMs even in the specific area of material science.Meanwhile, we also confirmed that 775 questions are almost making the accuracy converged for evaluating LLMs according to their performance in this task, see supplementary information (Figure S1) for more details.Furthermore, we discovered that the utilization of cue word engineering with the GPT-4 model enables rapid acquisition of summaries for nanomaterials papers, which is also demonstrated in existing work.[16,32,33] Furthermore, we discovered that the utilization of cue word engineering with the GPT-4 model enables rapid acquisition of summaries for nanomaterials papers, which is also demonstrated in another work.[34] This capability extends even to articles that do not contain experimental data (such as literature reviews, opinions, and comments), wherein GPT-4 provides feedback indicating the absence of extractable content, thereby demonstrating GPT-4's honesty in responding to user queries.</p>
<p>Ultimately, we can rephrase this refined condition-observation pair-wise data into a standardized format of questions and options with the gold answer using the GPT-4 along with predefined instructions, one case is shown in Figure 2c.These are formatted as multiple-choice questions with four options.Notably, sampling analyses have shown that the powerful paraphrasing capabilities of GPT-4 enable it to convert these data into equivalent test questions smoothly.Furthermore, we can filter, adjust and summarize them through modifications of cue words as deemed appropriate.For detailed methodologies on the aforementioned cue word engineering, and prompt with instructions, please refer to the supplementary information (Note S1).</p>
<p>LLM Baselines</p>
<p>We choose multiple existing models with different architectures and features for comparison, to better consider the inner design differences.</p>
<p>Vicuna Paradigm.Vicuna represents a pioneering effort within the open-source community.This model, conceptualized and refined by LMSYS, undergoes an extensive fine-tuning process leveraging the LLaMA series models [35], trained on a dataset comprising 70,000 user-generated dialogs.Furthermore, Vicuna is distinguished as one of the preeminent models within the subset of LLaMA-2 fine-tuned models (for vicuna-v1.3and v1.5 versions), attributed to its superior training quality and the voluminous corpus of data it utilizes.[36] Mistral and Mixtral architectures.Developed by Mistral.AI, these models represent two distinct approaches within the field of AI.The Mistral model integrates a Grouped-Query Attention mechanism to enhance inference speed and employs Sliding Window Attention to efficiently manage extended sequences with reduced computational demands.Conversely, the Mixtral model adopts an innovative architecture characterized by a high-quality Sparse Mixture of Experts.This decoder-only framework enables the feedforward block to select from eight unique parameter groups, with a router network at each layer determining the optimal combination of two groups, known as experts, to process each token and amalgamate their outputs in an additive fashion.[37,38] Qwen series.These models are meticulously fine-tuned using a dataset curated to align with a diverse range of tasks, including conversation, tool utilization, agency, and safety protocols.A notable distinction of the Qwen models lies in their token representation capacity.The 7B model processes 2.4 trillion tokens, while the 14B model handles 3.0 trillion tokens.This positions Qwen at the pinnacle of token representation capabilities compared to other models in its category.[39] Gemma framework.The Gemma model encapsulates a series of lightweight, cutting-edge open-source models that draw from the same foundational research and technological advancements underpinning the Gemini models.This lineage of models is celebrated for their formidable performance metrics, notably in comparison to the GPT-4 model.[40] Other framewroks.In the evaluation of models including, but not limited to, GPT-4 (gpt-4-0125-preview) and Claude 3 (claude-3-ops-20240229), our analysis endeavors to assess them based upon their sophisticated capabilities in solving general problems.It is pertinent to note that these models are not made available as opensource; nevertheless, they are developed through the training on extensive corpuses of data, encompassing a wide array of domains.This approach underscores the depth and breadth of knowledge these models can potentially harness, despite the proprietary nature of their development methodologies.In addition, Gemini is not considered due to its accessibility, thus, we use Gemma for a case to test its behavior.[13]</p>
<p>Evaluation metrics</p>
<p>The most direct metric for evaluation is the use of accuracy to assess the comprehensive performance of models across a certain number of test questions, with higher scores indicating stronger capabilities.This is widely used in various tasks, and benefits from a design similar to that of comprehensive human knowledge tests, directly aiming at the logical reasoning and knowledge understanding abilities of language models through a multiple-choice question format.[41,42] In this work, we aim for the model to answer the question with confidence, reflecting a quantitative understanding of the intrinsic sequential logic in material synthesis.Thus, we further evaluate LLMs by introducing the c-score with knowledge probing techniques, and more designing details are shown in the results section.Here we treat statistical accuracy, with the counting of true-or-false, as a baseline metric to evaluate the comprehensive capabilities of models, to explore their performance relative to random guessing.</p>
<p>To sum up, two perspectives with the proposed benchmark are illustrated regarding whether LLMs understand physicochemical principles, i.e., true-or-false-based accuracy, and ensure the discernment of correct answers, i.e., confidence-based score, as shown in Figure 2d.To achieve this, with a preliminary study of temperature effects on LLMs for examining the degree of stability, we use both intuitive accuracy and c-score to judge the capabilities of models in recognizing physicochemical principles.</p>
<p>Results</p>
<p>Temperature Effect Analysis</p>
<p>Language models based on the transformer architecture predict the next token through an autoregressive approach and iteratively generate the output for an entire sentence.This method also allows for the adjustment of the probability distribution of the final predicted token.</p>
<p>The decision to adjust the probability distribution using the temperature setting in language models is inspired by statistical thermodynamics, where a higher temperature signifies a greater likelihood of overcoming energy barriers.In probability models, logits function as a representation of kinetic energy.Low temperatures lead to a more concentrated distribution of values, whereas higher temperatures yield a dispersed distribution.The introduction of temperature into logits facilitates temperature sampling, which, upon being fed into the Softmax function, yields sampling probabilities.To examine the degree of stability, we investigated the impact of different temperatures on the performance of language models.We uniformly selected five temperature values ranging from 0 to 1, specifically 0.1, 0.3, 0.5, 0.7 and 0.9, to conduct controlled experiments.As demonstrated in the Figure 4, there is a trend of precision decline in models as temperature increases, with certain models exhibiting more complex fluctuations.For instance, both claude-3-ops-20240229, gpt-4-0125-preview, Mixtral-8x7B-Instruct-v0.1, Mistral-7B-Instuct-v0.2 and gpt-3.5-turboshowed a trend where accuracy initially decreased and then increased as the temperature slightly rose.</p>
<p>From the final outcomes, it is evident that temperature exerts a patterned influence on the performance of language models.Although there remains a possibility of incorrect responses, extensive statistical evidence suggests that their overall performance is marginally superior to that observed at higher temperatures, demonstrating an assured stability.</p>
<p>Results of Accuracy with Temperature</p>
<p>It is widely recognized that the capabilities of language models are predicated upon two distinct phases: pre-training and fine-tuning.On one hand, the method of pretraining is acknowledged to endow models with an understanding of context.Since it necessitates that language models predict masked words by comprehending the context (or the surrounding words), to elucidate, when describing a dog, one needs to employ descriptions such as a tailed mammal, mankind's best friend, etc., leveraging context to grasp the meaning of dog.On the other hand, the fine-tuning process equips language models with a degree of obedience to instructions and the ability for sustained dialogue, with its efficacy contingent upon the volume, variety of the final dataset, and the process of the model fine-tuning.</p>
<p>Contemporary large language models rely on the aforementioned pre-training and fine-tuning learning processes.Consequently, multiple-choice questions serve as one of the effective methods to evaluate the level of reasoning ability of the language model in a specific domain.This implies that the language model needs to provide answers according to the domain knowledge learned during the pre-training phase and the comprehensive abilities acquired during the fine-tuning phase, as required by the question.</p>
<p>The content we aim to evaluate primarily unfolds from two aspects:</p>
<p>1.The themes encompassed by the multiple-choice questions represent the knowledge being assessed, aiming to evaluate the understanding of the model in terms of the concepts and semantics demonstrated within the sentences.2. This question format supports both the different mechanisms expression and the examination of the language model's logical reasoning abilities, assessing whether it can answer based on the fundamental principles of gold nanoparticle synthesis.</p>
<p>In order to obtain the binary accuracy of the model, for each multiple-choice questions, one point will be given to the model if it selected the gold answer, otherwise zero.This process will also be repeated with different temperature settings, i.e., from 0.1 to 0.9 with 5 steps.Finally, the average score of each model will be ranked.</p>
<p>As illustrated in Figure 3a, all models significantly surpass the random guessing baseline of 25% with a remarkable margin, consistent with their capabilities in general tasks.</p>
<p>It is worth noting that Claude and GPT-4 are the best performing models in this evaluation, with accuracies of 84.8% and 80.5%, respectively.The other open source models are not very competitive, with accuracies around 70% or lower, showing a huge gap with the top two.Specifically, Claude and GPT-4 have outstanding performance on a wide range of benchmarks due to their excellent training.The open source models may perform relatively poorly due to a variety of reasons such as the type and quality of training data and model size.Among them, the Mixtral-8x7B model performs slightly better than all other open-sourced models tested, with an accuracy of about 70.4%.Gemma has an accuracy of about 44.7%, which is the last one in our evaluation.However, its accuracy is still much higher than 25% (random guess).In addition, the accuracy of other models is in the middle level, their accuracy values can be found in the supplementary information (Table S1).To sum up, we believe that all of these tested LLMs can explain some physicochemical principles, whether basic or complex, showing future potential in explaining synthetic mechanisms.</p>
<p>Results of C-scores with Temperature</p>
<p>Knowledge probing is a method designed to assess the capacity of language models, such as those in the GPT series, to understand and recall specific knowledge domains.This technique evaluates the model's comprehension by analyzing the probability distribution of tokens corresponding to the logits in the model's output.Since the prediction of the next token is governed by the distribution of logits and transformed by the Softmax function, each token is assigned a probability.Our focus is directed towards a subset of tokens with higher probabilities rather than a singular output result, as illustrated in Figure 5.Typically, a sharper distribution of predicted token probabilities indicates a higher certainty in the model response, and vice versa.This design enables an analysis of the model's responses to varied queries, discerning whether they are grounded in solid theoretical understanding or merely speculative guesses.</p>
<p>In our study, we examine the distribution at the logits layer before the model responses, which is considered an indicator of confidence, to better address the aforementioned question.We further assess this by combining previous accuracy metrics with c-score.In past multiple-choice assessments, the final answer of the model is assumed to be chosen with 100% confidence, meaning that either True or False only.This inspires us to measure its capabilities directly by the percentage of the gold answer's confidence, even the model chooses the drinkable gold answer.We here evaluate the overall confidence using the formulated c-score, which quantifies the confidence level assigned to each correct answer, as detailed in the E.q. 1:
c-score = 1 N N i=1 e L i G e L i A + e L i B + e L i C + e L i D(1)
where L i G is the probability (or confidence) of gold answer regarding the i-th question and L X is for other options.The probability of all options (assume four here) is normalized exponentially and N is the number of all questions.</p>
<p>Specifically, based on the evaluation of accuracy, we further evaluate the top-5 ranked open-source models with c-score for efficient comparison, considering only models that excel in the benchmark.These models represent typical scales in the open-source community.The evaluation process will also be repeated with different temperature settings, as mentioned before (0.1, 0.3, 0.5, 0.7 and 0.9).Finally, the average c-scores of top-5 models will be ranked for discussion due to their competitive performances.</p>
<p>Regarding the results, as shown in Figure 3, the Vicuna-33B exhibits a lower level of performance, whereas the other models showed slight improvements in the c-score compared to accuracy.In detail, the c-score of Mixtral-8x7B improves by about 8% compared to the accuracy, while the c-score of Vicuna-33B decreases by about 6%.This indicates a clear confidence difference between the two models.Similarly, Vicuna-13B, Mistral-7B and Vicuna-7B demonstrate remarkable differences between accuracy and c-scores.The results indicate that the c-score effectively measures the ability of LLMs in gold nanoparticle synthesis tasks in an interpretable manner.Such metrics suggest that utilizing c-score allows for a more appropriate assessment of language models compared with pure accuracy, revealing insights distinct from traditional accuracy statistics.</p>
<p>For a direct view of the knowledge probing, we showcase a probing result in Figure 6, where we consider the temperature effects, which are represented by the error bar.Here, option A is correct, because the higher the supersaturation (reduction rate), the more unbalanced the growth of the gold particle morphology will be, and thus a nanoparticle morphology with a high-index crystal face or high curvature will be developed.Options B, C, and D have opposite statements.For each model tested, except for option B, there is a tendency to choose A, C, and D -with a higher confidence.Among them, Mistral-7B has a confidence of nearly 100% for the correct option A, and a tendency for other options is almost 0.This result shows that the model has sufficient and solid learning of this knowledge point, and can distinguish the nanosynthesis logic involved in this question, while other models are more confused.One possible reason is that the model is interfered by certain keywords, resulting in a confidence level of about 50%.Some other examples of knowledge probing are in supplementary information (Note S2).</p>
<p>Related Work</p>
<p>In the field of materials science, existing datasets predominantly support tasks focused on factual knowledge, such as named entity recognition and classification.[20][21][22]43] Researchers utilize these datasets to benchmark the performance of language models in the materials domain.</p>
<p>Previously, three key chemistry-related capabilities in LLMs, understanding, reasoning, and explaining have been identified, and a benchmark containing eight chemistry tasks has been established.[28] Meanwhile, the potential of large language models to perform scientific synthesis, inference, and explanation across many domains for scientific discovery has been discussed, although this approach is based solely on knowledge graph inference.[29] To expend the task diversity, LLMs such as GPT-3 have been benchmarked on datasets spanning the chemical space, including molecules, materials, and reactions, across diverse tasks such as classification, regression, and inverse design.[27] With the continuous growing of the LLMs, a dataset of 650 challenging questions from the materials domain, requiring the knowledge and skills of a materials science student who has completed their undergraduate degree, has been curated.[23] While significant progress has been made in benchmarking, there remains a need for more comprehensive evaluations that encompass the full spectrum of capabilities required for advanced scientific applications.This includes the ability to reason about mechanisms and the fundamental rules of physics and chemistry.Our study developed a benchmark consisting of 775 multiple-choice questions focusing on the mechanisms of gold nanoparticle synthesis and propose a novel evaluation metric, the confidencebased score (c-score), which probes the output logits to derive the precise probability for the correct answer.</p>
<p>Conclusion</p>
<p>In this study, we introduced a novel evaluation method for assessing LLMs in the context of materials science, specifically focusing on the synthesis of gold nanoparticles.Our approach encompassed the development of 775 multiple-choice questions addressing both synthesis methods and morphological structures.By employing knowledge probing and confidence scores (c-scores), we evaluated a range of mainstream opensource and closed-source LLMs.The results of our evaluation demonstrate that c-scores are more effective in discerning whether LLMs' contributions to the synthesis tasks are rooted in an understanding of the physicochemical mechanisms, rather than mere recall of information.This finding underscores the importance of assessing the models' comprehension and logical reasoning abilities, which are crucial for facilitating genuine scientific discoveries.In conclusion, our study not only highlights the potential of LLMs in advancing materials science but also sets a precedent for the rigorous evaluation of their scientific and logical reasoning capabilities.The insights gained from this research can inform the development of more sophisticated models that are capable of making meaningful contributions to scientific discovery.</p>
<p>Fig. 1
1
Fig.1Semantic illustration of our proposed framework for large language model evaluation in nanomaterial synthesis prediction, highlighting concepts and workflow.a) Nanosynthesis study loop: begins with basic conditions, leading to the discovery of novel synthesis rules through experiments involving variable adjustments.b) exemplifies the synthesis mechanism, dissected into causality and correlations, with an emphasis on correlations described through condition-observation pairs.c) outlines the process from sourcing relevant literature (using key area keywords) for benchmark construction and model evaluation.</p>
<p>Fig. 2
2
Fig. 2 Evaluation data set illustration.a) shows the distribution of collected evaluation sets containing 775 questions categorized by synthesis methods and structures, respectively.b) displays a jittered scatter plot of manually curated research papers with the counts of mechanism, conditions and observations, with mechanism relevance from low to high, indicated by varying colors to represent the frequency of observations and varying sizes to represent the biasing towards mechanism.c) showcases the multiple selection question considered in the evaluation.The model is instructed to give the correct option.d) illustration of the probing test in our evaluation study based on the proposed c-score.</p>
<p>Fig. 3
3
Fig. 3 Evaluation accuracy of baselines and the confidence-based scores of top-5 opensourced models compared to the original accuracy in multiple selection questions.a) x-axis represents different models, while y-axis is the accuracy.The figure delineates the range in accuracy achieved by each model under different temperature settings (from 0.0 to 1.0), where the circles represent the accuracy at each temperature setting, and the diamonds denote their average values.b) The comparison between accuracy and condifence-based scores among 5 top-performance models, showing the performence increasing (green line) and decreasing (red line).</p>
<p>Fig. 4
4
Fig. 4 Evaluation results of temperature effects on baselines.Each subplot illustrates the accuracy (y-axis) of the corresponding model under various temperature settings (x-axis), organized in descending order based on the average of the model performance across multiple-choice questions at different temperatures.Each point denotes the accuracy at a fixed temperature.</p>
<p>Fig. 5
5
Fig. 5 Illustration of the knowledge probing method.Given the input with both questionoptions and instructions, the model should give answer with predefined vocabulary, which includes A, B, C and D. The probability of each option is drawn based one the ouput logits with fixed setting of the temperature before Softmax.By observing the distribution changes of all options, model behaviors can be revealed upon different test cases.We use this design to test models ability regarding the knowledge of AuNPs synthesis.</p>
<p>Fig. 6
6
Fig. 6 Knowledge probing case.The text on the left side displays the question and answer pair.The results indicate that LLMs assign different probabilities to the given options, with temperature effects represented as error bars.</p>
<p>Data and code availabilityThe dataset created for this study, along with the testing code, is available in the GitHub repository at https://github.com/Dandelionym/llmfor mechanisms.git.This repository contains all relevant data and scripts necessary to replicate our experiments and results.
Anisotropic gold nanoparticles: synthesis, properties, applications, and toxicity. N Li, P Zhao, D Astruc, Angewandte Chemie. 5372014</p>
<p>Big data in a nano world: A review on computational, datadriven design of nanomaterials structures, properties, and synthesis. R X Yang, ACS Nano. 1619873-19891 (2022</p>
<p>Shape-controlled synthesis of gold and silver nanoparticles. Y Sun, Y Xia, Science. 2982002</p>
<p>Machine learning-guided synthesis of advanced inorganic materials. B Tang, Materials Today. 412020</p>
<p>Toward autonomous design and synthesis of novel inorganic materials. N J Szymanski, Materials Horizons. 882021</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Shape-controlled synthesis of metal nanocrystals: simple chemistry meets complex physics?. Y Xia, Y Xiong, B Lim, S E Skrabalak, Angewandte Chemie. 4812009</p>
<p>Simultaneous phase and size control of upconversion nanocrystals through lanthanide doping. F Wang, Nature. 4632010</p>
<p>Highly controlled core/shell structures: tunable conductive polymer shells on gold nanoparticles and nanochains. S Xing, Journal of Materials Chemistry. 192009</p>
<p>Deep learning enabled inverse design in nanophotonics. S So, T Badloe, J.-K Noh, J Bravo-Abad, J Rho, Nanophotonics. 92020</p>
<p>Recent advances and applications of deep learning methods in materials science. K Choudhary, Computational Materials. 82021</p>
<p>Nanoparticle synthesis assisted by machine learning. H Tao, T Wu, M Aldeghi, T C Wu, A Aspuru-Guzik, E Kumacheva, Nature reviews materials. 682021</p>
<p>Gpt-4 technical report. O J Achiam, 2023Preprint at</p>
<p>Z Xiao, W Li, H Moon, G W Roell, Y Chen, Y J Tang, Generative Artificial Intelligence GPT-4 Accelerates Knowledge Mining and Machine Learning for Synthetic Biology. 202312</p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M Quantum, 2023Preprint at</p>
<p>A gpt-4 reticular chemist for guiding mof discovery. Z Zheng, Z Rong, N Rampal, C Borgs, J T Chayes, O M Yaghi, Angewandte Chemie. 6246e2023119832023</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, Neural Information Processing Systems Advances in Neural Information Processing Systems. 202336</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 2024</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, Computational Materials. 81842022</p>
<p>Named entity recognition and normalization applied to largescale information extraction from the materials science literature. L Weston, Journal of chemical information and modeling. 5992019</p>
<p>Text-mined dataset of gold nanoparticle synthesis procedures, morphologies, and size entities. K Cruse, Scientific data. 912342022</p>
<p>Looking through glass: Knowledge discovery from materials science literature using natural language processing. V Venugopal, S Sahoo, M Zaki, M Agarwal, N N Gosvami, N A Krishnan, Patterns. 272021</p>
<p>Mascqa: investigating materials science knowledge of large language models. M Zaki, N A Krishnan, Digital Discovery. 322024</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, Digital Discovery. 252023</p>
<p>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, Journal of the American Chemical Society. 145322023</p>
<p>Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. Y Kang, J Kim, 2023Preprint at</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 622024</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, Advances in Neural Information Processing Systems. 362023</p>
<p>Large language models for scientific synthesis, inference and explanation. Y Zheng, 2023Preprint at</p>
<p>The photoswitch dataset: a molecular machine learning benchmark for the advancement of synthetic chemistry. A R Thawani, 2020Preprint at</p>
<p>A survey on evaluation of large language models. Y Chang, ACM Transactions on Intelligent Systems and Technology. 1532024</p>
<p>All that's' human'is not gold: Evaluating human evaluation of generated text. E Clark, T August, S Serrano, N Haduong, S Gururangan, N A Smith, 2021Preprint at</p>
<p>Can large language models be an alternative to human evaluations?. C.-H Chiang, H.-Y Lee, 2023Preprint at</p>
<p>Single and multi-hop question-answering datasets for reticular chemistry with gpt-4-turbo. N Rampal, 2024</p>
<p>Llama: Open and efficient foundation language models. H Touvron, 2023Preprint at</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. W.-L Chiang, Advances in Neural Information Processing Systems. 202336</p>
<p>A Q Jiang, Mistral 7b. 2023Preprint at</p>
<p>A Q Jiang, Mixtral of experts. 2024Preprint at</p>
<p>. J Bai, 2023Qwen technical report. Preprint at</p>
<p>Gemini: a family of highly capable multimodal models. G Team, 2023Preprint at</p>
<p>A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. M T R Laskar, M S Bari, M Rahman, M A H Bhuiyan, S R Joty, J Huang, 2023</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 62079722023</p>
<p>Materials synthesis insights from scientific literature via text extraction and machine learning. E Kim, Chemistry of Materials. 29212017</p>            </div>
        </div>

    </div>
</body>
</html>
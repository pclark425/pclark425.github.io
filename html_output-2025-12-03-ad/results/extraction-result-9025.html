<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9025 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9025</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9025</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-277510184</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02789v1.pdf" target="_blank">A Framework for Robust Cognitive Evaluation of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Emergent cognitive abilities in large language models (LLMs) have been widely observed, but their nature and underlying mechanisms remain poorly understood. A growing body of research draws on cognitive science to investigate LLM cognition, but standard methodologies and experimen-tal pipelines have not yet been established. To address this gap we develop CognitivEval, a framework for systematically evaluating the artificial cognitive capabilities of LLMs, with a particular emphasis on robustness in response collection. The key features of CognitivEval include: (i) automatic prompt permutations, and (ii) testing that gathers both generations and model probability estimates. Our experiments demonstrate that these features lead to more robust experimental outcomes. Using CognitivEval, we replicate five classic experiments in cognitive science, illustrating the framework's generalizability across various experimental tasks and obtaining a cognitive profile of several state of the art LLMs. CognitivEval will be released publicly to foster broader collaboration within the cognitive science community.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9025.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9025.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WCST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wisconsin Card Sorting Task (textual adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only adaptation of the Wisconsin Card Sorting Task to measure cognitive flexibility/executive function in LLMs; models must infer a hidden sorting rule from feedback and adapt when the rule changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Framework for Robust Cognitive Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source transformer LLMs (Gemma2, Llama3.1, Qwen2) evaluated via Huggingface; larger models quantized to 4-bit for inference; logits/probabilities captured for targets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B, 27B, 8B, 70B, 7B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wisconsin Card Sorting Task (WCST)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A sequential feedback-driven classification task where participants infer and adapt to an implicit sorting rule (shape, color, count); measures cognitive flexibility and perseveration (preservation) errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Mean correct answer rates across all prompts/trials (from Table 1): Gemma2-9B 27%, Gemma2-27B 43%, Llama3.1-8B 43%, Llama3.1-70B 39%, Qwen2-7B 44%, Qwen2-72B 40%. Preservation (perseveration) error rates substantial (e.g., 36% for Gemma2-9B, 50% for Qwen2-7B). Accuracy declines with dialogue length (negative Pearson r per model).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Typical adult accuracy ~70–80% (paper estimates baseline ≈77% from normative studies). Humans take ~2 trials to infer a new rule and respond ~1s under speeded instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs are well below human baseline on WCST (substantially lower accuracy, high preservation errors); LLM accuracy declines over the dialogue whereas humans improve after rule inference.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>WCST translated into text and run as a chat dialogue of 102 stimuli; initial message included instructions + three examples; each rule presented 10–15 consecutive trials and rules repeated twice (order provided). Models responded with an integer option; response parsing extracted final digit. Prompt perturbations: 30 prompt variants (3 paraphrases × 10 data formats) were used; both model generations and internal probabilities recorded. No Chain-of-Thought elicitation; sequential dialogue used to emulate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Task is a textual translation of a visual task — modality differences may affect performance. LLM architecture differs from human cognition so direct analogy is limited. Results sensitive to prompt wording (prompt perturbations change accuracies and can reverse model comparisons). No human reaction-time data from the same experimental setup; authors note human-LLM comparison is imperfect for some constructs. Models were quantized for larger sizes (4-bit) which could affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Framework for Robust Cognitive Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9025.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9025.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flanker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eriksen Flanker Task (string-letter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Textual flanker task where model must respond to the center character mapping while ignoring flankers; measures attentional control and response inhibition (congruent vs. incongruent conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Framework for Robust Cognitive Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned open-source transformer LLMs evaluated with both generation accuracy and token probability measures; inference via Huggingface; prompt permutations applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B, 27B, 8B, 70B, 7B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Eriksen Flanker Task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants must respond to the central stimulus (mapped to a response key) while ignoring flanking stimuli; congruent trials (flankers support same response) vs. incongruent trials (flankers cue a different response) measure inhibitory control and attentional conflict.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Across 32 stimuli × 30 prompt variations (n=960): near-perfect accuracy on congruent strings (≈~100%); accuracy in incongruent strings approximately 40–60% across all models. Estimated model probabilities for correct answers are lower in incongruent conditions, even when model answers are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans: high accuracy in incongruent conditions reported ~93–96% (paper cites ~95% from Yantis & Johnston 1990); reaction times ~300–500 ms with slower RTs for incongruent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs match human-like performance on congruent trials but perform substantially worse than humans on incongruent trials, suggesting poor inhibition/attentional control in this paradigm.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>32 flanker stimuli with string lengths 5–11; instructions included congruent and incongruent examples. Responses constrained to one of two letters ('A' or 'L'). Prompt permutations (30 variants) applied; both accuracy and token-probability measurements collected. Reaction times not available for LLMs (only accuracy and probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Absence of timing/RT measures for models limits direct comparison to human RT-based effects. The text-only design may change task affordances (e.g., concept of 'center' in string). Prompt sensitivity observed; probability and generation metrics can diverge. Authors note ambiguity whether failures reflect inability to inhibit flankers or failure to represent 'center'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Framework for Robust Cognitive Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9025.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9025.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ForwardDigitSpan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward Digit Span Task (short-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence repetition task probing short-term memory: models must repeat a presented digit sequence in the same order (forward); assesses capacity of short-term storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Framework for Robust Cognitive Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLMs evaluated for content accuracy and perplexity on target sequences; token probabilities used when targets are multi-token via perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B, 27B, 8B, 70B, 7B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Forward Digit Span</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants are presented with a sequence of digits and must repeat them in order; measures short-term memory span (storage without manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Near-perfect content accuracy across tested lengths including super-human lengths: for lengths 7, 20, 30, 50 mean accuracies across prompts largely ≈100% (Table 4 shows nearly 1.00 for all models and lengths; a few 0.98–0.99 entries).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Normative human forward digit span mean ≈7 digits (sources cited). The authors estimate accuracy for a forward span of length 7 at ~50% (based on population variability and normative means).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs greatly outperform typical human short-term memory on forward span tasks (LLMs demonstrate 'over 50 digits' forward span in these tests), though the authors caution this comparison is imperfect due to architectural differences.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>70 digit-span stimuli with randomly generated digit lists of lengths 7, 15, 20, 30, 50; content-accuracy scoring (order-only, ignoring formatting). Prompt permutations (30 variants) used; both response generations and perplexity measures collected. No Chain-of-Thought prompting used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Direct comparison to human memory is imperfect: LLM storage and retrieval mechanisms differ and do not map cleanly to human encoding/decay; prompts/formatting may advantage LLMs; quantization and tokenization could affect extremely long sequences. Authors explicitly note the human-LLM memory comparison is flawed and should be interpreted cautiously.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Framework for Robust Cognitive Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9025.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9025.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BackwardDigitSpan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Backward Digit Span Task (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequence manipulation task probing working memory: models must repeat digits in reverse order, requiring an online transformation (reversal) and thus a test of working memory rather than simple storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Framework for Robust Cognitive Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLMs; evaluation used content-accuracy and perplexity across multiple prompt permutations; larger models quantized for inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B, 27B, 8B, 70B, 7B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Backward Digit Span</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Participants are presented with a digit sequence and must repeat it in reverse order; assesses working memory (storage plus manipulation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLMs show lower accuracy than forward task. Smaller models drop accuracy after lengths ~15–20 digits. Reported LLM backward spans are roughly in the range 11–20 digits (varies by model); larger models generally perform better (e.g., Llama3.1-70B shows notably high backward-digit performance). Example nuance: Qwen2-7B shows equal accuracy on some longer lengths but higher perplexities indicate degraded understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Normative human backward digit span mean ≈5 digits (sources cited); backward span is substantially lower than forward in humans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs outperform humans on absolute working memory span (LLM backward spans >> human mean of 5), but LLMs show a larger relative drop from forward→backward span compared to humans (LLMs: >50% decrease in some models). Larger models tend to have longer backward spans than smaller variants, but no model is uniformly best across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same stimulus set as forward digit span (70 sequences, multiple lengths). Responses evaluated for content accuracy; token-probability (perplexity) reported for multi-token targets. Prompt perturbations (30 variants) used; no CoT prompting. Authors provide plots of mean accuracy by sequence length and model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution the working-memory construct may not map cleanly onto LLM internal processes. Variation across prompt formats and measurement metric (accuracy vs. perplexity) can change apparent performance. The relative difference between forward and backward spans in LLMs is larger than in humans, but interpretation is limited by differences in architecture and task modality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Framework for Robust Cognitive Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9025.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9025.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deese-Roediger-McDermott (DRM) False/Gist Memory Task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classic false-memory paradigm: subjects study themed word lists that omit a highly associated critical word, then are tested on recognition/recollection of studied vs. critical (unseen) words to measure susceptibility to gist-based false memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Framework for Robust Cognitive Evaluation of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLMs tested in dialog format with full presentation of 12 DRM lists followed by 168 recognition queries; both generation labels and probability estimates recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B, 27B, 8B, 70B, 7B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>DRM (Deese-Roediger-McDermott) false memory task</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Models are exposed to multiple themed word lists that omit an associated critical word; later tested on whether words were present (seen), unseen critical words, or unseen control words to assess susceptibility to false/gist memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Larger models show nearly perfect performance at rejecting unseen critical words (i.e., low false alarm rates); smaller models have decreased accuracy on critical words. Quantitative table not reproduced in full in text, but qualitative statement: larger LLMs nearly perfect, smaller models worse.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants in cited work (Pardilla-Delgado & Payne 2017) recognize unseen critical words ≈70% (i.e., high false alarm; correspondingly ≈30% correct rejection) — paper notes variability in reported baselines (appendix mentions a 63% figure for a related recognition metric).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs (especially larger models) are less susceptible to human-like false-memory errors in this DRM paradigm and outperform human baselines on correct rejection of critical words (i.e., LLMs show stronger, more reliable encoding than human short-term memory in this task).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>First prompt contained all 12 DRM lists; following 168 messages tested presence/absence of words. Models replied with 'Z' for seen and 'M' for not seen. Word selection: seven 'seen' words per list, 18 unseen critical words, 18 unseen control words. Presentation order randomized. Prompt permutations (30 variants) used; both generations and probabilities measured.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution that comparisons to humans are flawed for memory tasks because LLMs do not have degradable short-term encoding similar to humans; near-perfect LLM performance may reflect deterministic storage and retrieval differences rather than human-like memory processes. Appendix contains some inconsistent citations/estimates for human baseline rates; results stable across prompt permutations for this task (less prompt sensitivity than other tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Framework for Robust Cognitive Evaluation of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CogBench: a large language model walks into a psychology lab <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Large language models fail on trivial alterations to theory-of-mind tasks <em>(Rating: 2)</em></li>
                <li>Prompting is not a substitute for probability measurements in large language models <em>(Rating: 2)</em></li>
                <li>Log probabilities are a reliable estimate of semantic plausibility in base and instruction-tuned language models <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9025",
    "paper_id": "paper-277510184",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "WCST",
            "name_full": "Wisconsin Card Sorting Task (textual adaptation)",
            "brief_description": "Text-only adaptation of the Wisconsin Card Sorting Task to measure cognitive flexibility/executive function in LLMs; models must infer a hidden sorting rule from feedback and adapt when the rule changes.",
            "citation_title": "A Framework for Robust Cognitive Evaluation of LLMs",
            "mention_or_use": "use",
            "model_name": "Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)",
            "model_description": "Instruction-tuned open-source transformer LLMs (Gemma2, Llama3.1, Qwen2) evaluated via Huggingface; larger models quantized to 4-bit for inference; logits/probabilities captured for targets.",
            "model_size": "9B, 27B, 8B, 70B, 7B, 72B",
            "test_battery_name": "Wisconsin Card Sorting Task (WCST)",
            "test_description": "A sequential feedback-driven classification task where participants infer and adapt to an implicit sorting rule (shape, color, count); measures cognitive flexibility and perseveration (preservation) errors.",
            "llm_performance": "Mean correct answer rates across all prompts/trials (from Table 1): Gemma2-9B 27%, Gemma2-27B 43%, Llama3.1-8B 43%, Llama3.1-70B 39%, Qwen2-7B 44%, Qwen2-72B 40%. Preservation (perseveration) error rates substantial (e.g., 36% for Gemma2-9B, 50% for Qwen2-7B). Accuracy declines with dialogue length (negative Pearson r per model).",
            "human_baseline_performance": "Typical adult accuracy ~70–80% (paper estimates baseline ≈77% from normative studies). Humans take ~2 trials to infer a new rule and respond ~1s under speeded instructions.",
            "performance_comparison": "LLMs are well below human baseline on WCST (substantially lower accuracy, high preservation errors); LLM accuracy declines over the dialogue whereas humans improve after rule inference.",
            "experimental_details": "WCST translated into text and run as a chat dialogue of 102 stimuli; initial message included instructions + three examples; each rule presented 10–15 consecutive trials and rules repeated twice (order provided). Models responded with an integer option; response parsing extracted final digit. Prompt perturbations: 30 prompt variants (3 paraphrases × 10 data formats) were used; both model generations and internal probabilities recorded. No Chain-of-Thought elicitation; sequential dialogue used to emulate feedback.",
            "limitations_or_caveats": "Task is a textual translation of a visual task — modality differences may affect performance. LLM architecture differs from human cognition so direct analogy is limited. Results sensitive to prompt wording (prompt perturbations change accuracies and can reverse model comparisons). No human reaction-time data from the same experimental setup; authors note human-LLM comparison is imperfect for some constructs. Models were quantized for larger sizes (4-bit) which could affect performance.",
            "uuid": "e9025.0",
            "source_info": {
                "paper_title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Flanker",
            "name_full": "Eriksen Flanker Task (string-letter variant)",
            "brief_description": "Textual flanker task where model must respond to the center character mapping while ignoring flankers; measures attentional control and response inhibition (congruent vs. incongruent conditions).",
            "citation_title": "A Framework for Robust Cognitive Evaluation of LLMs",
            "mention_or_use": "use",
            "model_name": "Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)",
            "model_description": "Instruction-tuned open-source transformer LLMs evaluated with both generation accuracy and token probability measures; inference via Huggingface; prompt permutations applied.",
            "model_size": "9B, 27B, 8B, 70B, 7B, 72B",
            "test_battery_name": "Eriksen Flanker Task",
            "test_description": "Participants must respond to the central stimulus (mapped to a response key) while ignoring flanking stimuli; congruent trials (flankers support same response) vs. incongruent trials (flankers cue a different response) measure inhibitory control and attentional conflict.",
            "llm_performance": "Across 32 stimuli × 30 prompt variations (n=960): near-perfect accuracy on congruent strings (≈~100%); accuracy in incongruent strings approximately 40–60% across all models. Estimated model probabilities for correct answers are lower in incongruent conditions, even when model answers are correct.",
            "human_baseline_performance": "Humans: high accuracy in incongruent conditions reported ~93–96% (paper cites ~95% from Yantis & Johnston 1990); reaction times ~300–500 ms with slower RTs for incongruent trials.",
            "performance_comparison": "LLMs match human-like performance on congruent trials but perform substantially worse than humans on incongruent trials, suggesting poor inhibition/attentional control in this paradigm.",
            "experimental_details": "32 flanker stimuli with string lengths 5–11; instructions included congruent and incongruent examples. Responses constrained to one of two letters ('A' or 'L'). Prompt permutations (30 variants) applied; both accuracy and token-probability measurements collected. Reaction times not available for LLMs (only accuracy and probabilities).",
            "limitations_or_caveats": "Absence of timing/RT measures for models limits direct comparison to human RT-based effects. The text-only design may change task affordances (e.g., concept of 'center' in string). Prompt sensitivity observed; probability and generation metrics can diverge. Authors note ambiguity whether failures reflect inability to inhibit flankers or failure to represent 'center'.",
            "uuid": "e9025.1",
            "source_info": {
                "paper_title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ForwardDigitSpan",
            "name_full": "Forward Digit Span Task (short-term memory)",
            "brief_description": "Sequence repetition task probing short-term memory: models must repeat a presented digit sequence in the same order (forward); assesses capacity of short-term storage.",
            "citation_title": "A Framework for Robust Cognitive Evaluation of LLMs",
            "mention_or_use": "use",
            "model_name": "Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)",
            "model_description": "Instruction-tuned transformer LLMs evaluated for content accuracy and perplexity on target sequences; token probabilities used when targets are multi-token via perplexity.",
            "model_size": "9B, 27B, 8B, 70B, 7B, 72B",
            "test_battery_name": "Forward Digit Span",
            "test_description": "Participants are presented with a sequence of digits and must repeat them in order; measures short-term memory span (storage without manipulation).",
            "llm_performance": "Near-perfect content accuracy across tested lengths including super-human lengths: for lengths 7, 20, 30, 50 mean accuracies across prompts largely ≈100% (Table 4 shows nearly 1.00 for all models and lengths; a few 0.98–0.99 entries).",
            "human_baseline_performance": "Normative human forward digit span mean ≈7 digits (sources cited). The authors estimate accuracy for a forward span of length 7 at ~50% (based on population variability and normative means).",
            "performance_comparison": "LLMs greatly outperform typical human short-term memory on forward span tasks (LLMs demonstrate 'over 50 digits' forward span in these tests), though the authors caution this comparison is imperfect due to architectural differences.",
            "experimental_details": "70 digit-span stimuli with randomly generated digit lists of lengths 7, 15, 20, 30, 50; content-accuracy scoring (order-only, ignoring formatting). Prompt permutations (30 variants) used; both response generations and perplexity measures collected. No Chain-of-Thought prompting used.",
            "limitations_or_caveats": "Direct comparison to human memory is imperfect: LLM storage and retrieval mechanisms differ and do not map cleanly to human encoding/decay; prompts/formatting may advantage LLMs; quantization and tokenization could affect extremely long sequences. Authors explicitly note the human-LLM memory comparison is flawed and should be interpreted cautiously.",
            "uuid": "e9025.2",
            "source_info": {
                "paper_title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "BackwardDigitSpan",
            "name_full": "Backward Digit Span Task (working memory)",
            "brief_description": "Sequence manipulation task probing working memory: models must repeat digits in reverse order, requiring an online transformation (reversal) and thus a test of working memory rather than simple storage.",
            "citation_title": "A Framework for Robust Cognitive Evaluation of LLMs",
            "mention_or_use": "use",
            "model_name": "Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)",
            "model_description": "Instruction-tuned transformer LLMs; evaluation used content-accuracy and perplexity across multiple prompt permutations; larger models quantized for inference.",
            "model_size": "9B, 27B, 8B, 70B, 7B, 72B",
            "test_battery_name": "Backward Digit Span",
            "test_description": "Participants are presented with a digit sequence and must repeat it in reverse order; assesses working memory (storage plus manipulation).",
            "llm_performance": "LLMs show lower accuracy than forward task. Smaller models drop accuracy after lengths ~15–20 digits. Reported LLM backward spans are roughly in the range 11–20 digits (varies by model); larger models generally perform better (e.g., Llama3.1-70B shows notably high backward-digit performance). Example nuance: Qwen2-7B shows equal accuracy on some longer lengths but higher perplexities indicate degraded understanding.",
            "human_baseline_performance": "Normative human backward digit span mean ≈5 digits (sources cited); backward span is substantially lower than forward in humans.",
            "performance_comparison": "LLMs outperform humans on absolute working memory span (LLM backward spans &gt;&gt; human mean of 5), but LLMs show a larger relative drop from forward→backward span compared to humans (LLMs: &gt;50% decrease in some models). Larger models tend to have longer backward spans than smaller variants, but no model is uniformly best across tasks.",
            "experimental_details": "Same stimulus set as forward digit span (70 sequences, multiple lengths). Responses evaluated for content accuracy; token-probability (perplexity) reported for multi-token targets. Prompt perturbations (30 variants) used; no CoT prompting. Authors provide plots of mean accuracy by sequence length and model.",
            "limitations_or_caveats": "Authors caution the working-memory construct may not map cleanly onto LLM internal processes. Variation across prompt formats and measurement metric (accuracy vs. perplexity) can change apparent performance. The relative difference between forward and backward spans in LLMs is larger than in humans, but interpretation is limited by differences in architecture and task modality.",
            "uuid": "e9025.3",
            "source_info": {
                "paper_title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DRM",
            "name_full": "Deese-Roediger-McDermott (DRM) False/Gist Memory Task",
            "brief_description": "Classic false-memory paradigm: subjects study themed word lists that omit a highly associated critical word, then are tested on recognition/recollection of studied vs. critical (unseen) words to measure susceptibility to gist-based false memory.",
            "citation_title": "A Framework for Robust Cognitive Evaluation of LLMs",
            "mention_or_use": "use",
            "model_name": "Gemma2 (9B, 27B); Llama3.1 (8B, 70B); Qwen2 (7B, 72B)",
            "model_description": "Instruction-tuned transformer LLMs tested in dialog format with full presentation of 12 DRM lists followed by 168 recognition queries; both generation labels and probability estimates recorded.",
            "model_size": "9B, 27B, 8B, 70B, 7B, 72B",
            "test_battery_name": "DRM (Deese-Roediger-McDermott) false memory task",
            "test_description": "Models are exposed to multiple themed word lists that omit an associated critical word; later tested on whether words were present (seen), unseen critical words, or unseen control words to assess susceptibility to false/gist memory.",
            "llm_performance": "Larger models show nearly perfect performance at rejecting unseen critical words (i.e., low false alarm rates); smaller models have decreased accuracy on critical words. Quantitative table not reproduced in full in text, but qualitative statement: larger LLMs nearly perfect, smaller models worse.",
            "human_baseline_performance": "Human participants in cited work (Pardilla-Delgado & Payne 2017) recognize unseen critical words ≈70% (i.e., high false alarm; correspondingly ≈30% correct rejection) — paper notes variability in reported baselines (appendix mentions a 63% figure for a related recognition metric).",
            "performance_comparison": "LLMs (especially larger models) are less susceptible to human-like false-memory errors in this DRM paradigm and outperform human baselines on correct rejection of critical words (i.e., LLMs show stronger, more reliable encoding than human short-term memory in this task).",
            "experimental_details": "First prompt contained all 12 DRM lists; following 168 messages tested presence/absence of words. Models replied with 'Z' for seen and 'M' for not seen. Word selection: seven 'seen' words per list, 18 unseen critical words, 18 unseen control words. Presentation order randomized. Prompt permutations (30 variants) used; both generations and probabilities measured.",
            "limitations_or_caveats": "Authors caution that comparisons to humans are flawed for memory tasks because LLMs do not have degradable short-term encoding similar to humans; near-perfect LLM performance may reflect deterministic storage and retrieval differences rather than human-like memory processes. Appendix contains some inconsistent citations/estimates for human baseline rates; results stable across prompt permutations for this task (less prompt sensitivity than other tasks).",
            "uuid": "e9025.4",
            "source_info": {
                "paper_title": "A Framework for Robust Cognitive Evaluation of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CogBench: a large language model walks into a psychology lab",
            "rating": 2,
            "sanitized_title": "cogbench_a_large_language_model_walks_into_a_psychology_lab"
        },
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "rating": 2,
            "sanitized_title": "large_language_models_fail_on_trivial_alterations_to_theoryofmind_tasks"
        },
        {
            "paper_title": "Prompting is not a substitute for probability measurements in large language models",
            "rating": 2,
            "sanitized_title": "prompting_is_not_a_substitute_for_probability_measurements_in_large_language_models"
        },
        {
            "paper_title": "Log probabilities are a reliable estimate of semantic plausibility in base and instruction-tuned language models",
            "rating": 1,
            "sanitized_title": "log_probabilities_are_a_reliable_estimate_of_semantic_plausibility_in_base_and_instructiontuned_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.015294249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Framework for Robust Cognitive Evaluation of LLMs
3 Apr 2025</p>
<p>Karin De Langis 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>Jong Inn Park 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>Bin Hu 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>Khanh Chi Le 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>Andreas Schramm a.schramm@hamline.edu 
Department of Linguistics
Hamline University</p>
<p>Michael C Mensink mensinkm@uwstout.edu 
Department of Psychology
University of Wisconsin-Stout</p>
<p>Andrew Elfenbein 
Department of English
University of Minnesota</p>
<p>Dongyeop Kang dongyeop@umn.edu 
Department of Computer Science and Engineering
University of Minnesota</p>
<p>A Framework for Robust Cognitive Evaluation of LLMs
3 Apr 2025E4B52B56727BF426F99123FD9543B7F9arXiv:2504.02789v1[cs.CL]
Emergent cognitive abilities in large language models (LLMs) have been widely observed, but their nature and underlying mechanisms remain poorly understood.A growing body of research draws on cognitive science to investigate LLM cognition, but standard methodologies and experimental pipelines have not yet been established.To address this gap we develop COGNITIVEVAL, a framework for systematically evaluating the artificial cognitive capabilities of LLMs, with a particular emphasis on robustness in response collection.The key features of COGNITIVEVAL include: (i) automatic prompt permutations, and (ii) testing that gathers both generations and model probability estimates.Our experiments demonstrate that these features lead to more robust experimental outcomes.Using COGNITIVE-VAL, we replicate five classic experiments in cognitive science, illustrating the framework's generalizability across various experimental tasks and obtaining a cognitive profile of several state-of-the-art LLMs.COGNITIVE-VAL will be released publicly to foster broader collaboration within the cognitive science community.</p>
<p>Introduction</p>
<p>As large language models (LLMs) become increasingly advanced, a growing body of research applies methods from cognitive science to better understand both the nature and extent of LLM cognition.This approach is motivated by the existence of cleverly designed experimental paradigms and datasets in cognitive science: these experiments seek to understand the processes of the mind, which is not directly observable, and may therefore also be useful for understanding aspects of the LLM "black box."However, applying these experiments to LLMs presents unique challenges: in particular, careful consideration must be made to how tasks are adapted, how performance is measured, how results are interpreted, and how comparisons to human data are made (Ivanova, 2025;Ying et al., 2025).</p>
<p>Cognitive science experiments have promise in probing LLMs because they have been used to identify various components of human cognition and assess their structure and functions, making them well-suited for bootstrapping our understanding of cognition in LLMs.Cognitive science has historically pursued converging evidence, or findings from multiple independent experimental paradigms that converge to the same conclusion, in order to develop theories of cognition (Friedenberg et al., 2021).Since cognitive processes cannot be directly observed, each experiment rests on assumptions about how task performance reflects internal processes.An individual experimental result is therefore relatively weak evidence, as it may be an artifact of the experimental task, materials, or assumptions.On the other hand, consistent patterns across diverse paradigms including different experimental tasks and measures, are more compelling.We maintain that the principle of converging evidence is equally important when assessing cognition in LLMs.To support this pursuit, Figure 1: Conceptual overview of the COGNITIVEVAL pipeline.Two key phases encourage robustness in cognitive assessment of LLMs: first, prompt paraphrasing creates a variety of prompt formatting and wording, to mitigate any prompt-specific effects.Second, capturing both LLM responses and the LLM's internal probability estimates of the target answer allows for a more nuanced understanding of LLM confidence.</p>
<p>we suggest the development of a generalizable, modular experimental framework that can facilitate rapid development and robust cognitive evaluation of LLMs.</p>
<p>In this work, we introduce COGNITIVEVAL, a unified experimental pipeline designed to apply cognitive science toward the evaluation of LLMs (see Figure 1).COGNITIVEVAL enables robust assessments by incorporating two key components: (1) automatic prompt paraphrasing to generate diverse prompt variants, minimizing prompt-specific artifacts; and</p>
<p>(2) dual measurement of both model outputs and internal probability estimates, enabling more nuanced analysis of model performance and certainty.In addition, COGNITIVE-VAL supports the definition of experimental conditions and variables within stimuli and automatic response parsing, making it broadly applicable across various experiment setups.</p>
<p>While prior work applies cognitive science experiments to LLMs, and proposed best practices, there is no standardized approach or shared infrastructure.COGNITIVEVAL is designed to fill this gap.We demonstrate the utility of COGNITIVEVAL by adapting five classic cognitive science experiments to probe memory and executive function in LLMs.Our findings suggest that while LLMs exhibit strong short-term memory, their working memory -i.e., ability to manipulate information in memory -is notably weaker.LLMs also show weak executive function, demonstrating low cognitive flexibility and weak inhibition in the experiments.Importantly, our results highlight the models' sensitivity to prompt perturbations across these tasks, and we demonstrate the unique insights that can be gained from considering both direct responses and model probabilities.(Jones &amp; Steinhardt, 2022;Lal et al., 2024;Lv et al., 2024) for evaluation.For example, Lee &amp; Lim (2024) design linguistic tasks that rely on visual perception of the orthography to complete, demonstrating how LLMs' lack of visual language perception affects their understanding, and Joshi et al. (2024) evaluate LLM reasoning patterns by introducing conflicting facts in prompts.</p>
<p>2024) or creating new datasets</p>
<p>Metrics.</p>
<p>Metrics in these studies typically vary between explicit output, i.e., the model generations (Dasgupta et al., 2022;Hagendorff et al., 2023), and implicit model signals, such as estimated token probabilities (Lee et al., 2024;Ullman, 2023) and attention scores (Bazhukov et al., 2024).Many works choose to measure either the explicit output or the implicit signal.However, evidence suggests that model generations and probability measurements can be divergent (Hu &amp; Levy, 2023;Kauf et al., 2024;Mahaut et al., 2024).Our pipeline obtains both results.Another important consideration in applying human experiments to LLMs is controlling for effects of prompt wording and structure.It is well-established that LLM responses can vary substantially based on prompt wording and formatting (Sclar et al., 2024;Wahle et al., 2024), and while some prior works have applied prompt perturbation to cognitive assessments (Coda-Forno et al.;Bazhukov et al., 2024), many do not.Our proposed COGNITIVEVAL pipeline includes automatic prompt perturbation to assist practitioners in controlling for these effects.</p>
<p>COGNITIVEVAL</p>
<p>The goal of COGNITIVEVAL is to offer a flexible and robust framework for adapting a wide range of cognitive experimental studies, ultimately helping researchers gather converging evidence to support their conclusions in cognitive evaluation of LLMs.Guided by these design principles, we outline the core components of COGNITIVEVAL.Details on how COGNITIVEVAL supports flexible experiment setups can be found in §A.1.</p>
<p>Prompt Permutations.</p>
<p>It is more reliable to marginalize over all prompts for a given task, rather than relying on one prompt to evaluate performance (Sclar et al., 2024;Wahle et al., 2024).COGNITIVEVAL provides automatic prompt permutations to assist users in diversifying their prompts.COGNITIVEVAL prompts have two components which are acted on separately:</p>
<p>• Instructions, which explain the task, e.g.: Read the following story and phrase, and determine if the phrase is true based on the story.</p>
<p>• Data format, which describes generally how the individual stimuli should be presented to the model, e.g.: Story: { }, Phrase: { }.Sclar et al. (2024) show that formatting templates in prompts affect model outputs; we use their proposed FORMATSPREAD approach to develop ten different data format templates that vary across punctuation, whitespace, and letter casing.The automatic prompt paraphrasing pipeline acts separately on both components: the general instructions are paraphrased by GPT-4o (Achiam et al., 2023), and the data format is diversified across punctuation and whitespace according to FORMATSPREAD.We generate three distinct paraphrases combined with 10 data formats, yielding 30 prompt variations (see §A.2 for more details).</p>
<p>Experiment Dialogues.COGNITIVEVAL supports interactive dialogue experiments, or presenting each stimulus in a separate dialogue.In many cases separate dialogues are preferred to order to avoid order effects.However, in some experimental settings, it is desirable to present stimulus sequentially instead -particularly if the experiment includes stimuli and tasks (either related or distractor) that are expected to influence subsequent responses.For example, in our WCST, the LLM must play a game and respond to feedback.</p>
<p>In such cases, a chat dialogue is conducive to replicating the experiment in LLMs.</p>
<p>Metric Collection.</p>
<p>COGNITIVEVAL collects two measures: (1) response accuracy, by comparing model output with a target output, and (2) model probabilities of a target output.When computing probabilities, in the event that the target output is a single token long (e.g."A," or "False") this is obtained by inputting the prompt to the model and taking the softmax of the logits from the language modeling head.In the event that the target output is several tokens long, we compute the perplexity of the target answer.</p>
<p>Models and Inference.</p>
<p>We use six open-source LLMs from three different model families, each with two size variations: Gemma2 with 9B and 27B parameters (Riviere et al., 2024), Llama3.1 with 8B and 70B parameters (Dubey et al., 2024), and Qwen2 with 7B and 72B parameters (Yang et al., 2024).All models are the instruction-tuned variants.Our pipeline also enables the use of proprietary models such as GPT-4o (Achiam et al., 2023) and reasoningoriented models like DeepSeek-R1 (DeepSeek-AI et al., 2025); however, we exclude them from our experiments to ensure a fair comparison, as our evaluation requires access to model logits at specific points in their generated responses.We use the Huggingface library (Wolf et al., 2020) for model inference, applying 4-bit quantization to meet computational constraints when working with larger models (those with at least 27B parameters).For experiments with long contexts (i.e., those with long dialogues) we also use Flash Attention (Dao et al., 2022) and dynamic key value caching.Otherwise, default model configurations and generation hyperparameters are used in all cases.All experiments are completed using two Nvidia A100 GPUs.Generations are parsed based on the format specified in the prompt, such that answers can be automatically extracted for analysis.</p>
<p>Experiments</p>
<p>We use COGNITIVEVAL to adapt five classic cognitive experimental tasks for LLMs.The tasks are chosen to balance variety and depth: we explore tasks related to two broad types of cognition, and within those types, select different domains and experimental procedures (see Figure 2).In this way we can demonstrate the versatility of our proposed pipeline while also working toward converging evidence for LLM executive function and memory.Executive function is a broad set of cognitive functions, including attention regulation and task-switching.Executive function is understood to enable goal-directed behavior, making it an interesting area of study in LLMs.Memory processes have to do with either the encoding, storage, or retrieval of information and past experiences.There are clear differences between human and LLM memory, making this another fitting area of study.</p>
<p>Note on Prompting Strategy.All experiments selected actively prevent high-level, conscious thinking from human participants by requiring fast reactions and limiting display  times of stimuli.For better parity with human studies, we therefore do not test reasoning models in this work, and we do not elicit Chain-of-Thought style responses from models.Prompting details can be found in §A.2</p>
<p>[Executive Function] (Cognitive Flexibility) WCST</p>
<p>The Wisconsin Card Sorting Task (WCST) requires participants to infer an implicit sorting rule based on feedback: participants propose a sorting action and are told whether the action is correct (Grant &amp; Berg, 1948).Critically, the implicit sorting rule changes several times during the task, testing participants' cognitive flexibility (Miles et al., 2021).</p>
<p>Input.We translate the WCST to a textual format1 for LLMs (see Figure 3a).102 stimuli are presented serially to the LLM in a chat dialogue, with the initial message containing the instructions and three examples demonstrating each possible sorting rule.The instruction state that the sorting rule will change throughout the experiment, and that the current rule must be inferred based on feedback.Subsequent dialogue turns contain feedback on whether the LLM's previous answer was correct, the sorting options, and a new item to sort.The experiment is conducted such that each rule is presented for 10-15 consecutive times.Sorting rules are presented twice; the order is count, shape, color, count, color, shape.</p>
<p>Output and evaluation.The LLM is instructed to respond with the integer corresponding to the chosen sorting option.The LLM answer is parsed for the last digit, and this is compared with the correct answer for the item in evaluation.Aside from accuracy, a key metric in the WCST is preservation errors, or whether participants mistakenly respond with the previous rule versus the current rule.</p>
<p>Human baseline.Normative studies have found that typical adults take on average two trials to infer the new rule and about 1 second to respond (instructions ask people to prioritize speed in their answers), and accuracy rates have been found to be between 70-80% (Barcel ó et al., 1997;Grant &amp; Berg, 1948;Milner, 1963).Preservation errors decline significantly after participants infer the new rule.</p>
<p>Results.</p>
<p>Results are summarized in Figure 3 and Table 1.All models tested have substantially lower accuracy than humans.Unlike humans, models do not appear to do very well adapting to changing sorting rules: we find no correlation between model accuracy and number of turns exposed to a given rule. 2 We also see that preservation errors do not decrease with increased exposure to the new rule.Moreover, we find that models are unable to adapt to the feedback to infer a rule: there is no positive relationship between number  of turns exposed to a rule and accuracy.Over the course of the entire task, accuracy goes down across all models, as evidenced by significant negative correlations between dialogue length and accuracy across all models.Details results are in §A.4.2.</p>
<p>[Executive Function] (Attentional Control) Flanker Task</p>
<p>The Eriksen flanker task (Eriksen &amp; Eriksen, 1974) requires participants to respond differentially to two sets of stimuli.For example:
• If you see letters 'X' or 'C' −→ Press ('A') • If you see letters 'B' or 'V' −→ Press ('L')
Critically, participants are shown a sequence of stimuli, but must only respond to the one in the center (and ignore the "flankers").The are two types of strings participants are shown:</p>
<p>• Congruent strings, e.g., 'XXCXX'.Both 'X' and 'C' map to the 'A' response.</p>
<p>• Incongruent strings, e.g., 'BBCBB'.The center letter 'C' maps to the 'R' response, but the flanking letters map to the 'L' response.</p>
<p>Participants must respond as quickly as possible.In the incongruent conditions, participants have slower response times.This is because participants must inhibit the response for the flanking letters, and inhibition requires additional cognitive resources, slowing the reaction.</p>
<p>Input.We create 32 Flanker stimuli, varying the string length between five and eleven.We include both a congruent and an incongruent example in the instructions.</p>
<p>Output and evaluation.</p>
<p>The model responds with one of the designated letters ('A' or 'L').</p>
<p>Human baselines.Human reaction times are typically around 300-500 milliseconds (humans are asked to respond as quickly as possible).Reports of mean accuracy in the incongruent conditions are around 93% to 96%, e.g., see Eriksen &amp; Eriksen (1974); Yantis &amp; Johnston (1990), although human studies focus most of their analyses on reaction time.</p>
<p>Results.</p>
<p>Our results are summarized in Figure 4. Model performance is near perfect for the congruent letter strings, but has accuracy of around 40-60% for the incongruent letter strings across all models tested, substantially below human baselines.The estimated Larger models do better at the backward digit task than their smaller counterparts.</p>
<p>(b) Perplexity of the correct sequences in the backward digit span task yields more nuance, e.g., note that Qwen2-7B accuracy is equal for lengths 15, 20, and 30, the perplexities indicate that its understanding of 30-length digits is worse.</p>
<p>Figure 5: Digit Span task: Larger models have longer backward digit spans than their smaller counterparts, but overall performance is worse relative to the forward task.</p>
<p>probability of the correct answer is lower in the incongruent condition, even when only considering cases in which the model's answer is correct.While LLM cognitive architecture is not analogous to humans', making it difficult to immediately draw general conclusions, these results do suggest that LLMs may have difficulty inhibiting certain responses and intentionally ignoring certain inputs.It is also possible that LLMs have difficulty with the concept of the center; future work is required to disentangle these potential causes.</p>
<p>[Executive Function &amp; Memory] (Working &amp; Short-term Memory) Digit Span Tasks</p>
<p>The forward and backward digit span tasks probe short term memory and working memory, respectively.Participants are briefly presented with a series of digits and must repeat them, either in the same order (forward digit span task) or in the reverse order (backward digit span task).The backward digit span task involves working memory rather than short term memory because rather than rote repetition, it requires an operation (reversal) to be performed on the information held in memory.</p>
<p>Input.We create 70 digit span stimuli, consisting of randomly generated lists of digits 0-9.Because we hypothesize that LLMs will have near-perfect digit memory, we include digit lists of length 7 (matching human performance) and also digit lists with super-human lengths of 15, 20, 30, and 50.The prompt includes two examples of the task.</p>
<p>Output and evaluation.The model responds with a list of digits.We compare accuracy across the different lengths to estimate LLM forward and backward digit span.We consider content accuracy, in which only the order of the digits presented is evaluated (e.g. if commas are omitted but digits are presented in the correct order, the response is considered correct).</p>
<p>Human baseline.</p>
<p>Normative studies find a mean forward digit span of seven, and a mean backward digit span of five (Banken, 1985;Monaco et al., 2013).</p>
<p>Results.In the forward digit span task, all LLMs tested have nearly perfect responses for all digit lengths (see Table 4).These results indicate that the forward digit span of even smaller LLMs is over 50 digits long.In the backward digit span task, smaller models have decreased accuracy after 15-20 digits (Figure 5a).Like humans, LLMs find the reversal operation makes this task more difficult.Unlike humans, where the difficulty results in a relatively modest difference (backwards digit span of five is about a 30% decrease from the forward span of seven), most LLMs tested have a forward digit span over 50 and a backward digit span of 11-20, a comparably substantial decrease of over 50%.</p>
<p>[Memory] (False/Gist Memory) DRM Task</p>
<p>The Deese-Roediger-McDermott (DRM) task is designed to induce false recall of words from studied word lists (Deese, 1959;Roediger &amp; McDermott, 1995).It begins with participants studying several different lists.Each list has a semantic theme, but omits a critical word with high semantic relation to all other items on the list.For example, the list "Rest, Peace, Doze, Slumber, Wake, Bed, Nap, Tired, Yawn, Dream, Drowsy, Blankey, Awake, Snore, Snooze" omits the critical word sleep.After studying, people have a tendency to later falsely recall the presence of the critical words.This effect has been attributed to gist memory in the literature: i.e., the list's gist is encoded, resulting in a false recall of the critical word.</p>
<p>Input.The first message in the dialogue consists of 12 word lists and instructions.The following 168 messages quiz the LLM on whether a specific word was present or absent in the original message.We present stimuli in dialogue form so that we can investigate whether performance deteriorates over the course of the dialogue.</p>
<p>Human baseline.We use the word lists from Pardilla-Delgado &amp; Payne (2017), who find humans recognize unseen critical words 70% of the time (30% accuracy).</p>
<p>Output and evaluation.When presented with a word recall task, the LLM outputs the letter 'Z' to indicate that the presented word was on one of the studied lists, and the letter 'M' to indicate that the word was not on any of the lists.Table 2: DRM accuracies across different conditions.Note the difference in accuracy between the unseen control and unseen critical words, indicating susceptibility to false memory.</p>
<p>Results.</p>
<p>Although smaller LLMs have decreased accuracy in their responses to the critical words, larger LLMs have nearly perfect performance (Table 2).The responses indicate that larger LLMs are not susceptible to human-like false reports triggered by the semantic interference of the critical word.We hypothesize that this is likely because LLM information encoding is much more reliable than human short-term memory.Unlike in the WCST, we find no significant negative correlation between model accuracy and dialogue turn.</p>
<p>Assessing Experimental Robustness in COGNITIVEVAL</p>
<p>The previous section demonstrates that our framework can be flexibly applied to a variety of cognitive experiments.Building on these results, we next consider the effects of two key features of our evaluation pipeline: prompt perturbation and metric collection.The benefit of collecting both generation accuracy and probability can be seen throughout our experiments, as the two metrics provide unique insights as shown in Figures 5b and 4b.</p>
<p>To evaluate the efficacy of our prompt perturbations in gathering robust model responses, we find the range of accuracies obtained under each prompt variations for each task (Figure 6); with the exception of the digit span task, in which models have near perfect performance, we find a range of accuracies across prompts.We also find that comparisons of model cognitive ability can be reversed based solely on the prompt variation presented to the model, replicating findings from other domains (Sclar et al., 2024;Wahle et al., 2024).For all model pairs (M, M ′ ) with accuracies that differ by at least d under prompt variation p, we consider how often the "better model" reverses under a different prompt variation, p ′ (where the model must again be better by at least d). Figure 7a shows that model comparisons are regularly reversed under different prompt variations, e.g., for accuracy threshold d = 10%, Qwen2-7B and Llama-3.1-8Breverse with probability 0.16 under different prompt variations.</p>
<p>Conclusion</p>
<p>We present COGNITIVEVAL, a framework for the cognitive evaluation of LLMs, and use it to assess state-of-the-art LLMs on five cognitive tasks.Our results showcase the flexibility of our proposed pipeline, and we demonstrate the importance of COGNITIVEVAL features like prompt permutations in cognitive assessment of LLMs, showing that model performance on cognitive tasks varies across different prompts.</p>
<p>In our assessment of LLM cognition, we find that LLMs generally have stronger memory than humans, which we believe can be explained by the lack of degradation in information encoding for LLMs relative to humans.Through the backward and forward digit span tasks and the DRM task, we see that LLMs consistently outperform human baselines, although we note that the comparison is inherently flawed in the case of memory.However, it is interesting that our results suggest that the relative difference in short-term and working memory span is larger in LLMs than in humans.An interesting direction of future work is exploring various types of LLM working memory, and executive function in general, for a more comprehensive understanding of this result.Figure 7b summarizes our findings.</p>
<p>The tasks involving executive function (WCST and Flanker/Inhibition), on the other hand, show lower accuracy for LLMs relative to humans.While a Chain-of-Thought (CoT) prompting strategy (Wei et al., 2022) may result in higher accuracy on some of these tasks (Coda-Forno et al., 2024), we emphasize that these experiments are designed to be completed with little to no conscious deliberation by humans, and the human baseline values reflect that.Therefore, reasoning or CoT is not a fair comparison: these tasks measure automatic, rather than conscious and deliberative, cognition.</p>
<p>Comparisons of the LLMs tested yield a few insights.First, bigger is not always better on these cognitive tasks.While memory tasks (backward digit span, DRM) show advantages of model size, executive function tasks (WCST, Flanker) do not.Future research is needed to explore whether this pattern holds across more tasks and models.Second, we find no consistently strong model: there is no "winner" across all cognitive experiments.For example, the only exceptionally high performance came from the 70B variant of Llama3 on the backward digit task, but this model does not do particularly well on the other tasks.</p>
<p>COGNITIVEVAL provides a flexible and robust framework to further explore the nuances of these results, and also to expand LLM evaluation throughout other areas of cognition.As more experimental evidence is gathered, we can begin to form stronger theories to explain the artificial cognitive processes in LLMs.</p>
<p>Steven Yantis and James C Johnston.On the locus of visual selection: evidence from focused attention tasks.</p>
<p>A Appendix</p>
<p>A.1 Using COGNITIVEVAL COGNITIVEVAL experiments can be set up using a web interface or through json configuration files.An experiment specification includes:</p>
<p>• Stimuli file (csv).Requirements are intentionally lax: it can contain any number of columns, but should include column(s) corresponding to relevant text to include in the prompt, as well as column(s) for independent variable(s).• Group specification (json or web form).Designate stimuli columns as describing independent variables (IVs) and create groups based on the different IV levels.IVs can be combined, for example if one IV is politness and another IV is sentiment, a positive + impolite group could be created.• Metric specification (json or web form).Describe whether you would like COGNI-TIVEVAL to automatically evaluate responses for accuracy rates, average reported numbers, or a custom function.• Prediction specification (json or web form).Specify how you would like COGNI-TIVEVAL to automatically compare your groups with respect to which specified metrics.</p>
<p>• Prompt (json or web form).Instructions for the model that describe the experimental task.The web interface allows users to sandbox their prompts with GPT-4o using the OpenAI API.• Metadata (json).Experiment setup details, such as which models you would like to test (COGNITIVEVAL currently supports Huggingface and OpenAI models), and whether the stimuli should be served in an interactive dialogue or one-at-a-time.</p>
<p>A.2 Prompt Specifications</p>
<p>During prompt preparation in the evaluation pipeline, each row in the stimuli spreadsheet is first integrated into each specified data format.Each resulting data instance is then inserted into a corresponding prompt template.For our experiment, we generated 10 data formats and 3 prompt template variants per task-this includes the original prompt template and two paraphrased versions.Below, we present the data formats and prompt templates used for the WCST.In the data formats, placeholders of the form (|column name|) are replaced with values from the corresponding spreadsheet columns.In the prompt templates, the placeholder &lt;<DATA>&gt; is replaced with the formatted data instance.backward digit span norms µ = 5.4, σ = 1.5, we estimate that about 14% of people could have a backward digit span of 7.For the forward digit span of length 7, multiple sources report means of 7 (Banken, 1985;Monaco et al., 2013), so we estimate the accuracy at 50%.</p>
<p>We use the reported mean human WCST errors in Barcel ó et al. (1997) to estimate human WCST accuracy.The reported mean number of errors is 58.9 across 252 trials, so we estimate the baseline accuracy to be about 77%.</p>
<p>To estimate human DRM accuracy on the unseen critical words, we refer to the reported mean accuracy of 63% in Pardilla-Delgado &amp; Payne (2017) on the recognition task in Experiment 1, as our task uses the same stimuli.</p>
<p>We use the reported incongruent error rate in the Flanker task from Yantis &amp; Johnston (1990), 4.7%, to estimate the baseline human accuracy for incongruent stimuli to be about 95%.</p>
<p>A.4 Additional Results</p>
<p>A.4.1 Forward Digit Span</p>
<p>The mean accuracies for forward digit span across all models can be found in Table 4 .</p>
<p>Length Gemma-2-27B Gemma-2-9B Llama-3.1-70BLlama-3.1-8BQwen2-72B Qwen2-7B 7 1.00 1.00 0.99 1.00 1.00 1.00 20 1.00 1.00 1.00 1.00 1.00 0.98 30 1.00 1.00 1.00 1.00 1.00 1.00 50 1.00 0.99 0.99 1.00 1.00 1.00</p>
<p>Table 4: Mean accuracies on forward digit span across all prompts.</p>
<p>A.4.2 Wisconsin Card Sorting Task</p>
<p>The average error rates over the first ten turns after a rule is introduced are shown in Figure 8 for each model.</p>
<p>We also investigate model performance over all dialogue turns.Correlation results are in Table 5, and the remaining model plots are in Figure 9.</p>
<p>Finally, decreases in accuracy between first and second exposure to a sorting rule can be seen in Figure 10 Gemma-27B Gemma-9B Llama-70B Llama-8B Qwen-72B Qwen-7B Pearson's r -0.36<em> -0.51</em> -0.31<em> -0.33</em> -0.45<em> -0.28</em></p>
<p>Figure 2 :
2
Figure 2: We use COGNITIVEVAL to adapt these five human cognitive science experiments to LLMs.The cognitive taxonomy reflects common interpretations of these experiments in cognitive science literature, although variant taxonomies exist.</p>
<p>(a) We translate the Wisconsin Card Sorting Task (WCST), typically presented visually (left), to a text-only format (right).The correct sorting option depends on the underlying sorting rule e.g., if the sorting by color, option 4 is correct.If sorting by shape or count, options 3 and 2 are correct, respectively.(b) Accuracy for each turn in the dialogue for Qwen2-72B (mean accuracy taken across n = 30 prompts per turn).Colors indicate underlying sorting rule for that turn.The models do not adapt well to the implicit rule changes, and the accuracy declines as the dialogue continues (Pearson's r = −.43,p &lt; 0.01).</p>
<p>Figure 3 :
3
Figure 3: WCST task: (a) setup, and (b) accuracy over the course of the dialogue for Qwen2-72B.Accuracy plots for other models can be found in §A.4.2.</p>
<p>(a) Average accuracy on the flanker task (n = 960, 32 stimuli x 30 prompt variations).All models do substantially worse in the incongruent condition.(b)When models answer correctly, model estimates of the correct answer's probability still tend to be lower in the incongruent condition.</p>
<p>Figure 4 :
4
Figure 4: Flanker task: (a) average accuracy and (b) probabilities of correct answers.All models tested perform worse in the incongruent condition.</p>
<p>(a) Mean model accuracy, by digit sequence length, in the backward digit span task across all trials (n = 2100, 70 stimuli x 30 prompts).</p>
<p>Figure 6 :
6
Figure 6: For each prompt variation, we compute each model's average accuracy on the task.These box plots display the range in resulting model accuracies over the prompts.For challenging tasks like Flanker and WCST, the range accuracies are comparatively large.</p>
<p>Figure 7 :
7
Figure 7: Model outputs on cognitive assessments are affected by prompt perturbations; these differences can impact model comparisons with respect to cognitive abilities.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: We take the average error rate for preservation (red) and other (blue) errors across the first 10 rounds after a new rule is introduced.We find no correlation between any type of model error and the number of rounds exposed to a new rule.</p>
<p>Figure 10 :
10
Figure 10: Models tend to have lower accuracy the second time they are exposed to the count and shape sorting rules (second and third rows).</p>
<p>Table 1 :
1
WCST: Mean frequencies of correct answers, answers with preservation errors, and answers with other errors on the WCST across all trials; n = 3060 (102 trials x 30 prompts).
GemmaLlamaQwen9B27B8B70B7B72BCorrect0.27 0.43 0.43 0.39 0.44 0.40Preservation 0.36 0.31 0.47 0.39 0.50 0.43Other0.36 0.26 0.09 0.22 0.06 0.17</p>
<p>Journal of experimental psychology: Human perception and performance, 16(1):135, 1990.
Jiahao Ying, Yixin Cao, Kai Xiong, Long Cui, Yidong He, and Yongbin Liu. Intuitive ordependent? investigating LLMs' behavior style to conflicting prompts. In Lun-Wei Ku,Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 4221-4246, Bangkok,Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.232. URL https://aclanthology.org/2024.acl-long.232/.Lance Ying, Katherine M. Collins, Lionel Wong, Ilia Sucholutsky, Ryan Liu, Adrian Weller,Tianmin Shu, Thomas L. Griffiths, and Joshua B. Tenenbaum. On benchmarking human-like intelligence in machines, 2025. URL https://arxiv.org/abs/2502.20502.</p>
<p>Table 5 :
5
Pearson correlation results for model accuracy and dialogue length in the WCST.Significant correlations (p &lt; 0.05) are marked with *.We find all model accuracy decreases as the dialogue goes on.</p>
<p>Extending COGNITIVEVAL to include vision-language models is a possible direction for future work.
This aligns withCoda-Forno et al. (2024)'s findings that LLMs tend to place more weight on prior beliefs than observations in decision-making tasks.
Ethics StatementIt is possible to use a tool like COGNITIVEVAL irresponsibly to falsely create the impression that LLMs possess certain cognitive abilities.Careful consideration of not only the stimulus design, but also the prompting strategy and result interpretation are important (see, e.g., the extensive debate about whether LLMs are capable of Theory of Mind), and close interdisciplinary collaboration is ideal.Respond with <em>only the option number</em> you would like to sort the card into.After you respond, you will get feedback about your response.You will have to try a different classification rule if the feedback says you were wrong.This is an example in which the underlying rule is color: Paraphrased Prompt Template 1 -WCST You will be shown an item, and your task is to match it with one of four options.Your objective is to determine the hidden classification rule that assigns the item into one of these four options.The classification rule may be shape, color, or count.Reply with <em>only the option number</em> you believe the item should be matched with, based on the classification rule.After you reply, you will receive feedback regarding your choice.If the feedback says you were incorrect, you will need to attempt a different classification rule.Note that the rule may change at any point; keep using the feedback to figure out the current rule.Here's an example in which the underlying rule is color:A.3 Additional Experiment DetailsA.3.1 DRM TaskThe DRM word lists are the same as those used inPardilla-Delgado &amp; Payne (2017).We reproduce them for convenience here in Table3.We put all lists in the first prompt.The remaining prompts ask the model to determine whether a word was on those lists.The words we present are selected as follows: seven words from each list are presented as "Seen" words, and the 18 critical words presented as "Unseen (critical)" words.We include the following 18 words as "Unseen (control)" words: Robber, Vegetable, Thief, Fruit, Up, High, Sister, Dance, Young, Money, Sky, Jump, Web, Small, Chess, Palace, Strong.These unseen controls words are sourced from unused DRM word lists inRoediger &amp; McDermott (1995).Presentation order of the words is randomized.A.3.2 Estimation of human baselinesIt is difficult to precisely estimate human baselines due to the sheer number of studies applying these experimental tasks across a variety of human populations.We estimate human baseline accuracy for the backward digit span of length 7 by referring to the means and standard deviations presented inChoi et al. (2013).Ideally we would use the Weschler norms, but these are proprietary and we do not have access.Given the
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. GPT-4 technical report. 2023arXiv preprint</p>
<p>Clinical utility of considering digits forward and digits backward as separate components of the wechsler adult intelligence scale-revised. Joseph A Banken, Journal of Clinical Psychology. 4151985</p>
<p>The wisconsin card sorting test and the assessment of frontal function: A validation study with event-related potentials. Francisco Barcel, Ó , Marta Sanz, Vicente Molina, Francisco J Rubia, Neuropsychologia. 3541997</p>
<p>Of models and men: Probing neural networks for agreement attraction with psycholinguistic data. Maxim Bazhukov, Ekaterina Voloshina, Sergey Pletenev, Arseny Anisimov, Oleg Serikov, Svetlana Toldova, 10.18653/v1/2024.conll-1.22Proceedings of the 28th Conference on Computational Natural Language Learning. Libby Barak, Malihe Alikhani, the 28th Conference on Computational Natural Language LearningMiami, FL, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>A normative study of the digit span in an educationally diverse elderly population. Hyo Jung Choi, Dong Young Lee, Eun , Hyun Seo, Min , Kyung Jo, Bo Kyung Sohn, Young , Min Choe, Min Soo Byun, Jee Wook Kim, Shin Gyeom Kim, Jong Choul Yoon, Psychiatry investigation. 111392013</p>
<p>Inducing anxiety in large language models increases exploration and bias. Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, Eric Schulz, </p>
<p>CogBench: a large language model walks into a psychology lab. Julian Coda-Forno, Marcel Binz, Jane X Wang, Eric Schulz, Proceedings of the 41st International Conference on Machine Learning. Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, Felix Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in neural information processing systems. 202235</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 20222207arXiv e-prints</p>
<p>. Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J L Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R J Chen, R L Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, S S Shuting Pan, Shuang Li, Shaoqing Zhou, Shengfeng Wu, Tao Ye, Tian Yun, Tianyu Pei, T Sun, Wangding Wang, Wanjia Zeng, Wen Zhao, Wenfeng Liu, Wenjun Liang, Wenqin Gao, Wentao Yu, W L Zhang, Wei Xiao, Xiaodong An, Xiaohan Liu, Xiaokang Wang, Xiaotao Chen, Xin Nie, Xin Cheng, Xin Liu, Xingchao Xie, Xinyu Liu, Xinyuan Yang, Xuecheng Li, Xuheng Su, X Q Lin, Xiangyue Li, Xiaojin Jin, Xiaosha Shen, Xiaowen Chen, Xiaoxiang Sun, Xinnan Wang, Xinyi Song, Xianzu Zhou, Xinxia Wang, Y K Shan, Y Q Li, Y X Wang, Yang Wei, Yanhong Zhang, Yao Xu, Yao Li, Yaofeng Zhao, Yaohui Sun, Yi Wang, Yichao Yu, Yifan Zhang, Yiliang Shi, Ying Xiong, Yishi He, Yisong Piao, Yixuan Wang, Yiyang Tan, Yiyuan Ma, Yongqiang Liu, Yuan Guo, Yuduan Ou, Yue Wang, Yuheng Gong, Yujia Zou, Yunfan He, Yuxiang Xiong, Yuxiang Luo, Yuxuan You, Yuyang Liu, Y X Zhou, Yanhong Zhu, Yanping Xu, Yaohui Huang, Yi Li, Yuchen Zheng, Yunxian Zhu, Ying Ma, Yukun Tang, Yuting Zha, Z Z Yan, Zehui Ren, Zhangli Ren, Zhe Sha, Zhean Fu, Zhenda Xu, Zhengyan Xie, Zhewen Zhang, Zhicheng Hao, Zhigang Ma, Zhiyu Yan, Zihui Wu, Zijia Gu, Zijun Zhu, Zilin Liu, Ziwei Li, Xie, 2025Ziyang Song, Zizheng PanZhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</p>
<p>On the prediction of occurrence of particular verbal intrusions in immediate recall. James Deese, Journal of experimental psychology. 581171959</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Cognitive bias in decision-making with LLMs. Jessica Maria Echterhoff, Yao Liu, Abeer Alessa, Julian Mcauley, Zexue He, 10.18653/v1/2024.findings-emnlp.739Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Effects of noise letters upon the identification of a target letter in a nonsearch task. A Barbara, Charles W Eriksen, Eriksen, Perception &amp; psychophysics. 1611974</p>
<p>Cognitive science: An introduction to the study of mind. Jay Friedenberg, Gordon Silverman, Michael J Spivey, 2021Sage Publications</p>
<p>A behavioral analysis of degree of reinforcement and ease of shifting to new responses in a weigl-type card-sorting problem. A David, Esta Grant, Berg, Journal of experimental psychology. 3844041948</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Nature Computational Science. 3102023</p>
<p>Prompting is not a substitute for probability measurements in large language models. Jennifer Hu, Roger Levy, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>How to evaluate the cognitive abilities of llms. Anna A Ivanova, Nature Human Behaviour. 2025</p>
<p>Capturing failures of large language models via human cognitive biases. Erik Jones, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202235</p>
<p>LLMs are prone to fallacies in causal inference. Nitish Joshi, Abulhair Saparov, Yixin Wang, He He, 10.18653/v1/2024.emnlp-main.590Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Log probabilities are a reliable estimate of semantic plausibility in base and instruction-tuned language models. Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko, Anna Ivanova, Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP2024</p>
<p>CaT-bench: Benchmarking language model understanding of causal and temporal dependencies in plans. Yash Kumar Lal, Vanya Cohen, Nathanael Chambers, Niranjan Balasubramanian, Ray Mooney, 10.18653/v1/2024.emnlp-main.1077Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Language models don't learn the physical manifestation of language. Bruce Lee, Jaehyuk Lim, 10.18653/v1/2024.acl-long.195Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>A psycholinguistic evaluation of language models' sensitivity to argument roles. Eun-Kyoung Rosa Lee, Sathvik Nair, Naomi Feldman, 10.18653/v1/2024.findings-emnlp.186Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Are emergent abilities in large language models just in-context learning?. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych, 10.18653/v1/2024.acl-long.279Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>CogGPT: Unleashing the power of cognitive dynamics on large language models. Yaojia Lv, Haojie Pan, Zekun Wang, Jiafeng Liang, Yuanxing Liu, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin, 10.18653/v1/2024.findings-emnlp.352Findings of the Association for Computational Linguistics: EMNLP 2024. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, Miami, Florida, USANovember 2024Association for Computational Linguistics</p>
<p>Factual confidence of llms: on reliability and robustness of current estimators. Matéo Mahaut, Laura Aina, Paula Czarnowska, Momchil Hardalov, Thomas Mueller, Lluís Màrquez, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Stephanie Miles, Caitlin A Howlett, Carolyn Berryman, Maja Nedeljkovic, G Lorimer Moseley, Andrea Phillipou, Considerations for using the wisconsin card sorting test to assess cognitive flexibility. Behavior research methods. 202153</p>
<p>Effects of different brain lesions on card sorting: The role of the frontal lobes. Brenda Milner, Archives of neurology. 911963</p>
<p>Forward and backward span for verbal and visuo-spatial data: standardization and normative data from an italian adult population. Marco Monaco, Alberto Costa, Carlo Caltagirone, Giovanni Augusto Carlesimo, Neurological Sciences. 342013</p>
<p>The deese-roediger-mcdermott (drm) task: A simple cognitive paradigm to investigate false memories in the laboratory. Enmanuelle Pardilla, - Delgado, Jessica D Payne, Journal of visualized experiments: JoVE. 119547932017</p>
<p>Gemma 2: Improving open language models at a practical size. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Thomas Hussenot, Bobak Mesnard, Alexandre Shahriari, Ramé, arXiv:2408.001182024arXiv preprint</p>
<p>Using artificial populations to study psychological phenomena in neural models. Jesse Roberts, Kyle Moore, Drew Wilenzick, Douglas Fisher, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Creating false memories: Remembering words not presented in lists. L Henry, Kathleen B Roediger, Mcdermott, Journal of experimental psychology: Learning, Memory, and Cognition. 2148031995</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Development of cognitive intelligence in pre-trained language models. Raj Sanjay Shah, Khushi Bhardwaj, Sashank Varma, 10.18653/v1/2024.emnlp-main.539Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Amos Tversky, Daniel Kahneman, Science. 18541571974</p>
<p>Large language models fail on trivial alterations to theory-of-mind tasks. Tomer Ullman, arXiv:2302.083992023arXiv preprint</p>
<p>Paraphrase types elicit prompt engineering capabilities. Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
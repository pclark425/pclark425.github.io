<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9204 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9204</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9204</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-1e5e34f986d0c3cae596966e43055585b06bcaeb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1e5e34f986d0c3cae596966e43055585b06bcaeb" target="_blank">Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel UAD method for tabular data by evaluating how much noise is in the data is presented, proving that the proposed method can detect anomalous data successfully, although the method does not utilize any real anomalous data in the training stage.</p>
                <p><strong>Paper Abstract:</strong> Unsupervised anomaly detection (UAD) plays an important role in modern data analytics and it is crucial to provide simple yet effective and guaranteed UAD algorithms for real applications. In this paper, we present a novel UAD method for tabular data by evaluating how much noise is in the data. Specifically, we propose to learn a deep neural network from the clean (normal) training dataset and a noisy dataset, where the latter is generated by adding highly diverse noises to the clean data. The neural network can learn a reliable decision boundary between normal data and anomalous data when the diversity of the generated noisy data is sufficiently high so that the hard abnormal samples lie in the noisy region. Importantly, we provide theoretical guarantees, proving that the proposed method can detect anomalous data successfully, although the method does not utilize any real anomalous data in the training stage. Extensive experiments through more than 60 benchmark datasets demonstrate the effectiveness of the proposed method in comparison to 12 baselines of UAD. Our method obtains a 92.27\% AUC score and a 1.68 ranking score on average. Moreover, compared to the state-of-the-art UAD methods, our method is easier to implement.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9204.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9204.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-feature-extractor-for-noise-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using large language models as feature extractors for noise-evaluation-based anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions using recent large language models (LLMs) to extract latent text embeddings (e.g., CLS token or pooled sentence/document embeddings) so that the proposed noise-evaluation model can be applied in the embedding (tabular) space for textual anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>recent large language models (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (pretrained language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>textual data → latent embeddings (sentence- or document-level); i.e., sequence/text transformed into tabular/vector embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>text / documents / general textual data (not a specific domain in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>general out-of-distribution / anomalous documents or sequences (not concretely specified)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposed pipeline (mentioned): (1) extract latent features from text tokens using a pretrained LLM (e.g., use CLS token or aggregate token embeddings) to obtain sentence- or document-level embeddings; (2) apply the paper's noise-evaluation neural network on those embeddings (treating them as tabular feature vectors) to predict per-dimension noise magnitude and aggregate to an anomaly score. The paper frames this as an adaptation (no experiments reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Not applied in a text setting in the paper; general baselines used in the tabular experiments are listed (Isolation Forest, COPOD, Auto-encoder, KNN, LOF, DeepSVDD, AnoGAN, ECOD, SCAD, NeuTraLAD, PLAD, DPAD) and could serve as baselines if the pipeline were evaluated on text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When applied in the paper's tabular experiments, metrics were AUROC (AUC) and F1; however, no text/LLM experiments or metric results are reported for the LLM-based pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No empirical results reported for using language models; suggestion only—no numerical performance for LLM-based anomaly detection provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Not applicable — the paper does not evaluate LLM-based feature extraction or compare it to baselines; only suggests it as an approach for textual data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper explicitly warns that using a feature extractor (e.g., pretrained LLM) risks losing anomaly-relevant information: 'there is a risk that the feature extractor may fail to preserve the information of anomalies, which will lead to failure in detecting anomalies.' It recommends as future work to jointly train the feature extractor, a decoder, and the noise detector end-to-end in data space to mitigate this risk.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Key insight: rather than directly defining 'noise' in raw text, it is practical to map text into a continuous latent space (via an LLM) and apply the noise-evaluation model on those embeddings; the paper frames this as a general strategy for adapting their tabular noise-evaluation approach to textual/sequential modalities and highlights the importance of ensuring the feature extractor preserves anomaly signals (suggesting joint training as a promising direction).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning for anomaly detection: A review <em>(Rating: 2)</em></li>
                <li>Deep Anomaly Detection with Outlier Exposure <em>(Rating: 1)</em></li>
                <li>Using self-supervised learning can improve model robustness and uncertainty <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9204",
    "paper_id": "paper-1e5e34f986d0c3cae596966e43055585b06bcaeb",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "LLM-feature-extractor-for-noise-eval",
            "name_full": "Using large language models as feature extractors for noise-evaluation-based anomaly detection",
            "brief_description": "The paper mentions using recent large language models (LLMs) to extract latent text embeddings (e.g., CLS token or pooled sentence/document embeddings) so that the proposed noise-evaluation model can be applied in the embedding (tabular) space for textual anomaly detection.",
            "citation_title": "Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation",
            "mention_or_use": "mention",
            "model_name": "recent large language models (unspecified)",
            "model_type": "transformer (pretrained language model)",
            "model_size": null,
            "data_type": "textual data → latent embeddings (sentence- or document-level); i.e., sequence/text transformed into tabular/vector embeddings",
            "data_domain": "text / documents / general textual data (not a specific domain in experiments)",
            "anomaly_type": "general out-of-distribution / anomalous documents or sequences (not concretely specified)",
            "method_description": "Proposed pipeline (mentioned): (1) extract latent features from text tokens using a pretrained LLM (e.g., use CLS token or aggregate token embeddings) to obtain sentence- or document-level embeddings; (2) apply the paper's noise-evaluation neural network on those embeddings (treating them as tabular feature vectors) to predict per-dimension noise magnitude and aggregate to an anomaly score. The paper frames this as an adaptation (no experiments reported).",
            "baseline_methods": "Not applied in a text setting in the paper; general baselines used in the tabular experiments are listed (Isolation Forest, COPOD, Auto-encoder, KNN, LOF, DeepSVDD, AnoGAN, ECOD, SCAD, NeuTraLAD, PLAD, DPAD) and could serve as baselines if the pipeline were evaluated on text embeddings.",
            "performance_metrics": "When applied in the paper's tabular experiments, metrics were AUROC (AUC) and F1; however, no text/LLM experiments or metric results are reported for the LLM-based pipeline.",
            "performance_results": "No empirical results reported for using language models; suggestion only—no numerical performance for LLM-based anomaly detection provided.",
            "comparison_to_baseline": "Not applicable — the paper does not evaluate LLM-based feature extraction or compare it to baselines; only suggests it as an approach for textual data.",
            "limitations_or_failure_cases": "The paper explicitly warns that using a feature extractor (e.g., pretrained LLM) risks losing anomaly-relevant information: 'there is a risk that the feature extractor may fail to preserve the information of anomalies, which will lead to failure in detecting anomalies.' It recommends as future work to jointly train the feature extractor, a decoder, and the noise detector end-to-end in data space to mitigate this risk.",
            "unique_insights": "Key insight: rather than directly defining 'noise' in raw text, it is practical to map text into a continuous latent space (via an LLM) and apply the noise-evaluation model on those embeddings; the paper frames this as a general strategy for adapting their tabular noise-evaluation approach to textual/sequential modalities and highlights the importance of ensuring the feature extractor preserves anomaly signals (suggesting joint training as a promising direction).",
            "uuid": "e9204.0",
            "source_info": {
                "paper_title": "Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning for anomaly detection: A review",
            "rating": 2,
            "sanitized_title": "deep_learning_for_anomaly_detection_a_review"
        },
        {
            "paper_title": "Deep Anomaly Detection with Outlier Exposure",
            "rating": 1,
            "sanitized_title": "deep_anomaly_detection_with_outlier_exposure"
        },
        {
            "paper_title": "Using self-supervised learning can improve model robustness and uncertainty",
            "rating": 1,
            "sanitized_title": "using_selfsupervised_learning_can_improve_model_robustness_and_uncertainty"
        }
    ],
    "cost": 0.016854749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation</h1>
<p>Wei Dai, Kai Hwang, Jicong Fan*<br>The Chinese University of Hong Kong, Shenzhen, China<br>weidai@link.cuhk.edu.com, hwangkai@cuhk.edu.cn, fanjicong@cuhk.edu.cn</p>
<h4>Abstract</h4>
<p>Unsupervised anomaly detection (UAD) plays an important role in modern data analytics and it is crucial to provide simple yet effective and guaranteed UAD algorithms for real applications. In this paper, we present a novel UAD method for tabular data by evaluating how much noise is in the data. Specifically, we propose to learn a deep neural network from the clean (normal) training dataset and a noisy dataset, where the latter is generated by adding highly diverse noises to the clean data. The neural network can learn a reliable decision boundary between normal data and anomalous data when the diversity of the generated noisy data is sufficiently high so that the hard abnormal samples lie in the noisy region. Importantly, we provide theoretical guarantees, proving that the proposed method can detect anomalous data successfully, although the method does not utilize any real anomalous data in the training stage. Extensive experiments through more than 60 benchmark datasets demonstrate the effectiveness of the proposed method in comparison to 12 baselines of UAD. Our method obtains a $92.27 \%$ AUC score and a 1.68 ranking score on average. Moreover, compared to the state-of-the-art UAD methods, our method is easier to implement.</p>
<h2>Introduction</h2>
<p>In the realm of data analysis, anomaly detection (AD) stands as a pivotal challenge with far-reaching implications across various domains, including cybersecurity (Siddiqui et al. 2019; Saeed et al. 2023), healthcare (Yang, Qi, and Zhou 2023; Abououf et al. 2023), finance (Hilal, Gadsden, and Yawney 2022), and industrial processes (Fan, Chow, and Qin 2022; Roth et al. 2022). Existing deep learning-based unsupervised AD methods often rely on an auxiliary learning objective such as auto-encoder, generative model, and contrastive learning. These methods indirectly detect anomalous data using other metrics such as reconstruction error, which lack generalizability and reliability guarantees (Hussain et al. 2023). Explicitly learning a one-class decision boundary may resolve this issue. Many well-known unsupervised AD methods assume the normal training data has a special structure in their data space or embedding space (Schölkopf et al. 2001; Tax and Duin 2004; Ruff et al. 2018; Goyal et al. 2020; Zhang</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al. 2024; Xiao et al. 2025). Such assumptions may not hold or be guaranteed in practice and sometimes place a burden on the model training (Cai and Fan 2022; Fu, Zhang, and Fan 2024). For instance, in deep SVDD (Ruff et al. 2018), the optimal decision boundary in the embedding space may be very different from the learned hypersphere, leading to unsatisfactory detection performance (Zhang et al. 2024).</p>
<p>Given that tabular data is probably the most common data type and other types of data such as images can be converted to tabular data using feature encoders or pretrained models, in this work, we focus on the tabular data only. We propose a novel unsupervised AD method for tabular data without making any assumption about the distribution of normal data. Since hard anomalies are often close to normal data, it is reasonable to hypothesize that hard anomalies are special cases of perturbed samples of normal data. Therefore, if the diversity of the perturbations or added noises on normal data is sufficiently high, we can obtain hard anomalies. Consequently, if a model can recognize highly diverse perturbations or noises, it can detect hard anomalies as well as easy anomalies successfully. By directly learning from the diverse noise patterns and the clean data patterns, we can learn an effective decision boundary around the normal data, generalizing well to unseen data.</p>
<p>Our contributions are highlighted as follows.</p>
<ul>
<li>We propose a novel AD method for tabular data using noise evaluation. Our scheme generates highly diverse noise-augmented instances for the normal samples. By evaluating the noise magnitude, our method can accurately identify anomalies.</li>
<li>The proposed method provides a simple yet effective scheme that does not make assumptions about the normal training data. In addition, the noise generation is straightforward without any extra training. Compared with (Wang et al. 2021; Goyal et al. 2020; Yan et al. 2021; Cai and Fan 2022), our method is more lightweight for training (requires less module for training.)</li>
<li>We theoretically prove the generalizability and reliability of the proposed method.</li>
<li>We conduct extensive empirical experiments on 47 real datasets in an unsupervised anomaly detection setting and 25 real-world tabular datasets in a one-class classification setting to demonstrate the performance of the proposed</li>
</ul>
<p>schemes. The results show that our method achieved superior performance compared with 12 baseline methods including the state-of-the-art.</p>
<h2>Related Work</h2>
<p>Unsupervised anomaly detection (UAD) is also known as the one-class classification (OCC) problem, in which all or most training samples are assumed to be normal. The learning objective is to learn a decision boundary that distinguishes whether a sample belongs to the same distribution of the normal training data or not. There is another similar problem setting known as outlier detection on contaminated dataset <em>Huang et al. (2024)</em>. The goal is to detect noised samples or outliers within the training data <em>Ding et al. (2022a)</em>. This line of work is orthogonal to our UAD problem.</p>
<p>In the past decades, many UAD methods have been studied <em>Liu et al. (2008); Chang et al. (2023)</em>. Traditional methods like proximity-based <em>Breunig et al. (2000); Angiulli and Pizzuti (2002); Papadimitriou et al. (2003); He et al. (2003)</em>, probabilty-based <em>Yang et al. (2009); Zong et al. (2018); Li et al. (2020)</em>, and one-class support vector machine <em>Schölkopf et al. (2001); Tax and Duin (2004)</em> approaches struggle with high dimensionality and complex data structures. Deep neural network-based methods have been proposed to address these issues. For instance, auto-encoder methods identify outliers by detecting high reconstruction errors, as outlier samples do not conform to historical data patterns <em>Aggarwal (2016); Chen et al. (2017); Wang et al. (2021)</em>. Generative model methods compare latent features or generated samples to spot anomalies <em>Schlegl et al. (2017); Liu et al. (2019); Zhang et al. (2023); Tur et al. (2023); Xiao et al. (2025)</em>. For example, <em>Xiao et al. (2025)</em> proposed an inverse generative adversarial network that converts the data distribution to a compact Gaussian distribution, based on which the density of test data can be calculated for anomaly detection. Contrastive learning <em>Sohn et al. (2020); Jin et al. (2021); Shenkar and Wolf (2022)</em> leverages feature representation differences to detect anomalies. Unlike autoencoder-based methods that focus on reducing reconstruction error and reducing dimensionality to remove noise, our method aims to evaluate noise level, which is similar to the denoising diffusion model <em>Ho et al. (2020)</em>.</p>
<p>Some works explicitly build an anomaly detection or OCC objective <em>Ruff et al. (2018); Goyal et al. (2020); Yan et al. (2021); Chen et al. (2022); Cai and Fan (2022)</em>. For instance, Deep SVDD <em>Ruff et al. (2018)</em> trains a neural network to construct a hypersphere in the output space to enclose the normal training data. DROCC <em>Goyal et al. (2020)</em> assumes normal samples lie on low-dimensional manifolds and treats identifying the ideal hypersphere as an adversarial optimization problem. PLAD <em>Cai and Fan (2022)</em> outputs an anomaly score by learning a small perturbation of normal data as the negative sample with a classifier. Unlike PLAD, which uses extra additive and multiplicative perturbations requiring a perturbator, our method generates negative samples without extra parameters, making the training efficient.</p>
<p>It is worth mentioning that there are vision UAD methods utilizing synthetic anomalous data. However, the normality and abnormality can be visualized directly and there naturally</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: An illustration of the allocation of normal, noised, and true anomalous samples. $\mathcal{D}$, $\hat{\mathcal{D}}$, and $\hat{\mathcal{D}}$ are the normal, noised, and anomalous distributions respectively. $\hat{\mathcal{D}}$ is composed of a hard part $\hat{\mathcal{D}}<em E="E">{H}$ and an easy part $\hat{\mathcal{D}}</em>}$. Theorem 1 and Theorem 2 are for $\hat{\mathcal{D}<em E="E">{H}$ and $\hat{\mathcal{D}}</em>$ respectively.</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$x$</td>
<td>a scalar</td>
<td>$\boldsymbol{x}_{i}$</td>
<td>a vector with index $i$</td>
</tr>
<tr>
<td>$\mathcal{X}_{i}$</td>
<td>a set with index $i$</td>
<td>$[i]$</td>
<td>the set ${1,2,\ldots,i}$</td>
</tr>
<tr>
<td>$\mathcal{D}$</td>
<td>a distribution</td>
<td>$|\cdot|_{2}$</td>
<td>$\ell_{2}$ norm of vector</td>
</tr>
<tr>
<td>$|\cdot|_{1}$</td>
<td>$\ell_{1}$ norm of vector</td>
<td>$</td>
<td>\boldsymbol{x}</td>
</tr>
<tr>
<td>$\cup$</td>
<td>union of sets</td>
<td>$\subset$</td>
<td>subset</td>
</tr>
<tr>
<td>$H$</td>
<td>entropy</td>
<td>$h_{\boldsymbol{\theta}}$</td>
<td>model parameterized by $\boldsymbol{\theta}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Notations</p>
<p>exists prior knowledge about anomalies (e.g., visible spots or blurs) in visual data. Regarding tabular data, we do not have such prior knowledge. Vision AD methods require prior pre-trained models or external reference datasets to obtain the negative samples, such as DREAM <em>Zavrtanik et al. (2021)</em> with the Describable Textures Dataset or AnomalyDiffusion <em>Hu et al. (2024)</em> with a stable diffusion model. Such resources are costly and violate the principle of unsupervised learning. Tabular data, however, spans diverse domains (e.g., medical, industrial), making external datasets and pretrained model infeasible.</p>
<h2>Proposed Method</h2>
<h3>Problem Formulation and Notations</h3>
<p>Given a data set $\mathcal{X}=\left{\boldsymbol{x}<em 2="2">{1},\boldsymbol{x}</em>},\ldots,\boldsymbol{x<em i="i">{N}\right}$, where the element $\boldsymbol{x}</em>$. The main notations used in this paper are shown in Table 1.}$ are drawn from an unknown distribution $\mathcal{D}$ in $\mathbb{R}^{d}$ (deemed as a normal data distribution). In the context of UAD, the primary objective is to develop a function $f:\mathbb{R}^{d}\rightarrow{0,1}$, which effectively discriminates between in-distribution (normal) and out-of-distribution (anomalous) instances. This discriminative function is formulated to assign a binary label, where $f(x)=1$ indicates $\boldsymbol{x}$ does not belong to $\mathcal{D}$ and $f(x)=0$ corresponds to $\boldsymbol{x}$ coming from $\mathcal{D</p>
<p>Anomalous Data Decomposition</p>
<p>Let $\tilde{\mathcal{X}}$ be the set consisting of all anomalous data drawn from some unknown distribution $\tilde{\mathcal{D}}$ deemed as an anomalous distribution. For any $\hat{\boldsymbol{x}}\in\tilde{\mathcal{X}}$, we decompose it as</p>
<p>$\hat{\boldsymbol{x}}=\boldsymbol{x}+\boldsymbol{\epsilon},$ (1)</p>
<p>where $\boldsymbol{x}\in\mathcal{D}$ is a normal counterpart of $\hat{\boldsymbol{x}}$ and $\boldsymbol{\epsilon}$ denotes the derivation of $\hat{\boldsymbol{x}}$ from $\boldsymbol{x}$. The magnitude of $\boldsymbol{\epsilon}$, denoted as $|\boldsymbol{\epsilon}|<em 1="1">{1}$, measures how anomalous $\hat{\boldsymbol{x}}$ is. Note that this decomposition is not unique and hence one may seek the one with the smallest $|\boldsymbol{\epsilon}|</em>$, i.e.,}$. If we can learn a model $h$ to predict $\boldsymbol{\epsilon}$ for $\hat{\boldsymbol{x}</p>
<p>$\boldsymbol{\epsilon}=h(\hat{\boldsymbol{x}}),$ (2)</p>
<p>we will be able to determine whether $\hat{\boldsymbol{x}}$ is normal or not according to $|\boldsymbol{\epsilon}|_{1}$. The challenge is that there is no available information about $\tilde{\mathcal{X}}$ in the training stage and we can only utilize $\mathcal{X}$.</p>
<p>Although $\tilde{\mathcal{X}}$ is unknown, we further theoretically partition $\tilde{\mathcal{X}}$ into two subsets without overlap, i.e.,</p>
<p>$\tilde{\mathcal{X}}\triangleq\tilde{\mathcal{X}}<em H="H">{E}\cup\tilde{\mathcal{X}}</em>},\ \tilde{\mathcal{X}<em H="H">{E}\cap\tilde{\mathcal{X}}</em>}=\emptyset,\tilde{\mathcal{X}<em E="E">{E}\sim\tilde{\mathcal{D}}</em>},\tilde{\mathcal{X}<em H="H">{H}\sim\tilde{\mathcal{D}}</em>.$ (3)</p>
<p>$\tilde{\mathcal{X}}<em E="E">{E}$ denotes an easy set, drawn from the easy part $\tilde{\mathcal{D}}</em>|}$ of $\tilde{\mathcal{D}}$, in which $|\boldsymbol{\epsilon<em H="H">{1}$ for each sample is sufficiently large, while $\tilde{\mathcal{X}}</em>}$ denotes a hard set, drawn from the hard part $\tilde{\mathcal{D}<em 1="1">{H}$ of $\tilde{\mathcal{D}}$, in which $|\boldsymbol{\epsilon}|</em>}$ for each sample is small. After the partition, we can assert that $\tilde{\mathcal{X}<em E="E">{H}$ is closer to the normal data. Consequently, it is easier for a model to recognize samples in $\tilde{\mathcal{X}}</em>}$ than those in $\tilde{\mathcal{X}<em 1="1">{H}$, as the $|\boldsymbol{\epsilon}|</em>}$ values of samples in $\tilde{\mathcal{D}<em H="H">{E}$ are significantly larger than those in $\tilde{\mathcal{D}}</em>$. Figure 1 provides an intuitive example. The ultimate goal is to learn the decision boundary around the normal data.</p>
<p>Here, we focus on how to detect the samples in $\tilde{\mathcal{X}}<em H="H">{H}$ or drawn from $\tilde{\mathcal{D}}</em>$, i.e.,}$. Since $\tilde{\mathcal{X}}_{H}$ is very close to the normal data, it is reasonable to hypothesize that hard anomalies are special cases of perturbed samples of normal data. We propose to generate a noisy dataset $\hat{\mathcal{X}}\subset\mathbb{R}^{d}$ from $\mathcal{X}$ by adding various noise to $\mathcal{X}$, and assume $\hat{\mathcal{X}}$ is drawn from certain perturbed distribution $\tilde{\mathcal{D}</p>
<p>$\hat{\mathcal{X}}\leftarrow\operatorname{Gen}(\mathcal{X})\sim\tilde{\mathcal{D}},$ (4)</p>
<p>where Gen denotes the noisy data generator and $|\hat{\mathcal{X}}|\gg N$. Let the diversity of added noise is sufficiently large, such that</p>
<p>$\tilde{\mathcal{X}}_{H}\subset\hat{\mathcal{X}},$ (5)</p>
<p>i.e., anomaly patterns of the hard set $\tilde{\mathcal{X}}<em H="H">{H}$ are included in $\hat{\mathcal{X}}$. Even if (5) does not hold, as shown by Theorem 1, it is still possible to obtain correct detection, provided that $\tilde{\mathcal{D}}$ is not too far from $\tilde{\mathcal{D}}</em>$, i.e.,</p>
<p>$\operatorname{dist}(\tilde{\mathcal{D}},{\tilde{\mathcal{D}}_{H}})\leq\gamma,$ (6)</p>
<p>where $\operatorname{dist}(\cdot,\cdot)$ is some distance or divergence measure between two distributions and $\gamma$ is not too large. Therefore, a model $h$ learned from $\hat{\mathcal{X}}$ is able to generalize to $\tilde{\mathcal{X}}_{H}$ and then detect anomaly.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The training process of noise evaluation model. Noise with 0 mean and $\sigma$ standard deviation is added to the original data $\boldsymbol{x}$ to create noised versions $\hat{\boldsymbol{x}}=\boldsymbol{x}+\boldsymbol{\epsilon}$. The model $h_{\boldsymbol{\theta}}$ is trained to discern the zero vector for the original data and identify the noise vector $|\boldsymbol{\epsilon}|$ for the noised data. The final anomaly decision is made using an aggregation function $g(\cdot)$, where high-magnitude noise indicates abnormality.</p>
<h3>Noise Evaluation Model</h3>
<p>To generate $\hat{\mathcal{X}}$, we add random noise to the elements of each sample $\boldsymbol{x}\in\mathcal{X}$. This operation will reduce the quality of the data, that is, the quality of $\hat{\mathcal{X}}$ is lower than that of $\mathcal{X}$, supported by</p>
<p><strong>Proposition 1.</strong> <em>Adding random noises independently to the entries of $\mathcal{X}$ makes the data more disordered (higher entropy).</em></p>
<p>We would like to learn a deep neural network $h(\cdot):\mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ parameterized by $\boldsymbol{\theta}$ to quantify the quality of the input data. Its output is a vector with the same size as the input while each entry tells whether the input feature is noised or not. We generate the noised dataset by</p>
<p>$$
\hat{\mathcal{X}} = \hat{\mathcal{X}}<em 2="2">{1} \cup \hat{\mathcal{X}}</em>
$$} \dots \cup \hat{\mathcal{X}}_{K}, \tag{7</p>
<p>where each $\hat{\mathcal{X}}_{k}$ is composed of the samples generated by</p>
<p>$$
\hat{\boldsymbol{x}} = \boldsymbol{x} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \text{Noise}<em k="k">{k}(\mathbf{0}, \boldsymbol{\sigma}</em>
$$}), \quad \boldsymbol{x} \in \mathcal{X}. \tag{8</p>
<p>In (8), Noise<sub>k</sub>(0, σ<sub>k</sub>) (to be detailed in the next section) is a multivariate noise distribution with 0 mean value and σ<sub>k</sub> standard deviation in $\mathbb{R}^{d}$, abbreviated as Noise(σ<sub>k</sub>). For instance, Noise<sub>k</sub> can be a Gaussian distribution. We also denote the standard deviation of the noise as <em>noise level</em> <sup>1</sup>. According to (7) and (8), we see that $\hat{\mathcal{X}}$ can contain different types of noise distributions with different standard deviations, and the diversity is controlled by $\sum_{k=1}^{K} | \sigma_k |$. Refer to Table 3 for the choices of the noise generator. Then, we write the learning objective as</p>
<p>$$
\min_{\boldsymbol{\theta}} \sum_{\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">i \in \mathcal{X}} | h</em>}}(\boldsymbol{x<em 2="2">i) - \mathbf{0} |</em>}^{2} + \sum_{\hat{\boldsymbol{x}<em _boldsymbol_theta="\boldsymbol{\theta">i \in \hat{\mathcal{X}}} | h</em>
$$}}(\hat{\boldsymbol{x}}_i) - |\boldsymbol{\epsilon}_i| |_2^2, \tag{9</p>
<p>where 0 is a d-dimension vector with all zero values, ε is drawn from some Noise(0, σ), and $\hat{\boldsymbol{x}} = \boldsymbol{x} + \boldsymbol{\epsilon}$ for some</p>
<p><sup>1</sup>We use noise level and standard deviation of noise interchangeably in this paper.</p>
<p>$\boldsymbol{x} \in \mathcal{X}$. Note that in (9), there is no hyperparameter to determine except the network structure, while many previous methods such as (Ruff et al. 2018; Goyal et al. 2020; Cai and Fan 2022; Zhang et al. 2024) have at least one more crucial hyperparameter to tune in the learning objective.</p>
<p>In (9), the absolute value is used to let the model output a positive signal indicating an anomaly. Here, we only require the model to predict how much the noise is instead of the exact noise value, which also reduces the difficulty of the learning process. In addition, our method is closely related to the denoising score matching (Song and Ermon 2019; Vincent 2011), which estimates the gradient of the probability density of the data distribution. We prove (9) is a lower bound of denoising score matching learning objective in Appendix A. Hence, our method also learns data distribution $\mathcal{D}$ by proxy. Predicting the noise magnitude has the following advantages.
Claim 1. Estimating the element-wise magnitude of noise enables the model to quantify the deviation of a sample from the normal data distribution $\mathcal{D}$. It improves the ability to identify specific features with abnormalities.</p>
<p>Since the model output is a $d$-dimension vector, we introduce an aggregation operation after the training, $g(\cdot)$, to map the output vector to a scalar for anomaly detection. The operation in our design can be maximum, minimum, mean, median, or a combination. The final anomaly score is determined by</p>
<p>$$
\operatorname{score}(\boldsymbol{x}):=g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)
$$</p>
<p>Taking the maximum as an example, we have $\operatorname{score}(\boldsymbol{x})=$ $\max <em _boldsymbol_theta="\boldsymbol{\theta">{i \in[d]}\left[h</em>$ is anomalous. An overview of our model is shown in Figure 2.}}(\boldsymbol{x})\right]_{i}$. We need to determine a threshold $\tau&gt;0$ for the anomaly score. If $\operatorname{score}(\boldsymbol{x})&gt;\tau, \boldsymbol{x</p>
<h2>Noise Generation and Model Training</h2>
<p>The noise generation in our design is not arbitrary. To cover the sampling space of the noised distributions as much as possible, we randomly generate the noise for each training sample in terms of noise level and position. An intuitive example is as follows:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>For one type of distribution (e.g. Gaussian), there are often two hyper-parameters to control the noise generation process. The first one is $\sigma_{\max }$, denoting the maximum noise level. The other is $m$, describing how many parts of the feature vector are added by the same level of noise. Therefore, for an $\boldsymbol{x}$, different elements may be corrupted by different levels of noise, and, if necessary, one $\boldsymbol{x}$ can produce multiple $\hat{\boldsymbol{x}}$ with different types of distribution. A detailed process for generating noised samples on a batch of data is in Algorithm 1. According to Algorithm 1, the noise generation time complexity is $\mathcal{O}(b d)$. In contrast, other methods involving perturbation (Cai and Fan 2022; Qiu et al. 2021) and adversarial sample (Goyal et al. 2020) have time complexity with $\mathcal{O}(b d W)$, where $W$ is workload related to a neural network module. Compared with them, our noise generation is more efficient, where no</p>
<p>Algorithm 1: Noise Generation
Input: maximum noise level $\sigma_{\max }$, number of noise distributions $m$, batch size $b$, the dimensionality of data d.
$\Delta \leftarrow\left[0, \frac{1}{m} \sigma_{\max }, \frac{2}{m} \sigma_{\max }, \ldots, \frac{m}{m} \sigma_{\max }\right] \quad \triangleright$ make $m$ intervals
2: Initialize $\mathcal{E}$ with empty
3: for $j \in{1, \ldots, b}$ do
4: for $i \in{1, \ldots, m}$ do
5: //random noise level
6: $\quad \hat{\sigma} \leftarrow \operatorname{Uniform}(\Delta[i], \Delta[i+1])$
7: $\quad \boldsymbol{\epsilon}\left[\frac{i-1}{m} d: \frac{i}{m} d\right] \leftarrow$ Noise $(0, \hat{\sigma})$ //generate noise from $m$ noise levels for $m$ parts
8: end for
9: //shuffle position of noise elements
10: $\quad \boldsymbol{\epsilon} \leftarrow$ Shuffle $(\boldsymbol{\epsilon})$
11: $\quad \mathcal{E}[j] \leftarrow \boldsymbol{\epsilon}$
12: end for
Output: Generated noise $\mathcal{E}$ for a batch of input data
learnable parameter is required. A comparative study on time cost is shown in Appendix I.</p>
<p>In Figure 2, the lower pathway illustrates the noise synthesis mechanism, where a noise vector $\boldsymbol{\epsilon}$ (mean 0 , standard deviation $\boldsymbol{\sigma}$ ) is randomly generated and added to the input $\boldsymbol{x}$, producing a noise-augmented variant $\hat{\boldsymbol{x}}=\boldsymbol{x}+\boldsymbol{\epsilon}$. Multiple noised samples can be generated from a single input. Both $\boldsymbol{x}$ and $\hat{\boldsymbol{x}}$ are processed by the noise evaluation network $h_{\theta}$, optimized to regress towards zero for $\boldsymbol{x}$ and estimate the noise vector $|\boldsymbol{\epsilon}|$ for $\hat{\boldsymbol{x}}$. Training details are in Appendix B, Algorithm 2. Notice that for each training epoch, we randomly generate a new noised instance for each training sample. This helps us enlarge the sampling number from the noise distribution, and avoid over-fitting on some ineffective noise. There are some optional noise generation schemes such as using different noise types, different noise levels, and different noise ratios, which are studied later. These options add several hyper-parameters to our methods. However, it is still simpler than many recent methods. We compare them in Appendix J.</p>
<h2>Theoretical Analysis</h2>
<p>In the proposed method, we learn a one-class classification decision boundary closely around the normal samples. In Figure 1, we divide the anomaly region into two non-overlap distributions, easy $\mathcal{\mathcal { D }}<em H="H">{E}$, and hard $\mathcal{\mathcal { D }}</em>}$, respectively. In this section, we theoretically show the anomaly detection ability of the model meeting easy anomaly samples $\mathcal{\mathcal { X }<em E="E">{E}$ from $\mathcal{\mathcal { D }}</em>}$ and hard anomaly samples $\mathcal{\mathcal { X }<em H="H">{H}$ from $\mathcal{\mathcal { D }}</em>$ in Theorem 1 and Theorem 2, respectively.</p>
<p>Without loss of generality, we let $h_{\boldsymbol{\theta}} \in \mathcal{H}$, where $\mathcal{H}$ is the hypothesis space of ReLU-activated neural network with $L$ layers and the number of neurons at each layer is in the order of $p$. We give the following definition.
Definition 1. For the noise evaluation hypothesis $h_{\theta}$, the risk of the hypothesis on a distribution $\mathcal{D}$ is defined by</p>
<p>the probability according to $\mathcal{D}$ that a processed hypothesis $I\left(g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)&gt;\tau\right)$ disagrees with a labeling function $\rho: \mathbb{R}^{d} \rightarrow{0,1}$. Mathematically,</p>
<p>$$
\varepsilon_{\mathcal{D}}(h):=\mathbb{E}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{x} \sim \mathcal{D}}\left[\mid I\left(g\left(h</em>)\right]
$$}}(\boldsymbol{x})\right)&gt;\tau\right)-\rho(\boldsymbol{x</p>
<p>where $I(\cdot)$ is the indicator function, $\tau&gt;0$ is some threshold, and $\rho(\cdot)$ is the true labeling function.</p>
<p>Definition 1 is a form of the disagreement metric (Hanneke et al. 2014). Based on Definition 1 and results of (Ben-David et al. 2010; Bartlett et al. 2019), we can provide the following theoretical guarantee (proved in Appendix C) for the hard anomalous data $\hat{\mathcal{X}}<em _hat_mathcal_D="\hat{\mathcal{D">{H}$.
Theorem 1. (Generalization Error Bound) Let $\varepsilon</em>}<em _hat_mathcal_D="\hat{\mathcal{D">{H}}(h)$ and $\varepsilon</em>}}}(h)$ be the risks of hypothesis $h$ on $\hat{\mathcal{D}<em H="H">{H}$ and $\hat{\mathcal{D}}$, respectively. If $\hat{\mathcal{X}}$ and $\hat{\mathcal{X}}</em>}$ are unlabeled samples of size $N$ each, drawn from $\hat{\mathcal{D}}$ and $\hat{\mathcal{D}<em _hat_mathcal_D="\hat{\mathcal{D">{H}$ respectively, then for any $\delta \in(0,1)$, with probability at least $1-\delta$, for every $h \in \mathcal{H}$ :
$\varepsilon</em>}<em _hat_mathcal_D="\hat{\mathcal{D">{H}}(h) \leq \varepsilon</em>}}}(h)+\frac{1}{2} \hat{d<em H="H">{\mathcal{H} \Delta \mathcal{H}}\left(\hat{\mathcal{X}}</em>+\lambda$,
where $d_{v c}=\mathcal{O}(p L \log (p L)) . \lambda=\varepsilon_{\hat{\mathcal{D}}}\left(h^{}, \hat{\mathcal{X}}\right)+4 \sqrt{\frac{2 d_{v c} \log (2 N)+\log \left(\frac{2}{\delta}\right)}{N}<em>}\right)+\varepsilon_{\hat{\mathcal{D}}_{H}}\left(h^{</em>}\right)$ where $h^{*}$ is the ideal joint hypothesis that minimize $\varepsilon_{\hat{\mathcal{D}}}(h)+$ $\varepsilon_{\hat{\mathcal{D}}_{H}}(h)$.</p>
<p>The definition of the divergence $\hat{d}<em _mathcal_H="\mathcal{H">{\mathcal{H} \Delta \mathcal{H}}$ is shown in Appendix C for simplicity. As shown in Figure 1, Theorem 1 describes that if the divergence $\hat{d}</em>} \Delta \mathcal{H}}$ between the perturbed training data $\hat{\mathcal{X}}$ and the hard test data $\hat{\mathcal{X}<em _boldsymbol_theta="\boldsymbol{\theta">{H}$ is small, the model can correctly identify the anomaly. In other words, the generated data $\hat{\mathcal{X}}$ can be very useful for learning a reasonable detection model $h</em>}}$. Since there is no available information about $\hat{\mathcal{X}<em _mathcal_H="\mathcal{H">{H}$ during the training, we cannot obtain the divergence $\hat{d}</em>} \Delta \mathcal{H}}$ in real practice. Hence, we always standardize the normal training data and make the added noise level relatively small. In the ablation study, we show that a large noise level is harmful to the training. Note that this guarantee does not apply to $\hat{\mathcal{X}<em E="E">{E}$ because $\hat{\mathcal{D}}</em>}$ may be very far from $\hat{\mathcal{D}}$. The following context will provide a guarantee for detecting $\hat{\mathcal{X}<em E="E">{E}$. We make the following assumption and present the theoretical result (proved in Appendix C).
Assumption 1. For any $\boldsymbol{x}$ and $\tilde{\boldsymbol{x}}$ drawn from $\mathcal{D}$ and $\tilde{\mathcal{D}}</em>)\right)\right|$.
Theorem 2. Let $d_{\text {min }}=\inf }$ respectively, there exists a constant $c&gt;0$ such that $c | \boldsymbol{x}-$ $\tilde{\boldsymbol{x}} | \leq\left|g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)-g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}<em E="E">{\boldsymbol{x} \sim \mathcal{D}, \tilde{\boldsymbol{x}} \sim \tilde{\mathcal{D}}</em>}}|\boldsymbol{x}-\tilde{\boldsymbol{x}}|$ (shown by Figure 1). Suppose $g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)&lt;\epsilon$ for any $\boldsymbol{x} \in \mathcal{D}$, where $\epsilon \geq 0$. Then, under Assumption 1, any anomalous samples drawn from $\tilde{\mathcal{D}<em _min="\min">{E}$ can be successfully detected if $d</em>&gt;\max {\epsilon / c, \tau / c}$.</p>
<p>This theorem provides a theoretical guarantee for our method to detect any anomalous sample in $\hat{\mathcal{X}}<em E="E">{E}$ or drawn from $\tilde{\mathcal{D}}</em>$ is $1 / L$-Lipschitz, meaning that $c=\alpha L$, where
$\alpha=\inf }$ (depicted by the blue double-headed arrow in Figure 1) as anomaly successfully. When $c$ is larger, the detection is easier. We can derive a bound for $c$ with more assumptions: if the learned model $h$ is bijective and $L$-Lipschitz, then $h^{-1<em j="j">{\boldsymbol{x} \in \operatorname{dom}(g)}|\nabla g(z)|$. Using an invertible neural network (Behrmann et al. 2019) could achieve this. For an $h$ with $q$ layers, spectral norm $\left|\mathbf{W}</em>\right|<em i="1">{\sigma}$ for layer $i$, and $\rho$-Lipschitz activation, we have $c=\alpha \prod</em>}^{q} \rho^{q}\left|\mathbf{W<em _sigma="\sigma">{i}\right|</em>$. In our experiments, we used both MLP and ResMLP without dimensionality reduction. According to Theorem 1 of (Behrmann et al. 2019), ResMLP is invertible if each layer's Lipschitz constant is under 1, which is easy to ensure.}^{q</p>
<p>To sum up, Theorem 1 and Theorem 2 provide guarantees for detecting $\mathcal{X}<em E="E">{H}$ and $\mathcal{X}</em>$, although we never use any real anomalous data in the training stage.}$ respectively. Therefore, our method is theoretically guaranteed to detect $\hat{\mathcal{X}</p>
<h2>Experiments</h2>
<h2>Experimental Settings</h2>
<p>Datasets We evaluate our method in two common settings: unsupervised anomaly detection and one-class classification. In the anomaly detection setting, where anomalous samples are few, we use 47 real-world tabular datasets ${ }^{2}$ from (Han et al. 2022), covering domains like healthcare, image processing, and finance. For one-class classification setting, we collected 25 benchmark tabular datasets used in previous works (Pang et al. 2021; Shenkar and Wolf 2022). The raw data was sourced from the UCI Machine Learning Repository (Kelly, Longjohn, and Nottingham) and their official websites. For categoric value, we use a one-hot encoding. We test on all classes in multi-class datasets, reporting the average performance score per class, which is similar to oneclass classification on image dataset (Cai and Fan 2022). For datasets with validation/testing sets, we train on all normal samples. If only a training set is available, we randomly split $50 \%$ of normal samples for training and use the rest with anomalous data for testing. Data is standardized using the training set's mean and standard deviation. Dataset details are in Table 4 of Appendix D.</p>
<p>Baseline Methods We select 12 baseline methods for comparative analysis, including probabilistic-based, proximitybased, deep neural network-based, ensemble-based methods, and recent UAD methods that can be applied to tabular data. They are Isolated Forest (IForest) (Liu, Ting, and Zhou 2008), COPOD (Li et al. 2020), auto-encoder (AE) (Aggarwal 2016), KNN (Angiulli and Pizzuti 2002), Local Outlier Factor (LOF) (Breunig et al. 2000), DeepSVDD (Ruff et al. 2018), AnoGAN (Schlegl et al. 2017), ECOD (Li et al. 2023), SCAD (Shenkar and Wolf 2022), NeuTraLAD (Qiu et al. 2021), PLAD (Cai and Fan 2022), and DPAD (Fu, Zhang, and Fan 2024). For DPAD, PLAD, SCAD, and NeuTraLAD, we use the code provided by the authors. For the other baseline methods, we utilize PyOD, a Python library developed by (Zhao, Nasrullah, and Li 2019). The default settings are adopted. We repeat 10 times for each baseline.</p>
<p>Implementation All experiments are implemented by Pytorch (Paszke et al. 2017) on NVIDIA Tesla V100 and Intel Xeon Gold 6200 platform. We utilize two network architectures for the evaluation, VanillaMLP (MLP) and ResMLP.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />Figure 3: Comparison of different $g(\cdot)$, i.e. mean, maximum, and minimum, on KDD-CUP99, at each optimization epoch.</p>
<p>VanillaMLP is a plain ReLU-activated feed-forward neural network with 4 fully connected layers. ResMLP has a similar architecture to <em>Touvron et al. (2022)</em> but has a simpler structure. Detailed architectures are in Appendix E. The network model is optimized by AMSGrad <em>Reddi et al. (2018)</em> with $10^{-4}$ learning rate and $5\times 10^{-4}$ weight decay. In the results, we adopt Gaussian noise in the noise generation, maximum noise level $\sigma_{\max}=2$, and the number of different noise distributions $m=3$. To enlarge the noised sample number, we generate 3 noise-augmented instances for the same input instance with 3 different noise ratios, $0.5$, $0.8$, and $1.0$ at each epoch. Unless specified, we train the model for $500$ epochs and manually decay the learning rate at the 100-th epoch in a factor of 0.1.</p>
<p>Performance Metric Since the output of $h$ is a noise evaluation vector, an operation $g(\cdot)$ is introduced, as explained by (10). Here we select the maximum. A comparative result on KDD-CUP99 among maximum, minimum, and mean is shown in Figure 3 where we train the model for 300 epochs. The maximum reaches the highest performance and the fastest convergence speed. For the performance metric, we report the area under the ROC curve (AUC) and F1 score. The calculation of the F1 score and the determination of anomaly threshold $\tau$ are consistent with <em>Shenkar and Wolf (2022); Qiu et al. (2021)</em>.</p>
<h3>Unsupervised Anomaly Detection Results</h3>
<p>The average result under the unsupervised anomaly detection setting of ten runs is reported in Figure 4. We conducted a comparative analysis of 11 tabular UAD methods across 47 datasets, evaluating average AUC, average F1, average ranking in AUC, and average ranking in F1. It is seen that our methods not only significantly outperform the AE methods, but also reach the highest ranking. The detailed results on each dataset and p-value from paired t-test are reported in Appendix F (Tables 6 and 7), which emphasizes the statistical significance of the improvements achieved by our methods.</p>
<h3>One-Class Classification Results</h3>
<p>We compared our noise evaluation method with 12 baseline methods on the OCC dataset setting. The results in Table 2 show that our anomaly detection techniques, Ours-ResMLP and Ours-MLP, consistently outperform baselines across various tabular datasets, with mean AUC values of $92.68 \pm 11.10$ and $92.27 \pm 11.1$, indicating greater effectiveness and lower variability. While traditional methods like IForest and KNN perform well, our methods excel, especially on complex datasets like musk and optdigits. Though our methods may not always lead in AUC, the difference is minimal, around $1 \%$. For faster inference, the MLP model is recommended. The last two lines in the table are the p-values of paired ttest of our two methods (ResMLP and MLP) against each baseline method. The paired t-test is based on 25 pairs (as there are 25 datasets). Our approach also achieved the highest average F1 score and rank, $94.18 \%$ and 1.52, as detailed in Appendix G.</p>
<p>Compared with many popular UAD methods, our noise evaluation method has the following advantages:</p>
<ul>
<li>Noise evaluation shows superior performance in the extensive empirical experiments, indicating our method can accurately identify anomalies in tabular data.</li>
<li>The generalization and anomaly detection ability of noise evaluation can be theoretically guaranteed, whereas many deep learning based methods lack such guarantees <em>Hussain et al. (2023)</em>.</li>
<li>The implementation of our method is easier. The perturbed sample generation can be pre-generated without extra training. In addition, no hyper-parameter is tuned in the learning objective.</li>
</ul>
<table>
<thead>
<tr>
<th>Type</th>
<th>Parameter</th>
<th>Value</th>
<th>Offset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian</td>
<td>$\mu, \sigma$</td>
<td>$\mu=0, \sigma=\hat{\sigma}$</td>
<td>0</td>
</tr>
<tr>
<td>Laplace</td>
<td>$\mu, \sigma$</td>
<td>$\mu=0, \sigma=\hat{\sigma}$</td>
<td>0</td>
</tr>
<tr>
<td>Uniform</td>
<td>$a, b$</td>
<td>$a=-\sqrt{3} \hat{\sigma}, b=\sqrt{3} \hat{\sigma}$</td>
<td>0</td>
</tr>
<tr>
<td>Rayleigh</td>
<td>$s$</td>
<td>$s=\sqrt{\frac{2}{4-\pi}} \hat{\sigma}$</td>
<td>$-\sqrt{\frac{\pi}{4-\pi}} \hat{\sigma}$</td>
</tr>
<tr>
<td>Gamma</td>
<td>$\alpha, \beta$</td>
<td>$\alpha=\sqrt{\frac{1}{2}} \hat{\sigma}, \beta=\beta$</td>
<td>$-\sqrt{\beta} \hat{\sigma}$</td>
</tr>
<tr>
<td>Poisson</td>
<td>$\lambda$</td>
<td>$\lambda=\hat{\sigma}$</td>
<td>$-\hat{\sigma}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Eight different noise types are adopted. We adjust the parameter to make it 0 mean value and $\hat{\sigma}$ standard deviation, where $\hat{\sigma}$ is the designated noise level. If the noise is non-negative by default, we offset its mean to 0 . For the Gamma noise, $\beta$ is a non-negative hyper-parameter, where we evaluate $\beta=1$ and $\beta=3$. For Salt\&amp;Pepper and Bernoulli noise, we generate a probability vector based on a uniform distribution. Then, generate a binary vector using the probability vector. If there is noise, we change the value into the maximum or minimum value in the batch or flip the sign of the element of $\boldsymbol{x}$.</p>
<h3>Ablation Study</h3>
<p>As discussed in the Proposed Method Section, we have several alternatives. In this section, we study how different noise levels, noise ratios, and noise types affect the performance of our method. We utilize the OCC setting to perform the ablation study. Detailed results are shown in Appendix H.</p>
<p>Sensitivity of Different Noise Levels To explore how different noise levels affect performance, we use Gaussian</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: AUC (\%) and F1 (\%) score of the proposed method compared with 11 baselines on 47 benchmark datasets. Each experiment is repeated 10 times with random seed from 0 to 9 , and mean value and $95 \%$ confidence interval are reported. Rank (the lower the better) is calculated out of 12 tested methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">DPAO <br> (2024)</th>
<th style="text-align: center;">PLAD <br> (2022)</th>
<th style="text-align: center;">NeuTraLAD <br> (2021)</th>
<th style="text-align: center;">SCAD <br> (2022)</th>
<th style="text-align: center;">AE</th>
<th style="text-align: center;">AnoGAN</th>
<th style="text-align: center;">COPOD</th>
<th style="text-align: center;">DeepSVDD</th>
<th style="text-align: center;">$\begin{gathered} \text { ECOD } \ (2023) \end{gathered}$</th>
<th style="text-align: center;">IForest</th>
<th style="text-align: center;">KNN</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">Ours- <br> ResMLP</th>
<th style="text-align: center;">Ours- <br> MLP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">abalone</td>
<td style="text-align: center;">70.64 $\pm 13.2$</td>
<td style="text-align: center;">64.23 $\pm 15.4$</td>
<td style="text-align: center;">45.29 $\pm 8.8$</td>
<td style="text-align: center;">63.90 $\pm 14.7$</td>
<td style="text-align: center;">71.69 $\pm 11.1$</td>
<td style="text-align: center;">68.65 $\pm 14.7$</td>
<td style="text-align: center;">43.57 $\pm 15.0$</td>
<td style="text-align: center;">59.78 $\pm 16.1$</td>
<td style="text-align: center;">45.00 $\pm 16.9$</td>
<td style="text-align: center;">71.55 $\pm 12.2$</td>
<td style="text-align: center;">69.11 $\pm 14.1$</td>
<td style="text-align: center;">68.89 $\pm 13.0$</td>
<td style="text-align: center;">76.90 $\pm 11.0$</td>
<td style="text-align: center;">76.78 $\pm 10.9$</td>
</tr>
<tr>
<td style="text-align: center;">arrhythmia</td>
<td style="text-align: center;">76.38 $\pm 1.0$</td>
<td style="text-align: center;">63.15 $\pm 5.1$</td>
<td style="text-align: center;">52.13 $\pm 0.0$</td>
<td style="text-align: center;">69.60 $\pm 2.0$</td>
<td style="text-align: center;">76.12 $\pm 1.8$</td>
<td style="text-align: center;">69.61 $\pm 2.6$</td>
<td style="text-align: center;">75.23 $\pm 1.4$</td>
<td style="text-align: center;">72.82 $\pm 2.2$</td>
<td style="text-align: center;">75.21 $\pm 1.5$</td>
<td style="text-align: center;">77.06 $\pm 1.8$</td>
<td style="text-align: center;">75.87 $\pm 2.0$</td>
<td style="text-align: center;">75.85 $\pm 1.9$</td>
<td style="text-align: center;">77.34 $\pm 2.4$</td>
<td style="text-align: center;">76.77 $\pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">bexarox</td>
<td style="text-align: center;">98.36 $\pm 0.3$</td>
<td style="text-align: center;">87.4 $\pm 15.0$</td>
<td style="text-align: center;">78.77 $\pm 3.0$</td>
<td style="text-align: center;">97.29 $\pm 0.7$</td>
<td style="text-align: center;">98.83 $\pm 0.4$</td>
<td style="text-align: center;">99.11 $\pm 0.3$</td>
<td style="text-align: center;">99.29 $\pm 0.2$</td>
<td style="text-align: center;">95.58 $\pm 1.1$</td>
<td style="text-align: center;">98.62 $\pm 0.3$</td>
<td style="text-align: center;">98.36 $\pm 0.2$</td>
<td style="text-align: center;">98.84 $\pm 0.3$</td>
<td style="text-align: center;">95.43 $\pm 1.6$</td>
<td style="text-align: center;">98.49 $\pm 0.7$</td>
<td style="text-align: center;">98.84 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">cardio</td>
<td style="text-align: center;">82.13 $\pm 3.3$</td>
<td style="text-align: center;">74.82 $\pm 8.9$</td>
<td style="text-align: center;">55.84 $\pm 2.6$</td>
<td style="text-align: center;">69.37 $\pm 1.8$</td>
<td style="text-align: center;">83.36 $\pm 0.8$</td>
<td style="text-align: center;">76.78 $\pm 12.2$</td>
<td style="text-align: center;">63.98 $\pm 0.7$</td>
<td style="text-align: center;">53.12 $\pm 7.7$</td>
<td style="text-align: center;">78.03 $\pm 0.6$</td>
<td style="text-align: center;">81.74 $\pm 2.0$</td>
<td style="text-align: center;">73.96 $\pm 1.0$</td>
<td style="text-align: center;">77.37 $\pm 1.5$</td>
<td style="text-align: center;">85.59 $\pm 2.4$</td>
<td style="text-align: center;">86.01 $\pm 3.3$</td>
</tr>
<tr>
<td style="text-align: center;">ecoli</td>
<td style="text-align: center;">92.25 $\pm 4.4$</td>
<td style="text-align: center;">68.40 $\pm 2.6$</td>
<td style="text-align: center;">48.46 $\pm 12.0$</td>
<td style="text-align: center;">88.17 $\pm 5.7$</td>
<td style="text-align: center;">93.18 $\pm 4.0$</td>
<td style="text-align: center;">90.86 $\pm 6.2$</td>
<td style="text-align: center;">49.16 $\pm 22.2$</td>
<td style="text-align: center;">87.63 $\pm 5.8$</td>
<td style="text-align: center;">50.98 $\pm 11.6$</td>
<td style="text-align: center;">93.21 $\pm 3.6$</td>
<td style="text-align: center;">93.19 $\pm 4.2$</td>
<td style="text-align: center;">91.42 $\pm 5.7$</td>
<td style="text-align: center;">92.98 $\pm 3.7$</td>
<td style="text-align: center;">93.53 $\pm 3.7$</td>
</tr>
<tr>
<td style="text-align: center;">glass</td>
<td style="text-align: center;">76.73 $\pm 7.4$</td>
<td style="text-align: center;">64.8 $\pm 10.5$</td>
<td style="text-align: center;">57.68 $\pm 9.0$</td>
<td style="text-align: center;">78.89 $\pm 6.0$</td>
<td style="text-align: center;">73.39 $\pm 10.4$</td>
<td style="text-align: center;">74.76 $\pm 11.0$</td>
<td style="text-align: center;">44.61 $\pm 26.1$</td>
<td style="text-align: center;">75.39 $\pm 8.2$</td>
<td style="text-align: center;">44.75 $\pm 24.2$</td>
<td style="text-align: center;">76.96 $\pm 10.5$</td>
<td style="text-align: center;">76.66 $\pm 9.0$</td>
<td style="text-align: center;">71.09 $\pm 9.7$</td>
<td style="text-align: center;">83.12 $\pm 10.1$</td>
<td style="text-align: center;">84.41 $\pm 10.2$</td>
</tr>
<tr>
<td style="text-align: center;">ionosphere</td>
<td style="text-align: center;">96.66 $\pm 0.4$</td>
<td style="text-align: center;">64.26 $\pm 15.2$</td>
<td style="text-align: center;">90.77 $\pm 2.4$</td>
<td style="text-align: center;">96.83 $\pm 0.7$</td>
<td style="text-align: center;">90.40 $\pm 1.2$</td>
<td style="text-align: center;">86.19 $\pm 5.9$</td>
<td style="text-align: center;">80.05 $\pm 1.6$</td>
<td style="text-align: center;">95.56 $\pm 1.1$</td>
<td style="text-align: center;">74.33 $\pm 1.6$</td>
<td style="text-align: center;">89.53 $\pm 2.9$</td>
<td style="text-align: center;">97.47 $\pm 0.8$</td>
<td style="text-align: center;">95.44 $\pm 1.3$</td>
<td style="text-align: center;">97.53 $\pm 0.3$</td>
<td style="text-align: center;">97.11 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">kdd</td>
<td style="text-align: center;">81.76 $\pm 11.9$</td>
<td style="text-align: center;">90.87 $\pm 6.6$</td>
<td style="text-align: center;">93.6 $\pm 1.7$</td>
<td style="text-align: center;">91.56 $\pm 4.0$</td>
<td style="text-align: center;">95.12 $\pm 3.3$</td>
<td style="text-align: center;">92.71 $\pm 2.5$</td>
<td style="text-align: center;">76.51 $\pm 1.2$</td>
<td style="text-align: center;">72.81 $\pm 27.8$</td>
<td style="text-align: center;">78.75 $\pm 1.3$</td>
<td style="text-align: center;">96.12 $\pm 0.3$</td>
<td style="text-align: center;">94.45 $\pm 0.2$</td>
<td style="text-align: center;">85.05 $\pm 0.2$</td>
<td style="text-align: center;">95.69 $\pm 1.2$</td>
<td style="text-align: center;">96.92 $\pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">letter</td>
<td style="text-align: center;">93.39 $\pm 3.3$</td>
<td style="text-align: center;">59.05 $\pm 15.3$</td>
<td style="text-align: center;">90.95 $\pm 4.6$</td>
<td style="text-align: center;">99.26 $\pm 0.5$</td>
<td style="text-align: center;">88.91 $\pm 4.3$</td>
<td style="text-align: center;">80.23 $\pm 8.5$</td>
<td style="text-align: center;">50.03 $\pm 15.8$</td>
<td style="text-align: center;">94.25 $\pm 2.4$</td>
<td style="text-align: center;">50.09 $\pm 15.4$</td>
<td style="text-align: center;">91.15 $\pm 3.4$</td>
<td style="text-align: center;">97.95 $\pm 1.6$</td>
<td style="text-align: center;">95.61 $\pm 2.6$</td>
<td style="text-align: center;">99.42 $\pm 0.3$</td>
<td style="text-align: center;">99.42 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">lympho</td>
<td style="text-align: center;">74.21 $\pm 3.6$</td>
<td style="text-align: center;">64.96 $\pm 8.1$</td>
<td style="text-align: center;">46.6 $\pm 2.2$</td>
<td style="text-align: center;">74.91 $\pm 4.6$</td>
<td style="text-align: center;">75.84 $\pm 3.7$</td>
<td style="text-align: center;">70.15 $\pm 6.7$</td>
<td style="text-align: center;">53.38 $\pm 3.7$</td>
<td style="text-align: center;">73.37 $\pm 5.8$</td>
<td style="text-align: center;">53.52 $\pm 3.5$</td>
<td style="text-align: center;">75.14 $\pm 5.3$</td>
<td style="text-align: center;">77.86 $\pm 3.2$</td>
<td style="text-align: center;">76.50 $\pm 3.2$</td>
<td style="text-align: center;">81.14 $\pm 4.7$</td>
<td style="text-align: center;">82.57 $\pm 5.0$</td>
</tr>
<tr>
<td style="text-align: center;">mammo.</td>
<td style="text-align: center;">88.32 $\pm 1.7$</td>
<td style="text-align: center;">82.38 $\pm 2.3$</td>
<td style="text-align: center;">65.57 $\pm 2.3$</td>
<td style="text-align: center;">78.18 $\pm 2.2$</td>
<td style="text-align: center;">85.65 $\pm 1.1$</td>
<td style="text-align: center;">78.67 $\pm 13.4$</td>
<td style="text-align: center;">90.54 $\pm 0.1$</td>
<td style="text-align: center;">69.87 $\pm 7.1$</td>
<td style="text-align: center;">90.63 $\pm 0.1$</td>
<td style="text-align: center;">87.91 $\pm 0.7$</td>
<td style="text-align: center;">87.55 $\pm 0.3$</td>
<td style="text-align: center;">84.12 $\pm 1.2$</td>
<td style="text-align: center;">90.11 $\pm 0.9$</td>
<td style="text-align: center;">89.89 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">mulcross</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">99.90 $\pm 0.0$</td>
<td style="text-align: center;">76.59 $\pm 11.3$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">99.89 $\pm 0.1$</td>
<td style="text-align: center;">93.24 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">95.97 $\pm 0.1$</td>
<td style="text-align: center;">99.90 $\pm 0.1$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">musk</td>
<td style="text-align: center;">71.36 $\pm 2.9$</td>
<td style="text-align: center;">74.42 $\pm 3.3$</td>
<td style="text-align: center;">78.86 $\pm 1.6$</td>
<td style="text-align: center;">81.30 $\pm 0.7$</td>
<td style="text-align: center;">30.68 $\pm 0.6$</td>
<td style="text-align: center;">41.37 $\pm 12.2$</td>
<td style="text-align: center;">32.03 $\pm 0.4$</td>
<td style="text-align: center;">62.02 $\pm 6.0$</td>
<td style="text-align: center;">30.16 $\pm 0.4$</td>
<td style="text-align: center;">46.74 $\pm 3.7$</td>
<td style="text-align: center;">81.94 $\pm 0.4$</td>
<td style="text-align: center;">81.31 $\pm 0.6$</td>
<td style="text-align: center;">85.06 $\pm 0.5$</td>
<td style="text-align: center;">83.69 $\pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">opidigits</td>
<td style="text-align: center;">93.43 $\pm 4.3$</td>
<td style="text-align: center;">80.43 $\pm 15.5$</td>
<td style="text-align: center;">67.5 $\pm 15.0$</td>
<td style="text-align: center;">97.80 $\pm 1.8$</td>
<td style="text-align: center;">95.69 $\pm 2.6$</td>
<td style="text-align: center;">90.04 $\pm 6.6$</td>
<td style="text-align: center;">71.72 $\pm 8.8$</td>
<td style="text-align: center;">94.86 $\pm 3.7$</td>
<td style="text-align: center;">68.69 $\pm 10.1$</td>
<td style="text-align: center;">97.55 $\pm 1.7$</td>
<td style="text-align: center;">97.25 $\pm 2.2$</td>
<td style="text-align: center;">97.17 $\pm 2.3$</td>
<td style="text-align: center;">97.93 $\pm 1.7$</td>
<td style="text-align: center;">98.00 $\pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;">pendigits</td>
<td style="text-align: center;">98.04 $\pm 2.1$</td>
<td style="text-align: center;">86.45 $\pm 14.9$</td>
<td style="text-align: center;">97.23 $\pm 2.4$</td>
<td style="text-align: center;">99.65 $\pm 0.3$</td>
<td style="text-align: center;">95.46 $\pm 4.7$</td>
<td style="text-align: center;">92.56 $\pm 9.8$</td>
<td style="text-align: center;">49.90 $\pm 25.2$</td>
<td style="text-align: center;">94.25 $\pm 5.3$</td>
<td style="text-align: center;">49.93 $\pm 22.9$</td>
<td style="text-align: center;">98.08 $\pm 1.7$</td>
<td style="text-align: center;">99.59 $\pm 0.5$</td>
<td style="text-align: center;">99.11 $\pm 1.0$</td>
<td style="text-align: center;">99.82 $\pm 0.2$</td>
<td style="text-align: center;">99.84 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">pima</td>
<td style="text-align: center;">70.5 $\pm 1.7$</td>
<td style="text-align: center;">63.67 $\pm 5.7$</td>
<td style="text-align: center;">48.63 $\pm 4.6$</td>
<td style="text-align: center;">67.97 $\pm 1.3$</td>
<td style="text-align: center;">69.73 $\pm 1.5$</td>
<td style="text-align: center;">70.94 $\pm 5.0$</td>
<td style="text-align: center;">65.58 $\pm 1.0$</td>
<td style="text-align: center;">58.18 $\pm 2.9$</td>
<td style="text-align: center;">59.46 $\pm 1.3$</td>
<td style="text-align: center;">73.33 $\pm 1.4$</td>
<td style="text-align: center;">74.63 $\pm 1.0$</td>
<td style="text-align: center;">71.06 $\pm 1.5$</td>
<td style="text-align: center;">72.42 $\pm 2.6$</td>
<td style="text-align: center;">73.53 $\pm 2.9$</td>
</tr>
<tr>
<td style="text-align: center;">satimage</td>
<td style="text-align: center;">92.83 $\pm 5.1$</td>
<td style="text-align: center;">54.83 $\pm 19.4$</td>
<td style="text-align: center;">47.98 $\pm 12.0$</td>
<td style="text-align: center;">93.35 $\pm 4.8$</td>
<td style="text-align: center;">93.39 $\pm 5.4$</td>
<td style="text-align: center;">91.98 $\pm 6.8$</td>
<td style="text-align: center;">69.24 $\pm 26.8$</td>
<td style="text-align: center;">75.10 $\pm 7.7$</td>
<td style="text-align: center;">62.75 $\pm 22.8$</td>
<td style="text-align: center;">95.33 $\pm 3.8$</td>
<td style="text-align: center;">94.85 $\pm 4.4$</td>
<td style="text-align: center;">89.67 $\pm 6.6$</td>
<td style="text-align: center;">94.51 $\pm 3.6$</td>
<td style="text-align: center;">93.72 $\pm 4.7$</td>
</tr>
<tr>
<td style="text-align: center;">seismic</td>
<td style="text-align: center;">74.81 $\pm 0.8$</td>
<td style="text-align: center;">72.59 $\pm 2.8$</td>
<td style="text-align: center;">67.75 $\pm 1.2$</td>
<td style="text-align: center;">72.47 $\pm 0.8$</td>
<td style="text-align: center;">71.13 $\pm 0.5$</td>
<td style="text-align: center;">72.82 $\pm 1.5$</td>
<td style="text-align: center;">73.87 $\pm 0.5$</td>
<td style="text-align: center;">54.32 $\pm 10.2$</td>
<td style="text-align: center;">70.17 $\pm 0.4$</td>
<td style="text-align: center;">73.48 $\pm 0.6$</td>
<td style="text-align: center;">74.57 $\pm 0.5$</td>
<td style="text-align: center;">60.87 $\pm 1.6$</td>
<td style="text-align: center;">71.38 $\pm 1.3$</td>
<td style="text-align: center;">72.62 $\pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;">shuttle</td>
<td style="text-align: center;">98.93 $\pm 1.4$</td>
<td style="text-align: center;">88.55 $\pm 14.2$</td>
<td style="text-align: center;">97.42 $\pm 2.4$</td>
<td style="text-align: center;">98.30 $\pm 2.0$</td>
<td style="text-align: center;">92.49 $\pm 8.8$</td>
<td style="text-align: center;">88.42 $\pm 16.2$</td>
<td style="text-align: center;">31.08 $\pm 31.4$</td>
<td style="text-align: center;">96.51 $\pm 6.5$</td>
<td style="text-align: center;">35.53 $\pm 30.8$</td>
<td style="text-align: center;">91.55 $\pm 6.7$</td>
<td style="text-align: center;">98.66 $\pm 1.4$</td>
<td style="text-align: center;">95.60 $\pm 6.2$</td>
<td style="text-align: center;">99.35 $\pm 0.8$</td>
<td style="text-align: center;">99.40 $\pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">speech</td>
<td style="text-align: center;">54.38 $\pm 2.7$</td>
<td style="text-align: center;">57.32 $\pm 4.9$</td>
<td style="text-align: center;">56.31 $\pm 4.8$</td>
<td style="text-align: center;">57.85 $\pm 2.4$</td>
<td style="text-align: center;">47.08 $\pm 0.5$</td>
<td style="text-align: center;">50.33 $\pm 5.4$</td>
<td style="text-align: center;">49.15 $\pm 0.6$</td>
<td style="text-align: center;">52.20 $\pm 3.8$</td>
<td style="text-align: center;">47.08 $\pm 0.5$</td>
<td style="text-align: center;">46.88 $\pm 1.6$</td>
<td style="text-align: center;">48.67 $\pm 0.7$</td>
<td style="text-align: center;">49.81 $\pm 0.5$</td>
<td style="text-align: center;">57.14 $\pm 2.6$</td>
<td style="text-align: center;">58.38 $\pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">thyroid</td>
<td style="text-align: center;">91.96 $\pm 1.0$</td>
<td style="text-align: center;">97.02 $\pm 1.7$</td>
<td style="text-align: center;">80.02 $\pm 3.4$</td>
<td style="text-align: center;">86.53 $\pm 1.4$</td>
<td style="text-align: center;">83.38 $\pm 0.6$</td>
<td style="text-align: center;">72.63 $\pm 4.3$</td>
<td style="text-align: center;">78.46 $\pm 0.0$</td>
<td style="text-align: center;">57.35 $\pm 3.5$</td>
<td style="text-align: center;">79.17 $\pm 0.0$</td>
<td style="text-align: center;">90.08 $\pm 1.4$</td>
<td style="text-align: center;">91.71 $\pm 0.0$</td>
<td style="text-align: center;">88.02 $\pm 0.0$</td>
<td style="text-align: center;">97.47 $\pm 0.1$</td>
<td style="text-align: center;">96.91 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">vertebral</td>
<td style="text-align: center;">87.31 $\pm 2.6$</td>
<td style="text-align: center;">56.47 $\pm 28.6$</td>
<td style="text-align: center;">50.35 $\pm 3.5$</td>
<td style="text-align: center;">78.56 $\pm 3.3$</td>
<td style="text-align: center;">88.58 $\pm 1.3$</td>
<td style="text-align: center;">87.96 $\pm 3.2$</td>
<td style="text-align: center;">78.63 $\pm 2.0$</td>
<td style="text-align: center;">80.43 $\pm 3.8$</td>
<td style="text-align: center;">60.02 $\pm 2.9$</td>
<td style="text-align: center;">86.08 $\pm 2.3$</td>
<td style="text-align: center;">87.18 $\pm 1.0$</td>
<td style="text-align: center;">88.20 $\pm 1.5$</td>
<td style="text-align: center;">90.91 $\pm 1.4$</td>
<td style="text-align: center;">89.91 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">vowels</td>
<td style="text-align: center;">80.22 $\pm 4.1$</td>
<td style="text-align: center;">79.77 $\pm 17.1$</td>
<td style="text-align: center;">97.25 $\pm 1.1$</td>
<td style="text-align: center;">99.52 $\pm 0.2$</td>
<td style="text-align: center;">62.80 $\pm 0.8$</td>
<td style="text-align: center;">59.32 $\pm 7.8$</td>
<td style="text-align: center;">49.73 $\pm 0.8$</td>
<td style="text-align: center;">72.01 $\pm 6.5$</td>
<td style="text-align: center;">59.39 $\pm 0.7$</td>
<td style="text-align: center;">78.17 $\pm 2.2$</td>
<td style="text-align: center;">97.16 $\pm 0.4$</td>
<td style="text-align: center;">95.53 $\pm 0.7$</td>
<td style="text-align: center;">99.42 $\pm 0.3$</td>
<td style="text-align: center;">99.14 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">wbc</td>
<td style="text-align: center;">95.10 $\pm 1.1$</td>
<td style="text-align: center;">68.07 $\pm 15.0$</td>
<td style="text-align: center;">53.01 $\pm 12.6$</td>
<td style="text-align: center;">93.11 $\pm 1.3$</td>
<td style="text-align: center;">95.63 $\pm 0.6$</td>
<td style="text-align: center;">94.01 $\pm 2.5$</td>
<td style="text-align: center;">86.53 $\pm 1.0$</td>
<td style="text-align: center;">90.22 $\pm 3.3$</td>
<td style="text-align: center;">62.48 $\pm 1.4$</td>
<td style="text-align: center;">95.74 $\pm 0.8$</td>
<td style="text-align: center;">94.68 $\pm 0.9$</td>
<td style="text-align: center;">95.09 $\pm 0.9$</td>
<td style="text-align: center;">96.98 $\pm 1.1$</td>
<td style="text-align: center;">96.73 $\pm 1.1$</td>
</tr>
<tr>
<td style="text-align: center;">wine</td>
<td style="text-align: center;">95.68 $\pm 4.5$</td>
<td style="text-align: center;">67.13 $\pm 7.8$</td>
<td style="text-align: center;">43.95 $\pm 12.0$</td>
<td style="text-align: center;">86.21 $\pm 7.8$</td>
<td style="text-align: center;">97.00 $\pm 4.1$</td>
<td style="text-align: center;">94.47 $\pm 10.4$</td>
<td style="text-align: center;">49.37 $\pm 7.8$</td>
<td style="text-align: center;">95.46 $\pm 5.5$</td>
<td style="text-align: center;">49.37 $\pm 11.5$</td>
<td style="text-align: center;">96.23 $\pm 3.1$</td>
<td style="text-align: center;">97.81 $\pm 2.8$</td>
<td style="text-align: center;">97.98 $\pm 2.4$</td>
<td style="text-align: center;">98.76 $\pm 1.8$</td>
<td style="text-align: center;">98.33 $\pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">88.05 $\pm 12.4$</td>
<td style="text-align: center;">67.42 $\pm 19.6$</td>
<td style="text-align: center;">71.45 $\pm 22.6$</td>
<td style="text-align: center;">88.59 $\pm 15.3$</td>
<td style="text-align: center;">85.68 $\pm 13.2$</td>
<td style="text-align: center;">80.17 $\pm 14.8$</td>
<td style="text-align: center;">53.38 $\pm 22.4$</td>
<td style="text-align: center;">80.85 $\pm 19.1$</td>
<td style="text-align: center;">52.77 $\pm 20.9$</td>
<td style="text-align: center;">87.0 $\pm 12.8$</td>
<td style="text-align: center;">90.27 $\pm 13.5$</td>
<td style="text-align: center;">88.72 $\pm 13.4$</td>
<td style="text-align: center;">92.68 $\pm 11.0$</td>
<td style="text-align: center;">92.27 $\pm 11.1$</td>
</tr>
<tr>
<td style="text-align: center;">mean rank</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">5.96</td>
<td style="text-align: center;">6.00</td>
<td style="text-align: center;">8.12</td>
<td style="text-align: center;">10.12</td>
<td style="text-align: center;">8.68</td>
<td style="text-align: center;">10.40</td>
<td style="text-align: center;">4.96</td>
<td style="text-align: center;">3.92</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">1.68</td>
</tr>
<tr>
<td style="text-align: center;">p-value (ResMLP)</td>
<td style="text-align: center;">0.0003</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.00000</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0066</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0054</td>
<td style="text-align: center;">0.0025</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">p-value (MLP)</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0051</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0036</td>
<td style="text-align: center;">0.0022</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: AUC (\%) score of the proposed method compared with 12 baselines on 25 benchmark datasets. Each experiment is repeated 10 times with random seed from 0 to 9 , and mean value and standard deviation are reported. Mean is the average AUC score under all experiments. Mean rank (the lower the better) is calculated out of 10 tested methods. Mammo. refers to mammography.
noise and generate 3 noised instances per training batch, consistent with previous settings. We test noise levels in $[0.1,0.2,0.5,0.8,1.0,2.0,3.0,5.0]$. The results are reported in Figure 5. Results show that too small noise levels confuse the model due to the minimal distance between normal samples and anomalies, while too high noise levels expand the output value range and sampling space, reducing effectivenesses as theorem 1 suggested. Hence, the optimal noise level is around 1.0 .</p>
<p>Sensitivity of Different Noise Types We explore different noise types with a mean of 0 and a standard deviation (noise level) of $\sigma$. We use Salt\&amp;Pepper noise, Gaussian, Laplace, Uniform, Rayleigh, Gamma, Poisson, and Bernoulli distri-
butions. For Salt\&amp;Pepper and Bernoulli noise, a probability vector from a uniform distribution generates a binary vector, dictating feature value alterations by changing values or reversing signs. Other distributions are adjusted to have zero mean and $\sigma$ standard deviation. Detailed parameters are in described in Table 3, with results in Appendix H, Figure 10. Results show Salt\&amp;Pepper and Bernoulli noise performs poorly, indicating the effectiveness of our noise generation design. Gaussian, Rayleigh, and Uniform noise maintain stable performance with $92 \%$ AUC and $94 \%$ F1 scores, indicating our method's robustness with various noise types.</p>
<p>Qualitative Visualization We utilize t-Distributed Stochastic Neighbor Embedding (t-SNE) visualization (Van der</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(a) AUC and Its Rank on ResMLP
with Different Noise Levels
(b) F1 and Its Rank on ResMLP
with Different Noise Levels
(c) AUC and Its Rank on MLP with
Different Noise Levels
(d) F1 and Its Rank on MLP with Different Noise Levels</p>
<p>Figure 5: Sensitivity of Different Noise Level in $[0.1,0.2,0.5,0.8,1.0,2.0,3.0,5.0]$. The mean rank (the lower the better) is calculated out of 8 noise levels.
<img alt="img-6.jpeg" src="img-6.jpeg" />
(a) ResMLP T-SNE Visual.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) MLP T-SNE Visualization</p>
<p>Figure 6: Qualitative visualization of the penultimate layer on the KDD-CUP99 dataset shows normal (red), noised (yellow), and anomalous (blue) instances. The noised instances help the model establish a decision boundary between in-distribution and out-of-distribution data. Real anomalies consistently fall outside this boundary in both ResMLP and MLP, confirming their effectiveness in anomaly detection.</p>
<p>Maaten and Hinton 2008) to show the allocation of normal, noised, and true anomalous samples in the neural network embedding space qualitatively. In Figure 6, we visualize the penultimate layer of the neural network using KDDCUP99. It is seen that the introduction of noised instances into the dataset ostensibly aids the model in constructing a discriminative boundary that proficiently segregates the in-distribution data from its out-of-distribution counterparts. Empirical observations reveal that the actual anomalous data predominantly falls outside of this established boundary, a phenomenon consistently manifested in the experimental results for both ResMLP and MLP models.</p>
<h2>Conclusion</h2>
<p>In conclusion, we presented a novel noise evaluation-based method for unsupervised anomaly detection in tabular data. We assume the model can learn the anomalous pattern from the noised normal data. Predicting the magnitude of the noise shows inspiration on how much and where the abnormality is. We theoretically proved the generalizability and reliability of our method. Extensive experiments demonstrated that our approach outperforms other anomaly detection methods through 47 real tabular datasets in the UAD setting and 25
real tabular datasets in the OCC setting. An ablation study suggests that using Gaussian, Rayleigh, and Uniform noise has a stable performance. One potential limitation of this work is that we focused on tabular data only. Nevertheless, it is possible to extend our method to image data via the following two steps: 1) extract features of input images using a pretrained encoder; 2) apply our method to the extracted image features. This could be an interesting future work.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by the Shenzhen Science and Technology Program under the Grant No.JCYJ20210324130208022 (Fundamental Algorithms of Natural Language Understanding for Chinese Medical Text Processing) and the General Program of Guangdong Basic and Applied Basic Research under Grant No.2024A1515011771.</p>
<h2>References</h2>
<p>Abououf, M.; Singh, S.; Mizouni, R.; and Otrok, H. 2023. Explainable AI for Event and Anomaly Detection and Classification in Healthcare Monitoring Systems. IEEE Internet of Things Journal.
Aggarwal, C. C. 2016. Outlier analysis second edition.
Angiulli, F.; and Pizzuti, C. 2002. Fast outlier detection in high dimensional spaces. In European conference on principles of data mining and knowledge discovery, 15-27. Springer.
Anthony, M.; Bartlett, P. L.; Bartlett, P. L.; et al. 1999. Neural network learning: Theoretical foundations, volume 9. cambridge university press Cambridge.
Bartlett, P. L.; Harvey, N.; Liaw, C.; and Mehrabian, A. 2019. Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research, 20(1): 2285-2301.
Behrmann, J.; Grathwohl, W.; Chen, R. T.; Duvenaud, D.; and Jacobsen, J.-H. 2019. Invertible residual networks. In International conference on machine learning, 573-582. PMLR.
Ben-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.; Pereira, F.; and Vaughan, J. W. 2010. A theory of learning from different domains. Machine learning, 79: 151-175.</p>
<p>Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; and Sander, J. 2000. LOF: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, 93-104.
Cai, C.; Hy, T. S.; Yu, R.; and Wang, Y. 2023. On the connection between mpnn and graph transformer. In International Conference on Machine Learning, 3408-3430. PMLR.
Cai, J.; and Fan, J. 2022. Perturbation learning based anomaly detection. Advances in Neural Information Processing Systems, 35.
Chang, C.-H.; Yoon, J.; Arik, S. Ö.; Udell, M.; and Pfister, T. 2023. Data-efficient and interpretable tabular anomaly detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 190-201.
Chen, J.; Sathe, S.; Aggarwal, C.; and Turaga, D. 2017. Outlier detection with autoencoder ensembles. In Proceedings of the 2017 SIAM international conference on data mining, 90-98. SIAM.
Chen, Y.; Tian, Y.; Pang, G.; and Carneiro, G. 2022. Deep one-class classification via interpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 383-392.
Ding, X.; Zhao, L.; and Akoglu, L. 2022. Hyperparameter sensitivity in deep outlier detection: Analysis and a scalable hyper-ensemble solution. Advances in Neural Information Processing Systems, 35: 9603-9616.
Fan, J.; Chow, T. W. S.; and Qin, S. J. 2022. Kernel-Based Statistical Process Monitoring and Fault Detection in the Presence of Missing Data. IEEE Transactions on Industrial Informatics, 18(7): 4477-4487.
Fu, D.; Zhang, Z.; and Fan, J. 2024. Dense projection for anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 8398-8408.
Golan, I.; and El-Yaniv, R. 2018. Deep anomaly detection using geometric transformations. Advances in neural information processing systems, 31.
Goyal, S.; Raghunathan, A.; Jain, M.; Simhadri, H. V.; and Jain, P. 2020. DROCC: Deep robust one-class classification. In International conference on machine learning, 3711-3721. PMLR.
Han, S.; Hu, X.; Huang, H.; Jiang, M.; and Zhao, Y. 2022. Adbench: Anomaly detection benchmark. Advances in Neural Information Processing Systems, 35: 32142-32159.
Hanneke, S.; et al. 2014. Theory of disagreement-based active learning. Foundations and Trends ${ }^{\circledR}$ in Machine Learning, 7(2-3): 131-309.
He, Z.; Xu, X.; and Deng, S. 2003. Discovering clusterbased local outliers. Pattern recognition letters, 24(9-10): $1641-1650$.
Hendrycks, D.; Mazeika, M.; and Dietterich, T. 2018. Deep Anomaly Detection with Outlier Exposure. In International Conference on Learning Representations.
Hendrycks, D.; Mazeika, M.; Kadavath, S.; and Song, D. 2019. Using self-supervised learning can improve model robustness and uncertainty. Advances in neural information processing systems, 32.</p>
<p>Hilal, W.; Gadsden, S. A.; and Yawney, J. 2022. Financial fraud: a review of anomaly detection techniques and recent advances. Expert systems With applications, 193: 116429.
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 6840-6851.
Hu, T.; Zhang, J.; Yi, R.; Du, Y.; Chen, X.; Liu, L.; Wang, Y.; and Wang, C. 2024. Anomalydiffusion: Few-shot anomaly image generation with diffusion model. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 8526-8534.
Huang, Y.; Zhang, Y.; Wang, L.; Zhang, F.; and Lin, X. 2024. EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy. ACM KDD.
Hussain, M.; Suh, J.-W.; Seo, B.-S.; and Hong, J.-E. 2023. How Reliable are the Deep Learning-based Anomaly Detectors? A Comprehensive Reliability Analysis of Autoencoderbased Anomaly Detectors. In 2023 Fourteenth International Conference on Ubiquitous and Future Networks (ICUFN), 317-322. IEEE.
Jin, M.; Liu, Y.; Zheng, Y.; Chi, L.; Li, Y.-F.; and Pan, S. 2021. Anemone: Graph anomaly detection with multi-scale contrastive learning. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, 3122-3126.
Kelly, M.; Longjohn, R.; and Nottingham, K. ???? The UCI Machine Learning Repository. Accessed: 2023-12-27.
Kifer, D.; Ben-David, S.; and Gehrke, J. 2004. Detecting change in data streams. In VLDB, volume 4, 180-191. Toronto, Canada.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Li, Z.; Zhao, Y.; Botta, N.; Ionescu, C.; and Hu, X. 2020. COPOD: copula-based outlier detection. In 2020 IEEE international conference on data mining (ICDM), 1118-1123. IEEE.
Li, Z.; Zhao, Y.; Hu, X.; Botta, N.; Ionescu, C.; and Chen, G. H. 2023. ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. IEEE Transactions on Knowledge and Data Engineering, 35(12): 1218112193.</p>
<p>Liu, F. T.; Ting, K. M.; and Zhou, Z.-H. 2008. Isolation forest. In 2008 eighth ieee international conference on data mining, 413-422. IEEE.
Liu, Y.; Li, Z.; Zhou, C.; Jiang, Y.; Sun, J.; Wang, M.; and He, X. 2019. Generative adversarial active learning for unsupervised outlier detection. IEEE Transactions on Knowledge and Data Engineering, 32(8): 1517-1528.
Liznerski, P.; Ruff, L.; Vandermeulen, R. A.; Franks, B. J.; Kloft, M.; and Muller, K. R. 2020. Explainable Deep OneClass Classification. In International Conference on Learning Representations.
Pang, G.; Shen, C.; Cao, L.; and Hengel, A. V. D. 2021. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2): 1-38.</p>
<p>Papadimitriou, S.; Kitagawa, H.; Gibbons, P. B.; and Faloutsos, C. 2003. Loci: Fast outlier detection using the local correlation integral. In Proceedings 19th international conference on data engineering (Cat. No. 03CH37405), 315-326. IEEE.
Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Automatic differentiation in pytorch.</p>
<p>Qiu, C.; Pfrommer, T.; Kloft, M.; Mandt, S.; and Rudolph, M. 2021. Neural transformation learning for deep anomaly detection beyond images. In International conference on machine learning, 8703-8714. PMLR.
Reddi, S. J.; Kale, S.; and Kumar, S. 2018. On the Convergence of Adam and Beyond. In International Conference on Learning Representations.
Roth, K.; Pemula, L.; Zepeda, J.; Schölkopf, B.; Brox, T.; and Gehler, P. 2022. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14318-14328.
Ruff, L.; Vandermeulen, R.; Goernitz, N.; Deecke, L.; Siddiqui, S. A.; Binder, A.; Müller, E.; and Kloft, M. 2018. Deep one-class classification. In International conference on machine learning, 4393-4402. PMLR.
Saeed, M. M.; Saeed, R. A.; Abdelhaq, M.; Alsaqour, R.; Hasan, M. K.; and Mokhtar, R. A. 2023. Anomaly Detection in 6G Networks Using Machine Learning Methods. Electronics, 12(15): 3300.
Schlegl, T.; Seeböck, P.; Waldstein, S. M.; Schmidt-Erfurth, U.; and Langs, G. 2017. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, 146-157. Springer.
Schölkopf, B.; Platt, J. C.; Shawe-Taylor, J.; Smola, A. J.; and Williamson, R. C. 2001. Estimating the support of a high-dimensional distribution. Neural computation, 13(7): $1443-1471$.
Shenkar, T.; and Wolf, L. 2022. Anomaly detection for tabular data with internal contrastive learning. In International Conference on Learning Representations.
Siddiqui, M. A.; Stokes, J. W.; Seifert, C.; Argyle, E.; McCann, R.; Neil, J.; and Carroll, J. 2019. Detecting cyber attacks using anomaly detection with explanations and expert feedback. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2872-2876. IEEE.
Sohn, K.; Li, C.-L.; Yoon, J.; Jin, M.; and Pfister, T. 2020. Learning and Evaluating Representations for Deep One-Class Classification. In International Conference on Learning Representations.
Song, Y.; and Ermon, S. 2019. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32.
Song, Y.; and Ermon, S. 2020. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33: 12438-12448.</p>
<p>Stolfo, S.; Fan, W.; Lee, W.; Prodromidis, A.; and Chan, P. 1999. KDD Cup 1999 Data. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C51C7N.
Tax, D. M.; and Duin, R. P. 2004. Support vector data description. Machine learning, 54: 45-66.
Touvron, H.; Bojanowski, P.; Caron, M.; Cord, M.; El-Nouby, A.; Grave, E.; Izacard, G.; Joulin, A.; Synnaeve, G.; Verbeek, J.; et al. 2022. Resmlp: Feedforward networks for image classification with data-efficient training. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4): 53145321.</p>
<p>Tur, A. O.; Dall'Asen, N.; Beyan, C.; and Ricci, E. 2023. Exploring diffusion models for unsupervised video anomaly detection. In 2023 IEEE International Conference on Image Processing (ICIP), 2540-2544. IEEE.
Van der Maaten, L.; and Hinton, G. 2008. Visualizing data using t-SNE. Journal of machine learning research, 9(11).
Vincent, P. 2011. A connection between score matching and denoising autoencoders. Neural computation, 23(7): 16611674.</p>
<p>Wang, S.; Wang, X.; Zhang, L.; and Zhong, Y. 2021. AutoAD: Autonomous hyperspectral anomaly detection network based on fully convolutional autoencoder. IEEE Transactions on Geoscience and Remote Sensing, 60: 1-14.
Xiao, F.; Zhou, J.; Han, K.; Hu, H.; and Fan, J. 2025. Unsupervised anomaly detection using inverse generative adversarial networks. Information Sciences, 689: 121435.
Yan, X.; Zhang, H.; Xu, X.; Hu, X.; and Heng, P.-A. 2021. Learning semantic context from normal samples for unsupervised anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, volume 35, 3110-3118. Yang, X.; Latecki, L. J.; and Pokrajac, D. 2009. Outlier detection with globally optimal exemplar-based GMM. In Proceedings of the 2009 SIAM international conference on data mining, 145-154. SIAM.
Yang, X.; Qi, X.; and Zhou, X. 2023. Deep Learning Technologies for Time Series Abnormality Detection in Healthcare: A Review. IEEE Access.
Zavrtanik, V.; Kristan, M.; and Skočaj, D. 2021. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision, 8330-8339.
Zhang, X.; Li, N.; Li, J.; Dai, T.; Jiang, Y.; and Xia, S.-T. 2023. Unsupervised surface anomaly detection with diffusion probabilistic model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 6782-6791.
Zhang, Y.; Sun, Y.; Cai, J.; and Fan, J. 2024. Deep Orthogonal Hypersphere Compression for Anomaly Detection. In Proceedings of the International Conference on Learning Representations.
Zhao, Y.; Nasrullah, Z.; and Li, Z. 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of Machine Learning Research, 20(96): 1-7.
Zong, B.; Song, Q.; Min, M. R.; Cheng, W.; Lumezanu, C.; Cho, D.; and Chen, H. 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International conference on learning representations.</p>
<h2>Supplementary Material for: Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation</h2>
<h2>Appendix A: Connection Between Noise Evaluation and Denoising Score Matching</h2>
<p>We first review the denoising score matching (Song and Ermon 2019; Vincent 2011). The score of a probability density $p(\boldsymbol{x})$ is defined as $\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x})$. Score matching train a score network parametrized by $\theta, s_{\theta}(\boldsymbol{x})$, to estimate $\nabla_{\boldsymbol{x}} \log p_{\text {data }}(\boldsymbol{x})$. Denoising score matching is a variant of score matching. It first perturbs the data $\boldsymbol{x}$ with a known noise distribution $q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})$ and estimate the score of the perturbed data distribution $q_{\sigma}(\hat{\boldsymbol{x}}) \triangleq \int q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x}) p_{\text {data }}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}$. The learning objective was proved equivalent to the following:</p>
<p>$$
\min <em q__sigma="q_{\sigma">{\theta} \frac{1}{2} \mathbb{E}</em>
$$}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}\left|s_{\theta}(\hat{\boldsymbol{x}})-\nabla_{\hat{\boldsymbol{x}}} \log q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})\right|_{2}^{2</p>
<p>Here we claim optimizing the noise evaluation learning objective is a lower bound of the denoising score matching.
Proof. Without loss of generality, similar to (Vincent 2011), we consider the perturbation kernel $q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})$ satisfies</p>
<p>$$
\frac{\partial q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}{\partial \hat{\boldsymbol{x}}}=\Delta(\boldsymbol{x}-\hat{\boldsymbol{x}}), \Delta&gt;0
$$</p>
<p>If the perturbation kernel is Gaussian noise, $\Delta=\frac{1}{\sigma^{2}}$.
Let us now choose the model $p$ as</p>
<p>$$
\begin{gathered}
p_{\theta}(\boldsymbol{x})=\frac{1}{Z(\theta)} \exp \left(-f_{\theta}(\boldsymbol{x})\right) \
f_{\theta}(\boldsymbol{x})=-\Delta\left(\int h_{\theta}(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}\right), h_{\theta}(\boldsymbol{x}) \geq \mathbf{0}
\end{gathered}
$$</p>
<p>where $Z(\theta)$ is an intractable partition function and $h$ is our noise evaluation model. Then,</p>
<p>$$
s_{\theta}(\boldsymbol{x})=\frac{\partial \log p_{\theta}(\boldsymbol{x})}{\partial \boldsymbol{x}}=-\frac{f_{\theta}(\boldsymbol{x})}{\partial \boldsymbol{x}}=\Delta h_{\theta}(\boldsymbol{x})
$$</p>
<p>Since the noised sample in our method is generated as $\hat{\boldsymbol{x}}=$ $\boldsymbol{x}+\boldsymbol{\epsilon}$, then we have.</p>
<p>$$
\begin{aligned}
&amp; \frac{1}{2} \mathbb{E}<em _sigma="\sigma">{q</em>)\right|}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}\left|s_{\theta}(\hat{\boldsymbol{x}})-\nabla_{\hat{\boldsymbol{x}}} \log q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x<em q__sigma="q_{\sigma">{2}^{2} \
= &amp; \frac{1}{2} \Delta \mathbb{E}</em>)\right|}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}\left|h_{\theta}(\hat{\boldsymbol{x}})-(\boldsymbol{x}-\hat{\boldsymbol{x}<em q__sigma="q_{\sigma">{2}^{2} \
\geq &amp; \frac{1}{2} \Delta \mathbb{E}</em>|\right|}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}\left|h_{\theta}(\hat{\boldsymbol{x}})-|\boldsymbol{x}-\hat{\boldsymbol{x}<em q__sigma="q_{\sigma">{2}^{2} \
= &amp; \frac{1}{2} \Delta \mathbb{E}</em>
\end{aligned}
$$}(\hat{\boldsymbol{x}} \mid \boldsymbol{x})}\left|h_{\theta}(\hat{\boldsymbol{x}})-|\epsilon|\right|_{2}^{2</p>
<p>For the input data from the normal class, the noise magnitude is naturally 0 . According to (Song and Ermon 2019), setting $\boldsymbol{\epsilon}=\mathbf{0}$ in the learning objective makes the model can learn the underlying data distribution, i.e., $s_{\theta}(\boldsymbol{x})=\nabla_{\boldsymbol{x}} \log q_{\sigma}(\boldsymbol{x}) \approx$ $\nabla_{\boldsymbol{x}} \log p_{\text {data }}(\boldsymbol{x})$. Hence, we show that the noise evaluation learning objective is a lower bound of the denoising score matching.
Q.E.D.</p>
<p>Since the goal of our method is anomaly detection instead of sample generation, predicting the magnitude of the noise is enough to detect samples far from the $p_{\text {data }}(\boldsymbol{x})$ which are usually anomalies. For samples belonging to the normal class, it is natural that the gradient of its probability density is 0 . We provide an intuitive example here with Gaussian distribution. Suppose $x \sim \mathcal{N}(0, \sigma)$, its gradient of log density function $\frac{d}{d x} \log p(x)=-\frac{1}{\sigma^{2}} x$. We see if the magnitude of the gradient is large, the sample is far away from the distribution center 0 . In our method, we involve diverse noise with different variances because $p_{\text {data }}(\boldsymbol{x})$ is unknown and $\frac{\partial q_{\sigma}(\hat{\boldsymbol{x}} \mid \boldsymbol{x}<em _sigma="\sigma">{i})}{\partial \hat{\boldsymbol{x}}}$ and $\frac{\partial q</em>$ with different $i, j$. Since we want to enlarge such gradients, we use various noise levels. It is similar to (Song and Ermon 2019, 2020) using an annealed noise level. Therefore, noise evaluation can detect anomaly data from the perspective of score matching.}(\hat{\boldsymbol{x}} \mid \boldsymbol{x}_{i})}{\partial \hat{\boldsymbol{x}}}$ maybe very different for the same $\hat{\boldsymbol{x}</p>
<h2>Appendix B: Noise Evaluation Training Algorithm</h2>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">Noise</span><span class="w"> </span><span class="nt">Evaluation</span><span class="w"> </span><span class="nt">Training</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Training</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">X</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">max</span><span class="o">.</span><span class="w"> </span><span class="nt">iterations</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Normalize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">X</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">have</span><span class="w"> </span><span class="nt">mean</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">0</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">standard</span><span class="w"> </span><span class="nt">deviation</span>
<span class="w">        </span><span class="nt">of</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="o">;</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">1</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">while</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leq</span><span class="w"> </span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">each</span><span class="w"> </span><span class="nt">batch</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">data</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">X</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="o">//</span><span class="nt">get</span><span class="w"> </span><span class="nt">batch</span><span class="w"> </span><span class="nt">size</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">dimension</span><span class="o">.</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">b</span><span class="o">,</span><span class="w"> </span><span class="nt">d</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">shape</span><span class="o">;</span>
<span class="w">            </span><span class="o">//</span><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">.</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">E</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">NoiseGeneration</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">b</span><span class="o">,</span><span class="w"> </span><span class="nt">d</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">+</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">E</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">l</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="err">\\</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="nt">h_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{x</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="nt">-</span><span class="o">|</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\epsilon</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">|</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{x</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">B</span><span class="w"> </span><span class="p">}</span><span class="err">}}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\epsilon</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mathcal</span><span class="w"> </span><span class="err">{</span><span class="w"> </span><span class="err">E</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
<span class="w">            </span><span class="o">//</span><span class="nt">Equition</span><span class="w"> </span><span class="o">(</span><span class="nt">9</span><span class="o">).</span>
<span class="w">        </span><span class="nt">Optimize</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">by</span><span class="w"> </span><span class="nt">Adam</span><span class="w"> </span><span class="o">(</span><span class="nt">Kingma</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">Ba</span><span class="w"> </span><span class="nt">2014</span><span class="o">;</span>
<span class="w">        </span><span class="nt">Reddi</span><span class="o">,</span><span class="w"> </span><span class="nt">Kale</span><span class="o">,</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">Kumar</span><span class="w"> </span><span class="nt">2018</span><span class="o">)</span><span class="w"> </span><span class="nt">optimizer</span><span class="o">;</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">t</span><span class="o">+</span><span class="nt">1</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">while</span>
<span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="nt">Optimized</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">h_</span><span class="p">{</span><span class="err">\boldsymbol{\theta</span><span class="p">}</span><span class="err">}\</span><span class="o">);</span>
</code></pre></div>

<p>The overall training algorithm for the noise evaluation training is in Algorithm 2. Notice that for each training epoch, we randomly generate a new noised instance for each training sample. This helps us enlarge the sampling number from the noise distribution for better learning. In line 9, there are some optional noise generation schemes such as using different noise types, different noise levels, and different noise ratios.
Time and Space Complexity Let $b$ denote the batch size, $w_{\max }$ represent the maximum width of the hidden layers in an $L$-layer neural network, and $d$ indicate the dimension of the input data. Under these parameters, the time complexity of the proposed methods is at most $\mathcal{O}\left(b d w_{\max } L T\right)$, where $T$ is the maximum number of iterations. The space complexity</p>
<p>is at most $\mathcal{O}\left(b d+d w_{\max }+(L-1) w_{\max }^{2}\right)$. These complexities scale linearly with the number of samples, which underscores the scalability of the proposed methods to large datasets. Moreover, in the case of high-dimensional data (i.e., large $d$ ), choosing a smaller $w_{\max }$ can further enhance computational efficiency.</p>
<h2>Appendix C: Proof of Theoretical Analysis</h2>
<h2>Proof for Proposition 1</h2>
<p>Proof. For convenience, denote the noise variable as $E$, then $\hat{X}=X+E$. We have</p>
<p>$$
\begin{aligned}
&amp; H(\hat{X} \mid X) \
= &amp; -\sum_{x} p(X=x) \sum_{\hat{x}} p(\hat{X}=\hat{x} \mid X=x) \log p(\hat{X}=\hat{x} \mid X=x) \
= &amp; -\sum_{x} p(X=x) \
&amp; \sum_{\hat{x}} p(E=\hat{x}-x \mid X=x) \log p(E=\hat{x}-x \mid X=x) \
= &amp; -\sum_{x} p(X=x) \sum_{e^{\prime}} p\left(E=e^{\prime} \mid X=x\right) \log p\left(E=e^{\prime} \mid X=x\right) \
= &amp; H(E \mid X)
\end{aligned}
$$</p>
<p>Since $E$ and $X$ are independent, we have $H(E \mid X)=$ $H(E)$. On the other hand, we have $H(\hat{X}) \geq H(\hat{X} \mid X)$, which can be proved by the definition of conditional entropy and Jensen's inequality (or log sum inequality more specifically). Then we arrive at</p>
<p>$$
H(\hat{X}) \geq H(\hat{X} \mid X)=H(E \mid X)=H(E)
$$</p>
<p>Similarly, by symmetry, we have</p>
<p>$$
H(\hat{X}) \geq H(\hat{X} \mid E)=H(X \mid E)=H(X)
$$</p>
<p>This inequality holds for each variable in $\boldsymbol{x}$. Then the total entropy of the $d$ variables increases, which means the data becomes more disordered.
Q.E.D.</p>
<h2>Definition of $\mathcal{H}$-divergence</h2>
<p>Definition 2. (Based on Kifer et al., 2004 (Kifer, Ben-David, and Gehrke 2004)) Given a domain $\mathcal{X}$ with $D$ and $D^{\prime}$ probability distributions over $\mathcal{X}$, let $\mathcal{H}$ be a hypothesis class on $\mathcal{X}$ and denote by $I(h)$ the set for which $h \in \mathcal{H}$ is the characteristic function; that is, $x \in I(h) \Longleftrightarrow h(x)=1$. The $\mathcal{H}$-divergence between $D$ and $D^{\prime}$ is</p>
<p>$$
d_{\mathcal{H}}\left(D, D^{\prime}\right)=2 \sup <em D="D">{h \in \mathcal{H}}\left|\operatorname{Pr}</em>[I(h)]\right|
$$}[I(h)]-\operatorname{Pr}_{D^{\prime}</p>
<p>The definition is defined in (Ben-David et al. 2010). We repeat here to provide reader convenience. In addition, $\mathcal{H} \Delta \mathcal{H}$ is a symmetric difference hypothesis space, where $f \in$ $\mathcal{H} \Delta \mathcal{H}, f(\boldsymbol{x})=I(g(h(\boldsymbol{x}))&gt;\tau) \operatorname{XOR} I\left(g\left(h^{\prime}(\boldsymbol{x})\right)&gt;\tau\right)$ for some $h, h^{\prime} \in \mathcal{H}$.</p>
<h2>Proof of Theorem 1</h2>
<p>Before proving the theorem, we provide the following necessary lemma according to (Ben-David et al. 2010; Bartlett et al. 2019).
Lemma 1. Let $\mathcal{H}$ be a hypothesis space on $\mathbb{R}^{d}$ with VC dimension $d_{v c}$. $h \in \mathcal{H}$ is a ReLu-activated deep neural network with $L$ layers, and the number of neurons at each layer is in the order of $p$. If $\mathcal{X}$ and $\mathcal{X}^{\prime}$ are samples of size $N$ from two distributions $\mathcal{D}$ and $\mathcal{D}^{\prime}$ respectively and $\hat{d}_{\mathcal{H}}\left(\mathcal{X}, \mathcal{X}^{\prime}\right)$ is the empirical $\mathcal{H}$-divergence between samples, then for any $\delta \in(0,1)$, with probability at least $1-\delta$,</p>
<p>$$
d_{\mathcal{H}}\left(\mathcal{D}, \mathcal{D}^{\prime}\right) \leq \hat{d}<em c="c" v="v">{\mathcal{H}}\left(\mathcal{X}, \mathcal{X}^{\prime}\right)+4 \sqrt{\frac{d</em>
$$} \log (2 N)+\log \left(\frac{2}{\delta}\right)}{N}</p>
<p>where $d_{v c}=\mathcal{O}(p L \log (p L))$.
Proof. We slightly modify the lemma in (Ben-David et al. 2010) and Theorem 3.4 in (Kifer, Ben-David, and Gehrke 2004) with explicit definition of our neural network and specified VC dimension $\mathcal{O}(p L \log (p L))$.
Q.E.D.</p>
<p>Following (Ben-David et al. 2010), we have an additional lemma.
Lemma 2. For any hypotheses $h, h^{\prime} \in \mathcal{H}$,</p>
<p>$$
\left|\varepsilon\left(h, h^{\prime}\right)-\varepsilon\left(h, h^{\prime}\right)\right| \leq \frac{1}{2} d_{\mathcal{H} \Delta \mathcal{H}}\left(\mathcal{D}, \mathcal{D}^{\prime}\right)
$$</p>
<p>Proof. By the definition of $\mathcal{H} \Delta \mathcal{H}$-distance,</p>
<p>$$
\begin{aligned}
&amp; d_{\mathcal{H} \Delta \mathcal{H}}\left(\mathcal{D}, \mathcal{D}^{\prime}\right) \
&amp; =2 \sup <em _mathcal_D="\mathcal{D" _sim="\sim" x="x">{h, h^{\prime} \in \mathcal{H}}\left|\operatorname{Pr}</em>}}\left[h(x) \neq h^{\prime}(x)\right]-\operatorname{Pr<em h_="h," h_prime="h^{\prime">{x \sim \mathcal{D}^{\prime}}\left[h(x) \neq h^{\prime}(x)\right]\right| \
&amp; =2 \sup </em>\right)\right| .
\end{aligned}
$$} \in \mathcal{H}}\left|\varepsilon\left(h, h^{\prime}\right)-\varepsilon^{\prime}\left(h, h^{\prime}\right)\right| \geq 2\left|\varepsilon\left(h, h^{\prime}\right)-\varepsilon^{\prime}\left(h, h^{\prime</p>
<p>Q.E.D.</p>
<p>Then, we have the proof.
Proof. Define</p>
<p>$$
\varepsilon_{\mathcal{D}}\left(h, h^{<em>}\right):=\mathbb{E}<em _theta="\theta">{\boldsymbol{x} \sim \mathcal{D}}\left[\left|I\left(g\left(h</em>^{}(x)\right)&gt;\tau\right)-I\left(g\left(h_{\theta</em>}(x)\right)&gt;\tau\right)\right|\right]
$$</p>
<p>We have</p>
<p>$$
\begin{aligned}
\varepsilon_{\tilde{\mathcal{D}}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}}(h) &amp; \leq \varepsilon</em>}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}}\left(h^{<em>}\right)+\varepsilon_{\tilde{\mathcal{D}}_{H}}\left(h, h^{</em>}\right) \
&amp; \leq \varepsilon</em>}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}}\left(h^{<em>}\right)+\varepsilon_{\tilde{\mathcal{D}}}\left(h, h^{</em>}\right)+\left|\varepsilon</em>}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}}\left(h, h^{<em>}\right)-\varepsilon_{\tilde{\mathcal{D}}}\left(h, h^{</em>}\right)\right| \
&amp; \leq \varepsilon</em>}<em _mathcal_H="\mathcal{H">{H}}\left(h^{<em>}\right)+\varepsilon_{\tilde{\mathcal{D}}}\left(h, h^{</em>}\right)+\frac{1}{2} d</em>} \Delta \mathcal{H}}\left(\tilde{\mathcal{D}}, \tilde{\mathcal{D}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}\right) \
&amp; \leq \varepsilon</em>}<em _mathcal_H="\mathcal{H">{H}}\left(h^{<em>}\right)+\varepsilon_{\tilde{\mathcal{D}}}(h)+\varepsilon_{\tilde{\mathcal{D}}}\left(h^{</em>}\right)+\frac{1}{2} d</em>} \Delta \mathcal{H}}\left(\tilde{\mathcal{D}}, \tilde{\mathcal{D}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}\right) \
&amp; =\varepsilon</em>}}}(h)+\frac{1}{2} d_{\mathcal{H} \Delta \mathcal{H}}\left(\tilde{\mathcal{D}}, \tilde{\mathcal{D}<em _tilde_mathcal_D="\tilde{\mathcal{D">{H}\right)+\lambda \
&amp; \leq \varepsilon</em>}}}(h)+\frac{1}{2} \hat{d<em H="H">{\mathcal{H} \Delta \mathcal{H}}\left(\hat{\mathcal{X}}</em>+\lambda
\end{aligned}
$$}, \hat{\mathcal{X}}\right)+4 \sqrt{\frac{2 d_{v c} \log (2 N)+\log \left(\frac{2}{\delta}\right)}{N}</p>
<p>The VD dimension of $\mathcal{H} \Delta \mathcal{H}$ is at most twice the VC dimension of $\mathcal{H}$ (Anthony et al. 1999).
Q.E.D.</p>
<h2>Proof of Theorem 2</h2>
<p>Proof. Assumption 1 indicates that</p>
<p>$$
c|\boldsymbol{x}-\tilde{\boldsymbol{x}}| \leq\left|g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)-g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right)\right|
$$</p>
<p>According to the definition of $d_{\min }$, we have</p>
<p>$$
g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right) \geq c d_{\min }+g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)
$$</p>
<p>or</p>
<p>$$
g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right) \leq g\left(h_{\boldsymbol{\theta}}(\boldsymbol{x})\right)-c d_{\min }
$$</p>
<p>Since $\epsilon&lt;c d_{\min }$, we have $g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right)&lt;0$, which contradicts the fact that $g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right)$ is always positive. Therefore, (15) will not happen and we will only have (14). It follows that</p>
<p>$$
g\left(h_{\boldsymbol{\theta}}(\tilde{\boldsymbol{x}})\right) \geq c d_{\min }
$$</p>
<p>This means if $c d_{\min }&gt;\tau, \tilde{\boldsymbol{x}}$ can be detected successfully. Putting the conditions $\epsilon<c d_{\min }$ and $c d_{\min }>\tau, \tilde{\boldsymbol{x}}$ together, we complete the proof.</p>
<p>Q.E.D.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;"># Sample</th>
<th style="text-align: right;">Dims.</th>
<th style="text-align: right;"># Class</th>
<th style="text-align: right;">\% Anomaly</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">abalone</td>
<td style="text-align: right;">4177</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">28</td>
<td style="text-align: right;">94.19</td>
</tr>
<tr>
<td style="text-align: left;">arrhythmia</td>
<td style="text-align: right;">452</td>
<td style="text-align: right;">280</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">45.80</td>
</tr>
<tr>
<td style="text-align: left;">breastw</td>
<td style="text-align: right;">699</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">34.48</td>
</tr>
<tr>
<td style="text-align: left;">cardio</td>
<td style="text-align: right;">2126</td>
<td style="text-align: right;">22</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">22.15</td>
</tr>
<tr>
<td style="text-align: left;">ecoli</td>
<td style="text-align: right;">336</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">77.16</td>
</tr>
<tr>
<td style="text-align: left;">glass</td>
<td style="text-align: right;">214</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">72.74</td>
</tr>
<tr>
<td style="text-align: left;">ionosphere</td>
<td style="text-align: right;">351</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">35.90</td>
</tr>
<tr>
<td style="text-align: left;">kdd</td>
<td style="text-align: right;">5209460</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">80.52</td>
</tr>
<tr>
<td style="text-align: left;">letter</td>
<td style="text-align: right;">20000</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">26</td>
<td style="text-align: right;">96.15</td>
</tr>
<tr>
<td style="text-align: left;">lympho</td>
<td style="text-align: right;">142</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">42.96</td>
</tr>
<tr>
<td style="text-align: left;">mammography</td>
<td style="text-align: right;">11183</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2.32</td>
</tr>
<tr>
<td style="text-align: left;">mulcross</td>
<td style="text-align: right;">262144</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">10.00</td>
</tr>
<tr>
<td style="text-align: left;">musk</td>
<td style="text-align: right;">7074</td>
<td style="text-align: right;">167</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">17.30</td>
</tr>
<tr>
<td style="text-align: left;">optdigits</td>
<td style="text-align: right;">5620</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">90.00</td>
</tr>
<tr>
<td style="text-align: left;">pendigits</td>
<td style="text-align: right;">10992</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">90.00</td>
</tr>
<tr>
<td style="text-align: left;">pima</td>
<td style="text-align: right;">768</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">34.90</td>
</tr>
<tr>
<td style="text-align: left;">satimage</td>
<td style="text-align: right;">6435</td>
<td style="text-align: right;">37</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">83.33</td>
</tr>
<tr>
<td style="text-align: left;">seismic</td>
<td style="text-align: right;">2584</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">6.58</td>
</tr>
<tr>
<td style="text-align: left;">shuttle</td>
<td style="text-align: right;">58000</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">75.03</td>
</tr>
<tr>
<td style="text-align: left;">speech</td>
<td style="text-align: right;">3686</td>
<td style="text-align: right;">401</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">1.65</td>
</tr>
<tr>
<td style="text-align: left;">thyroid</td>
<td style="text-align: right;">7200</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">7.29</td>
</tr>
<tr>
<td style="text-align: left;">vertebral</td>
<td style="text-align: right;">310</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">67.74</td>
</tr>
<tr>
<td style="text-align: left;">vowels</td>
<td style="text-align: right;">1456</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">3.43</td>
</tr>
<tr>
<td style="text-align: left;">wbc</td>
<td style="text-align: right;">569</td>
<td style="text-align: right;">31</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">37.26</td>
</tr>
<tr>
<td style="text-align: left;">wine</td>
<td style="text-align: right;">178</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">66.67</td>
</tr>
</tbody>
</table>
<p>Table 4: 25 real-world tabular datasets tested in this paper under OCC setting. For datasets with more than 2 labeled classes, an average anomaly ratio is recorded. Kdd refers to KDD-CUP99 (Stolfo et al. 1999).</p>
<h2>Appendix D: Dataset Summary</h2>
<p>We conducted our experiment under the UAD dataset setting with 47 benchmark datasets commonly used in UAD research. The dataset information is shown in Table 5. As for the OCC dataset setting. The detailed dataset information including the
number of samples, dimensionality, the number of classes, and the percent of anomaly is summarized in Table 4. Note that there are datasets with the same name. However, due to the different dataset processing and training data splitting. They are completely different under OCC setting from the UAD setting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;"># Sample</th>
<th style="text-align: right;">Dim.</th>
<th style="text-align: right;">\% Anomaly</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ALOI</td>
<td style="text-align: right;">49534</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">3.04</td>
</tr>
<tr>
<td style="text-align: left;">anthyroid</td>
<td style="text-align: right;">7200</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7.42</td>
</tr>
<tr>
<td style="text-align: left;">backdoor</td>
<td style="text-align: right;">95329</td>
<td style="text-align: right;">196</td>
<td style="text-align: right;">2.44</td>
</tr>
<tr>
<td style="text-align: left;">breast</td>
<td style="text-align: right;">683</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">34.99</td>
</tr>
<tr>
<td style="text-align: left;">campaign</td>
<td style="text-align: right;">41188</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">11.27</td>
</tr>
<tr>
<td style="text-align: left;">cardio</td>
<td style="text-align: right;">1831</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">9.61</td>
</tr>
<tr>
<td style="text-align: left;">Cardiotocography</td>
<td style="text-align: right;">2114</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">22.04</td>
</tr>
<tr>
<td style="text-align: left;">celeba</td>
<td style="text-align: right;">202599</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">2.24</td>
</tr>
<tr>
<td style="text-align: left;">census</td>
<td style="text-align: right;">299285</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">cover</td>
<td style="text-align: right;">286048</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">6.20</td>
</tr>
<tr>
<td style="text-align: left;">donors</td>
<td style="text-align: right;">619326</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">5.93</td>
</tr>
<tr>
<td style="text-align: left;">fault</td>
<td style="text-align: right;">1941</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">34.67</td>
</tr>
<tr>
<td style="text-align: left;">fraud</td>
<td style="text-align: right;">284807</td>
<td style="text-align: right;">29</td>
<td style="text-align: right;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">glass</td>
<td style="text-align: right;">214</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">42.21</td>
</tr>
<tr>
<td style="text-align: left;">Hepatitis</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">16.25</td>
</tr>
<tr>
<td style="text-align: left;">http</td>
<td style="text-align: right;">567498</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr>
<td style="text-align: left;">InternetAds</td>
<td style="text-align: right;">1966</td>
<td style="text-align: right;">1555</td>
<td style="text-align: right;">18.72</td>
</tr>
<tr>
<td style="text-align: left;">Ionosphere</td>
<td style="text-align: right;">351</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">35.90</td>
</tr>
<tr>
<td style="text-align: left;">landsat</td>
<td style="text-align: right;">6435</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">20.71</td>
</tr>
<tr>
<td style="text-align: left;">letter</td>
<td style="text-align: right;">1600</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">15.10</td>
</tr>
<tr>
<td style="text-align: left;">Lymphography</td>
<td style="text-align: right;">148</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">4.05</td>
</tr>
<tr>
<td style="text-align: left;">magic</td>
<td style="text-align: right;">19020</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">35.16</td>
</tr>
<tr>
<td style="text-align: left;">mammography</td>
<td style="text-align: right;">11183</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">2.32</td>
</tr>
<tr>
<td style="text-align: left;">mnist</td>
<td style="text-align: right;">7603</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">9.21</td>
</tr>
<tr>
<td style="text-align: left;">musk</td>
<td style="text-align: right;">3062</td>
<td style="text-align: right;">166</td>
<td style="text-align: right;">3.17</td>
</tr>
<tr>
<td style="text-align: left;">optdigits</td>
<td style="text-align: right;">5216</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">2.88</td>
</tr>
<tr>
<td style="text-align: left;">PageBlocks</td>
<td style="text-align: right;">5393</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">9.46</td>
</tr>
<tr>
<td style="text-align: left;">pendigits</td>
<td style="text-align: right;">6870</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">2.27</td>
</tr>
<tr>
<td style="text-align: left;">Pima</td>
<td style="text-align: right;">768</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">34.90</td>
</tr>
<tr>
<td style="text-align: left;">satellite</td>
<td style="text-align: right;">6435</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">31.64</td>
</tr>
<tr>
<td style="text-align: left;">satimage-2</td>
<td style="text-align: right;">5803</td>
<td style="text-align: right;">36</td>
<td style="text-align: right;">1.22</td>
</tr>
<tr>
<td style="text-align: left;">shuttle</td>
<td style="text-align: right;">49097</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">7.15</td>
</tr>
<tr>
<td style="text-align: left;">skin</td>
<td style="text-align: right;">245057</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">20.75</td>
</tr>
<tr>
<td style="text-align: left;">smtp</td>
<td style="text-align: right;">95156</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">SpamBase</td>
<td style="text-align: right;">4207</td>
<td style="text-align: right;">57</td>
<td style="text-align: right;">39.91</td>
</tr>
<tr>
<td style="text-align: left;">speech</td>
<td style="text-align: right;">3686</td>
<td style="text-align: right;">400</td>
<td style="text-align: right;">1.65</td>
</tr>
<tr>
<td style="text-align: left;">Stamps</td>
<td style="text-align: right;">340</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">9.12</td>
</tr>
<tr>
<td style="text-align: left;">thyroid</td>
<td style="text-align: right;">3772</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">2.47</td>
</tr>
<tr>
<td style="text-align: left;">vertebral</td>
<td style="text-align: right;">240</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">12.50</td>
</tr>
<tr>
<td style="text-align: left;">vowels</td>
<td style="text-align: right;">1456</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">3.43</td>
</tr>
<tr>
<td style="text-align: left;">Waveform</td>
<td style="text-align: right;">3443</td>
<td style="text-align: right;">21</td>
<td style="text-align: right;">2.90</td>
</tr>
<tr>
<td style="text-align: left;">WBC</td>
<td style="text-align: right;">223</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">4.48</td>
</tr>
<tr>
<td style="text-align: left;">WDBC</td>
<td style="text-align: right;">367</td>
<td style="text-align: right;">30</td>
<td style="text-align: right;">2.72</td>
</tr>
<tr>
<td style="text-align: left;">Wilt</td>
<td style="text-align: right;">4819</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">5.33</td>
</tr>
<tr>
<td style="text-align: left;">wine</td>
<td style="text-align: right;">129</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">7.75</td>
</tr>
<tr>
<td style="text-align: left;">WPBC</td>
<td style="text-align: right;">198</td>
<td style="text-align: right;">33</td>
<td style="text-align: right;">23.74</td>
</tr>
<tr>
<td style="text-align: left;">yeast</td>
<td style="text-align: right;">1484</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">34.16</td>
</tr>
</tbody>
</table>
<p>Table 5: 47 real-world tabular datasets tested under UAD setting.</p>
<h2>Appendix E: Neural Network Architecture</h2>
<h3>VanillaMLP</h3>
<p>The VanillaMLP is a plain Relu-activated feed-forward neural network with 4 fully connected layers. The architecture is shown in Figure 7a. The input dimension is $d$. If $d\leq 64$, the size of the hidden layer is set to 64. If $d&gt;64$, the size of the hidden layer is set to 256. In the experiments, datasets, arrhythmia, musk, and speech, are with $d&gt;64$.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Deep Neural Network Architectures.</p>
<h3>ResMLP</h3>
<p>The ResMLP is a Relu-activated feed-forward neural network with 5 residual blocks. The architecture is shown in Figure 7b. The input dimension is $d$. If $d\leq 64$, the size of the hidden layer is set to 64. If $d&gt;64$, the size of the hidden layer is set to 256. In the experiments, datasets, arrhythmia, musk, and speech, are with $d&gt;64$.</p>
<h2>Appendix F: Detailed Anomaly Detection Results</h2>
<p>The detailed results in terms of AUC score and F1 score under the UAD setting are reported here in Table 6 and 7. We do not include the results for AnoGAN baseline because it is not efficient to run. So, we only compare 11 baselines for the UAD setting. The paired t-test is conducted based on the average AUC/F1 score through 10 runs in Tables 2, 5, 6, and 7. For example, in Table 2, every AD method has 25 values (as there are 25 datasets). Thus for any two AD methods, we have 25 pairs of values, on which we performed the paired t-test. The last two lines in the table are the p-values of our two methods (ResMLP and MLP) against each baseline method. Since the anomaly in the test set is rare, F1 score metric is more important to show the significance of our method. It is seen that our method has the highest mean F1 score (69.40%) and average rank (2.70). Compared with other baseline methods, our method has statistically significant improvement in terms of pair t-test p-value.</p>
<h2>Appendix G: F1 Score Results on OCC setting</h2>
<p>The performance in terms of F1 score under the OCC setting is presented in Table 8. Our approach also obtains the highest mean F1 score and mean rank out of 12 baselines.</p>
<h2>Appendix H: Ablation Experiment Results</h2>
<h3>Sensitivity of Different Noise Levels</h3>
<p>To explore how different noise levels contribute to the performance, we keep the noise type as Gaussian noise. In each training batch, we also generate 3 noised instances which is consistent with the former setting. We search through different noise levels in $[0.1,0.2,0.5,0.8,1.0,2.0,3.0,5.0]$. We report the average AUC, F1 score, and rank (out of 8 different noise levels) in Figure 5. We observe that in all results if the noise level is too small, the average performance is low. This indicates noise with a small level would confuse the model because the distance between the normal samples and anomalies is very small. On the other hand, excessively high noise levels result in an expanded output value range, thereby enlarging the sampling space. This enlargement impacts the expressiveness of the model, reducing its effectiveness. However, when the noise level grows too large the score is also decreased. This means a large noise level has a large learning target value and a large sampling space, which may result in under-fitting training. The best choice of noise level appears around 1.0. From the perspective of denoising score matching (Song and Ermon 2019), a higher noise level will lead to inaccurate score estimation while a lower noise level will make it hard to learn data distribution in a relatively low-density region. Hence, a diverse noise level selection is preferred.</p>
<h3>Sensitivity of Different Noise Ratios</h3>
<p>The noise ratio means the percentage of noise appearing the feature in a sample. For different noise ratios, we keep utilizing Gaussian noise and the same noise level generation as the default. In each training batch, we generate 1 noised instance with the specific noise ratio instead. We vary the noise ratio in $[0.2,0.5,0.8,1.0]$. We plot a heatmap jointly with different noise levels in Figure and 8 and 9. We observe an augmentation in model accuracy concomitant with an increase in the noise ratio across the spectrum of noise levels tested. This trend suggests that our model exhibits an affinity for a higher noise ratio in its training regimen, implying that a moderated presence of noise may be beneficial to the model's training efficacy and predictive accuracy. In ResMLP result, similar results are observed that a higher noise ratio is beneficial to the model performance. Noise levels either too high or too low can lead to a bad performance.</p>
<h3>Sensitivity of Different Noise Type</h3>
<p>We study utilizing different noise types to generate the noise. For a fair comparison, we sample all noise with 0 mean and $\sigma$ standard deviation (noise level). We adopt several common noise distributions, including Salt&amp;Pepper, Gaussian, Laplace, Uniform, Rayleigh, Gamma, Poisson, and Bernoulli. For incorporating noise of a Bernoulli nature, the initial step involves the generation of a probability vector, which is derived from a uniform distribution. This vector is then utilized to produce a corresponding binary vector. The binary vector dictates the positions at which the original feature values will be altered—should the noise be present, a reversal of the feature's sign is executed. For Salt&amp;Pepper, similar to Bernoulli,</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" />
(a) AUC on MLP with Different (b) Rank by AUC on MLP with Dif- (c) F1 on MLP with Different (d) Rank by F1 on MLP with Different Noise Levels and Ratios</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Noise Levels and Ratios</th>
<th style="text-align: left;">different Noise Levels and Ratios</th>
<th style="text-align: left;">Noise Levels and Ratios</th>
<th style="text-align: left;">ferent Noise Levels and Ratios</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 8: Sensitivity of Different Noise Levels and Ratio on MLP. We test different noise levels in $[0.1,0.5,1.0,3.0,5.0]$, and noise ratios in $[0.2,0.8,0.5,1.0]$. The mean rank (the lower the better) is calculated out of 20 different settings.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(a) AUC on ResMLP with Different (b) Rank by AUC on ResMLP with (c) F1 on ResMLP with Different (d) Rank by F1 on ResMLP with Noise Levels and Ratios Different Noise Levels and Ratios Noise Levels and Ratios Different Noise Levels and Ratios</p>
<p>Figure 9: Sensitivity of Different Noise Levels and Ratio on ResMLP. We test different noise levels in $[0.1,0.5,1.0,3.0,5.0]$, and noise ratios in $[0.2,0.8,0.5,1.0]$. The mean rank (the lower the better) is calculated out of 20 different settings.
we replace the value with the maximum or minimum value randomly in a batch instead of flipping the sign. For the other distributions, we adjust the parameter to make the generated noise having zero mean and $\sigma$ standard deviation. The detailed generation parameter is shown in Table 3. The result is illustrated in Figure 10. Note that gamma1 and gamma3 represent gamma distribution with hyper-parameter $b=1$, and 3, respectively. In the results, we observe that Bernoulli and Salt\&amp;Pepper noise has a low performance score. It indicates the effectiveness of involving different noise levels in our noise generation design. Gaussian, Rayleigh, and Uniform noise have relatively stable performance with $92 \%$ AUC score and $94 \%$ F1 score. It reveals that our method is robust with multiple types of noise.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: left;">Time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: left;">$5.32 \pm 0.014$</td>
</tr>
<tr>
<td style="text-align: left;">PLAD</td>
<td style="text-align: left;">$6.22 \pm 0.382$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparative study on the time cost for noise generation. Our method takes less time to generate the noised sample in an epoch ( 3800 batches). In addition, our method can be implemented in a pre-generation manner to further save time.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Training Time (s)</th>
<th style="text-align: center;">Inference Time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$37.19 \pm 1.33$</td>
<td style="text-align: center;">$6.41 \pm 0.27$</td>
</tr>
<tr>
<td style="text-align: left;">PLAD (2022)</td>
<td style="text-align: center;">$40.54 \pm 0.38$</td>
<td style="text-align: center;">$4.33 \pm 0.08$</td>
</tr>
<tr>
<td style="text-align: left;">NeuTraLAD (2021)</td>
<td style="text-align: center;">$39.72 \pm 1.14$</td>
<td style="text-align: center;">$6.79 \pm 0.24$</td>
</tr>
<tr>
<td style="text-align: left;">DPAD (2024)</td>
<td style="text-align: center;">$26.84 \pm 0.24$</td>
<td style="text-align: center;">$287.6 \pm 25.1$</td>
</tr>
<tr>
<td style="text-align: left;">SCAD (2022)</td>
<td style="text-align: center;">$37.17 \pm 0.61$</td>
<td style="text-align: center;">$6.46 \pm 0.28$</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparative study of the time cost for model training and inference on KDDCUP-99 dataset. All methods use the same 5-layer backbone MLP. Our method takes less time to train than PLAD and NeuTraLAD. DPAD takes the least time for training because it is a density estimation method, but it takes unacceptable time for inference. Note that our method can reach a faster training speed by utilizing the pregenerated noisy sample.</p>
<h2>Appendix I: Time Cost Analysis</h2>
<h2>Time Cost for Noise Generation</h2>
<p>Suppose the batch size is $b$ and the data dimensionality is $d$, the noise generation time complexity is $\mathcal{O}(b d)$ according to Algorithm 1. In contrast, other methods involving perturbation (Cai and Fan 2022; Qiu et al. 2021) and adversarial sample (Goyal et al. 2020) have time complexity with $\mathcal{O}(b d W)$, where $W$ is workload related to a neural network module. Hence, our method for processing the generated negative sample is more efficient. We report the average time cost</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" />
(a) AUC and Rank on ResMLP with (b) F1 and Rank on ResMLP with (c) AUC and Rank on MLP with (d) F1 and Rank on MLP with Dif-
Different Noise Type Different Noise Type Different Noise Type ferent Noise Type
Figure 10: Sensitivity of Different Noise Types. We test different noise types including Salt\&amp;Pepper, Gaussian, Laplace, Uniform, Rayleigh, Gamma, Poisson, and Bernoulli distributions. The mean rank (the lower the better) is calculated out of 9 different noise types. The results show that Gaussian, Rayleigh, and Uniform distributions have relatively stable performance.
for generating the noise sample per epoch on KDDCUP-99 dataset in Table 9. The time cost for our noise generation is $5.69 \pm 0.014$ seconds while the nagative sample processing time in PLAD is $6.22 \pm 0.38$ seconds. In addition, our noised sample can be generated before the training started. Hence, the generation time can be negligible.</p>
<h2>Time Cost for Training and Inference</h2>
<p>We also compare the training and inference time with other recent deep learning based methods, PLAD, NeuTraLAD, DPAD, and SCAD, on KDDCUP-99 dataset. The results are reported in Table 10. All methods use the same 5-layer backbone MLP. Our method takes less time to train than PLAD and NeuTraLAD. DPAD takes the least time for training because it is a density estimation method, but it takes unacceptable time for inference. Note that our method can reach a faster training speed by utilizing the pre-generated noisy sample. Hence, our method is more efficient at the training stage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Hyper-param.</th>
<th style="text-align: center;">$#$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">Noise Level, Ratio, and Type</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">PLAD (Cai and Fan 2022)</td>
<td style="text-align: center;">$\lambda$, Types of perturbator</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">NeuTraLAD (Qiu et al. 2021)</td>
<td style="text-align: center;">$\tau, K, m$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">DPAD</td>
<td style="text-align: center;">$m, \gamma, \lambda, k$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">(Fu, Zhang, and Fan 2024)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SCAD</td>
<td style="text-align: center;">$k, u, \tau, r$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">(Shenkar and Wolf 2022)</td>
<td style="text-align: center;">$r, \lambda, \mu, \eta$</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">DROCC (Goyal et al. 2020)</td>
<td style="text-align: center;">$k, k^{\prime}, \lambda, \mu, \nu$</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">DOHSC (Zhang et al. 2024)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 11: The comparison of the number of hyper-parameters. Although PLAD has fewer hyper-parameters, the choice of different types of perturbator would lead to a completely different implementation and would result in more new hyperparameters.</p>
<h2>Appendix J: Number of Hyper-parameters</h2>
<p>We compare the number of hyper-parameters with other recent deep learning based UAD methods. The comparison is listed in Table 11. Note that all hyper-parameters of our method are solely related to noise generation, and no
hyper-parameter is involved in the learning objective except for the network training design (which exists in all deep learning based methods). Although PLAD has fewer hyperparameters, the choice of different types of perturbator would lead to a completely different implementation and would result in more new hyper-parameters. Hence, our method is easy to implement.</p>
<h2>Discussions</h2>
<p>Interpretability For those deep learning-based methods (Golan and El-Yaniv 2018; Hendrycks et al. 2019; Hendrycks, Mazeika, and Dietterich 2018) that only output an anomaly score, it is hard to explain the forwarding non-linear process of the neural network black box. In our scheme, we can explain a little in the output. Similar to (Liznerski et al. 2020), our method outputs a vector with the same size as the input data where each dimension tells how much the noise is. Hence, if one dimension has a higher value, we can say the abnormality corresponds to this feature. We leave the detailed interpretability analysis in future research.
Extension to other data types Here, we provide insights for adapting noise evaluation to other data types like sequential, textual, and graph data. Adapting our approach to images, time series, text, and graphs is non-trivial but realizable, as each data type has distinct characteristics.</p>
<ul>
<li>Image data Images exhibit local smoothness or piecewise linear patterns across pixel values, so directly adding random noise (e.g., Gaussian) to pixel values is suboptimal. A better approach is to segment images into patches, sample contextual noise for each patch, and predict the average noise magnitude per segment.</li>
<li>Time series Time series data, such as voice recordings, exhibit natural dependencies between adjacent samples, and anomalies may represent meaningful sequences. Preprocessing (e.g., ADbench(Han et al. 2022) for speech) can extract frequency features, transforming the series into a tabular format compatible with our model.</li>
<li>Textual data Defining "noise" in textual data is nontrivial. A practical approach leverages recent large language models to extract latent features from text tokens. Using the CLS token or aggregation methods can yield</li>
</ul>
<p>sentence- or document-level embeddings, allowing our noise evaluation model to operate in this latent space.</p>
<ul>
<li>Graph data Arbitrarily adding noise to the adjacency matrix is not feasible. One can add an extra virtual node(Cai et al. 2023) to capture the latent feature of the whole graph. Then, graph-level AD with our noise evaluation can be performed.</li>
</ul>
<p>For all data types, instead of noise on raw data, we can apply diverse noise (e.g., Gaussian) to the latent features extracted by some (pre-trained) feature extractors (e.g. encoders) and then predict the noise amplitude. However, there is a risk that the feature extractor may fail to preserve the information of anomalies, which will lead to failure in detecting anomalies. A promising direction for further research involves training the feature extractor, a decoder, and the noise detector jointly, directly in the data space rather than the feature space.</p>
<p>Effectiveness on Dataset with Multiple Dense Regions Figure 1 is just for illustration. Real-world normal data can come from several dense regions, which can be illustrated by a larger picture combining multiple patterns like Figure 1. Our method and theory are applicable to datasets spanning multiple clusters. For instance, $\hat{\mathcal{D}}_{H}$ is the union of multiple hard-anomaly regions corresponding to different clusters.</p>
<p>We observe from real-world examples that our noise evaluation model remains effective when the dataset spans multiple clusters. For instance, in ADBench Letter and Vowels datasets, a t-SNE visualization in Figure 11 shows that Letter is composed of 5 clusters and Vowels has 3 dense regions. In Tables 5 and 6 of Appendix F, our method can obtain a high-performance score in terms of AUC (90.87%/99.03%) and F1 (67.79%/86.67%) on these datasets.</p>
<p>T-SNE Visual. on letter T-SNE Visual. on vowels T-SNE Visual. on vowels Figure 11: T-SNE Visual. of Letter and Vowels Figure 11: T-SNE Visual. of Letter and Vowels</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">DPAD</th>
<th style="text-align: center;">PLAD</th>
<th style="text-align: center;">NeuTraLAD</th>
<th style="text-align: center;">SCAD</th>
<th style="text-align: center;">AE</th>
<th style="text-align: center;">COPOD</th>
<th style="text-align: center;">DeepSVDD</th>
<th style="text-align: center;">ECOD</th>
<th style="text-align: center;">IForest</th>
<th style="text-align: center;">KNN</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">Ours- <br> ResMLP</th>
<th style="text-align: center;">Ours- <br> MLP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ALOI</td>
<td style="text-align: center;">64.48 $\pm 0.4$</td>
<td style="text-align: center;">53.16 $\pm 1.4$</td>
<td style="text-align: center;">53.54 $\pm 0.9$</td>
<td style="text-align: center;">53.97 $\pm 0.5$</td>
<td style="text-align: center;">54.96 $\pm 0.1$</td>
<td style="text-align: center;">51.51 $\pm 0.1$</td>
<td style="text-align: center;">51.44 $\pm 1.3$</td>
<td style="text-align: center;">53.07 $\pm 0.1$</td>
<td style="text-align: center;">53.78 $\pm 0.4$</td>
<td style="text-align: center;">66.72 $\pm 0.2$</td>
<td style="text-align: center;">74.69 $\pm 0.2$</td>
<td style="text-align: center;">60.59 $\pm 0.7$</td>
<td style="text-align: center;">59.95 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">annthyroid</td>
<td style="text-align: center;">90.91 $\pm 2.0$</td>
<td style="text-align: center;">95.09 $\pm 2.8$</td>
<td style="text-align: center;">81.18 $\pm 2.6$</td>
<td style="text-align: center;">86.52 $\pm 2.2$</td>
<td style="text-align: center;">84.66 $\pm 1.5$</td>
<td style="text-align: center;">77.59 $\pm 0.2$</td>
<td style="text-align: center;">56.97 $\pm 2.6$</td>
<td style="text-align: center;">79.11 $\pm 0.2$</td>
<td style="text-align: center;">91.39 $\pm 1.4$</td>
<td style="text-align: center;">93.6 $\pm 0.4$</td>
<td style="text-align: center;">94.78 $\pm 0.3$</td>
<td style="text-align: center;">97.02 $\pm 0.5$</td>
<td style="text-align: center;">96.78 $\pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;">backdoor</td>
<td style="text-align: center;">95.43 $\pm 0.7$</td>
<td style="text-align: center;">91.18 $\pm 13.3$</td>
<td style="text-align: center;">94.20 $\pm 0.7$</td>
<td style="text-align: center;">93.37 $\pm 0.5$</td>
<td style="text-align: center;">90.92 $\pm 0.0$</td>
<td style="text-align: center;">78.93 $\pm 0.1$</td>
<td style="text-align: center;">93.04 $\pm 1.9$</td>
<td style="text-align: center;">84.54 $\pm 0.1$</td>
<td style="text-align: center;">76.77 $\pm 1.9$</td>
<td style="text-align: center;">95.65 $\pm 0.0$</td>
<td style="text-align: center;">96.45 $\pm 0.1$</td>
<td style="text-align: center;">96.18 $\pm 0.9$</td>
<td style="text-align: center;">97.12 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">breastw</td>
<td style="text-align: center;">99.11 $\pm 0.7$</td>
<td style="text-align: center;">74.02 $\pm 2.7$</td>
<td style="text-align: center;">78.93 $\pm 3.9$</td>
<td style="text-align: center;">98.04 $\pm 0.8$</td>
<td style="text-align: center;">98.91 $\pm 0.4$</td>
<td style="text-align: center;">99.56 $\pm 0.1$</td>
<td style="text-align: center;">96.11 $\pm 0.7$</td>
<td style="text-align: center;">99.15 $\pm 0.2$</td>
<td style="text-align: center;">99.47 $\pm 0.1$</td>
<td style="text-align: center;">98.96 $\pm 0.3$</td>
<td style="text-align: center;">96.05 $\pm 1.1$</td>
<td style="text-align: center;">98.99 $\pm 0.4$</td>
<td style="text-align: center;">99.14 $\pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">campaign</td>
<td style="text-align: center;">75.07 $\pm 1.7$</td>
<td style="text-align: center;">72.31 $\pm 3.3$</td>
<td style="text-align: center;">70.88 $\pm 4.2$</td>
<td style="text-align: center;">79.31 $\pm 0.7$</td>
<td style="text-align: center;">76.94 $\pm 0.1$</td>
<td style="text-align: center;">78.32 $\pm 0.1$</td>
<td style="text-align: center;">56.27 $\pm 11.7$</td>
<td style="text-align: center;">76.94 $\pm 0.2$</td>
<td style="text-align: center;">73.54 $\pm 0.8$</td>
<td style="text-align: center;">78.4 $\pm 0.1$</td>
<td style="text-align: center;">70.37 $\pm 0.8$</td>
<td style="text-align: center;">69.33 $\pm 1.4$</td>
<td style="text-align: center;">74.6 $\pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;">cardio</td>
<td style="text-align: center;">91.80 $\pm 3.1$</td>
<td style="text-align: center;">61.16 $\pm 7.2$</td>
<td style="text-align: center;">72.38 $\pm 2.9$</td>
<td style="text-align: center;">89.57 $\pm 1.3$</td>
<td style="text-align: center;">96.25 $\pm 0.4$</td>
<td style="text-align: center;">92.39 $\pm 0.4$</td>
<td style="text-align: center;">59.95 $\pm 5.2$</td>
<td style="text-align: center;">93.44 $\pm 0.2$</td>
<td style="text-align: center;">94.6 $\pm 1.0$</td>
<td style="text-align: center;">92.53 $\pm 0.9$</td>
<td style="text-align: center;">92.49 $\pm 1.3$</td>
<td style="text-align: center;">97.06 $\pm 0.9$</td>
<td style="text-align: center;">96.66 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">Cardio.</td>
<td style="text-align: center;">76.92 $\pm 2.2$</td>
<td style="text-align: center;">65.39 $\pm 6.9$</td>
<td style="text-align: center;">54.98 $\pm 4.6$</td>
<td style="text-align: center;">69.54 $\pm 2.2$</td>
<td style="text-align: center;">83.35 $\pm 1.0$</td>
<td style="text-align: center;">66.37 $\pm 0.9$</td>
<td style="text-align: center;">49.21 $\pm 11.4$</td>
<td style="text-align: center;">78.49 $\pm 0.7$</td>
<td style="text-align: center;">80.16 $\pm 1.2$</td>
<td style="text-align: center;">72.3 $\pm 1.3$</td>
<td style="text-align: center;">76.96 $\pm 1.2$</td>
<td style="text-align: center;">85.04 $\pm 3.2$</td>
<td style="text-align: center;">84.81 $\pm 3.1$</td>
</tr>
<tr>
<td style="text-align: center;">celebra</td>
<td style="text-align: center;">63.50 $\pm 2.7$</td>
<td style="text-align: center;">83.47 $\pm 2.5$</td>
<td style="text-align: center;">58.15 $\pm 9.2$</td>
<td style="text-align: center;">72.91 $\pm 1.9$</td>
<td style="text-align: center;">79.8 $\pm 0.1$</td>
<td style="text-align: center;">75.1 $\pm 0.0$</td>
<td style="text-align: center;">54.24 $\pm 22.4$</td>
<td style="text-align: center;">75.7 $\pm 0.1$</td>
<td style="text-align: center;">70.97 $\pm 1.4$</td>
<td style="text-align: center;">68.26 $\pm 0.2$</td>
<td style="text-align: center;">45.77 $\pm 0.3$</td>
<td style="text-align: center;">59.21 $\pm 2.8$</td>
<td style="text-align: center;">67.25 $\pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;">census</td>
<td style="text-align: center;">50.03 $\pm 0.0$</td>
<td style="text-align: center;">55.20 $\pm 4.1$</td>
<td style="text-align: center;">53.46 $\pm 5.6$</td>
<td style="text-align: center;">70.54 $\pm 0.5$</td>
<td style="text-align: center;">70.74 $\pm 0.1$</td>
<td style="text-align: center;">67.4 $\pm 0.1$</td>
<td style="text-align: center;">52.08 $\pm 7.8$</td>
<td style="text-align: center;">65.92 $\pm 0.1$</td>
<td style="text-align: center;">63.43 $\pm 2.2$</td>
<td style="text-align: center;">72.06 $\pm 0.1$</td>
<td style="text-align: center;">60.49 $\pm 0.2$</td>
<td style="text-align: center;">65.63 $\pm 2.7$</td>
<td style="text-align: center;">68.45 $\pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;">cover</td>
<td style="text-align: center;">60.42 $\pm 10.8$</td>
<td style="text-align: center;">87.66 $\pm 0.5$</td>
<td style="text-align: center;">71.61 $\pm 10.4$</td>
<td style="text-align: center;">96.68 $\pm 1.2$</td>
<td style="text-align: center;">95.53 $\pm 0.9$</td>
<td style="text-align: center;">88.41 $\pm 0.0$</td>
<td style="text-align: center;">36.57 $\pm 5.8$</td>
<td style="text-align: center;">92.01 $\pm 0.1$</td>
<td style="text-align: center;">85.07 $\pm 3.1$</td>
<td style="text-align: center;">98.6 $\pm 0.1$</td>
<td style="text-align: center;">98.92 $\pm 0.2$</td>
<td style="text-align: center;">99.69 $\pm 0.1$</td>
<td style="text-align: center;">99.64 $\pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">denors</td>
<td style="text-align: center;">95.07 $\pm 6.6$</td>
<td style="text-align: center;">93.46 $\pm 2.9$</td>
<td style="text-align: center;">99.95 $\pm 0.0$</td>
<td style="text-align: center;">99.95 $\pm 0.0$</td>
<td style="text-align: center;">85.79 $\pm 3.0$</td>
<td style="text-align: center;">81.49 $\pm 0.0$</td>
<td style="text-align: center;">63.88 $\pm 25.7$</td>
<td style="text-align: center;">88.85 $\pm 0.0$</td>
<td style="text-align: center;">89.62 $\pm 2.7$</td>
<td style="text-align: center;">99.9 $\pm 0.0$</td>
<td style="text-align: center;">98.42 $\pm 0.0$</td>
<td style="text-align: center;">99.78 $\pm 0.0$</td>
<td style="text-align: center;">99.6 $\pm 0.2$</td>
</tr>
<tr>
<td style="text-align: center;">fault</td>
<td style="text-align: center;">73.06 $\pm 1.0$</td>
<td style="text-align: center;">61.51 $\pm 4.8$</td>
<td style="text-align: center;">69.39 $\pm 1.3$</td>
<td style="text-align: center;">79.27 $\pm 1.0$</td>
<td style="text-align: center;">55.26 $\pm 1.2$</td>
<td style="text-align: center;">45.13 $\pm 0.8$</td>
<td style="text-align: center;">50.6 $\pm 6.2$</td>
<td style="text-align: center;">46.79 $\pm 0.6$</td>
<td style="text-align: center;">65.84 $\pm 1.6$</td>
<td style="text-align: center;">79.83 $\pm 1.1$</td>
<td style="text-align: center;">65.91 $\pm 1.7$</td>
<td style="text-align: center;">72.27 $\pm 1.3$</td>
<td style="text-align: center;">73.49 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">fraud</td>
<td style="text-align: center;">89.73 $\pm 5.6$</td>
<td style="text-align: center;">96.29 $\pm 0.7$</td>
<td style="text-align: center;">91.59 $\pm 3.5$</td>
<td style="text-align: center;">94.71 $\pm 0.4$</td>
<td style="text-align: center;">95.38 $\pm 0.0$</td>
<td style="text-align: center;">94.74 $\pm 0.0$</td>
<td style="text-align: center;">85.15 $\pm 6.7$</td>
<td style="text-align: center;">94.97 $\pm 0.0$</td>
<td style="text-align: center;">94.88 $\pm 0.4$</td>
<td style="text-align: center;">96.17 $\pm 0.1$</td>
<td style="text-align: center;">77.85 $\pm 3.4$</td>
<td style="text-align: center;">94.44 $\pm 0.3$</td>
<td style="text-align: center;">95.11 $\pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;">glass</td>
<td style="text-align: center;">86.34 $\pm 2.6$</td>
<td style="text-align: center;">85.76 $\pm 8.1$</td>
<td style="text-align: center;">89.17 $\pm 5.0$</td>
<td style="text-align: center;">92.23 $\pm 2.5$</td>
<td style="text-align: center;">74.21 $\pm 1.7$</td>
<td style="text-align: center;">74.18 $\pm 1.0$</td>
<td style="text-align: center;">87.86 $\pm 3.3$</td>
<td style="text-align: center;">71.87 $\pm 4.0$</td>
<td style="text-align: center;">79.79 $\pm 3.9$</td>
<td style="text-align: center;">86.27 $\pm 3.3$</td>
<td style="text-align: center;">75.53 $\pm 4.7$</td>
<td style="text-align: center;">88.48 $\pm 3.4$</td>
<td style="text-align: center;">94.07 $\pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;">Hepatitis</td>
<td style="text-align: center;">77.67 $\pm 8.9$</td>
<td style="text-align: center;">73.08 $\pm 3.9$</td>
<td style="text-align: center;">51.70 $\pm 11.2$</td>
<td style="text-align: center;">69.7 $\pm 6.2$</td>
<td style="text-align: center;">79.63 $\pm 4.6$</td>
<td style="text-align: center;">80.89 $\pm 2.4$</td>
<td style="text-align: center;">68.44 $\pm 6.0$</td>
<td style="text-align: center;">74.22 $\pm 2.6$</td>
<td style="text-align: center;">79.02 $\pm 5.8$</td>
<td style="text-align: center;">82.44 $\pm 3.1$</td>
<td style="text-align: center;">80.93 $\pm 3.2$</td>
<td style="text-align: center;">81.73 $\pm 3.0$</td>
<td style="text-align: center;">87.75 $\pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">http</td>
<td style="text-align: center;">99.98 $\pm 0.0$</td>
<td style="text-align: center;">99.99 $\pm 0.2$</td>
<td style="text-align: center;">79.82 $\pm 32.1$</td>
<td style="text-align: center;">99.86 $\pm 0.3$</td>
<td style="text-align: center;">99.94 $\pm 0.0$</td>
<td style="text-align: center;">99.15 $\pm 0.0$</td>
<td style="text-align: center;">21.43 $\pm 43.1$</td>
<td style="text-align: center;">97.86 $\pm 0.0$</td>
<td style="text-align: center;">99.14 $\pm 0.6$</td>
<td style="text-align: center;">99.96 $\pm 0.0$</td>
<td style="text-align: center;">93.8 $\pm 0.4$</td>
<td style="text-align: center;">99.99 $\pm 0.0$</td>
<td style="text-align: center;">99.99 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">InternetAds</td>
<td style="text-align: center;">89.99 $\pm 0.9$</td>
<td style="text-align: center;">87.44 $\pm 1.5$</td>
<td style="text-align: center;">88.94 $\pm 1.0$</td>
<td style="text-align: center;">88.76 $\pm 1.4$</td>
<td style="text-align: center;">87.84 $\pm 0.3$</td>
<td style="text-align: center;">67.92 $\pm 0.7$</td>
<td style="text-align: center;">89.91 $\pm 0.4$</td>
<td style="text-align: center;">67.63 $\pm 0.5$</td>
<td style="text-align: center;">44.39 $\pm 2.1$</td>
<td style="text-align: center;">88.41 $\pm 0.7$</td>
<td style="text-align: center;">88.79 $\pm 0.6$</td>
<td style="text-align: center;">92.49 $\pm 0.7$</td>
<td style="text-align: center;">92.36 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere</td>
<td style="text-align: center;">94.55 $\pm 1.6$</td>
<td style="text-align: center;">52.05 $\pm 9.3$</td>
<td style="text-align: center;">85.68 $\pm 2.5$</td>
<td style="text-align: center;">96.62 $\pm 0.6$</td>
<td style="text-align: center;">90.99 $\pm 1.1$</td>
<td style="text-align: center;">78.6 $\pm 1.8$</td>
<td style="text-align: center;">97.21 $\pm 0.5$</td>
<td style="text-align: center;">72.61 $\pm 1.4$</td>
<td style="text-align: center;">88.49 $\pm 2.3$</td>
<td style="text-align: center;">97.42 $\pm 0.7$</td>
<td style="text-align: center;">94.86 $\pm 1.7$</td>
<td style="text-align: center;">97.51 $\pm 0.5$</td>
<td style="text-align: center;">97.56 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">landrat</td>
<td style="text-align: center;">74.67 $\pm 2.2$</td>
<td style="text-align: center;">68.35 $\pm 5.6$</td>
<td style="text-align: center;">72.21 $\pm 7.2$</td>
<td style="text-align: center;">73.75 $\pm 0.7$</td>
<td style="text-align: center;">41.89 $\pm 0.7$</td>
<td style="text-align: center;">42.19 $\pm 0.4$</td>
<td style="text-align: center;">65.49 $\pm 2.3$</td>
<td style="text-align: center;">36.86 $\pm 0.6$</td>
<td style="text-align: center;">62.43 $\pm 2.8$</td>
<td style="text-align: center;">76.56 $\pm 0.2$</td>
<td style="text-align: center;">74.33 $\pm 1.1$</td>
<td style="text-align: center;">58.79 $\pm 3.3$</td>
<td style="text-align: center;">62.16 $\pm 6.4$</td>
</tr>
<tr>
<td style="text-align: center;">letter</td>
<td style="text-align: center;">72.34 $\pm 2.2$</td>
<td style="text-align: center;">70.19 $\pm 5.5$</td>
<td style="text-align: center;">91.73 $\pm 1.3$</td>
<td style="text-align: center;">94.36 $\pm 0.8$</td>
<td style="text-align: center;">52.36 $\pm 0.4$</td>
<td style="text-align: center;">55.53 $\pm 0.9$</td>
<td style="text-align: center;">62.58 $\pm 2.8$</td>
<td style="text-align: center;">57.18 $\pm 0.7$</td>
<td style="text-align: center;">62.87 $\pm 2.4$</td>
<td style="text-align: center;">88.12 $\pm 0.9$</td>
<td style="text-align: center;">86.85 $\pm 1.2$</td>
<td style="text-align: center;">90.87 $\pm 0.8$</td>
<td style="text-align: center;">90.71 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">Lympho.</td>
<td style="text-align: center;">98.71 $\pm 0.2$</td>
<td style="text-align: center;">71.43 $\pm 38.5$</td>
<td style="text-align: center;">44.48 $\pm 9.6$</td>
<td style="text-align: center;">99.37 $\pm 0.9$</td>
<td style="text-align: center;">98.36 $\pm 2.4$</td>
<td style="text-align: center;">99.73 $\pm 0.4$</td>
<td style="text-align: center;">97.95 $\pm 1.5$</td>
<td style="text-align: center;">99.62 $\pm 0.4$</td>
<td style="text-align: center;">99.2 $\pm 0.9$</td>
<td style="text-align: center;">98.41 $\pm 2.5$</td>
<td style="text-align: center;">99.19 $\pm 0.7$</td>
<td style="text-align: center;">98.95 $\pm 1.9$</td>
<td style="text-align: center;">98.64 $\pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;">magic.</td>
<td style="text-align: center;">77.94 $\pm 4.6$</td>
<td style="text-align: center;">78.46 $\pm 2.7$</td>
<td style="text-align: center;">71.09 $\pm 2.0$</td>
<td style="text-align: center;">80.05 $\pm 0.3$</td>
<td style="text-align: center;">70.18 $\pm 0.9$</td>
<td style="text-align: center;">68.24 $\pm 0.3$</td>
<td style="text-align: center;">63.49 $\pm 0.6$</td>
<td style="text-align: center;">63.55 $\pm 0.2$</td>
<td style="text-align: center;">77.16 $\pm 1.1$</td>
<td style="text-align: center;">83.31 $\pm 0.3$</td>
<td style="text-align: center;">83.29 $\pm 0.2$</td>
<td style="text-align: center;">88.05 $\pm 0.4$</td>
<td style="text-align: center;">88.59 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">mammo.</td>
<td style="text-align: center;">87.06 $\pm 2.1$</td>
<td style="text-align: center;">80.92 $\pm 2.4$</td>
<td style="text-align: center;">67.43 $\pm 1.9$</td>
<td style="text-align: center;">78.79 $\pm 1.6$</td>
<td style="text-align: center;">85.45 $\pm 1.0$</td>
<td style="text-align: center;">90.61 $\pm 0.1$</td>
<td style="text-align: center;">64.47 $\pm 11.1$</td>
<td style="text-align: center;">90.68 $\pm 0.1$</td>
<td style="text-align: center;">87.99 $\pm 0.7$</td>
<td style="text-align: center;">87.66 $\pm 0.2$</td>
<td style="text-align: center;">82.9 $\pm 1.0$</td>
<td style="text-align: center;">89.94 $\pm 0.4$</td>
<td style="text-align: center;">90.25 $\pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;">Pima</td>
<td style="text-align: center;">69.02 $\pm 2.3$</td>
<td style="text-align: center;">63.54 $\pm 2.7$</td>
<td style="text-align: center;">49.61 $\pm 3.8$</td>
<td style="text-align: center;">67.61 $\pm 1.3$</td>
<td style="text-align: center;">69.93 $\pm 0.7$</td>
<td style="text-align: center;">64.69 $\pm 1.4$</td>
<td style="text-align: center;">55.99 $\pm 1.8$</td>
<td style="text-align: center;">59.29 $\pm 0.6$</td>
<td style="text-align: center;">73.53 $\pm 1.3$</td>
<td style="text-align: center;">74.1 $\pm 1.0$</td>
<td style="text-align: center;">70.79 $\pm 1.7$</td>
<td style="text-align: center;">71.2 $\pm 1.7$</td>
<td style="text-align: center;">73.9 $\pm 1.8$</td>
</tr>
<tr>
<td style="text-align: center;">satellite</td>
<td style="text-align: center;">85.43 $\pm 1.1$</td>
<td style="text-align: center;">50.90 $\pm 4.3$</td>
<td style="text-align: center;">76.87 $\pm 2.2$</td>
<td style="text-align: center;">87.74 $\pm 0.5$</td>
<td style="text-align: center;">67.21 $\pm 0.6$</td>
<td style="text-align: center;">63.53 $\pm 0.4$</td>
<td style="text-align: center;">73.5 $\pm 2.2$</td>
<td style="text-align: center;">58.36 $\pm 0.3$</td>
<td style="text-align: center;">80.6 $\pm 1.0$</td>
<td style="text-align: center;">87.59 $\pm 0.2$</td>
<td style="text-align: center;">84.6 $\pm 0.4$</td>
<td style="text-align: center;">81.22 $\pm 0.8$</td>
<td style="text-align: center;">82.06 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">satimage</td>
<td style="text-align: center;">99.81 $\pm 1.0$</td>
<td style="text-align: center;">71.72 $\pm 16.7$</td>
<td style="text-align: center;">91.76 $\pm 5.3$</td>
<td style="text-align: center;">99.81 $\pm 0.0$</td>
<td style="text-align: center;">98.04 $\pm 0.2$</td>
<td style="text-align: center;">97.41 $\pm 0.0$</td>
<td style="text-align: center;">95.96 $\pm 0.7$</td>
<td style="text-align: center;">96.45 $\pm 0.1$</td>
<td style="text-align: center;">99.35 $\pm 0.2$</td>
<td style="text-align: center;">99.88 $\pm 0.0$</td>
<td style="text-align: center;">99.6 $\pm 0.1$</td>
<td style="text-align: center;">99.87 $\pm 0.0$</td>
<td style="text-align: center;">99.86 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">shuttle</td>
<td style="text-align: center;">99.96 $\pm 0.0$</td>
<td style="text-align: center;">99.89 $\pm 0.1$</td>
<td style="text-align: center;">99.92 $\pm 0.1$</td>
<td style="text-align: center;">99.99 $\pm 0.0$</td>
<td style="text-align: center;">99.43 $\pm 0.1$</td>
<td style="text-align: center;">99.45 $\pm 0.0$</td>
<td style="text-align: center;">99.66 $\pm 0.0$</td>
<td style="text-align: center;">99.3 $\pm 0.0$</td>
<td style="text-align: center;">99.58 $\pm 0.1$</td>
<td style="text-align: center;">99.89 $\pm 0.0$</td>
<td style="text-align: center;">99.97 $\pm 0.0$</td>
<td style="text-align: center;">99.99 $\pm 0.0$</td>
<td style="text-align: center;">99.98 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">skin</td>
<td style="text-align: center;">99.48 $\pm 0.2$</td>
<td style="text-align: center;">98.43 $\pm 0.8$</td>
<td style="text-align: center;">89.65 $\pm 2.6$</td>
<td style="text-align: center;">70.11 $\pm 17.0$</td>
<td style="text-align: center;">66.47 $\pm 2.6$</td>
<td style="text-align: center;">47.12 $\pm 0.1$</td>
<td style="text-align: center;">62.49 $\pm 4.6$</td>
<td style="text-align: center;">48.91 $\pm 0.1$</td>
<td style="text-align: center;">89.09 $\pm 0.8$</td>
<td style="text-align: center;">99.81 $\pm 0.0$</td>
<td style="text-align: center;">91.83 $\pm 0.7$</td>
<td style="text-align: center;">97.74 $\pm 0.2$</td>
<td style="text-align: center;">99.22 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">sunp</td>
<td style="text-align: center;">96.00 $\pm 2.0$</td>
<td style="text-align: center;">87.24 $\pm 8.4$</td>
<td style="text-align: center;">94.09 $\pm 2.6$</td>
<td style="text-align: center;">96.81 $\pm 1.8$</td>
<td style="text-align: center;">83.26 $\pm 0.8$</td>
<td style="text-align: center;">91.2 $\pm 0.0$</td>
<td style="text-align: center;">89.52 $\pm 5.2$</td>
<td style="text-align: center;">88.01 $\pm 0.1$</td>
<td style="text-align: center;">90.7 $\pm 0.5$</td>
<td style="text-align: center;">93.1 $\pm 0.2$</td>
<td style="text-align: center;">93.05 $\pm 0.8$</td>
<td style="text-align: center;">92.37 $\pm 1.4$</td>
<td style="text-align: center;">93.96 $\pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">SpamBase</td>
<td style="text-align: center;">81.51 $\pm 0.9$</td>
<td style="text-align: center;">77.41 $\pm 3.3$</td>
<td style="text-align: center;">78.76 $\pm 1.7$</td>
<td style="text-align: center;">86.78 $\pm 1.2$</td>
<td style="text-align: center;">79.92 $\pm 1.0$</td>
<td style="text-align: center;">68.73 $\pm 0.5$</td>
<td style="text-align: center;">74.43 $\pm 4.1$</td>
<td style="text-align: center;">65.56 $\pm 0.4$</td>
<td style="text-align: center;">83.32 $\pm 1.7$</td>
<td style="text-align: center;">83.3 $\pm 0.7$</td>
<td style="text-align: center;">80.61 $\pm 1.0$</td>
<td style="text-align: center;">84.44 $\pm 0.8$</td>
<td style="text-align: center;">84.84 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">speech</td>
<td style="text-align: center;">57.12 $\pm 2.5$</td>
<td style="text-align: center;">58.68 $\pm 2.2$</td>
<td style="text-align: center;">55.43 $\pm 5.1$</td>
<td style="text-align: center;">56.55 $\pm 2.0$</td>
<td style="text-align: center;">47.29 $\pm 0.4$</td>
<td style="text-align: center;">49.18 $\pm 0.3$</td>
<td style="text-align: center;">50.83 $\pm 1.4$</td>
<td style="text-align: center;">47.29 $\pm 0.3$</td>
<td style="text-align: center;">48.18 $\pm 1.5$</td>
<td style="text-align: center;">48.62 $\pm 0.9$</td>
<td style="text-align: center;">49.64 $\pm 0.6$</td>
<td style="text-align: center;">55.98 $\pm 1.9$</td>
<td style="text-align: center;">58.13 $\pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;">Stamps</td>
<td style="text-align: center;">95.06 $\pm 1.2$</td>
<td style="text-align: center;">54.08 $\pm 11.2$</td>
<td style="text-align: center;">67.03 $\pm 11.1$</td>
<td style="text-align: center;">88.76 $\pm 3.2$</td>
<td style="text-align: center;">94.03 $\pm 1.5$</td>
<td style="text-align: center;">93.18 $\pm 0.7$</td>
<td style="text-align: center;">79.19 $\pm 5.1$</td>
<td style="text-align: center;">88.11 $\pm 2.1$</td>
<td style="text-align: center;">92.93 $\pm 1.7$</td>
<td style="text-align: center;">93.83 $\pm 1.5$</td>
<td style="text-align: center;">93.12 $\pm 1.3$</td>
<td style="text-align: center;">94.49 $\pm 1.2$</td>
<td style="text-align: center;">95.25 $\pm 1.2$</td>
</tr>
<tr>
<td style="text-align: center;">thyroid</td>
<td style="text-align: center;">97.95 $\pm 1.5$</td>
<td style="text-align: center;">97.39 $\pm 2.9$</td>
<td style="text-align: center;">74.34 $\pm 7.5$</td>
<td style="text-align: center;">94.95 $\pm 1.6$</td>
<td style="text-align: center;">98.71 $\pm 0.1$</td>
<td style="text-align: center;">93.93 $\pm 0.2$</td>
<td style="text-align: center;">80.64 $\pm 10.1$</td>
<td style="text-align: center;">97.73 $\pm 0.2$</td>
<td style="text-align: center;">98.97 $\pm 0.2$</td>
<td style="text-align: center;">98.61 $\pm 0.1$</td>
<td style="text-align: center;">96.31 $\pm 1.1$</td>
<td style="text-align: center;">97.74 $\pm 0.5$</td>
<td style="text-align: center;">98.49 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">vertebral</td>
<td style="text-align: center;">55.21 $\pm 3.6$</td>
<td style="text-align: center;">58.72 $\pm 13.8$</td>
<td style="text-align: center;">54.54 $\pm 8.6$</td>
<td style="text-align: center;">51.34 $\pm 3.4$</td>
<td style="text-align: center;">51.15 $\pm 4.1$</td>
<td style="text-align: center;">33.32 $\pm 2.3$</td>
<td style="text-align: center;">34.57 $\pm 4.0$</td>
<td style="text-align: center;">39.88 $\pm 1.4$</td>
<td style="text-align: center;">42.07 $\pm 3.0$</td>
<td style="text-align: center;">43.17 $\pm 3.4$</td>
<td style="text-align: center;">39.77 $\pm 2.3$</td>
<td style="text-align: center;">71.24 $\pm 5.3$</td>
<td style="text-align: center;">69.8 $\pm 3.9$</td>
</tr>
<tr>
<td style="text-align: center;">vowels</td>
<td style="text-align: center;">85.20 $\pm 4.5$</td>
<td style="text-align: center;">75.42 $\pm 7.7$</td>
<td style="text-align: center;">96.97 $\pm 1.3$</td>
<td style="text-align: center;">99.53 $\pm 0.2$</td>
<td style="text-align: center;">62.91 $\pm 1.0$</td>
<td style="text-align: center;">49.88 $\pm 0.9$</td>
<td style="text-align: center;">74.1 $\pm 5.1$</td>
<td style="text-align: center;">58.97 $\pm 0.7$</td>
<td style="text-align: center;">79.15 $\pm 2.2$</td>
<td style="text-align: center;">96.93 $\pm 0.2$</td>
<td style="text-align: center;">95.51 $\pm 0.9$</td>
<td style="text-align: center;">99.0 $\pm 0.2$</td>
<td style="text-align: center;">99.03 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">Waveform</td>
<td style="text-align: center;">68.80 $\pm 5.4$</td>
<td style="text-align: center;">83.53 $\pm 1.2$</td>
<td style="text-align: center;">72.88 $\pm 1.2$</td>
<td style="text-align: center;">68.54 $\pm 3.3$</td>
<td style="text-align: center;">65.22 $\pm 0.5$</td>
<td style="text-align: center;">73.42 $\pm 0.7$</td>
<td style="text-align: center;">60.59 $\pm 5.5$</td>
<td style="text-align: center;">60.04 $\pm 0.3$</td>
<td style="text-align: center;">73.0 $\pm 2.0$</td>
<td style="text-align: center;">75.54 $\pm 0.7$</td>
<td style="text-align: center;">75.34 $\pm 0.8$</td>
<td style="text-align: center;">75.29 $\pm 1.6$</td>
<td style="text-align: center;">72.38 $\pm 3.0$</td>
</tr>
<tr>
<td style="text-align: center;">WBC</td>
<td style="text-align: center;">98.66 $\pm 0.3$</td>
<td style="text-align: center;">48.87 $\pm 8.2$</td>
<td style="text-align: center;">63.30 $\pm 8.5$</td>
<td style="text-align: center;">95.61 $\pm 0.3$</td>
<td style="text-align: center;">99.04 $\pm 0.6$</td>
<td style="text-align: center;">99.58 $\pm 0.3$</td>
<td style="text-align: center;">93.32 $\pm 1.9$</td>
<td style="text-align: center;">99.53 $\pm 0.3$</td>
<td style="text-align: center;">99.53 $\pm 0.2$</td>
<td style="text-align: center;">98.98 $\pm 0.5$</td>
<td style="text-align: center;">97.3 $\pm 1.5$</td>
<td style="text-align: center;">98.41 $\pm 0.8$</td>
<td style="text-align: center;">99.17 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">WDBC</td>
<td style="text-align: center;">97.67 $\pm 2.4$</td>
<td style="text-align: center;">36.12 $\pm 18.6$</td>
<td style="text-align: center;">44.22 $\pm 11.9$</td>
<td style="text-align: center;">98.46 $\pm 1.5$</td>
<td style="text-align: center;">99.0 $\pm 0.6$</td>
<td style="text-align: center;">99.3 $\pm 0.3$</td>
<td style="text-align: center;">98.54 $\pm 0.6$</td>
<td style="text-align: center;">96.76 $\pm 0.8$</td>
<td style="text-align: center;">99.11 $\pm 0.4$</td>
<td style="text-align: center;">99.04 $\pm 0.3$</td>
<td style="text-align: center;">99.12 $\pm 0.5$</td>
<td style="text-align: center;">99.97 $\pm 0.1$</td>
<td style="text-align: center;">99.76 $\pm 0.4$</td>
</tr>
<tr>
<td style="text-align: center;">Wib</td>
<td style="text-align: center;">62.79 $\pm 3.2$</td>
<td style="text-align: center;">99.03 $\pm 3.1$</td>
<td style="text-align: center;">65.44 $\pm 2.4$</td>
<td style="text-align: center;">74.16 $\pm 4.6$</td>
<td style="text-align: center;">37.9 $\pm 2.0$</td>
<td style="text-align: center;">34.43 $\pm 0.4$</td>
<td style="text-align: center;">45.2 $\pm 2.0$</td>
<td style="text-align: center;">39.31 $\pm 0.4$</td>
<td style="text-align: center;">48.08 $\pm 2.2$</td>
<td style="text-align: center;">64.24 $\pm 1.8$</td>
<td style="text-align: center;">69.54 $\pm 3.0$</td>
<td style="text-align: center;">92.75 $\pm 1.2$</td>
<td style="text-align: center;">94.3 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">wine</td>
<td style="text-align: center;">96.07 $\pm 4.5$</td>
<td style="text-align: center;">72.54 $\pm 2.9$</td>
<td style="text-align: center;">64.58 $\pm 10.8$</td>
<td style="text-align: center;">95.72 $\pm 3.0$</td>
<td style="text-align: center;">90.39 $\pm 1.5$</td>
<td style="text-align: center;">87.16 $\pm 1.8$</td>
<td style="text-align: center;">97.08 $\pm 3.3$</td>
<td style="text-align: center;">73.83 $\pm 1.9$</td>
<td style="text-align: center;">89.87 $\pm 4.0$</td>
<td style="text-align: center;">96.75 $\pm 2.4$</td>
<td style="text-align: center;">96.75 $\pm 1.4$</td>
<td style="text-align: center;">98.62 $\pm 2.1$</td>
<td style="text-align: center;">99.42 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">WPBC</td>
<td style="text-align: center;">56.61 $\pm 2.0$</td>
<td style="text-align: center;">62.95 $\pm 6.8$</td>
<td style="text-align: center;">53.51 $\pm 3.5$</td>
<td style="text-align: center;">46.67 $\pm 3.3$</td>
<td style="text-align: center;">49.65 $\pm 3.2$</td>
<td style="text-align: center;">50.32 $\pm 2.8$</td>
<td style="text-align: center;">49.9 $\pm 2.3$</td>
<td style="text-align: center;">48.21 $\pm 1.4$</td>
<td style="text-align: center;">50.83 $\pm 3.1$</td>
<td style="text-align: center;">52.99 $\pm 2.8$</td>
<td style="text-align: center;">52.54 $\pm 2.8$</td>
<td style="text-align: center;">59.79 $\pm 1.8$</td>
<td style="text-align: center;">61.25 $\pm 2.4$</td>
</tr>
<tr>
<td style="text-align: center;">yeast</td>
<td style="text-align: center;">51.35 $\pm 3.1$</td>
<td style="text-align: center;">59.59 $\pm 3.2$</td>
<td style="text-align: center;">52.10 $\pm 3.4$</td>
<td style="text-align: center;">50.95 $\pm 2.2$</td>
<td style="text-align: center;">42.18 $\pm 0.8$</td>
<td style="text-align: center;">38.28 $\pm 0.5$</td>
<td style="text-align: center;">45.06 $\pm 3.1$</td>
<td style="text-align: center;">44.15 $\pm 1.5$</td>
<td style="text-align: center;">40.66 $\pm 1.0$</td>
<td style="text-align: center;">45.06 $\pm 1.3$</td>
<td style="text-align: center;">46.92 $\pm 1.4$</td>
<td style="text-align: center;">58.54 $\pm 1.8$</td>
<td style="text-align: center;">57.53 $\pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">mean rank</td>
<td style="text-align: center;">5.55</td>
<td style="text-align: center;">7.46</td>
<td style="text-align: center;">7.55</td>
<td style="text-align: center;">5.00</td>
<td style="text-align: center;">7.00</td>
<td style="text-align: center;">8.21</td>
<td style="text-align: center;">9.43</td>
<td style="text-align: center;">8.55</td>
<td style="text-align: center;">6.68</td>
<td style="text-align: center;">3.76</td>
<td style="text-align: center;">5.234</td>
<td style="text-align: center;">3.31</td>
<td style="text-align: center;">2.80</td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">82.75 $\pm 15.6$</td>
<td style="text-align: center;">75.40 $\pm 18.3$</td>
<td style="text-align: center;">74.47 $\pm 17.9$</td>
<td style="text-align: center;">84.44 $\pm 18.0$</td>
<td style="text-align: center;">78.46 $\pm 18.7$</td>
<td style="text-align: center;">74.60 $\pm 19.6$</td>
<td style="text-align: center;">68.37 $\pm 22.7$</td>
<td style="text-align: center;">74.15 $\pm 19.3$</td>
<td style="text-align: center;">79.82 $\pm 17.5$</td>
<td style="text-align: center;">85.91 $\pm 15.6$</td>
<td style="text-align: center;">83.58 $\pm 16.5$</td>
<td style="text-align: center;">87.07 $\pm 14.3$</td>
<td style="text-align: center;">87.89 $\pm 13.69$</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0020</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0325</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.3128</td>
<td style="text-align: center;">0.0030</td>
<td style="text-align: center;">0.0088</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0039</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0716</td>
<td style="text-align: center;">0.0006</td>
<td style="text-align: center;">0.0088</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 6: AUC (\%) score of the proposed method compared with 8 baselines on 47 benchmark datasets. Each experiment is repeated 10 times with seed from 0 to 9, and mean value and standard deviation are reported. Mean is the average AUC score under all experiments. Mean rank is calculated out of 10 tested methods. Cardio. refers to Cardiotocography. Mammo. refers to mammography. Lympho. refers to Lymphography.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">DPAD</th>
<th style="text-align: center;">PLAD</th>
<th style="text-align: center;">NeuTraLAD</th>
<th style="text-align: center;">SCAD</th>
<th style="text-align: center;">AE</th>
<th style="text-align: center;">COPOD</th>
<th style="text-align: center;">DeepSVDD</th>
<th style="text-align: center;">ECOD</th>
<th style="text-align: center;">IForest</th>
<th style="text-align: center;">KNN</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">Ours- <br> ResMLP</th>
<th style="text-align: center;">Ours- <br> MLP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ALOI</td>
<td style="text-align: center;">14.15±0.2</td>
<td style="text-align: center;">9.40±0.8</td>
<td style="text-align: center;">7.79±0.6</td>
<td style="text-align: center;">11.79±0.6</td>
<td style="text-align: center;">8.02±0.1</td>
<td style="text-align: center;">5.26±0.2</td>
<td style="text-align: center;">6.92±0.5</td>
<td style="text-align: center;">5.42±0.1</td>
<td style="text-align: center;">6.72±0.6</td>
<td style="text-align: center;">15.87±0.3</td>
<td style="text-align: center;">20.16±0.8</td>
<td style="text-align: center;">14.32±0.6</td>
<td style="text-align: center;">13.53±0.7</td>
</tr>
<tr>
<td style="text-align: center;">anithyroid</td>
<td style="text-align: center;">63.97±3.6</td>
<td style="text-align: center;">76.22±6.8</td>
<td style="text-align: center;">42.06±5.0</td>
<td style="text-align: center;">54.68±0.9</td>
<td style="text-align: center;">51.02±1.7</td>
<td style="text-align: center;">31.6±0.3</td>
<td style="text-align: center;">21.99±3.0</td>
<td style="text-align: center;">39.03±0.3</td>
<td style="text-align: center;">56.69±2.3</td>
<td style="text-align: center;">63.73±1.6</td>
<td style="text-align: center;">67.87±1.6</td>
<td style="text-align: center;">75.39±2.6</td>
<td style="text-align: center;">74.74±1.6</td>
</tr>
<tr>
<td style="text-align: center;">backdoor</td>
<td style="text-align: center;">85.62±0.7</td>
<td style="text-align: center;">80.68±14.1</td>
<td style="text-align: center;">87.35±0.2</td>
<td style="text-align: center;">87.25±0.2</td>
<td style="text-align: center;">85.14±0.0</td>
<td style="text-align: center;">13.67±0.3</td>
<td style="text-align: center;">84.56±2.6</td>
<td style="text-align: center;">16.87±0.2</td>
<td style="text-align: center;">3.24±1.1</td>
<td style="text-align: center;">85.3±0.0</td>
<td style="text-align: center;">86.28±0.2</td>
<td style="text-align: center;">85.64±0.8</td>
<td style="text-align: center;">85.47±0.2</td>
</tr>
<tr>
<td style="text-align: center;">breastw</td>
<td style="text-align: center;">96.9±0.2</td>
<td style="text-align: center;">73.64±2.8</td>
<td style="text-align: center;">72.38±3.2</td>
<td style="text-align: center;">94.77±0.7</td>
<td style="text-align: center;">95.94±0.6</td>
<td style="text-align: center;">97.12±0.3</td>
<td style="text-align: center;">91.72±1.5</td>
<td style="text-align: center;">95.31±0.5</td>
<td style="text-align: center;">97.28±0.5</td>
<td style="text-align: center;">96.37±0.6</td>
<td style="text-align: center;">91.97±1.4</td>
<td style="text-align: center;">96.3±0.9</td>
<td style="text-align: center;">96.57±1.1</td>
</tr>
<tr>
<td style="text-align: center;">campaign</td>
<td style="text-align: center;">46.22±1.9</td>
<td style="text-align: center;">46.29±2.9</td>
<td style="text-align: center;">42.22±3.9</td>
<td style="text-align: center;">50.63±0.6</td>
<td style="text-align: center;">49.15±0.2</td>
<td style="text-align: center;">49.6±0.2</td>
<td style="text-align: center;">31.62±11.6</td>
<td style="text-align: center;">48.84±0.3</td>
<td style="text-align: center;">43.3±0.7</td>
<td style="text-align: center;">50.56±0.5</td>
<td style="text-align: center;">42.1±0.9</td>
<td style="text-align: center;">44.79±2.4</td>
<td style="text-align: center;">49.94±1.7</td>
</tr>
<tr>
<td style="text-align: center;">cardio</td>
<td style="text-align: center;">70.91±8.6</td>
<td style="text-align: center;">44.32±5.9</td>
<td style="text-align: center;">39.66±5.4</td>
<td style="text-align: center;">66.05±1.5</td>
<td style="text-align: center;">78.33±1.9</td>
<td style="text-align: center;">65.77±1.7</td>
<td style="text-align: center;">43.41±6.3</td>
<td style="text-align: center;">65.8±0.9</td>
<td style="text-align: center;">70.1±3.5</td>
<td style="text-align: center;">67.71±3.0</td>
<td style="text-align: center;">65.0±4.1</td>
<td style="text-align: center;">81.63±2.8</td>
<td style="text-align: center;">80.28±1.8</td>
</tr>
<tr>
<td style="text-align: center;">Cardiotocography</td>
<td style="text-align: center;">62.15±2.7</td>
<td style="text-align: center;">56.22±5.4</td>
<td style="text-align: center;">41.07±3.8</td>
<td style="text-align: center;">53.86±1.6</td>
<td style="text-align: center;">65.14±1.3</td>
<td style="text-align: center;">48.42±0.9</td>
<td style="text-align: center;">37.47±7.7</td>
<td style="text-align: center;">62.66±0.8</td>
<td style="text-align: center;">62.82±1.8</td>
<td style="text-align: center;">56.12±1.7</td>
<td style="text-align: center;">61.24±1.1</td>
<td style="text-align: center;">69.42±4.6</td>
<td style="text-align: center;">69.23±3.5</td>
</tr>
<tr>
<td style="text-align: center;">celebra</td>
<td style="text-align: center;">11.09±1.4</td>
<td style="text-align: center;">24.98±1.8</td>
<td style="text-align: center;">7.0±4.6</td>
<td style="text-align: center;">14.03±2.4</td>
<td style="text-align: center;">27.3±0.3</td>
<td style="text-align: center;">23.39±0.2</td>
<td style="text-align: center;">8.87±8.0</td>
<td style="text-align: center;">23.54±0.2</td>
<td style="text-align: center;">17.63±1.7</td>
<td style="text-align: center;">15.45±0.4</td>
<td style="text-align: center;">1.41±0.2</td>
<td style="text-align: center;">8.2±0.7</td>
<td style="text-align: center;">10.51±1.8</td>
</tr>
<tr>
<td style="text-align: center;">census</td>
<td style="text-align: center;">12.36±0.0</td>
<td style="text-align: center;">20.52±2.7</td>
<td style="text-align: center;">15.88±2.7</td>
<td style="text-align: center;">23.32±0.6</td>
<td style="text-align: center;">21.46±0.1</td>
<td style="text-align: center;">12.86±0.1</td>
<td style="text-align: center;">17.56±2.4</td>
<td style="text-align: center;">12.22±0.1</td>
<td style="text-align: center;">11.24±1.2</td>
<td style="text-align: center;">22.3±0.2</td>
<td style="text-align: center;">13.72±0.3</td>
<td style="text-align: center;">26.86±1.5</td>
<td style="text-align: center;">23.44±3.1</td>
</tr>
<tr>
<td style="text-align: center;">cover</td>
<td style="text-align: center;">25.37±24.1</td>
<td style="text-align: center;">33.63±2.3</td>
<td style="text-align: center;">19.27±16.1</td>
<td style="text-align: center;">65.25±9.7</td>
<td style="text-align: center;">20.16±5.0</td>
<td style="text-align: center;">18.17±0.2</td>
<td style="text-align: center;">2.72±2.9</td>
<td style="text-align: center;">23.54±0.2</td>
<td style="text-align: center;">11.33±1.9</td>
<td style="text-align: center;">74.89±0.3</td>
<td style="text-align: center;">79.03±2.1</td>
<td style="text-align: center;">91.38±0.3</td>
<td style="text-align: center;">89.97±1.0</td>
</tr>
<tr>
<td style="text-align: center;">donors</td>
<td style="text-align: center;">84.49±10.8</td>
<td style="text-align: center;">84.12±15.7</td>
<td style="text-align: center;">97.82±0.9</td>
<td style="text-align: center;">97.87±0.6</td>
<td style="text-align: center;">32.06±4.8</td>
<td style="text-align: center;">41.8±0.1</td>
<td style="text-align: center;">28.41±33.1</td>
<td style="text-align: center;">44.89±0.1</td>
<td style="text-align: center;">43.97±5.5</td>
<td style="text-align: center;">99.14±0.1</td>
<td style="text-align: center;">86.23±0.3</td>
<td style="text-align: center;">94.51±1.2</td>
<td style="text-align: center;">92.56±2.4</td>
</tr>
<tr>
<td style="text-align: center;">fault</td>
<td style="text-align: center;">67.7±0.7</td>
<td style="text-align: center;">60.22±3.2</td>
<td style="text-align: center;">67.19±1.1</td>
<td style="text-align: center;">72.62±0.9</td>
<td style="text-align: center;">54.87±0.6</td>
<td style="text-align: center;">47.77±0.7</td>
<td style="text-align: center;">50.88±4.9</td>
<td style="text-align: center;">48.65±0.1</td>
<td style="text-align: center;">61.68±1.7</td>
<td style="text-align: center;">73.13±1.0</td>
<td style="text-align: center;">64.19±0.6</td>
<td style="text-align: center;">68.03±0.9</td>
<td style="text-align: center;">68.3±1.0</td>
</tr>
<tr>
<td style="text-align: center;">fraud</td>
<td style="text-align: center;">51.91±10.9</td>
<td style="text-align: center;">78.66±6.0</td>
<td style="text-align: center;">45.05±21.9</td>
<td style="text-align: center;">65.13±2.4</td>
<td style="text-align: center;">32.44±0.6</td>
<td style="text-align: center;">44.72±0.5</td>
<td style="text-align: center;">52.48±14.7</td>
<td style="text-align: center;">38.86±0.6</td>
<td style="text-align: center;">30.77±4.5</td>
<td style="text-align: center;">47.12±2.6</td>
<td style="text-align: center;">0.0±0.0</td>
<td style="text-align: center;">60.19±9.2</td>
<td style="text-align: center;">78.68±1.6</td>
</tr>
<tr>
<td style="text-align: center;">glass</td>
<td style="text-align: center;">31.11±4.4</td>
<td style="text-align: center;">46.67±9.3</td>
<td style="text-align: center;">33.33±24.3</td>
<td style="text-align: center;">38.1±8.7</td>
<td style="text-align: center;">12.5±3.9</td>
<td style="text-align: center;">15.28±8.3</td>
<td style="text-align: center;">28.89±9.9</td>
<td style="text-align: center;">15.56±6.1</td>
<td style="text-align: center;">12.5±7.1</td>
<td style="text-align: center;">20.37±8.4</td>
<td style="text-align: center;">15.56±6.1</td>
<td style="text-align: center;">40.28±8.3</td>
<td style="text-align: center;">56.57±7.8</td>
</tr>
<tr>
<td style="text-align: center;">Hepatitis</td>
<td style="text-align: center;">60.0±12.3</td>
<td style="text-align: center;">51.92±3.8</td>
<td style="text-align: center;">29.23±10.2</td>
<td style="text-align: center;">36.26±8.6</td>
<td style="text-align: center;">51.92±6.8</td>
<td style="text-align: center;">53.85±0.0</td>
<td style="text-align: center;">44.62±10.0</td>
<td style="text-align: center;">40.0±3.4</td>
<td style="text-align: center;">50.96±10.0</td>
<td style="text-align: center;">55.13±5.8</td>
<td style="text-align: center;">52.31±3.4</td>
<td style="text-align: center;">65.38±4.1</td>
<td style="text-align: center;">69.23±4.9</td>
</tr>
<tr>
<td style="text-align: center;">http</td>
<td style="text-align: center;">99.68±0.1</td>
<td style="text-align: center;">99.22±0.8</td>
<td style="text-align: center;">1.9±0.1</td>
<td style="text-align: center;">84.57±31.3</td>
<td style="text-align: center;">92.09±0.8</td>
<td style="text-align: center;">1.99±0.0</td>
<td style="text-align: center;">9.91±17.7</td>
<td style="text-align: center;">2.2±0.0</td>
<td style="text-align: center;">13.9±15.3</td>
<td style="text-align: center;">99.56±0.1</td>
<td style="text-align: center;">0.05±0.0</td>
<td style="text-align: center;">99.25±0.5</td>
<td style="text-align: center;">99.57±0.2</td>
</tr>
<tr>
<td style="text-align: center;">InternetAds</td>
<td style="text-align: center;">82.34±0.7</td>
<td style="text-align: center;">79.46±1.8</td>
<td style="text-align: center;">81.63±1.2</td>
<td style="text-align: center;">81.17±2.1</td>
<td style="text-align: center;">79.93±1.3</td>
<td style="text-align: center;">50.75±0.5</td>
<td style="text-align: center;">81.3±1.5</td>
<td style="text-align: center;">50.43±0.3</td>
<td style="text-align: center;">24.12±2.6</td>
<td style="text-align: center;">80.12±2.1</td>
<td style="text-align: center;">80.65±0.7</td>
<td style="text-align: center;">84.24±1.4</td>
<td style="text-align: center;">84.19±0.8</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere</td>
<td style="text-align: center;">88.57±2.5</td>
<td style="text-align: center;">56.51±8.7</td>
<td style="text-align: center;">80.63±1.3</td>
<td style="text-align: center;">89.55±0.9</td>
<td style="text-align: center;">81.65±1.6</td>
<td style="text-align: center;">69.44±1.4</td>
<td style="text-align: center;">91.59±1.2</td>
<td style="text-align: center;">65.71±0.7</td>
<td style="text-align: center;">79.17±2.6</td>
<td style="text-align: center;">92.06±1.4</td>
<td style="text-align: center;">87.14±3.3</td>
<td style="text-align: center;">94.33±1.9</td>
<td style="text-align: center;">94.88±0.7</td>
</tr>
<tr>
<td style="text-align: center;">landsat</td>
<td style="text-align: center;">56.55±2.2</td>
<td style="text-align: center;">52.17±4.0</td>
<td style="text-align: center;">53.67±8.5</td>
<td style="text-align: center;">60.4±0.4</td>
<td style="text-align: center;">31.85±0.5</td>
<td style="text-align: center;">28.81±0.4</td>
<td style="text-align: center;">49.21±2.2</td>
<td style="text-align: center;">25.25±0.6</td>
<td style="text-align: center;">44.7±1.1</td>
<td style="text-align: center;">58.73±0.4</td>
<td style="text-align: center;">59.01±1.1</td>
<td style="text-align: center;">42.69±2.4</td>
<td style="text-align: center;">45.82±4.1</td>
</tr>
<tr>
<td style="text-align: center;">letter</td>
<td style="text-align: center;">35.2±5.3</td>
<td style="text-align: center;">44.50±6.6</td>
<td style="text-align: center;">58.6±6.5</td>
<td style="text-align: center;">63.17±3.9</td>
<td style="text-align: center;">13.88±0.8</td>
<td style="text-align: center;">14.0±1.4</td>
<td style="text-align: center;">26.0±2.5</td>
<td style="text-align: center;">16.0±1.2</td>
<td style="text-align: center;">16.88±3.4</td>
<td style="text-align: center;">46.83±3.9</td>
<td style="text-align: center;">48.4±3.2</td>
<td style="text-align: center;">67.79±2.0</td>
<td style="text-align: center;">65.45±2.8</td>
</tr>
<tr>
<td style="text-align: center;">Lymphography</td>
<td style="text-align: center;">83.33±0.0</td>
<td style="text-align: center;">70.83±23.3</td>
<td style="text-align: center;">0.0±0.0</td>
<td style="text-align: center;">88.89±13.6</td>
<td style="text-align: center;">79.17±24.8</td>
<td style="text-align: center;">93.75±8.6</td>
<td style="text-align: center;">70.0±18.3</td>
<td style="text-align: center;">90.0±9.1</td>
<td style="text-align: center;">83.33±8.9</td>
<td style="text-align: center;">80.56±24.5</td>
<td style="text-align: center;">83.33±16.7</td>
<td style="text-align: center;">86.9±17.9</td>
<td style="text-align: center;">75.76±21.6</td>
</tr>
<tr>
<td style="text-align: center;">magic</td>
<td style="text-align: center;">71.52±3.7</td>
<td style="text-align: center;">72.46±2.6</td>
<td style="text-align: center;">66.77±1.1</td>
<td style="text-align: center;">73.5±0.2</td>
<td style="text-align: center;">64.71±0.8</td>
<td style="text-align: center;">63.1±0.2</td>
<td style="text-align: center;">60.36±0.3</td>
<td style="text-align: center;">59.94±0.2</td>
<td style="text-align: center;">70.12±1.2</td>
<td style="text-align: center;">76.02±0.3</td>
<td style="text-align: center;">75.99±0.3</td>
<td style="text-align: center;">81.02±0.3</td>
<td style="text-align: center;">81.74±0.3</td>
</tr>
<tr>
<td style="text-align: center;">mammography</td>
<td style="text-align: center;">43.54±3.8</td>
<td style="text-align: center;">27.4±2.4</td>
<td style="text-align: center;">7.85±1.3</td>
<td style="text-align: center;">26.47±1.6</td>
<td style="text-align: center;">43.41±2.1</td>
<td style="text-align: center;">53.32±0.8</td>
<td style="text-align: center;">27.54±12.8</td>
<td style="text-align: center;">53.31±1.2</td>
<td style="text-align: center;">38.27±5.3</td>
<td style="text-align: center;">42.31±0.8</td>
<td style="text-align: center;">36.54±0.8</td>
<td style="text-align: center;">50.71±3.0</td>
<td style="text-align: center;">49.83±1.9</td>
</tr>
<tr>
<td style="text-align: center;">mnist</td>
<td style="text-align: center;">59.46±5.8</td>
<td style="text-align: center;">58.50±5.5</td>
<td style="text-align: center;">55.0±1.9</td>
<td style="text-align: center;">67.62±2.0</td>
<td style="text-align: center;">65.8±1.0</td>
<td style="text-align: center;">38.57±0.6</td>
<td style="text-align: center;">34.94±5.6</td>
<td style="text-align: center;">33.71±1.2</td>
<td style="text-align: center;">53.55±4.4</td>
<td style="text-align: center;">72.26±0.7</td>
<td style="text-align: center;">69.89±1.4</td>
<td style="text-align: center;">70.22±1.9</td>
<td style="text-align: center;">69.66±1.4</td>
</tr>
<tr>
<td style="text-align: center;">musk</td>
<td style="text-align: center;">98.76±2.0</td>
<td style="text-align: center;">75.26±22.1</td>
<td style="text-align: center;">100.0±0.</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">48.97±1.1</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">55.05±1.6</td>
<td style="text-align: center;">54.12±16.7</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">100.0±0.0</td>
<td style="text-align: center;">100.0±0.0</td>
</tr>
<tr>
<td style="text-align: center;">opuligits</td>
<td style="text-align: center;">25.33±8.0</td>
<td style="text-align: center;">52.33±8.0</td>
<td style="text-align: center;">53.47±10.9</td>
<td style="text-align: center;">50.44±7.6</td>
<td style="text-align: center;">0.1±0.3</td>
<td style="text-align: center;">3.08±0.3</td>
<td style="text-align: center;">2.8±5.5</td>
<td style="text-align: center;">2.67±0.0</td>
<td style="text-align: center;">14.0±6.1</td>
<td style="text-align: center;">32.0±7.2</td>
<td style="text-align: center;">58.27±5.0</td>
<td style="text-align: center;">54.33±11.5</td>
<td style="text-align: center;">37.93±4.9</td>
</tr>
<tr>
<td style="text-align: center;">PageBlocks</td>
<td style="text-align: center;">80.63±1.6</td>
<td style="text-align: center;">68.24±4.4</td>
<td style="text-align: center;">62.67±1.8</td>
<td style="text-align: center;">75.42±2.0</td>
<td style="text-align: center;">68.96±0.5</td>
<td style="text-align: center;">48.97±0.9</td>
<td style="text-align: center;">60.31±9.5</td>
<td style="text-align: center;">57.22±0.7</td>
<td style="text-align: center;">63.46±1.8</td>
<td style="text-align: center;">78.99±0.9</td>
<td style="text-align: center;">81.29±0.5</td>
<td style="text-align: center;">83.37±1.0</td>
<td style="text-align: center;">83.94±1.4</td>
</tr>
<tr>
<td style="text-align: center;">pendigits</td>
<td style="text-align: center;">49.36±19.0</td>
<td style="text-align: center;">44.39±18.8</td>
<td style="text-align: center;">36.54±23.2</td>
<td style="text-align: center;">76.92±7.9</td>
<td style="text-align: center;">44.23±1.3</td>
<td style="text-align: center;">35.18±0.2</td>
<td style="text-align: center;">10.9±9.0</td>
<td style="text-align: center;">42.18±0.3</td>
<td style="text-align: center;">55.93±3.9</td>
<td style="text-align: center;">93.7±1.3</td>
<td style="text-align: center;">75.13±4.2</td>
<td style="text-align: center;">86.97±2.2</td>
<td style="text-align: center;">83.72±3.6</td>
</tr>
<tr>
<td style="text-align: center;">Pima</td>
<td style="text-align: center;">66.19±1.9</td>
<td style="text-align: center;">62.97±1.8</td>
<td style="text-align: center;">51.72±1.8</td>
<td style="text-align: center;">64.99±2.3</td>
<td style="text-align: center;">67.96±0.5</td>
<td style="text-align: center;">62.03±1.5</td>
<td style="text-align: center;">54.93±1.1</td>
<td style="text-align: center;">58.21±0.8</td>
<td style="text-align: center;">68.61±1.6</td>
<td style="text-align: center;">69.9±1.4</td>
<td style="text-align: center;">67.24±1.6</td>
<td style="text-align: center;">68.41±1.8</td>
<td style="text-align: center;">69.85±1.7</td>
</tr>
<tr>
<td style="text-align: center;">satellite</td>
<td style="text-align: center;">75.63±1.1</td>
<td style="text-align: center;">51.45±4.0</td>
<td style="text-align: center;">68.26±1.1</td>
<td style="text-align: center;">77.72±0.3</td>
<td style="text-align: center;">62.53±0.6</td>
<td style="text-align: center;">56.8±0.4</td>
<td style="text-align: center;">65.11±2.2</td>
<td style="text-align: center;">53.94±0.1</td>
<td style="text-align: center;">69.17±0.5</td>
<td style="text-align: center;">76.35±0.3</td>
<td style="text-align: center;">75.83±0.5</td>
<td style="text-align: center;">73.01±0.8</td>
<td style="text-align: center;">73.87±0.5</td>
</tr>
<tr>
<td style="text-align: center;">satimage</td>
<td style="text-align: center;">89.3±3.0</td>
<td style="text-align: center;">32.04±3.5</td>
<td style="text-align: center;">11.83±4.0</td>
<td style="text-align: center;">93.66±2.1</td>
<td style="text-align: center;">85.51±0.7</td>
<td style="text-align: center;">77.29±1.2</td>
<td style="text-align: center;">87.89±1.9</td>
<td style="text-align: center;">71.55±0.6</td>
<td style="text-align: center;">86.09±1.9</td>
<td style="text-align: center;">92.25±1.9</td>
<td style="text-align: center;">85.35±3.2</td>
<td style="text-align: center;">94.77±0.7</td>
<td style="text-align: center;">90.7±2.5</td>
</tr>
<tr>
<td style="text-align: center;">shuttle</td>
<td style="text-align: center;">98.17±0.2</td>
<td style="text-align: center;">98.63±0.3</td>
<td style="text-align: center;">98.15±1.0</td>
<td style="text-align: center;">99.01±0.1</td>
<td style="text-align: center;">95.95±0.2</td>
<td style="text-align: center;">95.96±0.1</td>
<td style="text-align: center;">97.56±0.5</td>
<td style="text-align: center;">91.55±0.3</td>
<td style="text-align: center;">94.72±1.8</td>
<td style="text-align: center;">98.03±0.2</td>
<td style="text-align: center;">98.26±0.2</td>
<td style="text-align: center;">98.56±0.2</td>
<td style="text-align: center;">98.48±0.2</td>
</tr>
<tr>
<td style="text-align: center;">skin</td>
<td style="text-align: center;">96.05±1.5</td>
<td style="text-align: center;">93.16±2.4</td>
<td style="text-align: center;">71.7±3.8</td>
<td style="text-align: center;">54.57±17.3</td>
<td style="text-align: center;">38.65±4.1</td>
<td style="text-align: center;">19.63±0.1</td>
<td style="text-align: center;">44.62±3.2</td>
<td style="text-align: center;">21.88±0.0</td>
<td style="text-align: center;">77.11±1.6</td>
<td style="text-align: center;">97.72±0.1</td>
<td style="text-align: center;">80.39±1.7</td>
<td style="text-align: center;">91.07±0.3</td>
<td style="text-align: center;">94.35±0.5</td>
</tr>
<tr>
<td style="text-align: center;">sunip</td>
<td style="text-align: center;">66.67±0.0</td>
<td style="text-align: center;">52.50±16.0</td>
<td style="text-align: center;">48.67±10.9</td>
<td style="text-align: center;">42.78±14.7</td>
<td style="text-align: center;">66.67±0.0</td>
<td style="text-align: center;">0.0±0.0</td>
<td style="text-align: center;">55.33±23.5</td>
<td style="text-align: center;">66.67±0.0</td>
<td style="text-align: center;">0.0±0.0</td>
<td style="text-align: center;">66.67±0.0</td>
<td style="text-align: center;">38.0±21.0</td>
<td style="text-align: center;">63.89±6.8</td>
<td style="text-align: center;">66.67±0.0</td>
</tr>
<tr>
<td style="text-align: center;">SpamBase</td>
<td style="text-align: center;">78.13±0.7</td>
<td style="text-align: center;">74.14±3.0</td>
<td style="text-align: center;">75.84±1.6</td>
<td style="text-align: center;">81.68±1.3</td>
<td style="text-align: center;">76.53±1.0</td>
<td style="text-align: center;">69.59±0.3</td>
<td style="text-align: center;">71.82±3.3</td>
<td style="text-align: center;">67.29±0.1</td>
<td style="text-align: center;">79.37±1.6</td>
<td style="text-align: center;">79.66±0.7</td>
<td style="text-align: center;">77.52±1.3</td>
<td style="text-align: center;">80.76±0.9</td>
<td style="text-align: center;">80.87±1.1</td>
</tr>
<tr>
<td style="text-align: center;">speech</td>
<td style="text-align: center;">8.85±2.5</td>
<td style="text-align: center;">10.25±4.5</td>
<td style="text-align: center;">7.54±4.2</td>
<td style="text-align: center;">5.25±2.7</td>
<td style="text-align: center;">4.92±0.0</td>
<td style="text-align: center;">3.28±0.0</td>
<td style="text-align: center;">3.93±1.9</td>
<td style="text-align: center;">4.92±0.0</td>
<td style="text-align: center;">5.33±2.1</td>
<td style="text-align: center;">4.92±1.0</td>
<td style="text-align: center;">6.23±0.7</td>
<td style="text-align: center;">12.02±2.0</td>
<td style="text-align: center;">13.11±1.5</td>
</tr>
<tr>
<td style="text-align: center;">Stamps</td>
<td style="text-align: center;">75.48±4.8</td>
<td style="text-align: center;">30.65±10.4</td>
<td style="text-align: center;">24.52±12.0</td>
<td style="text-align: center;">54.19±11.9</td>
<td style="text-align: center;">61.75±8.2</td>
<td style="text-align: center;">67.74±3.9</td>
<td style="text-align: center;">37.42±9.6</td>
<td style="text-align: center;">48.39±6.5</td>
<td style="text-align: center;">61.29±5.7</td>
<td style="text-align: center;">64.52±8.2</td>
<td style="text-align: center;">62.58±8.1</td>
<td style="text-align: center;">73.66±2.4</td>
<td style="text-align: center;">74.84±5.6</td>
</tr>
<tr>
<td style="text-align: center;">thyroid</td>
<td style="text-align: center;">76.77±5.5</td>
<td style="text-align: center;">74.19±6.5</td>
<td style="text-align: center;">7.96±7.0</td>
<td style="text-align: center;">61.72±7.2</td>
<td style="text-align: center;">76.5±1.1</td>
<td style="text-align: center;">30.24±0.4</td>
<td style="text-align: center;">47.96±17.6</td>
<td style="text-align: center;">61.94±1.4</td>
<td style="text-align: center;">81.32±2.9</td>
<td style="text-align: center;">73.84±1.6</td>
<td style="text-align: center;">58.06±5.0</td>
<td style="text-align: center;">70.25±5.9</td>
<td style="text-align: center;">76.24±5.2</td>
</tr>
<tr>
<td style="text-align: center;">vertebral</td>
<td style="text-align: center;">31.33±9.6</td>
<td style="text-align: center;">31.33±14.5</td>
<td style="text-align: center;">28.67±9.6</td>
<td style="text-align: center;">20.67±2.8</td>
<td style="text-align: center;">17.14±1.3</td>
<td style="text-align: center;">0.42±1.2</td>
<td style="text-align: center;">9.33±4.3</td>
<td style="text-align: center;">13.33±0.0</td>
<td style="text-align: center;">15.0±3.6</td>
<td style="text-align: center;">13.33±3.0</td>
<td style="text-align: center;">16.67±4.1</td>
<td style="text-align: center;">42.22±8.3</td>
<td style="text-align: center;">41.0±3.9</td>
</tr>
<tr>
<td style="text-align: center;">vowels</td>
<td style="text-align: center;">43.2±6.9</td>
<td style="text-align: center;">39.2±11.2</td>
<td style="text-align: center;">65.2±8.7</td>
<td style="text-align: center;">88.0±3.3</td>
<td style="text-align: center;">19.71±0.8</td>
<td style="text-align: center;">4.75±1.0</td>
<td style="text-align: center;">33.2±9.4</td>
<td style="text-align: center;">22.0±0.0</td>
<td style="text-align: center;">28.25±4.8</td>
<td style="text-align: center;">65.33±4.3</td>
<td style="text-align: center;">56.8±4.1</td>
<td style="text-align: center;">86.67±2.4</td>
<td style="text-align: center;">85.4±2.5</td>
</tr>
<tr>
<td style="text-align: center;">Waveform</td>
<td style="text-align: center;">20.6±2.9</td>
<td style="text-align: center;">57.00±3.4</td>
<td style="text-align: center;">47.8±1.6</td>
<td style="text-align: center;">19.75±4.4</td>
<td style="text-align: center;">9.14±0.7</td>
<td style="text-align: center;">9.5±1.1</td>
<td style="text-align: center;">15.8±2.5</td>
<td style="text-align: center;">7.4±0.5</td>
<td style="text-align: center;">10.75±1.8</td>
<td style="text-align: center;">29.5±2.7</td>
<td style="text-align: center;">28.8±1.1</td>
<td style="text-align: center;">27.0±3.3</td>
<td style="text-align: center;">20.2±2.5</td>
</tr>
<tr>
<td style="text-align: center;">WBC</td>
<td style="text-align: center;">82.0±4.0</td>
<td style="text-align: center;">30.00±33.9</td>
<td style="text-align: center;">12.0±7.5</td>
<td style="text-align: center;">57.5±5.0</td>
<td style="text-align: center;">85.71±5.3</td>
<td style="text-align: center;">87.5±7.1</td>
<td style="text-align: center;">54.0±11.4</td>
<td style="text-align: center;">88.0±4.5</td>
<td style="text-align: center;">86.25±5.2</td>
<td style="text-align: center;">83.33±8.2</td>
<td style="text-align: center;">68.0±11.0</td>
<td style="text-align: center;">81.67±7.5</td>
<td style="text-align: center;">83.0±9.5</td>
</tr>
<tr>
<td style="text-align: center;">WDBC</td>
<td style="text-align: center;">72.0±11.7</td>
<td style="text-align: center;">20.00±4.5</td>
<td style="text-align: center;">0.0±0.0</td>
<td style="text-align: center;">70.0±8.2</td>
<td style="text-align: center;">78.57±9.0</td>
<td style="text-align: center;">81.25±3.5</td>
<td style="text-align: center;">72.0±8.4</td>
<td style="text-align: center;">50.0±7.1</td>
<td style="text-align: center;">72.5±8.9</td>
<td style="text-align: center;">76.67±5.2</td>
<td style="text-align: center;">80.0±10.0</td>
<td style="text-align: center;">98.33±4.1</td>
<td style="text-align: center;">92.0±7.9</td>
</tr>
<tr>
<td style="text-align: center;">Wilt</td>
<td style="text-align: center;">14.86±5.4</td>
<td style="text-align: center;">89.88±3.4</td>
<td style="text-align: center;">28.87±5.4</td>
<td style="text-align: center;">31.23±5.5</td>
<td style="text-align: center;">2.95±0.7</td>
<td style="text-align: center;">1.61±0.1</td>
<td style="text-align: center;">3.97±1.4</td>
<td style="text-align: center;">5.68±0.2</td>
<td style="text-align: center;">1.9±0.4</td>
<td style="text-align: center;">2.01±0.5</td>
<td style="text-align: center;">17.98±4.0</td>
<td style="text-align: center;">62.52±5.7</td>
<td style="text-align: center;">68.6±2.9</td>
</tr>
<tr>
<td style="text-align: center;">wine</td>
<td style="text-align: center;">82.0±17.2</td>
<td style="text-align: center;">46.0±38.5</td>
<td style="text-align: center;">24.0±13.6</td>
<td style="text-align: center;">72.5±15.0</td>
<td style="text-align: center;">60.0±8.2</td>
<td style="text-align: center;">58.75±3.5</td>
<td style="text-align: center;">78.0±13.0</td>
<td style="text-align: center;">34.0±8.9</td>
<td style="text-align: center;">63.75±13.0</td>
<td style="text-align: center;">71.67±11.7</td>
<td style="text-align: center;">74.0±8.9</td>
<td style="text-align: center;">90.0±12.6</td>
<td style="text-align: center;">93.0±6.7</td>
</tr>
<tr>
<td style="text-align: center;">WPBC</td>
<td style="text-align: center;">48.09±3.2</td>
<td style="text-align: center;">48.94±5.3</td>
<td style="text-align: center;">41.7±5.1</td>
<td style="text-align: center;">32.98±2.1</td>
<td style="text-align: center;">36.17±5.2</td>
<td style="text-align: center;">31.38±4.5</td>
<td style="text-align: center;">37.02±2.4</td>
<td style="text-align: center;">36.17±0.0</td>
<td style="text-align: center;">35.64±2.2</td>
<td style="text-align: center;">35.46±2.6</td>
<td style="text-align: center;">38.3±3.4</td>
<td style="text-align: center;">50.35±2.6</td>
<td style="text-align: center;">51.06±2.0</td>
</tr>
<tr>
<td style="text-align: center;">yeast</td>
<td style="text-align: center;">52.54±2.7</td>
<td style="text-align: center;">58.54±2.3</td>
<td style="text-align: center;">52.24±3.2</td>
<td style="text-align: center;">52.07±1.9</td>
<td style="text-align: center;">44.24±0.8</td>
<td style="text-align: center;">41.72±0.6</td>
<td style="text-align: center;">47.61±1.2</td>
<td style="text-align: center;">46.43±1.2</td>
<td style="text-align: center;">43.44±1.0</td>
<td style="text-align: center;">47.4±1.1</td>
<td style="text-align: center;">49.03±1.1</td>
<td style="text-align: center;">57.66±1.3</td>
<td style="text-align: center;">57.22±1.6</td>
</tr>
<tr>
<td style="text-align: center;">mean rank</td>
<td style="text-align: center;">4.53</td>
<td style="text-align: center;">6.38</td>
<td style="text-align: center;">7.63</td>
<td style="text-align: center;">4.91</td>
<td style="text-align: center;">6.97</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">8.60</td>
<td style="text-align: center;">8.74</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">4.36</td>
<td style="text-align: center;">5.57</td>
<td style="text-align: center;">2.85</td>
<td style="text-align: center;">4.70</td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">60.34±27.4</td>
<td style="text-align: center;">55.1±25.1</td>
<td style="text-align: center;">44.1±29.1</td>
<td style="text-align: center;">60.64±26.2</td>
<td style="text-align: center;">51.95±29.0</td>
<td style="text-align: center;">40.82±28.1</td>
<td style="text-align: center;">44.56±29.5</td>
<td style="text-align: center;">42.22±25.1</td>
<td style="text-align: center;">46.22±29.1</td>
<td style="text-align: center;">62.66±27.8</td>
<td style="text-align: center;">57.06±28.4</td>
<td style="text-align: center;">69.09±24.7</td>
<td style="text-align: center;">69.4±24.5</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0011</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.68699</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0017</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.68699</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: F1 (\%) score of the proposed method compared with 8 baselines on 47 benchmark datasets. Each experiment is repeated 10 times with seed from 0 to 9, and mean value and standard deviation are reported. Mean is the average F1 score under all experiments. Mean rank is calculated out of 10 tested methods. Cardio. refers to Cardiotocography. Mammo. refers to mammography. Lympho. refers to Lymphography.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">DPAD</th>
<th style="text-align: center;">PLAD</th>
<th style="text-align: center;">NeuTraLAD</th>
<th style="text-align: center;">SCAD</th>
<th style="text-align: center;">AE</th>
<th style="text-align: center;">AnoGAN</th>
<th style="text-align: center;">COPOD</th>
<th style="text-align: center;">DeepSVDD</th>
<th style="text-align: center;">ECOD</th>
<th style="text-align: center;">IForest</th>
<th style="text-align: center;">KNN</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">Ours- <br> ResMLP</th>
<th style="text-align: center;">Ours- <br> MLP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">abalone</td>
<td style="text-align: center;">97.23 $\pm 2.7$</td>
<td style="text-align: center;">97.12 $\pm 2.8$</td>
<td style="text-align: center;">96.96 $\pm 2.9$</td>
<td style="text-align: center;">97.05 $\pm 2.8$</td>
<td style="text-align: center;">97.17 $\pm 2.6$</td>
<td style="text-align: center;">97.13 $\pm 2.7$</td>
<td style="text-align: center;">97.01 $\pm 2.7$</td>
<td style="text-align: center;">96.99 $\pm 2.9$</td>
<td style="text-align: center;">97.00 $\pm 2.7$</td>
<td style="text-align: center;">97.22 $\pm 2.6$</td>
<td style="text-align: center;">97.09 $\pm 2.8$</td>
<td style="text-align: center;">97.15 $\pm 2.7$</td>
<td style="text-align: center;">97.38 $\pm 2.5$</td>
<td style="text-align: center;">97.39 $\pm 2.5$</td>
</tr>
<tr>
<td style="text-align: center;">arrhythmia</td>
<td style="text-align: center;">80.10 $\pm 0.8$</td>
<td style="text-align: center;">70.53 $\pm 5.0$</td>
<td style="text-align: center;">66.18 $\pm 0.0$</td>
<td style="text-align: center;">74.01 $\pm 0.7$</td>
<td style="text-align: center;">78.03 $\pm 1.0$</td>
<td style="text-align: center;">73.14 $\pm 1.8$</td>
<td style="text-align: center;">73.75 $\pm 1.1$</td>
<td style="text-align: center;">76.37 $\pm 1.4$</td>
<td style="text-align: center;">74.59 $\pm 1.5$</td>
<td style="text-align: center;">75.74 $\pm 1.4$</td>
<td style="text-align: center;">78.08 $\pm 1.2$</td>
<td style="text-align: center;">78.23 $\pm 1.2$</td>
<td style="text-align: center;">80.12 $\pm 1.4$</td>
<td style="text-align: center;">79.16 $\pm 1.4$</td>
</tr>
<tr>
<td style="text-align: center;">breastw</td>
<td style="text-align: center;">95.6 $\pm 0.6$</td>
<td style="text-align: center;">80.50 $\pm 8.7$</td>
<td style="text-align: center;">71.78 $\pm 3.0$</td>
<td style="text-align: center;">93.73 $\pm 0.9$</td>
<td style="text-align: center;">95.75 $\pm 0.6$</td>
<td style="text-align: center;">96.18 $\pm 0.9$</td>
<td style="text-align: center;">96.35 $\pm 0.8$</td>
<td style="text-align: center;">91.10 $\pm 1.5$</td>
<td style="text-align: center;">94.27 $\pm 0.7$</td>
<td style="text-align: center;">96.83 $\pm 0.7$</td>
<td style="text-align: center;">96.15 $\pm 0.8$</td>
<td style="text-align: center;">91.74 $\pm 2.0$</td>
<td style="text-align: center;">95.97 $\pm 0.8$</td>
<td style="text-align: center;">95.79 $\pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">cardio</td>
<td style="text-align: center;">67.09 $\pm 4.0$</td>
<td style="text-align: center;">62.85 $\pm 9.6$</td>
<td style="text-align: center;">42.38 $\pm 2.1$</td>
<td style="text-align: center;">54.63 $\pm 2.4$</td>
<td style="text-align: center;">65.36 $\pm 0.9$</td>
<td style="text-align: center;">58.28 $\pm 15.7$</td>
<td style="text-align: center;">47.82 $\pm 1.0$</td>
<td style="text-align: center;">39.66 $\pm 6.1$</td>
<td style="text-align: center;">61.54 $\pm 0.6$</td>
<td style="text-align: center;">65.03 $\pm 2.1$</td>
<td style="text-align: center;">59.01 $\pm 1.2$</td>
<td style="text-align: center;">62.44 $\pm 2.0$</td>
<td style="text-align: center;">70.15 $\pm 3.0$</td>
<td style="text-align: center;">71.22 $\pm 3.8$</td>
</tr>
<tr>
<td style="text-align: center;">ecoli</td>
<td style="text-align: center;">96.09 $\pm 1.7$</td>
<td style="text-align: center;">89.38 $\pm 10.5$</td>
<td style="text-align: center;">86.32 $\pm 9.4$</td>
<td style="text-align: center;">94.05 $\pm 2.2$</td>
<td style="text-align: center;">95.77 $\pm 2.1$</td>
<td style="text-align: center;">95.10 $\pm 2.1$</td>
<td style="text-align: center;">88.91 $\pm 4.2$</td>
<td style="text-align: center;">93.42 $\pm 2.5$</td>
<td style="text-align: center;">87.55 $\pm 6.2$</td>
<td style="text-align: center;">95.71 $\pm 1.7$</td>
<td style="text-align: center;">95.87 $\pm 1.8$</td>
<td style="text-align: center;">95.73 $\pm 1.8$</td>
<td style="text-align: center;">96.47 $\pm 1.5$</td>
<td style="text-align: center;">96.61 $\pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;">glass</td>
<td style="text-align: center;">90.00 $\pm 5.9$</td>
<td style="text-align: center;">84.75 $\pm 7.2$</td>
<td style="text-align: center;">87.66 $\pm 4.1$</td>
<td style="text-align: center;">89.87 $\pm 5.3$</td>
<td style="text-align: center;">87.60 $\pm 8.0$</td>
<td style="text-align: center;">88.81 $\pm 7.2$</td>
<td style="text-align: center;">84.95 $\pm 6.7$</td>
<td style="text-align: center;">88.60 $\pm 6.5$</td>
<td style="text-align: center;">84.96 $\pm 7.3$</td>
<td style="text-align: center;">89.10 $\pm 6.8$</td>
<td style="text-align: center;">89.11 $\pm 6.4$</td>
<td style="text-align: center;">86.70 $\pm 6.3$</td>
<td style="text-align: center;">91.31 $\pm 5.8$</td>
<td style="text-align: center;">91.76 $\pm 5.6$</td>
</tr>
<tr>
<td style="text-align: center;">ionosphere</td>
<td style="text-align: center;">91.11 $\pm 0.8$</td>
<td style="text-align: center;">64.29 $\pm 10.4$</td>
<td style="text-align: center;">85.71 $\pm 2.2$</td>
<td style="text-align: center;">90.32 $\pm 0.7$</td>
<td style="text-align: center;">79.98 $\pm 1.9$</td>
<td style="text-align: center;">78.10 $\pm 6.1$</td>
<td style="text-align: center;">70.57 $\pm 1.5$</td>
<td style="text-align: center;">89.75 $\pm 1.1$</td>
<td style="text-align: center;">66.37 $\pm 1.5$</td>
<td style="text-align: center;">80.58 $\pm 4.5$</td>
<td style="text-align: center;">91.61 $\pm 1.9$</td>
<td style="text-align: center;">87.91 $\pm 2.4$</td>
<td style="text-align: center;">95.24 $\pm 0.6$</td>
<td style="text-align: center;">92.63 $\pm 2.0$</td>
</tr>
<tr>
<td style="text-align: center;">kdd</td>
<td style="text-align: center;">88.04 $\pm 4.6$</td>
<td style="text-align: center;">93.34 $\pm 0.5$</td>
<td style="text-align: center;">94.2 $\pm 1.1$</td>
<td style="text-align: center;">93.81 $\pm 0.8$</td>
<td style="text-align: center;">93.74 $\pm 2.9$</td>
<td style="text-align: center;">91.25 $\pm 2.0$</td>
<td style="text-align: center;">90.93 $\pm 0.8$</td>
<td style="text-align: center;">87.06 $\pm 8.5$</td>
<td style="text-align: center;">91.63 $\pm 0.3$</td>
<td style="text-align: center;">94.26 $\pm 0.6$</td>
<td style="text-align: center;">94.69 $\pm 0.1$</td>
<td style="text-align: center;">95.58 $\pm 0.1$</td>
<td style="text-align: center;">96.50 $\pm 0.4$</td>
<td style="text-align: center;">96.72 $\pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;">letter</td>
<td style="text-align: center;">98.86 $\pm 0.3$</td>
<td style="text-align: center;">98.01 $\pm 0.1$</td>
<td style="text-align: center;">98.99 $\pm 0.2$</td>
<td style="text-align: center;">99.68 $\pm 0.1$</td>
<td style="text-align: center;">98.67 $\pm 0.3$</td>
<td style="text-align: center;">98.47 $\pm 0.3$</td>
<td style="text-align: center;">98.05 $\pm 0.1$</td>
<td style="text-align: center;">98.77 $\pm 0.2$</td>
<td style="text-align: center;">98.05 $\pm 0.1$</td>
<td style="text-align: center;">98.98 $\pm 0.2$</td>
<td style="text-align: center;">99.4 $\pm 0.2$</td>
<td style="text-align: center;">98.86 $\pm 0.3$</td>
<td style="text-align: center;">99.55 $\pm 0.1$</td>
<td style="text-align: center;">99.57 $\pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">lympho</td>
<td style="text-align: center;">77.05 $\pm 2.7$</td>
<td style="text-align: center;">68.85 $\pm 1.6$</td>
<td style="text-align: center;">58.36 $\pm 3.4$</td>
<td style="text-align: center;">74.59 $\pm 4.4$</td>
<td style="text-align: center;">75.91 $\pm 3.0$</td>
<td style="text-align: center;">71.80 $\pm 4.8$</td>
<td style="text-align: center;">61.61 $\pm 2.2$</td>
<td style="text-align: center;">73.77 $\pm 4.6$</td>
<td style="text-align: center;">62.76 $\pm 2.7$</td>
<td style="text-align: center;">73.36 $\pm 3.8$</td>
<td style="text-align: center;">75.78 $\pm 3.3$</td>
<td style="text-align: center;">75.55 $\pm 3.2$</td>
<td style="text-align: center;">80.56 $\pm 3.3$</td>
<td style="text-align: center;">81.73 $\pm 2.9$</td>
</tr>
<tr>
<td style="text-align: center;">mammo.</td>
<td style="text-align: center;">50.08 $\pm 4.0$</td>
<td style="text-align: center;">31.28 $\pm 7.5$</td>
<td style="text-align: center;">7.08 $\pm 1.1$</td>
<td style="text-align: center;">25.42 $\pm 3.2$</td>
<td style="text-align: center;">44.09 $\pm 2.7$</td>
<td style="text-align: center;">36.41 $\pm 11.9$</td>
<td style="text-align: center;">53.37 $\pm 0.9$</td>
<td style="text-align: center;">33.40 $\pm 12.4$</td>
<td style="text-align: center;">53.65 $\pm 0.8$</td>
<td style="text-align: center;">38.81 $\pm 3.6$</td>
<td style="text-align: center;">41.36 $\pm 2.0$</td>
<td style="text-align: center;">37.23 $\pm 1.4$</td>
<td style="text-align: center;">49.92 $\pm 5.0$</td>
<td style="text-align: center;">45.69 $\pm 5.1$</td>
</tr>
<tr>
<td style="text-align: center;">mulcross</td>
<td style="text-align: center;">99.95 $\pm 0.1$</td>
<td style="text-align: center;">99.67 $\pm 0.6$</td>
<td style="text-align: center;">25.41 $\pm 26.5$</td>
<td style="text-align: center;">99.95 $\pm 0.1$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">98.52 $\pm 2.0$</td>
<td style="text-align: center;">66.00 $\pm 0.1$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">74.58 $\pm 0.2$</td>
<td style="text-align: center;">99.11 $\pm 0.6$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
<td style="text-align: center;">100.0 $\pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">musk</td>
<td style="text-align: center;">50.29 $\pm 4.0$</td>
<td style="text-align: center;">53.84 $\pm 4.5$</td>
<td style="text-align: center;">62.73 $\pm 0.8$</td>
<td style="text-align: center;">66.90 $\pm 0.6$</td>
<td style="text-align: center;">6.99 $\pm 0.5$</td>
<td style="text-align: center;">24.29 $\pm 10.3$</td>
<td style="text-align: center;">11.68 $\pm 0.3$</td>
<td style="text-align: center;">44.30 $\pm 6.3$</td>
<td style="text-align: center;">15.59 $\pm 0.4$</td>
<td style="text-align: center;">26.65 $\pm 5.0$</td>
<td style="text-align: center;">63.64 $\pm 0.7$</td>
<td style="text-align: center;">66.58 $\pm 1.0$</td>
<td style="text-align: center;">71.84 $\pm 1.9$</td>
<td style="text-align: center;">68.78 $\pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;">epidigits</td>
<td style="text-align: center;">96.66 $\pm 1.7$</td>
<td style="text-align: center;">91.73 $\pm 2.5$</td>
<td style="text-align: center;">92.83 $\pm 2.2$</td>
<td style="text-align: center;">98.64 $\pm 0.9$</td>
<td style="text-align: center;">97.95 $\pm 1.1$</td>
<td style="text-align: center;">95.84 $\pm 1.7$</td>
<td style="text-align: center;">92.18 $\pm 1.4$</td>
<td style="text-align: center;">96.95 $\pm 1.4$</td>
<td style="text-align: center;">92.04 $\pm 1.4$</td>
<td style="text-align: center;">98.13 $\pm 1.1$</td>
<td style="text-align: center;">98.60 $\pm 0.8$</td>
<td style="text-align: center;">98.63 $\pm 0.8$</td>
<td style="text-align: center;">98.80 $\pm 0.8$</td>
<td style="text-align: center;">98.91 $\pm 0.8$</td>
</tr>
<tr>
<td style="text-align: center;">pendigits</td>
<td style="text-align: center;">98.81 $\pm 0.7$</td>
<td style="text-align: center;">96.28 $\pm 1.2$</td>
<td style="text-align: center;">98.76 $\pm 0.6$</td>
<td style="text-align: center;">99.64 $\pm 0.3$</td>
<td style="text-align: center;">98.48 $\pm 1.1$</td>
<td style="text-align: center;">98.15 $\pm 1.2$</td>
<td style="text-align: center;">94.82 $\pm 1.0$</td>
<td style="text-align: center;">97.53 $\pm 1.1$</td>
<td style="text-align: center;">94.77 $\pm 0.5$</td>
<td style="text-align: center;">99.04 $\pm 0.5$</td>
<td style="text-align: center;">99.59 $\pm 0.3$</td>
<td style="text-align: center;">99.38 $\pm 0.5$</td>
<td style="text-align: center;">99.77 $\pm 0.2$</td>
<td style="text-align: center;">99.78 $\pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">ptma</td>
<td style="text-align: center;">67.31 $\pm 1.3$</td>
<td style="text-align: center;">64.55 $\pm 3.3$</td>
<td style="text-align: center;">51.34 $\pm 3.2$</td>
<td style="text-align: center;">65.07 $\pm 1.1$</td>
<td style="text-align: center;">67.72 $\pm 1.2$</td>
<td style="text-align: center;">67.01 $\pm 3.1$</td>
<td style="text-align: center;">62.69 $\pm 1.2$</td>
<td style="text-align: center;">56.96 $\pm 2.6$</td>
<td style="text-align: center;">58.12 $\pm 0.9$</td>
<td style="text-align: center;">68.94 $\pm 1.4$</td>
<td style="text-align: center;">70.06 $\pm 1.0$</td>
<td style="text-align: center;">67.24 $\pm 1.2$</td>
<td style="text-align: center;">69.03 $\pm 2.0$</td>
<td style="text-align: center;">69.78 $\pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">satimage</td>
<td style="text-align: center;">94.76 $\pm 2.3$</td>
<td style="text-align: center;">83.49 $\pm 7.0$</td>
<td style="text-align: center;">83.19 $\pm 7.0$</td>
<td style="text-align: center;">94.76 $\pm 1.9$</td>
<td style="text-align: center;">95.05 $\pm 2.4$</td>
<td style="text-align: center;">94.75 $\pm 2.6$</td>
<td style="text-align: center;">89.43 $\pm 2.9$</td>
<td style="text-align: center;">86.66 $\pm 5.2$</td>
<td style="text-align: center;">87.21 $\pm 3.5$</td>
<td style="text-align: center;">96.05 $\pm 2.0$</td>
<td style="text-align: center;">95.65 $\pm 2.6$</td>
<td style="text-align: center;">93.89 $\pm 3.0$</td>
<td style="text-align: center;">95.57 $\pm 2.1$</td>
<td style="text-align: center;">95.50 $\pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;">seismic</td>
<td style="text-align: center;">34.12 $\pm 3.0$</td>
<td style="text-align: center;">35.29 $\pm 4.1$</td>
<td style="text-align: center;">17.18 $\pm 1.9$</td>
<td style="text-align: center;">29.06 $\pm 2.9$</td>
<td style="text-align: center;">28.12 $\pm 1.4$</td>
<td style="text-align: center;">29.56 $\pm 1.7$</td>
<td style="text-align: center;">30.07 $\pm 1.5$</td>
<td style="text-align: center;">24.89 $\pm 6.2$</td>
<td style="text-align: center;">29.9 $\pm 0.3$</td>
<td style="text-align: center;">29.36 $\pm 1.5$</td>
<td style="text-align: center;">30.22 $\pm 2.0$</td>
<td style="text-align: center;">16.12 $\pm 1.8$</td>
<td style="text-align: center;">31.18 $\pm 1.3$</td>
<td style="text-align: center;">34.03 $\pm 1.7$</td>
</tr>
<tr>
<td style="text-align: center;">shuttle</td>
<td style="text-align: center;">99.30 $\pm 1.1$</td>
<td style="text-align: center;">98.15 $\pm 3.4$</td>
<td style="text-align: center;">98.6 $\pm 2.6$</td>
<td style="text-align: center;">99.38 $\pm 0.8$</td>
<td style="text-align: center;">92.72 $\pm 13.2$</td>
<td style="text-align: center;">91.74 $\pm 16.2$</td>
<td style="text-align: center;">87.8 $\pm 14.3$</td>
<td style="text-align: center;">96.67 $\pm 11.0$</td>
<td style="text-align: center;">88.32 $\pm 14.4$</td>
<td style="text-align: center;">93.28 $\pm 12.0$</td>
<td style="text-align: center;">99.24 $\pm 1.1$</td>
<td style="text-align: center;">98.98 $\pm 1.3$</td>
<td style="text-align: center;">99.41 $\pm 0.9$</td>
<td style="text-align: center;">99.41 $\pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">speech</td>
<td style="text-align: center;">7.54 $\pm 2.5$</td>
<td style="text-align: center;">9.64 $\pm 3.8$</td>
<td style="text-align: center;">4.92 $\pm 4.1$</td>
<td style="text-align: center;">5.41 $\pm 2.2$</td>
<td style="text-align: center;">4.64 $\pm 0.6$</td>
<td style="text-align: center;">3.28 $\pm 2.3$</td>
<td style="text-align: center;">3.28 $\pm 0.0$</td>
<td style="text-align: center;">4.31 $\pm 2.3$</td>
<td style="text-align: center;">4.92 $\pm 0.0$</td>
<td style="text-align: center;">4.37 $\pm 2.3$</td>
<td style="text-align: center;">5.76 $\pm 0.9$</td>
<td style="text-align: center;">6.84 $\pm 1.0$</td>
<td style="text-align: center;">13.35 $\pm 1.8$</td>
<td style="text-align: center;">11.94 $\pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">thyroid</td>
<td style="text-align: center;">55.52 $\pm 3.9$</td>
<td style="text-align: center;">69.20 $\pm 9.9$</td>
<td style="text-align: center;">27.6 $\pm 7.2$</td>
<td style="text-align: center;">42.48 $\pm 2.6$</td>
<td style="text-align: center;">42.64 $\pm 0.8$</td>
<td style="text-align: center;">37.20 $\pm 1.2$</td>
<td style="text-align: center;">24.40 $\pm 0.0$</td>
<td style="text-align: center;">18.31 $\pm 4.8$</td>
<td style="text-align: center;">30.80 $\pm 0.0$</td>
<td style="text-align: center;">42.53 $\pm 3.2$</td>
<td style="text-align: center;">49.20 $\pm 0.0$</td>
<td style="text-align: center;">30.80 $\pm 0.0$</td>
<td style="text-align: center;">65.04 $\pm 1.0$</td>
<td style="text-align: center;">64.48 $\pm 1.3$</td>
</tr>
<tr>
<td style="text-align: center;">vertebral</td>
<td style="text-align: center;">90.19 $\pm 1.1$</td>
<td style="text-align: center;">81.27 $\pm 6.0$</td>
<td style="text-align: center;">81.14 $\pm 0.8$</td>
<td style="text-align: center;">86.33 $\pm 1.5$</td>
<td style="text-align: center;">89.97 $\pm 0.9$</td>
<td style="text-align: center;">89.62 $\pm 2.3$</td>
<td style="text-align: center;">87.24 $\pm 0.8$</td>
<td style="text-align: center;">85.17 $\pm 1.5$</td>
<td style="text-align: center;">82.63 $\pm 0.9$</td>
<td style="text-align: center;">89.38 $\pm 1.1$</td>
<td style="text-align: center;">88.93 $\pm 0.7$</td>
<td style="text-align: center;">89.51 $\pm 1.1$</td>
<td style="text-align: center;">92.04 $\pm 0.7$</td>
<td style="text-align: center;">91.50 $\pm 0.7$</td>
</tr>
<tr>
<td style="text-align: center;">vowels</td>
<td style="text-align: center;">40.4 $\pm 8.5$</td>
<td style="text-align: center;">48.67 $\pm 34.5$</td>
<td style="text-align: center;">62.91 $\pm 8.0$</td>
<td style="text-align: center;">86.20 $\pm 2.6$</td>
<td style="text-align: center;">19.94 $\pm 0.8$</td>
<td style="text-align: center;">9.60 $\pm 7.4$</td>
<td style="text-align: center;">4.44 $\pm 0.8$</td>
<td style="text-align: center;">33.54 $\pm 8.8$</td>
<td style="text-align: center;">21.20 $\pm 1.0$</td>
<td style="text-align: center;">28.33 $\pm 4.1$</td>
<td style="text-align: center;">64.86 $\pm 4.2$</td>
<td style="text-align: center;">57.49 $\pm 3.5$</td>
<td style="text-align: center;">87.71 $\pm 3.5$</td>
<td style="text-align: center;">86.57 $\pm 1.9$</td>
</tr>
<tr>
<td style="text-align: center;">wbc</td>
<td style="text-align: center;">89.25 $\pm 1.7$</td>
<td style="text-align: center;">66.67 $\pm 12.1$</td>
<td style="text-align: center;">57.26 $\pm 8.7$</td>
<td style="text-align: center;">86.89 $\pm 1.4$</td>
<td style="text-align: center;">88.97 $\pm 0.9$</td>
<td style="text-align: center;">87.45 $\pm 3.6$</td>
<td style="text-align: center;">79.53 $\pm 1.0$</td>
<td style="text-align: center;">83.61 $\pm 4.0$</td>
<td style="text-align: center;">62.74 $\pm 1.1$</td>
<td style="text-align: center;">89.9 $\pm 1.1$</td>
<td style="text-align: center;">88.54 $\pm 1.0$</td>
<td style="text-align: center;">88.96 $\pm 0.9$</td>
<td style="text-align: center;">92.65 $\pm 1.3$</td>
<td style="text-align: center;">92.05 $\pm 1.6$</td>
</tr>
<tr>
<td style="text-align: center;">wine</td>
<td style="text-align: center;">95.44 $\pm 3.6$</td>
<td style="text-align: center;">87.88 $\pm 5.3$</td>
<td style="text-align: center;">80.16 $\pm 4.4$</td>
<td style="text-align: center;">90.77 $\pm 4.4$</td>
<td style="text-align: center;">96.33 $\pm 4.0$</td>
<td style="text-align: center;">95.54 $\pm 4.7$</td>
<td style="text-align: center;">80.69 $\pm 2.8$</td>
<td style="text-align: center;">95.25 $\pm 3.8$</td>
<td style="text-align: center;">80.83 $\pm 4.1$</td>
<td style="text-align: center;">95.77 $\pm 2.5$</td>
<td style="text-align: center;">97.10 $\pm 2.6$</td>
<td style="text-align: center;">96.85 $\pm 2.4$</td>
<td style="text-align: center;">97.98 $\pm 2.0$</td>
<td style="text-align: center;">97.71 $\pm 2.3$</td>
</tr>
<tr>
<td style="text-align: center;">mean</td>
<td style="text-align: center;">92.67 $\pm 15.1$</td>
<td style="text-align: center;">89.00 $\pm 17.4$</td>
<td style="text-align: center;">87.95 $\pm 20.5$</td>
<td style="text-align: center;">92.8 $\pm 15.7$</td>
<td style="text-align: center;">91.37 $\pm 18.3$</td>
<td style="text-align: center;">88.58 $\pm 21.4$</td>
<td style="text-align: center;">87.83 $\pm 19.4$</td>
<td style="text-align: center;">90.39 $\pm 18.5$</td>
<td style="text-align: center;">87.87 $\pm 18.3$</td>
<td style="text-align: center;">91.85 $\pm 17.3$</td>
<td style="text-align: center;">93.43 $\pm 14.8$</td>
<td style="text-align: center;">92.6 $\pm 16.1$</td>
<td style="text-align: center;">94.23 $\pm 14.0$</td>
<td style="text-align: center;">94.18 $\pm 13.8$</td>
</tr>
<tr>
<td style="text-align: center;">mean rank</td>
<td style="text-align: center;">4.36</td>
<td style="text-align: center;">8.88</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">6.12</td>
<td style="text-align: center;">6.20</td>
<td style="text-align: center;">8.32</td>
<td style="text-align: center;">10.12</td>
<td style="text-align: center;">9.08</td>
<td style="text-align: center;">10.32</td>
<td style="text-align: center;">5.76</td>
<td style="text-align: center;">5.92</td>
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">1.52</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0292</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0006</td>
<td style="text-align: center;">0.0190</td>
<td style="text-align: center;">0.0068</td>
<td style="text-align: center;">0.0008</td>
<td style="text-align: center;">0.0007</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0106</td>
<td style="text-align: center;">0.0023</td>
<td style="text-align: center;">0.0024</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">0.0363</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0006</td>
<td style="text-align: center;">0.0198</td>
<td style="text-align: center;">0.0069</td>
<td style="text-align: center;">0.0008</td>
<td style="text-align: center;">0.0008</td>
<td style="text-align: center;">0.0002</td>
<td style="text-align: center;">0.0107</td>
<td style="text-align: center;">0.0030</td>
<td style="text-align: center;">0.0037</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 8: F1 (\%) score of the proposed method compared with 12 baselines on 25 benchmark datasets. Each experiment is repeated 10 times with seed from 0 to 9, and mean value and standard deviation are reported. Mean is the average F1 score under all experiments. Mean rank is calculated out of 10 tested methods. Mammo. refers to mammography.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/Minqi824/ADBench/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
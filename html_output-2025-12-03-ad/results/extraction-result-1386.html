<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1386 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1386</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1386</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-b43a51f83e7ca2f9022dda6e0f32beac3dcb1494</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b43a51f83e7ca2f9022dda6e0f32beac3dcb1494" target="_blank">Contrastive Variational Model-Based Reinforcement Learning for Complex Observations</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Contrastive Variational Reinforcement Learning (CVRL), an MBRL framework for complex natural observations that achieves comparable performance with the state-of-the-art (SOTA) generative MBRL approaches on a series of Mujoco tasks, and significantly outperforms SOTAs on Natural MuJoco tasks.</p>
                <p><strong>Paper Abstract:</strong> Deep model-based reinforcement learning (MBRL) has achieved great sample-efficiency and generalization in decision making for sophisticated simulated tasks, such as Atari games. However, real-world robot decision making requires reasoning with complex natural visual observations. This paper presents Contrastive Variational Reinforcement Learning (CVRL), an MBRL framework for complex natural observations. In contrast to the commonly used generative world models, CVRL learns a contrastive variational world model by maximizing the mutual information between latent states and observations discriminatively by contrastive learning. Contrastive learning avoids modeling the complex observation space and is significantly more robust than the standard generative world models. For decision making, CVRL discovers long-horizon behavior by online search guided by an actor-critic. CVRL achieves comparable performance with the state-of-the-art (SOTA) generative MBRL approaches on a series of Mujoco tasks, and significantly outperforms SOTAs on Natural Mujoco tasks, a new, more challenging continuous control RL benchmark with complex observations introduced in this paper.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1386.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1386.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CVRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Variational Reinforcement Learning (contrastive variational world model / CELBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL world model that learns latent dynamics via contrastive mutual-information maximization (InfoNCE) instead of pixel-level generative observation likelihoods, combined with a hybrid actor-critic and latent-guided MPC for decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Contrastive variational world model (CELBO)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-state world model built on an RSSM-style architecture: a deterministic recurrent state h_t (GRU) and a stochastic latent s_t. Instead of a generative p(o_t | s_t) observation decoder, it learns a discriminative compatibility function f_theta(s_t, o_t)=exp(z_t^T W s'_t) where z_t is a learned embedding of the image and s'_t an embedding of the latent; the model maximizes a contrastive evidence lower bound (CELBO) derived from mutual information estimated with InfoNCE. Dynamics p_theta(s_t | h_t) and reward model p_theta(r_t | h_t, s_t) are still learned; the encoder q_phi(s_t | h_t, o_t) approximates the posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (contrastive variational latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous-control robotics and MuJoCo-style simulated locomotion; robotic manipulation (box pushing) with complex visual observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Indirect: mutual information lower bound (InfoNCE / CELBO) replacing observation log-likelihood; downstream task returns and ELBO-style dynamics KL term; no explicit pixel MSE reported for the contrastive model.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>The paper does not report direct numeric prediction / reconstruction metrics (e.g., MSE). Fidelity is evaluated by downstream task performance: CVRL matches or exceeds baselines on many tasks (example: standard walker-walk 980.3 vs Dreamer 961.7) and strongly outperforms on complex-visual tasks (Natural walker-walk CVRL 941.5 vs Dreamer 206.6). CELBO is described as a lower bound on the ELBO but no numeric bound values are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representation is a learned dense neural embedding (200-d deterministic + 30-d stochastic); the model is largely a black-box neural latent model. The approach is claimed to discover state–observation correspondence but the paper provides no claim that latent dims map to explicit physical variables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>No explicit interpretability methods for latent semantics are used; assessment is via downstream task performance and qualitative comparison of generative reconstructions (for baselines) — reconstructions are visualized for the generative baseline to illustrate failure modes. The contrastive model uses compatibility scores (f_theta) that can be inspected but no analysis of latent semantics is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training reported on a single NVIDIA RTX 2080Ti GPU (Intel Xeon Gold CPU); models trained for 5e6 environment steps in primary experiments. Model sizes: deterministic state size 200, stochastic state size 30; encoder: 4 conv layers (channels 32, 64, 128, 256); contrastive embeddings via 200-d fully connected layers and a 200x200 weight matrix. Latent-guided MPC unrolls 15 steps and uses SGD updates (learning rate 0.003) at inference time. Four separate optimizers (model, value, actor, SAC) used during training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports higher sample efficiency than model-free SAC (CVRL and Dreamer each trained with 5e6 steps vs SAC baseline) and competitive or superior efficiency versus generative Dreamer on complex-visual tasks; also notes that avoiding pixel reconstruction simplifies optimization under complex observations. No wall-clock or FLOP comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Representative results from paper: On standard MuJoCo tasks CVRL comparable to Dreamer (e.g., walker-walk CVRL 980.3 vs Dreamer 961.7). On Natural MuJoCo (complex video backgrounds) CVRL outperforms Dreamer dramatically (e.g., walker-walk CVRL 941.5 vs Dreamer 206.6). In Box Pushing (PyBullet with randomized lighting) CVRL achieves the highest rewards and shortest execution length compared to Dreamer and SAC (no absolute numeric reward shown in text). Ablation: CVRL-generative variant and CVRL-reward-only perform much worse, indicating the utility of contrastive observation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>CELBO/contrastive world model prioritizes task-relevant, decision-making features rather than full observation reconstruction, which improves robustness and downstream policy quality in complex-visual domains. However, in simpler observation settings some generative methods can be better (paper notes Dreamer outperforms CVRL on some standard tasks like walker-run and cheetah-run). Hybrid actor-critic (Dreamer-style imagination + SAC off-policy signal) improves robustness when the latent model is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs highlighted: contrastive learning improves robustness to complex, variable observations and sample efficiency but depends strongly on negative-sample strategy and can underperform on some simple-observation tasks; contrastive model is less directly interpretable (latent embeddings) and provides no explicit pixel reconstruction; adding SAC (off-policy) sacrifices some model-based purity for robustness. Computational trade-off: latent-guided MPC adds online planning cost (unrolling, gradient updates) but improves long-horizon reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: replace generative p(o_t|s_t) with a contrastive InfoNCE compatibility f_theta(s_t,o_t) and optimize CELBO; use RSSM (deterministic GRU h_t + stochastic s_t) with sizes h:200, s:30; bi-linear compatibility f_theta = exp(z_t^T W s_t'); negative samples drawn from other observations in the same replay-batch (B*T-1 negatives); hybrid training objective combining Dreamer-style imagined-gradient actor loss and SAC off-policy loss; latent-guided MPC via shooting with analytic gradients and 15-step unrolls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Dreamer (generative RSSM with pixel decoder): CVRL is more robust under complex visual observations and often yields higher task scores on Natural MuJoCo and Box Pushing, while matching Dreamer on many standard tasks; Dreamer can outperform CVRL on some simple-observation tasks. Compared to model-free SAC: CVRL is far more sample-efficient, achieving higher returns with far fewer environment steps. Compared to a CVRL-generative ablation (same architecture + generative decoder), the contrastive variant performs substantially better in complex-visual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using contrastive CELBO for domains with complex, variable observations and combining it with hybrid actor-critic (latent imagination + SAC) and latent-guided MPC for robust long-horizon decision making. It also notes that the quality of contrastive learning depends on negative-sample selection and suggests future work on better sampling strategies (e.g., meta-learned sampling). No single numeric optimal configuration is prescribed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrastive Variational Model-Based Reinforcement Learning for Complex Observations', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1386.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1386.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State Space Model (deterministic + stochastic latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An architecture that decomposes latent world dynamics into a deterministic recurrent state (h_t, typically a GRU) and a stochastic latent s_t, used to model multi-step dynamics and enable latent imagination for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequential latent dynamics model with deterministic recurrent update h_t = f_theta(h_{t-1}, s_{t-1}, a_{t-1}) and stochastic state s_t ~ p_theta(s_t | h_t); observation model typically p_theta(o_t | s_t) in generative variants, and reward model p_theta(r_t | h_t, s_t). RSSM supports approximate posterior q_phi(s_t | h_t, o_t) and enables latent rollouts (imagination) for actor training and MPC.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent stochastic-deterministic hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based continuous control (MuJoCo), robotic tasks requiring multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically assessed via ELBO components (reconstruction log-likelihood of observations, KL divergence between posterior and prior) and downstream task performance; in this paper the RSSM prior/posterior KL term is retained in the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric fidelity values for RSSM prediction reported in this paper; RSSM serves as the backbone for both CVRL and Dreamer architectures and supports latent imagination that yields competitive downstream task results (see CVRL task performance numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>RSSM latent states are learned neural vectors (deterministic and stochastic) and are not directly interpretable as explicit physical quantities in the paper. RSSM facilitates multi-step memory through deterministic h_t but does not provide explicit interpretable variables by design.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper beyond inspection of downstream behavior and use of latent imagination; RSSM is treated as internal learned state for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lightweight recurrent architecture: deterministic size 200 (GRU), stochastic size 30 as used in experiments; supports efficient latent rollouts enabling sample-efficient training. No explicit FLOP or time costs reported beyond single-GPU training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>RSSM enables latent rollouts and analytic gradients for actor training (Dreamer-style), reducing required environment interactions versus pure model-free methods; this property is used by both CVRL and Dreamer to achieve better sample efficiency than SAC in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>RSSM-based agents (Dreamer baseline) achieve strong performance on standard MuJoCo tasks but degrade under complex visual backgrounds; integrated into CVRL (with contrastive observation learning) RSSM supports higher robustness under complex observations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM is an effective backbone for latent imagination and planning; its utility depends on the quality of the observation model—when paired with generative pixel decoders under complex observations, task performance suffers due to reconstruction difficulty; pairing RSSM with contrastive observation learning (as in CVRL) preserves planning utility while avoiding pixel reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM gives good temporal memory and enables imagination-based training but requires a compatible observation-learning method: generative decoders increase modelling burden under complex visuals (worse performance), while discriminative/contrastive observation learning reduces that burden. RSSM size choices (h and s dimensions) trade representational capacity vs compute.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use deterministic GRU for h_t (size 200) and stochastic latent s_t (size 30); approximate posterior q_phi(s_t | h_t, o_t); supports both generative decoders and contrastive discriminators for observation coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to purely generative latent models, RSSM with contrastive observation learning (CVRL) is more robust under complex observations; compared to model-free approaches RSSM enables imagination and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper uses h=200, s=30 as configuration that balances capacity and efficiency for MuJoCo/robot tasks; recommends pairing RSSM with observation-learning methods suited to observation complexity (contrastive CELBO for complex visuals, generative decoders may suffice for simple visuals).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrastive Variational Model-Based Reinforcement Learning for Complex Observations', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1386.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1386.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer (generative latent world model with pixel decoder / latent imagination)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art generative latent world model that trains an RSSM with a pixel-level generative decoder and uses latent imagination to learn behaviors and value functions (used as a baseline in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer (generative RSSM with pixel decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM backbone with a generative observation model p_theta(o_t | s_t) (pixel decoder) trained via ELBO (reconstruction log-likelihood + KL dynamics), and latent imagination used to compute imagined trajectories for actor-critic learning (Dreamer objective).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent generative world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pixel-based continuous control (MuJoCo), classic benchmarks for model-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood / ELBO (reconstruction loss) and downstream task returns; pixel reconstruction quality inspected qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper Dreamer attains strong performance on standard MuJoCo tasks (e.g., walker-walk 961.7) but degrades substantially on Natural MuJoCo with complex video backgrounds (e.g., Natural walker-walk 206.6), attributed to difficulty of pixel-level reconstruction under complex observations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations are neural embeddings; pixel decoder allows visualizing reconstructions to qualitatively assess what is being modeled (paper shows blurry reconstructions that lose agent details under complex backgrounds). Overall model remains a black-box neural system.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of pixel-level reconstructions from the generative decoder; ELBO terms provide diagnostic signals (reconstruction vs KL).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable RSSM backbone as used in CVRL; training in paper performed on single 2080Ti GPU for 5e6 steps; generative decoder training adds pixel reconstruction computation but no explicit wall-clock numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dreamer is sample-efficient relative to model-free baselines on standard tasks but less robust (thus less effective) under complex visual observations compared to CVRL's contrastive approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Examples from paper: Standard walker-walk Dreamer 961.7 (comparable to CVRL 980.3); Natural walker-walk Dreamer 206.6 (CVRL 941.5). Other tasks show similar patterns: strong on simple visuals, poor on complex video backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity pixel reconstruction can help in simple, low-variation observation domains by providing richer supervisory signal, but when observations are complex/variable the generative objective forces the model to capture irrelevant visual details and harms downstream policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Generative models achieve strong performance when pixel reconstruction is feasible, but at the cost of modeling irrelevant background complexity; contrastive models avoid this but risk losing fine-grained observation fidelity if negative-sampling is poor. Dreamer (generative) suffers more under complex observations.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Pixel decoder for observation model p(o_t | s_t) trained via ELBO; RSSM latent sizes as per implementation; latent imagination for actor-critic training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly against CVRL: Dreamer performs comparably on standard MuJoCo but significantly worse on Natural MuJoCo and Box Pushing with dynamic shadows; compared to SAC (model-free) Dreamer is more sample-efficient in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Dreamer is recommended for tasks where pixel-level reconstruction is reliable (simple backgrounds); for domains with complex visual variability, the paper suggests alternative observation objectives (e.g., contrastive CELBO) are preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrastive Variational Model-Based Reinforcement Learning for Complex Observations', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1386.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1386.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfoNCE (contrastive estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfoNCE (mutual information lower-bound via contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive learning objective that lower-bounds mutual information between two variables by discriminating positive pairs from negative samples (used here to learn state–observation correspondence instead of pixel reconstruction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representation learning with contrastive predictive coding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InfoNCE-based contrastive observation model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compatibility scoring function f_theta(s_t, o_t) used with InfoNCE to maximize mutual information I(s_t, o_t). Implemented as a bi-linear score exp(z_t^T W s'_t) where z_t is visual embedding and s'_t is embedded latent; negative samples are drawn from other observations in the replay-batch.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>contrastive estimator used to supervise latent observation coupling</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Learning representations for pixel-based continuous-control RL under complex observations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Mutual information lower bound (InfoNCE objective) and downstream task performance; InfoNCE objective substituted into ELBO to form CELBO.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numeric mutual-information values reported; empirical effectiveness demonstrated by improved downstream task returns on complex-visual benchmarks (see CVRL task performance).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>The compatibility function yields scalar scores for state–observation pairs which can be interpreted as compatibility, but latent components remain dense embeddings without explicit semantic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>The paper inspects qualitative behavior by contrasting correct vs negative observations; negative-sample construction strategy is described (batch-based negatives) but no further interpretability tooling is used.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Computes dot-products between embeddings for positives and O(B*T) negatives per batch; this adds contrastive scoring cost but avoids heavy pixel-decoder training. No numeric timing provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>InfoNCE avoids the heavier optimization burden of pixel reconstruction under complex observations, leading to more stable learning and better sample efficiency in those domains compared to generative decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using InfoNCE within CELBO (CVRL) yields substantially better performance than generative observation learning on Natural MuJoCo and Box Pushing benchmarks in the paper (see CVRL vs Dreamer numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>InfoNCE encourages latent representations that are predictive/compatible with actual observations relevant to the state without forcing full reconstruction of irrelevant details, improving policy learning in complex-visual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>InfoNCE performance depends strongly on negative-sample strategy; naive batch negatives used in the paper work well empirically but are acknowledged as a potential bottleneck and area for future improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Bi-linear compatibility function f_theta = exp(z_t^T W s'_t); negative samples = other observations in the same replay-batch (B*T-1 negatives); CELBO substitutes InfoNCE lower bound for the observation log-likelihood term in the ELBO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>InfoNCE-based contrastive objective outperforms pixel-level generative observation learning under complex, variable visual backgrounds, while being competitive on standard benchmarks; alternative contrastive setups or smarter negative sampling may further improve results.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper uses simple batch-based negatives and a 200-d embedding with a 200x200 weight matrix; recommends exploration of smarter negative-sample strategies (e.g., meta-learned sampling) to improve InfoNCE's performance in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrastive Variational Model-Based Reinforcement Learning for Complex Observations', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Discriminative particle filter reinforcement learning for complex partial observations <em>(Rating: 2)</em></li>
                <li>Contrastive learning of structured world models <em>(Rating: 2)</em></li>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 2)</em></li>
                <li>Auto-encoding variational Bayes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1386",
    "paper_id": "paper-b43a51f83e7ca2f9022dda6e0f32beac3dcb1494",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "CVRL",
            "name_full": "Contrastive Variational Reinforcement Learning (contrastive variational world model / CELBO)",
            "brief_description": "A model-based RL world model that learns latent dynamics via contrastive mutual-information maximization (InfoNCE) instead of pixel-level generative observation likelihoods, combined with a hybrid actor-critic and latent-guided MPC for decision making.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Contrastive variational world model (CELBO)",
            "model_description": "Latent-state world model built on an RSSM-style architecture: a deterministic recurrent state h_t (GRU) and a stochastic latent s_t. Instead of a generative p(o_t | s_t) observation decoder, it learns a discriminative compatibility function f_theta(s_t, o_t)=exp(z_t^T W s'_t) where z_t is a learned embedding of the image and s'_t an embedding of the latent; the model maximizes a contrastive evidence lower bound (CELBO) derived from mutual information estimated with InfoNCE. Dynamics p_theta(s_t | h_t) and reward model p_theta(r_t | h_t, s_t) are still learned; the encoder q_phi(s_t | h_t, o_t) approximates the posterior.",
            "model_type": "latent world model (contrastive variational latent model)",
            "task_domain": "Continuous-control robotics and MuJoCo-style simulated locomotion; robotic manipulation (box pushing) with complex visual observations",
            "fidelity_metric": "Indirect: mutual information lower bound (InfoNCE / CELBO) replacing observation log-likelihood; downstream task returns and ELBO-style dynamics KL term; no explicit pixel MSE reported for the contrastive model.",
            "fidelity_performance": "The paper does not report direct numeric prediction / reconstruction metrics (e.g., MSE). Fidelity is evaluated by downstream task performance: CVRL matches or exceeds baselines on many tasks (example: standard walker-walk 980.3 vs Dreamer 961.7) and strongly outperforms on complex-visual tasks (Natural walker-walk CVRL 941.5 vs Dreamer 206.6). CELBO is described as a lower bound on the ELBO but no numeric bound values are reported.",
            "interpretability_assessment": "Latent representation is a learned dense neural embedding (200-d deterministic + 30-d stochastic); the model is largely a black-box neural latent model. The approach is claimed to discover state–observation correspondence but the paper provides no claim that latent dims map to explicit physical variables.",
            "interpretability_method": "No explicit interpretability methods for latent semantics are used; assessment is via downstream task performance and qualitative comparison of generative reconstructions (for baselines) — reconstructions are visualized for the generative baseline to illustrate failure modes. The contrastive model uses compatibility scores (f_theta) that can be inspected but no analysis of latent semantics is provided.",
            "computational_cost": "Training reported on a single NVIDIA RTX 2080Ti GPU (Intel Xeon Gold CPU); models trained for 5e6 environment steps in primary experiments. Model sizes: deterministic state size 200, stochastic state size 30; encoder: 4 conv layers (channels 32, 64, 128, 256); contrastive embeddings via 200-d fully connected layers and a 200x200 weight matrix. Latent-guided MPC unrolls 15 steps and uses SGD updates (learning rate 0.003) at inference time. Four separate optimizers (model, value, actor, SAC) used during training.",
            "efficiency_comparison": "Paper reports higher sample efficiency than model-free SAC (CVRL and Dreamer each trained with 5e6 steps vs SAC baseline) and competitive or superior efficiency versus generative Dreamer on complex-visual tasks; also notes that avoiding pixel reconstruction simplifies optimization under complex observations. No wall-clock or FLOP comparisons provided.",
            "task_performance": "Representative results from paper: On standard MuJoCo tasks CVRL comparable to Dreamer (e.g., walker-walk CVRL 980.3 vs Dreamer 961.7). On Natural MuJoCo (complex video backgrounds) CVRL outperforms Dreamer dramatically (e.g., walker-walk CVRL 941.5 vs Dreamer 206.6). In Box Pushing (PyBullet with randomized lighting) CVRL achieves the highest rewards and shortest execution length compared to Dreamer and SAC (no absolute numeric reward shown in text). Ablation: CVRL-generative variant and CVRL-reward-only perform much worse, indicating the utility of contrastive observation learning.",
            "task_utility_analysis": "CELBO/contrastive world model prioritizes task-relevant, decision-making features rather than full observation reconstruction, which improves robustness and downstream policy quality in complex-visual domains. However, in simpler observation settings some generative methods can be better (paper notes Dreamer outperforms CVRL on some standard tasks like walker-run and cheetah-run). Hybrid actor-critic (Dreamer-style imagination + SAC off-policy signal) improves robustness when the latent model is imperfect.",
            "tradeoffs_observed": "Trade-offs highlighted: contrastive learning improves robustness to complex, variable observations and sample efficiency but depends strongly on negative-sample strategy and can underperform on some simple-observation tasks; contrastive model is less directly interpretable (latent embeddings) and provides no explicit pixel reconstruction; adding SAC (off-policy) sacrifices some model-based purity for robustness. Computational trade-off: latent-guided MPC adds online planning cost (unrolling, gradient updates) but improves long-horizon reasoning.",
            "design_choices": "Key choices: replace generative p(o_t|s_t) with a contrastive InfoNCE compatibility f_theta(s_t,o_t) and optimize CELBO; use RSSM (deterministic GRU h_t + stochastic s_t) with sizes h:200, s:30; bi-linear compatibility f_theta = exp(z_t^T W s_t'); negative samples drawn from other observations in the same replay-batch (B*T-1 negatives); hybrid training objective combining Dreamer-style imagined-gradient actor loss and SAC off-policy loss; latent-guided MPC via shooting with analytic gradients and 15-step unrolls.",
            "comparison_to_alternatives": "Compared to Dreamer (generative RSSM with pixel decoder): CVRL is more robust under complex visual observations and often yields higher task scores on Natural MuJoCo and Box Pushing, while matching Dreamer on many standard tasks; Dreamer can outperform CVRL on some simple-observation tasks. Compared to model-free SAC: CVRL is far more sample-efficient, achieving higher returns with far fewer environment steps. Compared to a CVRL-generative ablation (same architecture + generative decoder), the contrastive variant performs substantially better in complex-visual settings.",
            "optimal_configuration": "Paper recommends using contrastive CELBO for domains with complex, variable observations and combining it with hybrid actor-critic (latent imagination + SAC) and latent-guided MPC for robust long-horizon decision making. It also notes that the quality of contrastive learning depends on negative-sample selection and suggests future work on better sampling strategies (e.g., meta-learned sampling). No single numeric optimal configuration is prescribed.",
            "uuid": "e1386.0",
            "source_info": {
                "paper_title": "Contrastive Variational Model-Based Reinforcement Learning for Complex Observations",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State Space Model (deterministic + stochastic latent dynamics)",
            "brief_description": "An architecture that decomposes latent world dynamics into a deterministic recurrent state (h_t, typically a GRU) and a stochastic latent s_t, used to model multi-step dynamics and enable latent imagination for planning.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "Recurrent State Space Model (RSSM)",
            "model_description": "Sequential latent dynamics model with deterministic recurrent update h_t = f_theta(h_{t-1}, s_{t-1}, a_{t-1}) and stochastic state s_t ~ p_theta(s_t | h_t); observation model typically p_theta(o_t | s_t) in generative variants, and reward model p_theta(r_t | h_t, s_t). RSSM supports approximate posterior q_phi(s_t | h_t, o_t) and enables latent rollouts (imagination) for actor training and MPC.",
            "model_type": "latent world model (recurrent stochastic-deterministic hybrid)",
            "task_domain": "Pixel-based continuous control (MuJoCo), robotic tasks requiring multi-step reasoning",
            "fidelity_metric": "Typically assessed via ELBO components (reconstruction log-likelihood of observations, KL divergence between posterior and prior) and downstream task performance; in this paper the RSSM prior/posterior KL term is retained in the objective.",
            "fidelity_performance": "No explicit numeric fidelity values for RSSM prediction reported in this paper; RSSM serves as the backbone for both CVRL and Dreamer architectures and supports latent imagination that yields competitive downstream task results (see CVRL task performance numbers).",
            "interpretability_assessment": "RSSM latent states are learned neural vectors (deterministic and stochastic) and are not directly interpretable as explicit physical quantities in the paper. RSSM facilitates multi-step memory through deterministic h_t but does not provide explicit interpretable variables by design.",
            "interpretability_method": "None specific in this paper beyond inspection of downstream behavior and use of latent imagination; RSSM is treated as internal learned state for planning.",
            "computational_cost": "Lightweight recurrent architecture: deterministic size 200 (GRU), stochastic size 30 as used in experiments; supports efficient latent rollouts enabling sample-efficient training. No explicit FLOP or time costs reported beyond single-GPU training.",
            "efficiency_comparison": "RSSM enables latent rollouts and analytic gradients for actor training (Dreamer-style), reducing required environment interactions versus pure model-free methods; this property is used by both CVRL and Dreamer to achieve better sample efficiency than SAC in the experiments.",
            "task_performance": "RSSM-based agents (Dreamer baseline) achieve strong performance on standard MuJoCo tasks but degrade under complex visual backgrounds; integrated into CVRL (with contrastive observation learning) RSSM supports higher robustness under complex observations.",
            "task_utility_analysis": "RSSM is an effective backbone for latent imagination and planning; its utility depends on the quality of the observation model—when paired with generative pixel decoders under complex observations, task performance suffers due to reconstruction difficulty; pairing RSSM with contrastive observation learning (as in CVRL) preserves planning utility while avoiding pixel reconstruction.",
            "tradeoffs_observed": "RSSM gives good temporal memory and enables imagination-based training but requires a compatible observation-learning method: generative decoders increase modelling burden under complex visuals (worse performance), while discriminative/contrastive observation learning reduces that burden. RSSM size choices (h and s dimensions) trade representational capacity vs compute.",
            "design_choices": "Use deterministic GRU for h_t (size 200) and stochastic latent s_t (size 30); approximate posterior q_phi(s_t | h_t, o_t); supports both generative decoders and contrastive discriminators for observation coupling.",
            "comparison_to_alternatives": "Compared to purely generative latent models, RSSM with contrastive observation learning (CVRL) is more robust under complex observations; compared to model-free approaches RSSM enables imagination and sample efficiency.",
            "optimal_configuration": "Paper uses h=200, s=30 as configuration that balances capacity and efficiency for MuJoCo/robot tasks; recommends pairing RSSM with observation-learning methods suited to observation complexity (contrastive CELBO for complex visuals, generative decoders may suffice for simple visuals).",
            "uuid": "e1386.1",
            "source_info": {
                "paper_title": "Contrastive Variational Model-Based Reinforcement Learning for Complex Observations",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "Dreamer (generative)",
            "name_full": "Dreamer (generative latent world model with pixel decoder / latent imagination)",
            "brief_description": "A state-of-the-art generative latent world model that trains an RSSM with a pixel-level generative decoder and uses latent imagination to learn behaviors and value functions (used as a baseline in the paper).",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "mention",
            "model_name": "Dreamer (generative RSSM with pixel decoder)",
            "model_description": "RSSM backbone with a generative observation model p_theta(o_t | s_t) (pixel decoder) trained via ELBO (reconstruction log-likelihood + KL dynamics), and latent imagination used to compute imagined trajectories for actor-critic learning (Dreamer objective).",
            "model_type": "latent generative world model",
            "task_domain": "Pixel-based continuous control (MuJoCo), classic benchmarks for model-based RL",
            "fidelity_metric": "Reconstruction log-likelihood / ELBO (reconstruction loss) and downstream task returns; pixel reconstruction quality inspected qualitatively.",
            "fidelity_performance": "In this paper Dreamer attains strong performance on standard MuJoCo tasks (e.g., walker-walk 961.7) but degrades substantially on Natural MuJoCo with complex video backgrounds (e.g., Natural walker-walk 206.6), attributed to difficulty of pixel-level reconstruction under complex observations.",
            "interpretability_assessment": "Latent representations are neural embeddings; pixel decoder allows visualizing reconstructions to qualitatively assess what is being modeled (paper shows blurry reconstructions that lose agent details under complex backgrounds). Overall model remains a black-box neural system.",
            "interpretability_method": "Visualization of pixel-level reconstructions from the generative decoder; ELBO terms provide diagnostic signals (reconstruction vs KL).",
            "computational_cost": "Comparable RSSM backbone as used in CVRL; training in paper performed on single 2080Ti GPU for 5e6 steps; generative decoder training adds pixel reconstruction computation but no explicit wall-clock numbers provided.",
            "efficiency_comparison": "Dreamer is sample-efficient relative to model-free baselines on standard tasks but less robust (thus less effective) under complex visual observations compared to CVRL's contrastive approach.",
            "task_performance": "Examples from paper: Standard walker-walk Dreamer 961.7 (comparable to CVRL 980.3); Natural walker-walk Dreamer 206.6 (CVRL 941.5). Other tasks show similar patterns: strong on simple visuals, poor on complex video backgrounds.",
            "task_utility_analysis": "High-fidelity pixel reconstruction can help in simple, low-variation observation domains by providing richer supervisory signal, but when observations are complex/variable the generative objective forces the model to capture irrelevant visual details and harms downstream policy learning.",
            "tradeoffs_observed": "Generative models achieve strong performance when pixel reconstruction is feasible, but at the cost of modeling irrelevant background complexity; contrastive models avoid this but risk losing fine-grained observation fidelity if negative-sampling is poor. Dreamer (generative) suffers more under complex observations.",
            "design_choices": "Pixel decoder for observation model p(o_t | s_t) trained via ELBO; RSSM latent sizes as per implementation; latent imagination for actor-critic training.",
            "comparison_to_alternatives": "Compared directly against CVRL: Dreamer performs comparably on standard MuJoCo but significantly worse on Natural MuJoCo and Box Pushing with dynamic shadows; compared to SAC (model-free) Dreamer is more sample-efficient in many settings.",
            "optimal_configuration": "Dreamer is recommended for tasks where pixel-level reconstruction is reliable (simple backgrounds); for domains with complex visual variability, the paper suggests alternative observation objectives (e.g., contrastive CELBO) are preferable.",
            "uuid": "e1386.2",
            "source_info": {
                "paper_title": "Contrastive Variational Model-Based Reinforcement Learning for Complex Observations",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "InfoNCE (contrastive estimator)",
            "name_full": "InfoNCE (mutual information lower-bound via contrastive learning)",
            "brief_description": "A contrastive learning objective that lower-bounds mutual information between two variables by discriminating positive pairs from negative samples (used here to learn state–observation correspondence instead of pixel reconstruction).",
            "citation_title": "Representation learning with contrastive predictive coding",
            "mention_or_use": "use",
            "model_name": "InfoNCE-based contrastive observation model",
            "model_description": "Compatibility scoring function f_theta(s_t, o_t) used with InfoNCE to maximize mutual information I(s_t, o_t). Implemented as a bi-linear score exp(z_t^T W s'_t) where z_t is visual embedding and s'_t is embedded latent; negative samples are drawn from other observations in the replay-batch.",
            "model_type": "contrastive estimator used to supervise latent observation coupling",
            "task_domain": "Learning representations for pixel-based continuous-control RL under complex observations",
            "fidelity_metric": "Mutual information lower bound (InfoNCE objective) and downstream task performance; InfoNCE objective substituted into ELBO to form CELBO.",
            "fidelity_performance": "No numeric mutual-information values reported; empirical effectiveness demonstrated by improved downstream task returns on complex-visual benchmarks (see CVRL task performance).",
            "interpretability_assessment": "The compatibility function yields scalar scores for state–observation pairs which can be interpreted as compatibility, but latent components remain dense embeddings without explicit semantic labels.",
            "interpretability_method": "The paper inspects qualitative behavior by contrasting correct vs negative observations; negative-sample construction strategy is described (batch-based negatives) but no further interpretability tooling is used.",
            "computational_cost": "Computes dot-products between embeddings for positives and O(B*T) negatives per batch; this adds contrastive scoring cost but avoids heavy pixel-decoder training. No numeric timing provided.",
            "efficiency_comparison": "InfoNCE avoids the heavier optimization burden of pixel reconstruction under complex observations, leading to more stable learning and better sample efficiency in those domains compared to generative decoders.",
            "task_performance": "Using InfoNCE within CELBO (CVRL) yields substantially better performance than generative observation learning on Natural MuJoCo and Box Pushing benchmarks in the paper (see CVRL vs Dreamer numbers).",
            "task_utility_analysis": "InfoNCE encourages latent representations that are predictive/compatible with actual observations relevant to the state without forcing full reconstruction of irrelevant details, improving policy learning in complex-visual tasks.",
            "tradeoffs_observed": "InfoNCE performance depends strongly on negative-sample strategy; naive batch negatives used in the paper work well empirically but are acknowledged as a potential bottleneck and area for future improvement.",
            "design_choices": "Bi-linear compatibility function f_theta = exp(z_t^T W s'_t); negative samples = other observations in the same replay-batch (B*T-1 negatives); CELBO substitutes InfoNCE lower bound for the observation log-likelihood term in the ELBO.",
            "comparison_to_alternatives": "InfoNCE-based contrastive objective outperforms pixel-level generative observation learning under complex, variable visual backgrounds, while being competitive on standard benchmarks; alternative contrastive setups or smarter negative sampling may further improve results.",
            "optimal_configuration": "Paper uses simple batch-based negatives and a 200-d embedding with a 200x200 weight matrix; recommends exploration of smarter negative-sample strategies (e.g., meta-learned sampling) to improve InfoNCE's performance in future work.",
            "uuid": "e1386.3",
            "source_info": {
                "paper_title": "Contrastive Variational Model-Based Reinforcement Learning for Complex Observations",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Discriminative particle filter reinforcement learning for complex partial observations",
            "rating": 2
        },
        {
            "paper_title": "Contrastive learning of structured world models",
            "rating": 2
        },
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 2
        },
        {
            "paper_title": "Auto-encoding variational Bayes",
            "rating": 1
        }
    ],
    "cost": 0.0167085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Contrastive Variational Reinforcement Learning for Complex Observations</h1>
<p>Xiao Ma, Siwei Chen, David Hsu, Wee Sun Lee<br>National University of Singapore<br>{xiao-ma, siwei-15,leews,dyhsu}@comp.nus.edu.sg</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (DRL) has achieved significant success in various robot tasks: manipulation, navigation, etc. . However, complex visual observations in natural environments remains a major challenge. This paper presents Contrastive Variational Reinforcement Learning (CVRL), a model-based method that tackles complex visual observations in DRL. CVRL learns a contrastive variational world model discriminatively by maximizing the mutual information between latent states and observations, through contrastive learning. It avoids modeling the complex observation space unnecessarily, as the commonly used generative observation model often does, and is significantly more robust. We evaluated CVRL on challenging RL benchmark tasks that require continuous control. CVRL achieved comparable performance with state-of-the-art model-based DRL methods on standard Mujoco tasks. It significantly outperformed them on Natural Mujoco tasks and a robot box-pushing task with complex observations, e.g., dynamic shadows. The CVRL code is available publicly at https://github.com/Yusufma03/CVRL.</p>
<p>Keywords: Model-Based RL, Contrastive Learning, Complex Observations</p>
<h2>1 Introduction</h2>
<p>Model-free reinforcement learning (MFRL) has achieved great success in game playing [1, 2], robot navigation [3, 4] and etc. However, extending existing RL methods to real-world environments remains challenging, because they require long-horizon reasoning with the low-dimensional useful features, e.g., the position of a robot, embedded in high-dimensional complex observations, e.g., visually rich images. Consider a four-legged mini-cheetah robot [5] navigating on the campus. To determine the traversable path, the robot must extract the relevant geometric features that coexist with irrelevant variable backgrounds, such as the moving pedestrians, paintings on the wall, etc.
Model-based RL (MBRL), in contrast to the model-free methods, reasons a world model trained by generative learning and greatly improves the sample efficiency of the model-free methods [6, 7, 8]. Recent MBRL methods learn compact latent world models from high-dimensional visual inputs with Variational Autoencoders (VAEs) [9] by optimizing the evidence lower bound (ELBO) of an observation sequence [10, 11]. However, learning a generative model under complex observations is challenging. VAEs learn the correspondence between observation $o_{t}$ and latent state $s_{t}$ by maximizing the conditional observation likelihood $p\left(o_{t} \mid s_{t}\right)$, i.e., pixel-level reconstruction of observation $o_{t}$ from agent state $s_{t}$. The generative parameterization unavoidably models the entire observation space, including the complex but irrelevant information to decision making. For example in robot navigation, a generative model will try to capture the pixel-level distribution of the paintings on walls, which is irrelevant to the task of navigating to the goal. As a result, standard MBRL based on generative models have a more difficult optimization landscape given complex observations and will be ineffective when applied to natural environments.
In this paper, we present Contrastive Variational Reinforcement Learning (CVRL) for robust MBRL under complex observations with high sample-efficiency and long-horizon planning ability. To be robust to complex observations, CVRL learns a contrastive variational world model by discriminative contrastive learning, which captures the environment dynamics without modeling the complex observations. Specifically, CVRL maximizes the mutual information between state $s_{t}$ and observation $o_{t}$ by scoring the real pair $\left(s_{t}, o_{t}\right)$ against the fake pairs $\left{\left(s_{t}, o^{\prime}\right)\right}$ using a simple non-negative</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) CVRL addresses the tasks of sparse rewards, many degrees of freedom, and complex observations. (b) Standard generative observation model learns a observation likelihood function $p(o_{t} \mid s_{t})$, i.e., reconstructing observations $o_{t}$ from $s_{t}$, which includes the irrelevant background features. (c) CVRL discovers the correspondence between state $s_{t}$ and observation $o_{t}$ by maximizing their mutual information $I(s_{t}, o_{t})$ by scoring the real pair $\left(s_{t}, o_{t}\right)$ against fake pairs $\left{s_{t}, o_{t}^{\prime}\right}$, which avoids pixel-level reconstruction.</p>
<p>function. As a result, contrastive learning avoids directly modeling complex observations and is more robust than the generative models. For example, by contrasting observations from different places, the mini-cheetah can identify its current position $s_{t}$ by simply understanding what observations $\left{o^{\prime}\right}$ are unlikely to receive. Mathematically, we derive a <em>contrastive evidence lower bound</em> (CELBO), a new lower bound of $p(o_{1:T})$ from the mutual information perspective and it sidesteps the difficulty of learning a complex generative latent world model. CVRL solves the decision making problem combining online model predictive control (MPC) [12] with learned heuristics, i.e., an efficiently and robustly trained actor-critic, for learning long-horizon behavior.</p>
<p>We evaluate CVRL on three challenging continuous control domains: Mujoco tasks designed in Deepmind Control Suite [13], Natural Mujoco tasks, a new domain with more complex observations that we introduce, and Box Pushing, a robot pushing experiment in PyBullet simulator [14]. CVRL outperforms the state-of-the-art model-based RL method in most cases. Results show that CVRL significantly improves the MBRL performance with contrastive representation learning.</p>
<h2>2 Related Works</h2>
<p>MBRL with World Models. Classic MBRL approaches have focused on planning in a predefined low-dimensional state space [15]. However, manually specifying a world model is difficult [16, 17]. Recently several works demonstrated that we could learn world models from raw pixel inputs. The majority rely on sequential variational autoencoders, which aims to minimize the reconstruction loss of the observations, to capture the stochastic dynamics of the environment [10, 11, 18]. Some other works in robotics learn to predict videos directly for planning [19, 20]. However, real-world observations are complex and noisy, building an accurate generative model over the entire observation space is challenging, which leads to an accumulated compositional error of the world model.</p>
<p>Contrastive Learning. Contrastive learning are widely used for learning word embeddings [21], image representation learning [22], graph representation learning [23] and etc. The main idea is to construct real and fake sample pairs and use a function to score them in different ways. Concurrent to our work, contrastive learning has been applied to learn latent world models [18, 24], motivated from different perspectives. Specifically, Hafner et al. [18] use contrastive learning as an alternative to image reconstruction, where the contrastive learned agent gives worse performance compared with the one learned by image reconstruction. On the contrary, we would like to emphasize the strength of contrastive learning in handling complex visual observations. CVRL significantly outperforms the SOTA model [18] on tasks with complex observations.</p>
<p>Reinforcement Learning under Complex Observations. Given complex observations, discriminative training is generally used to improve the robustness of the agent. Recent works suggest that learning task-oriented observation functions by end-to-end training improves the robustness of observation models [25, 17, 26, 27]. In particular, Ma et al. [26] introduced DPFRL which successfully addressed a challenging task with natural video in the background as well as robot navigation in a simulator constructed from real-world data. However, DPFRL relies on only the RL signal and is sample inefficient compared to model-based approaches. Besides, the generalization ability of DPFRL is also limited due to the model-free policy, and it failed on specific games. CVRL addresses the complex observation from a different perspective: we use contrastive learning to learn the latent world model, which avoids the modeling the complex observations. CVRL benefits both the sample efficiency of the model-based approaches and the robustness of the model-free approaches.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) CVRL follows a contrastive latent world model, where the latent states are discovered by contrastive learning, i.e., maximizing the correspondence between state $s_{t}$ and the positive sample $o_{t}$ (in green) and minimizing the correspondence between a set of negative samples $\left{o_{t}^{\prime}\right}$ (in red). (b) CVRL chooses actions with a latent guided MPC using latent analytic gradients, which combines online planning with learned heuristics, i.e., an efficiently and robustly learned actor-critic.</p>
<h1>3 Contrastive Variational Reinforcement Learning</h1>
<p>We introduce contrastive variational reinforcement learning (CVRL) for reinforcement learnign with complex visual observations (Fig. 2). In general, the visual observation reveals only part of the true underlying state. We thus treat the visual control task as a partially observable Markov decision process (POMDP) with discrete time $t=1,2, \ldots, T$, continuous action $a_{t}$, visual observation $o_{t}$, and scalar reward $r_{t}$. CVRL learns a contrastive latent world model, which consists of a probabilistic transition function $p\left(s_{t} \mid s_{t-1}, a_{t}\right)$, a reward function $p\left(r_{t} \mid s_{t}\right)$, and a "discriminative" observation function $f\left(o_{t}, s_{t}\right)$ through contrastive learning. Contrastive learning scores a positive state-observation pair $\left(s_{t}, o_{t}\right)$ against a set of negative observations $\left{o_{t}^{\prime}\right}$ by maximizing $f\left(s_{t}, o_{t}\right)$ while minimizing $f\left(s_{t}, o_{t}^{\prime}\right)$. Contrastive learning is significantly more robust than generative learning, as it avoids pixel-level reconstruction of complex observations. We introduce a new optimization objective, Contrastive Evidence Lower Bound which provides a lower bound on the generative optimization objective. CVRL performs decision making by Latent Guided Model Predictive Control (MPC) using analytic gradients with learned dynamics. The latent guided MPC combines online planning with learned heuristics, i.e., an efficiently and robustly learned Hybrid Actor-Critic model.</p>
<h3>3.1 Variational Latent World Models</h3>
<p>CVRL adopts a variational latent model for discovering environment dynamics from pixel inputs. Variational latent world models are the sequential version of variational autoencoders (VAEs) [9]. For an observable variable $x$, VAEs learn a latent variable $z$ that generates $x$ by optimizing an Evidence Lower Bound (ELBO) of $\log p(x)$</p>
<p>$$
\log p(x)=\log \int_{z} p(x \mid z) p(z) d z \geq \mathbb{E}_{q(z \mid x)}[p(x \mid z)]-K L[q(z \mid x) | p(z)]
$$</p>
<p>where $K L[q(z \mid x) | p(z)]$ denotes the Kullback-Leibler divergence between the prior distribution $p(z)$ and a proposal distribution $q(z \mid x)$ that samples $z$ conditioned on $x$.
In a sequential decision making task, CVRL applies a multi-step generalization to the single step ELBO by optimizing an ELBO of $p\left(o_{1: T}, r_{1: T} \mid a_{1: T}\right)[10,11]$</p>
<p>$$
\begin{aligned}
&amp; \log p\left(o_{1: T}, r_{1: T} \mid a_{1: T}\right)=\log \int p_{\theta}\left(s_{t} \mid s_{t-1}, a_{t-1}\right) p_{\theta}\left(o_{t} \mid s_{t}\right) p_{\theta}\left(r_{t} \mid s_{t}\right) d s_{1: T} \
&amp; \geq \sum_{t=1}^{T}\left(\underbrace{\mathbb{E}\left[\log p_{\theta}\left(o_{t} \mid s_{t}\right)\right]+\mathbb{E}\left[\log p_{\theta}\left(r_{t} \mid s_{t}\right)\right]}<em _phi="\phi">{q</em>\right) \
&amp; \text { dynamics }
\end{aligned}
$$}\left(s_{t} \mid o_{\leq t}, a_{\leq t}\right)}-\underbrace{\mathbb{E}\left[K L\left[q_{\phi}\left(s_{t} \mid o_{\leq t}, a_{\leq t}\right)\right] | p_{\theta}\left(s_{t} \mid s_{t-1}, a_{t-1}\right)\right]}_{\text {reconstruction }</p>
<p>where $\theta$ and $\phi$ are model parameters. The first part encourages accurate reconstructions of the observation likelihood $p_{\theta}\left(o_{t} \mid s_{t}\right)$ and reward likelihood $p_{\theta}\left(r_{t} \mid s_{t}\right)$; the second part encourages</p>
<p>learning self-consistent dynamics by KL-divergence. Specifically, the second part minimizes the KL divergence between the prior distribution $p_{\theta}\left(s_{t} \mid s_{t-1}, a_{t-1}\right)$ and the posterior distribution $q_{\phi}\left(s_{t} \mid o_{\leq t}, a_{&lt;t}\right)$ conditioned on the observation sequences.</p>
<p>However, the pure stochastic transitions might have difficulties remembering the history and learning stability. Introducing a sequence of additional deterministic states $h_{1: T}$ tackles this issue [28, 11]. In this work, we use the recurrent state space model (RSSM) [11] that decomposes the original latent dynamic model into the following four components</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Deterministic state model: $h_{t}=f_{\theta}\left(h_{t-1}, s_{t-1}, a_{t-1}\right)$</th>
<th style="text-align: left;">Stochastic state model: $s_{t} \sim p_{\theta}\left(s_{t} \mid h_{t}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Observation model: $o_{t} \sim p_{\theta}\left(o_{t} \mid s_{t}\right)$</td>
<td style="text-align: left;">Reward model: $r_{t} \sim p_{\theta}\left(r_{t} \mid h_{t}, s_{t}\right)$</td>
</tr>
</tbody>
</table>
<p>As a result, during training, RSSM approximate $q_{\phi}\left(s_{t} \mid o_{\leq t}, a_{\leq t}\right)$ by $q_{\phi}\left(s_{t} \mid h_{t}, o_{t}\right)$.</p>
<h1>3.2 Contrastive Evidence Lower Bound</h1>
<p>One big issue of RSSM is that the pixel level generative observation model $p\left(o_{t} \mid s_{t}\right)$ has to capture the entire observation space, which is problematic given complex observations, e.g., natural observations in autonomous driving or the natural Mujoco games. Given various videos, pixel-level reconstruction becomes difficult which leads to the inaccuracy in the learned latent world model. We introduce Contrastive Evidence Lower Bound (CELBO), a robust optimization objective that avoids reconstructing the observations and lower bounds the original ELBO (Eqn. 2).
Instead of maximizing the observation likelihood $p(x \mid z)$, we motivate the solution from a mutual information perspective. The mutual information between two variables $x$ and $y$ is defined as</p>
<p>$$
I(x, y)=\int p(x, y) \log \frac{p(x, y)}{p(x) p(y)} d x d y=\mathbb{E}_{p(x, y)}\left[\log \frac{p(x \mid y)}{p(x)}\right]
$$</p>
<p>In Eqn. 2, the observation likelihood is computed for a specific trajectory $\tau=\left{o_{1: T}, r_{1: T}, a_{1: T}\right}$. In practice, during optimization, we consider the observation likelihood over a distribution of $\tau$. We can rewrite the observation likelihood in Eqn. 2 as</p>
<p>$$
\mathbb{E}<em _theta="\theta">{\substack{\log p</em>
$$}\left(o_{t} \mid s_{t}\right)-\log p\left(o_{t}\right)+\log p\left(o_{t}\right)]=I\left(s_{t}, o_{t}\right)+E_{p(o \leq t)}\left[\log p\left(o_{t}\right)\right]</p>
<p>where the second term $E_{p(o \leq t)}\left[\log p\left(o_{t}\right)\right]$ could be treated as a constant that can be ignored during optimization. Eqn. 4 suggests that maximizing the observation likelihood is equivalent to maximizing the mutual information of the state-observation pairs. The benefit of such a formulation is that mutual information could be estimated without reconstructing the observations, e.g., using energy models [29] or the "compatibility function" [25, 26]. When the observations are complex, mutual information formulation is more robust than the generative parameterization.
To efficiently optimize the mutual information, we use the InfoNCE, which is a contrastive learning method that optimizes a lower bound of the mutual information [30] and is proven to be powerful in a set of self-supervised learning tasks [30, 31]. Using the result in InfoNCE, the mutual information $I\left(s_{t}, o_{t}\right)$ could be lower bounded by</p>
<p>$$
I\left(s_{t}, o_{t}\right) \geq \mathbb{E}<em _phi="\phi">{q</em>}\left(s_{t} \mid o_{\leq t}, a_{\leq t}\right) p\left(o_{\leq t}, a_{\leq t}\right)}\left[\log f_{\theta}\left(s_{t}, o_{t}\right)-\log \sum_{o_{t}^{\prime} \in \mathcal{O<em _theta="\theta">{t}} f</em>\right)\right]
$$}\left(s_{t}, o_{t}^{\prime</p>
<p>where function $f_{\theta}\left(s_{t}, o_{t}\right)$ is a non-negative function that measures the compatibility between state $s_{t}$ and observation $o_{t}$, and $\mathcal{O}<em t="t">{t}$ is a set of irrelevant observations sampled from a replay buffer. An intuition for Eqn. 5 is that we want to maximize the compatibility between the state $s</em>$ is a learnable weight matrix parameterized by $\theta$.}$ and the real observation $o_{t}$ (positive sample), while minimizing its compatibility between a set of irrelevant observations (negative samples). In our case, we follow the setup of the original InfoNCE loss and use a simple bi-linear model for $f_{\theta}\left(s_{t}, o_{t}\right)=\exp \left(z_{t}^{T} W_{\theta} s_{t}\right)$, where $z_{t}$ is an embedding vector for observation $o_{t}$ and $W_{\theta</p>
<p>Substituting Eqn. 4 and Eqn. 5 into Eqn. 2, we derive the CELBO of $p(o_{1:T}, r_{1:T} \mid a_{1: T})$ as $\log p\left(o_{1: T}, r_{1: T} \mid a_{1: T}\right) \geq$</p>
<p>The CELBO objective is similar to the Deep Variational Information Bottleneck [32] in the sense of mutual information maximization. The difference is that we take a mixed approach: we use contrastive learning to optimize the mutual information for only the state-observation pairs, and maximize the reward likelihood $p\left(r_{t} \mid s_{t}\right)$. Compared to the complex observations, the scalar reward is easy to reconstruct. The quality of contrastive learning highly depends on the choice of negative samples. Reward reconstruction is easier to optimize compared to contrastive learning.</p>
<h1>3.3 Hybrid Actor-Critic</h1>
<p>CVRL trains an actor-critic using a hybrid-approach, benefiting from the sample-efficiency of the model-based learning and the task-oriented feature learning from the model-free RL.
Actor-Critic from Latent Imagination. First, CVRL uses latent imagination to train the actor-critic, i.e., reasoning the latent world model, which reduces the amount of the interactions needed with the non-differentiable environment. In particular, since the predicted reward and latent dynamics are differentiable, the analytic gradients can back-propagate through the dynamics. As a result, the actor-critic can potentially approximate long-horizon planning behaviors [18].
We adopt the same strategy with Dreamer [18]. We parameterize the actor model $a_{t} \sim q_{\eta}\left(a_{t} \mid s_{t}\right)$ as a tanh-transformed Gaussian, i.e., $a_{t}=\tanh \left(\mu_{\eta}\left(s_{t}\right)+\sigma_{\eta}\left(s_{t}\right) \epsilon\right)$, where $\epsilon \sim \mathcal{N}(0, \mathbb{I})$. For value model, we use a feed-forward network $v_{\psi}\left(s_{t}\right)$ with a scalar output. To compute the analytic gradient, we first estimate the state values of the imagined trajectory $\left{\tilde{s}<em _tau="\tau">{\tau}, \tilde{a}</em>}, \tilde{r<em _tau="t">{\tau}\right}</em>\right)$. Detailed descriptions of the value estimation and imagined trajectory generation are in the appendix. The Dreamer learning objective is thus given by}^{t+H}$, where the actions are sampled from the actor network. We denote the value estimate of $s_{\tau}$ as a function $\tilde{V}\left(s_{\tau</p>
<p>$$
L_{\text {Dreamer }}=\underbrace{-\mathbb{E}<em _theta="\theta">{p</em>}, q_{\eta}}\left[\sum_{\tau=t}^{t+H} \tilde{V}\left(s_{\tau}\right)\right]<em p__theta="p_{\theta">{\text {actor loss }}+\underbrace{\mathbb{E}</em>
$$}, q_{\eta}}\left[\sum_{\tau=t}^{t+H} \frac{1}{2}\left|v_{\psi}\left(s_{\tau}\right)-\tilde{V}\left(s_{\tau}\right)\right|^{2}\right]}_{\text {critic loss }</p>
<p>Hybrid Actor-Critic. The performance of latent imagination highly relies on the accuracy of the learned latent world model. Given complex observations, learning an accurate world model is difficult, even with CELBO. We introduce a simple yet effective hybrid training scheme to address this issue. CVRL combines the Dreamer objective with a secondary training signal from off-policy RL, using the ground truth trajectories. Discriminative RL objective can improve the robustness of the actor-critic, while sacrificing the sample-efficiency [26]. Thus, CVRL benefits from both the sample-efficiency of the latent analytic gradient and the robustness of discriminative RL gradient.
In our experiment, we use the Soft Actor-Critic (SAC) [33] to perform off-policy RL. During each optimization step, we use the ground truth trajectory $\left{s_{t}, a_{t}, r_{t}\right}<em _tau="\tau">{t=1}^{T}$, and use the imagined trajectories $\left{\tilde{s}</em>}, \tilde{a<em _tau="\tau">{\tau}, \tilde{r}</em>\right}<em _mathrm_CVRL="\mathrm{CVRL">{\tau=t}^{t+H}$. We have the final objective as $L</em>$.}}=L_{\text {Dreamer }}+\alpha * L_{\mathrm{SAC}</p>
<h3>3.4 Latent Guided Model Predictive Control</h3>
<p>Although the learned actor-critic maximizes the accumulated rewards, a model-free policy, without explicit reasoning with world models, might be stuck in local optimum [16, 34]. Model predictive</p>
<table>
<thead>
<tr>
<th></th>
<th>Standard</th>
<th></th>
<th></th>
<th></th>
<th>Natural</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>CVRL</td>
<td>Dreamer [18]</td>
<td>SAC</td>
<td>D4PG [18]</td>
<td>CVRL</td>
<td>Dreamer</td>
<td>SAC</td>
</tr>
<tr>
<td>walker-walk</td>
<td>980.3</td>
<td>961.7</td>
<td>355.7</td>
<td>968.3</td>
<td>941.5</td>
<td>206.6</td>
<td>44.1</td>
</tr>
<tr>
<td>walker-run</td>
<td>377.7</td>
<td>824.6</td>
<td>153.0</td>
<td>567.2</td>
<td>382.1</td>
<td>82.7</td>
<td>78.1</td>
</tr>
<tr>
<td>cheetah-run</td>
<td>528.1</td>
<td>894.5</td>
<td>181.8</td>
<td>523.8</td>
<td>248.7</td>
<td>100.7</td>
<td>35.8</td>
</tr>
<tr>
<td>finger-spin</td>
<td>717.8</td>
<td>498.8</td>
<td>258.5</td>
<td>985.7</td>
<td>850.4</td>
<td>13.6</td>
<td>23.8</td>
</tr>
<tr>
<td>caripole-balance</td>
<td>997.1</td>
<td>979.6</td>
<td>355.5</td>
<td>992.8</td>
<td>911.9</td>
<td>163.7</td>
<td>206.0</td>
</tr>
<tr>
<td>carpole-swingup</td>
<td>863.4</td>
<td>833.6</td>
<td>252.5</td>
<td>862.0</td>
<td>413.8</td>
<td>117.6</td>
<td>150.5</td>
</tr>
<tr>
<td>cup-catch</td>
<td>964.9</td>
<td>962.5</td>
<td>421.4</td>
<td>980.5</td>
<td>894.2</td>
<td>131.1</td>
<td>202.2</td>
</tr>
<tr>
<td>reacher-easy</td>
<td>968.2</td>
<td>935.1</td>
<td>239.2</td>
<td>967.4</td>
<td>909.1</td>
<td>133.7</td>
<td>137.7</td>
</tr>
<tr>
<td>quadruped-walk</td>
<td>950.3</td>
<td>931.6</td>
<td>337.1</td>
<td>-</td>
<td>878.7</td>
<td>153.2</td>
<td>204.3</td>
</tr>
<tr>
<td>pendulum-swingup</td>
<td>912.1</td>
<td>833.0</td>
<td>28.6</td>
<td>680.9</td>
<td>842.9</td>
<td>12.4</td>
<td>14.8</td>
</tr>
</tbody>
</table>
<p>Table 1: CVRL achieves comparable performance with the SOTA method, Dreamer [18], on standard Mujoco tasks and significantly outperforms Dreamer on Natural Mujoco tasks. CVRL, Dreamer and SAC are trained for $5\times 10^{6}$ steps, while the best model-free baseline D4PG is trained for $1\times 10^{8}$ steps, which we use as an indicator for the performance in standard Mujoco tasks. ${}^{\dagger}$ Results are taken directly from Dreamer paper.
control (MPC) is widely used to address the continuous control problems, where multiple iterations are required for the policy to converge to the optimal solution [35].
We introduce latent guided model predictive control. Specifically, we use the shooting method in trajectory optimization to address the MPC task. For state $s_{t}$, we perform a forward search using the latent world model guided by the learned actor-critic, and generate latent imagination trajectory $\left{\tilde{s}<em _tau="\tau">{\tau},{\tilde{a}</em>},\tilde{r<em _tau="t">{\tau}}\right}</em>\right)$ and update the action sequences with analytic gradients. In practice, the combination of the offline training with online planning gives a better performance. The detailed description of the algorithm can be found in the appendix.}^{t+H}$. We compute the value estimate for the sampled trajectory using $\hat{V}\left(s_{t}\right)$, compute the analytic gradient by maximizing $\hat{V}\left(s_{t</p>
<h1>4 Experiments</h1>
<p>We first evaluate CVRL on 10 continuous Mujoco control tasks in Deepmind Control Suite [13]. We then introduce a new, more challenging benchmark, Natural Mujoco. Finally, we apply CVRL to a robot pushing task with RGB images and randomized lighting sources in PyBullet simulator [14].
We compare CVRL with the SOTA generative MBRL method, Dreamer [18], and a model-free baseline, Soft Actor-Critic [33] ${ }^{1}$. We also include the result of D4PG [36] trained for sufficient time on standard Mujoco tasks, as a baseline for Mujoco tasks. All results are averaged over 3 random seeds. A detailed description to the experiment setup can be found in the appendix. We show that: 1) CVRL significantly outperforms SAC in all cases, with much fewer training iterations; 2) CVRL significantly outperforms Dreamer on natural Mujoco tasks because of the robust contrastive learning, and achieves comparable performance on standard Mujoco; 3) the proposed hybrid actor-critic training scheme and guided model predictive control further improves the performance of CVRL.</p>
<h3>4.1 Mujoco Tasks</h3>
<p>The Mujoco tasks are difficult for reinforcement learning methods due to the sparse reward, 3D scenes and contact dynamics. Specifically, Natural Mujoco tasks are significantly more challenging, where they bridge the gap between simulated environment and real robots by replacing the simple backgrounds with natural videos sampled from ILSVRC dataset [37]. We present the results in Table 1 and analyze the quantitative results as follows.</p>
<p>Specifically, Natural Mujoco tasks brides the gap between simulated environment and real robots by replacing the simple backgrounds with natural videos sampled from ILSVRC dataset [37].
Model-based CVRL outperforms the model-free baseline. We observe that both CVRL reaches the best achievable performance, indicated by D4PG, the state-of-the-art model-free baseline trained for 20 times more steps ( $5 \times 10^{6}$ steps for CVRL and Dreamer, and $1 \times 10^{8}$ steps for D4PG). The learned latent world model successfully captures the real environment dynamics from pixel-level input, so that the trained actor-critic achieves comparable performance with the SOTA D4PG trained</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Generative models learn a latent world model by pixel level reconstruction, which is difficult when the observations are complex and variable. The first row shows the complex observations of natural Walker with varying video backgrounds, and the second row shows the reconstruction of generative models.
by ground truth trajectories. In contrast, given the same number of training steps, CVRL and Dreamer significantly outperform SAC on all tasks. This also suggests that the benefit of CVRL comes from the overall framework design, rather than the SAC.</p>
<p>CVRL is more robust to the natural observations. In Natural Mujoco tasks where the observations are more complex and variable, CVRL significantly outperforms the generative Dreamer in all cases. Although Dreamer achieves SOTA performance on the standard Mujoco tasks with relatively simple observations, its performance drops dramatically on natural Mujoco given complex observations introduced by the video background (e.g., on walker-walk, 961.7 V.S. 206.6). CVRL, however, achieves comparable performance on 8 out of 10 tasks with or without the video background (e.g., on walker-walk, 980.3 V.S. 941.5). This suggests that the contrastive learning, which avoids the pixel-level reconstruction, helps to learn a more robust latent world model than the generative models. Even with the variable complex video background, the learned latent world still successfully captures the underlying dynamics and achieves comparable performance with the simple observations. Besides, we visualize the reconstruction of generative models in Fig. 3. The reconstructions are blurry and lose information about the agent, which explains the failure of the generative Dreamer.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Box Pushing environment is challenging due to the contact physics, 3D scene, and the changing shadow introduced by the robot arm, which commonly exists in robot manipulation scenarios. CVRL achieves the highest performance with the shortest execution length.</p>
<h1>4.2 Box Pushing</h1>
<p>Robot pushing poses great challenges to reinforcement learning [38]. Specifically, the shadow and the occlusion by the robot arm often introduce confusing information to the perception module. We evaluate CVRL for robot pushing in PyBullet which gives an efficient and high-quality physics simulation. The task is to push the red box to the goal indicated by a green ball with an action of $a=(d x, d y)$ for the positional displacement. The positions of the box, the goal, and the lightning source are randomized at every episode. The negative Euclidean distance from the object to the goal is used as the reward, and the robot receives a reward of +1 when the box reaches the goal.
The results are presented in Fig. 4. We observe that CVRL achieves the highest reward with the shortest execution length, and it learns faster than Dreamer and SAC. Specifically, Dreamer struggles because reconstructing the shadow and the moving robot arm is challenging for generative learning. In contrast, although model-free SAC has failed on most of the Mujoco tasks with complex control</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CVRL</th>
<th style="text-align: center;">CVRL-generative</th>
<th style="text-align: center;">CVRL-no-MPC</th>
<th style="text-align: center;">CVRL-no-SAC</th>
<th style="text-align: center;">CVRL-reward-only</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">walker-walk</td>
<td style="text-align: center;">$\mathbf{9 4 1 . 5}$</td>
<td style="text-align: center;">297.7</td>
<td style="text-align: center;">904.8</td>
<td style="text-align: center;">915.2</td>
<td style="text-align: center;">197.9</td>
</tr>
<tr>
<td style="text-align: left;">walker-run</td>
<td style="text-align: center;">$\mathbf{3 8 2 . 1}$</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">343.2</td>
<td style="text-align: center;">378.3</td>
<td style="text-align: center;">115.4</td>
</tr>
<tr>
<td style="text-align: left;">cheetah-run</td>
<td style="text-align: center;">248.7</td>
<td style="text-align: center;">113.3</td>
<td style="text-align: center;">$\mathbf{4 3 0 . 1}$</td>
<td style="text-align: center;">301.0</td>
<td style="text-align: center;">284.8</td>
</tr>
<tr>
<td style="text-align: left;">finger-spin</td>
<td style="text-align: center;">$\mathbf{8 5 0 . 4}$</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">753.3</td>
<td style="text-align: center;">668.8</td>
<td style="text-align: center;">68.7</td>
</tr>
<tr>
<td style="text-align: left;">cartpole-balance</td>
<td style="text-align: center;">911.9</td>
<td style="text-align: center;">188.4</td>
<td style="text-align: center;">$\mathbf{9 9 6 . 3}$</td>
<td style="text-align: center;">962.3</td>
<td style="text-align: center;">431.6</td>
</tr>
<tr>
<td style="text-align: left;">catpole-swingup</td>
<td style="text-align: center;">$\mathbf{4 1 3 . 8}$</td>
<td style="text-align: center;">160.5</td>
<td style="text-align: center;">353.0</td>
<td style="text-align: center;">465.9</td>
<td style="text-align: center;">176.3</td>
</tr>
<tr>
<td style="text-align: left;">ball_in_cup-catch</td>
<td style="text-align: center;">894.2</td>
<td style="text-align: center;">254.8</td>
<td style="text-align: center;">881.4</td>
<td style="text-align: center;">$\mathbf{9 3 0 . 4}$</td>
<td style="text-align: center;">368.7</td>
</tr>
<tr>
<td style="text-align: left;">reacher-easy</td>
<td style="text-align: center;">$\mathbf{9 0 9 . 1}$</td>
<td style="text-align: center;">235.8</td>
<td style="text-align: center;">858.9</td>
<td style="text-align: center;">880.5</td>
<td style="text-align: center;">167.2</td>
</tr>
<tr>
<td style="text-align: left;">quadruped-walk</td>
<td style="text-align: center;">$\mathbf{8 7 8 . 7}$</td>
<td style="text-align: center;">157.3</td>
<td style="text-align: center;">595.2</td>
<td style="text-align: center;">213.5</td>
<td style="text-align: center;">188.7</td>
</tr>
<tr>
<td style="text-align: left;">pendulum-swingup</td>
<td style="text-align: center;">$\mathbf{8 4 2 . 9}$</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">831.5</td>
<td style="text-align: center;">813.3</td>
<td style="text-align: center;">20.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation Studies on natural Mujoco tasks. CVRL generally outperforms all other variants.
dynamics, it outperforms Dreamer on Box Pushing by avoiding generative modeling, given the relatively simple action space. CVRL benefits from the sample efficiency of model-based learning, and it maintains the robustness to complex observations with contrastive learning.</p>
<h1>4.3 Ablation Studies</h1>
<p>We conduct a comprehensive ablation study on the Natural Mujoco tasks to better understand each proposed component. The results are presented in Table 2.
Contrastive variational latent world model is more robust to complex observations. CVRL-generative replaces the contrastive learning with a generative model that performs image-level reconstruction. Unlike Dreamer, CVRL-generative only differs from the CVRL in the parameterization of the representation learning method, and still has the rest of the proposed components. However, its performance degrades on all cases compared to CVRL. This aligns with our previous observation that contrastive learning is more robust given complex observations.</p>
<p>Latent guided MPC improves the reasoning ability for long-horizon behaviors. CVRL-no-MPC uses only the actor-critic for decision making. We observe it performs worse than CVRL on 8 out of 10 tasks, especially on some of the challenging tasks, e.g., cartpole-swingup and quadruped-walk, where multi-step reasoning is required. The latent guided MPC improves the overall performance of CVRL.</p>
<p>The hybrid actor-critic is robust given complex observations. CVRL-no-SAC removes the SAC during actor-critic learning. Its performance drops on certain cases, compared to CVRL (on cheetah-run, 497.3 V.S. 301.0 and finger-spin, 987.1 V.S. 668.8). This is because when the useful features are highly coupled with variable and complex background, learning an accurate latent world model becomes difficult, even for CELBO. With ground-truth trajectories, SAC can provide accurate training signals to compensate for the compositional error of the latent world model.</p>
<p>Reward signal alone is not enough for learning the latent world model. CVRL-reward-only uses only reward prediction for representation learning. Its performance drops in all cases. This suggests that the robustness of CVRL comes from the contrastive learning, rather than only the reward learning.</p>
<h2>5 Conclusions</h2>
<p>We introduce CVRL, a framework for robust MBRL under natural complex observations. CVRL learns a contrastive variational world model with CELBO objective, a contrastive learning alternative to the ELBO, which avoids reconstructing the complex observations. CVRL lerans a robust hybrid actor-critic and uses guided MPC for decision making. It achieves comparable performance with the SOTA methods on 10 challenging Mujoco control tasks, and significantly outperforms SOTA methods on more challenging Natural Mujoco tasks and Box Pushing tasks.</p>
<p>However, CVRL does not perform as well as Dreamer on some tasks on standard Mujcoo tasks (walker-run and cheetah-run), where the observation is simple. While contrastive learning is robust to complex observations, its quality highly depends on the sampling strategy of negative samples. Currently we use a very simple strategy. Further work may consider smarter sampling strategies, e.g., learning to sample using meta-learning.</p>
<h1>References</h1>
<p>[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
[2] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.
[3] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
[4] G. Kahn, A. Villaflor, B. Ding, P. Abbeel, and S. Levine. Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1-8. IEEE, 2018.
[5] W. Bosworth, S. Kim, and N. Hogan. The mit super mini cheetah: A small, low-cost quadrupedal robot for dynamic locomotion. In 2015 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR), pages 1-8. IEEE, 2015.
[6] J. F. Allen and J. A. Koomen. Planning using a temporal world model. In Proceedings of the Eighth international joint conference on Artificial intelligence-Volume 2, pages 741-747, 1983.
[7] K. Basye, T. Dean, J. Kirman, and M. Lejter. A decision-theoretic approach to planning, perception, and control. IEEE Expert, 7(4):58-65, 1992.
[8] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pages 2450-2462, 2018.
[9] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In Proceedings of the International Conference on Learning Representations, 2014.
[10] M. Igl, L. Zintgraf, T. A. Le, F. Wood, and S. Whiteson. Deep variational reinforcement learning for POMDPs. In Proceedings of the International Conference on Machine Learning, pages $2117-2126,2018$.
[11] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.
[12] E. F. Camacho and C. B. Alba. Model predictive control. Springer Science \&amp; Business Media, 2013.
[13] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
[14] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2019.
[15] K. Doya, K. Samejima, K.-i. Katagiri, and M. Kawato. Multiple model-based reinforcement learning. Neural computation, 14(6):1347-1369, 2002.
[16] P. Karkus, D. Hsu, and W. S. Lee. QMDP-net: Deep learning for planning under partial observability. In Advances in Neural Information Processing Systems, pages 4694-4704, 2017.
[17] X. Ma, P. Karkus, D. Hsu, and W. S. Lee. Particle filter recurrent neural networks. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, 2020, pages 5101-5108.
[18] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.
[19] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in neural information processing systems, pages 5074-5082, 2016.
[20] C. Finn and S. Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.</p>
<p>[21] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic language models. arXiv preprint arXiv:1206.6426, 2012.
[22] S. Pedagadi, J. Orwell, S. Velastin, and B. Boghossian. Local fisher discriminant analysis for pedestrian re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3318-3325, 2013.
[23] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855-864, 2016.
[24] T. Kipf, E. van der Pol, and M. Welling. Contrastive learning of structured world models. In International Conference on Learning Representations, 2020.
[25] P. Karkus, D. Hsu, and W. S. Lee. Particle filter networks with application to visual localization. In Proceedings of the Conference on Robot Learning, pages 169-178, 2018.
[26] X. Ma, P. Karkus, D. Hsu, W. S. Lee, and N. Ye. Discriminative particle filter reinforcement learning for complex partial observations. In International Conference on Learning Representations, 2020.
[27] P. Karkus, A. Angelova, V. Vanhoucke, and R. Jonschkowski. Differentiable mapping networks: Learning structured map representations for sparse visual localization. arXiv preprint arXiv:2005.09530, 2020.
[28] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable model for sequential data. In Advances in Neural Information Processing Systems, pages 2980-2988, 2015.
[29] Y. LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.
[30] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[31] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019.
[32] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.
[33] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
[34] P. Karkus, X. Ma, D. Hsu, L. P. Kaelbling, W. S. Lee, and T. Lozano-Pérez. Differentiable algorithm networks for composable robot learning. arXiv preprint arXiv:1905.11602, 2019.
[35] R. Tedrake. Underactuated robotics: Learning, planning, and control for efficient and agile machines course notes for mit 6.832. Working draft edition, 3, 2009.
[36] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, D. Tb, A. Muldal, N. Heess, and T. Lillicrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.
[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
[38] J. K. Li, W. S. Lee, and D. Hsu. Push-net: Deep planar pushing for objects with unknown physical properties. In Robotics: Science and Systems, volume 14, pages 1-9, 2018.
[39] Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. arXiv preprint arXiv:1810.01566, 2018.</p>
<h1>A Algorithm Details</h1>
<h2>A. 1 Latent Imagination</h2>
<p>CVRL first generates the imagined trajectories using the learned world model parameterized by $\theta$. Specifically, given a state $\tilde{s}<em _tau="\tau">{\tau-1}$, we sample the next imagined state by $\tilde{s}</em>} \sim p_{\theta}\left(\tilde{s<em _tau-1="\tau-1">{\tau} \mid \tilde{s}</em>}, \tilde{a<em _tau="\tau">{\tau-1}\right)$, which further generates a reward $\tilde{r}</em>} \sim p_{\theta}\left(\tilde{r<em _tau="\tau">{\tau} \mid \tilde{s}</em>}\right)$ and the next action $\tilde{a<em _eta="\eta">{\tau} \sim q</em>}\left(\tilde{a<em _tau="\tau">{\tau} \mid \tilde{s}</em>}\right)$. We repeat this process until we have an imagined trajectory $\left{\tilde{s<em _tau="\tau">{\tau}, \tilde{a}</em>}, \tilde{r<em _tau="t">{\tau}\right}</em>$.}^{t+H</p>
<h2>A. 2 Value Estimation of Dreamer</h2>
<p>Dreamer estimates the value $\tilde{V}\left(s_{\tau}\right)$ of imagined trajectories using the following equations:</p>
<p>$$
\begin{aligned}
\tilde{V}<em _tau="\tau">{N}^{k}\left(\tilde{s}</em>}\right) &amp; =\mathbb{E<em _theta="\theta">{p</em>}, q_{\eta}}\left(\sum_{n=\tau}^{h-1} \gamma^{n-\tau} \tilde{r<em _psi="\psi">{n}+\gamma^{h-\tau} v</em>}\left(\tilde{s<em _lambda="\lambda">{h}\right)\right), \quad \text { where } h=\min (\tau+k, t+H) \
\tilde{V}</em>}\left(\tilde{s<em n="1">{\tau}\right) &amp; =(1-\lambda) \sum</em>}^{H-1} \lambda^{n-1} \tilde{V<em _tau="\tau">{N}^{n}\left(\tilde{s}</em>}\right)+\lambda^{H-1} \tilde{V<em _tau="\tau">{N}^{H}\left(\tilde{s}</em>\right)
\end{aligned}
$$</p>
<p>$\tilde{V}<em _tau="\tau">{N}^{k}\left(\tilde{s}</em>}\right)$ estimates the value of $\tilde{s<em _psi="\psi">{\tau}$ using the rewards of $k$ steps of rollouts and the value function estimate $v</em>}$ of the last state. Dreamer ues $\tilde{V<em _tau="\tau">{\lambda}\left(\tilde{s}</em>\right)$ as the final value estimation, which is an exponentially-weighted average of different $k$-step rollouts to tradeoff the bias and variance.</p>
<h2>A. 3 Latent Guided MPC</h2>
<p>Originally, for each state $s_{t}$, the actor network $q_{\eta}\left(a_{t} \mid s_{t}\right)$ generates the action which maximizes the long-horizon accumulated reward. However, the approximation highly depends on the quality of the learned world model and might have difficulties approximating complex policies. Most importantly, it lacks the reasoning ability to adapt to variable environments.</p>
<p>We use the shooting method for MPC with differentiable world model. Specifically, we use stochastic gradient ascent to optimize the action sequences to output high accumulated reward. During execution, for each obsevation $o_{t}$, previous state $s_{t-1}$ and action $a_{t-1}$, we encode / propose the current state by $q_{\phi}\left(s_{t} \mid o_{\leq t}, a_{\leq t}\right)$. Next, we perform latent imagination and sample the imagined trajectories $\left{\tilde{s}<em _tau="\tau">{\tau}, \tilde{a}</em>}, \tilde{r<em _tau="t">{\tau}\right}</em>}^{t+H}$ and estimate $\tilde{V<em t="t">{\lambda}\left(s</em>}\right)$. As $\tilde{V<em t="t">{\lambda}\left(s</em>}\right)$ is computed using predicted rewards and value estimations, which are conditioned on the action sequences, we can backpropagate the gradients from $\tilde{V<em t="t">{\lambda}\left(s</em>\right)$ to the actions. We update the actions by</p>
<p>$$
\tilde{a}<em _tau="\tau">{\tau}^{\prime}=\tilde{a}</em>}+\nabla_{\tilde{a<em _lambda="\lambda">{\tau}} \tilde{V}</em>\right)
$$}\left(s_{t</p>
<p>We repeat this for all actions and return the first action after update.
Our latent guided MPC is similar to the planning algorithm used in DPI-Net [39]. The difference is that DPI-Net requires a pre-defined observation of the goal to compute the loss, whereas CVRL directly maximizes the accumulated reward and alleviate this assumption.</p>
<h2>B Implementation Details</h2>
<h2>B. 1 Negative Sample Selection</h2>
<p>We adopt a simple strategy to generate negative samples. We sample a batch of sequences $\left{o_{1: T}^{(i)}, a_{1: T}^{(i)}, r_{1: T}^{(i)}\right}<em t="t">{i=1}^{B}$ from a replay buffer, where $T$ is the sequence length and $B$ is the batch size. For each state-observation pair $\left(s</em> \neq t$, CELBO learns to model the temporal dynamics of the task. We found this simple strategy works well in practice.}, o_{t}\right)$, we treat the other $B * T-1$ observations $\left{o^{\prime}\right}$ in the same batch as negative samples. An intuition of this choice is that: 1) by contrasting $\left(s_{t}^{(i)}, o_{t}^{(i)}\right)$ with $\left(s_{t}^{(i)}, o_{t^{\prime}}^{(j)}\right)$ where $j \neq i$ and $t^{\prime} \in[1, T]$, CELBO learns to identify invariant features of the task given variable visual features; 2) by contrasting $\left(s_{t}^{(i)}, o_{t}^{(i)}\right)$ with $\left(s_{t}^{(i)}, o_{t^{\prime}}^{(i)}\right)$ where $t^{\prime</p>
<h1>B. 2 Hardware and Software.</h1>
<p>We train all models on single NVidia RTX 2080Ti GPUs with Intel Xeon Gold 5220 CPU @ 2.20GHz. We implement all models with Tensorflow 2.2.0 and Tensorflow Probability 0.10.0. Specifically, our code is developed based on the official Tensorflow implementation of Dreamer, but heavily modified. We use the official implementation of Dreamer as our baseline, and we use the SAC implementation of OpenAI baselines. For all methods, we share certain structure, including the encoder, RSSM model and the actor-critic networks to make it a fair comparison.</p>
<h2>B. 3 Observation Encoder.</h2>
<p>We use an encoder of 4 convolutional layers for image observations, which have a fixed kernel size of 4 with increasing channel numbers: $32,65,128,256$. We do not encode the actions again and directly concatenate it with the states.</p>
<h2>B. 4 RSSM.</h2>
<p>We use a stochastic state $s_{t}$ with size 30 and a deterministic state with size 200. The deterministic update function is parameterized using a GRU and the for the stochastic part, we learn the mean and variance of $s_{t}$ using two fully connected layers with size 200 and 30.</p>
<h2>B. 5 Contrastive Learning.</h2>
<p>In contrastive learning, we learn the compatibility between state $s_{t}$ and observation $o_{t}$ with a function $f_{\theta}\left(s_{t}, o_{t}\right)$. In our implementation, we first encode both $o_{t}$ and $s_{t}$ by two separate fully connected layers with size 200, then we compute the value of $f_{\theta}\left(s_{t}, o_{t}\right)=\exp \left(z_{t}^{\mathrm{T}} W_{\theta} s_{t}^{\prime}\right)$, where $z_{t}$ and $s_{t}^{\prime}$ are the embeddings of the observation and the state, and $w_{\theta}$ is a $200 \times 200$ matrix.</p>
<h2>B. 6 Actor-Critic.</h2>
<p>For the actor network, we use 4 fully connected layer which takes in the concatenation of $s_{t}$ and $h_{t}$ as input, with intermediate hidden dimension of 400, and output the corresponding action, with tanh as the activation function. Specifically, a transformed distribution is used to achieve differentiable sampling. For the value network, 3 fully connected layers are used with hidden dimension of 400 and output dimension of 1. In addition, SAC needs additional Q-value network during training. For models needs SAC, we use 2 Q-value network with similar structure, except that the input is a concatenation of $s_{t}, h_{t}$ and $a_{t}$.</p>
<h2>B. 7 Model Learning.</h2>
<p>We train CVRL by 4 separate optimizers for different part of the network: model optimizer, value optimizer, actor optimizer and SAC optimizer. For all optimizers, we use Adam optimizer in our implementation with different learning rate. Model optimizer updates all contrastive variational world model dynamics by representation learning defined in Eqn. 6 with learning rate $6 \times 10^{-4}$; value optimizer updates only value network parameters with learning rate $8 \times 10^{-5}$; actor optimizer updates the actor parameters with learning rate $8 \times 10^{-5}$; SAC optimizer updates the actor parameters and the two Q-value network parameters with learning rate $8 \times 10^{-5}$.</p>
<h2>B. 8 Latent Guided MPC.</h2>
<p>In latent guided MPC, we unroll for 15 steps and update the actions by standard SGD with learning rate 0.003 .</p>
<h1>C Additional Visualizations</h1>
<h2>C. 1 Natural Mujoco Tasks</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h1>C. 2 Standard Mujoco Tasks</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" />
(j) Standard Pendulum Swingup</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ we use the official implementation of Dreamer and the SAC implementation from OpenAI baselines&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
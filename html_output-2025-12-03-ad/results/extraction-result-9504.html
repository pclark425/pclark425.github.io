<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9504 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9504</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9504</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-e78188daf9a18840933f3acfc9b3ccfea3db7856</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e78188daf9a18840933f3acfc9b3ccfea3db7856" target="_blank">Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy</a></p>
                <p><strong>Paper Venue:</strong> Science Advances</p>
                <p><strong>Paper TL;DR:</strong> It is shown that the LLM crowd outperforms a no-information benchmark and is statistically indistinguishable from the human crowd, suggesting that LLM predictions can rival the human crowd's forecasting accuracy through simple aggregation.</p>
                <p><strong>Paper Abstract:</strong> Human forecasting accuracy improves through the “wisdom of the crowd” effect, in which aggregated predictions tend to outperform individual ones. Past research suggests that individual large language models (LLMs) tend to underperform compared to human crowd aggregates. We simulate a wisdom of the crowd effect with LLMs. Specifically, we use an ensemble of 12 LLMs to make probabilistic predictions about 31 binary questions, comparing them with those made by 925 human forecasters in a 3-month tournament. We show that the LLM crowd outperforms a no-information benchmark and is statistically indistinguishable from the human crowd. We also observe human-like biases, such as the acquiescence bias. In another study, we find that LLM predictions (of GPT-4 and Claude 2) improve when exposed to the median human prediction, increasing accuracy by 17 to 28%. However, simply averaging human and machine forecasts yields more accurate results. Our findings suggest that LLM predictions can rival the human crowd’s forecasting accuracy through simple aggregation.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9504.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9504.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Ensemble (Wisdom of the Silicon Crowd)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregate forecasting method that takes the median of probabilistic predictions from a crowd of twelve diverse LLMs to forecast 31 real-time binary events drawn from a Metaculus forecasting tournament; evaluated against a human crowd median and a 50% no-information baseline using Brier scores and calibration analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ensemble of 12 LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A heterogeneous collection of twelve transformer-based LLMs queried via web interfaces (default parameters) and prompted to act as 'superforecasters'; per-question median across non-missing model runs forms the ensemble forecast.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 real-world binary forecasting questions from a Metaculus three-month tournament (politics, economics, space/engineering events such as a Starship integrated flight test liftoff and NASA Voyager 1 communications, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability elicitation from each model using a standardized 'superforecaster' prompt (three independent runs per model per question); aggregate produced by taking the median of non-missing model forecasts per question, then averaging Brier scores across questions.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%), up to 2 decimal places; ensemble uses point medians (Study 1).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score (mean squared error of predicted probability vs outcome), calibration plots and Calibration Index (CI), statistical hypothesis testing (t-tests vs 0.25 baseline, equivalence tests, ANOVA, Tukey HSD), Benjamini-Hochberg FDR adjustment.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Ensemble mean Brier score = 0.20 (SD = 0.12), significantly better than the 0.25 no-information baseline (t(30) = -2.35, p = 0.026). Ensemble Brier was not statistically different from the human crowd mean Brier (human = 0.19, SD = 0.19; t(60)=0.19, p=0.850); equivalence test within medium-effect bounds suggested practical equivalence. Aggregate exhibited overconfidence (CI = 0.041) and an acquiescence bias (mean forecast ≈57.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Aggregate and individual models were poorly calibrated and overconfident; ensemble showed acquiescence bias (predictions skewed >50% despite ~45% positive resolutions); limited ensemble size (12 models); some missing/masked responses due to censorship or interface changes; results restricted to the specific question set and time window.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed the 50% no-information baseline (Brier 0.25). Not significantly different from the human crowd median aggregate (human mean Brier 0.19). Some individual models (e.g., GPT-4) had lower Brier scores than the human aggregate, but only Coral (Command) was found to significantly underperform others in post-hoc tests.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Simple median aggregation across diverse models improved performance; suggested improvements include enlarging model pool, post-hoc calibration, and combining ensemble with retrieval or advanced systems; Study 2 shows exposing models to human crowd medians can improve individual model accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9504.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier proprietary transformer-based large language model from OpenAI used as an individual forecaster in the ensemble and as a focal model in updating experiments; prompted to output probabilistic forecasts and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM accessed via OpenAI web interface with default parameters; prompted with a 'superforecaster' instruction to produce probabilities and rationales. Used in both Study 1 (three runs per question) and Study 2 (pre/post update with human median; ranges requested).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>The same set of 31 real-world binary forecasting questions from the Metaculus tournament (including technical/space events like Starship liftoff and Voyager 1 communications).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability output via standardized prompting (step-by-step 'superforecaster' prompt). Study 1: point probability outputs; Study 2: probability ranges (midpoints used as point estimates) and an updating prompt that included the human crowd median.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%) up to 2 decimals for point forecasts; Study 2 used probability ranges with two-decimal precision (midpoint used for analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score for probabilistic accuracy; calibration curves and Calibration Index; paired tests in Study 2 to compare pre/post updates; correlation analyses for update magnitude vs initial deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Study 1 individual average Brier score = 0.15 (SD = 0.11) — one of the best-performing single models. Study 2: GPT-4 improved after exposure to human median from Brier 0.17 (SD 0.13) to 0.14 (SD 0.11), p = 0.003. However, its updated forecasts were significantly less accurate than a simple average of machine and human (exploratory: paired test showing simple average yielded lower Brier than GPT-4 update; reported paired t-test for the naive average Brier = 0.13, t(92)=2.583, p=0.011).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tended toward acquiescence/overly positive probabilities in aggregate; although improved after human-median exposure, its updating underperformed a naive average of machine and human forecasts; calibration issues remain (CI for GPT-4 = 0.075).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed the 0.25 no-information baseline and matched or exceeded many other single models; performed comparably to the human crowd when aggregated as part of the ensemble. Updating improved accuracy but a simple average of GPT-4 and human median often outperformed GPT-4's own update.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Longer superforecasting prompts, providing human crowd medians as evidence (Study 2), ensemble aggregation with other LLMs, and post-hoc calibration suggested as routes to improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9504.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (with Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Bing (OpenAI w/ web access)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of GPT-4 with internet access used in the ensemble to probe whether web access affects forecasting accuracy; queried via web interface with default parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 variant configured with web access (Bing) and accessed via OpenAI interface; default parameters used and same superforecaster prompt applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Same 31 Metaculus binary forecasting questions covering politics, economics, and technical/space events.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability outputs elicited via the standardized prompt; three independent runs per question; medians used per model-question for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%) up to 2 decimals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score, calibration curves, ANOVA comparisons across models.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.16 (SD = 0.11), close to GPT-4 without web access; no strong evidence that web-access variant significantly outperformed the non-web variant in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No clear advantage observed from web access in this study; calibration issues and acquiescence bias persisted (CI for GPT-4 w/ Bing = 0.088).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed better than the 0.25 baseline and comparably to many other top-performing models; ensemble-level aggregation remained the primary competitive strategy versus human crowd.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Same as other models: aggregation, exposure to human medians, and potential calibrated post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9504.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 2 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Frontier LLM from Anthropic used both as a member of the ensemble and as a focal model in Study 2 updating experiments, prompted to provide probabilistic ranges and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic transformer-based model accessed via the Anthropic website with default parameters; in Study 1 provided point forecasts (3 runs), and in Study 2 provided probability ranges and updates after being shown the human crowd median.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>The 31 Metaculus real-time binary forecasting questions (politics, economics, technical/space events).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probabilistic forecasts via superforecaster-style prompting; in Study 2 collected pre/post probability ranges and rationales with explicit update prompt including the human median.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage point forecasts in Study 1; Study 2 used probability ranges (0–100%) with two-decimal precision (midpoint used for analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score for pre/post comparisons, calibration plots and Calibration Index, correlation between initial deviation and update magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Study 1 average Brier score = 0.21 (SD = 0.16). Study 2: improved after exposure to human median from pre-update Brier 0.22 (SD 0.19) to post-update Brier 0.15 (SD 0.14), p < 0.001. Updates were strongly correlated with initial deviation (r = 0.87, p < 0.001). However, updated forecasts were still less accurate than a simple human+machine average (exploratory Brier for naive average cited as 0.14).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poor calibration relative to ideal (CI = 0.082), acquiescence bias present, and updating did not outperform naive aggregation with human medians.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performance improved with human median exposure but remained roughly comparable to other high-performing models; ensemble aggregation matched human crowd performance overall.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Exposure to human crowd medians (Study 2), improved prompting and potential post-hoc calibration or ensemble combination with other model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9504.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo-Instruct (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior-generation OpenAI LLM included in the ensemble that provided probabilistic forecasts for the tournament questions; evaluated by Brier score and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI transformer-based instruct-tuned model accessed via web interfaces with default parameters and standardized prompting to produce probability forecasts and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 binary forecasting questions from the Metaculus tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct percentage outputs under the 'superforecaster' prompt; three runs per question, medians used per model-question.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and calibration analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.25 (SD = 0.20), approximately equal to the no-information baseline; calibration index CI = 0.106 indicating relatively poor calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performed near the no-information baseline and showed poor calibration and overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Did not outperform the 0.25 baseline on average; weaker than top models like GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation with other models, improved prompting, or exposing to human medians were suggested routes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9504.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-70B (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter open-source LLM included in the ensemble to contribute diversity of model architectures and training data; used to produce probabilistic forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 70B-parameter transformer LLM accessed via third-party interfaces (Poe) with default settings and prompted as a superforecaster.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions (politics, economics, space/technical events).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct point probability outputs using standardized prompting; three runs per question, medians used for per-model forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier scores and Calibration Index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.25 (SD = 0.16). Calibration index CI = 0.071. Performance was around the mid-range of models in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Moderate calibration error and no standout performance; acquiescence bias contributed to aggregate skew.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed near the no-information baseline and below top models like GPT-4 in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Ensembling (median), post-hoc calibration, or exposure to human medians for updating were proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9504.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter open-source instruct-tuned LLM used in the ensemble to increase model diversity; provided probabilistic forecasts for the question set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B-parameter transformer model accessed via third-party web interfaces and prompted with the superforecaster template to output probabilities and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability outputs from the standardized prompt; three runs per question, per-model medians computed.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and calibration index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.24 (SD = 0.16). Calibration index CI = 0.080.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Moderate performance relative to larger frontier models; calibration and overconfidence issues noted.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed slightly better than some smaller models but not as well as top performers (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation, calibration, and human-median interventions suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9504.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM 2 (Chat-Bison@002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 (Chat-Bison@002) (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM 2 variant accessed through third-party interfaces and included in the ensemble to provide diversity, contributing probabilistic forecasts across the question set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 (Chat-Bison@002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary transformer-based model from Google accessed via Poe (third-party) with default parameters; prompted using the superforecaster prompt to output probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct percentage outputs produced by standardized prompting; three independent runs per question and medians used.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and calibration index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.23 (SD = 0.15). Calibration index CI = 0.068.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Calibration error and acquiescence bias present; some interface-driven missing data issues for certain models during collection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Better than several lower-performing models but below top single-model performers.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation and exposure to human medians suggested as ways to improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9504.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard (PaLM 2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bard (PaLM 2) (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google Bard (PaLM 2) queried via Google's interface with internet access for some period; used as part of the 12-model ensemble to forecast the Metaculus questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard (PaLM 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's conversational model (PaLM 2) accessed via Bard interface (internet-enabled at times during data collection) and prompted as a superforecaster to provide probabilities and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 real-time binary forecasting questions from the Metaculus tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability elicitation using the standardized prompt; three runs per question and medians taken per model-question.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and Calibration Index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.19 (SD = 0.17). Calibration index CI = 0.071.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some missing data due to underlying model changes (e.g., Bard moving to Gemini Pro) leading to halted collection at times; calibration and acquiescence bias remained issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed comparably to the human median and better than several other models; no systematic advantage attributed to web access in ensemble comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation and calibrated post-processing; human-median exposure for updating not tested specifically for Bard in Study 2.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9504.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Solar-0-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solar-0-70B (Upstage)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter open-source model used in the ensemble to increase diversity of model origins and training; provided probabilistic forecasts for the tournament questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Solar-0-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Upstage open-source transformer model (70B parameter class) accessed via Poe; default web-interface parameters used and prompted to provide probability forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct percentage forecasts with standardized superforecaster prompt; three runs per question with medians used per model-question.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and Calibration Index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.22 (SD = 0.16). Calibration index CI = 0.081.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Moderate calibration errors and overconfidence; contributed to ensemble diversity but not a top performer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed better than some small/instruction-tuned models but below top performers like GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation and human-median exposure suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9504.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-180B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-180B (Technology Innovation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-capacity 180B-parameter model included in the ensemble; contributed probabilistic forecasts and exhibited one of the better calibration indices among models evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-180B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM (named 'Falcon-180B') accessed via Hugging Face interface in this study; default parameters used and prompted to provide forecasts and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>180B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 binary forecasting questions from the Metaculus tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability outputs via the standardized superforecaster prompt; three runs per question, medians used.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and Calibration Index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.21 (SD = 0.13). Notably had one of the lowest Calibration Index values (CI = 0.027), indicating comparatively better calibration among models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still displayed some over/underconfidence patterns but calibration was relatively good compared to others; missing forecasts due to interface technical issues were recorded for some models during collection period (though not specifically for Falcon).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed similarly to other mid-high performers; contributed useful calibration signal to ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation and potential further calibration; combining with human medians as in Study 2 could improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e9504.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-7B-Chat (Institute)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter chat-oriented model included for diversity in the ensemble; provided probabilistic forecasts for the question set and evaluated by Brier score and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B-parameter transformer chat model accessed via a web interface; used default parameters and the standardized superforecaster prompt to produce probability forecasts and rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct percentage outputs using the superforecaster prompt; three independent runs per question and medians used.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score and Calibration Index.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.24 (SD = 0.17). Calibration index CI = 0.055.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller model size correlated with moderate predictive performance and calibration error relative to frontier models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Performed above the worst models but below top performers; contributed to ensemble diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Aggregation, calibration adjustments, or human-median prompting could improve outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e9504.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coral (Command)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coral (Command) (Cohere)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A web-access model from Cohere included in the ensemble that underperformed relative to most other models and the human crowd in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Coral (Command)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cohere transformer-based model with internet access via the Cohere platform; accessed via web interface and prompted using the superforecaster template.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct probability outputs with standardized prompting, three runs per question.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Percentage (0–100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score, calibration index, and ANOVA comparisons with Tukey HSD post-hoc tests.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Average Brier score = 0.38 (SD = 0.40), the worst-performing single model in the set; Calibration Index CI = 0.212 indicating large calibration errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Substantially worse calibration and accuracy relative to other models; significant underperformance detected in pairwise comparisons (Tukey HSD).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperformed the 0.25 baseline and the human aggregate; significantly worse than several other individual models (e.g., GPT-4, Claude 2).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Potential fixes include improved prompting, calibration, and inclusion in larger ensembles where its errors might be canceled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9504.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e9504.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Study2 Update (Human median intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Updating with Human Crowd Median (Study 2 Intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention in which GPT-4 and Claude 2 were explicitly shown the human crowd median forecast and asked to update their probabilistic forecast and rationale; used to measure whether human cognitive output improves model accuracy and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 and Claude 2 (updating experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frontier LLMs (GPT-4, Claude 2) accessed via respective web interfaces and prompted with an initial elaborate 'superforecaster' instruction; then provided the human crowd median and asked to update and justify changes. Forecasts collected as ranges (midpoints used for Brier calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>31 Metaculus binary forecasting questions (same set as Study 1).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Two-stage prompting: (1) initial forecast (range) under superforecasting guidance; (2) intervention prompt that provides the human crowd median and asks for an updated probabilistic range and reasons. Six total forecasts per model per question (three initial, three updated).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Probability ranges between 0% and 100% (two-decimal precision); analysis used the midpoint as point estimate and interval size for uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Brier score pre/post comparisons (paired tests), analysis of change in prediction interval size, correlation between initial deviation from human median and magnitude of update, Benjamini-Hochberg FDR correction for multiple tests.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Both models improved after exposure to the human median: GPT-4 Brier improved from 0.17 to 0.14 (p = 0.003); Claude 2 improved from 0.22 to 0.15 (p < 0.001). Prediction intervals narrowed significantly when the human median lay within the original interval (GPT-4 interval size reduced from mean 17.75 to 14.22; Claude 2 from 11.67 to 8.28; both p < 0.001). Update magnitude strongly correlated with initial deviation (GPT-4 r = 0.88; Claude 2 r = 0.87; both p < 0.001). Exploratory analysis found that the models' updated forecasts were still less accurate than a simple average of the human and machine forecasts (naive average Brier ~0.13–0.14 better than model updates).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Although updates were directionally appropriate, updating procedures underperformed a simple aggregation benchmark; models remain imperfectly calibrated and showed acquiescence bias; updates may reflect agreement-focused behavior rather than optimal Bayesian assimilation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Updated models improved relative to their own pre-update performance and the no-information baseline, but a naive average of machine and human medians outperformed the model-updated forecasts in exploratory tests.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Directly providing human crowd medians improves accuracy and reduces uncertainty; further gains achievable via simple aggregation (averaging) or more sophisticated combination and calibration methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Approaching Human-Level Forecasting with Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament <em>(Rating: 2)</em></li>
                <li>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy <em>(Rating: 2)</em></li>
                <li>Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI <em>(Rating: 1)</em></li>
                <li>Strictly proper scoring rules, prediction, and estimation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9504",
    "paper_id": "paper-e78188daf9a18840933f3acfc9b3ccfea3db7856",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "LLM Ensemble",
            "name_full": "LLM Ensemble (Wisdom of the Silicon Crowd)",
            "brief_description": "An aggregate forecasting method that takes the median of probabilistic predictions from a crowd of twelve diverse LLMs to forecast 31 real-time binary events drawn from a Metaculus forecasting tournament; evaluated against a human crowd median and a 50% no-information baseline using Brier scores and calibration analyses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Ensemble of 12 LLMs",
            "model_description": "A heterogeneous collection of twelve transformer-based LLMs queried via web interfaces (default parameters) and prompted to act as 'superforecasters'; per-question median across non-missing model runs forms the ensemble forecast.",
            "model_size": null,
            "prediction_target": "31 real-world binary forecasting questions from a Metaculus three-month tournament (politics, economics, space/engineering events such as a Starship integrated flight test liftoff and NASA Voyager 1 communications, etc.).",
            "prediction_method": "Direct probability elicitation from each model using a standardized 'superforecaster' prompt (three independent runs per model per question); aggregate produced by taking the median of non-missing model forecasts per question, then averaging Brier scores across questions.",
            "probability_format": "Percentage (0–100%), up to 2 decimal places; ensemble uses point medians (Study 1).",
            "evaluation_method": "Brier score (mean squared error of predicted probability vs outcome), calibration plots and Calibration Index (CI), statistical hypothesis testing (t-tests vs 0.25 baseline, equivalence tests, ANOVA, Tukey HSD), Benjamini-Hochberg FDR adjustment.",
            "results": "Ensemble mean Brier score = 0.20 (SD = 0.12), significantly better than the 0.25 no-information baseline (t(30) = -2.35, p = 0.026). Ensemble Brier was not statistically different from the human crowd mean Brier (human = 0.19, SD = 0.19; t(60)=0.19, p=0.850); equivalence test within medium-effect bounds suggested practical equivalence. Aggregate exhibited overconfidence (CI = 0.041) and an acquiescence bias (mean forecast ≈57.35%).",
            "limitations_or_challenges": "Aggregate and individual models were poorly calibrated and overconfident; ensemble showed acquiescence bias (predictions skewed &gt;50% despite ~45% positive resolutions); limited ensemble size (12 models); some missing/masked responses due to censorship or interface changes; results restricted to the specific question set and time window.",
            "comparison_to_baselines": "Outperformed the 50% no-information baseline (Brier 0.25). Not significantly different from the human crowd median aggregate (human mean Brier 0.19). Some individual models (e.g., GPT-4) had lower Brier scores than the human aggregate, but only Coral (Command) was found to significantly underperform others in post-hoc tests.",
            "methods_for_improvement": "Simple median aggregation across diverse models improved performance; suggested improvements include enlarging model pool, post-hoc calibration, and combining ensemble with retrieval or advanced systems; Study 2 shows exposing models to human crowd medians can improve individual model accuracy.",
            "uuid": "e9504.0",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A frontier proprietary transformer-based large language model from OpenAI used as an individual forecaster in the ensemble and as a focal model in updating experiments; prompted to output probabilistic forecasts and rationales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based LLM accessed via OpenAI web interface with default parameters; prompted with a 'superforecaster' instruction to produce probabilities and rationales. Used in both Study 1 (three runs per question) and Study 2 (pre/post update with human median; ranges requested).",
            "model_size": null,
            "prediction_target": "The same set of 31 real-world binary forecasting questions from the Metaculus tournament (including technical/space events like Starship liftoff and Voyager 1 communications).",
            "prediction_method": "Direct probability output via standardized prompting (step-by-step 'superforecaster' prompt). Study 1: point probability outputs; Study 2: probability ranges (midpoints used as point estimates) and an updating prompt that included the human crowd median.",
            "probability_format": "Percentage (0–100%) up to 2 decimals for point forecasts; Study 2 used probability ranges with two-decimal precision (midpoint used for analysis).",
            "evaluation_method": "Brier score for probabilistic accuracy; calibration curves and Calibration Index; paired tests in Study 2 to compare pre/post updates; correlation analyses for update magnitude vs initial deviation.",
            "results": "Study 1 individual average Brier score = 0.15 (SD = 0.11) — one of the best-performing single models. Study 2: GPT-4 improved after exposure to human median from Brier 0.17 (SD 0.13) to 0.14 (SD 0.11), p = 0.003. However, its updated forecasts were significantly less accurate than a simple average of machine and human (exploratory: paired test showing simple average yielded lower Brier than GPT-4 update; reported paired t-test for the naive average Brier = 0.13, t(92)=2.583, p=0.011).",
            "limitations_or_challenges": "Tended toward acquiescence/overly positive probabilities in aggregate; although improved after human-median exposure, its updating underperformed a naive average of machine and human forecasts; calibration issues remain (CI for GPT-4 = 0.075).",
            "comparison_to_baselines": "Outperformed the 0.25 no-information baseline and matched or exceeded many other single models; performed comparably to the human crowd when aggregated as part of the ensemble. Updating improved accuracy but a simple average of GPT-4 and human median often outperformed GPT-4's own update.",
            "methods_for_improvement": "Longer superforecasting prompts, providing human crowd medians as evidence (Study 2), ensemble aggregation with other LLMs, and post-hoc calibration suggested as routes to improvement.",
            "uuid": "e9504.1",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 (with Bing)",
            "name_full": "GPT-4 with Bing (OpenAI w/ web access)",
            "brief_description": "Variant of GPT-4 with internet access used in the ensemble to probe whether web access affects forecasting accuracy; queried via web interface with default parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with Bing)",
            "model_description": "GPT-4 variant configured with web access (Bing) and accessed via OpenAI interface; default parameters used and same superforecaster prompt applied.",
            "model_size": null,
            "prediction_target": "Same 31 Metaculus binary forecasting questions covering politics, economics, and technical/space events.",
            "prediction_method": "Direct probability outputs elicited via the standardized prompt; three independent runs per question; medians used per model-question for comparisons.",
            "probability_format": "Percentage (0–100%) up to 2 decimals.",
            "evaluation_method": "Brier score, calibration curves, ANOVA comparisons across models.",
            "results": "Average Brier score = 0.16 (SD = 0.11), close to GPT-4 without web access; no strong evidence that web-access variant significantly outperformed the non-web variant in this dataset.",
            "limitations_or_challenges": "No clear advantage observed from web access in this study; calibration issues and acquiescence bias persisted (CI for GPT-4 w/ Bing = 0.088).",
            "comparison_to_baselines": "Performed better than the 0.25 baseline and comparably to many other top-performing models; ensemble-level aggregation remained the primary competitive strategy versus human crowd.",
            "methods_for_improvement": "Same as other models: aggregation, exposure to human medians, and potential calibrated post-processing.",
            "uuid": "e9504.2",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude 2",
            "name_full": "Claude 2 (Anthropic)",
            "brief_description": "Frontier LLM from Anthropic used both as a member of the ensemble and as a focal model in Study 2 updating experiments, prompted to provide probabilistic ranges and rationales.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 2",
            "model_description": "Anthropic transformer-based model accessed via the Anthropic website with default parameters; in Study 1 provided point forecasts (3 runs), and in Study 2 provided probability ranges and updates after being shown the human crowd median.",
            "model_size": null,
            "prediction_target": "The 31 Metaculus real-time binary forecasting questions (politics, economics, technical/space events).",
            "prediction_method": "Direct probabilistic forecasts via superforecaster-style prompting; in Study 2 collected pre/post probability ranges and rationales with explicit update prompt including the human median.",
            "probability_format": "Percentage point forecasts in Study 1; Study 2 used probability ranges (0–100%) with two-decimal precision (midpoint used for analyses).",
            "evaluation_method": "Brier score for pre/post comparisons, calibration plots and Calibration Index, correlation between initial deviation and update magnitude.",
            "results": "Study 1 average Brier score = 0.21 (SD = 0.16). Study 2: improved after exposure to human median from pre-update Brier 0.22 (SD 0.19) to post-update Brier 0.15 (SD 0.14), p &lt; 0.001. Updates were strongly correlated with initial deviation (r = 0.87, p &lt; 0.001). However, updated forecasts were still less accurate than a simple human+machine average (exploratory Brier for naive average cited as 0.14).",
            "limitations_or_challenges": "Poor calibration relative to ideal (CI = 0.082), acquiescence bias present, and updating did not outperform naive aggregation with human medians.",
            "comparison_to_baselines": "Performance improved with human median exposure but remained roughly comparable to other high-performing models; ensemble aggregation matched human crowd performance overall.",
            "methods_for_improvement": "Exposure to human crowd medians (Study 2), improved prompting and potential post-hoc calibration or ensemble combination with other model outputs.",
            "uuid": "e9504.3",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo-Instruct",
            "name_full": "GPT-3.5-Turbo-Instruct (OpenAI)",
            "brief_description": "A prior-generation OpenAI LLM included in the ensemble that provided probabilistic forecasts for the tournament questions; evaluated by Brier score and calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo-Instruct",
            "model_description": "OpenAI transformer-based instruct-tuned model accessed via web interfaces with default parameters and standardized prompting to produce probability forecasts and rationales.",
            "model_size": null,
            "prediction_target": "31 binary forecasting questions from the Metaculus tournament.",
            "prediction_method": "Direct percentage outputs under the 'superforecaster' prompt; three runs per question, medians used per model-question.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and calibration analyses.",
            "results": "Average Brier score = 0.25 (SD = 0.20), approximately equal to the no-information baseline; calibration index CI = 0.106 indicating relatively poor calibration.",
            "limitations_or_challenges": "Performed near the no-information baseline and showed poor calibration and overconfidence.",
            "comparison_to_baselines": "Did not outperform the 0.25 baseline on average; weaker than top models like GPT-4.",
            "methods_for_improvement": "Aggregation with other models, improved prompting, or exposing to human medians were suggested routes.",
            "uuid": "e9504.4",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama-2-70B",
            "name_full": "Llama-2-70B (Meta)",
            "brief_description": "A 70B-parameter open-source LLM included in the ensemble to contribute diversity of model architectures and training data; used to produce probabilistic forecasts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-70B",
            "model_description": "Open-source 70B-parameter transformer LLM accessed via third-party interfaces (Poe) with default settings and prompted as a superforecaster.",
            "model_size": "70B",
            "prediction_target": "31 Metaculus binary forecasting questions (politics, economics, space/technical events).",
            "prediction_method": "Direct point probability outputs using standardized prompting; three runs per question, medians used for per-model forecasts.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier scores and Calibration Index.",
            "results": "Average Brier score = 0.25 (SD = 0.16). Calibration index CI = 0.071. Performance was around the mid-range of models in the study.",
            "limitations_or_challenges": "Moderate calibration error and no standout performance; acquiescence bias contributed to aggregate skew.",
            "comparison_to_baselines": "Performed near the no-information baseline and below top models like GPT-4 in this dataset.",
            "methods_for_improvement": "Ensembling (median), post-hoc calibration, or exposure to human medians for updating were proposed.",
            "uuid": "e9504.5",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct",
            "name_full": "Mistral-7B-Instruct",
            "brief_description": "A 7B-parameter open-source instruct-tuned LLM used in the ensemble to increase model diversity; provided probabilistic forecasts for the question set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct",
            "model_description": "Open-source 7B-parameter transformer model accessed via third-party web interfaces and prompted with the superforecaster template to output probabilities and rationales.",
            "model_size": "7B",
            "prediction_target": "31 Metaculus binary forecasting questions.",
            "prediction_method": "Direct probability outputs from the standardized prompt; three runs per question, per-model medians computed.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and calibration index.",
            "results": "Average Brier score = 0.24 (SD = 0.16). Calibration index CI = 0.080.",
            "limitations_or_challenges": "Moderate performance relative to larger frontier models; calibration and overconfidence issues noted.",
            "comparison_to_baselines": "Performed slightly better than some smaller models but not as well as top performers (GPT-4).",
            "methods_for_improvement": "Aggregation, calibration, and human-median interventions suggested.",
            "uuid": "e9504.6",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PaLM 2 (Chat-Bison@002)",
            "name_full": "PaLM 2 (Chat-Bison@002) (Google)",
            "brief_description": "Google's PaLM 2 variant accessed through third-party interfaces and included in the ensemble to provide diversity, contributing probabilistic forecasts across the question set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2 (Chat-Bison@002)",
            "model_description": "Proprietary transformer-based model from Google accessed via Poe (third-party) with default parameters; prompted using the superforecaster prompt to output probabilities.",
            "model_size": null,
            "prediction_target": "31 Metaculus binary forecasting questions.",
            "prediction_method": "Direct percentage outputs produced by standardized prompting; three independent runs per question and medians used.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and calibration index.",
            "results": "Average Brier score = 0.23 (SD = 0.15). Calibration index CI = 0.068.",
            "limitations_or_challenges": "Calibration error and acquiescence bias present; some interface-driven missing data issues for certain models during collection.",
            "comparison_to_baselines": "Better than several lower-performing models but below top single-model performers.",
            "methods_for_improvement": "Aggregation and exposure to human medians suggested as ways to improve accuracy.",
            "uuid": "e9504.7",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Bard (PaLM 2)",
            "name_full": "Bard (PaLM 2) (Google)",
            "brief_description": "Google Bard (PaLM 2) queried via Google's interface with internet access for some period; used as part of the 12-model ensemble to forecast the Metaculus questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard (PaLM 2)",
            "model_description": "Google's conversational model (PaLM 2) accessed via Bard interface (internet-enabled at times during data collection) and prompted as a superforecaster to provide probabilities and rationales.",
            "model_size": null,
            "prediction_target": "31 real-time binary forecasting questions from the Metaculus tournament.",
            "prediction_method": "Direct probability elicitation using the standardized prompt; three runs per question and medians taken per model-question.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and Calibration Index.",
            "results": "Average Brier score = 0.19 (SD = 0.17). Calibration index CI = 0.071.",
            "limitations_or_challenges": "Some missing data due to underlying model changes (e.g., Bard moving to Gemini Pro) leading to halted collection at times; calibration and acquiescence bias remained issues.",
            "comparison_to_baselines": "Performed comparably to the human median and better than several other models; no systematic advantage attributed to web access in ensemble comparisons.",
            "methods_for_improvement": "Aggregation and calibrated post-processing; human-median exposure for updating not tested specifically for Bard in Study 2.",
            "uuid": "e9504.8",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Solar-0-70B",
            "name_full": "Solar-0-70B (Upstage)",
            "brief_description": "A 70B-parameter open-source model used in the ensemble to increase diversity of model origins and training; provided probabilistic forecasts for the tournament questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Solar-0-70B",
            "model_description": "Upstage open-source transformer model (70B parameter class) accessed via Poe; default web-interface parameters used and prompted to provide probability forecasts.",
            "model_size": "70B",
            "prediction_target": "31 Metaculus binary forecasting questions.",
            "prediction_method": "Direct percentage forecasts with standardized superforecaster prompt; three runs per question with medians used per model-question.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and Calibration Index.",
            "results": "Average Brier score = 0.22 (SD = 0.16). Calibration index CI = 0.081.",
            "limitations_or_challenges": "Moderate calibration errors and overconfidence; contributed to ensemble diversity but not a top performer.",
            "comparison_to_baselines": "Performed better than some small/instruction-tuned models but below top performers like GPT-4.",
            "methods_for_improvement": "Aggregation and human-median exposure suggested.",
            "uuid": "e9504.9",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Falcon-180B",
            "name_full": "Falcon-180B (Technology Innovation)",
            "brief_description": "A large-capacity 180B-parameter model included in the ensemble; contributed probabilistic forecasts and exhibited one of the better calibration indices among models evaluated.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Falcon-180B",
            "model_description": "Transformer-based LLM (named 'Falcon-180B') accessed via Hugging Face interface in this study; default parameters used and prompted to provide forecasts and rationales.",
            "model_size": "180B",
            "prediction_target": "31 binary forecasting questions from the Metaculus tournament.",
            "prediction_method": "Direct probability outputs via the standardized superforecaster prompt; three runs per question, medians used.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and Calibration Index.",
            "results": "Average Brier score = 0.21 (SD = 0.13). Notably had one of the lowest Calibration Index values (CI = 0.027), indicating comparatively better calibration among models tested.",
            "limitations_or_challenges": "Still displayed some over/underconfidence patterns but calibration was relatively good compared to others; missing forecasts due to interface technical issues were recorded for some models during collection period (though not specifically for Falcon).",
            "comparison_to_baselines": "Performed similarly to other mid-high performers; contributed useful calibration signal to ensemble.",
            "methods_for_improvement": "Aggregation and potential further calibration; combining with human medians as in Study 2 could improve accuracy.",
            "uuid": "e9504.10",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Qwen-7B-Chat",
            "name_full": "Qwen-7B-Chat (Institute)",
            "brief_description": "A 7B-parameter chat-oriented model included for diversity in the ensemble; provided probabilistic forecasts for the question set and evaluated by Brier score and calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-7B-Chat",
            "model_description": "7B-parameter transformer chat model accessed via a web interface; used default parameters and the standardized superforecaster prompt to produce probability forecasts and rationales.",
            "model_size": "7B",
            "prediction_target": "31 Metaculus binary forecasting questions.",
            "prediction_method": "Direct percentage outputs using the superforecaster prompt; three independent runs per question and medians used.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score and Calibration Index.",
            "results": "Average Brier score = 0.24 (SD = 0.17). Calibration index CI = 0.055.",
            "limitations_or_challenges": "Smaller model size correlated with moderate predictive performance and calibration error relative to frontier models.",
            "comparison_to_baselines": "Performed above the worst models but below top performers; contributed to ensemble diversity.",
            "methods_for_improvement": "Aggregation, calibration adjustments, or human-median prompting could improve outputs.",
            "uuid": "e9504.11",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Coral (Command)",
            "name_full": "Coral (Command) (Cohere)",
            "brief_description": "A web-access model from Cohere included in the ensemble that underperformed relative to most other models and the human crowd in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Coral (Command)",
            "model_description": "Cohere transformer-based model with internet access via the Cohere platform; accessed via web interface and prompted using the superforecaster template.",
            "model_size": null,
            "prediction_target": "31 Metaculus binary forecasting questions.",
            "prediction_method": "Direct probability outputs with standardized prompting, three runs per question.",
            "probability_format": "Percentage (0–100%).",
            "evaluation_method": "Brier score, calibration index, and ANOVA comparisons with Tukey HSD post-hoc tests.",
            "results": "Average Brier score = 0.38 (SD = 0.40), the worst-performing single model in the set; Calibration Index CI = 0.212 indicating large calibration errors.",
            "limitations_or_challenges": "Substantially worse calibration and accuracy relative to other models; significant underperformance detected in pairwise comparisons (Tukey HSD).",
            "comparison_to_baselines": "Underperformed the 0.25 baseline and the human aggregate; significantly worse than several other individual models (e.g., GPT-4, Claude 2).",
            "methods_for_improvement": "Potential fixes include improved prompting, calibration, and inclusion in larger ensembles where its errors might be canceled.",
            "uuid": "e9504.12",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Study2 Update (Human median intervention)",
            "name_full": "LLM Updating with Human Crowd Median (Study 2 Intervention)",
            "brief_description": "An intervention in which GPT-4 and Claude 2 were explicitly shown the human crowd median forecast and asked to update their probabilistic forecast and rationale; used to measure whether human cognitive output improves model accuracy and uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 and Claude 2 (updating experiments)",
            "model_description": "Frontier LLMs (GPT-4, Claude 2) accessed via respective web interfaces and prompted with an initial elaborate 'superforecaster' instruction; then provided the human crowd median and asked to update and justify changes. Forecasts collected as ranges (midpoints used for Brier calculations).",
            "model_size": null,
            "prediction_target": "31 Metaculus binary forecasting questions (same set as Study 1).",
            "prediction_method": "Two-stage prompting: (1) initial forecast (range) under superforecasting guidance; (2) intervention prompt that provides the human crowd median and asks for an updated probabilistic range and reasons. Six total forecasts per model per question (three initial, three updated).",
            "probability_format": "Probability ranges between 0% and 100% (two-decimal precision); analysis used the midpoint as point estimate and interval size for uncertainty.",
            "evaluation_method": "Brier score pre/post comparisons (paired tests), analysis of change in prediction interval size, correlation between initial deviation from human median and magnitude of update, Benjamini-Hochberg FDR correction for multiple tests.",
            "results": "Both models improved after exposure to the human median: GPT-4 Brier improved from 0.17 to 0.14 (p = 0.003); Claude 2 improved from 0.22 to 0.15 (p &lt; 0.001). Prediction intervals narrowed significantly when the human median lay within the original interval (GPT-4 interval size reduced from mean 17.75 to 14.22; Claude 2 from 11.67 to 8.28; both p &lt; 0.001). Update magnitude strongly correlated with initial deviation (GPT-4 r = 0.88; Claude 2 r = 0.87; both p &lt; 0.001). Exploratory analysis found that the models' updated forecasts were still less accurate than a simple average of the human and machine forecasts (naive average Brier ~0.13–0.14 better than model updates).",
            "limitations_or_challenges": "Although updates were directionally appropriate, updating procedures underperformed a simple aggregation benchmark; models remain imperfectly calibrated and showed acquiescence bias; updates may reflect agreement-focused behavior rather than optimal Bayesian assimilation.",
            "comparison_to_baselines": "Updated models improved relative to their own pre-update performance and the no-information baseline, but a naive average of machine and human medians outperformed the model-updated forecasts in exploratory tests.",
            "methods_for_improvement": "Directly providing human crowd medians improves accuracy and reduces uncertainty; further gains achievable via simple aggregation (averaging) or more sophisticated combination and calibration methods.",
            "uuid": "e9504.13",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Approaching Human-Level Forecasting with Language Models",
            "rating": 2
        },
        {
            "paper_title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
            "rating": 2
        },
        {
            "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
            "rating": 2
        },
        {
            "paper_title": "Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI",
            "rating": 1
        },
        {
            "paper_title": "Strictly proper scoring rules, prediction, and estimation",
            "rating": 1
        }
    ],
    "cost": 0.021208,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy</h1>
<p>Philipp Schoenegger<br>London School of Economics and Political Science<br>Inter S. Park<br>MIT</p>
<p>Indre Tuminauskaite<br>Independent Researcher<br>Philip E. Tetlock<br>University of Pennsylvania</p>
<h4>Abstract</h4>
<p>Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above $50 \%$, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between $17 \%$ and $28 \%$ : though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.</p>
<h1>1 Introduction</h1>
<p>In the field of artificial intelligence (AI), the rapidly increasing capabilities of large language models (LLMs) have shown promise and even market-competitiveness in a rapidly increasing number of economically valuable and cognitively demanding tasks (Naveed et al. 2023; Sutton 2023). State-of-the-art LLMs with billions of parameters, built on the Transformer architecture (Vaswani et al. 2017), are trained on a very large amount of internet text data (Shen et al. 2023b), before being fine-tuned. The LLMs are trained on this data to predict the next word or subword (token) when given an input string. This step of next-token prediction-when applied repeatedly-generates a sequence of tokens that form an output string coherently text-completing the input, often at a level of coherence previously thought to be only achievable by human cognition (Anthropic 2023; Gemini Team et al. 2023; OpenAI et al. 2023; Touvron et al. 2023) and at a high level of applicability to chat interfaces and various other settings.</p>
<p>This general training objective of next-token prediction, coupled with fine-tuning, also indirectly results in these LLMs displaying an array of specialized skills, which are often only emergently observed after the fact: in ways that were not-and for all practical purposes, likely could not have been-predicted before the first observation of the given capability (Wei et al. 2022). Such skills include but are not limited to marketing (Fraiwan and Khasawneh 2023), reading comprehension (Winter 2023), teaching (Fraiwan and Khasawneh 2023; Sallam et al. 2023), abstract object classification (Atari et al. 2023), cyberattacks (Heiding et al. 2023), robotics (Vemprala et al. 2023), social-science applications (Abdurahman et al. 2023; Park, Schoenegger, and Zhu 2024), medical analysis (Bubeck et al. 2023; Nori et al. 2023; Sallam et al. 2023), legal analysis (Bubeck et al. 2023; Katz et al. 2023), deception (Park et al. 2023), surgical knowledge (Beaulieu-Jones et al. 2024), and computer graphics assessment (Feng et al. 2024).</p>
<p>When evaluating the capabilities of a given AI system, the predominant traditional method is to measure how well an AI system performs at fixed benchmarks for specific tasks (Kistowski et al. 2015). The significant advancements achieved by transformer-based LLMs in these domains have rendered many previously established benchmarks obsolete (Laskar et al. 2023; Shen et al. 2023a), moving the metaphorical goalposts forward in the form of more challenging and comprehensive benchmarks (Alzahrani et al. 2024). It is plausible that a significant portion of the unprecedented successes that state-of-the-art LLMs have achieved on past task benchmarks is genuinely due to a deep understanding of the task-relevant cognitive skills achieved by the LLMs (Bubeck et al. 2023). Indeed, this argument is corroborated by the economic competitiveness-and even promises of economic superiority-that LLMs are achieving for an increasing array of human occupations (Sutton 2023), such as transcription (Peng et al. 2023), translation (Jiao et al. 2023), and programming (Bubeck et al. 2023).</p>
<p>However, it is also plausible that a significant portion of these successes on task benchmarks is due to a superficial memorization of the task's solutions: and shallow understanding of training-set patterns in general (Bender et al. 2021; Biderman et al. 2023; Carlini et al. 2023; Magar and Schwartz 2022). Distinguishing between deep understanding and shallow memorization is a complex challenge, and is central to accurate assessments of advanced reasoning capabilities in AI. This is akin to the examiner's problem of testing their student for deep understanding of the course material, even when many of the potential exam questions can be correctly answered by shallow memorization instead. In fact, just like the student can memorize the answers to exam questions if they see it beforehand, so too can an LLM if its training data contain the questions and answers used in the task benchmark. To resolve this ambiguity, one can exploit the testable presence or absence of the ability to generalize out-of-distribution: to apply learned knowledge beyond the settings represented in the training data (Arora and Goyal 2023). Such a test is arguably key to discerning deep understanding of the task at hand (Grove and Bretz 2012), but is difficult to design when aiming to assess broad LLM capabilities.</p>
<p>In contrast to task benchmarks, where questions and answers are fixed and potentially contained in an LLM's training data, there are contexts where this concern can be ruled out fully: for example, when predicting the future in real-world settings (Schoenegger and Park 2023; Schoenegger et al. 2024). This test stands out for its high external validity, in that the correct answer to a given real-world forecasting question cannot be in a given LLM's training set, as not even the human evaluator knows the answer at the time of evaluation. Moreover, the practice of forecasting is omnipresent in the cognitive tasks undertaken by humans, encompassing a wide range of applications from forecasting the trajectory of current events to setting long-term plans. The ubiquity of forecasting-especially</p>
<p>in white-collar occupations where the increasing capabilities of LLMs are predicted to disrupt or even replace human professionals (Acemoğlu 2023; Park and Tegmark 2023; Summers and Rattner 2023)—combined with the intrinsic external validity makes testing the forecasting capabilities of AI systems an ideal test for assessing the real-world applicability of LLMs.</p>
<p>One context where this can be tested directly are forecasting tournaments. These tournaments involve participants who make probabilistic predictions about future occurrences and are then evaluated and rewarded for their accuracy (Tetlock et al. 2014). Across a set of questions, prediction accuracy of these forecasts determines the reputational or monetary rewards, with more precise predictions yielding greater rewards, incentivising forecasters to research the questions and to provide wellinformed predictions. Based on the predictions of a crowd of forecasters, their aggregate is a gold-standard for human intelligence gathering. This effectiveness of the aggregate of competitive forecasting endeavors relies on the 'wisdom of the crowd' phenomenon, which is the effect that results in the collective accuracy of a set of predictions often surpasses the vast majority of individual judgments that make up the respective crowd. This concept is supported by extensive research across various fields such as prediction markets (Bassamboo, Cui, and Moreno 2018), political forecasting, and more, showing that the combined forecasts of many individuals tend to be remarkably precise (Da and Huang 2020; Lichtendahl Jr, Grushka-Cockayne, and Pfeifer 2013; Surowiecki 2004). This 'wisdom of the crowd' effect relies on independent and unbiased judgements, which achieves an error-cancellation effect (Budescu and Chen 2015) and thereby causes the aggregate to outperform randomly selected forecasts from parts of that crowd (Davis-Stober et al. 2014). As Budescu points out, this aggregation mechanism increases information and accounts for extremes (Budescu 2006), with the 'wisdom of the crowd' effect also holding in contexts of biased inputs (Koriat 2012) or when there are correlations among judgements (Davis-Stober et al. 2014), showing remarkable robustness. Moreover, there is a large literature on improvements of this aggregation process (Baron et al. 2014; Himmelstein, Budescu, and Han 2023; Himmelstein, Budescu, and Ho 2023), with a central take-away being that a simple median is a surprisingly powerful aggregation mechanism across contexts.</p>
<p>Past work has compared the prediction performance of frontier models against a human crowd. With respect to evaluating a single model, Schoenegger and Park (2023) found that the frontier model GPT-4 performed poorly when comparing its predictions to that of a crowd drawn from a forecasting tournament. In fact, GPT-4 did not even significantly outperform the no-information benchmark strategy of predicting 50% on every question. Also, the work of Halawi et al. (2024) has investigated the prediction capabilities of an LLM system, including a combination of news retrieval and reasoning systems. They replicated the finding of Schoenegger and Park (2023) that individual models show poor prediction accuracy, but also found that their optimised system approach aggregated human accuracy. This suggests that individual LLMs may have poor forecasting accuracy, but can produce accurate predictions if they are set in an advanced system.</p>
<p>A hypothesis worth probing is that the underperformance of individual LLMs in real-time forecasting may be, at least in part, due to not making use of the 'wisdom of the crowd' effect. It is reasonable that LLM forecast accuracy may be enhanced by aggregation, as crowd aggregates are known to result in better predictions even high-performing individuals. To test whether such a 'wisdom of the silicon crowd' effect exists, we simulate a crowd of diverse LLMs and draw questions from a real-world forecasting tournament, directly comparing the LLM crowd estimate to that of the human crowd, without introducing further additions like retrieval systems.</p>
<p>In Study 1, we test this LLM ensemble approach, aggregating twelve LLMs' forecasts into a collective crowd forecast, leveraging the diversity inherent in the different models' training data, parameters, and methodologies (such as idiosyncratic fine-tuning). We test whether this diversity improves machine forecast accuracy by reducing the impact of individual model biases and errors. We first test whether the LLM ensemble, unlike GPT-4 in the study of Schoenegger and Park (2023), will significantly outperform the no-information benchmark in a forecasting tournament. This benchmark provides a minimal benchmark of accuracy that is equivalent to guessing 50% on every question.</p>
<p>Null hypothesis 1, Study 1: The average of median LLM forecasts is neither statistically significantly more nor less accurate than the 50% baseline, $H_{0_{1}}: \bar{B}_{LLM}=0.25$.</p>
<p>We also conduct the stronger test of whether the LLM ensemble will significantly outperform the human crowd drawn from the real-world forecasting tournament. For both studies, we use a threemonth tournament run on the platform Metaculus as our human crowd comparison. This provides a</p>
<p>more direct comparison of two aggregated forecasts and would present a result that had so far not been achieved.</p>
<p>Null hypothesis 2, Study 1: The average of median LLM forecasts is neither statistically significantly more nor less accurate than the average of median human forecasts, $H_{0_{2}}: \mu_{L L M}=\mu_{\text {Human }}$.
Lastly for Study 1, we test for differences in forecasting accuracy between the twelve models. Some of these models are variations of each other, like GPT-4 and GPT-4 with Bing access, PaLM2 and PaLM2 in Bard, or Llama-2-70B and Solar-0-70B; while others differ on more fundamental grounds. Testing whether we find differences between models with different capabilities, endpoints, fine-tunings, sizes, etc. might provide further insight into which aspects help or hinder prediction accuracy.
Null hypothesis 3, Study 1: There are no statistically significant differences in the average accuracy across the different LLMs and humans, $H_{0_{3}}: \mu_{1}=\mu_{2}=\ldots=\mu_{k}$.
In Study 2, we investigate the ability of two frontier models (GPT-4 and Claude 2) to integrate human intelligence into their forecasting updating processes. This contributes to work on the interactions between humans and AI. While previous work has focused on how AI can improve human predictions (Schoenegger et al. 2024), this study looks at the reverse; how human forecasts can improve LLM predictions. This is studied in a context where models update their forecasts in response to receiving the human crowd prediction. This investigation of updating behavior is grounded in the premise that access to external information, such as the median forecast of a human crowd, can serve as a valuable reference point for recalibrating predictions. This process builds on Bayesian principles (Ghirardato 2002; Park 2022; Savage 1972) where prior beliefs (in this case, initial forecasts) are adjusted in light of new evidence (the human crowd median) to produce updated posterior beliefs (revised forecasts). The interaction between human and machine intelligence in this context is of particular interest, as it exemplifies the potential synergies that can emerge from integrating the intuitive, experience-based judgments of humans with the data-processing capabilities of LLMs.
We first investigate whether for each of the two LLMs, its average forecast becomes more accurate after being presented with the human crowd's median forecast. This is the most straightforward test of whether human cognitive output in this setting can augment machine-generated forecasts, as measured by forecasting accuracy.
Null hypothesis 1, Study 2: There is no statistically significant difference in the average accuracy of either LLM model before and after having been provided the human crowd median, $H_{0_{1}}: \mu_{\text {pre }}=$ $\mu_{\text {post }}$.
We next investigate the impact of human median forecasting exposure on the precision of LLM forecasts. Specifically, we investigate whether the prediction intervals become narrower, indicating increased confidence in the forecasts: an effect that would suggest that the human predictions-to which the LLMs have been exposed-have nontrivial information value.
Null hypothesis 2, Study 2: The size of the prediction intervals do not become narrower after exposure to the human crowd median, $H_{0_{2}}: \Delta_{\text {range }} \geq 0$.
Finally, we investigate the relationship between the initial deviation of LLM forecasts from the human median and the magnitude of subsequent adjustments. This probes the extent to which larger discrepancies prompt more significant forecast revisions as would be expected.
Null hypothesis 3, Study 2: The magnitude of LLM forecast adjustments is not correlated with the initial deviation of their forecasts from the human crowd median, $H_{0_{3}}: \rho=0$.
Both studies jointly provide the next step in LLM prediction capabilities research. Building on previous work (Schoenegger and Park 2023; Schoenegger et al. 2024), the present paper examines an LLM ensemble approach instead of a single model. Additionally, while other work (Schoenegger et al. 2024) has looked at how AI predictions can improve human accuracy, the present paper also tests the converse, thereby helping complete the picture of how humans and AI systems may interact in real-world contexts that require accurate forecasting.</p>
<h1>2 Methods</h1>
<p>All analyses were preregistered on the Open Science Framework ${ }^{1}$. We clearly label all exploratory and non-preregistered analyses as such throughout the paper to indicate which tests were decided on after having seen the data.</p>
<h3>2.1 Study 1</h3>
<p>In Study 1, we collected data from a total of twelve diverse large language models to simulate the LLM crowd. Specifically, these twelve models were GPT-4, GPT-4 (with Bing), Claude 2, GPT3.5-Turbo-Instruct, Solar-0-70b, Llama-2-70b, PaLM 2 (Chat-Bison@002), Coral (Command), Mistral-7B-Instruct, Bard (PaLM 2), Falcon-180B, and Qwen-7B-Chat. We accessed each model through a web interface and did not query any models via their APIs to hold the query method constant, thus using default parameters (e.g., temperature) for all models. These web interfaces included company-specific interfaces like those offered for the models by OpenAI, Anthropic, Cohere, and Google, as well as interfaces provided by other third parties such as Poe, Huggingface, and Modelscope that provided access to the remaining models. We took this approach to maximise the number of models that we could reliably query throughout the study period that we collected data for while retaining heterogeneity of model specifications as our goal was to draw on a diverse set of models. Additionally, this also kept this study in the context of publicly available and easily accessible models. The final set of models includes frontier proprietary models (GPT-4, Claude 2) as well open-source models (e.g., Llama-2-70b, Mistral 7B-Instruct) from a variety of demographically diverse companies originating from China, France, United Arab Emirates, South Korea, Canada, and the United States. We also have a variety of models with internet access (e.g., GPT-4 with Bing, Bard, Coral) and a large diversity of model sizes, ranging from 7 billion parameters to an estimated 1.6 trillion. ${ }^{2}$ For a full list of all models and their central specifications, see Table 1 below.
In order to assess the prediction capabilities of these models, we drew on a set of forecasting questions that were asked in real time to a public forecasting tournament that ran from October 2023 to January 2024 on the platform Metaculus, where over the course of this tournament, 925 human forecasters provided at least one prediction. In this tournament, forecasters were able to sign up to Metaculus (Metaculus 2024) and predict on as many questions as they wanted. The questions posed ranged from conflict in the Middle East, interest rates, literary prizes, and English electoral politics to Indian air quality, cryptocurrency, consumer technology, and space travel. We focused exclusively on binary probabilistic forecasts, collecting a total of 31 questions. Each question included a question title, a background section detailing the context of the question being asked, and a resolution passage that spelled out in detail how the question will resolve. We drew on the same set of questions and used the publicly available human median predictions for each question as the human benchmark. For a full list of the questions, see Table 4 in the appendix.
For every probabilistic question, within 48 hours of the question opening, we queried each model three independent times and recorded their predictions at the default settings. We recorded both the quantitative forecast and the qualitative rationale for all entries. If a model was unresponsive because of a technical reason, we attempted to collect a forecast 24 hours after the first failed attempt. If a model failed to provide a forecast for non-technical reasons like model censorship/content restrictions after several attempts, we did not reattempt data collection and recorded the prediction as missing. For each question, we prompted each model three times and recorded all predictions. ${ }^{3}$ For cases in which a model failed to provide a forecast for the second or third run after having provided a forecast before, we continued to query the model until all three forecasts were provided.
Our prompt that we used for all models included instructions on how to format the output as well as a number of prompting techniques that include instructing the model to respond as a superforecaster and to approach these questions step-by-step as is current best prompting practice. Each prompt also</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Model Details</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Company</th>
<th style="text-align: left;">Internet <br> Access</th>
<th style="text-align: left;">Open Source</th>
<th style="text-align: left;">Hosting Platform</th>
<th style="text-align: left;">Country of <br> Company</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 Bing</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">Claude 2</td>
<td style="text-align: left;">Anthropic</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Anthropic</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo-</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">Instruct</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Solar-0-70B</td>
<td style="text-align: left;">Upstage</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Poe</td>
<td style="text-align: left;">South Korea</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: left;">Meta</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Poe</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2</td>
<td style="text-align: left;">Google</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Poe</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">(Chat-Bison@002)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Coral (Command)</td>
<td style="text-align: left;">Cohere</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Cohere</td>
<td style="text-align: left;">Canada</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: left;">Mistral</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Poe</td>
<td style="text-align: left;">France</td>
</tr>
<tr>
<td style="text-align: left;">Bard (PaLM 2)</td>
<td style="text-align: left;">Google</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Google</td>
<td style="text-align: left;">United States</td>
</tr>
<tr>
<td style="text-align: left;">Falcon 180B</td>
<td style="text-align: left;">Technology</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Huggingface</td>
<td style="text-align: left;">United Arab</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Innovation</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Emirates</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B-Chat</td>
<td style="text-align: left;">Institute</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>included detailed question background, resolution criteria, and question text as they were posed on the public forecasting tournament, see Figure 1.</p>
<h1>Full Prompt</h1>
<p>In this chat, you are a superforecaster that has a strong track record of accurate forecasts of the future. As an experienced forecaster, you evaluate past data and trends carefully and aim to predict future events as accurately as you can, even though you cannot know the answer. This means you put probabilities on outcomes that you are uncertain about (ranging from 0 to $100 \%$ ). You aim to provide as accurate predictions as you can, ensuring that they are consistent with how you predict the future to be. You also outline your reasons for this forecasting. In your reasons, you will carefully consider the reasons for and against your probability estimate, you will make use of comparison classes of similar events and probabilities and take into account base rates and past events as well as other forecasts and predictions. In your reasons, you will also consider different perspectives. Once you have written your reasons, ensure that they directly inform your forecast.
Then, you will provide me with a number between 0 and 100 (up to 2 decimal places) that is your best prediction of the event. Take a deep breath and work on this problem step-by-step. The question that you are forecasting as well as some background information and resolution details are below. Read them carefully before making your prediction.</p>
<h2>Background: <br> Resolution: <br> Question:</h2>
<p>Figure 1: Full prompt for Study 1
For every set of machine forecasts, we also recorded the publicly available median human crowd prediction at the end of the day that the machine forecast was entered. If the prediction was entered on the first day, we collected the human crowd predictions at the end of the second day that the question was open to allow for higher participation rates. This was done to ensure a fair comparison of machine and human forecasts, as many LLMs can recall the current date, thus making timed forecasts of the nature studied here potentially sensitive to asynchronous queries while also introducing bias with respect to the human crowd. For roughly half the questions, the human forecasters were not able to see the human crowd forecast, though there is significant heterogeneity when the community predictions were made available to human forecasters. In 15 out of 31 questions, our data was collected prior to the revelation of the community prediction to the human forecasters.
For the human forecasts, we took the publicly available median forecast for each question. For the LLM ensemble approach, we computed the median from all non-missing forecasts on each question. We also computed the median forecast on each question for each model to enable cross-model comparisons. See Figure 2 for an overview of our LLM ensemble approach.</p>
<h3>2.2 Study 2</h3>
<p>In Study 2, we focused exclusively on two frontier models, GPT-4 and Claude 2. We used the same real-world forecasting tournament as in Study 1 as our study context, functioning as a source of questions and human forecasts. For Study 2, we employed a within-model research design that collected two forecasts (pre- and post-intervention) per model run for each question, with each question being posed three times at the standard temperature settings, resulting in six forecasts per model for each question. Our goal was to investigate LLM updating behaviour with respect to human cognitive output, i.e., whether and how LLMs take into account the human prediction estimates that forecasting tournament aggregates provide. We queried GPT-4 and Claude 2 via the OpenAI and Anthropic websites respectively.
We used a significantly longer and more elaborate set of prompts than in Study 1. The first prompt built on the ' 10 commandments of superforecasting' (Tetlock and Gardner 2016) as well as the literature on forecasting and updating, instructing models to carefully consider distinguishing different degrees of doubt, strike the correct balance between under- and overconfidence, and break difficult problems into sub-problems that are easier to solve, among other instructions. The second prompt, the intervention,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: LLM Ensemble Mechanism Overview
informed the model of the respective human crowd's median forecast and asked it to update, if necessary, and to outline the reasons for the update (if any). For a full text of both prompts, see Figure 3 and Figure 4.</p>
<p>For both prompts, we collected forecasts not as point estimates but as probability ranges between $0 \%$ and $100 \%$ with two decimal point specificity. For further analysis, we treat the midpoint of this range as the point estimate and the provided predictions as upper and lower estimates. The human crowd median that is provided to the models is collected within 48 hours of the community prediction being revealed to allow human forecasters to learn about it and update their forecasts accordingly, generally leading to more well-calibrated predictions. Because of the time difference, the human forecasts are more accurate than those used in Study 1.</p>
<h1>3 Results</h1>
<h3>3.1 Study 1</h3>
<p>We collected a total of 1007 individual forecasts over the 31 questions from twelve LLMs that make up the ensemble. For 109 forecasts that we did not collect, this was due to technical problems with the model or interface at the time of forecast collection (in the case of Falcon-180B and PaLM 2),or because other models selectively chose not to answer certain questions, presumably due to their content restriction policies (this was the case for Coral (Command) and Qwen-7B-Chat). We also recorded some missing forecasts for Bard, which was due to the fact that the underlying model powering the interface was changed to Gemini Pro. To ensure consistency and allow comparisons between the different contexts of PaLM 2, we stopped collecting data at this point.
Across all models and questions, we observe a minimum raw forecast value of $0.1 \%$ and a maximum raw forecast value of $99.5 \%$, with a median forecast of $60 \%$. This indicates that the LLM models are more likely to make predictions above the $50 \%$ mid-point, with the mean forecast value of the crowd $\mathrm{M}=57.35(\mathrm{SD}=20.93)$ being significantly above the $50 \%$ mark, $\mathrm{t}(1006)=86.20, \mathrm{p}&lt;0.001$. Importantly, the total question set resolved close to evenly, with 14/31 questions resolving positively. This imbalance thus suggests that LLM predictions generally favour positive resolutions above and beyond the appropriate empirical expectation, with just over $45 \%$ of questions resolving positively.</p>
<h1>Initial Prompt</h1>
<p>In this chat, you are a superforecaster who has a strong track record of accurate forecasting. You evaluate past data and trends carefully for potential clues to future events, while recognising that the past is an imperfect guide to the future so you will need to put probabilities on possible future outcomes (ranging from 0 to $100 \%$ ). Your specific goal is to maximize the accuracy of these probability judgments by minimising the Brier scores that your probability judgments receive once future outcomes are known. Brier scores have two key components: calibration (across all questions you answer, the probability estimates you assign to possible future outcomes should correspond as closely as possible to the objective frequency with which outcomes occur) and resolution (across all questions, aim to assign higher probabilities to events that occur than to events that do not occur).
You outline your reasons for each forecast: list the strongest evidence and arguments for making lower or higher estimates and explain how you balance the evidence to make your own forecast. You begin this analytic process by looking for reference or comparison classes of similar events and grounding your initial estimates in base rates of occurrence (how often do events of this sort occur in situations that look like the present one?). You then adjust that initial estimate in response to the latest news and distinctive features of the present situation, recognising the need for flexible adjustments but also the risks of over-adjusting and excessive volatility. Superforecasting requires weighing the risks of opposing errors: e.g., of failing to learn from useful historical patterns vs. over-relying on misleading patterns. In this process of error balancing, you draw on the 10 commandments of superforecasting (Tetlock \&amp; Gardner, 2015) as well as on other peer-reviewed research on superforecasting:</p>
<ol>
<li>Triage</li>
<li>Break seemingly intractable problems into tractable sub-problems</li>
<li>Strike the right balance between inside and outside views</li>
<li>Strike the right balance between under- and overreacting to evidence</li>
<li>Look for the clashing causal forces at work in each problem</li>
<li>Strive to distinguish as many degrees of doubt as the problem permits but no more</li>
<li>Strike the right balance between under- and overconfidence, between prudence and decisiveness</li>
<li>Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases</li>
<li>Bring out the best in others and let others bring out the best in you</li>
<li>Master the error-balancing bicycle</li>
</ol>
<p>Once you have written your reasons, ensure that they directly inform you forecast.
Then, you will provide me with your forecast that is a range between two numbers, each between between 0 and 100 (up to 2 decimal places) that is your best range of prediction of the event. Output your prediction as "My Prediction: Between XX.XX\% and YY.YY\%". Take a deep breath and work on this problem step-by-step.
The question that you are forecasting as well as some background information and resolution criteria are below. Read them carefully before making your prediction.</p>
<h2>Background: <br> Resolution Criteria: <br> Question:</h2>
<p>Figure 3: Initial prompt for Study 2</p>
<p>Such a bias towards more positive predictions may be a function of the machine-equivalent of acquiescence bias (Costello and Roodenburg 2015), where human responders tend to favour the positive/agreement option irrespective of question content (Hinz et al. 2007). See Figure 5 for a scatter plot of all model forecasts across all questions that shows heterogeneity between models of forecast distribution, ranges, and acquiescence bias.</p>
<h1>Prediction Intervention</h1>
<p>You have made your forecast based on careful reasoning and analysis. Now consider the following new piece of information: The median crowd prediction in the forecasting tournament where this question was posed was XXX\%.
Please adjust your reasoning and forecast based on this information, as you deem appropriate. The large research literature on the "wisdom of the crowd" suggests it is difficult for any single forecaster to out-predict crowd medians or averages. But there are occasions when the crowd has proven to be wrong. In considering whether/how much to revise your earlier forecast, keep in mind the theme of error-balancing: the need to balance the risk of giving too little weight to the crowd judgment vs. the risk of over-relying on the crowd. Please explain how you balanced these risks. Please also make this prediction be in the same format as before: "My Prediction: Between XX.XX\% and YY.YY\%".</p>
<p>Figure 4: Prediction intervention prompt for Study 2
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Scatter Plot of all LLM predictions across all questions</p>
<p>In order to assess forecasting accuracy, we use the strictly proper scoring rule (Gneiting and Raftery 2007) of Brier scores (Brier 1950). It is a metric for assessing the accuracy of probabilistic predictions by taking the mean squared difference between the forecasted probability and the actual outcome. It is defined mathematically as</p>
<p>$$
\text { Brier Score }=\left(f_{i}-o_{i}\right)^{2}
$$</p>
<p>where $f_{i}$ is the forecasted probability for the instance, and $o_{i}$ is the actual outcome, which can be 0 or 1. A lower Brier score indicates higher accuracy, with 0 being the perfect accuracy score. A score of 0.250 represents a typical benchmark that would be arrived at if all predictions were $50 \%$.</p>
<p>Testing our first hypothesis as preregistered, we investigate whether the LLM crowd can outperform the simple baseline of assigning a $50 \%$ prediction on every question, a baseline that GPT-4 was unable to beat in previous work (Schoenegger and Park 2023). To arrive at our LLM median forecast for this and further analysis using this aggregate, we calculate the median LLM forecast across all models for every question. We then take these medians and average them across all questions. We then take this average and compare it a Brier score of 0.25 (the result of predicting $50 \%$ on all questions). We are able to reject our null hypothesis, with the LLM crowd, $\mathrm{M}=0.20(\mathrm{SD}=0.12)$, being significantly more accurate than the benchmark, $\mathrm{t}(30)=-2.35, \mathrm{p}=0.026$. This is first evidence that crowd-aggregated LLM forecasts can improve upon basic benchmarks.</p>
<p>Next, we compare the LLM crowd performance to that of the human crowd for our second hypothesis, directly putting the two crowd-aggregation mechanisms head-to-head. To do this, we use the same LLM crowd average as before (taking the median LLM prediction on each question and averaging up</p>
<p>the Brier scores across questions). We compare this to the average of median human predictions on the same questions. In our preregistered analysis, we fail to find statistically significant differences between the LLM crowd's mean Brier score of $\mathrm{M}=0.20(\mathrm{SD}=0.12)$ and that of the human crowd, $\mathrm{M}=0.19(\mathrm{SD}=0.19), \mathrm{t}(60)=0.19, \mathrm{p}=0.850$.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: KDE of the LLM and human crowd forecasts (averaged median scores over all questions). Vertical dotted black line represents the $50 \%$ baseline.</p>
<p>This result only enables us to directly conclude that the LLM crowd is neither more nor less accurate than the human crowd in the question set studied here. To provide some evidence in favour of the equivalence of these two approaches, we conduct a non-preregistered equivalence test with the conventional medium effect size of Cohen's $\mathrm{d}=0.5$ as equivalence bounds (Cohen 2013), which allows us to test whether the effect is zero or less than a 0.081 change in Brier scores. For these equivalence bounds, we find that the LLM crowd and the human crowd are equally accurate, with both tests for the lower bound, $\mathrm{t}(60)=2.16, \mathrm{p}=0.017$ and the upper bound, $\mathrm{t}(60)=-1.78, \mathrm{p}=0.040$, being statistically significant. This provides evidence that the LLM crowd is as accurate as the human crowd within these bounds, though note that the bounds are quite wide.</p>
<p>Table 2: Average Brier Score for Each Model</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (with Bing)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr>
<td style="text-align: left;">Bard (PaLM 2)</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-180B</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.13</td>
</tr>
<tr>
<td style="text-align: left;">Claude 2</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">Solar-0-70B</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2 (Chat-Bison@002)</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B-Chat</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">GPT3.5-Turbo-Instruct</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">Coral (Command)</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.19</td>
</tr>
</tbody>
</table>
<p>For our third null hypothesis, we compare the forecasting accuracy of each model (and the human crowd) against each other to find potential effects of internet access (GPT-4 vs. GPT-4 with Bing) or access points (Bard with PaLM2 vs. PaLM2). Using an analysis of variance, we find significant aggregate differences, $\mathrm{F}(12,354)=2.64, \mathrm{p}=0.002$, leading us to reject our third null hypothesis. Using Tukey HSD to adjust for multiple comparisons in the post-hoc pair-wise tests, we find that Coral</p>
<p>(Command) underperforms a set of models (e.g., Claude 2, GPT-4) as well as the human crowd. However, we fail to find statistically significant effects between any other pairs not involving Coral (Command), thus being unable to provide evidence in favour or against potential effects of internet access, access points, or fine-tuning on prediction accuracy. See Table 2 for average Brier scores for each model.</p>
<p>Brier Scores of Each Model and Humans
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Raincloud plots of Brier scores for each LLM model as well as the human crowd.
For all three hypotheses, we implemented the Benjamini-Hochberg (BH) procedure to adjust the p-values obtained from multiple hypothesis tests. This method was selected to control the False Discovery Rate (FDR) and thereby reduce the risk of Type I errors. The original p-values for null hypotheses 1, 2, and 3 were $0.026,0.850$, and 0.002 , respectively. These p -values were first sorted in ascending order and then ranked accordingly. The adjusted p -values were computed using the Benjamini-Hochberg procedure, which calculates the adjusted p -value for the $i$-th hypothesis as $\min \left{1, \frac{p_{i} \pm m}{i}\right}$, where $p_{i}$ is the $i$-th p -value in the sorted list, $m$ is the total number of hypotheses tested, and $i$ is the rank of the p-value. The results show that the adjusted p -values for the hypotheses were 0.039 for the first hypothesis (original $\mathrm{p}=0.026$ ), 0.850 for the second hypothesis (original $\mathrm{p}=0.850$ ), and 0.006 for the third hypothesis (original $\mathrm{p}=0.002$ ). These results indicate that our rejections of the first and third null hypothesis remain robust after adjusting for multiple comparisons.
In non-preregistered analyses, we conduct calibration analyses using the Murphy Decomposition (Mandel and Barnes 2014; Siegert 2017) to provide data on how well calibrated the LLM models are</p>
<p>in this context, i.e., how reliably their probability estimates match the fraction of real outcomes. In Figure 8, calibration curves for each model and their aggregate are plotted against the ideal 45-degree dotted line. This dotted line represents perfect calibration, where predicted probabilities match observed frequencies. Deviations from this line indicate calibration errors: curves above the line suggest underconfidence (predicting events as less likely than they actually are), while those below indicate overconfidence (predicting events as more likely than they actually are). Figure 8 visually represents how closely the models' predictions align with actual outcomes. We also calculate the Calibration Index (CI), which quantifies this deviation, with lower values indicating better calibration. CI is calculated using the formula:</p>
<p>$$
C I=\frac{1}{N} \sum_{k=1}^{K} N_{k}\left(f_{k}-o_{k}\right)^{2}
$$</p>
<p>where $N$ is the total number of forecasts, $K$ the number of bins, $N_{k}$ the number of forecasts in bin $k, f_{k}$ the mean forecast probability in bin $k$, and $o_{k}$ the observed relative frequency in bin $k$. This weights each bin's contribution to the Calibration Index (CI) by the number of forecasts it contains. This approach ensures that bins with more forecasts, which provide a more statistically reliable estimate of forecasting accuracy, have a proportionately greater impact on the overall CI.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Calibration plot for all LLM models as well as the aggregate (bolded)</p>
<p>Our results demonstrate poor calibration of most models and overconfidence of the aggregate, suggesting that models overpredict outcomes compared to their actual rate of occurrence, see Figure 8. This is in line with the finding that we find a acquiescence bias of LLMs on a question set where less than half of questions resolve positively. We also find generally poor calibration across all models. However, there are substantial differences in the CI scores, with some models having substantially better calibration than others, see Table 3. This suggests that a further line of research may build upon improving calibration of models in an attempt to improve machine prediction capabilities and reliability further.</p>
<p>Table 3: Calibration index values for all LLM models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Calibration Index</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Falcon-180B</td>
<td style="text-align: center;">0.027</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-7B-Chat</td>
<td style="text-align: center;">0.055</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2 (Chat-Bison@002)</td>
<td style="text-align: center;">0.068</td>
</tr>
<tr>
<td style="text-align: left;">Bard (PaLM 2)</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">0.075</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.080</td>
</tr>
<tr>
<td style="text-align: left;">Solar-0-70B</td>
<td style="text-align: center;">0.081</td>
</tr>
<tr>
<td style="text-align: left;">Claude 2</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (with Bing)</td>
<td style="text-align: center;">0.088</td>
</tr>
<tr>
<td style="text-align: left;">GPT3.5-Turbo-Instruct</td>
<td style="text-align: center;">0.106</td>
</tr>
<tr>
<td style="text-align: left;">Coral (Command)</td>
<td style="text-align: center;">0.212</td>
</tr>
<tr>
<td style="text-align: left;">Aggregate</td>
<td style="text-align: center;">0.041</td>
</tr>
</tbody>
</table>
<h1>3.2 Study 2</h1>
<p>For Study 2, we collected a total of 186 primary forecasts and 186 updated forecasts from both frontier models (GPT-4 and Claude 2) over the 31 binary questions studied. Neither model refused to provide a forecast or failed to respond to our querying.
First, we test whether exposure to the human crowd median improves model accuracy. We are able to reject the first null hypothesis of Study 2 for both models: For GPT-4, there is a statistically significant difference in Brier Scores before and after exposure to the human median, with an average Brier score for the primary forecast of 0.17 (SD: 0.13 ) and an updated score of 0.14 (SD: 0.11 ), $\mathrm{p}=$ 0.003 . For Claude 2, we also find a statistically significant difference in Brier Scores before and after exposure to the human median, improving from 0.22 (SD: 0.19 ) to 0.15 (SD: 0.14 ), $\mathrm{p}&lt;0.001$. This suggests that the provision of human cognition in the form of crowd forecasts can improve model prediction capabilities.
We also find that, testing our second hypothesis, the size of the prediction interval narrows after exposure to human crowd predictions that lie within the probability range provided by the model, as would be predicted by theory: The prediction intervals for GPT-4 become significantly narrower after exposure to the human median, ranging from an average interval size of 17.75 (SD: 5.66)to 14.22 (SD: 5.97), $\mathrm{p}&lt;0.001$. The prediction intervals for Claude 2 also become significantly narrower after exposure to the human median forecast, narrowing from 11.67 (SD: 4.201 to 8.28 (SD: 3.63), $\mathrm{p}&lt;0.001$. This suggests that the models appropriately reduce their prediction uncertainty when the human forecast is already included in the LLM's, incorporating this additional information and adjusting their uncertainty. See Figure 9 for a graphical illustration of LLM forecasts for either model before and after exposure to the human forecasts.
Lastly, with respect to our third hypothesis, we analyse whether LLMs' updates are proportional to the distance between their point forecast and that of the human benchmark. We are able to reject our null hypothesis for both models, finding significant correlation between the initial deviation and the magnitude of forecast adjustment for GPT-4, $\mathrm{r}=0.88, \mathrm{p}&lt;0.001$ as well as for Claude $2 \mathrm{r}=0.87, \mathrm{p}&lt;$ 0.001 . This suggests that models move their predictions roughly in accordance with how large the difference between their prediction and the human median is.
As in Study 1, we use the Benjamini-Hochberg procedure for controlling multiple comparisons, given our three hypotheses each tested for each model, resulting in six tests. The original p-values were $[0.001,0.001,0.001,0.001,0.001,0.003]$. After applying the Benjamini-Hochberg adjustment, the p-values were $[0.006,0.006,0.006,0.006,0.006,0.003]$, all of which were below the 0.05 FDR threshold. This indicates that, post-adjustment, the results from all tests remained statistically significant.
We also conduct the following exploratory analysis. Instead of comparing the LLM forecast after having been exposed to the human median to the LLM forecast before this exposure as preregistered, we compare this updated prediction to a simple average of the machine and human predictions as a</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: LLM forecasts for GPT-4 (left) and Claude 2 (right) before and after exposure to the human forecast. Colours distinguish first forecasts above, below, or within 20 percentage points of the human median forecast. Highlighted changes and intervals are of the respective median forecast within that group.
naive benchmark using straightforward aggregation. This allows us to test whether the improvements the models make are due to understanding of the need to appropriately update or simply as an agreement-focused response. We find in paired t-tests that for both GPT-4 at a Brier score of 0.13, $t(92)=2.583, p=.011$, and Claude 2 at a Brier score of $0.14, t(92)=3.530, p=.001$, their updated forecasts are significantly less accurate than a simple average between the machine and the human median forecasts. This suggests that the updating itself is directionally correct but fails to improve upon a simple benchmark.</p>
<h1>4 Discussion</h1>
<p>Our results show that LLM prediction capabilities can rival the gold standard of the human crowd tournament method, if they themselves draw on what we call the 'wisdom of the silicon crowd.' Previous results on single models (Halawi et al. 2024; Schoenegger and Park 2023) showed that LLMs not only underperformed compared to a human crowd in a probabilistic forecasting context, but also failed to clear simple benchmarks; while others (Abolghasemi, Ganbold, and Rotaru 2023) failed to find evidence in favour of the LLMs outperforming humans in the context of time-series forecasting. ${ }^{4}$ However, taking into account more sophisticated systems built on top of LLMs, such as combined retrieval and reasoning systems (Halawi et al. 2024), human-level prediction accuracy may already be considered matched in some aspects. We propose that the capabilities jump in moving from single frontier models to crowds of simple models in the same probabilistic forecasting context is a benefit that can be exploited in a variety of real-world contexts, as this aggregation approach remains simple to implement and does not require additions like that of news retrieval on each question. Our finding opens the door for simple, practically applicable steps like forecast aggregation to increase current AI models' forecasting ability-to predict future events in politics, economics, technology, and other real-world subjects-to a level on par with the human crowd. This opens up a lot of directly applied work, given that LLM prediction capabilities can inform decision-makers and businesses in circumstances where accurate probabilistic forecasts are difficult or expensive to acquire. Furthermore, since both our finding and the finding of Halawi et al. (2024) suggest that placing individual LLMs in advanced systems can increase their forecasting ability to a market-competitive level, it is natural to expect LLM predictions to be more widely applied across society in the near future.</p>
<p>Importantly, our finding holds despite the presence of an acquiescence bias (Costello and Roodenburg 2015; Hinz et al. 2007) in model predictions, in that our models' predictions are more likely to be above $50 \%$, despite the resolution rate of all questions being almost even. This suggests that the 'wisdom of crowds' effect using median as our aggregation is able to counteract even this acquiescence bias that is present in the majority of individual models, a robustness feature of the 'wisdom of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>crowds' mechanism (Koriat 2012). In our aggregation results, we also find that only three of the twelve models outperform the model median, which is also in line with standard accounts of wisdom of crowds. This overall suggests that the 'wisdom of the crowd' effect-in addition to applying to the human context-also applies to the silicon context. The literature on the size of the crowd needed to produce reliable 'wisdom of the crowd' effects is not very well established, though a central finding is that increasing crowd size does lead to better performance (Walter, Kölle, and Collmar 2022). As such, a natural next step in this line of research is to expand the set of models queried from the twelve we used to a substantially higher number.
However, there also do remain substantial areas of improvement for machine predictions in probabilistic forecasting. Most directly, both the aggregate and the individual models were badly calibrated, with most models showing overconfidence, i.e., they assign higher probabilities to outcomes than is warranted by the empirical facts. Improving calibration is central to providing reliable predictions over the long run (Buizza 2018), and our results of acquiescence bias suggest that this may be an actionable area for future work. Additionally, as the distance between the end of the training data and the forecasted period grows, forecasts may become less accurate as necessary background knowledge is no longer readily available to the model. Moreover, our study could draw on a well-curated set of questions from the forecasting tournament. Applications in contexts where neutral background information and question details are not available may further reduce performance.
Our results from Study 2 contribute to the literature on human-AI interactions (Kim et al. 2024; Yang et al. 2024). While previous work in the context of forecasting has looked at how LLMs can augment humans in improving prediction accuracy (Schoenegger et al. 2024), this paper provides evidence for the reverse. Specifically, our results show that machine predictions can be improved substantially by the provision of human cognition output drawn from forecasting tournaments. This finding suggests at first glance that LLM reasoning is already advanced enough to properly exploit the informational value provided by human cognition output. However, our exploratory analyses find that this process is substantially less effective than simply averaging the two estimates, suggesting that aggregation methods based on the reasoning capabilities of frontier models (in this case, GPT-4 and Claude 2) still underperform simple aggregations.
On the other hand, our findings that both frontier models (GPT-4 and Claude 2) respond as expected in their forecast updates-reducing their uncertainty when the human estimate lies within their prediction intervals, and updating in relation to the distance between their own point estimate and the human forecasts-match past theory and results pertaining to human forecasters (Atanasov et al. 2020). This overall suggests that the ability of these models to reason and act as expected-by past theory and results pertaining to human forecasters - depend on the type of task and benchmark applied. While this is not a massively strict test of their reasoning abilities-as alternative explanations of model behaviour being explained by simple expectation fulfilling remain-it does provide some evidence in favour of it.
Importantly, both studies reported in this paper test LLM capabilities in a context where it is not possible that any of the answers used to resolve the questions were part of the training data, as we queried the models in real-time alongside the human tournament. Because all question answers were unknown at the time of data collection-even to the study authors-this provides an ideal evaluation criterion for LLM capabilities: one at which our LLM ensemble approach excelled. Our findings provide evidence of LLMs' advanced reasoning capabilities, and does so in a robust way such that many of the challenges that may be raised with respect to traditional benchmarks do not apply.
In conclusion, the present paper is among the first to show that current LLMs are able to provide a human-crowd-competitive level of accurate forecasting about future real-world events. In order to do so, it is sufficient to apply the simple, practically applicable method of forecast aggregation: manifesting as the LLM ensemble approach in the so-called silicon setting. This replicates the human forecasting tournament's 'wisdom of the crowd' effect for LLMs: a phenomenon we call the 'wisdom of the silicon crowd.' Our finding opens up a number of areas for further research as well as practical applications, since the LLM ensemble approach is substantially cheaper and faster than data collection from human forecasters. Future research can aim to combine the ensemble approach with model and scaffolding progress, which may potentially result in even stronger capability gains in the domain of forecasting.</p>
<h1>Acknowledgements</h1>
<p>We are grateful to Lawrence Phillips and Peter Mühelbacher for helping us discover and correct a coding error in the non-preregistered equivalence test pertaining to the second null hypothesis of Study 1.</p>
<h2>References</h2>
<p>Abdurahman, Suhaib et al. (2023). "Perils and Opportunities in Using Large Language Models in Psychological Research". In: PsyArXiv. URL: https://osf.io/preprints/psyarxiv/d695y. Abolghasemi, Mahdi, Odkhishig Ganbold, and Kristian Rotaru (2023). "Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI". In: arXiv preprint arXiv:2312.06941. URL: https://arxiv.org/abs/2312.06941.
Acemoğlu, Daron (2023). "Harms of AI". In: The Oxford Handbook of AI Governance. Oxford University Press. ISBN: 9780197579329. DOI: 10.1093/oxfordhb/9780197579329.013.65. URL: https://doi.org/10.1093/oxfordhb/9780197579329.013.65.
Alzahrani, Norah et al. (2024). When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards. arXiv: 2402.01781 [cs.CL].
Anthropic (2023). Model Card and Evaluations for Claude Models. URL: https : / / www - cdn . anthropic . com / files / 4zrzovbb / website / bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf.
Arora, Sanjeev and Anirudh Goyal (2023). "A Theory for Emergence of Complex Skills in Language Models". In: arXiv preprint arXiv:2307.15936.
Atanasov, Pavel et al. (2020). "Small steps to accuracy: Incremental belief updaters are better forecasters". In: Proceedings of the 21st ACM Conference on Economics and Computation, pp. 873874.</p>
<p>Atari, Mohammad et al. (2023). "Which humans?" In: PsyArXiv. URL: https://osf.io/ preprints/psyarxiv/5b26t.
Baron, Jonathan et al. (2014). "Two reasons to make aggregated probability forecasts more extreme". In: Decision Analysis 11.2, pp. 133-145.
Bassamboo, Achal, Ruomeng Cui, and Antonio Moreno (2018). Wisdom of crowds: Forecasting using prediction markets. Tech. rep. Kellogg School of Management, Northwestern University.
Beaulieu-Jones, Brendin R et al. (2024). "Evaluating capabilities of large language models: Performance of GPT-4 on surgical knowledge assessments". In: Surgery.
Bender, Emily M. et al. (2021). "On the Dangers of Stochastic Parrots: Can Language Models be too Big?" In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual Event, Canada: Association for Computing Machinery, pp. 610-623. ISBN: 9781450383097. DOI: 10.1145/3442188.3445922.</p>
<p>Biderman, Stella et al. (2023). Emergent and Predictable Memorization in Large Language Models. arXiv: 2304.11158 [cs.CL].
Brier, Glenn W. (1950). "Verification of forecasts expressed in terms of probability". In: Monthly weather review 78.1, pp. 1-3.
Bubeck, Sébastien et al. (2023). Sparks of Artificial General Intelligence: Early Experiments with GPT-4. arXiv: 2303.12712 [cs.CL].
Budescu and Chen (2015). "Identifying expertise to extract the wisdom of crowds". In: Management science 61.2, pp. 267-280.
Budescu, David V. (2006). "Confidence in aggregation of opinions from multiple sources". In: Information Sampling and Adaptive Cognition. Ed. by Klaus Fiedler and Peter Juslin. Cambridge, UK: Cambridge University Press, pp. 327-352.
Buizza, Roberto (2018). "Ensemble forecasting and the need for calibration". In: Statistical postprocessing of ensemble forecasts. Elsevier, pp. 15-48.
Carlini, Nicholas et al. (2023). "Quantifying Memorization Across Neural Language Models". In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. URL: https://openreview.net/pdf?id=TatRHT\%5C_1cK.
Cholakov, Radostin and Todor Kolev (2021). "Transformers predicting the future. Applying attention in next-frame and time series forecasting". In: arXiv preprint arXiv:2108.08224.
Cohen, Jacob (2013). Statistical power analysis for the behavioral sciences. Academic press.
Costello, Shane and John Roodenburg (2015). "Acquiescence response bias-Yeasaying and higher education". In: The Educational and Developmental Psychologist 32.2, pp. 105-119.</p>
<p>Da, Zhi and Xing Huang (2020). "Harnessing the wisdom of crowds". In: Management Science 66.5, pp. 1847-1867.
Davis-Stober, Clintin P. et al. (2014). "When is a crowd wise?" In: Decision 1.2, p. 79.
Feng, Tony Haoran et al. (2024). "More Than Meets the AI: Evaluating the performance of GPT-4 on Computer Graphics assessment questions". In: Proceedings of the 26th Australasian Computing Education Conference, pp. 182-191.
Fraiwan, Mohammad and Natheer Khasawneh (2023). A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. arXiv: 2305.00237 [cs.CY].
Gemini Team et al. (2023). Gemini: A Family of Highly Capable Multimodal Models. arXiv: 2312. 11805 [cs.CL].
Ghirardato, Paolo (2002). "Revisiting Savage in a conditional world". In: Economic Theory 20, pp. 83-92.
Gneiting, Tilmann and Adrian E Raftery (2007). "Strictly proper scoring rules, prediction, and estimation". In: Journal of the American statistical Association 102.477, pp. 359-378.
Grove, Nathaniel P. and Stacey Lowery Bretz (2012). "A Continuum of Learning: From Rote Memorization to Meaningful Learning in Organic Chemistry". In: Chemistry Education Research and Practice 13.3, pp. 201-208.
Gruver, Nate et al. (2024). "Large language models are zero-shot time series forecasters". In: Advances in Neural Information Processing Systems 36.
Halawi, Danny et al. (2024). Approaching Human-Level Forecasting with Language Models. arXiv: 2402.18563 [cs.LG].</p>
<p>Heiding, Fredrik et al. (2023). "Devising and detecting phishing: Large language models vs. smaller human models". In: arXiv preprint arXiv:2308.12287.
Himmelstein, Michael, David V. Budescu, and Yoonjin Han (2023). "The wisdom of timely crowds". In: Judgment in predictive analytics. Springer International Publishing, pp. 215-242.
Himmelstein, Michael, David V. Budescu, and Elizabeth H. Ho (2023). "The wisdom of many in few: Finding individuals who are as wise as the crowd". In: Journal of Experimental Psychology: General.
Hinz, Andreas et al. (2007). "The acquiescence effect in responding to a questionnaire". In: GMS Psycho-Social Medicine 4.
Jiao, Wenxiang et al. (2023). Is ChatGPT a Good Translator? Yes with GPT-4 as the Engine. arXiv: 2301.08745 [cs.CL].
Jin, Ming et al. (2023). "Time-llm: Time series forecasting by reprogramming large language models". In: arXiv preprint arXiv:2310.01728.
Katz, Daniel Martin et al. (2023). "GPT-4 Passes the Bar Exam". In: SSRN.
Kim, Su Hwan et al. (2024). "Human-AI Collaboration in Large Language Model-Assisted Brain MRI Differential Diagnosis: A Usability Study". In: medRxiv, pp. 2024-02.
Kistowski, Jóakim v. et al. (2015). "How to build a benchmark". In: Proceedings of the 6th ACM/SPEC international conference on performance engineering, pp. 333-336.
Koriat, Asher (2012). "When are two heads better than one and why?" In: Science 336.6079, pp. 360362.</p>
<p>Laskar, Md Tahmid Rahman et al. (2023). A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. arXiv: 2305.18486 [cs.CL].
Lichtendahl Jr, Kenneth C., Yael Grushka-Cockayne, and Phillip E. Pfeifer (2013). "The wisdom of competitive crowds". In: Operations Research 61.6, pp. 1383-1398.
Magar, Inbal and Roy Schwartz (May 2022). "Data Contamination: From Memorization to Exploitation". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Dublin, Ireland: Association for Computational Linguistics, pp. 157-165. DOI: 10.18653/v1/2022.acl-short.18. URL: https://aclanthology.org/2022.aclshort. 18 .
Mandel, David R. and Alan Barnes (2014). "Accuracy of forecasts in strategic intelligence". In: Proceedings of the National Academy of Sciences 111.30, pp. 10984-10989.
Metaculus (2024). Metaculus. https://www.metaculus.com/home/. Accessed: 2024-02-21.
Naveed, Humza et al. (2023). A Comprehensive Overview of Large Language Models. arXiv: 2307. 06435 [cs.CL].
Nori, Harsha et al. (2023). Capabilities of GPT-4 on Medical Challenge Problems. arXiv: 2303. 13375 [cs.CL].
OpenAI et al. (2023). GPT-4 Technical Report. arXiv: 2303.08774 [cs.CL].</p>
<p>Park, Peter S. (2022). "The evolution of cognitive biases in human learning". In: Journal of Theoretical Biology 541, p. 111031.
Park, Peter S., Philipp Schoenegger, and Chongyang Zhu (2024). "Diminished diversity-of-thought in a standard large language model". In: Behavior Research Methods, pp. 1-17.
Park, Peter S. and Max Tegmark (2023). Divide-and-Conquer Dynamics in AI-Driven Disempowerment. arXiv: 2310.06009 [cs.CY].
Park, Peter S. et al. (2023). AI Deception: A Survey of Examples, Risks, and Potential Solutions. arXiv: 2308.14752 [cs.CY].
Peng, Yifan et al. (2023). "Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data". In: 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, pp. 1-8.
Sallam, Malik et al. (2023). "ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations". In: Narra J 3.1, e103-e103.
Savage, Leonard J. (1972). The Foundations of Statistics. eng. Second revised ed. New York: Dover Publications. ISBN: 0-486-62349-1.
Schoenegger, Philipp and Peter S. Park (2023). "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament". In: arXiv preprint arXiv:2310.13014.
Schoenegger, Philipp et al. (2024). "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy". In: arXiv preprint arXiv:2402.07862. DOI: 10.48550/arXiv. 2402 . 07862. URL: https://doi.org/10.48550/arXiv. 2402.07862.</p>
<p>Shen, Chenhui et al. (2023a). Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. arXiv: 2305.13091 [cs.CL].
Shen, Zhiqiang et al. (2023b). "SlimPajama-DC: Understanding Data Combinations for LLM Training". In: arXiv preprint arXiv:2309.10818.
Siegert, Stefan (2017). "Simplifying and generalising Murphy's Brier score decomposition". In: Quarterly Journal of the Royal Meteorological Society 143.703, pp. 1178-1183.
Summers, Lawrence H. and Steve Rattner (2023). Larry Summers on who could be replaced by AI [Interviewed by Bloomberg TV's David Westin]. URL: https://www.youtube.com/watch?v= 8Ep19yAuOgk.
Surowiecki, James (2004). The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations. London: Little, Brown.
Sutton, Rich (2023). AI succession [Youtube video of talk]. World Artificial Intelligence Conference in Shanghai. URL: https://www.youtube.com/watch?v=NgHFMolXs3U.
Tetlock, Philip E. and Dan Gardner (2016). Superforecasting: The Art and Science of Prediction. Random House.
Tetlock, Philip E. et al. (2014). "Forecasting Tournaments: Tools for Increasing Transparency and Improving the Quality of Debate". In: Current Directions in Psychological Science 23.4, pp. 290295.</p>
<p>Touvron et al., Hugo (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv: 2307.09288 [cs.CL].</p>
<p>Vaswani, Ashish et al. (2017). "Attention is All You Need". In: Advances in Neural Information Processing Systems 30.
Vemprala, Sai et al. (2023). "Chatgpt for robotics: Design principles and model abilities". In: Microsoft Auton. Syst. Robot. Res 2, p. 20.
Walter, Volker, Michael Kölle, and David Collmar (2022). "Measuring the Wisdom of the Crowd: How Many is Enough?" In: PFG-Journal of Photogrammetry, Remote Sensing and Geoinformation Science 90.3, pp. 269-291.
Wei, Jason et al. (2022). "Emergent abilities of large language models". In: arXiv preprint arXiv:2206.07682.
Winter, Joost C. F. de (2023). "Can ChatGPT Pass High School Exams on English Language Comprehension?" In: International Journal of Artificial Intelligence in Education. ISSN: 15604292.</p>
<p>Yang, Shu et al. (2024). "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima". In: arXiv preprint arXiv:2402.11271.</p>
<p>Table 4: Full list of questions</p>
<h1>Questions</h1>
<p>Will a nearly continuous human chain stretch across the length of the Forth and Clyde Canal on 14 October 2023?
Will Hamas lose control of Gaza before 2024?
Will Yahya Sinwar cease to act as Hamas Chief in the Gaza Strip before 2024?
Will Israel carry out and explicitly acknowledge a deadly attack on Iran before 2024?
Will the Conservatives hold on to their seat in the Mid Bedfordshire by-election?
Will it be determined that Israel was responsible for the attack on the Al-Ahli Baptist Hospital in Gaza City before 2024?
Will the Federal Funds Rate be raised before December 14, 2023?
Will Peter Bone MP be suspended from Parliament in 2023?
Will George Weah win re-election in the 2023 Liberian General Election?
Will India request that another Canadian diplomat be recalled before 2024?
Will New Delhi experience a "Very Unhealthy" or worse air quality index on at least four of the seven days for the week starting October 29?
Will Mike Johnson remain Speaker until 2024?
Will there be an additional Russian IPO on the MICEX in 2023?
Will Donald Trump spend at least one hour confined in a jail cell before January 1, 2024?
Will the second Starship integrated flight test achieve liftoff before January 1, 2024?
Will Sarah Bernstein or Chetna Maroo win the 2023 Booker Prize?
Will Bitcoin reach \$40,000 before January 1, 2024?
Will Volodymyr Zelenskyy visit Israel before 2024?
Will Delhi perform cloud seeding before December 1, 2023?
Will the MONUSCO UN peacekeeping mission to the Democratic Republic of the Congo be extended with a military personnel ceiling above 11,000 before January 1, 2024?
Will OpenAI report having $\geq 99 \%$ uptime for ChatGPT and the OpenAI API in December 2023?
Will the November 2023 Israel-Hamas humanitarian pause be extended?
Will a majority of voters approve Venezuela's referendum on incorporating Guayana Esequiba into Venezuela?
Will any additional Republican candidates for president drop out before 2024?
Will there be a white Christmas in at least 4 of these 9 large European cities in 2023?
Will the US Supreme Court issue a decision on hearing the case about presidential immunity before January 1, 2024?
Before 2024, will it be announced that either of the Harvard or MIT presidents will vacate their positions?
Will a major shipping company announce that they are resuming shipments through the Red Sea before 2024?
Will the ban on imports of Apple watches with blood oxygen sensors take effect before December 27, 2023?</p>
<p>Will there be a US military combat death in the Red Sea before 2024?
Will NASA re-establish communications with Voyager 1 before 1 Jan 2024?</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ For more applications of LLMs in time-series forecasting see additional work (Cholakov and Kolev 2021; Gruver et al. 2024; Jin et al. 2023)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
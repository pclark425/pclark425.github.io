<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9632 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9632</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9632</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271859627</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.06574v1.pdf" target="_blank">SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable achievements across various language tasks. To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised fine-tuning on scientific literature, building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM. SparkRA is accessible online and provides three primary functions: literature investigation, paper reading, and academic writing. As of July 30, 2024, SparkRA has garnered over 50,000 registered users, with a total usage count exceeding 1.3 million.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9632.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9632.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciLit-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Literature Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter LLM created by continual pre-training and supervised fine-tuning of the iFLYTEK Spark base model on a large corpus of scientific literature to improve parsing, summarization, and generation of academic content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciLit-LLM (based on iFLYTEK Spark LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Built from the iFLYTEK Spark LLM via continual pre-training using the standard next-token (autoregressive) objective on scientific corpora and further supervised fine-tuning (SFT) using instruction–input–output examples created by a mix of Self-Instruct and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A large mixed corpus of scholarly documents (papers and patents) collected from public sources (including arXiv), plus additional general corpora to preserve general capabilities; PDFs parsed to text with an iFLYTEK parser and cleaned for high-quality academic text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>10000000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>General and targeted scholarly queries across domains, e.g., area-based surveys ('recent papers of fake news section in 2023'), scholar-based surveys, and arbitrary question-answering about papers (both in- and out-of-paper questions).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Continual domain-adaptive pretraining on scientific text (next-token prediction) followed by supervised fine-tuning using high-quality instruction–input–output examples (Self-Instruct + human-written), and deployment inside downstream pipelines (retrieval-augmented generation, clustering + inductive summarization) to synthesize knowledge across multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Domain-specialized natural-language synthesis: summaries, reviews, paper explanations, answers to document-centric or cross-document questions, polishing and translation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>For a query 'What are the recent papers of fake news section in 2023', SciLit-LLM returns a list of relevant papers plus a concise summary of recent trends in that subfield.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation (mean opinion score, MOS, 1–5) by >5 raters per task; BLEU used for machine translation evaluation; task-specific measures like factuality and informativeness for paper reading, and fluency/fidelity/academic metrics for polishing/translation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>When used in SparkRA, SciLit-LLM (13B) achieved top results in the paper-reading task (Factuality 4.68, Informativeness 4.45, Avg 4.57), in paper polishing (Avg 4.49) and strong translation fidelity (Fidelity 4.91, BLEU 0.198); overall reported to outperform GPT-3.5 and Llama baselines and to match or exceed GPT-4 on several metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Domain adaptation increases factuality and reduces hallucination tendency; strong performance on scholarly tasks (reading, summarization, polishing, translation); cost-effective 13B size easier to train and deploy; designed to combine general corpora with scientific corpora to retain broad capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Long context windows were not trained from scratch (costly), so RAG is required for long documents; some human-evaluation metrics (for translation) were lower than GPT-4; persistent risks of hallucination are reduced but not eliminated (model exhibits higher rejection tendency for specialized queries).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit theory-distillation failure cases listed, but reported issues include hallucinated (non-existent) content for some other models (e.g., Llama2-13B) and a need to use retrieval augmentation for long documents; SciLit-LLM relies on retrieval and may abstain or reject on specialized queries, indicating coverage gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9632.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9632.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SparkRA ReviewGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SparkRA Review Generation (literature review module)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module within SparkRA that automatically generates structured review reports over up to 30 selected papers by clustering documents and performing inductive summarization to produce introductions, bodies, conclusions and headings, with citation hyperlinks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SparkRA Review Generation (powered by SciLit-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses the SciLit-LLM's clustering and inductive summarization capabilities to analyze a set of papers (max 30), structure the review (intro/body/conclusion), generate headings, and append hyperlink-style citation annotations to support verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-selected subset of papers from the SparkRA academic library (up to 30 papers per generated report); the underlying library is populated from the broader SciLit corpus described above.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>30</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User-specified research domain or author-based selection (e.g., 'papers by Chris Manning' or a topical set); guides clustering and summarization to produce a review of that area or author output.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Document clustering followed by inductive summarization performed by the LLM: the papers are clustered into themes, the LLM synthesizes the grouped content to create structured review sections and headings, and it outputs annotated text with citation hyperlinks referencing the input documents.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured narrative synthesis / literature review document with sections (introduction, body, conclusion), headings, and appended citation hyperlinks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A multi-section review that outlines publication-year distribution, recent focal topics, trends, potential future directions, and annotated citations (hyperlinks) to the source papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated as part of SparkRA system evaluations using human raters (MOS) on paper reading / literature tasks; no separate quantitative gold-standard for review-gen reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>System-wide evaluations indicate SparkRA outperforms baselines on literature-related tasks (human MOS metrics for reading and summarization-related tasks favored SparkRA); no isolated numeric score provided exclusively for review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables rapid comprehension of a small corpus (≤30) with structured output and citations; supports inductive reasoning, clustering-based structuring, and direct citation links for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited to up to 30 papers per generated review; fidelity and correctness depend on the underlying retrieval/clustering quality and the SciLit-LLM's factuality; no automated ground-truth evaluation for generated reviews reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure examples provided, but implicit risks include clustering errors causing misgrouping of themes, omission of important works when input selection is incomplete, and potential hallucinated paraphrases if citations are not carefully anchored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9632.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9632.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG Paper Reading</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation based Paper Reading</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that enables answering questions about long scientific papers by segmenting texts, retrieving relevant segments, and using SciLit-LLM to generate answers—supported by a cross-lingual retrieval embedding trained with contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG pipeline with SciLit-LLM + XLM-RoBERTa retriever</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text splitting and chapter recognition preserve semantics of segments; question generation from segments (via an LLM) yields (question, positive, negative) triples to fine-tune XLM-RoBERTa with contrastive learning for cross-lingual retrieval; retrieved segments and/or external search results are fed into SciLit-LLM for retrieval-augmented answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (SciLit-LLM); XLM-RoBERTa size not specified</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Long-format papers split into semantically coherent segments (chapters/sections); training data for retriever created by generating many (question, positive, negative) triples from those segments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User questions either within-paper (answered from the paper's segments) or outside-paper (requiring search via plugins); examples include factual questions about paper content and comparative queries across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation: 1) Segment papers preserving chapter semantics; 2) generate training Q/A pairs from segments using an LLM; 3) fine-tune XLM-RoBERTa via contrastive learning to build a cross-lingual retriever; 4) retrieve relevant segments and pass them with the user query into SciLit-LLM to produce answers; includes query rewriting and NER extraction for better retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Question answers grounded in retrieved segments, summaries of sections, multi-document comparison tables, and extractive/abstractive explanations tied to source segments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Given a paper and the question 'What are the main contributions?', the pipeline returns an answer synthesized from relevant retrieved segments with source segment references and, if needed, external citations for out-of-paper context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation using MOS on factuality and informativeness for paper reading; models compared included Llama variants, GPT-3.5, GPT-4, and SparkRA (SciLit-LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>SparkRA (RAG + SciLit-LLM) achieved highest reported scores on paper reading: Factuality 4.68, Informativeness 4.45, Average 4.57, slightly above GPT-4 (Avg 4.55) and Llama3-8B (Avg 4.41).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Handles long documents without training a long-context LLM from scratch; cross-lingual retrieval supports multilingual papers; reduces hallucination by grounding answers in retrieved segments; produces comparative analyses across multiple documents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on retriever training and segment quality; requires building and maintaining a retrieval index and contrastive-trained encoder; does not fully eliminate hallucination—reliance on retrieval may fail when relevant segments are missing or retrieval fails.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not enumerated in detail; general failure modes include retrieval misses (leading to incomplete/incorrect answers), incorrect segmentation harming semantic coherence, and potential mismatches between retrieved context and user queries causing partial or incorrect answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9632.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9632.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Literature Investigation Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SparkRA Literature Investigation (copilot + topic search engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented and LLM-driven subsystem that rewrites queries, extracts structured metadata (scholars, institutions, dates, domains, keywords), retrieves precise literature, and synthesizes high-level analyses like publication distribution, trending topics, and future directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Literature Investigation Copilot (powered by SciLit-LLM with search plugins and NER)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines SciLit-LLM for query rewriting and synthesis with NER/IE pipelines for metadata extraction and search plugin interfaces for precise retrieval; uses LLM clustering & summarization to produce topical overviews and generate review drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Large academic library assembled from the pretraining corpus (including arXiv and other public sources); retrieval returns sets of papers for a given topic or scholar.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User queries varying from noisy natural language questions to specific topical keywords or scholar names; system rewrites queries into retrieval-friendly forms (e.g., 'Applications of large models in library search domain').</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Query rewriting by SciLit-LLM, metadata extraction (NER), precise retrieval via search plugins, and LLM-based synthesis summarizing publication distributions, trends, focal topics, and suggested future directions; also supports scholar-based surveys (grouping a scholar's papers into research areas).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Topical syntheses, trend analyses, short literature summaries, lists of relevant papers, and candidate review outlines.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Input: 'In the library, what LLM technologies can assist users in improving the efficiency of finding books?'; Rewritten to 'Applications of large models in library search domain', then returns retrieved papers and a summarized synthesis of application trends and recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Implicitly evaluated within system-wide experiments using MOS; no separate numeric breakdown for the literature-investigation module alone provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>System-level evaluations indicate strong performance on literature tasks; overall SparkRA outperforms baseline LLMs on related metrics, but no module-level MOS numbers for investigation alone are given.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Improves retrieval effectiveness through query rewriting and metadata extraction; synthesizes higher-level insights (trends, distributions, future directions) useful for rapid literature surveying; integrates cross-lingual capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reliant on the correctness of query rewriting and metadata extraction; synthesis quality depends on retrieved set completeness and LLM's inductive summarization accuracy; no explicit automated factual ground-truth evaluation reported for synthesized trend/future-direction claims.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly listed; potential failure when rewritten queries diverge from user intent or when the retrieval layer returns biased/incomplete document sets, leading to partial or misleading syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9632.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9632.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLM-RoBERTa Retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLM-RoBERTa (contrastive fine-tuned cross-lingual retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-lingual encoder (XLM-RoBERTa) fine-tuned with contrastive learning on LLM-generated (question, positive, negative) triples to act as the retrieval embedding model in the SparkRA RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa (fine-tuned via contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Off-the-shelf XLM-RoBERTa language encoder used as the basis for a retriever; fine-tuned using contrastive learning with training triples generated by an LLM from paper segments to support cross-lingual retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Training triples constructed by generating questions from paper segments (using an LLM) and pairing with positive and negative samples from the same document segmentation; used to teach cross-lingual segment retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Segment-derived questions representing likely user queries about paper content; used to train retrieval to return relevant segments for arbitrary user questions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Contrastive fine-tuning on (question, positive sample, negative samples) triples derived from segmented paper content; used to produce dense embeddings for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Dense vector embeddings for retrieval; retrieved document segments for RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Not provided beyond description of the training process; used to retrieve segments that are then input to SciLit-LLM for answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Indirectly evaluated via downstream human-MOS metrics for paper reading; no standalone retriever metrics (e.g., recall@k) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Downstream improvements in factuality/informativeness for SparkRA's paper reading task imply effective retrieval, but no standalone numeric scores provided.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Cross-lingual capability and training on LLM-generated Q/A triples tailored to paper segments improves retrieval relevance for scholarly queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No direct quantitative retriever evaluation presented; effectiveness depends on quality of generated triples and segmentation; model size and hyperparameters not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly enumerated; potential failures include poor negative sampling or noisy generated questions producing suboptimal retrieval embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scibert: A pretrained language model for scientific text. <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models in retrieval-augmented generation. <em>(Rating: 2)</em></li>
                <li>Query rewriting for retrievalaugmented large language models. <em>(Rating: 2)</em></li>
                <li>Self-instruct: Aligning language models with self-generated instructions. <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback. <em>(Rating: 1)</em></li>
                <li>Improving retrieval-augmented large language models via data importance learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9632",
    "paper_id": "paper-271859627",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "SciLit-LLM",
            "name_full": "Scientific Literature Large Language Model",
            "brief_description": "A 13B-parameter LLM created by continual pre-training and supervised fine-tuning of the iFLYTEK Spark base model on a large corpus of scientific literature to improve parsing, summarization, and generation of academic content.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciLit-LLM (based on iFLYTEK Spark LLM)",
            "model_description": "Built from the iFLYTEK Spark LLM via continual pre-training using the standard next-token (autoregressive) objective on scientific corpora and further supervised fine-tuning (SFT) using instruction–input–output examples created by a mix of Self-Instruct and human experts.",
            "model_size": "13B",
            "input_corpus_description": "A large mixed corpus of scholarly documents (papers and patents) collected from public sources (including arXiv), plus additional general corpora to preserve general capabilities; PDFs parsed to text with an iFLYTEK parser and cleaned for high-quality academic text.",
            "input_corpus_size": 10000000,
            "topic_query_description": "General and targeted scholarly queries across domains, e.g., area-based surveys ('recent papers of fake news section in 2023'), scholar-based surveys, and arbitrary question-answering about papers (both in- and out-of-paper questions).",
            "distillation_method": "Continual domain-adaptive pretraining on scientific text (next-token prediction) followed by supervised fine-tuning using high-quality instruction–input–output examples (Self-Instruct + human-written), and deployment inside downstream pipelines (retrieval-augmented generation, clustering + inductive summarization) to synthesize knowledge across multiple papers.",
            "output_type": "Domain-specialized natural-language synthesis: summaries, reviews, paper explanations, answers to document-centric or cross-document questions, polishing and translation outputs.",
            "output_example": "For a query 'What are the recent papers of fake news section in 2023', SciLit-LLM returns a list of relevant papers plus a concise summary of recent trends in that subfield.",
            "evaluation_method": "Human evaluation (mean opinion score, MOS, 1–5) by &gt;5 raters per task; BLEU used for machine translation evaluation; task-specific measures like factuality and informativeness for paper reading, and fluency/fidelity/academic metrics for polishing/translation.",
            "evaluation_results": "When used in SparkRA, SciLit-LLM (13B) achieved top results in the paper-reading task (Factuality 4.68, Informativeness 4.45, Avg 4.57), in paper polishing (Avg 4.49) and strong translation fidelity (Fidelity 4.91, BLEU 0.198); overall reported to outperform GPT-3.5 and Llama baselines and to match or exceed GPT-4 on several metrics.",
            "strengths": "Domain adaptation increases factuality and reduces hallucination tendency; strong performance on scholarly tasks (reading, summarization, polishing, translation); cost-effective 13B size easier to train and deploy; designed to combine general corpora with scientific corpora to retain broad capabilities.",
            "limitations": "Long context windows were not trained from scratch (costly), so RAG is required for long documents; some human-evaluation metrics (for translation) were lower than GPT-4; persistent risks of hallucination are reduced but not eliminated (model exhibits higher rejection tendency for specialized queries).",
            "failure_cases": "No explicit theory-distillation failure cases listed, but reported issues include hallucinated (non-existent) content for some other models (e.g., Llama2-13B) and a need to use retrieval augmentation for long documents; SciLit-LLM relies on retrieval and may abstain or reject on specialized queries, indicating coverage gaps.",
            "uuid": "e9632.0",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SparkRA ReviewGen",
            "name_full": "SparkRA Review Generation (literature review module)",
            "brief_description": "A module within SparkRA that automatically generates structured review reports over up to 30 selected papers by clustering documents and performing inductive summarization to produce introductions, bodies, conclusions and headings, with citation hyperlinks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SparkRA Review Generation (powered by SciLit-LLM)",
            "model_description": "Uses the SciLit-LLM's clustering and inductive summarization capabilities to analyze a set of papers (max 30), structure the review (intro/body/conclusion), generate headings, and append hyperlink-style citation annotations to support verification.",
            "model_size": "13B",
            "input_corpus_description": "User-selected subset of papers from the SparkRA academic library (up to 30 papers per generated report); the underlying library is populated from the broader SciLit corpus described above.",
            "input_corpus_size": 30,
            "topic_query_description": "User-specified research domain or author-based selection (e.g., 'papers by Chris Manning' or a topical set); guides clustering and summarization to produce a review of that area or author output.",
            "distillation_method": "Document clustering followed by inductive summarization performed by the LLM: the papers are clustered into themes, the LLM synthesizes the grouped content to create structured review sections and headings, and it outputs annotated text with citation hyperlinks referencing the input documents.",
            "output_type": "Structured narrative synthesis / literature review document with sections (introduction, body, conclusion), headings, and appended citation hyperlinks.",
            "output_example": "A multi-section review that outlines publication-year distribution, recent focal topics, trends, potential future directions, and annotated citations (hyperlinks) to the source papers.",
            "evaluation_method": "Evaluated as part of SparkRA system evaluations using human raters (MOS) on paper reading / literature tasks; no separate quantitative gold-standard for review-gen reported.",
            "evaluation_results": "System-wide evaluations indicate SparkRA outperforms baselines on literature-related tasks (human MOS metrics for reading and summarization-related tasks favored SparkRA); no isolated numeric score provided exclusively for review generation.",
            "strengths": "Enables rapid comprehension of a small corpus (≤30) with structured output and citations; supports inductive reasoning, clustering-based structuring, and direct citation links for verification.",
            "limitations": "Limited to up to 30 papers per generated review; fidelity and correctness depend on the underlying retrieval/clustering quality and the SciLit-LLM's factuality; no automated ground-truth evaluation for generated reviews reported.",
            "failure_cases": "No explicit failure examples provided, but implicit risks include clustering errors causing misgrouping of themes, omission of important works when input selection is incomplete, and potential hallucinated paraphrases if citations are not carefully anchored.",
            "uuid": "e9632.1",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "RAG Paper Reading",
            "name_full": "Retrieval-Augmented Generation based Paper Reading",
            "brief_description": "A pipeline that enables answering questions about long scientific papers by segmenting texts, retrieving relevant segments, and using SciLit-LLM to generate answers—supported by a cross-lingual retrieval embedding trained with contrastive learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RAG pipeline with SciLit-LLM + XLM-RoBERTa retriever",
            "model_description": "Text splitting and chapter recognition preserve semantics of segments; question generation from segments (via an LLM) yields (question, positive, negative) triples to fine-tune XLM-RoBERTa with contrastive learning for cross-lingual retrieval; retrieved segments and/or external search results are fed into SciLit-LLM for retrieval-augmented answer generation.",
            "model_size": "13B (SciLit-LLM); XLM-RoBERTa size not specified",
            "input_corpus_description": "Long-format papers split into semantically coherent segments (chapters/sections); training data for retriever created by generating many (question, positive, negative) triples from those segments.",
            "input_corpus_size": null,
            "topic_query_description": "User questions either within-paper (answered from the paper's segments) or outside-paper (requiring search via plugins); examples include factual questions about paper content and comparative queries across papers.",
            "distillation_method": "Retrieval-augmented generation: 1) Segment papers preserving chapter semantics; 2) generate training Q/A pairs from segments using an LLM; 3) fine-tune XLM-RoBERTa via contrastive learning to build a cross-lingual retriever; 4) retrieve relevant segments and pass them with the user query into SciLit-LLM to produce answers; includes query rewriting and NER extraction for better retrieval.",
            "output_type": "Question answers grounded in retrieved segments, summaries of sections, multi-document comparison tables, and extractive/abstractive explanations tied to source segments.",
            "output_example": "Given a paper and the question 'What are the main contributions?', the pipeline returns an answer synthesized from relevant retrieved segments with source segment references and, if needed, external citations for out-of-paper context.",
            "evaluation_method": "Human evaluation using MOS on factuality and informativeness for paper reading; models compared included Llama variants, GPT-3.5, GPT-4, and SparkRA (SciLit-LLM).",
            "evaluation_results": "SparkRA (RAG + SciLit-LLM) achieved highest reported scores on paper reading: Factuality 4.68, Informativeness 4.45, Average 4.57, slightly above GPT-4 (Avg 4.55) and Llama3-8B (Avg 4.41).",
            "strengths": "Handles long documents without training a long-context LLM from scratch; cross-lingual retrieval supports multilingual papers; reduces hallucination by grounding answers in retrieved segments; produces comparative analyses across multiple documents.",
            "limitations": "Quality depends on retriever training and segment quality; requires building and maintaining a retrieval index and contrastive-trained encoder; does not fully eliminate hallucination—reliance on retrieval may fail when relevant segments are missing or retrieval fails.",
            "failure_cases": "Not enumerated in detail; general failure modes include retrieval misses (leading to incomplete/incorrect answers), incorrect segmentation harming semantic coherence, and potential mismatches between retrieved context and user queries causing partial or incorrect answers.",
            "uuid": "e9632.2",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Literature Investigation Copilot",
            "name_full": "SparkRA Literature Investigation (copilot + topic search engine)",
            "brief_description": "A retrieval-augmented and LLM-driven subsystem that rewrites queries, extracts structured metadata (scholars, institutions, dates, domains, keywords), retrieves precise literature, and synthesizes high-level analyses like publication distribution, trending topics, and future directions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Literature Investigation Copilot (powered by SciLit-LLM with search plugins and NER)",
            "model_description": "Combines SciLit-LLM for query rewriting and synthesis with NER/IE pipelines for metadata extraction and search plugin interfaces for precise retrieval; uses LLM clustering & summarization to produce topical overviews and generate review drafts.",
            "model_size": "13B",
            "input_corpus_description": "Large academic library assembled from the pretraining corpus (including arXiv and other public sources); retrieval returns sets of papers for a given topic or scholar.",
            "input_corpus_size": null,
            "topic_query_description": "User queries varying from noisy natural language questions to specific topical keywords or scholar names; system rewrites queries into retrieval-friendly forms (e.g., 'Applications of large models in library search domain').",
            "distillation_method": "Query rewriting by SciLit-LLM, metadata extraction (NER), precise retrieval via search plugins, and LLM-based synthesis summarizing publication distributions, trends, focal topics, and suggested future directions; also supports scholar-based surveys (grouping a scholar's papers into research areas).",
            "output_type": "Topical syntheses, trend analyses, short literature summaries, lists of relevant papers, and candidate review outlines.",
            "output_example": "Input: 'In the library, what LLM technologies can assist users in improving the efficiency of finding books?'; Rewritten to 'Applications of large models in library search domain', then returns retrieved papers and a summarized synthesis of application trends and recommendations.",
            "evaluation_method": "Implicitly evaluated within system-wide experiments using MOS; no separate numeric breakdown for the literature-investigation module alone provided.",
            "evaluation_results": "System-level evaluations indicate strong performance on literature tasks; overall SparkRA outperforms baseline LLMs on related metrics, but no module-level MOS numbers for investigation alone are given.",
            "strengths": "Improves retrieval effectiveness through query rewriting and metadata extraction; synthesizes higher-level insights (trends, distributions, future directions) useful for rapid literature surveying; integrates cross-lingual capabilities.",
            "limitations": "Reliant on the correctness of query rewriting and metadata extraction; synthesis quality depends on retrieved set completeness and LLM's inductive summarization accuracy; no explicit automated factual ground-truth evaluation reported for synthesized trend/future-direction claims.",
            "failure_cases": "Not explicitly listed; potential failure when rewritten queries diverge from user intent or when the retrieval layer returns biased/incomplete document sets, leading to partial or misleading syntheses.",
            "uuid": "e9632.3",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "XLM-RoBERTa Retriever",
            "name_full": "XLM-RoBERTa (contrastive fine-tuned cross-lingual retriever)",
            "brief_description": "A cross-lingual encoder (XLM-RoBERTa) fine-tuned with contrastive learning on LLM-generated (question, positive, negative) triples to act as the retrieval embedding model in the SparkRA RAG pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa (fine-tuned via contrastive learning)",
            "model_description": "Off-the-shelf XLM-RoBERTa language encoder used as the basis for a retriever; fine-tuned using contrastive learning with training triples generated by an LLM from paper segments to support cross-lingual retrieval.",
            "model_size": null,
            "input_corpus_description": "Training triples constructed by generating questions from paper segments (using an LLM) and pairing with positive and negative samples from the same document segmentation; used to teach cross-lingual segment retrieval.",
            "input_corpus_size": null,
            "topic_query_description": "Segment-derived questions representing likely user queries about paper content; used to train retrieval to return relevant segments for arbitrary user questions.",
            "distillation_method": "Contrastive fine-tuning on (question, positive sample, negative samples) triples derived from segmented paper content; used to produce dense embeddings for retrieval.",
            "output_type": "Dense vector embeddings for retrieval; retrieved document segments for RAG pipeline.",
            "output_example": "Not provided beyond description of the training process; used to retrieve segments that are then input to SciLit-LLM for answer generation.",
            "evaluation_method": "Indirectly evaluated via downstream human-MOS metrics for paper reading; no standalone retriever metrics (e.g., recall@k) reported.",
            "evaluation_results": "Downstream improvements in factuality/informativeness for SparkRA's paper reading task imply effective retrieval, but no standalone numeric scores provided.",
            "strengths": "Cross-lingual capability and training on LLM-generated Q/A triples tailored to paper segments improves retrieval relevance for scholarly queries.",
            "limitations": "No direct quantitative retriever evaluation presented; effectiveness depends on quality of generated triples and segmentation; model size and hyperparameters not specified.",
            "failure_cases": "Not explicitly enumerated; potential failures include poor negative sampling or noisy generated questions producing suboptimal retrieval embeddings.",
            "uuid": "e9632.4",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scibert: A pretrained language model for scientific text.",
            "rating": 2,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "Galactica: A large language model for science.",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Benchmarking large language models in retrieval-augmented generation.",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_in_retrievalaugmented_generation"
        },
        {
            "paper_title": "Query rewriting for retrievalaugmented large language models.",
            "rating": 2,
            "sanitized_title": "query_rewriting_for_retrievalaugmented_large_language_models"
        },
        {
            "paper_title": "Self-instruct: Aligning language models with self-generated instructions.",
            "rating": 1,
            "sanitized_title": "selfinstruct_aligning_language_models_with_selfgenerated_instructions"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback.",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Improving retrieval-augmented large language models via data importance learning.",
            "rating": 1,
            "sanitized_title": "improving_retrievalaugmented_large_language_models_via_data_importance_learning"
        }
    ],
    "cost": 0.01263375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model</p>
<p>Dayong Wu 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Jiaqi Li 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>University of Science and Technology of China
China</p>
<p>Baoxin Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Harbin Institute of Technology
China</p>
<p>Honghong Zhao 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Siyuan Xue 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Yanjie Yang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Zhijun Chang 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Rui Zhang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Li Qian 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Bo Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Shijin Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Zhixiong Zhang 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Guoping Hu 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model
241B7D3B9B5DA10EF36BEDE25F9E6A8D
Large language models (LLMs) have shown remarkable achievements across various language tasks.To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised finetuning on scientific literature, building upon the iFLYTEK Spark LLM.Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.SparkRA is accessible online 1 and provides three primary functions: literature investigation, paper reading, and academic writing.As of July 30, 2024, SparkRA has garnered over 50,000 registered users, with a total usage count exceeding 1.3 million.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved significant success in natural language processing, including text generation and language understanding (Brown et al., 2020;Chowdhery et al., 2023).Owing to their strong capabilities, LLMs have shown immense potential across many downstream fields, such as education, medicine, and finance (Kasneci et al., 2023;Thirunavukarasu et al., 2023;Clusmann et al., 2023;Shah et al., 2023).</p>
<p>As the performance of LLMs in scientific literature does not fully meet the needs of scholars, we developed a Scientific Literature LLM (SciLit-LLM).We began by collecting a large dataset of scientific literature, including academic papers and patents, and performed data cleaning to ensure high-quality academic text.We then continued pre-training the open-source iFLYTEK Spark LLM (13B) 2 using an autoregressive training task, followed by supervised fine-tuning, to create our SciLit-LLM.Traditional knowledge service systems generally provide limited functionalities, such as the retrieval of scholarly articles and assistive reading services.In this paper, we introduce the Spark Research Assistant (SparkRA), a knowledge service system based on our scientific literature LLM.SparkRA offers a comprehensive, one-stop solution for scientific literature services.Figure 1 depicts the process of constructing the SparkRA system.The features of SparkRA are as follows:</p>
<p>• Literature investigation: this sub-system can automatically analyze and summarize research areas, and generate research reviews.</p>
<p>• Paper reading: this sub-system can intelligently interpret papers and quickly answer questions.</p>
<p>• Academic writing: this sub-system can provide the functions for writing academic papers including one-click translation, polishing, and automatic error detection.</p>
<p>Experimental evaluation demonstrates that SparkRA outperforms existing models, including GPT-3.5 and Llama3-8B, across all tasks, establishing its efficacy in enhancing the productivity and accuracy of academic research activities.</p>
<p>arXiv:2408.06574v1 [cs.CL] 13 Aug 2024</p>
<p>2 Scientific Literature LLM</p>
<p>Base model</p>
<p>To build the LLM for scientific literature services, we selected the Spark LLM as the foundation model for building our scientific literature LLM (ScLlit-LLM).The Spark LLM, developed by iFLY-TEK Research, demonstrates impressive performance in processing both English and Chinese languages.iFlytekSpark-13B has consistently ranked among the top in numerous well-known public benchmarks, demonstrating its superiority.Its performance is notably superior to other open-source models of equivalent size.</p>
<p>Continual pre-training</p>
<p>While the Spark LLM exhibits strong capabilities in language comprehension and text generation, it may struggle to directly provide accurate responses to scholarly inquiries without targeted training in the scientific domain.Consequently, we have designed a Scientific literature LLM that is specifically oriented towards parsing and understanding scientific literature.</p>
<p>Inspired by the existing research (Beltagy et al., 2019;Hong et al., 2022), we have further pretrained the spark model on an extensive corpus of academic texts to enhance the model's performance in processing and generating scientific literature Data preparation.To enhance the foundational large language model (LLM), it is imperative to amass a vast corpus of high-quality data, which includes kinds of scholarly literature like papers and patents.We collected a vast number of academic papers from various publicly accessible websites, such as arXiv 3 .</p>
<p>Given that academic documents are predominantly archived in PDF format, it is crucial to convert these PDFs into text while meticulously eliminating any extraneous elements.For this purpose, we employed a sophisticated PDF parsing tool developed by iFLYTEK.In the process of advancing our scientific literature LLM, we have incorporated a dataset comprising over 10M academic papers.</p>
<p>To prevent LLM from losing its general capabilities, we also incorporated a significant amount of general corpora.This strategy ensures that after continual pre-training, the scientific literature LLM performs better in the field of science while maintaining the general capabilities.</p>
<p>3 https://arxiv.org/Pre-training.Similar to the traditional LLM pretraining process, the scientific literature LLM employs the same next-word prediction task for its continual pre-training on a corpus of scientific literature comprising billions of tokens.</p>
<p>Upon evaluation, the scientific literature LLM, continual pre-training, exhibits improved performance on general scholarly inquiries.Moreover, for specialized academic queries without provided context, the scientific literature LLM demonstrates a higher rejection tendency, effectively reducing instances of hallucination.</p>
<p>Supervised fine-tuning</p>
<p>Supervised fine-tuning (SFT) is a technique used to enhance large language models (LLMs) by further training a pre-trained model to improve its accuracy and relevance for specific tasks or domains.The efficacy of SFT in refining LLMs is well-documented (Wei et al., 2022;Ouyang et al., 2022).This process involves utilizing a carefully curated dataset with labeled examples that illustrate the desired output.During SFT, the model learns from these examples to comprehend the intricacies of the task more thoroughly.Consequently, SFT enables the model to retain its broad knowledge base while acquiring specialization in targeted areas, resulting in enhanced user experiences and more precise information delivery.</p>
<p>Data preparation.In the construction of our datasets for supervised fine-tuning, each instance within datasets is composed of three elements: an instruction, an input, and an output.We utilize a dual approach in formulating instructions, leveraging both Self-instruct (Wang et al., 2023b) and human writing.</p>
<p>To exemplify, consider the instruction: "Please translate the input English sentence into Chinese"; here, the input component would be an English sentence.For the generation of outputs corresponding to given instructions and inputs, we employ meticulously devised manual methods to craft expert responses.</p>
<p>Training.Upon completing the construction of SFT datasets, we commenced the Supervised Fine-Tuning (SFT) of scientific literature LLM.The instances within the dataset serve as labeled data for the SFT of the model.Since each instance is meticulously crafted by experts, they are of higher quality compared to the generic data used during the 3 SparkRA</p>
<p>Based on our SciLit-LLM, we developed a literature services system SparkRA.This platform is comprised of three functions: literature investigation, paper reading, and academic writing.Notably, SparkRA is equipped to process inputs in both Chinese and English, thereby catering to a diverse linguistic user base.The architecture of SparkRA is shown in Figure 2 and the demonstration video has been published on YouTube 4 .</p>
<p>Literature investigation</p>
<p>This function is designed to facilitate the exploration of academic literature and is comprised of three integral components: an investigation copilot, a research topic search engine, and a review generation module.The architecture and screenshot of the literature investigation function are respectively shown in Figure 3 and Figure 4.</p>
<p>Investigation copilot.This copilot assists users in deepening their understanding of specific research domains and various scholars through interactive natural language dialogue.(1) Area-based survey.Users can easily obtain the summarization and papers of a specific research area.For example, the user can send the query "What are the recent papers of fake news section in 2023".SparkRA will show the papers and give a summary.</p>
<p>(2) Scholar-based survey.This function can output the papers of the input scholar and divide the papers into different research areas.For example, the user can send the query "What research has Chris Manning from Stanford University conducted".</p>
<p>Topic search engine.The search interface accommodates queries pertaining to research topics in both Chinese and English.Upon receiving a specified topic, SparkRA retrieves relevant papers from an extensive academic library and provides concise summaries of their content.</p>
<p>(1) Query rewriting.There is a diversity of user retrieval query formats and the occasional inclusion of noise, such as "In the library, what LLM technologies can assist users in improving the efficiency of finding books?".Upon receiving a user's query, scientific literature LLM is used to revise the query into a format more suited for retrieval, like "Applications of large models in library search domain".This strategy can significantly enhance the system's ability to locate the desired literature.</p>
<p>(2) Precise Retrieval.Upon completion of the rewriting process, the revised query is subjected to information extraction through natural language understanding technologies, such as Named Entity Recognition (NER).The extracted information encompasses scholars, institutions, dates, domains, and keywords, among others.Based on the extracted content, the corresponding search plugin interfaces are invoked to obtain precise search results.</p>
<p>(3) Literature-based summary.Building on the retrieval outcomes, the scientific literature LLM synthesizes findings, encompassing the distribution of publication years, trends in literature popularity, recent focal topics, and potential future directions of development.</p>
<p>Review generation.This function enables the generation of a report based on a selection of papers, with a maximum limit of 30 papers.The generated report facilitates an expedited comprehension of a substantial volume of literature within a specific domain or authored by an individual.</p>
<p>In this function, we leveraged the clustering capabilities and inductive summarization prowess of LLM.Through the clustering of dozens of literature papers, the model structured the introduction, body, and conclusion of a comprehensive review, including the formulation of pertinent headings.Subsequently, the model demonstrated its robust capacity for inductive reasoning and summarization.It also featured the capability to annotate the analytical text with hyperlinks, serving as citations that facilitate reference validation at the end of the review and enable user verification.</p>
<p>Paper reading</p>
<p>This function can assist scholars and students in reading academic papers.With the rapid development of artificial intelligence technology, a large number of cutting-edge papers emerge every day.It is necessary to develop an intelligent system to help people understand papers.</p>
<p>For paper reading, LLMs with longer context windows are required because the full article of paper is usually long.However, training an LLM with long context windows from scratch requires significantly larger investments.To facilitate this, we employ a retrieval-augmented approach to enhance the effectiveness of the large model's answers.We initiate text splitting as a primary step and engage in chapter recognition to preserve the semantic integrity of segments.For the cross-language retrieval embedding model, firstly, we generate questions from paper segments using an LLM and con-struct a large set of (question, positive sample, negative samples) pairs for training.Subsequently, we use XLM-RoBERTa (Conneau et al., 2020) as the language encoder and fine-tune the model via contrastive learning.The input question and retrieved segments are finally fed into the SciLit-LLM to generate answers.</p>
<p>Reading Copilot enhances paper comprehension through natural language interactions.Questions fall into two categories: those within the paper, which SciLit-LLM answers using the input paper alone, and those outside the paper, which require a search engine plugin to retrieve relevant information.For the latter, answers are generated through retrieval-augmented generation using SciLit-LLM.</p>
<p>Multi-Document Comparison allows for the comparison of two to five papers.For each selected paper, SparkRA provides the abstract and contributions separately.It also generates a comparative analysis table that highlights the proposed approaches and advantages of each paper.SparkRA can identify and output both the similarities and differences among the selected papers.</p>
<p>Academic writing</p>
<p>This function is directly powered by SciLit-LLM and includes polishing and translation.</p>
<p>Paper polishing.This function is used to assist the scholar and students in polishing the academic paper draft.We construct a large corpus of texts requiring polishing based on a multitude of wellwritten academic papers, utilizing few-shot learning and chain-of-thought (COT) prompting methodologies, followed by supervised learning for instruction fine-tuning.</p>
<p>Academic translation.In order to accurately translate domain-specific terminology, we have implemented a dynamic perception prompts approach to guide the model in completing translation tasks.Based on the user's input prompts, we obtain prompts with professional terminology translations from a terminology translation lexicon in the knowledge base, which are then fed into the large language model.</p>
<p>Experiments</p>
<p>Experiment setting</p>
<p>To validate the results of SparkRA, we adopt the following LLMs as the baseline models: • Llama: a large-scale language model developed and open-sourced by Meta, was compared to SciLit-LLM using three versions: Llama2-7B, Llama2-13B, and Llama3-8B.</p>
<p>• ChatGPT (GPT-3.5): it is a large-scale language model in the field of artificial intelligence developed by OpenAI.</p>
<p>• GPT-4: GPT-4 Turbo serves as our baseline model, consistently outperforming in a range of NLP tasks.</p>
<p>We evaluate the performance of models using the mean opinion score (MOS) on a scale of 1 (poorest) to 5 (optimal), with evaluations conducted by more than five individuals per task.For the machine translation task, we also use the BLEU metric (Papineni et al., 2002) for model evaluation.We gathered 100 academic parallel paragraphs from public Chinese journals with Chinese and English abstracts to serve as test sets.</p>
<p>To assess paper reading performance, we employ following two measures:</p>
<p>• Factuality: evaluates the accuracy of the system's response to factual information;</p>
<p>• Informativeness: assesses the completeness of the system's response.</p>
<p>To evaluate paper polishing and academic translation performance, we use three criteria:</p>
<p>• Fluency: assesses the language coherence of model's outputs;</p>
<p>• Fidelity: measures content faithfulness to the original text;</p>
<p>• Academic: evaluates adherence to academic language standards.</p>
<p>Results</p>
<p>The results of the paper reading are shown in Table 2 shows the results of the paper polishing task.While Llama2-13B generates coherent text, it struggles with fidelity due to non-existent elements.Although Llama3-8B performs well across tasks, our SparkRA model, pre-trained on scientific literature and fine-tuned with 13 billion parameters, shows even greater improvement.SparkRA achieves state-of-the-art results compared to widely used LLMs like GPT-3.5 and GPT-4 across all evaluation metrics, excelling particularly in academic relevance.</p>
<p>Table 3 presents the academic translation results.SparkRA excels with the highest fidelity score (4.91) and the second-highest academic quality (4.75), showcasing its superior ability to preserve meaning and produce contextually appropriate translations.Additionally, SparkRA's BLEU score of 0.198 reflects its robustness in both human and automatic evaluations.Despite lower human evaluation scores than GPT-4, SparkRA's 13B parameter size offers flexibility, ease of training, and cost-effectiveness.</p>
<p>Related Work</p>
<p>Scientific literature pre-trained language model Since the release of the pre-trained models  (Vaswani et al., 2017;Radford et al., 2018;Devlin et al., 2019), the language models for scientific literature have attracted the attention of scholars.These models are trained on various scientific datasets, with SciBERT on PubMed Central (Beltagy et al., 2019), BioBERT and BioMegatron on biomedical literature (Lee et al., 2020;Shin et al., 2020), Galactica on multilingual articles (Taylor et al., 2022), and ScholarBERT on ACL Anthology Corpus (Hong et al., 2022).</p>
<p>Retrieval augmented generation to LLM Retrieval-Augmented Generation (RAG), introduced by Lewis et al. (2020), mitigates hallucinations in Large Language Models (LLMs) by integrating external data.Ma et al. (2023) advanced RAG with query rewriting, while Chen et al. ( 2023) benchmarked its effects, creating the RGB.Lyu et al. (2023) developed an algorithm for assessing retrieved data significance.</p>
<p>AI for science Artificial intelligence has significantly impacted scientific research, enhancing efficiency and literature growth (Merchant et al., 2023;Szymanski et al., 2023).Wang et al. (2023a) proposed an AI-based scientific research method that can automatically extract useful information from a large amount of data and then use this information to conduct scientific research and discovery.Artificial intelligence technology has great potential in scientific research and discovery.</p>
<p>Conclusion</p>
<p>The SparkRA system, built on the SciLit-LLM, provides a comprehensive solution for academic tasks, including literature investigation, paper reading, and academic writing.Through extensive experiments, SparkRA demonstrated superior performance compared to existing models like ChatGPT, and even surpassed GPT-4 in specific tasks such as paper polishing, demonstrating its potential to enhance productivity for researchers and students with its precise and context-aware support for academic activities.</p>
<p>Figure 1 :
1
Figure 1: The process of building SparkRA system.</p>
<p>Figure 2 :
2
Figure 2: The system architecture of SparkRA integrates iFLYTEK Spark LLM and Scientific Literature LLM to facilitate literature investigation, paper reading, and academic writing.</p>
<p>Figure 3 :
3
Figure 3: The architecture of RAG-based literature investigation.</p>
<p>Figure 4 :
4
Figure 4: Literature investigation page.</p>
<p>Table 1 :
1
Results of paper reading task.
Factuality Informativeness Avg.Llama2-7B3.983.503.74Llama2-13B4.473.724.10Llama3-8B4.634.194.41GPT-3.54.203.974.09GPT-44.674.434.55SparkRA4.684.454.57</p>
<p>Table 2 :
2
Results of paper polishing task.
Fluency Fidelity Academic Avg.Llama2-7B4.593.944.444.32Llama2-13B4.593.534.064.06Llama3-8B4.563.974.474.33GPT-3.54.264.234.384.29GPT-44.264.294.414.32SparkRA4.414.454.614.49</p>
<p>Table 1 .
1
The highest results in the table are highlighted in bold, and the second-highest results are underlined.
SparkRA outperforms other models across all met-rics. It achieves the highest score in Factuality witha score of 4.68, surpassing the closest competitor,GPT-4, which scores 4.67. In terms of Informative-ness, SparkRA attains a score of 4.45, again leadingover GPT-4, which scores 4.43. Overall, SparkRAachieves the highest average score of 4.57, demon-strating superior performance compared to othermodels like Llama3-8B. These results underscoreSparkRA's effectiveness in producing factually ac-curate and informative text, establishing it as astate-of-the-art model in the paper reading task.</p>
<p>Table 3 :
3
Results of academic translation task.
Fluency Fidelity Academic Avg. BLEULlama2-7B4.533.934.134.200.104Llama2-13B4.734.034.334.360.116Llama3-8B4.644.464.434.510.168GPT-3.54.414.754.544.570.193GPT-44.504.884.844.740.180SparkRA4.344.914.754.670.198</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Benchmarking large language models in retrieval-augmented generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, arXiv:2309.014312023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Gregory P Veldhuizen, et al. 2023. The future landscape of large language models in medicine. Jan Clusmann, Fiona R Kolbinger, Sophie Hannah, Muti, Jan-Niklas Zunamys I Carrero, Narmin Eckardt, Chiara Ghaffari Laleh, Lavinia Maria, Sophie-Caroline Löffler, Michaela Schwarzkopf, Unger, Communications Medicine. 31141</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Carl Malamud, Roger Magoulas, Kyle Chard, Ian Foster, arXiv:2205.11342Scholarbert: Bigger is not always better. 2022arXiv preprint</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Improving retrieval-augmented large language models via data importance learning. Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang, arXiv:2307.030272023arXiv preprint</p>
<p>Query rewriting for retrievalaugmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, arXiv:2305.142832023arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Creation and adoption of large language models in medicine. H Nigam, David Shah, Michael A Entwistle, Pfeffer, Jama. 33092023</p>
<p>Biomegatron: Larger biomedical domain language model. Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. </p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b1</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
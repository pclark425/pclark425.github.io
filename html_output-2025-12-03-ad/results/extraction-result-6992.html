<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6992 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6992</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6992</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-264555065</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.17936v1.pdf" target="_blank">Transformers as Graph-to-Graph Models</a></p>
                <p><strong>Paper Abstract:</strong> We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6992.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6992.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G2GT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Graph Transformer (G2GT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer architecture that explicitly inputs graph edges into self-attention computations via learned relation embeddings and predicts graph edges from pairwise vector functions; supports iterative non-autoregressive refinement (RNGT) to jointly embed input, predicted and latent graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>relation‑augmented attention encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph relations r_ij as learned relation embedding vectors which are injected into the Transformer's attention score computation (added to query-key interactions) and into the attention value outputs; edges are represented as labels (one‑hot over label set L) mapped to embedding vectors WR1, WR2, WR3 and used to bias attention scores and outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based / attention‑bias encoding (non-sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>direct pairwise edge injection into attention matrix (each i,j cell gets relation embedding r_ij projected via learned matrices; no traversal/linearization used)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Penn Treebank; Universal Dependencies Treebanks; CoNLL 2009; CoNLL 2012; CoNLL 2005 (various tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph prediction / structured prediction tasks (dependency parsing, semantic role labelling, coreference resolution, collocation extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph‑to‑Graph Transformer (G2GT) and Recursive Non‑Autoregressive G2GT (RNGT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder/decoder modified to accept relation embeddings in attention (see equations e_ij and z_i); supports non‑autoregressive parallel edge prediction using pairwise classifiers (linear/bilinear/MLP) and iterative refinement over predicted graphs (RNGT). Models are evaluated both from random init and initialized with pretrained Transformers (e.g., BERT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dependency LAS (Labeled Attachment Score), SRL F1, Coreference precision/recall/F1 (CoNLL metrics), relative error reduction (RER)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported substantial improvements vs baselines: e.g., transition‑based parsing integration: 9.97% LAS relative error reduction (RER) vs StateTr baseline; when initializing with BERT, 27.65% LAS RER reported; RNGT shows significant LAS improvements across 13 UD languages (average improvements reported in paper tables); SRL and coreference experiments show improved F1 over comparable baselines (specific numbers in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables non‑autoregressive parallel edge prediction (faster inference per iteration), iterative refinement captures between‑edge correlations improving final graph quality; integrates effectively with pretrained Transformers (BERT initialization yields large gains), and allows joint embedding of text and graph for global structured decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not linearize graphs into textual sequences — representation is an attention/bias augmentation rather than a text serialization; output-node set must be (a subset of) input nodes (paper notes limitation); scaling to very large graphs and tractable embedding size are open problems; canonicalization / deterministic serialization not addressed because representation is not text sequence based.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms or significantly improves over baselines such as UDify and SynTr on dependency parsing when used as a refinement module; compared to hard‑coding graph structure into networks, G2GT injects graph as a soft bias via attention and learns complementary attention patterns; RNGT iterative refinement yields better joint modelling of entire graphs compared to independent non‑autoregressive edge prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Graph-to-Graph Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6992.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6992.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JointGT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JointGT (Graph‑text joint representation learning for text generation from knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph‑text joint encoding approach that uses Graph Transformer style representations to jointly encode knowledge graphs and text for knowledge‑to‑text generation tasks (cited as an application of Graph Transformer approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JointGT: Graph-text joint representation learning for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph‑text joint encoding (JointGT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A joint encoding that fuses graph (knowledge graph) structure with text representations so that generation models can condition on graph structure for text generation; described in the paper only as a related work example without implementation details here.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>joint graph+text encoding (architecture embedding), not a pure textual linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge‑to‑text generation / text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>JointGT (Ke et al., 2021) — Graph‑text joint representation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not detailed in this paper; cited as a Graph Transformer approach that jointly represents graphs and text for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only mentioned in related work here; paper does not report advantages/limitations of JointGT itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Mentioned as an example of GT approach applied to knowledge→text generation; no direct empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Graph-to-Graph Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6992.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6992.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TableFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TableFormer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Graph Transformer approach that combines text and table encodings for table‑based question answering, cited as an example of Graph Transformer application to structured (table) + text input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TableFormer: Robust transformer modeling for table-text encoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>table‑text joint encoding (TableFormer)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes table structure (rows, columns, cell relations) together with text using Graph Transformer style biases in attention to model table‑text interactions; referenced in related work without technical detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>joint structured+text embedding (attention bias), not an explicit graph→text linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>table‑based question answering / joint table‑text encoding</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TableFormer (Yang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified here; cited as a GT application combining table structure with text in attention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only mentioned in related work; no experimental details or limitations provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Cited as an application of graph‑aware attention for structured inputs (table + text); no direct comparisons reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transformers as Graph-to-Graph Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph-to-graph transformer for transition-based dependency parsing <em>(Rating: 2)</em></li>
                <li>Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement <em>(Rating: 2)</em></li>
                <li>Syntax-aware graph-to-graph transformer for semantic role labelling <em>(Rating: 2)</em></li>
                <li>Graph refinement for coreference resolution <em>(Rating: 2)</em></li>
                <li>JointGT: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>TableFormer: Robust transformer modeling for table-text encoding <em>(Rating: 2)</em></li>
                <li>Graphormer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6992",
    "paper_id": "paper-264555065",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "G2GT",
            "name_full": "Graph-to-Graph Transformer (G2GT)",
            "brief_description": "A Transformer architecture that explicitly inputs graph edges into self-attention computations via learned relation embeddings and predicts graph edges from pairwise vector functions; supports iterative non-autoregressive refinement (RNGT) to jointly embed input, predicted and latent graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "relation‑augmented attention encoding",
            "representation_description": "Encodes graph relations r_ij as learned relation embedding vectors which are injected into the Transformer's attention score computation (added to query-key interactions) and into the attention value outputs; edges are represented as labels (one‑hot over label set L) mapped to embedding vectors WR1, WR2, WR3 and used to bias attention scores and outputs.",
            "representation_type": "token‑based / attention‑bias encoding (non-sequential)",
            "encoding_method": "direct pairwise edge injection into attention matrix (each i,j cell gets relation embedding r_ij projected via learned matrices; no traversal/linearization used)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Penn Treebank; Universal Dependencies Treebanks; CoNLL 2009; CoNLL 2012; CoNLL 2005 (various tasks)",
            "task_name": "graph prediction / structured prediction tasks (dependency parsing, semantic role labelling, coreference resolution, collocation extraction)",
            "model_name": "Graph‑to‑Graph Transformer (G2GT) and Recursive Non‑Autoregressive G2GT (RNGT)",
            "model_description": "Transformer encoder/decoder modified to accept relation embeddings in attention (see equations e_ij and z_i); supports non‑autoregressive parallel edge prediction using pairwise classifiers (linear/bilinear/MLP) and iterative refinement over predicted graphs (RNGT). Models are evaluated both from random init and initialized with pretrained Transformers (e.g., BERT).",
            "performance_metric": "Dependency LAS (Labeled Attachment Score), SRL F1, Coreference precision/recall/F1 (CoNLL metrics), relative error reduction (RER)",
            "performance_value": "Reported substantial improvements vs baselines: e.g., transition‑based parsing integration: 9.97% LAS relative error reduction (RER) vs StateTr baseline; when initializing with BERT, 27.65% LAS RER reported; RNGT shows significant LAS improvements across 13 UD languages (average improvements reported in paper tables); SRL and coreference experiments show improved F1 over comparable baselines (specific numbers in paper tables).",
            "impact_on_training": "Enables non‑autoregressive parallel edge prediction (faster inference per iteration), iterative refinement captures between‑edge correlations improving final graph quality; integrates effectively with pretrained Transformers (BERT initialization yields large gains), and allows joint embedding of text and graph for global structured decisions.",
            "limitations": "Does not linearize graphs into textual sequences — representation is an attention/bias augmentation rather than a text serialization; output-node set must be (a subset of) input nodes (paper notes limitation); scaling to very large graphs and tractable embedding size are open problems; canonicalization / deterministic serialization not addressed because representation is not text sequence based.",
            "comparison_with_other": "Outperforms or significantly improves over baselines such as UDify and SynTr on dependency parsing when used as a refinement module; compared to hard‑coding graph structure into networks, G2GT injects graph as a soft bias via attention and learns complementary attention patterns; RNGT iterative refinement yields better joint modelling of entire graphs compared to independent non‑autoregressive edge prediction.",
            "uuid": "e6992.0",
            "source_info": {
                "paper_title": "Transformers as Graph-to-Graph Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "JointGT",
            "name_full": "JointGT (Graph‑text joint representation learning for text generation from knowledge graphs)",
            "brief_description": "A graph‑text joint encoding approach that uses Graph Transformer style representations to jointly encode knowledge graphs and text for knowledge‑to‑text generation tasks (cited as an application of Graph Transformer approaches).",
            "citation_title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "mention_or_use": "mention",
            "representation_name": "graph‑text joint encoding (JointGT)",
            "representation_description": "A joint encoding that fuses graph (knowledge graph) structure with text representations so that generation models can condition on graph structure for text generation; described in the paper only as a related work example without implementation details here.",
            "representation_type": "joint graph+text encoding (architecture embedding), not a pure textual linearization",
            "encoding_method": null,
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "knowledge‑to‑text generation / text generation from knowledge graphs",
            "model_name": "JointGT (Ke et al., 2021) — Graph‑text joint representation",
            "model_description": "Not detailed in this paper; cited as a Graph Transformer approach that jointly represents graphs and text for generation.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Only mentioned in related work here; paper does not report advantages/limitations of JointGT itself.",
            "comparison_with_other": "Mentioned as an example of GT approach applied to knowledge→text generation; no direct empirical comparison provided in this paper.",
            "uuid": "e6992.1",
            "source_info": {
                "paper_title": "Transformers as Graph-to-Graph Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "TableFormer",
            "name_full": "TableFormer",
            "brief_description": "A Graph Transformer approach that combines text and table encodings for table‑based question answering, cited as an example of Graph Transformer application to structured (table) + text input.",
            "citation_title": "TableFormer: Robust transformer modeling for table-text encoding",
            "mention_or_use": "mention",
            "representation_name": "table‑text joint encoding (TableFormer)",
            "representation_description": "Encodes table structure (rows, columns, cell relations) together with text using Graph Transformer style biases in attention to model table‑text interactions; referenced in related work without technical detail in this paper.",
            "representation_type": "joint structured+text embedding (attention bias), not an explicit graph→text linearization",
            "encoding_method": null,
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "table‑based question answering / joint table‑text encoding",
            "model_name": "TableFormer (Yang et al., 2022)",
            "model_description": "Not specified here; cited as a GT application combining table structure with text in attention.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": null,
            "limitations": "Only mentioned in related work; no experimental details or limitations provided in this paper.",
            "comparison_with_other": "Cited as an application of graph‑aware attention for structured inputs (table + text); no direct comparisons reported in this paper.",
            "uuid": "e6992.2",
            "source_info": {
                "paper_title": "Transformers as Graph-to-Graph Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph-to-graph transformer for transition-based dependency parsing",
            "rating": 2,
            "sanitized_title": "graphtograph_transformer_for_transitionbased_dependency_parsing"
        },
        {
            "paper_title": "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement",
            "rating": 2,
            "sanitized_title": "recursive_nonautoregressive_graphtograph_transformer_for_dependency_parsing_with_iterative_refinement"
        },
        {
            "paper_title": "Syntax-aware graph-to-graph transformer for semantic role labelling",
            "rating": 2,
            "sanitized_title": "syntaxaware_graphtograph_transformer_for_semantic_role_labelling"
        },
        {
            "paper_title": "Graph refinement for coreference resolution",
            "rating": 2,
            "sanitized_title": "graph_refinement_for_coreference_resolution"
        },
        {
            "paper_title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "TableFormer: Robust transformer modeling for table-text encoding",
            "rating": 2,
            "sanitized_title": "tableformer_robust_transformer_modeling_for_tabletext_encoding"
        },
        {
            "paper_title": "Graphormer",
            "rating": 1,
            "sanitized_title": "graphormer"
        }
    ],
    "cost": 0.012178249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformers as Graph-to-Graph Models</p>
<p>James Henderson james.henderson@idiap.ch 
Idiap Research Institute</p>
<p>Alireza Mohammadshahi alireza.mohammadshahi@epfl.com 
Idiap Research Institute</p>
<p>EPFL</p>
<p>University of Zurich</p>
<p>Andrei C Coman andrei.coman@idiap.ch 
Idiap Research Institute</p>
<p>EPFL</p>
<p>Lesly Miculicich lmiculicich@google.com 
Transformers as Graph-to-Graph Models
BFFC49A683C3924C4DA2DED3748025CA
We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case.Attention weights are functionally equivalent to graph edges.Our Graphto-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers.Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy.Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.</p>
<p>Introduction</p>
<p>Computational linguists have traditionally made extensive use of structured representations to capture the regularities found in natural language.The huge success of Transformers (Vaswani et al., 2017) and their pre-trained large language models (Devlin et al., 2019;Zhang et al., 2022;Touvron et al., 2023a,b) have brought these representations into question, since these models are able to capture even subtle generalisations about language and meaning in an end-to-end sequence-to-sequence model (Wu et al., 2020;Michael et al., 2020;Hewitt et al., 2021).This raises issues for research that still needs to model structured representations, such as work on knowledge graphs, hyperlink graphs, citation graphs, or social networks.</p>
<p>In this paper we show that the sequence-tosequence nature of most Transformer models is only a superficial characteristic; underlyingly they * Work done while working at Idiap Research Institute.† Now at Google are in fact modelling complex structured representations.We survey versions of the Transformer architecture which integrate explicit structured representations with the latent structured representations of Transformers.These models can jointly embed both the explicit structures and the latent structures in a Transformer's sequence-of-vectors hidden representation, and can predict explicit structures from this embedding.In the process, we highlight evidence that the latent structures of pretrained Transformers already include much information about traditional linguistic structures.These Transformer architectures support explicit structures which are general graphs, making them applicable to a wide range of structured representations and their integration with text.</p>
<p>The key insight of this line of work is that attention weights and graph structure edges are effectively the same thing.Linguistic structures are fundamentally an expression of locality in the interaction between different components of a representation.As Henderson (2020) argued, incorporating this information about locality in the inductive bias of a neural network means putting connections between hidden vectors if their associated components are local in the structure.In Transformers (Vaswani et al., 2017), these connections are learned in the form of attention weights.Thus, these attention weights are effectively the induced structure of the Transformer's latent representation.</p>
<p>However, attention weights are not explicitly part of a Transformer's hidden representation.The output of a Transformer encoder is a sequence of vectors, and the same is true of each lower layer of self-attention.The latent attention weights are extracted from these sequence-of-vector embeddings with learned functions of pairs of vectors.Edges in explicit graphs can be predicted in the same way (from pairs of vectors), assuming that these graphs have also been embedded in the sequence of vectors.</p>
<p>In recent years, the main innovation has been in how to embed explicit graphs in the hidden representations of Transformers.In our work on this topic, we follow the above insight and input the edges of the graph into the computation of attention weights.Attention weights are computed from an n × n matrix of attention scores (where n is the sequence length), so we input the label of the edge between nodes i and j into the score computation for the i,j cell of this matrix.Each edge label has a learned embedding vector, which is input to the attention score function in various ways depending on the architecture.This allows the Transformer to integrate the explicit graph into its own latent attention graph in flexible and powerful ways.This integrated attention graph can then determine the Transformer's sequence-of-vectors embedding in the same way as standard Transformers.</p>
<p>Researchers from the Natural Language Understanding group at Idiap Research Institute have developed this architecture for inputting and predicting graphs under the name of Graph-to-Graph Transformer (G2GT).G2GT allows conditioning on an observed graph and predicting a target graph.For the case where a graph is only observed at training time, we not only want to predict its edges, we also want to integrate the predicted graph into the Transformer embedding.This has a number of advantages, most notably the ability to jointly model all the edges of the graph.By iteratively refining the previous predicted graph, G2GT can jointly model the entire predicted graph even though the actual prediction is done independently for each edge.And this joint modelling can be done in conjunction with other explicit graphs, as well as with the Transformer's induced latent graph.</p>
<p>Our work on G2GT has included a number of different explicit graph structures.The original methods were developed on syntactic parsing (Mohammadshahi andHenderson, 2021, 2020;Mohammadshahi, 2023).The range of architectures was further explored for semantic role labelling (Mohammadshahi and Henderson, 2023) and collocation recognition (Espinosa Anke et al., 2022).G2GT's application to coreference resolution extended the complexity of graphs to two levels of representation (mention spans and coreference chains) over an entire document, which was all modelled with iterative refinement of a single graph (Miculicich and Henderson, 2022) In the rest of this paper, we start with a review of related work on deep learning for graph modelling (Section 2).We then present the general G2GT architecture with iterative refinement (Section 3), before discussing the specific versions we have evaluated on specific tasks (Section 4).We then discuss the broader implications of these results (Section 5), and conclude with a discussion of future work (Section 6).</p>
<p>Deep Learning for Graphs</p>
<p>Graph Neural Networks.Early attempts at broadening the application of neural networks to graph structures were pursued by Gori et al. (2005) and Scarselli et al. (2008), who introduced the Graph Neural Networks (GNNs) architecture as a natural expansion of Recurrent Neural Networks (RNNs) (Hopfield, 1982).This architecture regained interest in the context of deep learning, expanded through the inclusion of spectral convolution layers (Bruna et al., 2013), gated recurrent units (Li et al., 2015), spatial convolution layers (Kipf and Welling, 2017), and attention layers (Veličković et al., 2018).GNNs generally employ the iterative local message passing mechanism to aggregate information from neighbouring nodes (Gilmer et al., 2017).Recent research, analysing GNNs through the lens of Weisfeiler and Leman (1968), has highlighted two key issues: over-smoothing (Oono and Suzuki, 2020) and over-squashing (Alon and Yahav, 2021).Over-smoothing arises from repeated aggregation across layers, leading to convergence of node features and loss of discriminative information.Over-squashing, on the other hand, results from activation functions during message aggregation, causing significant information and gradient loss.These issues limit the capacity of GNNs to effectively capture long-range dependencies and nuanced graph relationships (Topping et al., 2021).The Transformer architecture (Vaswani et al., 2017) can be seen as addressing these issues, in that its stacked layers of self-attention can be seen as a fixed sequence of learned aggregation steps.</p>
<p>Graph Transformers.Transformers (Vaswani et al., 2017), initially designed for sequence tasks, represent a viable and versatile alternative to GNNs due to their intrinsic graph processing capabilities.Through their self-attention mechanism, they can seamlessly capture global wide-ranging relationships, akin to handling a fully-connected graph.Shaw et al. (2018) explicitly input relative position relations as embeddings into the attention function, thereby effectively inputting the relative position graph, instead of absolute position embeddings, to represent the sequence.Generalising this explicit input strategy to arbitrary graphs (Henderson, 2020) has led to a general class of models which we will refer to as Graph Transformers (GT).</p>
<p>GT Evolution and Applications.The history of graph input methods used in GTs started with Transformer variations that experimented with relative positions to more effectively capture distance between input elements.Rather than adopting the sinusoidal position embedding introduced by Vaswani et al. (2017) or the absolute position embedding proposed by Devlin et al. (2019), Shaw et al. (2018) added relative position embeddings to attention keys and values, capturing token distance within a defined range.Dai et al. (2019) proposed Transformer-XL, which used content-dependent positional scores and a global positional score in attention weights.Mohammadshahi and Henderson (2020) demonstrated one of the earliest successful integration of an explicit graph into Transformer's latent attention graph.They introduced the Graph-To-Graph Transformer (G2GT) architecture and applied it to syntactic parsing tasks by effectively leveraging pre-trained models such as BERT (Devlin et al., 2019).Huang et al. (2020) introduced new methods to enhance interaction between query, key and relative position embeddings within the self-attention mechanism.Su et al. (2021) proposed RoFormer, which utilises a rotation matrix to encode absolute positions while also integrating explicit relative position dependencies into the self-attention formulation.Liutkus et al. (2021) and Chen (2021) extended Performer (Choromanski et al., 2020) to support relative position encoding while scaling Transformers to longer sequences with a linear attention mechanism.Graphormer (Ying et al., 2021) introduced node centrality encoding as an additional input level embedding vector, node distances and edges as soft biases added at attention level, and obtained excellent results on a broad range of graph representation learning tasks.Mohammadshahi and Henderson (2021) built upon the G2GT architecture and proposed an iterative refinement procedure over previously predicted graphs, using a non-autoregressive approach.SSAN (Xu et al., 2021) leveraged the GT approach to effectively model mention dependencies for document-level relation extraction tasks.JointGT (Ke et al., 2021) exploited the GT approach for knowledge to text generation tasks via a joint graph-text encoding.Similarly, TableFormer (Yang et al., 2022) demonstrated the successful utilisation of the GT approach for combined text-table encoding in tablebased question answering tasks.Espinosa Anke et al. ( 2022) proposed a GT architecture for simultaneous collocation extraction and lexical function typification, incorporating syntactic dependencies into the attention mechanism.Miculicich and Henderson (2022) showed that the G2GT iterative refinement procedure can be effectively applied to graphs at multiple levels of representation.Diao and Loynd (2022) further extended a GT architecture with new edge and node update methods and applied them to graph-structured problems.QAT (Park et al., 2022) substantially expanded upon GT models to jointly handle language and graph reasoning in question answering tasks.In the study conducted by Mohammadshahi and Henderson (2023), the G2GT model showed substantial improvements in the semantic role labelling tasks.The multitude of successful applications and extensions firmly establish Graph Transformers as a robust and adaptable framework for addressing complex challenges in language and graphs.</p>
<p>Graph-to-Graph</p>
<p>Transformer Architecture</p>
<p>Our Graph-to-Graph Transformer (G2GT) architecture combines the idea of inputting graph edges into the self-attention function with the idea of predicting graph edges with an attention-like function.By encoding the graph relations into the self-attention mechanism of Transformers, the model has an appropriate linguistic bias, without imposing hard restrictions.Specifically, G2GT modifies the attention mechanism of Transformers (Vaswani et al., 2017) to input any graph.Given the input sequence W = (x 1 ,x 2 ,...,x n ), and graph relations G = {(x i ,x j ,l),1 ≤ i,j ≤ n,l ∈ L} (where L is the set of labels), the modified self-attention mechanism is calculated as1 :</p>
<p>(1)
e ij = 1 √ d x i W Q (x j W K ) T +x i W Q (r ij W R 1 ) T +r ij W R 2 (x j W K ) T
where r ij ∈ {0, 1} |L| is a one-hot vector which specifies the type of the relation between x i and x j , 2 W R 1 , W R 2 ∈ R |L|×d are matrices of graph relation embeddings which are learned during training, |L| is the label size, and d is the size of hidden representations.The value equation of Transformer (Vaswani et al., 2017) is also modified to pass information about graph relations to the output of the attention function:
z i = j α ij (x j W V +r ij W R 3 ) (2)
where W R 3 ∈ R |L|×d is another learned relation embedding matrix.</p>
<p>To extract the explicit graph from the sequence of vectors output by the Transformer, a classification module is applied to pairs of vectors and maps them into the label space L. Initially, the module transforms each vector into distinct head and tail representations using dedicated projection matrices.Subsequently, a classifier (linear, bilinear or MLP) is applied, to map the vector pair onto predictions over the label space.Notably, each edge prediction can be computed in parallel (i.e. in a non-autoregressive manner), as predictions for each pair are independent of one another.Given the discrete nature of the output, various decoding methods can be employed to impose desired constraints on the complete output graph.These can range from straightforward head-tail order constraints, to more complex decoding algorithms such as the Minimum Spanning Tree (MST) algorithm.</p>
<p>Having an architecture which can both condition on graphs and predict graphs gives us the powerful ability to do iterative refinement of arbitrary graphs.Even when graph prediction is non-autoregressive, conditioning on the previously predicted graph allows the model to capture between-edge correlations like an autoregressive model.As illustrated in Figure 1, we propose Recursive Non-autoregressive G2GT (RNGT), et al. ( 2022) provide a survey of previous proposals for relative position encoding.In ongoing work, we have found that using a relation embedding vector to reweight the dimensions in standard dot-product attention works well for some applications.</p>
<p>2 This formulation can be easily extended to multilabel graphs by removing the one-hot constraint.We are investigating the most effective method for doing this.</p>
<p>Initial Graph Predictor</p>
<p>Input Sentence which predicts all edges of the graph in parallel, and is therefore non-autoregressive, but can still condition every edge prediction on all other edge predictions by conditioning on the previous version of the graph (using Equations 1 and 2).
G 0 RNG Transformer G t-1 G T S 1 S 2 S 3 S 4 S 5 L 1 L 4 L 2 L 3 Decoder CLS S 1 S 2 S 3 S 4 S 5 SEP Z 0 Z 1 Z 2 Z 3 Z 4 Z 5 Z 6 ... ...
The input to the model is the input graph W (e.g. a sequence of tokens), and the output is the final graph G T over the same set of nodes.First, we compute an initial graph G 0 over the nodes of W , which can be done with any model.Then each recursive iteration encodes the previous graph G t−1 and predicts a new graph G t .It can be formalised in terms of an encoder E RNG and a decoder D RNG :
Z t = E RNG (W,G t−1 ) G t = D RNG (Z t ) t = 1,...,T(3)
where Z represents the set of vectors output by the model, and T indicates the number of refinement iterations.Note that in each step of this iterative refinement process, the G2G Transformer first computes a set of vectors which embeds the predicted graph (i.e.E RNG (W, G t−1 )), before extracting the edges of the predicted graph from this set-of-vectors embedding (i.e.D RNG (Z t )).</p>
<p>G2GT Models and Results</p>
<p>This section provides a more comprehensive explanation of each alternative G2GT model we have ex-plored, along with an outline of how we've applied these models to address various graph modelling problems.The empirical success of these models demonstrate the computational adequacy of Transformers for extracting and modelling graph structures which are central to the nature of language.</p>
<p>The large further improvements gained by initialising with pretrained models demonstrates that Transformer pretraining encodes information about linguistic structures in its attention mechanisms.</p>
<p>Syntactic Parsing</p>
<p>Syntactic parsing is the process of analysing the grammatical structure of a sentence, including identifying the subject, verb, and object.Syntactic dependency parsing is a critical component in a variety of natural language understanding tasks, such as semantic role labelling (Henderson et al., 2013;Marcheggiani andTitov, 2017, 2020), machine translation (Chen et al., 2017), relation extraction (Zhang et al., 2018), and natural language inference (Pang et al., 2019).It is also a benchmark structured prediction task, because architectures which are not powerful enough to learn syntactic parsing cannot be computationally adequate for language understanding.Syntactic structure is generally specified in one of two popular grammar styles, constituency parsing (i.e.phrase-structure parsing) (Manning and Schutze, 1999;Henderson, 2003Henderson, , 2004;;Titov and Henderson, 2007a) and dependency parsing (Nivre, 2003;Titov and Henderson, 2007b;Carreras, 2007;Nivre and McDonald, 2008;Dyer et al., 2015;Barry et al., 2021).There are two main approaches to compute the dependency tree: transition-based and graph-based parsers.Transition-based parsers predict the dependency graph one edge at a time through a sequence of parsing actions (Yamada and Matsumoto, 2003;Nivre and Scholz, 2004;Titov and Henderson, 2007b;Zhang and Nivre, 2011;Weiss et al., 2015;Yazdani and Henderson, 2015), and graph-based parsers compute scores for every possible dependency edge and then apply a decoding algorithm to find the highest scoring total tree (McDonald et al., 2005;Koo and Collins, 2010;Kuncoro et al., 2016;Zhou and Zhao, 2019).</p>
<p>In the following, we outline our proposals for using G2GT for syntactic parsing tasks.</p>
<p>Transition-based Dependency Parsing</p>
<p>In (Mohammadshahi and Henderson, 2020), we integrate the G2GT model with two baselines, named StateTransformer (StateTr) and Sentence-Transformer (SentTr).In the former model, we directly input the parser state into the G2GT model, while the latter takes the initial sentence as the input.For better efficiency of our transition-based model, we used an alternative version of G2GT, introduced in Section 3, where the interaction of graph relations with key matrices in Equation 1is removed.Each parser decision is conditioned on the history of previous decisions by inputting an unlabelled partially constructed dependency graph to the G2GT model.Mohammadshahi and Henderson (2020) evaluate the integrated models on the English Penn Treebank (Marcus et al., 1993), and 13 languages of Universal Dependencies Treebanks (Nivre et al., 2018).</p>
<p>Results of our models on the Penn Treebank are shown in Table 1 (see (Mohammadshahi and Henderson, 2020) for further results on UD Treebanks).Integrating the G2GT model with the StateTr baseline achieves 9.97% LAS Relative Error Reduction (RER) improvement, which confirms the effectiveness of modelling the graph information in the attention mechanism.Furthermore, initialising our model weights with the BERT model (Devlin et al., 2019), provides significant improvement (27.65% LAS RER), which shows the compatibility of our modified attention mechanism with the latent representations learned by BERT pretraining.Integrating the G2GT model with the SentTr baseline results in a similar significant improvement (4.62% LAS RER).</p>
<p>Graph-based Dependency Parsing</p>
<p>The StateTr and SentTr models generate the dependency graph in an autoregressive manner, predicting each parser action conditioned on the history of parser actions.Many previous models have achieved better results with graph-based parsing methods, which use non-autoregressive computation of scores for all individual candidate dependency relations and then use a decoding method to reach the maximum scoring structure (McDonald et al., 2005;Koo and Collins, 2010;Ballesteros et al., 2016;Wang and Chang, 2016;Kuncoro et al., 2016;Zhou and Zhao, 2019).However, these models usually ignore correlations between edges while predicting the complete graph.In (Mohammadshahi and Henderson, 2021), we propose the Recursive Non-autoregressive Graphto-Graph Transformer (RNGT) architecture, as discussed in Section 3. The RNGT architecture can be applied to any task with a sequence or graph as input and a graph over the same set of nodes as output.Here, we apply it for the syntactic dependency parsing task, and preliminary experiments showed that removing the interaction of graph relations with key vectors, in Equation 1, results in better performance and a more efficient attention mechanism.Mohammadshahi and Henderson (2021) evaluate this RNGT model on Universal Dependency (UD) Treebanks (Nivre et al., 2018), Penn Treebanks (Marcus et al., 1993), and the German CoNLL 2009 Treebank (Hajič et al., 2009) for the syntactic dependency parsing task.</p>
<p>Table 2 shows the results on 13 languages of UD Treebanks.First, we use UDify (Kondratyuk and Straka, 2019), the previous state-of-the-art multilingual dependency parser, as the initial parser for the RNGT model.The integrated model achieves significantly better LAS performance than the UDify model in all languages, which demonstrates the effectiveness of the RNGT model at refining a dependency graph.Then, we combine RNGT with Syntactic Transformer (SynTr), a stronger monolingual dependency parser, which has the same architecture as the RNGT model except without the graph input mechanism.The SynTr+RNGT model reaches further improvement over the strong SynTr baseline (four languages are significant), which is stronger evidence for the effectiveness of the graph refinement method.Interestingly, there is little difference between the performance with different initial parsers, implying that the RNGT model is effective enough to refine any initial graphs.In fact, even when we initialise with an empty parse, the Empty+RNGT model achieves competitive results with the other RNGT models, again confirming our powerful method of graph refinement.</p>
<p>Semantic Role Labelling</p>
<p>The semantic role labelling (SRL) task provides a shallow semantic representation of a sentence and builds event properties and relations among relevant words, and is defined in both dependencybased (Surdeanu et al., 2008) andspan-based (Carreras andMàrquez, 2005;Pradhan et al., 2012) styles.Previous work (Marcheggiani and Titov, 2017;Strubell et al., 2018;Cai and Lapata, 2019;Fei et al., 2021;Zhou et al., 2020) showed that the syntactic graph helps SRL models to predict better output graphs, but finding the most effective way to incorporate the auxiliary syntactic information into SRL models was still an open question.In (Mohammadshahi and Henderson, 2023), we introduce the Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) architecture.The model conditions on the sentence's dependency structure and jointly predicts both span-based (Carreras and Màrquez, 2005) and dependency-based (Hajič et al., 2009) SRL structures.Regarding the self-attention mechanism, we remove the interaction of graph embeddings with value vectors in Equation 2, as it reaches better performance in this particular task (Mohammadshahi and Henderson, 2023).</p>
<p>Results for span-based SRL are shown in Table 3.Without initialising the models with BERT (Devlin et al., 2019), the SynG2G-Tr model outperforms a previous comparable state-of-the-art model (Strubell et al., 2018) in both end-to-end and given-predicate scenarios.The improvement indicates the benefit of encoding the graph information in the self-attention mechanism of Transformer with a soft bias, instead of hard-coding the graph structure into deep learning models (Marcheggiani and Titov, 2017;Strubell et al., 2018;Xia et al., 2019), as the model can still learn other attention patterns in combination with this graph knowledge.BERT (Devlin et al., 2019) initialisation results in further significant improvement in both settings, which again shows the compatibility of the G2GT modified self-attention mechanism with the latent structures learned by BERT pretraining.</p>
<p>Coreference Resolution</p>
<p>Coreference resolution (CR) is an important and complex task which is necessary for higher-level semantic representations.We show that it benefits from a graph-based global optimisation of all the coreference chains in a document.</p>
<p>CR Task Definition and Background</p>
<p>Coreference resolution is the task of linking all linguistic expressions in a text that refer to the same entity.Solutions for this task involve three parts: mention-detection (Yu et al., 2020;Miculicich and Henderson, 2020), classification or ranking of mentions, and finally reconciling the decisions to create entity chains.These approaches fall within three principal categories: mention-pair models which perform binary decisions (McCarthy and Lehnert, 1995;Aone and William, 1995;Soon et al., 2001), entity-based models which focus on maintaining single underlying entity representation, contrasting the independent pair-wise decisions of mentionpair approaches (Clark andManning, 2015, 2016), and ranking models which aim at ranking the possible antecedents of each mention instead of making binary decisions (Wiseman et al., 2016).A  limitation of these methods lies in their bottom-up construction, resulting in an underutilisation of comprehensive global information regarding coreference links among all mentions in individual decisions.Furthermore, these methods tend to exhibit significant complexity.Modelling of coreference resolution as a graph-based approach offer an alternative to deal with these limitations.</p>
<p>Iterative Graph-based CR</p>
<p>Miculicich and Henderson (2022) proposed a novel approach to modelling coreference resolution, treating it as a graph problem.In this framework, the tokens within the text serve as nodes, and the connections between them signify coreference links (see Figure 2).Given a document D = [x 1 ,...,x N ] with length N , the coreference graph is formally defined as the matrix G ⊂ N N ×N , which represents the relationships between tokens.Specifically, the relationship type between any two tokens, x i and x j , is labelled as g i,j ∈ {0,1,2} for the three distinct relation types: (0) no link, (1) mention link, and (2) coreference link.The primary objective of this approach is to learn the conditional probability distribution p(G|D).To achieve this, an iterative refinement strategy is employed, which captures interdependencies among relations.The model iterates over the same document D for a total of T iterations.In each iteration t, the predicted coreference graph G t is conditioned on the previous prediction, denoted as G t−1 .Thus, the conditional probability distribution of the model is defined as follows:
p(G t |D,G t−1 ) = N i=1 i j=1 p(g i,j |D,G t−1 ) (4)
The proposed model operates on two levels of representation.In each iteration, it predicts the entire graph.However, during the first iteration, the model focuses on predicting edges that pinpoint mention spans, given that coreferent links only have relevance when mentions are detected.From the second iteration, both mention links, and coreference links are refined.This iterative strategy permits the model to enhance mention-related decisions based on coreference resolutions, and vice versa.This framework utilises iterative graph refinement as a substitute for conventional pipeline architectures in multi-level deep learning models.The iterative process concludes either when the graph no longer undergoes changes or when a predetermined maximum iteration count is attained (see Figure 3).</p>
<p>Ideally, encoding the entirety of the document in a single pass would be optimal.However, in practical scenarios, a constraint on maximum length arises due to limitations in hardware memory capacity.To address this challenge, Miculicich and Henderson (2022) introduce two strategies: overlapping windows and reduced document approach.In the latter strategy, mentions are identified during an initial iteration with a focus on optimising recall, as previously suggested in (Miculicich and Henderson, 2020).Only the representations of these identified spans are subsequently used as inputs for the following iterations.Miculicich and Henderson (2022) conducted experiments on the CoNLL 2012 corpus (Pradhan et al., 2012) and showed improvements over relevant baselines and previous state-of-the-art methods, summarised in Table 4.We compare our model with three baselines: Lee et al. (2017) proposed the first end-to-end model for coreference resolution; Lee et al. (2018) extended the previous model by introducing higher order inference; and Xu and Choi (2020) used the span based pretrained model SpanBERT (Joshi et al., 2020).The 'Baseline' of Lee et al. (2018) uses ELMo (Peters et al., 2018) to obtain token representations, so versions of this Baseline which use 'BERT-large' (Joshi et al., 2019) and 'SpanBERT-large' (Joshi et al., 2020) as their pretrained models, are directly comparable to our 'G2GT BERT-large' and 'G2GT SpanBERT-large' models, respectively.</p>
<p>These results show that coreference resolution benefits from making global coreference decisions using document-level information, as supported by the G2GT architecture.Our model achieves its optimal solution within a maximum of three iterations.Notably, due to the model's ability to predict the entire graph in a single iteration, its computational complexity is lower compared to that of the baseline approaches.</p>
<p>Discussion</p>
<p>The empirical success of Graph-to-Graph Transformers on modelling these various graph structures helps us understand how Transformers model language.This success demonstrates that Transformers are computationally adequate for modelling linguistic structures, which are central to the nature of language.The reliance of these G2GT models on using self-attention mechanisms to extract and encode these graph relations shows that self-attention is crucial to how Transformers can do this modelling.The large improvements gained by initialising with pretrained models indicates that pretrained Transformers are in fact using the same mechanisms to learn about this linguistic structure, but in an unsupervised fashion.</p>
<p>These insights into pretrained Transformers give us a better understanding of the current generation of Large Language Models (LLMs).It is not that these models do not need linguistic structure (since their attention mechanisms do learn it); it is that these models do not need supervised learning of linguistic structure.But perhaps in a Table 4: Evaluation of CR on the test set (CoNLL 2012) in terms of precision (P), recall (R) and F1 score for three metrics, as well as the average F1 over metrics.* significant at p &lt; 0.01 compared to (Joshi et al., 2020), † significant at p &lt; 0.05 compared to (Xu and Choi, 2020).low-resource scenario LLMs would benefit from the inductive bias provided by supervised learning of linguistic structures, such as for many of the world's languages other than English.And these insights are potentially relevant to the issues of interpretability and controllability of LLMs.</p>
<p>These insights are also relevant for any applications which could benefit from integrating text with structured representations.Our current work investigates jointly embedding text and parts of a knowledge base in a single G2GT model, providing a way to integrate interpretable structured knowledge with knowledge in text.Such representations would be useful for information extraction, question answering and information retrieval, amongst many other applications.Other graphs we might want to model with a Transformer and integrate with text include hyperlink graphs, citation graphs, and social networks.An important open problem with such models is the scale of the resulting Transformer embedding.</p>
<p>Conclusion and Future Work</p>
<p>The Graph-to-Graph Transformer architecture makes explicit the implicit graph processing abilities of Transformers, but further research is needed to fully leverage the potential of G2GT.</p>
<p>Conclusions</p>
<p>The success of the above models of a variety linguistic structures shows that Transformers are underlyingly graph-to-graph models, not limited to sequence-to-sequence tasks.The G2GT architecture with its RNGT method provides an effective way to exploit this underlying ability when modelling explicit graphs, effectively integrating them with the implicit graphs learned by pre-trained Transformers.Inputting graph relations as features to the self-attention mechanism enables the information input to the model to be steered by domain-specific knowledge or desired outcomes but still learned by the Transformer, opening up the possibility for a more tailored and customised encoding process.Predicting graph relations with attention-like functions and then re-inputting them for iterative refinement, encodes the input, predicted and latent graphs in a single joint Transformer embedding which is effective for making global decisions about structure in a text.</p>
<p>Future Work</p>
<p>One topic of research where explicit graphs are indispensable is knowledge graphs.Knowledge needs to be interpretable, so that it can be audited, edited, and learned by people.And it needs to be integrated with existing knowledge graphs.Our current work uses G2GT to integrate knowledge graphs with knowledge conveyed by text.</p>
<p>One of the limitations of the models discussed in this paper is that the set of nodes in the output graph needs to be (a subset of) the nodes in the input graph.General purpose graph-to-graph mappings would require also predicting a set of new nodes in the output graph.One natural solution would be autoregressive prediction of one node at a time, as is done for text generation, but an exciting alternative would be to use methods from non-autoregressive text generation in combination with our iterative refinement method RNGT.</p>
<p>The excellent performance of the models presented in this paper suggest that many more problems can be successfully formulated as graph-to-graph problems and modelled with G2GT, in NLP and beyond.The code for G2GT and RNGT is open-source and publicly available at https://github.com/idiap/g2g-transformer.</p>
<p>Figure 1 :
1
Figure 1: The Recursive Non-autoregressive Graph-to-Graph Transformer architecture.</p>
<p>Figure 2 :
2
Figure 2: Example of a graph structure for coreference.Mention spans are shown in bold, and colours represent entity clusters.The mention heads are underlined.</p>
<p>Figure 3 :
3
Figure 3: Example of iterations with G2GT in two stages.</p>
<p>. Current work on knowledge extraction poses further challenges, most notably the issue of tractably modelling large graphs.The code for G2GT is open-source and available for other groups to use for other graph structures (at https://github.com/idiap/g2g-transformer).</p>
<p>Table 2 :
2
(Devlin et al., 2019)a, 2019) monolingual (SynTr) and multilingual (UDify(Kondratyuk and Straka, 2019)) baselines, and the refined models (+RNGT) pre-trained with BERT(Devlin et al., 2019)on 13 languages of UD Treebanks.The relative error reduction after the integration is illustrated in parentheses.Bold scores are not significantly different from the best score in that row (with α = 0.01).
LanguageMulti Multi+Mono UDify UDify+RNGTMono Mono SynTr SynTr+RNGTMono Empty+RNGTArabic82.88 85.93 (+17.81%) 86.23 86.31 (+0.58%)86.05Basque80.97 87.55 (+34.57%) 87.49 88.2 (+5.68%)87.96Chinese83.75 89.05 (+32.62%) 89.53 90.48 (+9.08%)89.82English88.591.23 (+23.74%) 91.41 91.52 (+1.28%)91.23Finnish82.03 91.87 (+54.76%) 91.80 91.92 (+1.46%)91.78Hebrew88.11 90.80 (+22.62%) 91.07 91.32 (+2.79%)90.56Hindi91.46 93.94 (+29.04%) 93.95 94.21 (+4.3%)93.97Italian93.69 94.65 (+15.21%) 95.08 95.16 (+1.62%)94.96Japanese92.08 95.41 (+42.06%) 95.66 95.71 (+1.16%)95.56Korean74.26 89.12 (+57.73%) 89.29 89.45 (+1.5%)89.1Russian93.13 94.51 (+20.09%) 94.60 94.47 (-2.4%)94.31Swedish89.03 92.02 (+27.26%) 92.03 92.46 (+5.4%)92.40Turkish67.44 72.07 (+14.22%) 72.52 73.08 (+2.04%)71.99Average85.18 89.8690.05 90.3389.98Modelin-domain out-of-domainend-to-endStrubell et al. (2018)84.9974.66SynG2G-Tr (w/o BERT)85.4575.26+pre-trainingStrubell et al. (2018)86.978.25SynG2G-Tr87.5780.53given predicateStrubell et al. (2018)86.0476.54SynG2G-Tr (w/o BERT)86.5077.45+pre-trainingJia et al. (2022)88.2581.90SynG2G-Tr88.9383.21</p>
<p>Table 3 :
3
Comparing our SynG2G-Tr with previous comparable SoTA model on CoNLL 2005 test sets for both indomain (WSJ), and out-of-domain (Brown) sets.Scores being boldfaced means that they are significantly better.</p>
<p>Various alternative functions are possible for inputting relation embeddings into attention weight computations. Dufter
AcknowledgementWe would like to especially thank the Swiss National Science Foundation for funding this work, under grants 200021E_189458, CRSII5_180320, and 200021_178862.We would also like to thank other members of the the Natural Language Understanding group at Idiap Research Institute for useful discussion and feedback, including Florian Mai, Rabeeh Karimi, Andreas Marfurt, Melika Behjati, and Fabio Fehr.
. P R F1 P R F1 P R F1 Model, Avg, F1</p>
<p>. Lee, 2017</p>
<p>. Choi Xu, 2020</p>
<p>. Lee Baseline, 2018</p>
<p>. + Bert-Large (joshi, 2019</p>
<p>. -Large ( Spanbert, Joshi, 202085</p>
<ul>
<li>References Uri Alon and Eran Yahav. 2021. On the bottleneck of graph neural networks and its practical implications. G2GT SpanBERT-large reduced 85.9 86.0 * † 85.9 * 79.3 * 79.4 * † 79.3 * 76.4 75.9 * 76.1 * 80.5International Conference on Learning Representations. </li>
</ul>
<p>Globally normalized transition-based neural networks. Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, Michael Collins, 10.18653/v1/P16-1231Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Evaluating automated and manual acquisition of anaphora resolution strategies. Chinatsu Aone, Scott William, 10.3115/981658.98167533rd Annual Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, USAAssociation for Computational Linguistics1995</p>
<p>Training with exploration improves a greedy stack LSTM parser. Miguel Ballesteros, Yoav Goldberg, Chris Dyer, Noah A Smith, 10.18653/v1/D16-1211Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016. 2005-2010</p>
<p>The dcu-epfl enhanced dependency parser at the iwpt 2021 shared task. James Barry, Alireza Mohammadshahi, Joachim Wagner, Jennifer Foster, James Henderson, 10.18653/v1/2021.iwpt-1.22Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies. the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies2021IWPT 2021</p>
<p>Spectral networks and locally connected networks on graphs. Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, CoRR, abs/1312.62032013</p>
<p>Semi-supervised semantic role labeling with cross-view training. Rui Cai, Mirella Lapata, 10.18653/v1/D19-1094Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Experiments with a higher-order projective dependency parser. Xavier Carreras, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)Prague, Czech RepublicAssociation for Computational Linguistics2007</p>
<p>Introduction to the CoNLL-2005 shared task: Semantic role labeling. Xavier Carreras, Lluís Màrquez, Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005). the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)Ann Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Improved neural machine translation with a syntax-aware encoder and decoder. Huadong Chen, Shujian Huang, David Chiang, Jia , 10.18653/v1/p17-1177Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguisticsjun Chen. 20171</p>
<p>PermuteFormer: Efficient relative position encoding for long sequences. Peng Chen, 10.18653/v1/2021.emnlp-main.828Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Rethinking attention with performers. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J Colwell, Adrian Weller, ArXiv, abs/2009.147942020</p>
<p>Entity-centric coreference resolution with model stacking. Kevin Clark, Christopher D Manning, 10.3115/v1/P15-1136Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Improving coreference resolution by learning entity-level distributed representations. Kevin Clark, Christopher D Manning, 10.18653/v1/P16-1061Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Transformer-XL: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov, 10.18653/v1/P19-1285Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Relational attention: Generalizing transformers for graph-structured tasks. Cameron Diao, Ricky Loynd, ArXiv, abs/2210.050622022</p>
<p>Position information in transformers: An overview. Philipp Dufter, Martin Schmitt, Hinrich Schütze, 10.1162/coli_a_00445Computational Linguistics. 4832022</p>
<p>Transitionbased dependency parsing with stack long short-term memory. Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, Noah A Smith, 10.3115/v1/P15-1033Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Multilingual extraction and categorization of lexical collocations with graph-aware transformers. Luis Espinosa Anke, Alexander Shvets, Alireza Mohammadshahi, James Henderson, Leo Wanner, 10.18653/v1/2022.starsem-1.8Proceedings of the 11th Joint Conference on Lexical and Computational Semantics. the 11th Joint Conference on Lexical and Computational SemanticsSeattle, WashingtonAssociation for Computational Linguistics2022</p>
<p>Encoder-decoder based unified semantic role labeling with label-aware syntax. Fei Hao Fei, Bobo Li, Donghong Li, Ji, 10.1609/aaai.v35i14.17514Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Neural message passing for quantum chemistry. Justin Gilmer, Patrick F Samuel S Schoenholz, Oriol Riley, George E Vinyals, Dahl, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. Pmlr Marco Gori, Gabriele Monfardini, Franco Scarselli, 2005 IEEE International Joint Conference on Neural NetworksIEEE2017. 2005. 20052International conference on machine learning</p>
<p>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. Jan Hajič, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu, Nianwen Xue, Yi Zhang, Proceedings of the Thirteenth Conference on Computational Natural Language Learning. the Thirteenth Conference on Computational Natural Language LearningBoulder, ColoradoAssociation for Computational Linguistics2009. CoNLL 2009Shared Task</p>
<p>Inducing history representations for broad coverage statistical parsing. James Henderson, Proceedings of the 2003 Human Language Technology Conference of the North American Chapter. the 2003 Human Language Technology Conference of the North American Chapterthe Association for Computational Linguistics2003</p>
<p>Discriminative training of a neural network statistical parser. James Henderson, 10.3115/1218955.1218968Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, Spain2004</p>
<p>The unstoppable rise of computational linguistics in deep learning. James Henderson, 10.18653/v1/2020.acl-main.561Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model. James Henderson, Paola Merlo, Ivan Titov, Gabriele Musillo, 10.1162/COLI_a_00158Computational Linguistics. 3942013</p>
<p>Conditional probing: measuring usable information beyond a baseline. John Hewitt, Kawin Ethayarajh, Percy Liang, Christopher Manning, 10.18653/v1/2021.emnlp-main.122Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Neural networks and physical systems with emergent collective computational abilities. J John, Hopfield, Proceedings of the national academy of sciences. the national academy of sciences198279</p>
<p>Improve transformer models with better relative position embeddings. Zhiheng Huang, Davis Liang, Peng Xu, Bing Xiang, 10.18653/v1/2020.findings-emnlp.298Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Span-based semantic role labeling with argument pruning and second-order inference. Zixia Jia, Zhaohui Yan, Haoyi Wu, Kewei Tu, 10.1609/aaai.v36i10.21328Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>SpanBERT: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, Omer Levy, 10.1162/tacl_a_00300Transactions of the Association for Computational Linguistics. 82020</p>
<p>BERT for coreference resolution: Baselines and analysis. Mandar Joshi, Omer Levy, Luke Zettlemoyer, Daniel Weld, 10.18653/v1/D19-1588Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>JointGT: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2021.findings-acl.223Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Semisupervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, International Conference on Learning Representations. 2017</p>
<p>75 languages, 1 model: Parsing Universal Dependencies universally. Dan Kondratyuk, Milan Straka, 10.18653/v1/D19-1279Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Efficient third-order dependency parsers. Terry Koo, Michael Collins, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational Linguistics2010</p>
<p>Distilling an ensemble of greedy dependency parsers into one MST parser. Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Noah A Smith, 10.18653/v1/D16-1180Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>End-to-end neural coreference resolution. Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, 10.18653/v1/D17-1018Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Higher-order coreference resolution with coarse-to-fine inference. Kenton Lee, Luheng He, Luke Zettlemoyer, 10.18653/v1/N18-2108Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>Gated graph sequence neural networks. Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard S Zemel, CoRR, abs/1511.054932015</p>
<p>Relative positional encoding for transformers with linear complexity. Antoine Liutkus, Ondřej Cífka, Shih-Lun, Umut Wu, Yi-Hsuan Simsekli, Gaël Yang, Richard, International Conference on Machine Learning. 2021</p>
<p>Foundations of statistical natural language processing. Christopher Manning, Hinrich Schutze, 1999MIT press</p>
<p>Encoding sentences with graph convolutional networks for semantic role labeling. Diego Marcheggiani, Ivan Titov, 10.18653/v1/D17-1159Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Graph convolutions over constituent trees for syntax-aware semantic role labeling. Diego Marcheggiani, Ivan Titov, 10.18653/v1/2020.emnlp-main.322Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Building a large annotated corpus of English: The Penn Treebank. Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, Computational Linguistics. 1921993</p>
<p>Using decision trees for coreference resolution. Joseph F Mccarthy, Wendy G Lehnert, International Joint Conference on Artificial Intelligence. 1995</p>
<p>Online large-margin training of dependency parsers. Ryan Mcdonald, Koby Crammer, Fernando Pereira, 10.3115/1219840.1219852Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)Ann Arbor, MichiganAssociation for Computational Linguistics2005</p>
<p>Asking without telling: Exploring latent ontologies in contextual representations. Julian Michael, Jan A Botha, Ian Tenney, 10.18653/v1/2020.emnlp-main.552Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Partiallysupervised mention detection. Lesly Miculicich, James Henderson, Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference. the Third Workshop on Computational Models of Reference, Anaphora and CoreferenceBarcelona, SpainAssociation for Computational Linguistics2020</p>
<p>Graph refinement for coreference resolution. Lesly Miculicich, James Henderson, 10.18653/v1/2022.findings-acl.215Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Modeling structured data in attention-based models. EPFL. Alireza Mohammadshahi, 10.5075/epfl-thesis-98042023180</p>
<p>Graph-to-graph transformer for transition-based dependency parsing. Alireza Mohammadshahi, James Henderson, 10.18653/v1/2020.findings-emnlp.294Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement. Alireza Mohammadshahi, James Henderson, 10.1162/tacl_a_00358Transactions of the Association for Computational Linguistics. 92021</p>
<p>Syntax-aware graph-to-graph transformer for semantic role labelling. Alireza Mohammadshahi, James Henderson, 10.18653/v1/2023.repl4nlp-1.15Proceedings of the 8th Workshop on Representation Learning for NLP. the 8th Workshop on Representation Learning for NLPToronto, CanadaAssociation for Computational Linguistics2023. RepL4NLP 2023</p>
<p>An efficient algorithm for projective dependency parsing. Joakim Nivre, Proceedings of the Eighth International Conference on Parsing Technologies. the Eighth International Conference on Parsing TechnologiesNancy, France2003</p>
<p>Joakim Nivre, Mitchell Abrams, Željko Agić, Lars Ahrenberg, Lene Antonsen, Katya Aplonova, Maria Jesus Aranzabe, Gashaw Arutie, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Liesbeth Augustinus, Elena Badmaeva, Miguel Ballesteros, Esha Banerjee, Sebastian Bank, Barbu Verginica, Victoria Mititelu, John Basmov, Sandra Bauer, Kepa Bellato, Yevgeni Bengoetxea, Berzak, Ahmad Irshad, Riyaz Bhat, Erica Ahmad Bhat, Eckhard Biagetti, Rogier Bick, Victoria Blokland, Carl Bobicev, Cristina Börstell, Gosse Bosco, Sam Bouma, Adriane Bowman, Aljoscha Boyd, Marie Burchardt, Bernard Candito, Gauthier Caron, Gülşen Caron, Flavio Cebiroglu Eryigit, Giuseppe G A Massimiliano Cecchini, Slavomír Celano, Savas Čéplö, Fabricio Cetin, Jinho Chalub, Yongseok Choi, Jayeol Cho, Silvie Chun, Aurélie Cinková, Çagrı Collomb, Miriam Çöltekin, Marine Connor, Elizabeth Courtin, Carly Davidson, Peter Dickerson, Kaja Dirix, Timothy Dobrovoljc, Kira Dozat, Puneet Droganova, Marhaba Dwivedi, Ali Eli, Binyam Elkahky, Tomaž Ephrem, Aline Erjavec, Richárd Etienne, Hector Farkas, Jennifer Fernandez Alcalde, Cláudia Foster, Katarína Freitas, Daniel Gajdošová, Marcos Galbraith, Moa Garcia, Sebastian Gärdenfors, Kim Garza, Filip Gerdes, Iakes Ginter, Koldo Goenaga, Memduh Gojenola, Yoav Gökırmak, Xavier Goldberg, Berta Gonzáles Gómez Guinovart, Matias Saavedra, Normunds Grioni, Bruno Grūzītis, Céline Guillaume, Nizar Guillot-Barbance, Jan Habash, Jan Hajič, Linh Hajič Jr, Na-Rae Hà Mỹ, Kim Han, Dag Harris, Barbora Haug, Jaroslava Hladká, Florinel Hlaváčová, Petter Hociung, Jena Hohle, Radu Hwang, Elena Ion, O Irimia, Tomáš Ishola, Anders Jelínek, Fredrik Johannsen, Hüner Jørgensen, Sylvain Kaşıkara, Hiroshi Kahane, Jenna Kanayama, Boris Kanerva, Tolga Katz, Jessica Kayadelen, Václava Kenney, Jesse Kettnerová, Kamil Kirchner, Natalia Kopacewicz, Simon Kotsyba, Sookyoung Krek, Veronika Kwak, Lorenzo Laippala, Lucia Lambertino, Tatiana Lam, Lando, Dian Septina, Alexei Larasati, John Lavrentiev, Phùòng Lee, Alessandro Lê H Ồng, Saran Lenci, Herman Lertpradit, Leung, Ying Cheuk, Josie Li, Keying Li, Kyung-Tae Li, Nikola Lim, Olga Ljubešić, Olga Loginova, Teresa Lyashevskaya, Vivien Lynn, Aibek Macketanz, Michael Makazhanov, Christopher Mandl, Ruli Manning, Cȃtȃlina Manurung, David Mȃrȃnduc, Katrin Mareček, Marheinecke, Martínez Héctor, André Alonso, Jan Martins, Yuji Mašek, Ryan Matsumoto, Gustavo Mc-Donald, Niko Mendonça, Margarita Miekka, Anna Misirpashayeva, Cȃtȃlin Missilä, Yusuke Mititelu, Simonetta Miyao, Amir Montemagni, Laura Moreno More, Keiko Sophie Romero, Shinsuke Mori, Bjartur Mori, Bohdan Mortensen, Kadri Moskalevskyi, Yugo Muischnek, Kaili Murawaki, Pinkey Müürisep, Juan Nainwani, Ignacio Navarro Horñiacek, Hanna Nurmi, Stina Ojala, Adédayò Olúòkun, Mai Omura, Petya Osenova, Robert Östling, Lilja Øvrelid, Niko Partanen, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme Paulino-Passos, Siyao Peng, Cenel-Augusto Perez, Guy Perrier, Slav Petrov, Jussi Piitulainen, Emily Pitler, Gunta Nešpore-Bērzkalne, Lùòng Nguy ễn Thi . , Huy ền Nguy ễn Thi . Minh, Vitaly Nikolaev, Rattima Nitisaroj. Milan Stella, Jana Straka, Alane Strnadová, Umut Suhr, Zsolt Sulubacak, Dima Szántó, Yuta Taji, Takaaki Takahashi, Isabelle Tanaka, Trond Tellier, Trosterud, Anna Nedoluzhko; Barbara Plank; Nathan Schneider, Sebastian Schuster, Djamé Seddah, Wolfgang Seeker, Mojgan Seraji, Mo Shen, Atsuko Shimada; Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simkó, Mária Šimková, Kiril Simov, Aaron Smith, Isabela Soares-Bastos, Carolyn Spadine; Anna Trukhina, Reut Tsarfaty, Francis Tyers; Jonathan North Washington2018Marie-Catherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza, ; Muh Shohibussirri, Dmitry Sichinava ; Sumire Uematsu, Zdeňka Urešová, Larraitz Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Eric Villemonte de la Clergerie, Veronika Vincze, Lars Wallin, Jing Xian Wang ; Charles UniversityFaculty of Mathematics and Physics</p>
<p>Integrating graph-based and transition-based dependency parsers. Joakim Nivre, Ryan Mcdonald, Proceedings of ACL-08: HLT. ACL-08: HLTColumbus, OhioAssociation for Computational Linguistics2008</p>
<p>Deterministic dependency parsing of English text. Joakim Nivre, Mario Scholz, COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. Geneva, Switzerland. COLING2004</p>
<p>Graph neural networks exponentially lose expressive power for node classification. Kenta Oono, Taiji Suzuki, International Conference on Learning Representations. 2020</p>
<p>Improving natural language inference with a pretrained parser. Deric Pang, Lucy H Lin, Noah A Smith, 2019</p>
<p>Relation-aware language-graph transformer for question answering. Jinyoung Park, Kyu Hyeong, Juyeon Choi, Ko, Ji-Hoon Hyeon Ju Park, Jisu Kim, Kyungmin Jeong, Hyunwoo J Kim, Kim, ArXiv, abs/2212.009752022</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter. Long Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20181</p>
<p>CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. Alessandro Sameer Pradhan, Nianwen Moschitti, Olga Xue, Yuchen Uryupina, Zhang, Joint Conference on EMNLP and CoNLL -Shared Task. Jeju Island, KoreaAssociation for Computational Linguistics2012</p>
<p>The graph neural network model. Franco Scarselli, Marco Gori, Chung Ah, Markus Tsoi, Gabriele Hagenbuchner, Monfardini, 200820</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, 10.18653/v1/N18-2074Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>A machine learning approach to coreference resolution of noun phrases. Wee Meng Soon, Hwee Tou Ng, Daniel Chung, Yong Lim, 10.1162/089120101753342653Computational Linguistics. 2742001</p>
<p>Linguisticallyinformed self-attention for semantic role labeling. Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew Mccallum, 10.18653/v1/D18-1548Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu, ArXiv, abs/2104.098642021</p>
<p>The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, Joakim Nivre, CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning. Manchester, England; Prague, Czech RepublicAssociation for Computational Linguistics2008. 2008. 2007aProceedings of the 45th Annual Meeting of the Association of Computational Linguistics</p>
<p>A latent variable model for generative dependency parsing. Ivan Titov, James Henderson, Proceedings of the Tenth International Conference on Parsing Technologies. the Tenth International Conference on Parsing TechnologiesPrague, Czech RepublicAssociation for Computational Linguistics2007b</p>
<p>Understanding over-squashing and bottlenecks on graphs via curvature. Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M Bronstein, ArXiv, abs/2111.145222021</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. </p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, Graph Attention Networks. International Conference on Learning Representations. 2018Accepted as poster</p>
<p>Graph-based dependency parsing with bidirectional LSTM. Wenhui Wang, Baobao Chang, 10.18653/v1/P16-1218Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>The reduction of a graph to canonical form and the algebra which appears therein. nti, Series. Boris Weisfeiler, Andrei Leman, 19682</p>
<p>Structured training for neural network transition-based parsing. David Weiss, Chris Alberti, Michael Collins, Slav Petrov, 10.3115/v1/P15-1032Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Learning global features for coreference resolution. Sam Wiseman, Alexander M Rush, Stuart M Shieber, 10.18653/v1/N16-1114Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational Linguistics2016</p>
<p>Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu, 10.18653/v1/2020.acl-main.383Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Syntax-aware neural semantic role labeling. Qingrong Xia, Zhenghua Li, Min Zhang, Meishan Zhang, Guohong Fu, Rui Wang, Luo Si, 10.1609/aaai.v33i01.33017305Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction. Benfeng Xu, Quan Wang, Yajuan Lyu, Yong Zhu, Zhendong Mao, AAAI Conference on Artificial Intelligence. 2021</p>
<p>Revealing the myth of higher-order inference in coreference resolution. Liyan Xu, Jinho D Choi, 10.18653/v1/2020.emnlp-main.686Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Statistical dependency analysis with support vector machines. Hiroyasu Yamada, Yuji Matsumoto, Proceedings of the Eighth International Conference on Parsing Technologies. the Eighth International Conference on Parsing TechnologiesNancy, France2003</p>
<p>TableFormer: Robust transformer modeling for table-text encoding. Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, Shachi Paul, 10.18653/v1/2022.acl-long.40Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Incremental recurrent neural network dependency parser with search-based discriminative training. Majid Yazdani, James Henderson, 10.18653/v1/K15-1015Proceedings of the Nineteenth Conference on Computational Natural Language Learning. the Nineteenth Conference on Computational Natural Language LearningBeijing, ChinaAssociation for Computational Linguistics2015</p>
<p>Do transformers really perform bad for graph representation?. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu, Neural Information Processing Systems. 2021</p>
<p>Neural mention detection. Juntao Yu, Bernd Bohnet, Massimo Poesio, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pre-trained transformer language models</p>
<p>Transition-based dependency parsing with rich non-local features. Yue Zhang, Joakim Nivre, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USAAssociation for Computational Linguistics2011</p>
<p>Graph convolution over pruned dependency trees improves relation extraction. Yuhao Zhang, Peng Qi, Christopher D Manning, 10.18653/v1/D18-1244Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Parsing all: Syntax and semantics, dependencies and spans. Junru Zhou, Zuchao Li, Hai Zhao, 10.18653/v1/2020.findings-emnlp.398Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Head-Driven Phrase Structure Grammar parsing on Penn Treebank. Junru Zhou, Hai Zhao, 10.18653/v1/P19-1230Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>